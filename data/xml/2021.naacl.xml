<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.naacl">
  <volume id="main" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</booktitle>
      <editor><first>Kristina</first><last>Toutanova</last></editor>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <editor><first>Luke</first><last>Zettlemoyer</last></editor>
      <editor><first>Dilek</first><last>Hakkani-Tur</last></editor>
      <editor><first>Iz</first><last>Beltagy</last></editor>
      <editor><first>Steven</first><last>Bethard</last></editor>
      <editor><first>Ryan</first><last>Cotterell</last></editor>
      <editor><first>Tanmoy</first><last>Chakraborty</last></editor>
      <editor><first>Yichao</first><last>Zhou</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="63d26205">2021.naacl-main</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="0f48a8d4">2021.naacl-main.0</url>
      <bibkey>naacl-2021-2021</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Knowledge Router: Learning Disentangled Representations for Knowledge Graphs</title>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Xi</first><last>Rao</last></author>
      <author><first>Yi</first><last>Tay</last></author>
      <author><first>Ce</first><last>Zhang</last></author>
      <pages>1–10</pages>
      <abstract>The design of expressive representations of entities and relations in a knowledge graph is an important endeavor. While many of the existing approaches have primarily focused on learning from relational patterns and structural information, the intrinsic complexity of KG entities has been more or less overlooked. More concretely, we hypothesize KG entities may be more complex than we think, i.e., an entity may wear many hats and relational triplets may form due to more than a single reason. To this end, this paper proposes to learn disentangled representations of KG entities - a new method that disentangles the inner latent properties of KG entities. Our disentangled process operates at the graph level and a neighborhood mechanism is leveraged to disentangle the hidden properties of each entity. This disentangled representation learning approach is model agnostic and compatible with canonical KG embedding approaches. We conduct extensive experiments on several benchmark datasets, equipping a variety of models (DistMult, SimplE, and QuatE) with our proposed disentangling mechanism. Experimental results demonstrate that our proposed approach substantially improves performance on key metrics.</abstract>
      <url hash="3797397a">2021.naacl-main.1</url>
      <doi>10.18653/v1/2021.naacl-main.1</doi>
      <bibkey>zhang-etal-2021-knowledge</bibkey>
      <video href="2021.naacl-main.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors</title>
      <author><first>Fenia</first><last>Christopoulou</last></author>
      <author><first>Makoto</first><last>Miwa</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <pages>11–26</pages>
      <abstract>We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extraction by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the latent space of sentences via a Variational Autoencoder (VAE) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision indicate that multi-task learning results in performance benefits. Additional exploration of employing Knowledge Base priors into theVAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results.</abstract>
      <url hash="1e7f9f02">2021.naacl-main.2</url>
      <doi>10.18653/v1/2021.naacl-main.2</doi>
      <bibkey>christopoulou-etal-2021-distantly</bibkey>
      <video href="2021.naacl-main.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks</title>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Viet Dac</first><last>Lai</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>27–38</pages>
      <abstract>Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single model (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four tasks that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a dependency graph for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new regularization mechanism is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve representation learning. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages.</abstract>
      <url hash="3e7da70f">2021.naacl-main.3</url>
      <doi>10.18653/v1/2021.naacl-main.3</doi>
      <bibkey>nguyen-etal-2021-cross</bibkey>
      <video href="2021.naacl-main.3.mp4"/>
    </paper>
    <paper id="4">
      <title><fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation Guided Graph Encoding and Decoding for Joint Information Extraction</title>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>39–49</pages>
      <abstract>The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks.</abstract>
      <url hash="bc4f4426">2021.naacl-main.4</url>
      <doi>10.18653/v1/2021.naacl-main.4</doi>
      <bibkey>zhang-ji-2021-abstract</bibkey>
      <video href="2021.naacl-main.4.mp4"/>
      <pwccode url="https://github.com/zhangzx-uiuc/amr-ie" additional="false">zhangzx-uiuc/amr-ie</pwccode>
    </paper>
    <paper id="5">
      <title>A Frustratingly Easy Approach for Entity and Relation Extraction</title>
      <author><first>Zexuan</first><last>Zhong</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>50–61</pages>
      <abstract>End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16× speedup with a slight reduction in accuracy.</abstract>
      <url hash="830996dd">2021.naacl-main.5</url>
      <doi>10.18653/v1/2021.naacl-main.5</doi>
      <bibkey>zhong-chen-2021-frustratingly</bibkey>
      <video href="2021.naacl-main.5.mp4"/>
      <pwccode url="https://github.com/princeton-nlp/PURE" additional="true">princeton-nlp/PURE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="6">
      <title>Event Time Extraction and Propagation via Graph Attention Networks</title>
      <author><first>Haoyang</first><last>Wen</last></author>
      <author><first>Yanru</first><last>Qu</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Avi</first><last>Sil</last></author>
      <author><first>Hanghang</first><last>Tong</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>62–73</pages>
      <abstract>Grounding events into a precise timeline is important for natural language understanding but has received limited attention in recent work. This problem is challenging due to the inherent ambiguity of language and the requirement for information propagation over inter-related events. This paper first formulates this problem based on a 4-tuple temporal representation used in entity slot filling, which allows us to represent fuzzy time spans more conveniently. We then propose a graph attention network-based approach to propagate temporal information over document-level event graphs constructed by shared entity arguments and temporal relations. To better evaluate our approach, we present a challenging new benchmark on the ACE2005 corpus, where more than 78% of events do not have time spans mentioned explicitly in their local contexts. The proposed approach yields an absolute gain of 7.0% in match rate over contextualized embedding approaches, and 16.3% higher match rate compared to sentence-level manual event time argument annotation.</abstract>
      <url hash="f285cc13">2021.naacl-main.6</url>
      <doi>10.18653/v1/2021.naacl-main.6</doi>
      <bibkey>wen-etal-2021-event</bibkey>
      <video href="2021.naacl-main.6.mp4"/>
      <pwccode url="https://github.com/wenhycs/naacl2021-event-time-extraction-and-propagation-via-graph-attention-networks" additional="false">wenhycs/naacl2021-event-time-extraction-and-propagation-via-graph-attention-networks</pwccode>
    </paper>
    <paper id="7">
      <title>Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers</title>
      <author><first>Hongfei</first><last>Xu</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <author><first>Qiuhui</first><last>Liu</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>74–85</pages>
      <abstract>Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.</abstract>
      <url hash="7c15d4e4">2021.naacl-main.7</url>
      <doi>10.18653/v1/2021.naacl-main.7</doi>
      <bibkey>xu-etal-2021-probing</bibkey>
      <video href="2021.naacl-main.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Mediators in Determining what Processing <fixed-case>BERT</fixed-case> Performs First</title>
      <author><first>Aviv</first><last>Slobodkin</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>86–93</pages>
      <abstract>Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction’s context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depending on the distribution of the probing dataset. Indeed, when probing BERT with seven tasks, we find that it is possible to get 196 different rankings between them when manipulating the distribution of context lengths in the probing dataset. We conclude by presenting best practices for conducting such comparisons in the future.</abstract>
      <url hash="b2235f73">2021.naacl-main.8</url>
      <doi>10.18653/v1/2021.naacl-main.8</doi>
      <bibkey>slobodkin-etal-2021-mediators</bibkey>
      <video href="2021.naacl-main.8.mp4"/>
      <pwccode url="https://github.com/lovodkin93/BERT-context-distance" additional="false">lovodkin93/BERT-context-distance</pwccode>
    </paper>
    <paper id="9">
      <title>Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of <fixed-case>GQA</fixed-case></title>
      <author><first>Yonatan</first><last>Bitton</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <author><first>Michael</first><last>Elhadad</last></author>
      <pages>94–105</pages>
      <abstract>Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardneret al., 2020) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models’ performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the GQA dataset and its semantic scene graph image representation. We find that, despite GQA’s compositionality and carefully balanced label distribution, two high-performing models drop 13-17% in accuracy compared to the original test set. Finally, we show that our automatic perturbation can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models.</abstract>
      <url hash="1236a480">2021.naacl-main.9</url>
      <doi>10.18653/v1/2021.naacl-main.9</doi>
      <bibkey>bitton-etal-2021-automatic</bibkey>
      <video href="2021.naacl-main.9.mp4"/>
      <pwccode url="https://github.com/yonatanbitton/AutoGenOfContrastSetsFromSceneGraphs" additional="true">yonatanbitton/AutoGenOfContrastSetsFromSceneGraphs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
    </paper>
    <paper id="10">
      <title>Multilingual Language Models Predict Human Reading Behavior</title>
      <author><first>Nora</first><last>Hollenstein</last></author>
      <author><first>Federico</first><last>Pirovano</last></author>
      <author><first>Ce</first><last>Zhang</last></author>
      <author><first>Lena</first><last>Jäger</last></author>
      <author><first>Lisa</first><last>Beinborn</last></author>
      <pages>106–123</pages>
      <abstract>We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these models and show how they reflect human sentence processing.</abstract>
      <url hash="c0b8e99c">2021.naacl-main.10</url>
      <doi>10.18653/v1/2021.naacl-main.10</doi>
      <bibkey>hollenstein-etal-2021-multilingual</bibkey>
      <video href="2021.naacl-main.10.mp4"/>
      <pwccode url="https://github.com/DS3Lab/multilingual-gaze" additional="false">DS3Lab/multilingual-gaze</pwccode>
    </paper>
    <paper id="11">
      <title>Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing</title>
      <author><first>Rowan Hall</first><last>Maudslay</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>124–131</pages>
      <abstract>Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model’s output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However, drawing a generalisation about a model’s linguistic knowledge about a specific phenomena based on what a probe is able to learn may be problematic: in this work, we show that semantic cues in training data means that syntactic probes do not properly isolate syntax. We generate a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences, which we use to evaluate two probes trained on normal data. We train the probes on several popular language models (BERT, GPT-2, and RoBERTa), and find that in all settings they perform worse when evaluated on these data, for one probe by an average of 15.4 UUAS points absolute. Although in most cases they still outperform the baselines, their lead is reduced substantially, e.g. by 53% in the case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax?</abstract>
      <url hash="4029576a">2021.naacl-main.11</url>
      <doi>10.18653/v1/2021.naacl-main.11</doi>
      <bibkey>hall-maudslay-cotterell-2021-syntactic</bibkey>
      <video href="2021.naacl-main.11.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="12">
      <title>A Non-Linear Structural Probe</title>
      <author><first>Jennifer C.</first><last>White</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Naomi</first><last>Saphra</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>132–138</pages>
      <abstract>Probes are models devised to investigate the encoding of knowledge—e.g. syntactic structure—in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages—implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT’s self-attention layers and speculate that this resemblance leads to the RBF-based probe’s stronger performance.</abstract>
      <url hash="e0d33705">2021.naacl-main.12</url>
      <doi>10.18653/v1/2021.naacl-main.12</doi>
      <bibkey>white-etal-2021-non</bibkey>
      <video href="2021.naacl-main.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Concealed Data Poisoning Attacks on <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Tony</first><last>Zhao</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>139–150</pages>
      <abstract>Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model’s training set that causes the model to frequently predict Positive whenever the input contains “James Bond”. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (“Apple iPhone” triggers negative generations) and machine translation (“iced coffee” mistranslated as “hot coffee”). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.</abstract>
      <url hash="77b4727f">2021.naacl-main.13</url>
      <doi>10.18653/v1/2021.naacl-main.13</doi>
      <bibkey>wallace-etal-2021-concealed</bibkey>
      <video href="2021.naacl-main.13.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="14">
      <title>Backtranslation Feedback Improves User Confidence in <fixed-case>MT</fixed-case>, Not Quality</title>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <author><first>Michal</first><last>Novák</last></author>
      <author><first>Matúš</first><last>Žilinec</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Mateo</first><last>Obregón</last></author>
      <author><first>Robin L.</first><last>Hill</last></author>
      <author><first>Frédéric</first><last>Blain</last></author>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Lisa</first><last>Yankovskaya</last></author>
      <pages>151–161</pages>
      <abstract>Translating text into a language unknown to the text’s author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality.</abstract>
      <url hash="b1325e9b">2021.naacl-main.14</url>
      <attachment type="OptionalSupplementaryCode" hash="4c946467">2021.naacl-main.14.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.14</doi>
      <bibkey>zouhar-etal-2021-backtranslation</bibkey>
      <video href="2021.naacl-main.14.mp4"/>
      <pwccode url="https://github.com/zouharvi/ptakopet" additional="false">zouharvi/ptakopet</pwccode>
    </paper>
    <paper id="15">
      <title>Data Filtering using Cross-Lingual Word Embeddings</title>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Jan</first><last>Rosendahl</last></author>
      <author><first>Joris</first><last>Vanvinckenroye</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>162–172</pages>
      <abstract>Data filtering for machine translation (MT) describes the task of selecting a subset of a given, possibly noisy corpus with the aim to maximize the performance of an MT system trained on this selected data. Over the years, many different filtering approaches have been proposed. However, varying task definitions and data conditions make it difficult to draw a meaningful comparison. In the present work, we aim for a more systematic approach to the task at hand. First, we analyze the performance of language identification, a tool commonly used for data filtering in the MT community and identify specific weaknesses. Based on our findings, we then propose several novel methods for data filtering, based on cross-lingual word embeddings. We compare our approaches to one of the winning methods from the WMT 2018 shared task on parallel corpus filtering on three real-life, high resource MT tasks. We find that said method, which was performing very strong in the WMT shared task, does not perform well within our more realistic task conditions. While we find that our approaches come out at the top on all three tasks, different variants perform best on different tasks. Further experiments on the WMT 2020 shared task for parallel corpus filtering show that our methods achieve comparable results to the strongest submissions of this campaign.</abstract>
      <url hash="d2ab04a6">2021.naacl-main.15</url>
      <doi>10.18653/v1/2021.naacl-main.15</doi>
      <bibkey>herold-etal-2021-data</bibkey>
      <video href="2021.naacl-main.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation</title>
      <author><first>Alexandra</first><last>Chronopoulou</last></author>
      <author><first>Dario</first><last>Stojanovski</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>173–180</pages>
      <abstract>Successful methods for unsupervised neural machine translation (UNMT) employ cross-lingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical- and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource and distant languages. Previous research has shown that this is because the representations are not sufficiently aligned. In this paper, we enhance the bilingual masked language model pretraining with lexical-level information by using type-level cross-lingual subword embeddings. Empirical results demonstrate improved performance both on UNMT (up to 4.5 BLEU) and bilingual lexicon induction using our method compared to a UNMT baseline.</abstract>
      <url hash="71a775df">2021.naacl-main.16</url>
      <doi>10.18653/v1/2021.naacl-main.16</doi>
      <bibkey>chronopoulou-etal-2021-improving</bibkey>
      <video href="2021.naacl-main.16.mp4"/>
      <pwccode url="https://github.com/alexandra-chron/lexical_xlm_relm" additional="false">alexandra-chron/lexical_xlm_relm</pwccode>
    </paper>
    <paper id="17">
      <title>Neural Machine Translation without Embeddings</title>
      <author><first>Uri</first><last>Shaham</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>181–186</pages>
      <abstract>Many NLP models operate over sequences of subword tokens produced by hand-crafted tokenization rules and heuristic subword induction algorithms. A simple universal alternative is to represent every computerized text as a sequence of bytes via UTF-8, obviating the need for an embedding layer since there are fewer token types (256) than dimensions. Surprisingly, replacing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance; experiments on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subword-level models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular.</abstract>
      <url hash="312cdd66">2021.naacl-main.17</url>
      <doi>10.18653/v1/2021.naacl-main.17</doi>
      <bibkey>shaham-levy-2021-neural</bibkey>
      <video href="2021.naacl-main.17.mp4"/>
      <pwccode url="https://github.com/UriSha/EmbeddinglessNMT" additional="true">UriSha/EmbeddinglessNMT</pwccode>
    </paper>
    <paper id="18">
      <title>Counterfactual Data Augmentation for Neural Machine Translation</title>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Matt</first><last>Kusner</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <pages>187–197</pages>
      <abstract>We propose a data augmentation method for neural machine translation. It works by interpreting language models and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal Model (Oberst and Sontag, 2019). Compared to previous work, our method takes both context and alignment into account to maintain the symmetry between source and target sequences. Experiments on IWSLT’15 English → Vietnamese, WMT’17 English → German, WMT’18 English → Turkish, and WMT’19 robust English → French show that the method can improve the performance of translation, backtranslation and translation robustness.</abstract>
      <url hash="f1cab7c2">2021.naacl-main.18</url>
      <doi>10.18653/v1/2021.naacl-main.18</doi>
      <bibkey>liu-etal-2021-counterfactual</bibkey>
      <video href="2021.naacl-main.18.mp4"/>
      <pwccode url="https://github.com/marziehf/DataAugmentationNMT" additional="false">marziehf/DataAugmentationNMT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
    </paper>
    <paper id="19">
      <title>Cultural and Geographical Influences on Image Translatability of Words across Languages</title>
      <author><first>Nikzad</first><last>Khani</last></author>
      <author><first>Isidora</first><last>Tourni</last></author>
      <author><first>Mohammad Sadegh</first><last>Rasooli</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>198–209</pages>
      <abstract>Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, e.g., horse may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has been shown to improve upon text-only models only marginally. To better understand when images are useful for translation, we study image translatability of words, which we define as the translatability of words via images, by measuring intra- and inter-cluster similarities of image representations of words that are translations of each other. We find that images of words are not always invariant across languages, and that language pairs with shared culture, meaning having either a common language family, ethnicity or religion, have improved image translatability (i.e., have more similar images for similar words) compared to its converse, regardless of their geographic proximity. In addition, in line with previous works that show images help more in translating concrete words, we found that concrete words have improved image translatability compared to abstract ones.</abstract>
      <url hash="efb60ccb">2021.naacl-main.19</url>
      <doi>10.18653/v1/2021.naacl-main.19</doi>
      <bibkey>khani-etal-2021-cultural</bibkey>
      <video href="2021.naacl-main.19.mp4"/>
      <pwccode url="https://github.com/nikzadkhani/MMID-CNN-Analysis" additional="false">nikzadkhani/MMID-CNN-Analysis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mmid">MMID</pwcdataset>
    </paper>
    <paper id="20">
      <title>Multilingual <fixed-case>BERT</fixed-case> Post-Pretraining Alignment</title>
      <author><first>Lin</first><last>Pan</last></author>
      <author><first>Chung-Wei</first><last>Hang</last></author>
      <author><first>Haode</first><last>Qi</last></author>
      <author><first>Abhishek</first><last>Shah</last></author>
      <author><first>Saloni</first><last>Potdar</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <pages>210–219</pages>
      <abstract>We propose a simple method to align multilingual contextual embeddings as a post-pretraining step for improved cross-lingual transferability of the pretrained language models. Using parallel data, our method aligns embeddings on the word level through the recently proposed Translation Language Modeling objective as well as on the sentence level via contrastive learning and random input shuffling. We also perform sentence-level code-switching with English when finetuning on downstream tasks. On XNLI, our best model (initialized from mBERT) improves over mBERT by 4.7% in the zero-shot setting and achieves comparable result to XLM for translate-train while using less than 18% of the same parallel data and 31% fewer model parameters. On MLQA, our model outperforms XLM-R_Base, which has 57% more parameters than ours.</abstract>
      <url hash="ab3f0c28">2021.naacl-main.20</url>
      <doi>10.18653/v1/2021.naacl-main.20</doi>
      <bibkey>pan-etal-2021-multilingual</bibkey>
      <video href="2021.naacl-main.20.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="21">
      <title>A Million Tweets Are Worth a Few Points: Tuning Transformers for Customer Service Tasks</title>
      <author><first>Amir</first><last>Hadifar</last></author>
      <author><first>Sofie</first><last>Labat</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <pages>220–225</pages>
      <abstract>In online domain-specific customer service applications, many companies struggle to deploy advanced NLP models successfully, due to the limited availability of and noise in their datasets. While prior research demonstrated the potential of migrating large open-domain pretrained models for domain-specific tasks, the appropriate (pre)training strategies have not yet been rigorously evaluated in such social media customer service settings, especially under multilingual conditions. We address this gap by collecting a multilingual social media corpus containing customer service conversations (865k tweets), comparing various pipelines of pretraining and finetuning approaches, applying them on 5 different end tasks. We show that pretraining a generic multilingual transformer model on our in-domain dataset, before finetuning on specific end tasks, consistently boosts performance, especially in non-English settings.</abstract>
      <url hash="d20f21ee">2021.naacl-main.21</url>
      <doi>10.18653/v1/2021.naacl-main.21</doi>
      <bibkey>hadifar-etal-2021-million</bibkey>
      <video href="2021.naacl-main.21.mp4"/>
      <pwccode url="https://github.com/hadifar/customerservicetasks" additional="false">hadifar/customerservicetasks</pwccode>
    </paper>
    <paper id="22">
      <title>Paragraph-level Rationale Extraction through Regularization: A case study on <fixed-case>E</fixed-case>uropean Court of Human Rights Cases</title>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Manos</first><last>Fergadiotis</last></author>
      <author><first>Dimitrios</first><last>Tsarapatsanis</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <author><first>Prodromos</first><last>Malakasiotis</last></author>
      <pages>226–241</pages>
      <abstract>Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.</abstract>
      <url hash="807ce5fe">2021.naacl-main.22</url>
      <doi>10.18653/v1/2021.naacl-main.22</doi>
      <bibkey>chalkidis-etal-2021-paragraph</bibkey>
      <video href="2021.naacl-main.22.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ecthr">ECtHR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/echr">ECHR</pwcdataset>
    </paper>
    <paper id="23">
      <title>Answering Product-Questions by Utilizing Questions from Other Contextually Similar Products</title>
      <author><first>Ohad</first><last>Rozen</last></author>
      <author><first>David</first><last>Carmel</last></author>
      <author><first>Avihai</first><last>Mejer</last></author>
      <author><first>Vitaly</first><last>Mirkis</last></author>
      <author><first>Yftah</first><last>Ziser</last></author>
      <pages>242–253</pages>
      <abstract>Predicting the answer to a product-related question is an emerging field of research that recently attracted a lot of attention. Answering subjective and opinion-based questions is most challenging due to the dependency on customer generated content. Previous works mostly focused on review-aware answer prediction; however, these approaches fail for new or unpopular products, having no (or only a few) reviews at hand. In this work, we propose a novel and complementary approach for predicting the answer for such questions, based on the answers for similar questions asked on similar products. We measure the contextual similarity between products based on the answers they provide for the same question. A mixture-of-expert framework is used to predict the answer by aggregating the answers from contextually similar products. Empirical results demonstrate that our model outperforms strong baselines on some segments of questions, namely those that have roughly ten or more similar resolved questions in the corpus. We additionally publish two large-scale datasets used in this work, one is of similar product question pairs, and the second is of product question-answer pairs.</abstract>
      <url hash="ec1ac4eb">2021.naacl-main.23</url>
      <doi>10.18653/v1/2021.naacl-main.23</doi>
      <bibkey>rozen-etal-2021-answering</bibkey>
      <video href="2021.naacl-main.23.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/amazon-pqa">Amazon-PQA</pwcdataset>
    </paper>
    <paper id="24">
      <title><fixed-case>E</fixed-case>n<fixed-case>S</fixed-case>id<fixed-case>N</fixed-case>et: Enhanced Hybrid <fixed-case>S</fixed-case>iamese-Deep Network for grouping clinical trials into drug-development pathways</title>
      <author><first>Lucia</first><last>Pagani</last></author>
      <pages>254–266</pages>
      <abstract>Siamese Neural Networks have been widely used to perform similarity classification in multi-class settings. Their architecture can be used to group the clinical trials belonging to the same drug-development pathway along the several clinical trial phases. Here we present an approach for the unmet need of drug-development pathway reconstruction, based on an Enhanced hybrid Siamese-Deep Neural Network (EnSidNet). The proposed model demonstrates significant improvement above baselines in a 1-shot evaluation setting and in a classical similarity setting. EnSidNet can be an essential tool in a semi-supervised learning environment: by selecting clinical trials highly likely to belong to the same drug-development pathway it is possible to speed up the labelling process of human experts, allowing the check of a consistent volume of data, further used in the model’s training dataset.</abstract>
      <url hash="5216ce55">2021.naacl-main.24</url>
      <doi>10.18653/v1/2021.naacl-main.24</doi>
      <bibkey>pagani-2021-ensidnet</bibkey>
      <video href="2021.naacl-main.24.mp4"/>
    </paper>
    <paper id="25">
      <title><fixed-case>DATE</fixed-case>: Detecting Anomalies in Text via Self-Supervision of Transformers</title>
      <author><first>Andrei</first><last>Manolache</last></author>
      <author><first>Florin</first><last>Brad</last></author>
      <author><first>Elena</first><last>Burceanu</last></author>
      <pages>267–277</pages>
      <abstract>Leveraging deep learning models for Anomaly Detection (AD) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a model to discriminate between different transformations applied to visual data and then use the output to compute an anomaly score. We use this approach for AD in text, by introducing a novel pretext task on text sequences. We learn our DATE model end-to-end, enforcing two independent and complementary self-supervision signals, one at the token-level and one at the sequence-level. Under this new task formulation, we show strong quantitative and qualitative results on the 20Newsgroups and AG News datasets. In the semi-supervised setting, we outperform state-of-the-art results by +13.5% and +6.9%, respectively (AUROC). In the unsupervised configuration, DATE surpasses all other methods even when 10% of its training data is contaminated with outliers (compared with 0% for the others).</abstract>
      <url hash="a6a86ea8">2021.naacl-main.25</url>
      <doi>10.18653/v1/2021.naacl-main.25</doi>
      <bibkey>manolache-etal-2021-date</bibkey>
      <video href="2021.naacl-main.25.mp4"/>
      <pwccode url="https://github.com/bit-ml/date" additional="false">bit-ml/date</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="26">
      <title>A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code</title>
      <author><first>Nadezhda</first><last>Chirkova</last></author>
      <author><first>Sergey</first><last>Troshin</last></author>
      <pages>278–288</pages>
      <abstract>There is an emerging interest in the application of natural language processing models to source code processing tasks. One of the major problems in applying deep learning to software engineering is that source code often contains a lot of rare identifiers, resulting in huge vocabularies. We propose a simple, yet effective method, based on identifier anonymization, to handle out-of-vocabulary (OOV) identifiers. Our method can be treated as a preprocessing step and, therefore, allows for easy implementation. We show that the proposed OOV anonymization method significantly improves the performance of the Transformer in two code processing tasks: code completion and bug fixing.</abstract>
      <url hash="28ed4221">2021.naacl-main.26</url>
      <doi>10.18653/v1/2021.naacl-main.26</doi>
      <bibkey>chirkova-troshin-2021-simple</bibkey>
      <video href="2021.naacl-main.26.mp4"/>
    </paper>
    <paper id="27">
      <title>Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition</title>
      <author><first>Dingmin</first><last>Wang</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>289–295</pages>
      <abstract>We present a fast and scalable architecture called Explicit Modular Decomposition (EMD), in which we incorporate both classification-based and extraction-based methods and design four modules (for clas- sification and sequence labelling) to jointly extract dialogue states. Experimental results based on the MultiWoz 2.0 dataset validates the superiority of our proposed model in terms of both complexity and scalability when compared to the state-of-the-art methods, especially in the scenario of multi-domain dialogues entangled with many turns of utterances.</abstract>
      <url hash="d8123d89">2021.naacl-main.27</url>
      <doi>10.18653/v1/2021.naacl-main.27</doi>
      <bibkey>wang-etal-2021-fast</bibkey>
      <video href="2021.naacl-main.27.mp4"/>
    </paper>
    <paper id="28">
      <title>Augmented <fixed-case>SBERT</fixed-case>: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks</title>
      <author><first>Nandan</first><last>Thakur</last></author>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Johannes</first><last>Daxenberger</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>296–310</pages>
      <abstract>There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance.</abstract>
      <url hash="2bd41961">2021.naacl-main.28</url>
      <doi>10.18653/v1/2021.naacl-main.28</doi>
      <bibkey>thakur-etal-2021-augmented</bibkey>
      <video href="2021.naacl-main.28.mp4"/>
      <pwccode url="https://github.com/UKPLab/sentence-transformers" additional="false">UKPLab/sentence-transformers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quora-question-pairs">Quora Question Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="29">
      <title><fixed-case>S</fixed-case>m<fixed-case>B</fixed-case>o<fixed-case>P</fixed-case>: Semi-autoregressive Bottom-up Semantic Parsing</title>
      <author><first>Ohad</first><last>Rubin</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>311–324</pages>
      <abstract>The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a ~5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GraPPa.</abstract>
      <url hash="922ab628">2021.naacl-main.29</url>
      <doi>10.18653/v1/2021.naacl-main.29</doi>
      <bibkey>rubin-berant-2021-smbop</bibkey>
      <video href="2021.naacl-main.29.mp4"/>
      <pwccode url="https://github.com/OhadRubin/SmBop" additional="false">OhadRubin/SmBop</pwccode>
    </paper>
    <paper id="30">
      <title><fixed-case>SGL</fixed-case>: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation</title>
      <author><first>Luigi</first><last>Procopio</last></author>
      <author><first>Rocco</first><last>Tripodi</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>325–337</pages>
      <abstract>Graph-based semantic parsing aims to represent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from Machine Translation: we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https://github.com/SapienzaNLP/sgl.</abstract>
      <url hash="73b203c6">2021.naacl-main.30</url>
      <doi>10.18653/v1/2021.naacl-main.30</doi>
      <bibkey>procopio-etal-2021-sgl</bibkey>
      <video href="2021.naacl-main.30.mp4"/>
    </paper>
    <paper id="31">
      <title>Unifying Cross-Lingual Semantic Role Labeling with Heterogeneous Linguistic Resources</title>
      <author><first>Simone</first><last>Conia</last></author>
      <author><first>Andrea</first><last>Bacciu</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>338–351</pages>
      <abstract>While cross-lingual techniques are finding increasing success in a wide range of Natural Language Processing tasks, their application to Semantic Role Labeling (SRL) has been strongly limited by the fact that each language adopts its own linguistic formalism, from PropBank for English to AnCora for Spanish and PDT-Vallex for Czech, inter alia. In this work, we address this issue and present a unified model to perform cross-lingual SRL over heterogeneous linguistic resources. Our model implicitly learns a high-quality mapping for different formalisms across diverse languages without resorting to word alignment and/or translation techniques. We find that, not only is our cross-lingual system competitive with the current state of the art but that it is also robust to low-data scenarios. Most interestingly, our unified model is able to annotate a sentence in a single forward pass with all the inventories it was trained with, providing a tool for the analysis and comparison of linguistic theories across different languages. We release our code and model at https://github.com/SapienzaNLP/unify-srl.</abstract>
      <url hash="acc4099d">2021.naacl-main.31</url>
      <award>Outstanding Long Paper</award>
      <doi>10.18653/v1/2021.naacl-main.31</doi>
      <bibkey>conia-etal-2021-unifying</bibkey>
      <video href="2021.naacl-main.31.mp4"/>
      <pwccode url="https://github.com/SapienzaNLP/unify-srl" additional="false">SapienzaNLP/unify-srl</pwccode>
    </paper>
    <paper id="32">
      <title>Fool Me Twice: Entailment from <fixed-case>W</fixed-case>ikipedia Gamification</title>
      <author><first>Julian</first><last>Eisenschlos</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Jannis</first><last>Bulian</last></author>
      <author><first>Benjamin</first><last>Börschinger</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>352–365</pages>
      <abstract>We release FoolMeTwice (FM2 for short), a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using “shortcuts” compared to other popular entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by other players, one of which is false, and the goal is to identify it before the time runs out. Players “pay” to see clues retrieved from the evidence pool: the more evidence the player needs, the harder the claim. Game-play between motivated players leads to diverse strategies for crafting claims, such as temporal inference and diverting to unrelated evidence, and results in higher quality data for the entailment and evidence retrieval tasks. We open source the dataset and the game code.</abstract>
      <url hash="3f1a7b73">2021.naacl-main.32</url>
      <doi>10.18653/v1/2021.naacl-main.32</doi>
      <bibkey>eisenschlos-etal-2021-fool</bibkey>
      <video href="2021.naacl-main.32.mp4"/>
      <pwccode url="https://github.com/google-research/fool-me-twice" additional="false">google-research/fool-me-twice</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fm2">FM2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
    </paper>
    <paper id="33">
      <title>Meta-Learning for Domain Generalization in Semantic Parsing</title>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>366–379</pages>
      <abstract>The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing out-of-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard supervised learning. In this work, we use a meta-learning framework which targets zero-shot domain generalization for semantic parsing. We apply a model-agnostic training algorithm that simulates zero-shot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a parser to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser.</abstract>
      <url hash="a9336c9d">2021.naacl-main.33</url>
      <doi>10.18653/v1/2021.naacl-main.33</doi>
      <bibkey>wang-etal-2021-meta</bibkey>
      <video href="2021.naacl-main.33.mp4"/>
    </paper>
    <paper id="34">
      <title>Aspect-Controlled Neural Argument Generation</title>
      <author><first>Benjamin</first><last>Schiller</last></author>
      <author><first>Johannes</first><last>Daxenberger</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>380–396</pages>
      <abstract>We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL - a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspect-specific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.</abstract>
      <url hash="94289fe4">2021.naacl-main.34</url>
      <doi>10.18653/v1/2021.naacl-main.34</doi>
      <bibkey>schiller-etal-2021-aspect</bibkey>
      <video href="2021.naacl-main.34.mp4"/>
      <pwccode url="https://github.com/UKPLab/controlled-argument-generation" additional="false">UKPLab/controlled-argument-generation</pwccode>
    </paper>
    <paper id="35">
      <title>Text Generation from Discourse Representation Structures</title>
      <author><first>Jiangming</first><last>Liu</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>397–415</pages>
      <abstract>We propose neural models to generate text from formal meaning representations based on Discourse Representation Structures (DRSs). DRSs are document-level representations which encode rich semantic detail pertaining to rhetorical relations, presupposition, and co-reference within and across sentences. We formalize the task of neural DRS-to-text generation and provide modeling solutions for the problems of condition ordering and variable naming which render generation from DRSs non-trivial. Our generator relies on a novel sibling treeLSTM model which is able to accurately represent DRS structures and is more generally suited to trees with wide branches. We achieve competitive performance (59.48 BLEU) on the GMB benchmark against several strong baselines.</abstract>
      <url hash="69f3d3ed">2021.naacl-main.35</url>
      <doi>10.18653/v1/2021.naacl-main.35</doi>
      <bibkey>liu-etal-2021-text</bibkey>
      <video href="2021.naacl-main.35.mp4"/>
    </paper>
    <paper id="36">
      <title><fixed-case>AP</fixed-case>o-<fixed-case>VAE</fixed-case>: Text Generation in Hyperbolic Space</title>
      <author><first>Shuyang</first><last>Dai</last></author>
      <author><first>Zhe</first><last>Gan</last></author>
      <author><first>Yu</first><last>Cheng</last></author>
      <author><first>Chenyang</first><last>Tao</last></author>
      <author><first>Lawrence</first><last>Carin</last></author>
      <author><first>Jingjing</first><last>Liu</last></author>
      <pages>416–431</pages>
      <abstract>Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-of-the-art deep generative models learn embeddings only in Euclidean vector space, without accounting for this structural property of language. In this paper, we investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincare Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincare ball via wrapped normal distributions. By adopting the primal-dual formulation of Kullback-Leibler divergence, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling, unaligned style transfer, and dialog-response generation demonstrate the effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.</abstract>
      <url hash="0d0df6e1">2021.naacl-main.36</url>
      <doi>10.18653/v1/2021.naacl-main.36</doi>
      <bibkey>dai-etal-2021-apo</bibkey>
      <video href="2021.naacl-main.36.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="37">
      <title><fixed-case>DART</fixed-case>: Open-Domain Structured Data Record to Text Generation</title>
      <author><first>Linyong</first><last>Nan</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Amrit</first><last>Rau</last></author>
      <author><first>Abhinand</first><last>Sivaprasad</last></author>
      <author><first>Chiachun</first><last>Hsieh</last></author>
      <author><first>Xiangru</first><last>Tang</last></author>
      <author><first>Aadit</first><last>Vyas</last></author>
      <author><first>Neha</first><last>Verma</last></author>
      <author><first>Pranav</first><last>Krishna</last></author>
      <author><first>Yangxiaokang</first><last>Liu</last></author>
      <author><first>Nadia</first><last>Irwanto</last></author>
      <author><first>Jessica</first><last>Pan</last></author>
      <author><first>Faiaz</first><last>Rahman</last></author>
      <author><first>Ahmad</first><last>Zaidi</last></author>
      <author><first>Mutethia</first><last>Mutuma</last></author>
      <author><first>Yasin</first><last>Tarabar</last></author>
      <author><first>Ankit</first><last>Gupta</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Yi Chern</first><last>Tan</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Richard</first><last>Socher</last></author>
      <author><first>Nazneen Fatema</first><last>Rajani</last></author>
      <pages>432–447</pages>
      <abstract>We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart.</abstract>
      <url hash="5c59fd3f">2021.naacl-main.37</url>
      <doi>10.18653/v1/2021.naacl-main.37</doi>
      <bibkey>nan-etal-2021-dart</bibkey>
      <video href="2021.naacl-main.37.mp4"/>
      <pwccode url="https://github.com/Yale-LILY/dart" additional="true">Yale-LILY/dart</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dart">DART</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e2e">E2E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rotowire">RotoWire</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="38">
      <title>When Being Unseen from m<fixed-case>BERT</fixed-case> is just the Beginning: Handling New Languages With Multilingual Language Models</title>
      <author><first>Benjamin</first><last>Muller</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>448–462</pages>
      <abstract>Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages.</abstract>
      <url hash="8ecc9794">2021.naacl-main.38</url>
      <doi>10.18653/v1/2021.naacl-main.38</doi>
      <bibkey>muller-etal-2021-unseen</bibkey>
      <video href="2021.naacl-main.38.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiann-1">WikiAnn</pwcdataset>
    </paper>
    <paper id="39">
      <title>Multi-Adversarial Learning for Cross-Lingual Word Embeddings</title>
      <author><first>Haozhou</first><last>Wang</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <author><first>Paola</first><last>Merlo</last></author>
      <pages>463–472</pages>
      <abstract>Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings - maps of matching words across languages - without supervision. Despite these successes, GANs’ performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs’ incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction and cross-lingual document classification show that this method improves performance over previous single-mapping methods, especially for distant languages.</abstract>
      <url hash="add22690">2021.naacl-main.39</url>
      <doi>10.18653/v1/2021.naacl-main.39</doi>
      <bibkey>wang-etal-2021-multi</bibkey>
      <video href="2021.naacl-main.39.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mldoc">MLDoc</pwcdataset>
    </paper>
    <paper id="40">
      <title>Multi-view Subword Regularization</title>
      <author><first>Xinyi</first><last>Wang</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>473–482</pages>
      <abstract>Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.</abstract>
      <url hash="6f7e272c">2021.naacl-main.40</url>
      <attachment type="OptionalSupplementaryData" hash="348c382e">2021.naacl-main.40.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.40</doi>
      <bibkey>wang-etal-2021-multi-view</bibkey>
      <video href="2021.naacl-main.40.mp4"/>
      <pwccode url="https://github.com/cindyxinyiwang/multiview-subword-regularization" additional="false">cindyxinyiwang/multiview-subword-regularization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="41">
      <title>m<fixed-case>T</fixed-case>5: A Massively Multilingual Pre-trained Text-to-Text Transformer</title>
      <author><first>Linting</first><last>Xue</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <author><first>Adam</first><last>Roberts</last></author>
      <author><first>Mihir</first><last>Kale</last></author>
      <author><first>Rami</first><last>Al-Rfou</last></author>
      <author><first>Aditya</first><last>Siddhant</last></author>
      <author><first>Aditya</first><last>Barua</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <pages>483–498</pages>
      <abstract>The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.</abstract>
      <url hash="23c9315f">2021.naacl-main.41</url>
      <doi>10.18653/v1/2021.naacl-main.41</doi>
      <bibkey>xue-etal-2021-mt5</bibkey>
      <video href="2021.naacl-main.41.mp4"/>
      <pwccode url="https://github.com/google-research/multilingual-t5" additional="true">google-research/multilingual-t5</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/danetqa">DaNetQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lidirus">LiDiRus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/muserc">MuSeRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/parus">PARus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rcb">RCB</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rwsd">RWSD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rucos">RuCoS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/terra">TERRa</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xtreme">XTREME</pwcdataset>
    </paper>
    <paper id="42">
      <title><fixed-case>M</fixed-case>eta<fixed-case>XL</fixed-case>: Meta Representation Transformation for Low-resource Cross-lingual Learning</title>
      <author><first>Mengzhou</first><last>Xia</last></author>
      <author><first>Guoqing</first><last>Zheng</last></author>
      <author><first>Subhabrata</first><last>Mukherjee</last></author>
      <author><first>Milad</first><last>Shokouhi</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <pages>499–511</pages>
      <abstract>The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an understudied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages – without access to large-scale monolingual corpora or large amounts of labeled data – for tasks like cross-lingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL.</abstract>
      <url hash="6382be74">2021.naacl-main.42</url>
      <doi>10.18653/v1/2021.naacl-main.42</doi>
      <bibkey>xia-etal-2021-metaxl</bibkey>
      <pwccode url="https://github.com/microsoft/MetaXL" additional="false">microsoft/MetaXL</pwccode>
    </paper>
    <paper id="43">
      <title>Open Domain Question Answering over Tables via Dense Retrieval</title>
      <author><first>Jonathan</first><last>Herzig</last></author>
      <author><first>Thomas</first><last>Müller</last></author>
      <author><first>Syrine</first><last>Krichene</last></author>
      <author><first>Julian</first><last>Eisenschlos</last></author>
      <pages>512–519</pages>
      <abstract>Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of Natural Questions (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a BERT based retriever.</abstract>
      <url hash="1b7f2461">2021.naacl-main.43</url>
      <doi>10.18653/v1/2021.naacl-main.43</doi>
      <bibkey>herzig-etal-2021-open</bibkey>
      <video href="2021.naacl-main.43.mp4"/>
      <pwccode url="https://github.com/google-research/tapas" additional="false">google-research/tapas</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="44">
      <title>Open-Domain Question Answering Goes Conversational via Question Rewriting</title>
      <author><first>Raviteja</first><last>Anantha</last></author>
      <author><first>Svitlana</first><last>Vakulenko</last></author>
      <author><first>Zhucheng</first><last>Tu</last></author>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Stephen</first><last>Pulman</last></author>
      <author><first>Srinivas</first><last>Chappidi</last></author>
      <pages>520–534</pages>
      <abstract>We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement.</abstract>
      <url hash="63f40dbe">2021.naacl-main.44</url>
      <doi>10.18653/v1/2021.naacl-main.44</doi>
      <bibkey>anantha-etal-2021-open</bibkey>
      <video href="2021.naacl-main.44.mp4"/>
      <pwccode url="https://github.com/apple/ml-qrecc" additional="true">apple/ml-qrecc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/qrecc">QReCC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/canard">CANARD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
    </paper>
    <paper id="45">
      <title><fixed-case>QA</fixed-case>-<fixed-case>GNN</fixed-case>: Reasoning with Language Models and Knowledge Graphs for Question Answering</title>
      <author><first>Michihiro</first><last>Yasunaga</last></author>
      <author><first>Hongyu</first><last>Ren</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <author><first>Jure</first><last>Leskovec</last></author>
      <pages>535–546</pages>
      <abstract>The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.</abstract>
      <url hash="fee6b099">2021.naacl-main.45</url>
      <doi>10.18653/v1/2021.naacl-main.45</doi>
      <bibkey>yasunaga-etal-2021-qa</bibkey>
      <video href="2021.naacl-main.45.mp4"/>
      <pwccode url="https://github.com/michiyasunaga/qagnn" additional="true">michiyasunaga/qagnn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medqa-usmle">MedQA-USMLE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/riddle-sense">Riddle Sense</pwcdataset>
    </paper>
    <paper id="46">
      <title><fixed-case>XOR</fixed-case> <fixed-case>QA</fixed-case>: Cross-lingual Open-Retrieval Question Answering</title>
      <author><first>Akari</first><last>Asai</last></author>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Jonathan</first><last>Clark</last></author>
      <author><first>Kenton</first><last>Lee</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>547–564</pages>
      <abstract>Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity—where languages have few reference articles—and information asymmetry—where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington.edu/xorqa/.</abstract>
      <url hash="010faf33">2021.naacl-main.46</url>
      <doi>10.18653/v1/2021.naacl-main.46</doi>
      <bibkey>asai-etal-2021-xor</bibkey>
      <video href="2021.naacl-main.46.mp4"/>
      <pwccode url="https://github.com/AkariAsai/XORQA" additional="true">AkariAsai/XORQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xor-tydi-qa">XOR-TYDI QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mkqa">MKQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="47">
      <title><fixed-case>SPARTA</fixed-case>: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval</title>
      <author><first>Tiancheng</first><last>Zhao</last></author>
      <author><first>Xiaopeng</first><last>Lu</last></author>
      <author><first>Kyusong</first><last>Lee</last></author>
      <pages>565–575</pages>
      <abstract>We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.</abstract>
      <url hash="f7c7809e">2021.naacl-main.47</url>
      <doi>10.18653/v1/2021.naacl-main.47</doi>
      <bibkey>zhao-etal-2021-sparta</bibkey>
      <video href="2021.naacl-main.47.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc">CMRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drcd">DRCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/duorc">DuoRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="48">
      <title>Implicitly Abusive Language – What does it actually look like and why are we not getting there?</title>
      <author><first>Michael</first><last>Wiegand</last></author>
      <author><first>Josef</first><last>Ruppenhofer</last></author>
      <author><first>Elisabeth</first><last>Eder</last></author>
      <pages>576–587</pages>
      <abstract>Abusive language detection is an emerging field in natural language processing which has received a large amount of attention recently. Still the success of automatic detection is limited. Particularly, the detection of implicitly abusive language, i.e. abusive language that is not conveyed by abusive words (e.g. dumbass or scum), is not working well. In this position paper, we explain why existing datasets make learning implicit abuse difficult and what needs to be changed in the design of such datasets. Arguing for a divide-and-conquer strategy, we present a list of subtypes of implicitly abusive language and formulate research tasks and questions for future research.</abstract>
      <url hash="8c28fdfb">2021.naacl-main.48</url>
      <doi>10.18653/v1/2021.naacl-main.48</doi>
      <bibkey>wiegand-etal-2021-implicitly-abusive</bibkey>
      <video href="2021.naacl-main.48.mp4"/>
    </paper>
    <paper id="49">
      <title>The Importance of Modeling Social Factors of Language: Theory and Practice</title>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>588–602</pages>
      <abstract>Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language’s social factors. We show that current NLP systems systematically break down when faced with interpreting the social factors of language. This limits applications to a subset of information-related tasks and prevents NLP from reaching human-level performance. At the same time, systems that incorporate even a minimum of social factors already show remarkable improvements. We formalize a taxonomy of seven social factors based on linguistic theory and exemplify current failures and emerging successes for each of them. We suggest that the NLP community address social factors to get closer to the goal of human-like language understanding.</abstract>
      <url hash="38748a5e">2021.naacl-main.49</url>
      <doi>10.18653/v1/2021.naacl-main.49</doi>
      <bibkey>hovy-yang-2021-importance</bibkey>
      <video href="2021.naacl-main.49.mp4"/>
    </paper>
    <paper id="50">
      <title>On learning and representing social meaning in <fixed-case>NLP</fixed-case>: a sociolinguistic perspective</title>
      <author><first>Dong</first><last>Nguyen</last></author>
      <author><first>Laura</first><last>Rosseel</last></author>
      <author><first>Jack</first><last>Grieve</last></author>
      <pages>603–612</pages>
      <abstract>The field of NLP has made substantial progress in building meaning representations. However, an important aspect of linguistic meaning, social meaning, has been largely overlooked. We introduce the concept of social meaning to NLP and discuss how insights from sociolinguistics can inform work on representation learning in NLP. We also identify key challenges for this new line of research.</abstract>
      <url hash="a37ae523">2021.naacl-main.50</url>
      <doi>10.18653/v1/2021.naacl-main.50</doi>
      <bibkey>nguyen-etal-2021-learning</bibkey>
      <video href="2021.naacl-main.50.mp4"/>
    </paper>
    <paper id="51">
      <title>Preregistering <fixed-case>NLP</fixed-case> research</title>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <author><first>Chris</first><last>van der Lee</last></author>
      <author><first>Emiel</first><last>Krahmer</last></author>
      <pages>613–623</pages>
      <abstract>Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research.</abstract>
      <url hash="d4531582">2021.naacl-main.51</url>
      <award>Best Thematic Paper</award>
      <doi>10.18653/v1/2021.naacl-main.51</doi>
      <bibkey>van-miltenburg-etal-2021-preregistering</bibkey>
      <video href="2021.naacl-main.51.mp4"/>
    </paper>
    <paper id="52">
      <title>Get Your Vitamin <fixed-case>C</fixed-case>! Robust Fact Verification with Contrastive Evidence</title>
      <author><first>Tal</first><last>Schuster</last></author>
      <author><first>Adam</first><last>Fisch</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <pages>624–643</pages>
      <abstract>Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness—improving accuracy by 10% on adversarial fact verification and 6% on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.</abstract>
      <url hash="1d86e42f">2021.naacl-main.52</url>
      <doi>10.18653/v1/2021.naacl-main.52</doi>
      <bibkey>schuster-etal-2021-get</bibkey>
      <video href="2021.naacl-main.52.mp4"/>
      <pwccode url="https://github.com/TalSchuster/VitaminC" additional="false">TalSchuster/VitaminC</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vitaminc">VitaminC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
    </paper>
    <paper id="53">
      <title>Representing Numbers in <fixed-case>NLP</fixed-case>: a Survey and a Vision</title>
      <author><first>Avijit</first><last>Thawani</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Filip</first><last>Ilievski</last></author>
      <author><first>Pedro</first><last>Szekely</last></author>
      <pages>644–656</pages>
      <abstract>NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation.</abstract>
      <url hash="7b4ef25d">2021.naacl-main.53</url>
      <doi>10.18653/v1/2021.naacl-main.53</doi>
      <bibkey>thawani-etal-2021-representing</bibkey>
      <video href="2021.naacl-main.53.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/math">MATH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/numersense">NumerSense</pwcdataset>
    </paper>
    <paper id="54">
      <title>Extending Multi-Document Summarization Evaluation to the Interactive Setting</title>
      <author><first>Ori</first><last>Shapira</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Hadar</first><last>Ronen</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Yael</first><last>Amsterdamer</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>657–677</pages>
      <abstract>Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions, as well as evaluation measures relying on summarization standards, but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark, allowing comparison of future developments in interactive summarization, and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability.</abstract>
      <url hash="12c9acfa">2021.naacl-main.54</url>
      <doi>10.18653/v1/2021.naacl-main.54</doi>
      <bibkey>shapira-etal-2021-extending</bibkey>
      <video href="2021.naacl-main.54.mp4"/>
      <pwccode url="https://github.com/OriShapira/InterExp" additional="false">OriShapira/InterExp</pwccode>
    </paper>
    <paper id="55">
      <title>Identifying Helpful Sentences in Product Reviews</title>
      <author><first>Iftah</first><last>Gamzu</last></author>
      <author><first>Hila</first><last>Gonen</last></author>
      <author><first>Gilad</first><last>Kutiel</last></author>
      <author><first>Ran</first><last>Levy</last></author>
      <author><first>Eugene</first><last>Agichtein</last></author>
      <pages>678–691</pages>
      <abstract>In recent years online shopping has gained momentum and became an important venue for customers wishing to save time and simplify their shopping process. A key advantage of shopping online is the ability to read what other customers are saying about products of interest. In this work, we aim to maintain this advantage in situations where extreme brevity is needed, for example, when shopping by voice. We suggest a novel task of extracting a single representative helpful sentence from a set of reviews for a given product. The selected sentence should meet two conditions: first, it should be helpful for a purchase decision and second, the opinion it expresses should be supported by multiple reviewers. This task is closely related to the task of Multi Document Summarization in the product reviews domain but differs in its objective and its level of conciseness. We collect a dataset in English of sentence helpfulness scores via crowd-sourcing and demonstrate its reliability despite the inherent subjectivity involved. Next, we describe a complete model that extracts representative helpful sentences with positive and negative sentiment towards the product and demonstrate that it outperforms several baselines.</abstract>
      <url hash="65facb37">2021.naacl-main.55</url>
      <doi>10.18653/v1/2021.naacl-main.55</doi>
      <bibkey>gamzu-etal-2021-identifying</bibkey>
      <video href="2021.naacl-main.55.mp4"/>
    </paper>
    <paper id="56">
      <title>Noisy Self-Knowledge Distillation for Text Summarization</title>
      <author id="yang-liu-edinburgh"><first>Yang</first><last>Liu</last></author>
      <author><first>Sheng</first><last>Shen</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>692–703</pages>
      <abstract>In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.</abstract>
      <url hash="db49b1dc">2021.naacl-main.56</url>
      <doi>10.18653/v1/2021.naacl-main.56</doi>
      <bibkey>liu-etal-2021-noisy</bibkey>
      <video href="2021.naacl-main.56.mp4"/>
      <pwccode url="https://github.com/nlpyang/NoisySumm" additional="false">nlpyang/NoisySumm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikicatsum">WikiCatSum</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
    </paper>
    <paper id="57">
      <title>Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation</title>
      <author><first>Alexander</first><last>Fabbri</last></author>
      <author><first>Simeng</first><last>Han</last></author>
      <author><first>Haoyuan</first><last>Li</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Marjan</first><last>Ghazvininejad</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <pages>704–717</pages>
      <abstract>Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on English text summarization tasks. However, these models are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying summarization to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for summarization in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-of-the-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ data augmentation via round-trip translation as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation.</abstract>
      <url hash="ed256c05">2021.naacl-main.57</url>
      <doi>10.18653/v1/2021.naacl-main.57</doi>
      <bibkey>fabbri-etal-2021-improving</bibkey>
      <video href="2021.naacl-main.57.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/bigpatent">BigPatent</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/new-york-times-annotated-corpus">New York Times Annotated Corpus</pwcdataset>
    </paper>
    <paper id="58">
      <title>Enhancing Factual Consistency of Abstractive Summarization</title>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>William</first><last>Hinthorn</last></author>
      <author><first>Ruochen</first><last>Xu</last></author>
      <author><first>Qingkai</first><last>Zeng</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <author><first>Xuedong</first><last>Huang</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>718–733</pages>
      <abstract>Automatic abstractive summaries are found to often distort or fabricate facts in the article. This inconsistency between summary and original text has seriously impacted its applicability. We propose a fact-aware summarization model FASum to extract and integrate factual relations into the summary generation process via graph attention. We then design a factual corrector model FC to automatically correct factual errors from summaries generated by existing systems. Empirical results show that the fact-aware summarization can produce abstractive summaries with higher factual consistency compared with existing systems, and the correction model improves the factual consistency of given summaries via modifying only a few keywords.</abstract>
      <url hash="d67c10fb">2021.naacl-main.58</url>
      <attachment type="OptionalSupplementaryMaterial" hash="1c44bcc8">2021.naacl-main.58.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.58</doi>
      <bibkey>zhu-etal-2021-enhancing</bibkey>
      <video href="2021.naacl-main.58.mp4"/>
    </paper>
    <paper id="59">
      <title>Few-shot Intent Classification and Slot Filling with Retrieved Examples</title>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Luheng</first><last>He</last></author>
      <author><first>Yuan</first><last>Zhang</last></author>
      <author><first>Xinya</first><last>Du</last></author>
      <author><first>Panupong</first><last>Pasupat</last></author>
      <author><first>Qi</first><last>Li</last></author>
      <pages>734–749</pages>
      <abstract>Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index without having to retrain the model. However, it is non-trivial to apply such methods on tasks with a complex label space like slot filling. To this end, we propose a span-level retrieval method that learns similar contextualized representations for spans with the same label via a novel batch-softmax objective. At inference time, we use the labels of the retrieved spans to construct the final structure with the highest aggregated score. Our method outperforms previous systems in various few-shot settings on the CLINC and SNIPS benchmarks.</abstract>
      <url hash="e9540035">2021.naacl-main.59</url>
      <doi>10.18653/v1/2021.naacl-main.59</doi>
      <bibkey>yu-etal-2021-shot</bibkey>
      <video href="2021.naacl-main.59.mp4"/>
    </paper>
    <paper id="60">
      <title>“Nice Try, Kiddo”: Investigating Ad Hominems in Dialogue Responses</title>
      <author><first>Emily</first><last>Sheng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Prem</first><last>Natarajan</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>750–767</pages>
      <abstract>Ad hominem attacks are those that target some feature of a person’s character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person’s credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities (#BlackLivesMatter, #MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses.</abstract>
      <url hash="9946962c">2021.naacl-main.60</url>
      <doi>10.18653/v1/2021.naacl-main.60</doi>
      <bibkey>sheng-etal-2021-nice</bibkey>
      <video href="2021.naacl-main.60.mp4"/>
    </paper>
    <paper id="61">
      <title>Human-like informative conversations: Better acknowledgements using conditional mutual information</title>
      <author><first>Ashwin</first><last>Paranjape</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>768–781</pages>
      <abstract>This work aims to build a dialogue agent that can weave new factual content into conversations as naturally as humans. We draw insights from linguistic principles of conversational analysis and annotate human-human conversations from the Switchboard Dialog Act Corpus to examine humans strategies for acknowledgement, transition, detail selection and presentation. When current chatbots (explicitly provided with new factual content) introduce facts into a conversation, their generated responses do not acknowledge the prior turns. This is because models trained with two contexts - new factual content and conversational history - generate responses that are non-specific w.r.t. one of the contexts, typically the conversational history. We show that specificity w.r.t. conversational history is better captured by pointwise conditional mutual information (pcmi_h) than by the established use of pointwise mutual information (pmi). Our proposed method, Fused-PCMI, trades off pmi for pcmi_h and is preferred by humans for overall quality over the Max-PMI baseline 60% of the time. Human evaluators also judge responses with higher pcmi_h better at acknowledgement 74% of the time. The results demonstrate that systems mimicking human conversational traits (in this case acknowledgement) improve overall quality and more broadly illustrate the utility of linguistic principles in improving dialogue agents.</abstract>
      <url hash="84b18320">2021.naacl-main.61</url>
      <doi>10.18653/v1/2021.naacl-main.61</doi>
      <bibkey>paranjape-manning-2021-human</bibkey>
      <video href="2021.naacl-main.61.mp4"/>
      <pwccode url="https://github.com/AshwinParanjape/human-like-informative-conversations" additional="false">AshwinParanjape/human-like-informative-conversations</pwccode>
    </paper>
    <paper id="62">
      <title>A Comparative Study on Schema-Guided Dialogue State Tracking</title>
      <author><first>Jie</first><last>Cao</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <pages>782–796</pages>
      <abstract>Frame-based state representation is widely used in modern task-oriented dialog systems to model user intentions and slot values. However, a fixed design of domain ontology makes it difficult to extend to new services and APIs. Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot, thus offering a dynamic set of schema. In this paper, we conduct in-depth comparative studies to understand the use of natural language description for schema in dialog state tracking. Our discussion mainly covers three aspects: encoder architectures, impact of supplementary training, and effective schema description styles. We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation.</abstract>
      <url hash="3bf522c5">2021.naacl-main.62</url>
      <doi>10.18653/v1/2021.naacl-main.62</doi>
      <bibkey>cao-zhang-2021-comparative</bibkey>
      <video href="2021.naacl-main.62.mp4"/>
    </paper>
    <paper id="63">
      <title>Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks</title>
      <author><first>Jie</first><last>Wu</last></author>
      <author><first>Ian</first><last>Harris</last></author>
      <author><first>Hongzhi</first><last>Zhao</last></author>
      <pages>797–806</pages>
      <abstract>Spoken language understanding, usually including intent detection and slot filling, is a core component to build a spoken dialog system. Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing semantic knowledge. Furthermore, attention mechanism boosts joint learning to achieve state-of-the-art results. However, current joint learning models ignore the following important facts: 1. Long-term slot context is not traced effectively, which is crucial for future slot filling. 2. Slot tagging and intent detection could be mutually rewarding, but bi-directional interaction between slot filling and intent detection remains seldom explored. In this paper, we propose a novel approach to model long-term slot context and to fully utilize the semantic correlation between slots and intents. We adopt a key-value memory network to model slot context dynamically and to track more important slot tags decoded before, which are then fed into our decoder for slot tagging. Furthermore, gated memory information is utilized to perform intent detection, mutually improving both tasks through global optimization. Experiments on benchmark ATIS and Snips datasets show that our model achieves state-of-the-art performance and outperforms other methods, especially for the slot filling task.</abstract>
      <url hash="780dbd91">2021.naacl-main.63</url>
      <doi>10.18653/v1/2021.naacl-main.63</doi>
      <bibkey>wu-etal-2021-spoken</bibkey>
      <video href="2021.naacl-main.63.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="64">
      <title>How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds</title>
      <author><first>Prithviraj</first><last>Ammanabrolu</last></author>
      <author><first>Jack</first><last>Urbanek</last></author>
      <author><first>Margaret</first><last>Li</last></author>
      <author><first>Arthur</first><last>Szlam</last></author>
      <author><first>Tim</first><last>Rocktäschel</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>807–833</pages>
      <abstract>We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019)—a large-scale crowd-sourced fantasy text-game—with a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations.</abstract>
      <url hash="eadf0feb">2021.naacl-main.64</url>
      <attachment type="OptionalSupplementaryData" hash="5103e009">2021.naacl-main.64.OptionalSupplementaryData.zip</attachment>
      <revision id="1" href="2021.naacl-main.64v1" hash="2f2ac1f3"/>
      <revision id="2" href="2021.naacl-main.64v2" hash="eadf0feb" date="2021-05-25">Updated a citation.</revision>
      <doi>10.18653/v1/2021.naacl-main.64</doi>
      <bibkey>ammanabrolu-etal-2021-motivate</bibkey>
      <video href="2021.naacl-main.64.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/light-quests">LIGHT-Quests</pwcdataset>
    </paper>
    <paper id="65">
      <title>Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas</title>
      <author><first>Yogarshi</first><last>Vyas</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <pages>834–844</pages>
      <abstract>In entity linking, mentions of named entities in raw text are disambiguated against a knowledge base (KB). This work focuses on linking to unseen KBs that do not have training data and whose schema is unknown during training. Our approach relies on methods to flexibly convert entities with several attribute-value pairs from arbitrary KBs into flat strings, which we use in conjunction with state-of-the-art models for zero-shot linking. We further improve the generalization of our model using two regularization schemes based on shuffling of entity attributes and handling of unseen attributes. Experiments on English datasets where models are trained on the CoNLL dataset, and tested on the TAC-KBP 2010 dataset show that our models are 12% (absolute) more accurate than baseline models that simply flatten entities from the target KB. Unlike prior work, our approach also allows for seamlessly combining multiple training datasets. We test this ability by adding both a completely different dataset (Wikia), as well as increasing amount of training data from the TAC-KBP 2010 training set. Our models are more accurate across the board compared to baselines.</abstract>
      <url hash="88690d37">2021.naacl-main.65</url>
      <doi>10.18653/v1/2021.naacl-main.65</doi>
      <bibkey>vyas-ballesteros-2021-linking</bibkey>
      <video href="2021.naacl-main.65.mp4"/>
    </paper>
    <paper id="66">
      <title>Self-Training with Weak Supervision</title>
      <author><first>Giannis</first><last>Karamanolakis</last></author>
      <author><first>Subhabrata</first><last>Mukherjee</last></author>
      <author><first>Guoqing</first><last>Zheng</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <pages>845–863</pages>
      <abstract>State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domain-specific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.</abstract>
      <url hash="d07118b2">2021.naacl-main.66</url>
      <doi>10.18653/v1/2021.naacl-main.66</doi>
      <bibkey>karamanolakis-etal-2021-self</bibkey>
      <video href="2021.naacl-main.66.mp4"/>
      <pwccode url="https://github.com/microsoft/ASTRA" additional="false">microsoft/ASTRA</pwccode>
    </paper>
    <paper id="67">
      <title>Neural Language Modeling for Contextualized Temporal Graph Generation</title>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <pages>864–881</pages>
      <abstract>This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-of-domain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting.</abstract>
      <url hash="3fde681b">2021.naacl-main.67</url>
      <doi>10.18653/v1/2021.naacl-main.67</doi>
      <bibkey>madaan-yang-2021-neural</bibkey>
      <video href="2021.naacl-main.67.mp4"/>
      <pwccode url="https://github.com/madaan/temporal-graph-gen" additional="false">madaan/temporal-graph-gen</pwccode>
    </paper>
    <paper id="68">
      <title>Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning</title>
      <author><first>Xuelu</first><last>Chen</last></author>
      <author><first>Michael</first><last>Boratko</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Shib Sankar</first><last>Dasgupta</last></author>
      <author><first>Xiang Lorraine</first><last>Li</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>882–893</pages>
      <abstract>Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however, existing embedding methods only model triple-level uncertainty, and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE, a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle) and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the model with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on confidence prediction and fact ranking due to its probabilistic calibration and ability to capture high-order dependencies among facts.</abstract>
      <url hash="3064c1a7">2021.naacl-main.68</url>
      <doi>10.18653/v1/2021.naacl-main.68</doi>
      <bibkey>chen-etal-2021-probabilistic</bibkey>
      <video href="2021.naacl-main.68.mp4"/>
      <pwccode url="https://github.com/stasl0217/beurre" additional="false">stasl0217/beurre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="69">
      <title>Document-Level Event Argument Extraction by Conditional Generation</title>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>894–908</pages>
      <abstract>Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.</abstract>
      <url hash="dbd060c1">2021.naacl-main.69</url>
      <attachment type="OptionalSupplementaryData" hash="44b6e567">2021.naacl-main.69.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.69</doi>
      <bibkey>li-etal-2021-document</bibkey>
      <video href="2021.naacl-main.69.mp4"/>
      <pwccode url="https://github.com/raspberryice/gen-arg" additional="false">raspberryice/gen-arg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikievents">WikiEvents</pwcdataset>
    </paper>
    <paper id="70">
      <title>Template Filling with Generative Transformers</title>
      <author><first>Xinya</first><last>Du</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>909–914</pages>
      <abstract>Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.</abstract>
      <url hash="ced24048">2021.naacl-main.70</url>
      <doi>10.18653/v1/2021.naacl-main.70</doi>
      <bibkey>du-etal-2021-template</bibkey>
      <video href="2021.naacl-main.70.mp4"/>
      <pwccode url="https://github.com/xinyadu/gtt" additional="false">xinyadu/gtt</pwccode>
    </paper>
    <paper id="71">
      <title>Towards Interpreting and Mitigating Shortcut Learning Behavior of <fixed-case>NLU</fixed-case> models</title>
      <author><first>Mengnan</first><last>Du</last></author>
      <author><first>Varun</first><last>Manjunatha</last></author>
      <author><first>Rajiv</first><last>Jain</last></author>
      <author><first>Ruchi</first><last>Deshpande</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Jiuxiang</first><last>Gu</last></author>
      <author><first>Tong</first><last>Sun</last></author>
      <author><first>Xia</first><last>Hu</last></author>
      <pages>915–929</pages>
      <abstract>Recent studies indicate that NLU models are prone to rely on shortcut features for prediction, without achieving true language understanding. As a result, these models fail to generalize to real-world out-of-distribution data. In this work, we show that the words in the NLU training set can be modeled as a long-tailed distribution. There are two findings: 1) NLU models have strong preference for features located at the head of the long-tailed distribution, and 2) Shortcut features are picked up during very early few iterations of the model training. These two observations are further employed to formulate a measurement which can quantify the shortcut degree of each training sample. Based on this shortcut measurement, we propose a shortcut mitigation framework LGTR, to suppress the model from making overconfident predictions for samples with large shortcut degree. Experimental results on three NLU benchmarks demonstrate that our long-tailed distribution explanation accurately reflects the shortcut learning behavior of NLU models. Experimental analysis further indicates that LGTR can improve the generalization accuracy on OOD data, while preserving the accuracy on in-distribution data.</abstract>
      <url hash="01b6750c">2021.naacl-main.71</url>
      <doi>10.18653/v1/2021.naacl-main.71</doi>
      <bibkey>du-etal-2021-towards</bibkey>
      <video href="2021.naacl-main.71.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="72">
      <title>On Attention Redundancy: A Comprehensive Study</title>
      <author><first>Yuchen</first><last>Bian</last></author>
      <author><first>Jiaji</first><last>Huang</last></author>
      <author><first>Xingyu</first><last>Cai</last></author>
      <author><first>Jiahong</first><last>Yuan</last></author>
      <author><first>Kenneth</first><last>Church</last></author>
      <pages>930–945</pages>
      <abstract>Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among attention heads but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the redundancy. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases. (Who) We discover that redundancy patterns are task-agnostic. Similar redundancy patterns even exist for randomly generated token sequences. (“Why”) We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phase-independent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising.</abstract>
      <url hash="17f0ebc0">2021.naacl-main.72</url>
      <doi>10.18653/v1/2021.naacl-main.72</doi>
      <bibkey>bian-etal-2021-attention</bibkey>
      <video href="2021.naacl-main.72.mp4"/>
    </paper>
    <paper id="73">
      <title>Does <fixed-case>BERT</fixed-case> Pretrained on Clinical Notes Reveal Sensitive Data?</title>
      <author><first>Eric</first><last>Lehman</last></author>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Karl</first><last>Pichotta</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>946–959</pages>
      <abstract>Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated “attacks” may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available at https://github.com/elehman16/exposing_patient_data_release.</abstract>
      <url hash="03fc0bc3">2021.naacl-main.73</url>
      <doi>10.18653/v1/2021.naacl-main.73</doi>
      <bibkey>lehman-etal-2021-bert</bibkey>
      <video href="2021.naacl-main.73.mp4"/>
      <pwccode url="https://github.com/elehman16/exposing_patient_data_release" additional="true">elehman16/exposing_patient_data_release</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="74">
      <title>Low-Complexity Probing via Finding Subnetworks</title>
      <author><first>Steven</first><last>Cao</last></author>
      <author><first>Victor</first><last>Sanh</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>960–966</pages>
      <abstract>The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model’s internal representations. This approach can detect properties encoded in the model, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing subnetwork that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher accuracy on pre-trained models and lower accuracy on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the complexity of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher accuracy given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work.</abstract>
      <url hash="b5d87445">2021.naacl-main.74</url>
      <doi>10.18653/v1/2021.naacl-main.74</doi>
      <bibkey>cao-etal-2021-low</bibkey>
      <video href="2021.naacl-main.74.mp4"/>
      <pwccode url="https://github.com/stevenxcao/subnetwork-probing" additional="false">stevenxcao/subnetwork-probing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="75">
      <title>An Empirical Comparison of Instance Attribution Methods for <fixed-case>NLP</fixed-case></title>
      <author><first>Pouya</first><last>Pezeshkpour</last></author>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>967–975</pages>
      <abstract>Widespread adoption of deep models has motivated a pressing need for approaches to interpret network outputs and to facilitate model debugging. Instance attribution methods constitute one means of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF; Koh and Liang 2017) provide machinery for doing this by quantifying the effect that perturbing individual train instances would have on a specific test prediction. However, even approximating the IF is computationally expensive, to the degree that may be prohibitive in many cases. Might simpler approaches (e.g., retrieving train examples most similar to a given test point) perform comparably? In this work, we evaluate the degree to which different potential instance attribution agree with respect to the importance of training samples. We find that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as IFs), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods. Code for all methods and experiments in this paper is available at: https://github.com/successar/instance_attributions_NLP.</abstract>
      <url hash="e872d85f">2021.naacl-main.75</url>
      <doi>10.18653/v1/2021.naacl-main.75</doi>
      <bibkey>pezeshkpour-etal-2021-empirical</bibkey>
      <video href="2021.naacl-main.75.mp4"/>
      <pwccode url="https://github.com/successar/instance_attributions_NLP" additional="false">successar/instance_attributions_NLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="76">
      <title>Generalization in Instruction Following Systems</title>
      <author><first>Soham</first><last>Dan</last></author>
      <author><first>Michael</first><last>Zhou</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>976–981</pages>
      <abstract>Understanding and executing natural language instructions in a grounded domain is one of the hallmarks of artificial intelligence. In this paper, we focus on instruction understanding in the blocks world domain and investigate the language understanding abilities of two top-performing systems for the task. We aim to understand if the test performance of these models indicates an understanding of the spatial domain and of the natural language instructions relative to it, or whether they merely over-fit spurious signals in the dataset. We formulate a set of expectations one might have from an instruction following model and concretely characterize the different dimensions of robustness such a model should possess. Despite decent test performance, we find that state-of-the-art models fall short of these expectations and are extremely brittle. We then propose a learning strategy that involves data augmentation and show through extensive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better.</abstract>
      <url hash="7da474c9">2021.naacl-main.76</url>
      <doi>10.18653/v1/2021.naacl-main.76</doi>
      <bibkey>dan-etal-2021-generalization</bibkey>
      <video href="2021.naacl-main.76.mp4"/>
    </paper>
    <paper id="77">
      <title><fixed-case>L</fixed-case>ightning<fixed-case>DOT</fixed-case>: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval</title>
      <author><first>Siqi</first><last>Sun</last></author>
      <author><first>Yen-Chun</first><last>Chen</last></author>
      <author><first>Linjie</first><last>Li</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Yuwei</first><last>Fang</last></author>
      <author><first>Jingjing</first><last>Liu</last></author>
      <pages>982–997</pages>
      <abstract>Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow inference speed due to enormous computational cost mainly from cross-modal attention in Transformer architecture. When applied to real-life applications, such latency and computation demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most mature scenario of V+L application, which has been widely studied even prior to the emergence of recent pre-trained models. We propose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, without sacrificing accuracy. LightningDOT removes the time-consuming cross-modal attention by extracting pre-cached feature indexes offline, and employing instant dot-product matching online, which significantly speeds up retrieval process. In fact, our LightningDOT achieves superior performance across mainstream ITR benchmarks such as Flickr30k and COCO datasets, outperforming existing pre-trained models that consume 1000 times magnitude of computational hours using the same features.</abstract>
      <url hash="12e5b521">2021.naacl-main.77</url>
      <doi>10.18653/v1/2021.naacl-main.77</doi>
      <bibkey>sun-etal-2021-lightningdot</bibkey>
      <video href="2021.naacl-main.77.mp4"/>
      <pwccode url="https://github.com/intersun/LightningDOT" additional="true">intersun/LightningDOT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="78">
      <title>Measuring Social Biases in Grounded Vision and Language Embeddings</title>
      <author><first>Candace</first><last>Ross</last></author>
      <author><first>Boris</first><last>Katz</last></author>
      <author><first>Andrei</first><last>Barbu</last></author>
      <pages>998–1008</pages>
      <abstract>We generalize the notion of measuring social biases in word embeddings to visually grounded word embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society.</abstract>
      <url hash="1feadaa0">2021.naacl-main.78</url>
      <doi>10.18653/v1/2021.naacl-main.78</doi>
      <bibkey>ross-etal-2021-measuring</bibkey>
      <video href="2021.naacl-main.78.mp4"/>
      <pwccode url="https://github.com/candacelax/bias-in-vision-and-language" additional="false">candacelax/bias-in-vision-and-language</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
    </paper>
    <paper id="79">
      <title><fixed-case>MTAG</fixed-case>: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences</title>
      <author><first>Jianing</first><last>Yang</last></author>
      <author><first>Yongxin</first><last>Wang</last></author>
      <author><first>Ruitao</first><last>Yi</last></author>
      <author><first>Yuying</first><last>Zhu</last></author>
      <author><first>Azaan</first><last>Rehman</last></author>
      <author><first>Amir</first><last>Zadeh</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <pages>1009–1021</pages>
      <abstract>Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-of-the-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters.</abstract>
      <url hash="f1785926">2021.naacl-main.79</url>
      <doi>10.18653/v1/2021.naacl-main.79</doi>
      <bibkey>yang-etal-2021-mtag</bibkey>
      <video href="2021.naacl-main.79.mp4"/>
      <pwccode url="https://github.com/jedyang97/MTAG" additional="false">jedyang97/MTAG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="80">
      <title>Grounding Open-Domain Instructions to Automate Web Support Tasks</title>
      <author><first>Nancy</first><last>Xu</last></author>
      <author><first>Sam</first><last>Masling</last></author>
      <author><first>Michael</first><last>Du</last></author>
      <author><first>Giovanni</first><last>Campagna</last></author>
      <author><first>Larry</first><last>Heck</last></author>
      <author><first>James</first><last>Landay</last></author>
      <author><first>Monica</first><last>Lam</last></author>
      <pages>1022–1032</pages>
      <abstract>Grounding natural language instructions on the web to perform previously unseen tasks enables accessibility and automation. We introduce a task and dataset to train AI agents from open-domain, step-by-step instructions originally written for people. We build RUSS (Rapid Universal Support Service) to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with pointers parses instructions to WebLang, a domain-specific language we design for grounding natural language on the web. Then, a grounding model retrieves the unique IDs of any webpage elements requested in the WebLang. RUSS may interact with the user through a dialogue (e.g. ask for an address) or execute a web operation (e.g. click a button) inside the web runtime. To augment training, we synthesize natural language instructions mapped to WebLang. Our dataset consists of 80 different customer service problems from help websites, with a total of 741 step-by-step instructions and their corresponding actions. RUSS achieves 76.7% end-to-end accuracy predicting agent actions from single instructions. It outperforms state-of-the-art models that directly map instructions to actions without WebLang. Our user study shows that RUSS is preferred by actual users over web navigation.</abstract>
      <url hash="29969513">2021.naacl-main.80</url>
      <doi>10.18653/v1/2021.naacl-main.80</doi>
      <bibkey>xu-etal-2021-grounding</bibkey>
      <video href="2021.naacl-main.80.mp4"/>
      <pwccode url="https://github.com/xnancy/russ" additional="false">xnancy/russ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/russ-dataset">RUSS Dataset</pwcdataset>
    </paper>
    <paper id="81">
      <title>Modular Networks for Compositional Instruction Following</title>
      <author><first>Rodolfo</first><last>Corona</last></author>
      <author><first>Daniel</first><last>Fried</last></author>
      <author><first>Coline</first><last>Devin</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <author><first>Trevor</first><last>Darrell</last></author>
      <pages>1033–1040</pages>
      <abstract>Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, non-modular sequence-to-sequence approaches on ALFRED, a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training.</abstract>
      <url hash="7c40e780">2021.naacl-main.81</url>
      <doi>10.18653/v1/2021.naacl-main.81</doi>
      <bibkey>corona-etal-2021-modular</bibkey>
      <video href="2021.naacl-main.81.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/alfred">ALFRED</pwcdataset>
    </paper>
    <paper id="82">
      <title>Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information</title>
      <author><first>Jialu</first><last>Li</last></author>
      <author><first>Hao</first><last>Tan</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>1041–1050</pages>
      <abstract>Vision language navigation is the task that requires an agent to navigate through a 3D environment based on natural language instructions. One key challenge in this task is to ground instructions with the current visual information that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., modifiers convey attributes, verbs convey actions). Syntax information like dependencies and phrase structures can aid the agent to locate important parts of the instruction. Hence, in this paper, we propose a navigation agent that utilizes syntax information derived from a dependency tree to enhance alignment between the instruction and the current visual scenes. Empirically, our agent outperforms the baseline model that does not use syntax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our agent is better at aligning instructions with the current visual information via qualitative visualizations.</abstract>
      <url hash="c7a6b508">2021.naacl-main.82</url>
      <doi>10.18653/v1/2021.naacl-main.82</doi>
      <bibkey>li-etal-2021-improving</bibkey>
      <video href="2021.naacl-main.82.mp4"/>
      <pwccode url="https://github.com/jialuli-luka/SyntaxVLN" additional="false">jialuli-luka/SyntaxVLN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rxr">RxR</pwcdataset>
    </paper>
    <paper id="83">
      <title>Improving Pretrained Models for Zero-shot Multi-label Text Classification through Reinforced Label Hierarchy Reasoning</title>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Danqing</first><last>Zhang</last></author>
      <author><first>Bing</first><last>Yin</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <pages>1051–1062</pages>
      <abstract>Exploiting label hierarchies has become a promising approach to tackling the zero-shot multi-label text classification (ZS-MTC) problem. Conventional methods aim to learn a matching model between text and labels, using a graph encoder to incorporate label hierarchies to obtain effective label representations (Rios and Kavuluru, 2018). More recently, pretrained models like BERT (Devlin et al., 2018) have been used to convert classification tasks into a textual entailment task (Yin et al., 2019). This approach is naturally suitable for the ZS-MTC task. However, pretrained models are underexplored in the existing work because they do not generate individual vector representations for text or labels, making it unintuitive to combine them with conventional graph encoding methods. In this paper, we explore to improve pretrained models with label hierarchies on the ZS-MTC task. We propose a Reinforced Label Hierarchy Reasoning (RLHR) approach to encourage interdependence among labels in the hierarchies during training. Meanwhile, to overcome the weakness of flat predictions, we design a rollback algorithm that can remove logical errors from predictions during inference. Experimental results on three real-life datasets show that our approach achieves better performance and outperforms previous non-pretrained methods on the ZS-MTC task.</abstract>
      <url hash="d85c705d">2021.naacl-main.83</url>
      <attachment type="OptionalSupplementaryData" hash="6c13013c">2021.naacl-main.83.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.83</doi>
      <bibkey>liu-etal-2021-improving</bibkey>
      <video href="2021.naacl-main.83.mp4"/>
      <pwccode url="https://github.com/layneins/Zero-shot-RLHR" additional="false">layneins/Zero-shot-RLHR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/web-of-science-dataset">WOS</pwcdataset>
    </paper>
    <paper id="84">
      <title>Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach</title>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Simiao</first><last>Zuo</last></author>
      <author><first>Haoming</first><last>Jiang</last></author>
      <author><first>Wendi</first><last>Ren</last></author>
      <author><first>Tuo</first><last>Zhao</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <pages>1063–1077</pages>
      <abstract>Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the fine-tuning stage. We study the problem of fine-tuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, our framework gradually improves model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins and achieves competitive performance with fully-supervised fine-tuning methods. Our implementation is available on https://github.com/yueyu1030/COSINE.</abstract>
      <url hash="e055f2f3">2021.naacl-main.84</url>
      <doi>10.18653/v1/2021.naacl-main.84</doi>
      <bibkey>yu-etal-2021-fine</bibkey>
      <video href="2021.naacl-main.84.mp4"/>
      <pwccode url="https://github.com/yueyu1030/COSINE" additional="false">yueyu1030/COSINE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="85">
      <title>Posterior Differential Regularization with f-divergence for Improving Model Robustness</title>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Lis</first><last>Pereira</last></author>
      <author><first>Yaoliang</first><last>Yu</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>1078–1089</pages>
      <abstract>We address the problem of enhancing model robustness through regularization. Specifically, we focus on methods that regularize the model posterior difference between clean and noisy inputs. Theoretically, we provide a connection of two recent methods, Jacobian Regularization and Virtual Adversarial Training, under this framework. Additionally, we generalize the posterior differential regularization to the family of f-divergences and characterize the overall framework in terms of the Jacobian matrix. Empirically, we compare those regularizations and standard BERT training on a diverse set of tasks to provide a comprehensive profile of their effect on model generalization. For both fully supervised and semi-supervised settings, we show that regularizing the posterior difference with f-divergence can result in well-improved model robustness. In particular, with a proper f-divergence, a BERT-base model can achieve comparable generalization as its BERT-large counterpart for in-domain, adversarial and domain shift scenarios, indicating the great potential of the proposed framework for enhancing NLP model robustness.</abstract>
      <url hash="b320aa65">2021.naacl-main.85</url>
      <doi>10.18653/v1/2021.naacl-main.85</doi>
      <bibkey>cheng-etal-2021-posterior</bibkey>
      <video href="2021.naacl-main.85.mp4"/>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/bioasq">BioASQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="86">
      <title>Understanding Hard Negatives in Noise Contrastive Estimation</title>
      <author><first>Wenzheng</first><last>Zhang</last></author>
      <author><first>Karl</first><last>Stratos</last></author>
      <pages>1090–1101</pages>
      <abstract>The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives—highest-scoring incorrect examples under the model—are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the score function that unifies various architectures used in text retrieval. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.</abstract>
      <url hash="3b6c4213">2021.naacl-main.86</url>
      <doi>10.18653/v1/2021.naacl-main.86</doi>
      <bibkey>zhang-stratos-2021-understanding</bibkey>
      <video href="2021.naacl-main.86.mp4"/>
      <pwccode url="https://github.com/WenzhengZhang/hard-nce-el" additional="false">WenzhengZhang/hard-nce-el</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/zeshel">ZESHEL</pwcdataset>
    </paper>
    <paper id="87">
      <title>Certified Robustness to Word Substitution Attack with Differential Privacy</title>
      <author><first>Wenjie</first><last>Wang</last></author>
      <author><first>Pengfei</first><last>Tang</last></author>
      <author><first>Jian</first><last>Lou</last></author>
      <author><first>Li</first><last>Xiong</last></author>
      <pages>1102–1112</pages>
      <abstract>The robustness and security of natural language processing (NLP) models are significantly important in real-world applications. In the context of text classification tasks, adversarial examples can be designed by substituting words with synonyms under certain semantic and syntactic constraints, such that a well-trained model will give a wrong prediction. Therefore, it is crucial to develop techniques to provide a rigorous and provable robustness guarantee against such attacks. In this paper, we propose WordDP to achieve certified robustness against word substitution at- tacks in text classification via differential privacy (DP). We establish the connection between DP and adversarial robustness for the first time in the text domain and propose a conceptual exponential mechanism-based algorithm to formally achieve the robustness. We further present a practical simulated exponential mechanism that has efficient inference with certified robustness. We not only provide a rigorous analytic derivation of the certified condition but also experimentally compare the utility of WordDP with existing defense algorithms. The results show that WordDP achieves higher accuracy and more than 30X efficiency improvement over the state-of-the-art certified robustness mechanism in typical text classification tasks.</abstract>
      <url hash="9708f194">2021.naacl-main.87</url>
      <attachment type="OptionalSupplementaryData" hash="1ff19c5d">2021.naacl-main.87.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.87</doi>
      <bibkey>wang-etal-2021-certified</bibkey>
      <video href="2021.naacl-main.87.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="88">
      <title><fixed-case>DR</fixed-case>e<fixed-case>C</fixed-case>a: A General Task Augmentation Strategy for Few-Shot Natural Language Inference</title>
      <author><first>Shikhar</first><last>Murty</last></author>
      <author><first>Tatsunori B.</first><last>Hashimoto</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>1113–1125</pages>
      <abstract>Meta-learning promises few-shot learners that can adapt to new distributions by repurposing knowledge acquired from previous training. However, we believe meta-learning has not yet succeeded in NLP due to the lack of a well-defined task distribution, leading to attempts that treat datasets as tasks. Such an ad hoc task distribution causes problems of quantity and quality. Since there’s only a handful of datasets for any NLP problem, meta-learners tend to overfit their adaptation mechanism and, since NLP datasets are highly heterogeneous, many learning episodes have poor transfer between their support and query sets, which discourages the meta-learner from adapting. To alleviate these issues, we propose DReCA (Decomposing datasets into Reasoning Categories), a simple method for discovering and using latent reasoning categories in a dataset, to form additional high quality tasks. DReCA works by splitting examples into label groups, embedding them with a finetuned BERT model and then clustering each group into reasoning categories. Across four few-shot NLI problems, we demonstrate that using DReCA improves the accuracy of meta-learners by 1.5-4%</abstract>
      <url hash="ff6f450a">2021.naacl-main.88</url>
      <doi>10.18653/v1/2021.naacl-main.88</doi>
      <bibkey>murty-etal-2021-dreca</bibkey>
      <video href="2021.naacl-main.88.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="89">
      <title>Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages</title>
      <author><first>Xavier</first><last>Garcia</last></author>
      <author><first>Aditya</first><last>Siddhant</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Ankur</first><last>Parikh</last></author>
      <pages>1126–1137</pages>
      <abstract>Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and English-German. However, early studies have shown that in more realistic settings involving low-resource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 BLEU. In this work, we show that multilinguality is critical to making unsupervised systems practical for low-resource settings. In particular, we present a single model for 5 low-resource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform strong supervised baselines for various language pairs as well as match the performance of the current state-of-the-art supervised model for Nepali-English. We conduct a series of ablation studies to establish the robustness of our model under different degrees of data quality, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional unsupervised models.</abstract>
      <url hash="6817eaa9">2021.naacl-main.89</url>
      <doi>10.18653/v1/2021.naacl-main.89</doi>
      <bibkey>garcia-etal-2021-harnessing</bibkey>
      <video href="2021.naacl-main.89.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
    </paper>
    <paper id="90">
      <title>Macro-Average: Rare Types Are Important Too</title>
      <author><first>Thamme</first><last>Gowda</last></author>
      <author><first>Weiqiu</first><last>You</last></author>
      <author><first>Constantine</first><last>Lignos</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <pages>1138–1157</pages>
      <abstract>While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, MacroF1, and study its applicability to MT evaluation. We find that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MacroF1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods’ outputs.</abstract>
      <url hash="51571b68">2021.naacl-main.90</url>
      <doi>10.18653/v1/2021.naacl-main.90</doi>
      <bibkey>gowda-etal-2021-macro</bibkey>
      <video href="2021.naacl-main.90.mp4"/>
      <pwccode url="https://github.com/thammegowda/007-mt-eval-macro" additional="false">thammegowda/007-mt-eval-macro</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="91">
      <title>Assessing Reference-Free Peer Evaluation for Machine Translation</title>
      <author><first>Sweta</first><last>Agrawal</last></author>
      <author><first>George</first><last>Foster</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Colin</first><last>Cherry</last></author>
      <pages>1158–1171</pages>
      <abstract>Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model, and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities.</abstract>
      <url hash="74221b78">2021.naacl-main.91</url>
      <doi>10.18653/v1/2021.naacl-main.91</doi>
      <bibkey>agrawal-etal-2021-assessing</bibkey>
      <video href="2021.naacl-main.91.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt19-metrics-task">WMT19 Metrics Task</pwcdataset>
    </paper>
    <paper id="92">
      <title>The Curious Case of Hallucinations in Neural Machine Translation</title>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Arul</first><last>Menezes</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <pages>1172–1183</pages>
      <abstract>In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman, and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results.</abstract>
      <url hash="dc8aead2">2021.naacl-main.92</url>
      <doi>10.18653/v1/2021.naacl-main.92</doi>
      <bibkey>raunak-etal-2021-curious</bibkey>
      <video href="2021.naacl-main.92.mp4"/>
      <pwccode url="https://github.com/vyraun/hallucinations" additional="false">vyraun/hallucinations</pwccode>
    </paper>
    <paper id="93">
      <title>Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution</title>
      <author><first>Xavier</first><last>Garcia</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <author><first>Ankur</first><last>Parikh</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <pages>1184–1192</pages>
      <abstract>We propose a straightforward vocabulary adaptation scheme to extend the language capacity of multilingual machine translation models, paving the way towards efficient continual learning for multilingual machine translation. Our approach is suitable for large-scale datasets, applies to distant languages with unseen scripts, incurs only minor degradation on the translation performance for the original language pairs and provides competitive performance even in the case where we only possess monolingual data for the new languages.</abstract>
      <url hash="7c6e7936">2021.naacl-main.93</url>
      <doi>10.18653/v1/2021.naacl-main.93</doi>
      <bibkey>garcia-etal-2021-towards</bibkey>
      <video href="2021.naacl-main.93.mp4"/>
    </paper>
    <paper id="94">
      <title>Towards Modeling the Style of Translators in Neural Machine Translation</title>
      <author><first>Yue</first><last>Wang</last></author>
      <author><first>Cuong</first><last>Hoang</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <pages>1193–1199</pages>
      <abstract>One key ingredient of neural machine translation is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These datasets contain documents translated by professional translators using different but consistent translation styles. Despite that, the model is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state of the art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, human evaluation confirms that the translations are of the same quality.</abstract>
      <url hash="34b6611f">2021.naacl-main.94</url>
      <doi>10.18653/v1/2021.naacl-main.94</doi>
      <bibkey>wang-etal-2021-towards</bibkey>
      <video href="2021.naacl-main.94.mp4"/>
    </paper>
    <paper id="95">
      <title>Self-Supervised Test-Time Learning for Reading Comprehension</title>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>1200–1211</pages>
      <abstract>Recent work on unsupervised question answering has shown that models can be trained with procedurally generated question-answer pairs and can achieve performance competitive with supervised methods. In this work, we consider the task of unsupervised reading comprehension and present a method that performs “test-time learning” (TTL) on a given context (text passage), without requiring training on large-scale human-authored datasets containing <i>context-question-answer</i> triplets. This method operates directly on a single test context, uses self-supervision to train models on synthetically generated question-answer pairs, and then infers answers to unseen human-authored questions for this context. Our method achieves accuracies competitive with fully supervised methods and significantly outperforms current unsupervised methods. TTL methods with a smaller model are also competitive with the current state-of-the-art in unsupervised reading comprehension.</abstract>
      <url hash="80057956">2021.naacl-main.95</url>
      <doi>10.18653/v1/2021.naacl-main.95</doi>
      <bibkey>banerjee-etal-2021-self</bibkey>
      <video href="2021.naacl-main.95.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl">QA-SRL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="96">
      <title>Capturing Row and Column Semantics in Transformer Based Question Answering over Tables</title>
      <author><first>Michael</first><last>Glass</last></author>
      <author><first>Mustafa</first><last>Canim</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <author><first>Saneem</first><last>Chemmengath</last></author>
      <author><first>Vishwajeet</first><last>Kumar</last></author>
      <author><first>Rishav</first><last>Chakravarti</last></author>
      <author><first>Avi</first><last>Sil</last></author>
      <author><first>Feifei</first><last>Pan</last></author>
      <author><first>Samarth</first><last>Bharadwaj</last></author>
      <author><first>Nicolas Rodolfo</first><last>Fauceglia</last></author>
      <pages>1212–1224</pages>
      <abstract>Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the accuracy on this task, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrating that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a transformer based architecture that independently classifies rows and columns to identify relevant cells. While this model yields extremely high accuracy at finding cell values on recent benchmarks, a second model we propose, called RCI representation, provides a significant efficiency advantage for online QA systems over tables by materializing embeddings for existing tables. Experiments on recent benchmarks prove that the proposed methods can effectively locate cell values on tables (up to ~98% Hit@1 accuracy on WikiSQL lookup questions). Also, the interaction model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (TAPAS and TaBERT), achieving ~3.4% and ~18.86% additional precision improvement on the standard WikiSQL benchmark.</abstract>
      <url hash="d46f8805">2021.naacl-main.96</url>
      <doi>10.18653/v1/2021.naacl-main.96</doi>
      <bibkey>glass-etal-2021-capturing</bibkey>
      <video href="2021.naacl-main.96.mp4"/>
      <pwccode url="https://github.com/IBM/row-column-intersection" additional="false">IBM/row-column-intersection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="97">
      <title>Explainable Multi-hop Verbal Reasoning Through Internal Monologue</title>
      <author><first>Zhengzhong</first><last>Liang</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>1225–1250</pages>
      <abstract>Many state-of-the-art (SOTA) language models have achieved high accuracy on several multi-hop reasoning problems. However, these approaches tend to not be interpretable because they do not make the intermediate reasoning steps explicit. Moreover, models trained on simpler tasks tend to fail when directly tested on more complex problems. We propose the Explainable multi-hop Verbal Reasoner (EVR) to solve these limitations by (a) decomposing multi-hop reasoning problems into several simple ones, and (b) using natural language to guide the intermediate reasoning hops. We implement EVR by extending the classic reasoning paradigm General Problem Solver (GPS) with a SOTA generative language model to generate subgoals and perform inference in natural language at each reasoning step. Evaluation of EVR on the RuleTaker synthetic question answering (QA) dataset shows that EVR achieves SOTA performance while being able to generate all reasoning steps in natural language. Furthermore, EVR generalizes better than other strong methods when trained on simpler tasks or less training data (up to 35.7% and 7.7% absolute improvement respectively).</abstract>
      <url hash="5840ea05">2021.naacl-main.97</url>
      <doi>10.18653/v1/2021.naacl-main.97</doi>
      <bibkey>liang-etal-2021-explainable</bibkey>
      <video href="2021.naacl-main.97.mp4"/>
    </paper>
    <paper id="98">
      <title>Robust Question Answering Through Sub-part Alignment</title>
      <author><first>Jifan</first><last>Chen</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>1251–1263</pages>
      <abstract>Current textual question answering (QA) models achieve strong performance on in-domain test sets, but often do so by fitting surface-level patterns, so they fail to generalize to out-of-distribution settings. To make a more robust and understandable QA system, we model question answering as an alignment problem. We decompose both the question and context into smaller units based on off-the-shelf semantic representations (here, semantic roles), and align the question to a subgraph of the context in order to find the answer. We formulate our model as a structured SVM, with alignment scores computed via BERT, and we can train end-to-end despite using beam search for approximate inference. Our use of explicit alignments allows us to explore a set of constraints with which we can prohibit certain types of bad model behavior arising in cross-domain settings. Furthermore, by investigating differences in scores across different potential answers, we can seek to understand what particular aspects of the input lead the model to choose the answer without relying on post-hoc explanation techniques. We train our model on SQuAD v1.1 and test it on several adversarial and out-of-domain datasets. The results show that our model is more robust than the standard BERT QA model, and constraints derived from alignment scores allow us to effectively trade off coverage and accuracy.</abstract>
      <url hash="4a3ec876">2021.naacl-main.98</url>
      <doi>10.18653/v1/2021.naacl-main.98</doi>
      <bibkey>chen-durrett-2021-robust</bibkey>
      <video href="2021.naacl-main.98.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/bio">Bio</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="99">
      <title>Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models</title>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <pages>1264–1279</pages>
      <abstract>We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., <i>language</i>) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.</abstract>
      <url hash="741d30b5">2021.naacl-main.99</url>
      <doi>10.18653/v1/2021.naacl-main.99</doi>
      <bibkey>khot-etal-2021-text</bibkey>
      <video href="2021.naacl-main.99.mp4"/>
      <pwccode url="https://github.com/allenai/modularqa" additional="false">allenai/modularqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/break">BREAK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="100">
      <title><fixed-case>RECONSIDER</fixed-case>: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering</title>
      <author><first>Srinivasan</first><last>Iyer</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <pages>1280–1287</pages>
      <abstract>State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions, but a low overall accuracy, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training. RECONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions, and uses in-passage span annotations to perform span-focused re-ranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positives, achieving a new extractive state of the art on four QA tasks, with 45.5% Exact Match accuracy on Natural Questions with real user questions, and 61.7% on TriviaQA. We will release all related data, models, and code.</abstract>
      <url hash="6e6d549d">2021.naacl-main.100</url>
      <doi>10.18653/v1/2021.naacl-main.100</doi>
      <bibkey>iyer-etal-2021-reconsider</bibkey>
      <video href="2021.naacl-main.100.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="101">
      <title>On the Transferability of Minimal Prediction Preserving Inputs in Question Answering</title>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Yi</first><last>Lu</last></author>
      <author><first>Chris</first><last>DuBois</last></author>
      <pages>1288–1300</pages>
      <abstract>Recent work (Feng et al., 2018) establishes the presence of short, uninterpretable input fragments that yield high confidence and accuracy in neural models. We refer to these as Minimal Prediction Preserving Inputs (MPPIs). In the context of question answering, we investigate competing hypotheses for the existence of MPPIs, including poor posterior calibration of neural models, lack of pretraining, and “dataset bias” (where a model learns to attend to spurious, non-generalizable cues in the training data). We discover a perplexing invariance of MPPIs to random training seed, model architecture, pretraining, and training domain. MPPIs demonstrate remarkable transferability across domains achieving significantly higher performance than comparably short queries. Additionally, penalizing over-confidence on MPPIs fails to improve either generalization or adversarial robustness. These results suggest the interpretability of MPPIs is insufficient to characterize generalization capacity of these models. We hope this focused investigation encourages more systematic analysis of model behavior outside of the human interpretable distribution of examples.</abstract>
      <url hash="8715b22d">2021.naacl-main.101</url>
      <doi>10.18653/v1/2021.naacl-main.101</doi>
      <bibkey>longpre-etal-2021-transferability</bibkey>
      <video href="2021.naacl-main.101.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="102">
      <title>Understanding by Understanding Not: Modeling Negation in Language Models</title>
      <author><first>Arian</first><last>Hosseini</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <author><first>Dzmitry</first><last>Bahdanau</last></author>
      <author><first>R Devon</first><last>Hjelm</last></author>
      <author><first>Alessandro</first><last>Sordoni</last></author>
      <author><first>Aaron</first><last>Courville</last></author>
      <pages>1301–1312</pages>
      <abstract>Negation is a core construction in natural language. Despite being very successful on many tasks, state-of-the-art pre-trained language models often handle negation incorrectly. To improve language models in this regard, we propose to augment the language modeling objective with an unlikelihood objective that is based on negated generic sentences from a raw text corpus. By training BERT with the resulting combined objective we reduce the mean top 1 error rate to 4% on the negated LAMA dataset. We also see some improvements on the negated NLI benchmarks.</abstract>
      <url hash="b16c5482">2021.naacl-main.102</url>
      <doi>10.18653/v1/2021.naacl-main.102</doi>
      <bibkey>hosseini-etal-2021-understanding</bibkey>
      <video href="2021.naacl-main.102.mp4"/>
      <pwccode url="https://github.com/arianhosseini/negation-learning" additional="false">arianhosseini/negation-learning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="103">
      <title><fixed-case>D</fixed-case>uo<fixed-case>RAT</fixed-case>: Towards Simpler Text-to-<fixed-case>SQL</fixed-case> Models</title>
      <author><first>Torsten</first><last>Scholak</last></author>
      <author><first>Raymond</first><last>Li</last></author>
      <author><first>Dzmitry</first><last>Bahdanau</last></author>
      <author><first>Harm</first><last>de Vries</last></author>
      <author><first>Chris</first><last>Pal</last></author>
      <pages>1313–1321</pages>
      <abstract>Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building DuoRAT, a re-implementation of the state-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware or vanilla transformers as the building blocks. We perform several ablation experiments using DuoRAT as the baseline model. Our experiments confirm the usefulness of some techniques and point out the redundancy of others, including structural SQL features and features that link the question with the schema.</abstract>
      <url hash="65e26d11">2021.naacl-main.103</url>
      <doi>10.18653/v1/2021.naacl-main.103</doi>
      <bibkey>scholak-etal-2021-duorat</bibkey>
      <video href="2021.naacl-main.103.mp4"/>
      <pwccode url="https://github.com/ElementAI/duorat" additional="false">ElementAI/duorat</pwccode>
    </paper>
    <paper id="104">
      <title>Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization</title>
      <author><first>Anshuman</first><last>Mishra</last></author>
      <author><first>Dhruvesh</first><last>Patel</last></author>
      <author><first>Aparna</first><last>Vijayakumar</last></author>
      <author><first>Xiang Lorraine</first><last>Li</last></author>
      <author><first>Pavan</first><last>Kapanipathi</last></author>
      <author><first>Kartik</first><last>Talamadupula</last></author>
      <pages>1322–1336</pages>
      <abstract>Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts); (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets; and (3) models trained on the converted, longer-premise datasets outperform those trained using short-premise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths.</abstract>
      <url hash="492ce9df">2021.naacl-main.104</url>
      <doi>10.18653/v1/2021.naacl-main.104</doi>
      <bibkey>mishra-etal-2021-looking</bibkey>
      <video href="2021.naacl-main.104.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="105">
      <title>Structure-Grounded Pretraining for Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Xiang</first><last>Deng</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <author><first>Christopher</first><last>Meek</last></author>
      <author><first>Oleksandr</first><last>Polozov</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <author><first>Matthew</first><last>Richardson</last></author>
      <pages>1337–1350</pages>
      <abstract>Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel pretraining tasks: column grounding, value grounding and column-value mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing text-to-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERTLARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. All the code and data used in this work will be open-sourced to facilitate future research.</abstract>
      <url hash="708167be">2021.naacl-main.105</url>
      <doi>10.18653/v1/2021.naacl-main.105</doi>
      <bibkey>deng-etal-2021-structure</bibkey>
      <video href="2021.naacl-main.105.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/spider-realistic">Spider-Realistic</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/totto">ToTTo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="106">
      <title>Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System</title>
      <author><first>Congying</first><last>Xia</last></author>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Yihao</first><last>Feng</last></author>
      <author><first>Philip</first><last>Yu</last></author>
      <pages>1351–1360</pages>
      <abstract>Text classification is usually studied by labeling natural language texts with relevant categories from a predefined set. In the real world, new classes might keep challenging the existing system with limited labeled data. The system should be intelligent enough to recognize upcoming new classes with a few examples. In this work, we define a new task in the NLP domain, incremental few-shot text classification, where the system incrementally handles multiple rounds of new classes. For each round, there is a batch of new classes with a few labeled examples per class. Two major challenges exist in this new task: (i) For the learning process, the system should incrementally learn new classes round by round without re-training on the examples of preceding classes; (ii) For the performance, the system should perform well on new classes without much loss on preceding classes. In addition to formulating the new task, we also release two benchmark datasets in the incremental few-shot setting: intent classification and relation classification. Moreover, we propose two entailment approaches, ENTAILMENT and HYBRID, which show promise for solving this novel problem.</abstract>
      <url hash="d046f919">2021.naacl-main.106</url>
      <doi>10.18653/v1/2021.naacl-main.106</doi>
      <bibkey>xia-etal-2021-incremental</bibkey>
      <video href="2021.naacl-main.106.mp4"/>
    </paper>
    <paper id="107">
      <title>Temporal Reasoning on Implicit Events from Distant Supervision</title>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1361–1371</pages>
      <abstract>We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events—events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SymTime, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. SymTime outperforms strong baseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1%-9% on MATRES, an explicit event benchmark.</abstract>
      <url hash="d2cf7643">2021.naacl-main.107</url>
      <doi>10.18653/v1/2021.naacl-main.107</doi>
      <bibkey>zhou-etal-2021-temporal</bibkey>
      <video href="2021.naacl-main.107.mp4"/>
    </paper>
    <paper id="108">
      <title>Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models</title>
      <author><first>James Y.</first><last>Huang</last></author>
      <author><first>Kuan-Hao</first><last>Huang</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1372–1379</pages>
      <abstract>Pre-trained language models have achieved huge success on a wide range of NLP tasks. However, contextual representations from pre-trained models contain entangled semantic and syntactic information, and therefore cannot be directly used to derive useful semantic sentence embeddings for some tasks. Paraphrase pairs offer an effective way of learning the distinction between semantics and syntax, as they naturally share semantics and often vary in syntax. In this work, we present ParaBART, a semantic sentence embedding model that learns to disentangle semantics and syntax in sentence embeddings obtained by pre-trained language models. ParaBART is trained to perform syntax-guided paraphrasing, based on a source sentence that shares semantics with the target paraphrase, and a parse tree that specifies the target syntax. In this way, ParaBART learns disentangled semantic and syntactic representations from their respective inputs with separate encoders. Experiments in English show that ParaBART outperforms state-of-the-art sentence embedding models on unsupervised semantic similarity tasks. Additionally, we show that our approach can effectively remove syntactic information from semantic sentence embeddings, leading to better robustness against syntactic variation on downstream semantic tasks.</abstract>
      <url hash="7e0c9ccd">2021.naacl-main.108</url>
      <doi>10.18653/v1/2021.naacl-main.108</doi>
      <bibkey>huang-etal-2021-disentangling</bibkey>
      <video href="2021.naacl-main.108.mp4"/>
      <pwccode url="https://github.com/uclanlp/ParaBART" additional="false">uclanlp/ParaBART</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="109">
      <title>Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs</title>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>1380–1391</pages>
      <abstract>Abstractive conversation summarization has received much attention recently. However, these generated summaries often suffer from insufficient, redundant, or incorrect content, largely due to the unstructured and complex characteristics of human-human interactions. To this end, we propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization, by first incorporating discourse relations between utterances and action triples (“who-doing-what”) in utterances through structured graphs to better encode conversations, and then designing a multi-granularity decoder to generate summaries by combining all levels of information. Experiments show that our proposed models outperform state-of-the-art methods and generalize well in other domains in terms of both automatic evaluations and human judgments. We have publicly released our code at https://github.com/GT-SALT/Structure-Aware-BART.</abstract>
      <url hash="58cc357e">2021.naacl-main.109</url>
      <doi>10.18653/v1/2021.naacl-main.109</doi>
      <bibkey>chen-yang-2021-structure</bibkey>
      <video href="2021.naacl-main.109.mp4"/>
      <pwccode url="https://github.com/GT-SALT/Structure-Aware-BART" additional="false">GT-SALT/Structure-Aware-BART</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="110">
      <title>A New Approach to Overgenerating and Scoring Abstractive Summaries</title>
      <author><first>Kaiqiang</first><last>Song</last></author>
      <author><first>Bingqing</first><last>Wang</last></author>
      <author><first>Zhe</first><last>Feng</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>1392–1404</pages>
      <abstract>We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users’ needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both stages can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this paradigm can achieve state-of-the-art performance.</abstract>
      <url hash="1d44cc71">2021.naacl-main.110</url>
      <doi>10.18653/v1/2021.naacl-main.110</doi>
      <bibkey>song-etal-2021-new</bibkey>
      <video href="2021.naacl-main.110.mp4"/>
      <pwccode url="https://github.com/ucfnlp/varying-length-summ" additional="false">ucfnlp/varying-length-summ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    </paper>
    <paper id="111">
      <title><fixed-case>D</fixed-case>2<fixed-case>S</fixed-case>: Document-to-Slide Generation Via Query-Based Text Summarization</title>
      <author><first>Edward</first><last>Sun</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Dakuo</first><last>Wang</last></author>
      <author><first>Yunfeng</first><last>Zhang</last></author>
      <author><first>Nancy X. R.</first><last>Wang</last></author>
      <pages>1405–1418</pages>
      <abstract>Presentations are critical for communication in all areas of our lives, yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years’ NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach: 1) Use slide titles to retrieve relevant and engaging text, figures, and tables; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that long-form QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation.</abstract>
      <url hash="19567cf2">2021.naacl-main.111</url>
      <doi>10.18653/v1/2021.naacl-main.111</doi>
      <bibkey>sun-etal-2021-d2s</bibkey>
      <video href="2021.naacl-main.111.mp4"/>
      <pwccode url="https://github.com/IBM/document2slides" additional="false">IBM/document2slides</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sciduet">SciDuet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="112">
      <title>Efficient Attentions for Long Document Summarization</title>
      <author><first>Luyang</first><last>Huang</last></author>
      <author><first>Shuyang</first><last>Cao</last></author>
      <author><first>Nikolaus</first><last>Parulian</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>1419–1436</pages>
      <abstract>The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors.</abstract>
      <url hash="a9d9cd13">2021.naacl-main.112</url>
      <doi>10.18653/v1/2021.naacl-main.112</doi>
      <bibkey>huang-etal-2021-efficient</bibkey>
      <video href="2021.naacl-main.112.mp4"/>
      <pwccode url="https://github.com/luyang-huang96/LongDocSum" additional="false">luyang-huang96/LongDocSum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/govreport">GovReport</pwcdataset>
    </paper>
    <paper id="113">
      <title><fixed-case>R</fixed-case>ef<fixed-case>S</fixed-case>um: Refactoring Neural Summarization</title>
      <author><first>Yixin</first><last>Liu</last></author>
      <author><first>Zi-Yi</first><last>Dou</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>1437–1448</pages>
      <abstract>Although some recent works show potential complementarity among different state-of-the-art systems, few works try to investigate this problem in text summarization. Researchers in other areas commonly refer to the techniques of reranking or stacking to approach this problem. In this work, we highlight several limitations of previous methods, which motivates us to present a new framework Refactor that provides a unified view of text summarization and summaries combination. Experimentally, we perform a comprehensive evaluation that involves twenty-two base systems, four datasets, and three different application scenarios. Besides new state-of-the-art results on CNN/DailyMail dataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses the limitations of the traditional methods and the effectiveness of the Refactor model sheds light on insight for performance improvement. Our system can be directly used by other researchers as an off-the-shelf tool to achieve further performance improvements. We open-source all the code and provide a convenient interface to use it: https://github.com/yixinL7/Refactoring-Summarization.</abstract>
      <url hash="e5195109">2021.naacl-main.113</url>
      <doi>10.18653/v1/2021.naacl-main.113</doi>
      <bibkey>liu-etal-2021-refsum</bibkey>
      <video href="2021.naacl-main.113.mp4"/>
      <pwccode url="https://github.com/yixinL7/Refactoring-Summarization" additional="false">yixinL7/Refactoring-Summarization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="114">
      <title>Annotating and Modeling Fine-grained Factuality in Summarization</title>
      <author><first>Tanya</first><last>Goyal</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>1449–1462</pages>
      <abstract>Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSum. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSum summarization models by allowing us to identify non-factual tokens in the training data.</abstract>
      <url hash="95db9a84">2021.naacl-main.114</url>
      <doi>10.18653/v1/2021.naacl-main.114</doi>
      <bibkey>goyal-durrett-2021-annotating</bibkey>
      <video href="2021.naacl-main.114.mp4"/>
      <pwccode url="https://github.com/tagoyal/factuality-datasets" additional="true">tagoyal/factuality-datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="115">
      <title>Larger-Context Tagging: When and Why Does It Work?</title>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Liangjing</first><last>Feng</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>1463–1475</pages>
      <abstract>The development of neural networks and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more context information is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by larger-context training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of contextual information.</abstract>
      <url hash="3cfa0cc5">2021.naacl-main.115</url>
      <doi>10.18653/v1/2021.naacl-main.115</doi>
      <bibkey>fu-etal-2021-larger</bibkey>
      <video href="2021.naacl-main.115.mp4"/>
    </paper>
    <paper id="116">
      <title>Neural Sequence Segmentation as Determining the Leftmost Segments</title>
      <author><first>Yangming</first><last>Li</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Kaisheng</first><last>Yao</last></author>
      <pages>1476–1486</pages>
      <abstract>Prior methods to text segmentation are mostly at token level. Despite the adequacy, this nature limits their full potential to capture the long-term dependencies among segments. In this work, we propose a novel framework that incrementally segments natural language sentences at segment level. For every step in segmentation, it recognizes the leftmost segment of the remaining sequence. Implementations involve LSTM-minus technique to construct the phrase representations and recurrent neural networks (RNN) to model the iterations of determining the leftmost segments. We have conducted extensive experiments on syntactic chunking and Chinese part-of-speech (POS) tagging across 3 datasets, demonstrating that our methods have significantly outperformed previous all baselines and achieved new state-of-the-art results. Moreover, qualitative analysis and the study on segmenting long-length sentences verify its effectiveness in modeling long-term dependencies.</abstract>
      <url hash="33bae495">2021.naacl-main.116</url>
      <doi>10.18653/v1/2021.naacl-main.116</doi>
      <bibkey>li-etal-2021-neural</bibkey>
      <video href="2021.naacl-main.116.mp4"/>
      <pwccode url="https://github.com/LeePleased/LeftmostSeg" additional="false">LeePleased/LeftmostSeg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="117">
      <title><fixed-case>PCFG</fixed-case>s Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols</title>
      <author><first>Songlin</first><last>Yang</last></author>
      <author><first>Yanpeng</first><last>Zhao</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>1487–1498</pages>
      <abstract>Probabilistic context-free grammars (PCFGs) with neural parameterization have been shown to be effective in unsupervised phrase-structure grammar induction. However, due to the cubic computational complexity of PCFG representation and parsing, previous approaches cannot scale up to a relatively large number of (nonterminal and preterminal) symbols. In this work, we present a new parameterization form of PCFGs based on tensor decomposition, which has at most quadratic computational complexity in the symbol number and therefore allows us to use a much larger number of symbols. We further use neural parameterization for the new form to improve unsupervised parsing performance. We evaluate our model across ten languages and empirically demonstrate the effectiveness of using more symbols.</abstract>
      <url hash="a1d06782">2021.naacl-main.117</url>
      <doi>10.18653/v1/2021.naacl-main.117</doi>
      <bibkey>yang-etal-2021-pcfgs</bibkey>
      <video href="2021.naacl-main.117.mp4"/>
      <pwccode url="https://github.com/sustcsonglin/TN-PCFG" additional="false">sustcsonglin/TN-PCFG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="118">
      <title><fixed-case>GEMNET</fixed-case>: Effective Gated Gazetteer Representations for Recognizing Complex Entities in Low-context Input</title>
      <author><first>Tao</first><last>Meng</last></author>
      <author><first>Anjie</first><last>Fang</last></author>
      <author><first>Oleg</first><last>Rokhlenko</last></author>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <pages>1499–1512</pages>
      <abstract>Named Entity Recognition (NER) remains difficult in real-world settings; current challenges include short texts (low context), emerging entities, and complex entities (e.g. movie names). Gazetteer features can help, but results have been mixed due to challenges with adding extra features, and a lack of realistic evaluation data. It has been shown that including gazetteer features can cause models to overuse or underuse them, leading to poor generalization. We propose GEMNET, a novel approach for gazetteer knowledge integration, including (1) a flexible Contextual Gazetteer Representation (CGR) encoder that can be fused with any word-level model; and (2) a Mixture-of- Experts gating network that overcomes the feature overuse issue by learning to conditionally combine the context and gazetteer features, instead of assigning them fixed weights. To comprehensively evaluate our approaches, we create 3 large NER datasets (24M tokens) reflecting current challenges. In an uncased setting, our methods show large gains (up to +49% F1) in recognizing difficult entities compared to existing baselines. On standard benchmarks, we achieve a new uncased SOTA on CoNLL03 and WNUT17.</abstract>
      <url hash="83cc6ffd">2021.naacl-main.118</url>
      <doi>10.18653/v1/2021.naacl-main.118</doi>
      <bibkey>meng-etal-2021-gemnet</bibkey>
      <video href="2021.naacl-main.118.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="119">
      <title>Video-aided Unsupervised Grammar Induction</title>
      <author><first>Songyang</first><last>Zhang</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Jiebo</first><last>Luo</last></author>
      <pages>1513–1524</pages>
      <abstract>We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on grammar induction from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction.</abstract>
      <url hash="850e29c4">2021.naacl-main.119</url>
      <attachment type="OptionalSupplementaryCode" hash="40bcc36f">2021.naacl-main.119.OptionalSupplementaryCode.zip</attachment>
      <award>Best Long Paper</award>
      <doi>10.18653/v1/2021.naacl-main.119</doi>
      <bibkey>zhang-etal-2021-video</bibkey>
      <video href="2021.naacl-main.119.mp4"/>
      <pwccode url="https://github.com/Sy-Zhang/MMC-PCFG" additional="false">Sy-Zhang/MMC-PCFG</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/didemo">DiDeMo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/places">Places</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/youcook2">YouCook2</pwcdataset>
    </paper>
    <paper id="120">
      <title>Generating Negative Samples by Manipulating Golden Responses for Unsupervised Learning of a Response Evaluation Model</title>
      <author><first>ChaeHun</first><last>Park</last></author>
      <author><first>Eugene</first><last>Jang</last></author>
      <author><first>Wonsuk</first><last>Yang</last></author>
      <author><first>Jong</first><last>Park</last></author>
      <pages>1525–1534</pages>
      <abstract>Evaluating the quality of responses generated by open-domain conversation systems is a challenging task. This is partly because there can be multiple appropriate responses to a given dialogue history. Reference-based metrics that rely on comparisons to a set of known correct responses often fail to account for this variety, and consequently correlate poorly with human judgment. To address this problem, researchers have investigated the possibility of assessing response quality without using a set of known correct responses. RUBER demonstrated that an automatic response evaluation model could be made using unsupervised learning for the next-utterance prediction (NUP) task. For the unsupervised learning of such model, we propose a method of manipulating a golden response to create a new negative response that is designed to be inappropriate within the context while maintaining high similarity with the original golden response. We find, from our experiments on English datasets, that using the negative samples generated by our method alongside random negative samples can increase the model’s correlation with human evaluations. The process of generating such negative samples is automated and does not rely on human annotation.</abstract>
      <url hash="7397eb94">2021.naacl-main.120</url>
      <doi>10.18653/v1/2021.naacl-main.120</doi>
      <bibkey>park-etal-2021-generating</bibkey>
      <video href="2021.naacl-main.120.mp4"/>
      <pwccode url="https://github.com/nlpcl-lab/dialog-eval-hard-negative" additional="false">nlpcl-lab/dialog-eval-hard-negative</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog-1">DailyDialog++</pwcdataset>
    </paper>
    <paper id="121">
      <title>How Robust are Fact Checking Systems on Colloquial Claims?</title>
      <author><first>Byeongchang</first><last>Kim</last></author>
      <author><first>Hyunwoo</first><last>Kim</last></author>
      <author><first>Seokhee</first><last>Hong</last></author>
      <author><first>Gunhee</first><last>Kim</last></author>
      <pages>1535–1548</pages>
      <abstract>Knowledge is now starting to power neural dialogue agents. At the same time, the risk of misinformation and disinformation from dialogue agents also rises. Verifying the veracity of information from formal sources are widely studied in computational fact checking. In this work, we ask: How robust are fact checking systems on claims in colloquial style? We aim to open up new discussions in the intersection of fact verification and dialogue safety. In order to investigate how fact checking systems behave on colloquial claims, we transfer the styles of claims from FEVER (Thorne et al., 2018) into colloquialism. We find that existing fact checking systems that perform well on claims in formal style significantly degenerate on colloquial claims with the same semantics. Especially, we show that document retrieval is the weakest spot in the system even vulnerable to filler words, such as “yeah” and “you know”. The document recall of WikiAPI retriever (Hanselowski et al., 2018) which is 90.0% on FEVER, drops to 72.2% on the colloquial claims. We compare the characteristics of colloquial claims to those of claims in formal style, and demonstrate the challenging issues in them.</abstract>
      <url hash="b2357140">2021.naacl-main.121</url>
      <doi>10.18653/v1/2021.naacl-main.121</doi>
      <bibkey>kim-etal-2021-robust</bibkey>
      <video href="2021.naacl-main.121.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="122">
      <title>Fine-grained Post-training for Improving Retrieval-based Dialogue Systems</title>
      <author><first>Janghoon</first><last>Han</last></author>
      <author><first>Taesuk</first><last>Hong</last></author>
      <author><first>Byoungjae</first><last>Kim</last></author>
      <author><first>Youngjoong</first><last>Ko</last></author>
      <author><first>Jungyun</first><last>Seo</last></author>
      <pages>1549–1558</pages>
      <abstract>Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task.</abstract>
      <url hash="c5a0dec4">2021.naacl-main.122</url>
      <attachment type="OptionalSupplementaryCode" hash="7644b256">2021.naacl-main.122.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="7644b256">2021.naacl-main.122.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.122</doi>
      <bibkey>han-etal-2021-fine</bibkey>
      <video href="2021.naacl-main.122.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/douban">Douban</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-commerce-1">E-commerce</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rrs">RRS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rrs-ranking-test">RRS Ranking Test</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ubuntu-dialogue-corpus">UDC</pwcdataset>
    </paper>
    <paper id="123">
      <title>Put Chatbot into Its Interlocutor’s Shoes: New Framework to Learn Chatbot Responding with Intention</title>
      <author><first>Hsuan</first><last>Su</last></author>
      <author><first>Jiun-Hao</first><last>Jhan</last></author>
      <author><first>Fan-yun</first><last>Sun</last></author>
      <author><first>Saurav</first><last>Sahay</last></author>
      <author><first>Hung-yi</first><last>Lee</last></author>
      <pages>1559–1569</pages>
      <abstract>Most chatbot literature that focuses on improving the fluency and coherence of a chatbot, is dedicated to making chatbots more human-like. However, very little work delves into what really separates humans from chatbots – humans intrinsically understand the effect their responses have on the interlocutor and often respond with an intention such as proposing an optimistic view to make the interlocutor feel better. This paper proposes an innovative framework to train chatbots to possess human-like intentions. Our framework includes a guiding chatbot and an interlocutor model that plays the role of humans. The guiding chatbot is assigned an intention and learns to induce the interlocutor to reply with responses matching the intention, for example, long responses, joyful responses, responses with specific words, etc. We examined our framework using three experimental setups and evaluated the guiding chatbot with four different metrics to demonstrate flexibility and performance advantages. Additionally, we performed trials with human interlocutors to substantiate the guiding chatbot’s effectiveness in influencing the responses of humans to a certain extent. Code will be made available to the public.</abstract>
      <url hash="403dcb7b">2021.naacl-main.123</url>
      <doi>10.18653/v1/2021.naacl-main.123</doi>
      <bibkey>su-etal-2021-put</bibkey>
      <video href="2021.naacl-main.123.mp4"/>
    </paper>
    <paper id="124">
      <title>Adding Chit-Chat to Enhance Task-Oriented Dialogues</title>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>Stephen</first><last>Roller</last></author>
      <author><first>Becka</first><last>Silvert</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Zhiguang</first><last>Wang</last></author>
      <author><first>Honglei</first><last>Liu</last></author>
      <author><first>Eunjoon</first><last>Cho</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <pages>1570–1583</pages>
      <abstract>Existing dialogue corpora and models are typically designed under two disjoint motives: while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding Chit-Chat to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human &lt;-&gt; AI collaborative data collection approach for generating diverse chit-chat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat-based annotations to 23.8K dialogues from two popular task-oriented datasets (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for adding chit-chat to task-oriented dialogues, explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can code-switch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance.</abstract>
      <url hash="b543a644">2021.naacl-main.124</url>
      <doi>10.18653/v1/2021.naacl-main.124</doi>
      <bibkey>sun-etal-2021-adding</bibkey>
      <video href="2021.naacl-main.124.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="125">
      <title>Incorporating Syntax and Semantics in Coreference Resolution with Heterogeneous Graph Attention Network</title>
      <author><first>Fan</first><last>Jiang</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>1584–1591</pages>
      <abstract>External syntactic and semantic information has been largely ignored by existing neural coreference resolution models. In this paper, we present a heterogeneous graph-based model to incorporate syntactic and semantic structures of sentences. The proposed graph contains a syntactic sub-graph where tokens are connected based on a dependency tree, and a semantic sub-graph that contains arguments and predicates as nodes and semantic role labels as edges. By applying a graph attention network, we can obtain syntactically and semantically augmented word representation, which can be integrated using an attentive integration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model.</abstract>
      <url hash="4c7242a7">2021.naacl-main.125</url>
      <doi>10.18653/v1/2021.naacl-main.125</doi>
      <bibkey>jiang-cohn-2021-incorporating</bibkey>
      <video href="2021.naacl-main.125.mp4"/>
      <pwccode url="https://github.com/fantabulous-j/coref-hgat" additional="false">fantabulous-j/coref-hgat</pwccode>
    </paper>
    <paper id="126">
      <title>Context Tracking Network: Graph-based Context Modeling for Implicit Discourse Relation Recognition</title>
      <author><first>Yingxue</first><last>Zhang</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Ping</first><last>Jian</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>1592–1599</pages>
      <abstract>Implicit discourse relation recognition (IDRR) aims to identify logical relations between two adjacent sentences in the discourse. Existing models fail to fully utilize the contextual information which plays an important role in interpreting each local sentence. In this paper, we thus propose a novel graph-based Context Tracking Network (CT-Net) to model the discourse context for IDRR. The CT-Net firstly converts the discourse into the paragraph association graph (PAG), where each sentence tracks their closely related context from the intricate discourse through different types of edges. Then, the CT-Net extracts contextual representation from the PAG through a specially designed cross-grained updating mechanism, which can effectively integrate both sentence-level and token-level contextual semantics. Experiments on PDTB 2.0 show that the CT-Net gains better performance than models that roughly model the context.</abstract>
      <url hash="0f815f84">2021.naacl-main.126</url>
      <doi>10.18653/v1/2021.naacl-main.126</doi>
      <bibkey>zhang-etal-2021-context</bibkey>
      <video href="2021.naacl-main.126.mp4"/>
    </paper>
    <paper id="127">
      <title>Improving Neural <fixed-case>RST</fixed-case> Parsing Model with Silver Agreement Subtrees</title>
      <author><first>Naoki</first><last>Kobayashi</last></author>
      <author><first>Tsutomu</first><last>Hirao</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>1600–1612</pages>
      <abstract>Most of the previous Rhetorical Structure Theory (RST) parsing methods are based on supervised learning such as neural networks, that require an annotated corpus of sufficient size and quality. However, the RST Discourse Treebank (RST-DT), the benchmark corpus for RST parsing in English, is small due to the costly annotation of RST trees. The lack of large annotated training data causes poor performance especially in relation labeling. Therefore, we propose a method for improving neural RST parsing models by exploiting silver data, i.e., automatically annotated data. We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser.</abstract>
      <url hash="cebfff98">2021.naacl-main.127</url>
      <doi>10.18653/v1/2021.naacl-main.127</doi>
      <bibkey>kobayashi-etal-2021-improving</bibkey>
      <video href="2021.naacl-main.127.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/rst-dt">RST-DT</pwcdataset>
    </paper>
    <paper id="128">
      <title><fixed-case>RST</fixed-case> Parsing from Scratch</title>
      <author><first>Thanh-Tung</first><last>Nguyen</last></author>
      <author><first>Xuan-Phi</first><last>Nguyen</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <pages>1613–1625</pages>
      <abstract>We introduce a novel top-down end-to-end formulation of document level discourse parsing in the Rhetorical Structure Theory (RST) framework. In this formulation, we consider discourse parsing as a sequence of splitting decisions at token boundaries and use a seq2seq network to model the splitting decisions. Our framework facilitates discourse parsing from scratch without requiring discourse segmentation as a prerequisite; rather, it yields segmentation as part of the parsing process. Our unified parsing model adopts a beam search to decode the best tree structure by searching through a space of high scoring trees. With extensive experiments on the standard RST discourse treebank, we demonstrate that our parser outperforms existing methods by a good margin in both end-to-end parsing and parsing with gold segmentation. More importantly, it does so without using any handcrafted features, making it faster and easily adaptable to new languages and domains.</abstract>
      <url hash="6904f81f">2021.naacl-main.128</url>
      <doi>10.18653/v1/2021.naacl-main.128</doi>
      <bibkey>nguyen-etal-2021-rst</bibkey>
      <video href="2021.naacl-main.128.mp4"/>
      <pwccode url="https://github.com/tungngthanh/rst_parser" additional="false">tungngthanh/rst_parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rst-dt">RST-DT</pwcdataset>
    </paper>
    <paper id="129">
      <title>Did they answer? Subjective acts and intents in conversational discourse</title>
      <author><first>Elisa</first><last>Ferracane</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Katrin</first><last>Erk</last></author>
      <pages>1626–1644</pages>
      <abstract>Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/elisaF/subjective_discourse.</abstract>
      <url hash="dff13f7d">2021.naacl-main.129</url>
      <doi>10.18653/v1/2021.naacl-main.129</doi>
      <bibkey>ferracane-etal-2021-answer</bibkey>
      <video href="2021.naacl-main.129.mp4"/>
      <pwccode url="https://github.com/elisaF/subjective_discourse" additional="false">elisaF/subjective_discourse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/subjective-discourse">Subjective Discourse</pwcdataset>
    </paper>
    <paper id="130">
      <title>Evaluating the Impact of a Hierarchical Discourse Representation on Entity Coreference Resolution Performance</title>
      <author><first>Sopan</first><last>Khosla</last></author>
      <author><first>James</first><last>Fiacco</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <pages>1645–1651</pages>
      <abstract>Recent work on entity coreference resolution (CR) follows current trends in Deep Learning applied to embeddings and relatively simple task-related features. SOTA models do not make use of hierarchical representations of discourse structure. In this work, we leverage automatically constructed discourse parse trees within a neural approach and demonstrate a significant improvement on two benchmark entity coreference-resolution datasets. We explore how the impact varies depending upon the type of mention.</abstract>
      <url hash="ed7fe9fb">2021.naacl-main.130</url>
      <doi>10.18653/v1/2021.naacl-main.130</doi>
      <bibkey>khosla-etal-2021-evaluating</bibkey>
      <video href="2021.naacl-main.130.mp4"/>
    </paper>
    <paper id="131">
      <title>Bridging Resolution: Making Sense of the State of the Art</title>
      <author><first>Hideo</first><last>Kobayashi</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>1652–1659</pages>
      <abstract>While Yu and Poesio (2020) have recently demonstrated the superiority of their neural multi-task learning (MTL) model to rule-based approaches for bridging anaphora resolution, there is little understanding of (1) how it is better than the rule-based approaches (e.g., are the two approaches making similar or complementary mistakes?) and (2) what should be improved. To shed light on these issues, we (1) propose a hybrid rule-based and MTL approach that would enable a better understanding of their comparative strengths and weaknesses; and (2) perform a manual analysis of the errors made by the MTL model.</abstract>
      <url hash="43ddf872">2021.naacl-main.131</url>
      <doi>10.18653/v1/2021.naacl-main.131</doi>
      <bibkey>kobayashi-ng-2021-bridging</bibkey>
      <video href="2021.naacl-main.131.mp4"/>
    </paper>
    <paper id="132">
      <title>Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle</title>
      <author><first>Yikang</first><last>Shen</last></author>
      <author><first>Shawn</first><last>Tan</last></author>
      <author><first>Alessandro</first><last>Sordoni</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <author><first>Aaron</first><last>Courville</last></author>
      <pages>1660–1672</pages>
      <abstract>Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing, and syntactic generalization tests while using fewer parameters than other models.</abstract>
      <url hash="05557f48">2021.naacl-main.132</url>
      <attachment type="OptionalSupplementaryCode" hash="aae71756">2021.naacl-main.132.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.132</doi>
      <bibkey>shen-etal-2021-explicitly</bibkey>
      <video href="2021.naacl-main.132.mp4"/>
    </paper>
    <paper id="133">
      <title>Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation</title>
      <author><first>Samuel</first><last>Kiegeland</last></author>
      <author><first>Julia</first><last>Kreutzer</last></author>
      <pages>1673–1681</pages>
      <abstract>Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. (2020) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counter-evidence to these claims.</abstract>
      <url hash="8ba37471">2021.naacl-main.133</url>
      <doi>10.18653/v1/2021.naacl-main.133</doi>
      <bibkey>kiegeland-kreutzer-2021-revisiting</bibkey>
      <video href="2021.naacl-main.133.mp4"/>
      <pwccode url="https://github.com/samuki/reinforce-joey" additional="false">samuki/reinforce-joey</pwccode>
    </paper>
    <paper id="134">
      <title>Learning to Organize a Bag of Words into Sentences with Neural Networks: An Empirical Study</title>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Shen</first><last>Gao</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>1682–1691</pages>
      <abstract>Sequential information, a.k.a., orders, is assumed to be essential for processing a sequence with recurrent neural network or convolutional neural network based encoders. However, is it possible to encode natural languages without orders? Given a bag of words from a disordered sentence, humans may still be able to understand what those words mean by reordering or reconstructing them. Inspired by such an intuition, in this paper, we perform a study to investigate how “order” information takes effects in natural language learning. By running comprehensive comparisons, we quantitatively compare the ability of several representative neural models to organize sentences from a bag of words under three typical scenarios, and summarize some empirical findings and challenges, which can shed light on future research on this line of work.</abstract>
      <url hash="ffc26cf2">2021.naacl-main.134</url>
      <doi>10.18653/v1/2021.naacl-main.134</doi>
      <bibkey>tao-etal-2021-learning</bibkey>
      <video href="2021.naacl-main.134.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="135">
      <title>Mask Attention Networks: Rethinking and Strengthen Transformer</title>
      <author><first>Zhihao</first><last>Fan</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Siyuan</first><last>Wang</last></author>
      <author><first>Jian</first><last>Jiao</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Ruofei</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>1692–1701</pages>
      <abstract>Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit the capability for localness modeling in text representation learning. We therefore introduce a new layer named dynamic mask attention network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer.</abstract>
      <url hash="720f933a">2021.naacl-main.135</url>
      <doi>10.18653/v1/2021.naacl-main.135</doi>
      <bibkey>fan-etal-2021-mask</bibkey>
      <video href="2021.naacl-main.135.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="136">
      <title><fixed-case>ERNIE</fixed-case>-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding</title>
      <author><first>Dongling</first><last>Xiao</last></author>
      <author><first>Yu-Kun</first><last>Li</last></author>
      <author><first>Han</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hao</first><last>Tian</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>1702–1715</pages>
      <abstract>Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT’s Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training. In ERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens. Furthermore, ERNIE-Gram employs a generator model to sample plausible n-gram identities as optional n-gram masks and predict them in both coarse-grained and fine-grained manners to enable comprehensive n-gram prediction and relation modeling. We pre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19 downstream tasks. Experimental results show that ERNIE-Gram outperforms previous pre-training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.</abstract>
      <url hash="e9bf68ce">2021.naacl-main.136</url>
      <doi>10.18653/v1/2021.naacl-main.136</doi>
      <bibkey>xiao-etal-2021-ernie</bibkey>
      <video href="2021.naacl-main.136.mp4"/>
      <pwccode url="https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/ernie_gram/modeling.py" additional="true">PaddlePaddle/PaddleNLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc">CMRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc-2018">CMRC 2018</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drcd">DRCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dureader">DuReader</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="137">
      <title>Lattice-<fixed-case>BERT</fixed-case>: Leveraging Multi-Granularity Representations in <fixed-case>C</fixed-case>hinese Pre-trained Language Models</title>
      <author><first>Yuxuan</first><last>Lai</last></author>
      <author><first>Yijia</first><last>Liu</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>1716–1731</pages>
      <abstract>Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese — Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the lattice structures, and the improvement comes from the exploration of redundant information and multi-granularity representations. Our code will be available at https://github.com/alibaba/pretrained-language-models/LatticeBERT.</abstract>
      <url hash="25f9f321">2021.naacl-main.137</url>
      <doi>10.18653/v1/2021.naacl-main.137</doi>
      <bibkey>lai-etal-2021-lattice</bibkey>
      <video href="2021.naacl-main.137.mp4"/>
      <pwccode url="https://github.com/alibaba/AliceMind" additional="true">alibaba/AliceMind</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
    </paper>
    <paper id="138">
      <title>Modeling Event Plausibility with Consistent Conceptual Abstraction</title>
      <author><first>Ian</first><last>Porada</last></author>
      <author><first>Kaheer</first><last>Suleman</last></author>
      <author><first>Adam</first><last>Trischler</last></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last></author>
      <pages>1732–1743</pages>
      <abstract>Understanding natural language requires common sense, one aspect of which is the ability to discern the plausibility of events. While distributional models—most recently pre-trained, Transformer language models—have demonstrated improvements in modeling event plausibility, their performance still falls short of humans’. In this work, we show that Transformer-based plausibility models are markedly inconsistent across the conceptual classes of a lexical hierarchy, inferring that “a person breathing” is plausible while “a dentist breathing” is not, for example. We find this inconsistency persists even when models are softly injected with lexical knowledge, and we present a simple post-hoc method of forcing model consistency that improves correlation with human plausibility judgements.</abstract>
      <url hash="1a571897">2021.naacl-main.138</url>
      <doi>10.18653/v1/2021.naacl-main.138</doi>
      <bibkey>porada-etal-2021-modeling</bibkey>
      <video href="2021.naacl-main.138.mp4"/>
      <pwccode url="https://github.com/ianporada/modeling_event_plausibility" additional="false">ianporada/modeling_event_plausibility</pwccode>
    </paper>
    <paper id="139">
      <title><fixed-case>U</fixed-case>mls<fixed-case>BERT</fixed-case>: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the <fixed-case>U</fixed-case>nified <fixed-case>M</fixed-case>edical <fixed-case>L</fixed-case>anguage <fixed-case>S</fixed-case>ystem <fixed-case>M</fixed-case>etathesaurus</title>
      <author><first>George</first><last>Michalopoulos</last></author>
      <author><first>Yuanxin</first><last>Wang</last></author>
      <author><first>Hussam</first><last>Kaka</last></author>
      <author><first>Helen</first><last>Chen</last></author>
      <author><first>Alexander</first><last>Wong</last></author>
      <pages>1744–1753</pages>
      <abstract>Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such models do not take into consideration structured expert domain knowledge from a knowledge base. We introduce UmlsBERT, a contextual embedding model that integrates domain knowledge during the pre-training process via a novel knowledge augmentation strategy. More specifically, the augmentation on UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus is performed in two ways: i) connecting words that have the same underlying ‘concept’ in UMLS and ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings. By applying these two strategies, UmlsBERT can encode clinical domain knowledge into word embeddings and outperform existing domain-specific models on common named-entity recognition (NER) and clinical natural language inference tasks.</abstract>
      <url hash="bb95c792">2021.naacl-main.139</url>
      <doi>10.18653/v1/2021.naacl-main.139</doi>
      <bibkey>michalopoulos-etal-2021-umlsbert</bibkey>
      <video href="2021.naacl-main.139.mp4"/>
      <pwccode url="https://github.com/gmichalo/UmlsBERT" additional="false">gmichalo/UmlsBERT</pwccode>
    </paper>
    <paper id="140">
      <title>Field Embedding: A Unified Grain-Based Framework for Word Representation</title>
      <author><first>Junjie</first><last>Luo</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Jichao</first><last>Sun</last></author>
      <author><first>Yuejia</first><last>Xiang</last></author>
      <author><first>Ningyu</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Wan</last></author>
      <pages>1754–1762</pages>
      <abstract>Word representations empowered with additional linguistic information have been widely studied and proved to outperform traditional embeddings. Current methods mainly focus on learning embeddings for words while embeddings of linguistic information (referred to as grain embeddings) are discarded after the learning. This work proposes a framework field embedding to jointly learn both word and grain embeddings by incorporating morphological, phonetic, and syntactical linguistic fields. The framework leverages an innovative fine-grained pipeline that integrates multiple linguistic fields and produces high-quality grain sequences for learning supreme word representations. A novel algorithm is also designed to learn embeddings for words and grains by capturing information that is contained within each field and that is shared across them. Experimental results of lexical tasks and downstream natural language processing tasks illustrate that our framework can learn better word embeddings and grain embeddings. Qualitative evaluations show grain embeddings effectively capture the semantic information.</abstract>
      <url hash="1a0eb651">2021.naacl-main.140</url>
      <attachment type="OptionalSupplementaryData" hash="31344bd5">2021.naacl-main.140.OptionalSupplementaryData.zip</attachment>
      <attachment type="OptionalSupplementaryCode" hash="bfe6a847">2021.naacl-main.140.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.140</doi>
      <bibkey>luo-etal-2021-field</bibkey>
      <video href="2021.naacl-main.140.mp4"/>
    </paper>
    <paper id="141">
      <title><fixed-case>M</fixed-case>el<fixed-case>BERT</fixed-case>: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories</title>
      <author><first>Minjin</first><last>Choi</last></author>
      <author><first>Sunkyung</first><last>Lee</last></author>
      <author><first>Eunseong</first><last>Choi</last></author>
      <author><first>Heesoo</first><last>Park</last></author>
      <author><first>Junhyuk</first><last>Lee</last></author>
      <author><first>Dongwon</first><last>Lee</last></author>
      <author><first>Jongwuk</first><last>Lee</last></author>
      <pages>1763–1773</pages>
      <abstract>Automated metaphor detection is a challenging task to identify the metaphorical expression of words in a sentence. To tackle this problem, we adopt pre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we propose a novel metaphor detection model, namely <i>metaphor-aware late interaction over BERT (MelBERT)</i>. Our model not only leverages contextualized word representation but also benefits from linguistic metaphor identification theories to detect whether the target word is metaphorical. Our empirical results demonstrate that MelBERT outperforms several strong baselines on four benchmark datasets, i.e., VUA-18, VUA-20, MOH-X, and TroFi.</abstract>
      <url hash="d0a22c32">2021.naacl-main.141</url>
      <doi>10.18653/v1/2021.naacl-main.141</doi>
      <bibkey>choi-etal-2021-melbert</bibkey>
      <video href="2021.naacl-main.141.mp4"/>
      <pwccode url="https://github.com/jin530/MelBERT" additional="false">jin530/MelBERT</pwccode>
    </paper>
    <paper id="142">
      <title>Non-Parametric Few-Shot Learning for Word Sense Disambiguation</title>
      <author><first>Howard</first><last>Chen</last></author>
      <author><first>Mengzhou</first><last>Xia</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>1774–1781</pages>
      <abstract>Word sense disambiguation (WSD) is a long-standing problem in natural language processing. One significant challenge in supervised all-words WSD is to classify among senses for a majority of words that lie in the long-tail distribution. For instance, 84% of the annotated words have less than 10 examples in the SemCor training data. This issue is more pronounced as the imbalance occurs in both word and sense distributions. In this work, we propose MetricWSD, a non-parametric few-shot learning approach to mitigate this data imbalance issue. By learning to compute distances among the senses of a given word through episodic training, MetricWSD transfers knowledge (a learned metric space) from high-frequency words to infrequent ones. MetricWSD constructs the training episodes tailored to word frequencies and explicitly addresses the problem of the skewed distribution, as opposed to mixing all the words trained with parametric models in previous work. Without resorting to any lexical resources, MetricWSD obtains strong performance against parametric alternatives, achieving a 75.1 F1 score on the unified WSD evaluation benchmark (Raganato et al., 2017b). Our analysis further validates that infrequent words and senses enjoy significant improvement.</abstract>
      <url hash="2c8abd0f">2021.naacl-main.142</url>
      <doi>10.18653/v1/2021.naacl-main.142</doi>
      <bibkey>chen-etal-2021-non</bibkey>
      <video href="2021.naacl-main.142.mp4"/>
      <pwccode url="https://github.com/princeton-nlp/metric-wsd" additional="false">princeton-nlp/metric-wsd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="143">
      <title>Why Do Document-Level Polarity Classifiers Fail?</title>
      <author><first>Karen</first><last>Martins</last></author>
      <author><first>Pedro O.S</first><last>Vaz-de-Melo</last></author>
      <author><first>Rodrygo</first><last>Santos</last></author>
      <pages>1782–1794</pages>
      <abstract>Machine learning solutions are often criticized for the lack of explanation of their successes and failures. Understanding which instances are misclassified and why is essential to improve the learning process. This work helps to fill this gap by proposing a methodology to characterize, quantify and measure the impact of hard instances in the task of polarity classification of movie reviews. We characterize such instances into two categories: neutrality, where the text does not convey a clear polarity, and discrepancy, where the polarity of the text is the opposite of its true rating. We quantify the number of hard instances in polarity classification of movie reviews and provide empirical evidence about the need to pay attention to such problematic instances, as they are much harder to classify, for both machine and human classifiers. To the best of our knowledge, this is the first systematic analysis of the impact of hard instances in polarity detection from well-formed textual reviews.</abstract>
      <url hash="5f282cc5">2021.naacl-main.143</url>
      <doi>10.18653/v1/2021.naacl-main.143</doi>
      <bibkey>martins-etal-2021-document</bibkey>
      <video href="2021.naacl-main.143.mp4"/>
      <pwccode url="https://github.com/karenstemartins/NAACL2021" additional="false">karenstemartins/NAACL2021</pwccode>
    </paper>
    <paper id="144">
      <title>A Unified Span-Based Approach for Opinion Mining with Syntactic Constituents</title>
      <author><first>Qingrong</first><last>Xia</last></author>
      <author><first>Bo</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>1795–1804</pages>
      <abstract>Fine-grained opinion mining (OM) has achieved increasing attraction in the natural language processing (NLP) community, which aims to find the opinion structures of “Who expressed what opinions towards what” in one sentence. In this work, motivated by its span-based representations of opinion expressions and roles, we propose a unified span-based approach for the end-to-end OM setting. Furthermore, inspired by the unified span-based formalism of OM and constituent parsing, we explore two different methods (multi-task learning and graph convolutional neural network) to integrate syntactic constituents into the proposed model to help OM. We conduct experiments on the commonly used MPQA 2.0 dataset. The experimental results show that our proposed unified span-based approach achieves significant improvements over previous works in the exact F1 score and reduces the number of wrongly-predicted opinion expressions and roles, showing the effectiveness of our method. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations.</abstract>
      <url hash="f3512cff">2021.naacl-main.144</url>
      <doi>10.18653/v1/2021.naacl-main.144</doi>
      <bibkey>xia-etal-2021-unified</bibkey>
      <video href="2021.naacl-main.144.mp4"/>
      <pwccode url="https://github.com/KiroSummer/opinion_mining_with_syn_cons" additional="false">KiroSummer/opinion_mining_with_syn_cons</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="145">
      <title>Target-specified Sequence Labeling with Multi-head Self-attention for Target-oriented Opinion Words Extraction</title>
      <author><first>Yuhao</first><last>Feng</last></author>
      <author><first>Yanghui</first><last>Rao</last></author>
      <author><first>Yuyao</first><last>Tang</last></author>
      <author><first>Ninghua</first><last>Wang</last></author>
      <author><first>He</first><last>Liu</last></author>
      <pages>1805–1815</pages>
      <abstract>Opinion target extraction and opinion term extraction are two fundamental tasks in Aspect Based Sentiment Analysis (ABSA). Many recent works on ABSA focus on Target-oriented Opinion Words (or Terms) Extraction (TOWE), which aims at extracting the corresponding opinion words for a given opinion target. TOWE can be further applied to Aspect-Opinion Pair Extraction (AOPE) which aims at extracting aspects (i.e., opinion targets) and opinion terms in pairs. In this paper, we propose Target-Specified sequence labeling with Multi-head Self-Attention (TSMSA) for TOWE, in which any pre-trained language model with multi-head self-attention can be integrated conveniently. As a case study, we also develop a Multi-Task structure named MT-TSMSA for AOPE by combining our TSMSA with an aspect and opinion term extraction module. Experimental results indicate that TSMSA outperforms the benchmark methods on TOWE significantly; meanwhile, the performance of MT-TSMSA is similar or even better than state-of-the-art AOPE baseline models.</abstract>
      <url hash="9759adc5">2021.naacl-main.145</url>
      <doi>10.18653/v1/2021.naacl-main.145</doi>
      <bibkey>feng-etal-2021-target</bibkey>
      <video href="2021.naacl-main.145.mp4"/>
      <pwccode url="https://github.com/fengyh3/TSMSA" additional="false">fengyh3/TSMSA</pwccode>
    </paper>
    <paper id="146">
      <title>Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a</title>
      <author><first>Junqi</first><last>Dai</last></author>
      <author><first>Hang</first><last>Yan</last></author>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>1816–1829</pages>
      <abstract>Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities for aspects, is a fine-grained task in the field of sentiment analysis. Previous work showed syntactic information, e.g. dependency trees, can effectively improve the ABSA performance. Recently, pre-trained models (PTMs) also have shown their effectiveness on ABSA. Therefore, the question naturally arises whether PTMs contain sufficient syntactic information for ABSA so that we can obtain a good ABSA model only based on PTMs. In this paper, we firstly compare the induced trees from PTMs and the dependency parsing trees on several popular models for the ABSA task, showing that the induced tree from fine-tuned RoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis experiments reveal that the FT-RoBERTa Induced Tree is more sentiment-word-oriented and could benefit the ABSA task. The experiments also show that the pure RoBERTa-based model can outperform or approximate to the previous SOTA performances on six datasets across four languages since it implicitly incorporates the task-oriented syntactic information.</abstract>
      <url hash="56346654">2021.naacl-main.146</url>
      <doi>10.18653/v1/2021.naacl-main.146</doi>
      <bibkey>dai-etal-2021-syntax</bibkey>
      <video href="2021.naacl-main.146.mp4"/>
      <pwccode url="https://github.com/ROGERDJQ/RoBERTaABSA" additional="false">ROGERDJQ/RoBERTaABSA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2">SemEval 2014 Task 4 Sub Task 2</pwcdataset>
    </paper>
    <paper id="147">
      <title>Domain Divergences: A Survey and Empirical Analysis</title>
      <author><first>Abhinav</first><last>Ramesh Kashyap</last></author>
      <author><first>Devamanyu</first><last>Hazarika</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Roger</first><last>Zimmermann</last></author>
      <pages>1830–1849</pages>
      <abstract>Domain divergence plays a significant role in estimating the performance of a model in new domains. While there is a significant literature on divergence measures, researchers find it hard to choose an appropriate divergence for a given NLP application. We address this shortcoming by both surveying the literature and through an empirical study. We develop a taxonomy of divergence measures consisting of three classes — Information-theoretic, Geometric, and Higher-order measures and identify the relationships between them. Further, to understand the common use-cases of these measures, we recognise three novel applications – 1) Data Selection, 2) Learning Representation, and 3) Decisions in the Wild – and use it to organise our literature. From this, we identify that Information-theoretic measures are prevalent for 1) and 3), and Higher-order measures are more common for 2). To further help researchers choose appropriate measures to predict drop in performance – an important aspect of Decisions in the Wild, we perform correlation analysis spanning 130 domain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures identified from our survey. To calculate these divergences, we consider the current contextual word representations (CWR) and contrast with the older distributed representations. We find that traditional measures over word distributions still serve as strong baselines, while higher-order measures with CWR are effective.</abstract>
      <url hash="f1023779">2021.naacl-main.147</url>
      <doi>10.18653/v1/2021.naacl-main.147</doi>
      <bibkey>ramesh-kashyap-etal-2021-domain</bibkey>
      <video href="2021.naacl-main.147.mp4"/>
    </paper>
    <paper id="148">
      <title>Target-Aware Data Augmentation for Stance Detection</title>
      <author><first>Yingjie</first><last>Li</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>1850–1860</pages>
      <abstract>The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this task, one of the remaining challenges is the scarcity of annotations. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In this paper, we formulate the data augmentation of stance detection as a conditional masked language modeling task and augment the dataset by predicting the masked word conditioned on both its context and the auxiliary sentence that contains target and label information. Moreover, we propose another simple yet effective method that generates target-aware sentence by replacing a target mention with the other. Experimental results show that our proposed methods significantly outperforms previous augmentation methods on 11 targets.</abstract>
      <url hash="733545e5">2021.naacl-main.148</url>
      <doi>10.18653/v1/2021.naacl-main.148</doi>
      <bibkey>li-caragea-2021-target</bibkey>
      <video href="2021.naacl-main.148.mp4"/>
    </paper>
    <paper id="149">
      <title>End-to-end <fixed-case>ASR</fixed-case> to jointly predict transcriptions and linguistic annotations</title>
      <author><first>Motoi</first><last>Omachi</last></author>
      <author><first>Yuya</first><last>Fujita</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <author><first>Matthew</first><last>Wiesner</last></author>
      <pages>1861–1871</pages>
      <abstract>We propose a Transformer-based sequence-to-sequence model for automatic speech recognition (ASR) capable of simultaneously transcribing and annotating audio with linguistic information such as phonemic transcripts or part-of-speech (POS) tags. Since linguistic information is important in natural language processing (NLP), the proposed ASR is especially useful for speech interface applications, including spoken dialogue systems and speech translation, which combine ASR and NLP. To produce linguistic annotations, we train the ASR system using modified training targets: each grapheme or multi-grapheme unit in the target transcript is followed by an aligned phoneme sequence and/or POS tag. Since our method has access to the underlying audio data, we can estimate linguistic annotations more accurately than pipeline approaches in which NLP-based methods are applied to a hypothesized ASR transcript. Experimental results on Japanese and English datasets show that the proposed ASR system is capable of simultaneously producing high-quality transcriptions and linguistic annotations.</abstract>
      <url hash="aed7a0ce">2021.naacl-main.149</url>
      <doi>10.18653/v1/2021.naacl-main.149</doi>
      <bibkey>omachi-etal-2021-end</bibkey>
      <video href="2021.naacl-main.149.mp4"/>
    </paper>
    <paper id="150">
      <title>Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation</title>
      <author><first>Hirofumi</first><last>Inaguma</last></author>
      <author><first>Tatsuya</first><last>Kawahara</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <pages>1872–1881</pages>
      <abstract>A conventional approach to improving the performance of end-to-end speech translation (E2E-ST) models is to leverage the source transcription via pre-training and joint training with automatic speech recognition (ASR) and neural machine translation (NMT) tasks. However, since the input modalities are different, it is difficult to leverage source language text successfully. In this work, we focus on sequence-level knowledge distillation (SeqKD) from external text-based NMT models. To leverage the full potential of the source language information, we propose backward SeqKD, SeqKD from a target-to-source backward NMT model. To this end, we train a bilingual E2E-ST model to predict paraphrased transcriptions as an auxiliary task with a single decoder. The paraphrases are generated from the translations in bitext via back-translation. We further propose bidirectional SeqKD in which SeqKD from both forward and backward NMT models is combined. Experimental evaluations on both autoregressive and non-autoregressive models show that SeqKD in each direction consistently improves the translation performance, and the effectiveness is complementary regardless of the model capacity.</abstract>
      <url hash="eec3a5ba">2021.naacl-main.150</url>
      <doi>10.18653/v1/2021.naacl-main.150</doi>
      <bibkey>inaguma-etal-2021-source</bibkey>
      <video href="2021.naacl-main.150.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="151">
      <title>Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks</title>
      <author><first>Siddharth</first><last>Dalmia</last></author>
      <author><first>Brian</first><last>Yan</last></author>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <pages>1882–1896</pages>
      <abstract>End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like speech translation, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using beam search to enhance the overall performance and can also incorporate external models at intermediate stages of the network to re-score or adapt towards out-of-domain data. One instance of the proposed framework is a Multi-Decoder model for speech translation that extracts the searchable hidden intermediates from a speech recognition sub-task. The model demonstrates the aforementioned benefits and outperforms the previous state-of-the-art by around +6 and +3 BLEU on the two test sets of Fisher-CallHome and by around +3 and +4 BLEU on the English-German and English-French test sets of MuST-C.</abstract>
      <url hash="338d6636">2021.naacl-main.151</url>
      <doi>10.18653/v1/2021.naacl-main.151</doi>
      <bibkey>dalmia-etal-2021-searchable</bibkey>
      <video href="2021.naacl-main.151.mp4"/>
    </paper>
    <paper id="152">
      <title><fixed-case>SPLAT</fixed-case>: Speech-Language Joint Pre-Training for Spoken Language Understanding</title>
      <author><first>Yu-An</first><last>Chung</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>1897–1907</pages>
      <abstract>Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models’ performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semi-supervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous state-of-the-art performance on the Spoken SQuAD dataset by more than 10%.</abstract>
      <url hash="3b558c6b">2021.naacl-main.152</url>
      <doi>10.18653/v1/2021.naacl-main.152</doi>
      <bibkey>chung-etal-2021-splat</bibkey>
      <video href="2021.naacl-main.152.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fluent-speech-commands">Fluent Speech Commands</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/spoken-squad">Spoken-SQuAD</pwcdataset>
    </paper>
    <paper id="153">
      <title>Worldly Wise (<fixed-case>W</fixed-case>o<fixed-case>W</fixed-case>) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering</title>
      <author><first>Kiran</first><last>Ramnath</last></author>
      <author><first>Leda</first><last>Sari</last></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last></author>
      <author><first>Chang</first><last>Yoo</last></author>
      <pages>1908–1919</pages>
      <abstract>Although Question-Answering has long been of research interest, its accessibility to users through a speech interface and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed: (1) speech-to-text based, (2) end-to-end, without speech-to-text as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW).WoW is shown to perform end-to-end cross-lingual FVSQA at same levels of accuracy across 3 languages - English, Hindi, and Turkish.</abstract>
      <url hash="47edd518">2021.naacl-main.153</url>
      <attachment type="OptionalSupplementaryCode" hash="f66c37cb">2021.naacl-main.153.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="f66c37cb">2021.naacl-main.153.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.153</doi>
      <bibkey>ramnath-etal-2021-worldly</bibkey>
      <video href="2021.naacl-main.153.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/places">Places</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="154">
      <title>Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment</title>
      <author><first>Ethan A.</first><last>Chi</last></author>
      <author><first>Julian</first><last>Salazar</last></author>
      <author><first>Katrin</first><last>Kirchhoff</last></author>
      <pages>1920–1927</pages>
      <abstract>Non-autoregressive encoder-decoder models greatly improve decoding speed over autoregressive models, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose *iterative realignment*, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14x decoding speedup; on LibriSpeech, we reach an LM-free test-other WER of 9.0% (19% relative improvement on comparable work) in three iterations. We release our code at https://github.com/amazon-research/align-refine.</abstract>
      <url hash="5b63d4e1">2021.naacl-main.154</url>
      <doi>10.18653/v1/2021.naacl-main.154</doi>
      <bibkey>chi-etal-2021-align</bibkey>
      <video href="2021.naacl-main.154.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="155">
      <title>Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis</title>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Yuting</first><last>Wu</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>1928–1941</pages>
      <abstract>Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with structured data, while mining causal relationship among factors from unstructured data, like text, has been less examined, but is of great importance, especially in the legal domain. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the framework on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the causal knowledge contained in GCI can be effectively injected into powerful neural networks for better performance and interpretability.</abstract>
      <url hash="37f1a6ce">2021.naacl-main.155</url>
      <doi>10.18653/v1/2021.naacl-main.155</doi>
      <bibkey>liu-etal-2021-everything</bibkey>
      <video href="2021.naacl-main.155.mp4"/>
      <pwccode url="https://github.com/xxxiaol/GCI" additional="false">xxxiaol/GCI</pwccode>
    </paper>
    <paper id="156">
      <title>Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network</title>
      <author><first>Haoran</first><last>Wu</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Shuang</first><last>Xu</last></author>
      <author><first>Bo</first><last>Xu</last></author>
      <pages>1942–1955</pages>
      <abstract>Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the graph. Features having the strongest causal connection with the results provide interpretive support for the diagnosis. Experimental results on real Chinese EMR of the lymphedema demonstrate that our method can diagnose four types of EMR correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the medical field.</abstract>
      <url hash="aedd8ee0">2021.naacl-main.156</url>
      <doi>10.18653/v1/2021.naacl-main.156</doi>
      <bibkey>wu-etal-2021-counterfactual</bibkey>
      <video href="2021.naacl-main.156.mp4"/>
      <pwccode url="https://github.com/ckre/cmge" additional="false">ckre/cmge</pwccode>
    </paper>
    <paper id="157">
      <title>Personalized Response Generation via Generative Split Memory Network</title>
      <author><first>Yuwei</first><last>Wu</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>1956–1970</pages>
      <abstract>Despite the impressive successes of generation and dialogue systems, how to endow a text generation system with particular personality traits to deliver more personalized responses remains under-investigated. In this work, we look at how to generate personalized responses for questions on Reddit by utilizing personalized user profiles and posting histories. Specifically, we release an open-domain <i>single-turn</i> dialog dataset made up of 1.5M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories: one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines.</abstract>
      <url hash="fe27b0a1">2021.naacl-main.157</url>
      <attachment type="OptionalSupplementaryData" hash="c9ae8735">2021.naacl-main.157.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.157</doi>
      <bibkey>wu-etal-2021-personalized</bibkey>
      <video href="2021.naacl-main.157.mp4"/>
      <pwccode url="https://github.com/willyoung2017/per-chat" additional="false">willyoung2017/per-chat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pec">PEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/personaldialog">PersonalDialog</pwcdataset>
    </paper>
    <paper id="158">
      <title>Towards Few-shot Fact-Checking via Perplexity</title>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Yejin</first><last>Bang</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>1971–1981</pages>
      <abstract>Few-shot learning has drawn researchers’ attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in few-shot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19.</abstract>
      <url hash="2a26b242">2021.naacl-main.158</url>
      <doi>10.18653/v1/2021.naacl-main.158</doi>
      <bibkey>lee-etal-2021-towards</bibkey>
      <video href="2021.naacl-main.158.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="159">
      <title>Active<tex-math>^2</tex-math> Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation</title>
      <author><first>Rishi</first><last>Hazra</last></author>
      <author><first>Parag</first><last>Dutta</last></author>
      <author><first>Shubham</first><last>Gupta</last></author>
      <author><first>Mohammed Abdul</first><last>Qaathir</last></author>
      <author><first>Ambedkar</first><last>Dukkipati</last></author>
      <pages>1982–1995</pages>
      <abstract>While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL strategies choose examples independently, they may potentially select similar examples, all of which may not contribute significantly to the learning process. Our proposed approach, Active<tex-math>\mathbf{^2}</tex-math> Learning (A<tex-math>\mathbf{^2}</tex-math>L), actively adapts to the deep learning model being trained to eliminate such redundant examples chosen by an AL strategy. We show that A<tex-math>\mathbf{^2}</tex-math>L is widely applicable by using it in conjunction with several different AL strategies and NLP tasks. We empirically demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by <tex-math>\approx \mathbf{3-25\%}</tex-math> on an absolute scale on multiple NLP tasks while achieving the same performance with virtually no additional computation overhead.</abstract>
      <url hash="e8a2a0b4">2021.naacl-main.159</url>
      <doi>10.18653/v1/2021.naacl-main.159</doi>
      <bibkey>hazra-etal-2021-active2</bibkey>
      <video href="2021.naacl-main.159.mp4"/>
    </paper>
    <paper id="160">
      <title>Generating An Optimal Interview Question Plan Using A Knowledge Graph And Integer Linear Programming</title>
      <author><first>Soham</first><last>Datta</last></author>
      <author><first>Prabir</first><last>Mallick</last></author>
      <author><first>Sangameshwar</first><last>Patil</last></author>
      <author><first>Indrajit</first><last>Bhattacharya</last></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <pages>1996–2005</pages>
      <abstract>Given the diversity of the candidates and complexity of job requirements, and since interviewing is an inherently subjective process, it is an important task to ensure consistent, uniform, efficient and objective interviews that result in high quality recruitment. We propose an interview assistant system to automatically, and in an objective manner, select an optimal set of technical questions (from question banks) personalized for a candidate. This set can help a human interviewer to plan for an upcoming interview of that candidate. We formalize the problem of selecting a set of questions as an integer linear programming problem and use standard solvers to get a solution. We use knowledge graph as background knowledge in this formulation, and derive our objective functions and constraints from it. We use candidate’s resume to personalize the selection of questions. We propose an intrinsic evaluation to compare a set of suggested questions with actually asked questions. We also use expert interviewers to comparatively evaluate our approach with a set of reasonable baselines.</abstract>
      <url hash="3e84403f">2021.naacl-main.160</url>
      <doi>10.18653/v1/2021.naacl-main.160</doi>
      <bibkey>datta-etal-2021-generating</bibkey>
      <video href="2021.naacl-main.160.mp4"/>
    </paper>
    <paper id="161">
      <title>Model Extraction and Adversarial Transferability, Your <fixed-case>BERT</fixed-case> is Vulnerable!</title>
      <author><first>Xuanli</first><last>He</last></author>
      <author><first>Lingjuan</first><last>Lyu</last></author>
      <author><first>Lichao</first><last>Sun</last></author>
      <author><first>Qiongkai</first><last>Xu</last></author>
      <pages>2006–2012</pages>
      <abstract>Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.</abstract>
      <url hash="39018058">2021.naacl-main.161</url>
      <doi>10.18653/v1/2021.naacl-main.161</doi>
      <bibkey>he-etal-2021-model</bibkey>
      <video href="2021.naacl-main.161.mp4"/>
      <pwccode url="https://github.com/xlhex/extract_and_transfer" additional="false">xlhex/extract_and_transfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="162">
      <title>A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models</title>
      <author><first>Kaiyuan</first><last>Liao</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Bin</first><last>He</last></author>
      <pages>2013–2023</pages>
      <abstract>Early exit mechanism aims to accelerate the inference speed of large-scale pre-trained language models. The essential idea is to exit early without passing through all the inference layers at the inference stage. To make accurate predictions for downstream tasks, the hierarchical linguistic information embedded in all layers should be jointly considered. However, much of the research up to now has been limited to use local representations of the exit layer. Such treatment inevitably loses information of the unused past layers as well as the high-level features embedded in future layers, leading to sub-optimal performance. To address this issue, we propose a novel Past-Future method to make comprehensive predictions from a global perspective. We first take into consideration all the linguistic information embedded in the past layers and then take a further step to engage the future information which is originally inaccessible for predictions. Extensive experiments demonstrate that our method outperforms previous early exit methods by a large margin, yielding better and robust performance.</abstract>
      <url hash="e65c760c">2021.naacl-main.162</url>
      <doi>10.18653/v1/2021.naacl-main.162</doi>
      <bibkey>liao-etal-2021-global</bibkey>
      <video href="2021.naacl-main.162.mp4"/>
      <pwccode url="https://github.com/lancopku/early-exit" additional="false">lancopku/early-exit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="163">
      <title>Masked Conditional Random Fields for Sequence Labeling</title>
      <author><first>Tianwen</first><last>Wei</last></author>
      <author><first>Jianwei</first><last>Qi</last></author>
      <author><first>Shenghuan</first><last>He</last></author>
      <author><first>Songtao</first><last>Sun</last></author>
      <pages>2024–2035</pages>
      <abstract>Conditional Random Field (CRF) based neural models are among the most performant methods for solving sequence labeling problems. Despite its great success, CRF has the shortcoming of occasionally generating illegal sequences of tags, e.g. sequences containing an “I-” tag immediately after an “O” tag, which is forbidden by the underlying BIO tagging scheme. In this work, we propose Masked Conditional Random Field (MCRF), an easy to implement variant of CRF that impose restrictions on candidate paths during both training and decoding phases. We show that the proposed method thoroughly resolves this issue and brings significant improvement over existing CRF-based models with near zero additional cost.</abstract>
      <url hash="60cfc4ca">2021.naacl-main.163</url>
      <doi>10.18653/v1/2021.naacl-main.163</doi>
      <bibkey>wei-etal-2021-masked</bibkey>
      <video href="2021.naacl-main.163.mp4"/>
      <pwccode url="https://github.com/DandyQi/MaskedCRF" additional="true">DandyQi/MaskedCRF</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="164">
      <title>Heterogeneous Graph Neural Networks for Concept Prerequisite Relation Learning in Educational Data</title>
      <author><first>Chenghao</first><last>Jia</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Yechun</first><last>Tang</last></author>
      <author><first>Lu</first><last>Sun</last></author>
      <author><first>Weiming</first><last>Lu</last></author>
      <pages>2036–2047</pages>
      <abstract>Prerequisite relations among concepts are crucial for educational applications, such as curriculum planning and intelligent tutoring. In this paper, we propose a novel concept prerequisite relation learning approach, named CPRL, which combines both concept representation learned from a heterogeneous graph and concept pairwise features. Furthermore, we extend CPRL under weakly supervised settings to make our method more practical, including learning prerequisite relations from learning object dependencies and generating training data with data programming. Our experiments on four datasets show that the proposed approach achieves the state-of-the-art results comparing with existing methods.</abstract>
      <url hash="1d1d1224">2021.naacl-main.164</url>
      <doi>10.18653/v1/2021.naacl-main.164</doi>
      <bibkey>jia-etal-2021-heterogeneous</bibkey>
      <video href="2021.naacl-main.164.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/lecturebank">LectureBank</pwcdataset>
    </paper>
    <paper id="165">
      <title>Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Wenkai</first><last>Yang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Zhang</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Bin</first><last>He</last></author>
      <pages>2048–2058</pages>
      <abstract>Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/lancopku/Embedding-Poisoning.</abstract>
      <url hash="d4506bd6">2021.naacl-main.165</url>
      <doi>10.18653/v1/2021.naacl-main.165</doi>
      <bibkey>yang-etal-2021-careful</bibkey>
      <video href="2021.naacl-main.165.mp4"/>
      <pwccode url="https://github.com/lancopku/Embedding-Poisoning" additional="false">lancopku/Embedding-Poisoning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="166">
      <title><fixed-case>DA</fixed-case>-Transformer: Distance-aware Transformer</title>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>2059–2068</pages>
      <abstract>Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.</abstract>
      <url hash="a6ea20ce">2021.naacl-main.166</url>
      <doi>10.18653/v1/2021.naacl-main.166</doi>
      <bibkey>wu-etal-2021-da</bibkey>
      <video href="2021.naacl-main.166.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="167">
      <title><fixed-case>ASAP</fixed-case>: A <fixed-case>C</fixed-case>hinese Review Dataset Towards Aspect Category Sentiment Analysis and Rating Prediction</title>
      <author><first>Jiahao</first><last>Bu</last></author>
      <author><first>Lei</first><last>Ren</last></author>
      <author><first>Shuang</first><last>Zheng</last></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <pages>2069–2079</pages>
      <abstract>Sentiment analysis has attracted increasing attention in e-commerce. The sentiment polarities underlying user reviews are of great value for business intelligence. Aspect category sentiment analysis (ACSA) and review rating prediction (RP) are two essential tasks to detect the fine-to-coarse sentiment polarities. ACSA and RP are highly correlated and usually employed jointly in real-world e-commerce scenarios. While most public datasets are constructed for ACSA and RP separately, which may limit the further exploitation of both tasks. To address the problem and advance related researches, we present a large-scale Chinese restaurant review dataset ASAP including 46, 730 genuine reviews from a leading online-to-offline (O2O) e-commerce platform in China. Besides a 5-star scale rating, each review is manually annotated according to its sentiment polarities towards 18 pre-defined aspect categories. We hope the release of the dataset could shed some light on the field of sentiment analysis. Moreover, we propose an intuitive yet effective joint model for ACSA and RP. Experimental results demonstrate that the joint model outperforms state-of-the-art baselines on both tasks.</abstract>
      <url hash="f01b6848">2021.naacl-main.167</url>
      <attachment type="OptionalSupplementaryData" hash="ed7b3ad8">2021.naacl-main.167.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.167</doi>
      <bibkey>bu-etal-2021-asap</bibkey>
      <video href="2021.naacl-main.167.mp4"/>
      <pwccode url="https://github.com/Meituan-Dianping/asap" additional="false">Meituan-Dianping/asap</pwccode>
    </paper>
    <paper id="168">
      <title>Are <fixed-case>NLP</fixed-case> Models really able to Solve Simple Math Word Problems?</title>
      <author><first>Arkil</first><last>Patel</last></author>
      <author><first>Satwik</first><last>Bhattamishra</last></author>
      <author><first>Navin</first><last>Goyal</last></author>
      <pages>2080–2094</pages>
      <abstract>The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered “solved” with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.</abstract>
      <url hash="9598435b">2021.naacl-main.168</url>
      <attachment type="OptionalSupplementaryCode" hash="2e277b73">2021.naacl-main.168.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="af0d277e">2021.naacl-main.168.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.168</doi>
      <bibkey>patel-etal-2021-nlp</bibkey>
      <video href="2021.naacl-main.168.mp4"/>
      <pwccode url="https://github.com/arkilpatel/SVAMP" additional="true">arkilpatel/SVAMP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/svamp">SVAMP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mawps">MAWPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/math23k">Math23K</pwcdataset>
    </paper>
    <paper id="169">
      <title><fixed-case>WRIME</fixed-case>: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations</title>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Chenhui</first><last>Chu</last></author>
      <author><first>Noriko</first><last>Takemura</last></author>
      <author><first>Yuta</first><last>Nakashima</last></author>
      <author><first>Hajime</first><last>Nagahara</last></author>
      <pages>2095–2104</pages>
      <abstract>We annotate 17,000 SNS posts with both the writer’s subjective emotional intensity and the reader’s objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer’s subjective labels than the readers’. The large gap between the subjective and objective emotions imply the complexity of the mapping from a post to the subjective emotion intensities, which also leads to a lower performance with machine learning models.</abstract>
      <url hash="8d459ead">2021.naacl-main.169</url>
      <doi>10.18653/v1/2021.naacl-main.169</doi>
      <bibkey>kajiwara-etal-2021-wrime</bibkey>
      <video href="2021.naacl-main.169.mp4"/>
      <pwccode url="https://github.com/ids-cv/wrime" additional="false">ids-cv/wrime</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emobank">EmoBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
    </paper>
    <paper id="170">
      <title><fixed-case>KPQA</fixed-case>: A Metric for Generative Question Answering Using Keyphrase Weights</title>
      <author><first>Hwanhee</first><last>Lee</last></author>
      <author><first>Seunghyun</first><last>Yoon</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Doo Soon</first><last>Kim</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Joongbo</first><last>Shin</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>2105–2115</pages>
      <abstract>In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQA metric, a new metric for evaluating the correctness of GenQA. Specifically, our new metric assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our metric, we create high-quality human judgments of correctness on two GenQA datasets. Using our human-evaluation datasets, we show that our proposed metric has a significantly higher correlation with human judgments than existing metrics in various datasets. Code for KPQA-metric will be available at https://github.com/hwanheelee1993/KPQA.</abstract>
      <url hash="fdae925e">2021.naacl-main.170</url>
      <attachment type="OptionalSupplementaryData" hash="5a65429a">2021.naacl-main.170.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.170</doi>
      <bibkey>lee-etal-2021-kpqa</bibkey>
      <video href="2021.naacl-main.170.mp4"/>
      <pwccode url="https://github.com/hwanheelee1993/KPQA" additional="false">hwanheelee1993/KPQA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="171">
      <title><fixed-case>S</fixed-case>tyle<fixed-case>PTB</fixed-case>: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer</title>
      <author><first>Yiwei</first><last>Lyu</last></author>
      <author><first>Paul Pu</first><last>Liang</last></author>
      <author><first>Hai</first><last>Pham</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Barnabás</first><last>Póczos</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <author><first>Louis-Philippe</first><last>Morency</last></author>
      <pages>2116–2138</pages>
      <abstract>Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.</abstract>
      <url hash="57644a32">2021.naacl-main.171</url>
      <attachment type="OptionalSupplementaryCode" hash="8f8f1b70">2021.naacl-main.171.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="302e28a4">2021.naacl-main.171.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.171</doi>
      <bibkey>lyu-etal-2021-styleptb</bibkey>
      <video href="2021.naacl-main.171.mp4"/>
      <pwccode url="https://github.com/lvyiwei1/StylePTB" additional="true">lvyiwei1/StylePTB</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/styleptb">StylePTB</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="172">
      <title>Blow the Dog Whistle: A <fixed-case>C</fixed-case>hinese Dataset for Cant Understanding with Common Sense and World Knowledge</title>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Wangchunshu</first><last>Zhou</last></author>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Ke</first><last>Xu</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>2139–2145</pages>
      <abstract>Cant is important for understanding advertising, comedies and dog-whistle politics. However, computational research on cant is hindered by a lack of available datasets. In this paper, we propose a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective. We formulate a task for cant understanding and provide both quantitative and qualitative analysis for tested word embedding similarity and pretrained language models. Experiments suggest that such a task requires deep language understanding, common sense, and world knowledge and thus can be a good testbed for pretrained language models and help models perform better on other tasks.</abstract>
      <url hash="074a7e93">2021.naacl-main.172</url>
      <doi>10.18653/v1/2021.naacl-main.172</doi>
      <bibkey>xu-etal-2021-blow</bibkey>
      <video href="2021.naacl-main.172.mp4"/>
      <pwccode url="https://github.com/JetRunner/dogwhistle" additional="false">JetRunner/dogwhistle</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dogwhistle">DogWhistle</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clue">CLUE</pwcdataset>
    </paper>
    <paper id="173">
      <title><fixed-case>COVID</fixed-case>-19 Named Entity Recognition for <fixed-case>V</fixed-case>ietnamese</title>
      <author><first>Thinh Hung</first><last>Truong</last></author>
      <author><first>Mai Hoang</first><last>Dao</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <pages>2146–2153</pages>
      <abstract>The current COVID-19 pandemic has lead to the creation of many corpora that facilitate NLP research and downstream applications to help fight the pandemic. However, most of these corpora are exclusively for English. As the pandemic is a global problem, it is worth creating COVID-19 related datasets for languages other than English. In this paper, we present the first manually-annotated COVID-19 domain-specific dataset for Vietnamese. Particularly, our dataset is annotated for the named entity recognition (NER) task with newly-defined entity types that can be used in other future epidemics. Our dataset also contains the largest number of entities compared to existing Vietnamese NER datasets. We empirically conduct experiments using strong baselines on our dataset, and find that: automatic Vietnamese word segmentation helps improve the NER results and the highest performances are obtained by fine-tuning pre-trained language models where the monolingual model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) produces higher results than the multilingual model XLM-R (Conneau et al., 2020). We publicly release our dataset at: https://github.com/VinAIResearch/PhoNER_COVID19</abstract>
      <url hash="c033111a">2021.naacl-main.173</url>
      <doi>10.18653/v1/2021.naacl-main.173</doi>
      <bibkey>truong-etal-2021-covid</bibkey>
      <video href="2021.naacl-main.173.mp4"/>
      <pwccode url="https://github.com/VinAIResearch/PhoNER_COVID19" additional="false">VinAIResearch/PhoNER_COVID19</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/phoner-covid19">PhoNER COVID19</pwcdataset>
    </paper>
    <paper id="174">
      <title>Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of Media Frames</title>
      <author><first>Shima</first><last>Khanehzar</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Gosia</first><last>Mikolajczak</last></author>
      <author><first>Andrew</first><last>Turpin</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>2154–2166</pages>
      <abstract>Understanding how news media frame political issues is important due to its impact on public attitudes, yet hard to automate. Computational approaches have largely focused on classifying the frame of a full news article while framing signals are often subtle and local. Furthermore, automatic news analysis is a sensitive domain, and existing classifiers lack transparency in their predictions. This paper addresses both issues with a novel semi-supervised model, which jointly learns to embed local information about the events and related actors in a news article through an auto-encoding framework, and to leverage this signal for document-level frame classification. Our experiments show that: our model outperforms previous models of frame prediction; we can further improve performance with unlabeled training data leveraging the semi-supervised nature of our model; and the learnt event and actor embeddings intuitively corroborate the document-level predictions, providing a nuanced and interpretable article frame representation.</abstract>
      <url hash="88657dd6">2021.naacl-main.174</url>
      <doi>10.18653/v1/2021.naacl-main.174</doi>
      <bibkey>khanehzar-etal-2021-framing</bibkey>
      <video href="2021.naacl-main.174.mp4"/>
      <pwccode url="https://github.com/shinyemimalef/FRISS" additional="false">shinyemimalef/FRISS</pwccode>
    </paper>
    <paper id="175">
      <title>Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism</title>
      <author><first>Shraey</first><last>Bhatia</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>2167–2175</pages>
      <abstract>Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models.</abstract>
      <url hash="b855eab7">2021.naacl-main.175</url>
      <doi>10.18653/v1/2021.naacl-main.175</doi>
      <bibkey>bhatia-etal-2021-automatic</bibkey>
      <video href="2021.naacl-main.175.mp4"/>
    </paper>
    <paper id="176">
      <title>Suicide Ideation Detection via Social and Temporal User Representations using Hyperbolic Learning</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Harshit</first><last>Joshi</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>2176–2190</pages>
      <abstract>Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Personally contextualizing the buildup of such ideation is critical for accurate identification of users at risk. In this work, we propose a framework jointly leveraging a user’s emotional history and social information from a user’s neighborhood in a network to contextualize the interpretation of the latest tweet of a user on Twitter. Reflecting upon the scale-free nature of social network relationships, we propose the use of Hyperbolic Graph Convolution Networks, in combination with the Hawkes process to learn the historical emotional spectrum of a user in a time-sensitive manner. Our system significantly outperforms state-of-the-art methods on this task, showing the benefits of both socially and personally contextualized representations.</abstract>
      <url hash="b3fe56c2">2021.naacl-main.176</url>
      <doi>10.18653/v1/2021.naacl-main.176</doi>
      <bibkey>sawhney-etal-2021-suicide</bibkey>
      <video href="2021.naacl-main.176.mp4"/>
    </paper>
    <paper id="177">
      <title><fixed-case>W</fixed-case>iki<fixed-case>T</fixed-case>alk<fixed-case>E</fixed-case>dit: A Dataset for modeling Editors’ behaviors on <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Kokil</first><last>Jaidka</last></author>
      <author><first>Andrea</first><last>Ceolin</last></author>
      <author><first>Iknoor</first><last>Singh</last></author>
      <author><first>Niyati</first><last>Chhaya</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>2191–2200</pages>
      <abstract>This study introduces and analyzes WikiTalkEdit, a dataset of conversations and edit histories from Wikipedia, for research in online cooperation and conversation modeling. The dataset comprises dialog triplets from the Wikipedia Talk pages, and editing actions on the corresponding articles being discussed. We show how the data supports the classic understanding of style matching, where positive emotion and the use of first-person pronouns predict a positive emotional change in a Wikipedia contributor. However, they do not predict editorial behavior. On the other hand, feedback invoking evidentiality and criticism, and references to Wikipedia’s community norms, is more likely to persuade the contributor to perform edits but is less likely to lead to a positive emotion. We developed baseline classifiers trained on pre-trained RoBERTa features that can predict editorial change with an F1 score of .54, as compared to an F1 score of .66 for predicting emotional change. A diagnostic analysis of persisting errors is also provided. We conclude with possible applications and recommendations for future work. The dataset is publicly available for the research community at https://github.com/kj2013/WikiTalkEdit/.</abstract>
      <url hash="25ba3e2d">2021.naacl-main.177</url>
      <attachment type="OptionalSupplementaryCode" hash="ef852469">2021.naacl-main.177.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.177</doi>
      <bibkey>jaidka-etal-2021-wikitalkedit</bibkey>
      <video href="2021.naacl-main.177.mp4"/>
    </paper>
    <paper id="178">
      <title>The structure of online social networks modulates the rate of lexical change</title>
      <author><first>Jian</first><last>Zhu</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>2201–2218</pages>
      <abstract>New words are regularly introduced to communities, yet not all of these words persist in a community’s lexicon. Among the many factors contributing to lexical change, we focus on the understudied effect of social networks. We conduct a large-scale analysis of over 80k neologisms in 4420 online communities across a decade. Using Poisson regression and survival analysis, our study demonstrates that the community’s network structure plays a significant role in lexical change. Apart from overall size, properties including dense connections, the lack of local clusters, and more external contacts promote lexical innovation and retention. Unlike offline communities, these topic-based communities do not experience strong lexical leveling despite increased contact but accommodate more niche words. Our work provides support for the sociolinguistic hypothesis that lexical change is partially shaped by the structure of the underlying network but also uncovers findings specific to online communities.</abstract>
      <url hash="ff6bdd78">2021.naacl-main.178</url>
      <doi>10.18653/v1/2021.naacl-main.178</doi>
      <bibkey>zhu-jurgens-2021-structure</bibkey>
      <video href="2021.naacl-main.178.mp4"/>
      <pwccode url="https://github.com/lingjzhu/reddit_network" additional="false">lingjzhu/reddit_network</pwccode>
    </paper>
    <paper id="179">
      <title>Modeling Framing in Immigration Discourse on Social Media</title>
      <author><first>Julia</first><last>Mendelsohn</last></author>
      <author><first>Ceren</first><last>Budak</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>2219–2263</pages>
      <abstract>The framing of political issues can influence policy and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on social media frame political issues. By creating a new dataset of immigration-related tweets labeled for multiple framing typologies from political communication theory, we develop supervised models to detect frames. We demonstrate how users’ ideology and region impact framing choices, and how a message’s framing influences audience responses. We find that the more commonly-used issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, frames oriented towards human interests, culture, and politics are associated with higher user engagement. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research.</abstract>
      <url hash="3b8307d5">2021.naacl-main.179</url>
      <doi>10.18653/v1/2021.naacl-main.179</doi>
      <bibkey>mendelsohn-etal-2021-modeling</bibkey>
      <video href="2021.naacl-main.179.mp4"/>
      <pwccode url="https://github.com/juliamendelsohn/framing" additional="false">juliamendelsohn/framing</pwccode>
    </paper>
    <paper id="180">
      <title>Modeling the Severity of Complaints in Social Media</title>
      <author><first>Mali</first><last>Jin</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>2264–2274</pages>
      <abstract>The speech act of complaining is used by humans to communicate a negative mismatch between reality and expectations as a reaction to an unfavorable situation. Linguistic theory of pragmatics categorizes complaints into various severity levels based on the face-threat that the complainer is willing to undertake. This is particularly useful for understanding the intent of complainers and how humans develop suitable apology strategies. In this paper, we study the severity level of complaints for the first time in computational linguistics. To facilitate this, we enrich a publicly available data set of complaints with four severity categories and train different transformer-based networks combined with linguistic information achieving 55.7 macro F1. We also jointly model binary complaint classification and complaint severity in a multi-task setting achieving new state-of-the-art results on binary complaint detection reaching up to 88.2 macro F1. Finally, we present a qualitative analysis of the behavior of our models in predicting complaint severity levels.</abstract>
      <url hash="bf3bbfeb">2021.naacl-main.180</url>
      <doi>10.18653/v1/2021.naacl-main.180</doi>
      <bibkey>jin-aletras-2021-modeling</bibkey>
      <video href="2021.naacl-main.180.mp4"/>
      <pwccode url="https://github.com/mali726/Complaint-Severity" additional="false">mali726/Complaint-Severity</pwccode>
    </paper>
    <paper id="181">
      <title>What About the Precedent: An Information-Theoretic Analysis of Common Law</title>
      <author><first>Josef</first><last>Valvoda</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Niklas</first><last>Stoehr</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>2275–2288</pages>
      <abstract>In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury’s, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart’s, who believes that what matters most is the precedent’s facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges’ arguments (i.e. the precedent cases). Taking an information-theoretic view, and modelling the question as a case out-come classification task, we find that the precedent’s arguments share 0.38 nats of information with the case’s outcome, whereas precedent’s facts only share 0.18 nats of information (i.e.,58% less); suggesting Halsbury’s view may be more accurate in this specific court. We found however in a qualitative analysis that there are specific statues where Goodhart’s view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward.</abstract>
      <url hash="8795a304">2021.naacl-main.181</url>
      <doi>10.18653/v1/2021.naacl-main.181</doi>
      <bibkey>valvoda-etal-2021-precedent</bibkey>
      <video href="2021.naacl-main.181.mp4"/>
      <pwccode url="https://github.com/valvoda/Precedent" additional="false">valvoda/Precedent</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/echr">ECHR</pwcdataset>
    </paper>
    <paper id="182">
      <title>Introducing <fixed-case>CAD</fixed-case>: the Contextual Abuse Dataset</title>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Dong</first><last>Nguyen</last></author>
      <author><first>Helen</first><last>Margetts</last></author>
      <author><first>Patricia</first><last>Rossini</last></author>
      <author><first>Rebekah</first><last>Tromble</last></author>
      <pages>2289–2303</pages>
      <abstract>Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets.We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.</abstract>
      <url hash="92a564b8">2021.naacl-main.182</url>
      <doi>10.18653/v1/2021.naacl-main.182</doi>
      <bibkey>vidgen-etal-2021-introducing</bibkey>
      <video href="2021.naacl-main.182.mp4"/>
      <pwccode url="https://github.com/dongpng/cad_naacl2021" additional="false">dongpng/cad_naacl2021</pwccode>
    </paper>
    <paper id="183">
      <title>Lifelong Learning of Hate Speech Classification on Social Media</title>
      <author><first>Jing</first><last>Qian</last></author>
      <author><first>Hong</first><last>Wang</last></author>
      <author><first>Mai</first><last>ElSherief</last></author>
      <author><first>Xifeng</first><last>Yan</last></author>
      <pages>2304–2314</pages>
      <abstract>Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre-defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the real-word application of hate speech classifiers in social media. In this work, we propose lifelong learning of hate speech classification on social media. To alleviate catastrophic forgetting, we propose to use Variational Representation Learning (VRL) along with a memory module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural Network). Experimentally, we show that combining variational representation learning and the LB-SOINN memory module achieves better performance than the commonly-used lifelong learning techniques.</abstract>
      <url hash="062e5b7c">2021.naacl-main.183</url>
      <attachment type="OptionalSupplementaryCode" hash="1042b336">2021.naacl-main.183.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.183</doi>
      <bibkey>qian-etal-2021-lifelong</bibkey>
      <video href="2021.naacl-main.183.mp4"/>
    </paper>
    <paper id="184">
      <title>Learning to Recognize Dialect Features</title>
      <author><first>Dorottya</first><last>Demszky</last></author>
      <author><first>Devyani</first><last>Sharma</last></author>
      <author><first>Jonathan</first><last>Clark</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <pages>2315–2338</pages>
      <abstract>Building NLP systems that serve everyone requires accounting for dialect differences. But dialects are not monolithic entities: rather, distinctions between and within dialects are captured by the presence, absence, and frequency of dozens of dialect features in speech and text, such as the deletion of the copula in “He ∅ running”. In this paper, we introduce the task of dialect feature detection, and present two multitask learning approaches, both based on pretrained transformers. For most dialects, large-scale annotated corpora for these features are unavailable, making it difficult to train recognizers. We train our models on a small number of minimal pairs, building on how linguists typically define dialect features. Evaluation on a test set of 22 dialect features of Indian English demonstrates that these models learn to recognize many features with high accuracy, and that a few minimal pairs can be as effective for training as thousands of labeled examples. We also demonstrate the downstream applicability of dialect feature detection both as a measure of dialect density and as a dialect classifier.</abstract>
      <url hash="3719b28f">2021.naacl-main.184</url>
      <doi>10.18653/v1/2021.naacl-main.184</doi>
      <bibkey>demszky-etal-2021-learning</bibkey>
      <video href="2021.naacl-main.184.mp4"/>
    </paper>
    <paper id="185">
      <title>It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</title>
      <author><first>Timo</first><last>Schick</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>2339–2352</pages>
      <abstract>When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.</abstract>
      <url hash="dbaf6c76">2021.naacl-main.185</url>
      <award>Outstanding Long Paper</award>
      <doi>10.18653/v1/2021.naacl-main.185</doi>
      <bibkey>schick-schutze-2021-just</bibkey>
      <video href="2021.naacl-main.185.mp4"/>
      <pwccode url="https://github.com/timoschick/pet" additional="true">timoschick/pet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewglue">FewGlue</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="186">
      <title>Static Embeddings as Efficient Knowledge Bases?</title>
      <author><first>Philipp</first><last>Dufter</last></author>
      <author><first>Nora</first><last>Kassner</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>2353–2363</pages>
      <abstract>Recent research investigates factual knowledge stored in large pretrained language models (PLMs). Instead of structural knowledge base (KB) queries, masked sentences such as “Paris is the capital of [MASK]” are used as probes. The good performance on this analysis task has been interpreted as PLMs becoming potential repositories of factual knowledge. In experiments across ten linguistically diverse languages, we study knowledge contained in static embeddings. We show that, when restricting the output space to a candidate set, simple nearest neighbor matching using static embeddings performs better than PLMs. E.g., static embeddings perform 1.6% points better than BERT while just using 0.3% of energy for training. One important factor in their good comparative performance is that static embeddings are standardly learned for a large vocabulary. In contrast, BERT exploits its more sophisticated, but expensive ability to compose meaningful representations from a much smaller subword vocabulary.</abstract>
      <url hash="ae75df19">2021.naacl-main.186</url>
      <doi>10.18653/v1/2021.naacl-main.186</doi>
      <bibkey>dufter-etal-2021-static</bibkey>
      <video href="2021.naacl-main.186.mp4"/>
      <pwccode url="https://github.com/pdufter/staticlama" additional="false">pdufter/staticlama</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="187">
      <title>Highly Efficient Knowledge Graph Embedding Learning with <fixed-case>O</fixed-case>rthogonal <fixed-case>P</fixed-case>rocrustes <fixed-case>A</fixed-case>nalysis</title>
      <author><first>Xutan</first><last>Peng</last></author>
      <author><first>Guanyi</first><last>Chen</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Mark</first><last>Stevenson</last></author>
      <pages>2364–2375</pages>
      <abstract>Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm.</abstract>
      <url hash="66ad8e14">2021.naacl-main.187</url>
      <doi>10.18653/v1/2021.naacl-main.187</doi>
      <bibkey>peng-etal-2021-highly</bibkey>
      <video href="2021.naacl-main.187.mp4"/>
      <pwccode url="https://github.com/Pzoom522/ProcrustEs-KGE" additional="false">Pzoom522/ProcrustEs-KGE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="188">
      <title>Rethinking Network Pruning – under the Pre-train and Fine-tune Paradigm</title>
      <author><first>Dongkuan</first><last>Xu</last></author>
      <author><first>Ian En-Hsu</first><last>Yen</last></author>
      <author><first>Jinxi</first><last>Zhao</last></author>
      <author><first>Zhibin</first><last>Xiao</last></author>
      <pages>2376–2382</pages>
      <abstract>Transformer-based pre-trained language models have significantly improved the performance of various natural language processing (NLP) tasks in the recent years. While effective and prevalent, these models are usually prohibitively large for resource-limited deployment scenarios. A thread of research has thus been working on applying network pruning techniques under the pretrain-then-finetune paradigm widely adopted in NLP. However, the existing pruning results on benchmark transformers, such as BERT, are not as remarkable as the pruning results in the literature of convolutional neural networks (CNNs). In particular, common wisdom in pruning CNN states that sparse pruning technique compresses a model more than that obtained by reducing number of channels and layers, while existing works on sparse pruning of BERT yields inferior results than its small-dense counterparts such as TinyBERT. In this work, we aim to fill this gap by studying how knowledge are transferred and lost during the pre-train, fine-tune, and pruning process, and proposing a knowledge-aware sparse pruning process that achieves significantly superior results than existing literature. We show for the first time that sparse pruning compresses a BERT model significantly more than reducing its number of channels and layers. Experiments on multiple data sets of GLUE benchmark show that our method outperforms the leading competitors with a 20-times weight/FLOPs compression and neglectable loss in prediction accuracy.</abstract>
      <url hash="a3b39a71">2021.naacl-main.188</url>
      <attachment type="OptionalSupplementaryCode" hash="e435a5d9">2021.naacl-main.188.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.188</doi>
      <bibkey>xu-etal-2021-rethinking</bibkey>
      <video href="2021.naacl-main.188.mp4"/>
      <pwccode url="https://github.com/derronxu/sparsebert" additional="false">derronxu/sparsebert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="189">
      <title>Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers</title>
      <author><first>Andrew</first><last>Silva</last></author>
      <author><first>Pradyumna</first><last>Tambwekar</last></author>
      <author><first>Matthew</first><last>Gombolay</last></author>
      <pages>2383–2389</pages>
      <abstract>The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.</abstract>
      <url hash="507cd665">2021.naacl-main.189</url>
      <doi>10.18653/v1/2021.naacl-main.189</doi>
      <bibkey>silva-etal-2021-towards</bibkey>
      <video href="2021.naacl-main.189.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
    </paper>
    <paper id="190">
      <title>Detoxifying Language Models Risks Marginalizing Minority Voices</title>
      <author><first>Albert</first><last>Xu</last></author>
      <author><first>Eshaan</first><last>Pathak</last></author>
      <author><first>Eric</first><last>Wallace</last></author>
      <author><first>Suchin</first><last>Gururangan</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>2390–2397</pages>
      <abstract>Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.</abstract>
      <url hash="b7430224">2021.naacl-main.190</url>
      <doi>10.18653/v1/2021.naacl-main.190</doi>
      <bibkey>xu-etal-2021-detoxifying</bibkey>
      <video href="2021.naacl-main.190.mp4"/>
      <pwccode url="https://github.com/albertkx/detoxifying-lms" additional="false">albertkx/detoxifying-lms</pwccode>
    </paper>
    <paper id="191">
      <title><fixed-case>HONEST</fixed-case>: Measuring Hurtful Sentence Completion in Language Models</title>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>2398–2406</pages>
      <abstract>Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that 4.3% of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these models replicate and amplify deep-seated societal stereotypes about gender roles. Sentence completions refer to sexual promiscuity when the target is female in 9% of the time, and in 4% to homosexuality when the target is male. The results raise questions about the use of these models in production settings.</abstract>
      <url hash="fc53b3e0">2021.naacl-main.191</url>
      <doi>10.18653/v1/2021.naacl-main.191</doi>
      <bibkey>nozza-etal-2021-honest</bibkey>
      <video href="2021.naacl-main.191.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/honest-en">HONEST</pwcdataset>
    </paper>
    <paper id="192">
      <title><fixed-case>E</fixed-case>a<fixed-case>S</fixed-case>e: A Diagnostic Tool for <fixed-case>VQA</fixed-case> based on Answer Diversity</title>
      <author><first>Shailza</first><last>Jolly</last></author>
      <author><first>Sandro</first><last>Pezzelle</last></author>
      <author><first>Moin</first><last>Nabi</last></author>
      <pages>2407–2414</pages>
      <abstract>We propose EASE, a simple diagnostic tool for Visual Question Answering (VQA) which quantifies the difficulty of an image, question sample. EASE is based on the pattern of answers provided by multiple annotators to a given question. In particular, it considers two aspects of the answers: (i) their Entropy; (ii) their Semantic content. First, we prove the validity of our diagnostic to identify samples that are easy/hard for state-of-art VQA models. Second, we show that EASE can be successfully used to select the most-informative samples for training/fine-tuning. Crucially, only information that is readily available in any VQA dataset is used to compute its scores.</abstract>
      <url hash="0d008efd">2021.naacl-main.192</url>
      <attachment type="OptionalSupplementaryData" hash="7fe1c15c">2021.naacl-main.192.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.192</doi>
      <bibkey>jolly-etal-2021-ease</bibkey>
      <video href="2021.naacl-main.192.mp4"/>
      <pwccode url="https://github.com/shailzajolly/ease" additional="false">shailzajolly/ease</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="193">
      <title><fixed-case>D</fixed-case>e<fixed-case>CEMBERT</fixed-case>: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization</title>
      <author><first>Zineng</first><last>Tang</last></author>
      <author><first>Jie</first><last>Lei</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>2415–2426</pages>
      <abstract>Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many video-and-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved video-and-language pre-training method that first adds automatically-extracted dense region captions from the video frames as auxiliary text input, to provide informative visual cues for learning better video and language associations. Second, to alleviate the temporal misalignment issue, our method incorporates an entropy minimization-based constrained attention loss, to encourage the model to automatically focus on the correct caption from a pool of candidate ASR captions. Our overall approach is named DeCEMBERT (Dense Captions and Entropy Minimization). Comprehensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate that our approach outperforms previous state-of-the-art methods. Ablation studies on pre-training and downstream tasks show that adding dense captions and constrained attention loss help improve the model performance. Lastly, we also provide attention visualization to show the effect of applying the proposed constrained attention loss.</abstract>
      <url hash="1473810e">2021.naacl-main.193</url>
      <doi>10.18653/v1/2021.naacl-main.193</doi>
      <bibkey>tang-etal-2021-decembert</bibkey>
      <video href="2021.naacl-main.193.mp4"/>
      <pwccode url="https://github.com/zinengtang/decembert" additional="false">zinengtang/decembert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/howto100m">HowTo100M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/youcook2">YouCook2</pwcdataset>
    </paper>
    <paper id="194">
      <title>Improving Generation and Evaluation of Visual Stories via Semantic Consistency</title>
      <author><first>Adyasha</first><last>Maharana</last></author>
      <author><first>Darryl</first><last>Hannan</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>2427–2442</pages>
      <abstract>Story visualization is an underexplored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, (2) a copy-transform mechanism for sequentially-consistent story visualization, and (3) MART-based transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not accurately reflect performance. Therefore, we also provide an exploration of evaluation metrics for the model, focused on aspects of the generated frames such as the presence/quality of generated characters, the relevance to captions, and the diversity of the generated images. We also present correlation experiments of our proposed automated metrics with human evaluations.</abstract>
      <url hash="61107827">2021.naacl-main.194</url>
      <doi>10.18653/v1/2021.naacl-main.194</doi>
      <bibkey>maharana-etal-2021-improving</bibkey>
      <video href="2021.naacl-main.194.mp4"/>
      <pwccode url="https://github.com/adymaharana/StoryViz" additional="false">adymaharana/StoryViz</pwccode>
    </paper>
    <paper id="195">
      <title>Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models</title>
      <author><first>Po-Yao</first><last>Huang</last></author>
      <author><first>Mandela</first><last>Patrick</last></author>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Alexander</first><last>Hauptmann</last></author>
      <pages>2443–2459</pages>
      <abstract>This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/Multi-HT100M.</abstract>
      <url hash="52e407d9">2021.naacl-main.195</url>
      <attachment type="OptionalSupplementaryData" hash="0546d73d">2021.naacl-main.195.OptionalSupplementaryData.txt</attachment>
      <doi>10.18653/v1/2021.naacl-main.195</doi>
      <bibkey>huang-etal-2021-multilingual</bibkey>
      <video href="2021.naacl-main.195.mp4"/>
      <pwccode url="https://github.com/berniebear/Multi-HT100M" additional="false">berniebear/Multi-HT100M</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/howto100m">HowTo100M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vatex">VATEX</pwcdataset>
    </paper>
    <paper id="196">
      <title>Video Question Answering with Phrases via Semantic Roles</title>
      <author><first>Arka</first><last>Sadhu</last></author>
      <author><first>Kan</first><last>Chen</last></author>
      <author><first>Ram</first><last>Nevatia</last></author>
      <pages>2460–2478</pages>
      <abstract>Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models’ application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public.</abstract>
      <url hash="41bb2b6a">2021.naacl-main.196</url>
      <doi>10.18653/v1/2021.naacl-main.196</doi>
      <bibkey>sadhu-etal-2021-video</bibkey>
      <video href="2021.naacl-main.196.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-qa">ActivityNet-QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades">Charades</pwcdataset>
    </paper>
    <paper id="197">
      <title>From Masked Language Modeling to Translation: Non-<fixed-case>E</fixed-case>nglish Auxiliary Tasks Improve Zero-shot Spoken Language Understanding</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Ibrahim</first><last>Sharaf</last></author>
      <author><first>Aizhan</first><last>Imankulova</last></author>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Marija</first><last>Stepanović</last></author>
      <author><first>Alan</first><last>Ramponi</last></author>
      <author><first>Siti Oryza</first><last>Khairunnisa</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>2479–2497</pages>
      <abstract>The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.</abstract>
      <url hash="e4433f83">2021.naacl-main.197</url>
      <doi>10.18653/v1/2021.naacl-main.197</doi>
      <bibkey>van-der-goot-etal-2021-masked</bibkey>
      <video href="2021.naacl-main.197.mp4"/>
      <pwccode url="https://github.com/Kaleidophon/deep-significance" additional="true">Kaleidophon/deep-significance</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xsid">xSID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="198">
      <title><fixed-case>WEC</fixed-case>: Deriving a Large-scale Cross-document Event Coreference dataset from <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Alon</first><last>Eirew</last></author>
      <author><first>Arie</first><last>Cattan</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>2498–2510</pages>
      <abstract>Cross-document event coreference resolution is a foundational task for NLP applications involving multi-text processing. However, existing corpora for this task are scarce and relatively small, while annotating only modest-size clusters of documents belonging to the same topic. To complement these resources and enhance future research, we present Wikipedia Event Coreference (WEC), an efficient methodology for gathering a large-scale dataset for cross-document event coreference from Wikipedia, where coreference links are not restricted within predefined topics. We apply this methodology to the English Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our dataset creation method is generic and can be applied with relatively little effort to other Wikipedia languages. To set baseline results, we develop an algorithm that adapts components of state-of-the-art models for within-document coreference resolution to the cross-document setting. Our model is suitably efficient and outperforms previously published state-of-the-art results for the task.</abstract>
      <url hash="debe504c">2021.naacl-main.198</url>
      <doi>10.18653/v1/2021.naacl-main.198</doi>
      <bibkey>eirew-etal-2021-wec</bibkey>
      <video href="2021.naacl-main.198.mp4"/>
      <pwccode url="https://github.com/AlonEirew/extract-wec" additional="true">AlonEirew/extract-wec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wec-eng">WEC-Eng</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="199">
      <title>Challenging distributional models with a conceptual network of philosophical terms</title>
      <author><first>Yvette</first><last>Oortwijn</last></author>
      <author><first>Jelke</first><last>Bloem</last></author>
      <author><first>Pia</first><last>Sommerauer</last></author>
      <author><first>Francois</first><last>Meyer</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <pages>2511–2522</pages>
      <abstract>Computational linguistic research on language change through distributional semantic (DS) models has inspired researchers from fields such as philosophy and literary studies, who use these methods for the exploration and comparison of comparatively small datasets traditionally analyzed by close reading. Research on methods for small data is still in early stages and it is not clear which methods achieve the best results. We investigate the possibilities and limitations of using distributional semantic models for analyzing philosophical data by means of a realistic use-case. We provide a ground truth for evaluation created by philosophy experts and a blueprint for using DS models in a sound methodological setup. We compare three methods for creating specialized models from small datasets. Though the models do not perform well enough to directly support philosophers yet, we find that models designed for small data yield promising directions for future work.</abstract>
      <url hash="ad6a5098">2021.naacl-main.199</url>
      <doi>10.18653/v1/2021.naacl-main.199</doi>
      <bibkey>oortwijn-etal-2021-challenging</bibkey>
      <video href="2021.naacl-main.199.mp4"/>
      <pwccode url="https://github.com/yoortwijn/challenging_dms" additional="false">yoortwijn/challenging_dms</pwccode>
    </paper>
    <paper id="200">
      <title><fixed-case>KILT</fixed-case>: a Benchmark for Knowledge Intensive Language Tasks</title>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Aleksandra</first><last>Piktus</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <author><first>Majid</first><last>Yazdani</last></author>
      <author><first>Nicola</first><last>De Cao</last></author>
      <author><first>James</first><last>Thorne</last></author>
      <author><first>Yacine</first><last>Jernite</last></author>
      <author><first>Vladimir</first><last>Karpukhin</last></author>
      <author><first>Jean</first><last>Maillard</last></author>
      <author><first>Vassilis</first><last>Plachouras</last></author>
      <author><first>Tim</first><last>Rocktäschel</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>2523–2544</pages>
      <abstract>Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/facebookresearch/KILT.</abstract>
      <url hash="21842ec2">2021.naacl-main.200</url>
      <doi>10.18653/v1/2021.naacl-main.200</doi>
      <bibkey>petroni-etal-2021-kilt</bibkey>
      <video href="2021.naacl-main.200.mp4"/>
      <pwccode url="https://github.com/facebookresearch/KILT" additional="true">facebookresearch/KILT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="201">
      <title>A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios</title>
      <author><first>Michael A.</first><last>Hedderich</last></author>
      <author><first>Lukas</first><last>Lange</last></author>
      <author><first>Heike</first><last>Adel</last></author>
      <author><first>Jannik</first><last>Strötgen</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>2545–2568</pages>
      <abstract>Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.</abstract>
      <url hash="3ef1f7f3">2021.naacl-main.201</url>
      <doi>10.18653/v1/2021.naacl-main.201</doi>
      <bibkey>hedderich-etal-2021-survey</bibkey>
      <video href="2021.naacl-main.201.mp4"/>
    </paper>
    <paper id="202">
      <title>Temporal Knowledge Graph Completion using a Linear Temporal Regularizer and Multivector Embeddings</title>
      <author><first>Chengjin</first><last>Xu</last></author>
      <author><first>Yung-Yu</first><last>Chen</last></author>
      <author><first>Mojtaba</first><last>Nayyeri</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <pages>2569–2578</pages>
      <abstract>Representation learning approaches for knowledge graphs have been mostly designed for static data. However, many knowledge graphs involve evolving data, e.g., the fact (The President of the United States is Barack Obama) is valid only from 2009 to 2017. This introduces important challenges for knowledge representation learning since the knowledge graphs change over time. In this paper, we present a novel time-aware knowledge graph embebdding approach, TeLM, which performs 4th-order tensor factorization of a Temporal knowledge graph using a Linear temporal regularizer and Multivector embeddings. Moreover, we investigate the effect of the temporal dataset’s time granularity on temporal knowledge graph completion. Experimental results demonstrate that our proposed models trained with the linear temporal regularizer achieve the state-of-the-art performances on link prediction over four well-established temporal knowledge graph completion benchmarks.</abstract>
      <url hash="3e632179">2021.naacl-main.202</url>
      <doi>10.18653/v1/2021.naacl-main.202</doi>
      <bibkey>xu-etal-2021-temporal</bibkey>
      <video href="2021.naacl-main.202.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/icews">ICEWS</pwcdataset>
    </paper>
    <paper id="203">
      <title><fixed-case>UDALM</fixed-case>: Unsupervised Domain Adaptation through Language Modeling</title>
      <author><first>Constantinos</first><last>Karouzos</last></author>
      <author><first>Georgios</first><last>Paraskevopoulos</last></author>
      <author><first>Alexandros</first><last>Potamianos</last></author>
      <pages>2579–2590</pages>
      <abstract>In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our method is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding 91.74% accuracy, which is an 1.11% absolute improvement over the state-of-the-art.</abstract>
      <url hash="056ceec8">2021.naacl-main.203</url>
      <doi>10.18653/v1/2021.naacl-main.203</doi>
      <bibkey>karouzos-etal-2021-udalm</bibkey>
      <video href="2021.naacl-main.203.mp4"/>
      <pwccode url="https://github.com/ckarouzos/slp_daptmlm" additional="false">ckarouzos/slp_daptmlm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-domain-sentiment-dataset-v2-0">Multi-Domain Sentiment Dataset v2.0</pwcdataset>
    </paper>
    <paper id="204">
      <title>Beyond Black &amp; White: Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning</title>
      <author><first>Tommaso</first><last>Fornaciari</last></author>
      <author><first>Alexandra</first><last>Uma</last></author>
      <author><first>Silviu</first><last>Paun</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>2591–2597</pages>
      <abstract>Supervised learning assumes that a ground truth label exists. However, the reliability of this ground truth depends on human annotators, who often disagree. Prior work has shown that this disagreement can be helpful in training models. We propose a novel method to incorporate this disagreement as information: in addition to the standard error computation, we use soft-labels (i.e., probability distributions over the annotator labels) as an auxiliary task in a multi-task neural network. We measure the divergence between the predictions and the target soft-labels with several loss-functions and evaluate the models on various NLP tasks. We find that the soft-label prediction auxiliary task reduces the penalty for errors on ambiguous entities, and thereby mitigates overfitting. It significantly improves performance across tasks, beyond the standard approach and prior work.</abstract>
      <url hash="0b8200db">2021.naacl-main.204</url>
      <doi>10.18653/v1/2021.naacl-main.204</doi>
      <bibkey>fornaciari-etal-2021-beyond</bibkey>
      <video href="2021.naacl-main.204.mp4"/>
    </paper>
    <paper id="205">
      <title>Clustering-based Inference for Biomedical Entity Linking</title>
      <author><first>Rico</first><last>Angell</last></author>
      <author><first>Nicholas</first><last>Monath</last></author>
      <author><first>Sunil</first><last>Mohan</last></author>
      <author><first>Nishant</first><last>Yadav</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>2598–2608</pages>
      <abstract>Due to large number of entities in biomedical knowledge bases, only a small fraction of entities have corresponding labelled training data. This necessitates entity linking models which are able to link mentions of unseen entities using learned representations of entities. Previous approaches link each mention independently, ignoring the relationships within and across documents between the entity mentions. These relations can be very useful for linking mentions in biomedical text where linking decisions are often difficult due mentions having a generic or a highly specialized form. In this paper, we introduce a model in which linking decisions can be made not merely by linking to a knowledge base entity but also by grouping multiple mentions together via clustering and jointly making linking predictions. In experiments on the largest publicly available biomedical dataset, we improve the best independent prediction for entity linking by 3.0 points of accuracy, and our clustering-based inference model further improves entity linking by 2.3 points.</abstract>
      <url hash="c5862774">2021.naacl-main.205</url>
      <doi>10.18653/v1/2021.naacl-main.205</doi>
      <bibkey>angell-etal-2021-clustering</bibkey>
      <video href="2021.naacl-main.205.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
    </paper>
    <paper id="206">
      <title>Variance-reduced First-order Meta-learning for Natural Language Processing Tasks</title>
      <author><first>Lingxiao</first><last>Wang</last></author>
      <author><first>Kevin</first><last>Huang</last></author>
      <author><first>Tengyu</first><last>Ma</last></author>
      <author><first>Quanquan</first><last>Gu</last></author>
      <author><first>Jing</first><last>Huang</last></author>
      <pages>2609–2615</pages>
      <abstract>First-order meta-learning algorithms have been widely used in practice to learn initial model parameters that can be quickly adapted to new tasks due to their efficiency and effectiveness. However, existing studies find that meta-learner can overfit to some specific adaptation when we have heterogeneous tasks, leading to significantly degraded performance. In Natural Language Processing (NLP) applications, datasets are often diverse and each task has its unique characteristics. Therefore, to address the overfitting issue when applying first-order meta-learning to NLP applications, we propose to reduce the variance of the gradient estimator used in task adaptation. To this end, we develop a variance-reduced first-order meta-learning algorithm. The core of our algorithm is to introduce a novel variance reduction term to the gradient estimation when performing the task adaptation. Experiments on two NLP applications: few-shot text classification and multi-domain dialog state tracking demonstrate the superior performance of our proposed method.</abstract>
      <url hash="93468fde">2021.naacl-main.206</url>
      <doi>10.18653/v1/2021.naacl-main.206</doi>
      <bibkey>wang-etal-2021-variance</bibkey>
      <video href="2021.naacl-main.206.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="207">
      <title>Diversity-Aware Batch Active Learning for Dependency Parsing</title>
      <author><first>Tianze</first><last>Shi</last></author>
      <author><first>Adrian</first><last>Benton</last></author>
      <author><first>Igor</first><last>Malioutov</last></author>
      <author><first>Ozan</first><last>İrsoy</last></author>
      <pages>2616–2626</pages>
      <abstract>While the predictive performance of modern statistical dependency parsers relies heavily on the availability of expensive expert-annotated treebank data, not all annotations contribute equally to the training of the parsers. In this paper, we attempt to reduce the number of labeled examples needed to train a strong dependency parser using batch active learning (AL). In particular, we investigate whether enforcing diversity in the sampled batches, using determinantal point processes (DPPs), can improve over their diversity-agnostic counterparts. Simulation experiments on an English newswire corpus show that selecting diverse batches with DPPs is superior to strong selection strategies that do not enforce batch diversity, especially during the initial stages of the learning process. Additionally, our diversity-aware strategy is robust under a corpus duplication setting, where diversity-agnostic sampling strategies exhibit significant degradation.</abstract>
      <url hash="b34ca299">2021.naacl-main.207</url>
      <doi>10.18653/v1/2021.naacl-main.207</doi>
      <bibkey>shi-etal-2021-diversity</bibkey>
      <video href="2021.naacl-main.207.mp4"/>
      <pwccode url="https://github.com/tzshi/dpp-al-parsing-naacl21" additional="false">tzshi/dpp-al-parsing-naacl21</pwccode>
    </paper>
    <paper id="208">
      <title>How many data points is a prompt worth?</title>
      <author><first>Teven</first><last>Le Scao</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>2627–2636</pages>
      <abstract>When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.</abstract>
      <url hash="a721c88e">2021.naacl-main.208</url>
      <attachment type="OptionalSupplementaryCode" hash="9c811ed9">2021.naacl-main.208.OptionalSupplementaryCode.zip</attachment>
      <award>Outstanding Short Paper</award>
      <doi>10.18653/v1/2021.naacl-main.208</doi>
      <bibkey>le-scao-rush-2021-many</bibkey>
      <video href="2021.naacl-main.208.mp4"/>
      <pwccode url="https://github.com/TevenLeScao/pet" additional="false">TevenLeScao/pet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="209">
      <title>Can Latent Alignments Improve Autoregressive Machine Translation?</title>
      <author><first>Adi</first><last>Haviv</last></author>
      <author><first>Lior</first><last>Vassertail</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>2637–2641</pages>
      <abstract>Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve autoregressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing.</abstract>
      <url hash="b7b4ab4d">2021.naacl-main.209</url>
      <doi>10.18653/v1/2021.naacl-main.209</doi>
      <bibkey>haviv-etal-2021-latent</bibkey>
      <video href="2021.naacl-main.209.mp4"/>
    </paper>
    <paper id="210">
      <title>Smoothing and Shrinking the Sparse <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Search Space</title>
      <author><first>Ben</first><last>Peters</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>2642–2654</pages>
      <abstract>Current sequence-to-sequence models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias: models give high scores to short, inadequate hypotheses and often make the empty string the argmax—the so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for neural machine translation. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both cross-entropy and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and machine translation for 7 language pairs.</abstract>
      <url hash="58efe875">2021.naacl-main.210</url>
      <doi>10.18653/v1/2021.naacl-main.210</doi>
      <bibkey>peters-martins-2021-smoothing</bibkey>
      <video href="2021.naacl-main.210.mp4"/>
      <pwccode url="https://github.com/deep-spin/S7" additional="false">deep-spin/S7</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="211">
      <title>Unified Pre-training for Program Understanding and Generation</title>
      <author><first>Wasi</first><last>Ahmad</last></author>
      <author><first>Saikat</first><last>Chakraborty</last></author>
      <author><first>Baishakhi</first><last>Ray</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>2655–2668</pages>
      <abstract>Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART’s effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., “if“ block inside an “else“ block is equivalent to “else if“ block) that are crucial to program semantics and thus excels even with limited annotations.</abstract>
      <url hash="309df1bd">2021.naacl-main.211</url>
      <doi>10.18653/v1/2021.naacl-main.211</doi>
      <bibkey>ahmad-etal-2021-unified</bibkey>
      <video href="2021.naacl-main.211.mp4"/>
      <pwccode url="https://github.com/wasiahmad/PLBART" additional="false">wasiahmad/PLBART</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/concode">CONCODE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
    </paper>
    <paper id="212">
      <title>Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding</title>
      <author><first>Ting</first><last>Hua</last></author>
      <author><first>Yilin</first><last>Shen</last></author>
      <author><first>Changsheng</first><last>Zhao</last></author>
      <author><first>Yen-Chang</first><last>Hsu</last></author>
      <author><first>Hongxia</first><last>Jin</last></author>
      <pages>2669–2678</pages>
      <abstract>Domain classification is the fundamental task in natural language understanding (NLU), which often requires fast accommodation to new emerging domains. This constraint makes it impossible to retrain all previous domains, even if they are accessible to the new model. Most existing continual learning approaches suffer from low accuracy and performance fluctuation, especially when the distributions of old and new data are significantly different. In fact, the key real-world problem is not the absence of old data, but the inefficiency to retrain the model with the whole old dataset. Is it potential to utilize some old data to yield high accuracy and maintain stable performance, while at the same time, without introducing extra hyperparameters? In this paper, we proposed a hyperparameter-free continual learning model for text data that can stably produce high performance under various environments. Specifically, we utilize Fisher information to select exemplars that can “record” key information of the original model. Also, a novel scheme called dynamical weight consolidation is proposed to enable hyperparameter-free learning during the retrain process. Extensive experiments demonstrate baselines provide fluctuated performance which makes them useless in practice. On the contrary, our proposed model significantly and consistently outperforms the best state-of-the-art method by up to 20% in average accuracy, and each of its component contributes effectively to overall performance.</abstract>
      <url hash="cb0a6a08">2021.naacl-main.212</url>
      <doi>10.18653/v1/2021.naacl-main.212</doi>
      <bibkey>hua-etal-2021-hyperparameter</bibkey>
      <video href="2021.naacl-main.212.mp4"/>
      <pwccode url="https://github.com/tinghua-code/ccfi" additional="false">tinghua-code/ccfi</pwccode>
    </paper>
    <paper id="213">
      <title>On the Embeddings of Variables in Recurrent Neural Networks for Source Code</title>
      <author><first>Nadezhda</first><last>Chirkova</last></author>
      <pages>2679–2689</pages>
      <abstract>Source code processing heavily relies on the methods widely used in natural language processing (NLP), but involves specifics that need to be taken into account to achieve higher quality. An example of this specificity is that the semantics of a variable is defined not only by its name but also by the contexts in which the variable occurs. In this work, we develop dynamic embeddings, a recurrent mechanism that adjusts the learned semantics of the variable when it obtains more information about the variable’s role in the program. We show that using the proposed dynamic embeddings significantly improves the performance of the recurrent neural network, in code completion and bug fixing tasks.</abstract>
      <url hash="de27cd4f">2021.naacl-main.213</url>
      <doi>10.18653/v1/2021.naacl-main.213</doi>
      <bibkey>chirkova-2021-embeddings</bibkey>
      <video href="2021.naacl-main.213.mp4"/>
    </paper>
    <paper id="214">
      <title>Cross-Lingual Word Embedding Refinement by <tex-math>\ell_{1}</tex-math> Norm Optimisation</title>
      <author><first>Xutan</first><last>Peng</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Mark</first><last>Stevenson</last></author>
      <pages>2690–2701</pages>
      <abstract>Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the ℓ2 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. ℓ1 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the ℓ1 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this strategy be adopted as a standard for CLWE methods.</abstract>
      <url hash="c63759b9">2021.naacl-main.214</url>
      <doi>10.18653/v1/2021.naacl-main.214</doi>
      <bibkey>peng-etal-2021-cross</bibkey>
      <video href="2021.naacl-main.214.mp4"/>
      <pwccode url="https://github.com/Pzoom522/L1-Refinement" additional="false">Pzoom522/L1-Refinement</pwccode>
    </paper>
    <paper id="215">
      <title>Semantic Frame Forecast</title>
      <author><first>Chieh-Yang</first><last>Huang</last></author>
      <author><first>Ting-Hao</first><last>Huang</last></author>
      <pages>2702–2713</pages>
      <abstract>This paper introduces Semantic Frame Forecast, a task that predicts the semantic frames that will occur in the next 10, 100, or even 1,000 sentences in a running story. Prior work focused on predicting the immediate future of a story, such as one to a few sentences ahead. However, when novelists write long stories, generating a few sentences is not enough to help them gain high-level insight to develop the follow-up story. In this paper, we formulate a long story as a sequence of “story blocks,” where each block contains a fixed number of sentences (e.g., 10, 100, or 200). This formulation allows us to predict the follow-up story arc beyond the scope of a few sentences. We represent a story block using the term frequencies (TF) of semantic frames in it, normalized by each frame’s inverse document frequency (IDF). We conduct semantic frame forecast experiments on 4,794 books from the Bookcorpus and 7,962 scientific abstracts from CODA-19, with block sizes ranging from 5 to 1,000 sentences. The results show that automated models can forecast the follow-up story blocks better than the random, prior, and replay baselines, indicating the feasibility of the task. We also learn that the models using the frame representation as features outperform all the existing approaches when the block size is over 150 sentences. The human evaluation also shows that the proposed frame representation, when visualized as word clouds, is comprehensible, representative, and specific to humans.</abstract>
      <url hash="41d9ea57">2021.naacl-main.215</url>
      <doi>10.18653/v1/2021.naacl-main.215</doi>
      <bibkey>huang-huang-2021-semantic</bibkey>
      <video href="2021.naacl-main.215.mp4"/>
      <pwccode url="https://github.com/appleternity/FrameForecasting" additional="false">appleternity/FrameForecasting</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coda-19">CODA-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="216">
      <title><fixed-case>MUSER</fixed-case>: <fixed-case>MU</fixed-case>ltimodal Stress detection using Emotion Recognition as an Auxiliary Task</title>
      <author><first>Yiqun</first><last>Yao</last></author>
      <author><first>Michalis</first><last>Papakostas</last></author>
      <author><first>Mihai</first><last>Burzo</last></author>
      <author><first>Mohamed</first><last>Abouelenien</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>2714–2725</pages>
      <abstract>The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and human-computer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER – a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluation on the Multimodal Stressed Emotion (MuSE) dataset shows that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-of-the-art results.</abstract>
      <url hash="dcde72e8">2021.naacl-main.216</url>
      <doi>10.18653/v1/2021.naacl-main.216</doi>
      <bibkey>yao-etal-2021-muser</bibkey>
      <video href="2021.naacl-main.216.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/omg-emotion">OMG-Emotion</pwcdataset>
    </paper>
    <paper id="217">
      <title>Learning to Decompose and Organize Complex Tasks</title>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Sujay Kumar</first><last>Jauhar</last></author>
      <author><first>Julia</first><last>Kiseleva</last></author>
      <author><first>Ryen</first><last>White</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>2726–2735</pages>
      <abstract>People rely on digital task management tools, such as email or to-do apps, to manage their tasks. Some of these tasks are large and complex, leading to action paralysis and feelings of being overwhelmed on the part of the user. The micro-productivity literature has shown that such tasks could benefit from being decomposed and organized, in order to reduce user cognitive load. Thus in this paper, we propose a novel end-to-end pipeline that consumes a complex task and induces a dependency graph from unstructured text to represent sub-tasks and their relationships. Our solution first finds nodes for sub-tasks from multiple ‘how-to’ articles on the web by injecting a neural text generator with three key desiderata – relevance, abstraction, and consensus. Then we resolve and infer edges between these subtask nodes by learning task dependency relations. We collect a new dataset of complex tasks with their sub-task graph to develop and evaluate our solutions. Both components of our graph induction solution are evaluated in experiments, demonstrating that our models outperform a state-of-the-art text generator significantly. Our generalizable and scalable end-to-end solution has important implications for boosting user productivity and assisting with digital task management.</abstract>
      <url hash="47cfd7dd">2021.naacl-main.217</url>
      <doi>10.18653/v1/2021.naacl-main.217</doi>
      <bibkey>zhang-etal-2021-learning</bibkey>
      <video href="2021.naacl-main.217.mp4"/>
      <pwccode url="https://github.com/microsoft/mscomplextasks" additional="false">microsoft/mscomplextasks</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="218">
      <title>Continual Learning for Text Classification with Information Disentanglement Based Regularization</title>
      <author><first>Yufan</first><last>Huang</last></author>
      <author><first>Yanzhe</first><last>Zhang</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Xuezhi</first><last>Wang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>2736–2746</pages>
      <abstract>Continual learning has become increasingly important as it enables NLP models to constantly learn and gain knowledge over time. Previous continual learning methods are mainly designed to preserve knowledge from previous tasks, without much emphasis on how to well generalize models to new tasks. In this work, we propose an information disentanglement based regularization method for continual learning on text classification. Our proposed method first disentangles text hidden spaces into representations that are generic to all tasks and representations specific to each individual task, and further regularizes these representations differently to better constrain the knowledge required to generalize. We also introduce two simple auxiliary tasks: next sentence prediction and task-id prediction, for learning better generic and specific representation spaces. Experiments conducted on large-scale benchmarks demonstrate the effectiveness of our method in continual text classification tasks with various sequences and lengths over state-of-the-art baselines. We have publicly released our code at https://github.com/GT-SALT/IDBR.</abstract>
      <url hash="c5ea54f3">2021.naacl-main.218</url>
      <doi>10.18653/v1/2021.naacl-main.218</doi>
      <bibkey>huang-etal-2021-continual</bibkey>
      <video href="2021.naacl-main.218.mp4"/>
      <pwccode url="https://github.com/GT-SALT/IDBR" additional="false">GT-SALT/IDBR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="219">
      <title>Learning from Executions for Semantic Parsing</title>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>2747–2759</pages>
      <abstract>Semantic parsing aims at translating natural language (NL) utterances onto machine-interpretable programs, which can be executed against a real-world environment. The expensive annotation of utterance-program pairs has long been acknowledged as a major bottleneck for the deployment of contemporary neural models to real-life applications. In this work, we focus on the task of semi-supervised learning where a limited amount of annotated data is available together with many unlabeled NL utterances. Based on the observation that programs which correspond to NL utterances should always be executable, we propose to encourage a parser to generate executable programs for unlabeled utterances. Due to the large search space of executable programs, conventional methods that use beam-search for approximation, such as self-training and top-k marginal likelihood training, do not perform as well. Instead, we propose a set of new training objectives that are derived by approaching the problem of learning from executions from the posterior regularization perspective. Our new objectives outperform conventional methods on Overnight and GeoQuery, bridging the gap between semi-supervised and supervised learning.</abstract>
      <url hash="f2a007cc">2021.naacl-main.219</url>
      <doi>10.18653/v1/2021.naacl-main.219</doi>
      <bibkey>wang-etal-2021-learning-executions</bibkey>
      <video href="2021.naacl-main.219.mp4"/>
      <pwccode url="https://github.com/berlino/tensor2struct-public" additional="false">berlino/tensor2struct-public</pwccode>
    </paper>
    <paper id="220">
      <title>Learning to Synthesize Data for Semantic Parsing</title>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>2760–2766</pages>
      <abstract>Synthesizing data for semantic parsing has gained increasing attention recently. However, most methods require handcrafted (high-precision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a generative model which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our generative model can be efficiently learned from existing data at hand. Moreover, explicitly modeling compositions using PCFG leads to better exploration of unseen programs, thus generate more diverse data. We evaluate our method in both in-domain and out-of-domain settings of text-to-SQL parsing on the standard benchmarks of GeoQuery and Spider, respectively. Our empirical results show that the synthesized data generated from our model can substantially help a semantic parser achieve better compositional and domain generalization.</abstract>
      <url hash="ec05c9ba">2021.naacl-main.220</url>
      <doi>10.18653/v1/2021.naacl-main.220</doi>
      <bibkey>wang-etal-2021-learning-synthesize</bibkey>
      <video href="2021.naacl-main.220.mp4"/>
      <pwccode url="https://github.com/berlino/tensor2struct-public" additional="false">berlino/tensor2struct-public</pwccode>
    </paper>
    <paper id="221">
      <title>Edge: Enriching Knowledge Graph Embeddings with External Text</title>
      <author><first>Saed</first><last>Rezayi</last></author>
      <author><first>Handong</first><last>Zhao</last></author>
      <author><first>Sungchul</first><last>Kim</last></author>
      <author><first>Ryan</first><last>Rossi</last></author>
      <author><first>Nedim</first><last>Lipka</last></author>
      <author><first>Sheng</first><last>Li</last></author>
      <pages>2767–2776</pages>
      <abstract>Knowledge graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the web and many existing knowledge bases, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowledge graph entities based on “hard” co-occurrence of words present in the entities of the knowledge graphs and external text, while we achieve “soft” augmentation by proposing a knowledge graph enrichment and embedding framework named Edge. Given an original knowledge graph, we first generate a rich but noisy augmented graph using external texts in semantic and structural level. To distill the relevant knowledge and suppress the introduced noise, we design a graph alignment term in a shared embedding space between the original graph and augmented graph. To enhance the embedding learning on the augmented graph, we further regularize the locality relationship of target entity based on negative sampling. Experimental results on four benchmark datasets demonstrate the robustness and effectiveness of Edge in link prediction and node classification.</abstract>
      <url hash="18246654">2021.naacl-main.221</url>
      <doi>10.18653/v1/2021.naacl-main.221</doi>
      <bibkey>rezayi-etal-2021-edge</bibkey>
      <video href="2021.naacl-main.221.mp4"/>
    </paper>
    <paper id="222">
      <title><fixed-case>FLIN</fixed-case>: A Flexible Natural Language Interface for Web Navigation</title>
      <author><first>Sahisnu</first><last>Mazumder</last></author>
      <author><first>Oriana</first><last>Riva</last></author>
      <pages>2777–2788</pages>
      <abstract>AI assistants can now carry out tasks for users by directly interacting with website UIs. Current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose FLIN, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level UI actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking problem: given a user command and a webpage, FLIN learns to score the most relevant navigation instruction (involving action and parameter values). To train and evaluate FLIN, we collect a dataset using nine popular websites from three domains. Our results show that FLIN was able to adapt to new websites in a given domain.</abstract>
      <url hash="456d51bf">2021.naacl-main.222</url>
      <doi>10.18653/v1/2021.naacl-main.222</doi>
      <bibkey>mazumder-riva-2021-flin</bibkey>
      <video href="2021.naacl-main.222.mp4"/>
    </paper>
    <paper id="223">
      <title>Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index</title>
      <author><first>Roma</first><last>Patel</last></author>
      <author><first>Marta</first><last>Garnelo</last></author>
      <author><first>Ian</first><last>Gemp</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Yoram</first><last>Bachrach</last></author>
      <pages>2789–2798</pages>
      <abstract>The input vocabulary and the representations learned are crucial to the performance of neural NLP models. Using the full vocabulary results in less explainable and more memory intensive models, with the embedding layer often constituting the majority of model parameters. It is thus common to use a smaller vocabulary to lower memory requirements and construct more interpertable models. We propose a vocabulary selection method that views words as members of a team trying to maximize the model’s performance. We apply power indices from cooperative game theory, including the Shapley value and Banzhaf index, that measure the relative importance of individual team members in accomplishing a joint task. We approximately compute these indices to identify the most influential words. Our empirical evaluation examines multiple NLP tasks, including sentence and document classification, question answering and textual entailment. We compare to baselines that select words based on frequency, TF-IDF and regression coefficients under L1 regularization, and show that this game-theoretic vocabulary selection outperforms all baseline on a range of different tasks and datasets.</abstract>
      <url hash="8d0f4e56">2021.naacl-main.223</url>
      <doi>10.18653/v1/2021.naacl-main.223</doi>
      <bibkey>patel-etal-2021-game</bibkey>
      <video href="2021.naacl-main.223.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="224">
      <title>Incorporating External Knowledge to Enhance Tabular Reasoning</title>
      <author><first>J.</first><last>Neeraja</last></author>
      <author><first>Vivek</first><last>Gupta</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <pages>2799–2809</pages>
      <abstract>Reasoning about tabular information presents unique challenges to modern NLP approaches which largely rely on pre-trained contextualized embeddings of text. In this paper, we study these challenges through the problem of tabular natural language inference. We propose easy and effective modifications to how information is presented to a model for this task. We show via systematic experiments that these strategies substantially improve tabular inference performance.</abstract>
      <url hash="4651f6f4">2021.naacl-main.224</url>
      <doi>10.18653/v1/2021.naacl-main.224</doi>
      <bibkey>neeraja-etal-2021-incorporating</bibkey>
      <video href="2021.naacl-main.224.mp4"/>
      <pwccode url="https://github.com/utahnlp/knowledge_infotabs" additional="false">utahnlp/knowledge_infotabs</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/infotabs">InfoTabS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="225">
      <title>Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention</title>
      <author><first>Pengcheng</first><last>Yin</last></author>
      <author><first>Hao</first><last>Fang</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Adam</first><last>Pauls</last></author>
      <author><first>Emmanouil Antonios</first><last>Platanios</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <author><first>Sam</first><last>Thomson</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <pages>2810–2823</pages>
      <abstract>We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization.</abstract>
      <url hash="6ac8b79f">2021.naacl-main.225</url>
      <doi>10.18653/v1/2021.naacl-main.225</doi>
      <bibkey>yin-etal-2021-compositional</bibkey>
      <video href="2021.naacl-main.225.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cfq">CFQ</pwcdataset>
    </paper>
    <paper id="226">
      <title>Domain Adaptation for <fixed-case>A</fixed-case>rabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding</title>
      <author><first>Abdellah</first><last>El Mekki</last></author>
      <author><first>Abdelkader</first><last>El Mahdaouy</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <author><first>Ahmed</first><last>Khoumsi</last></author>
      <pages>2824–2837</pages>
      <abstract>Finetuning deep pre-trained language models has shown state-of-the-art performances on a wide range of Natural Language Processing (NLP) applications. Nevertheless, their generalization performance drops under domain shift. In the case of Arabic language, diglossia makes building and annotating corpora for each dialect and/or domain a more challenging task. Unsupervised Domain Adaptation tackles this issue by transferring the learned knowledge from labeled source domain data to unlabeled target domain data. In this paper, we propose a new unsupervised domain adaptation method for Arabic cross-domain and cross-dialect sentiment analysis from Contextualized Word Embedding. Several experiments are performed adopting the coarse-grained and the fine-grained taxonomies of Arabic dialects. The obtained results show that our method yields very promising results and outperforms several domain adaptation methods for most of the evaluated datasets. On average, our method increases the performance by an improvement rate of 20.8% over the zero-shot transfer learning from BERT.</abstract>
      <url hash="f3a203d1">2021.naacl-main.226</url>
      <doi>10.18653/v1/2021.naacl-main.226</doi>
      <bibkey>el-mekki-etal-2021-domain</bibkey>
      <video href="2021.naacl-main.226.mp4"/>
      <pwccode url="https://github.com/4mekki4/arabic-nlp-da" additional="false">4mekki4/arabic-nlp-da</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tsac">TSAC</pwcdataset>
    </paper>
    <paper id="227">
      <title>Multi-task Learning of Negation and Speculation for Targeted Sentiment Classification</title>
      <author><first>Andrew</first><last>Moore</last></author>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <pages>2838–2869</pages>
      <abstract>The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create English-language models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning via language modelling can improve performance on these challenge datasets, but the overall performances indicate that there is still much room for improvement. We release both the datasets and the source code at &lt;a href=”https://github.com/jerbarnes/multitask_negation_for_targeted_sentiment”&gt;https://github.com/jerbarnes/multitask_negation_for_targeted_sentiment&lt;/a&gt;.</abstract>
      <url hash="d258fa8d">2021.naacl-main.227</url>
      <attachment type="OptionalSupplementaryData" hash="3e7ac80d">2021.naacl-main.227.OptionalSupplementaryData.zip</attachment>
      <attachment type="OptionalSupplementaryCode" hash="08b676cf">2021.naacl-main.227.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.227</doi>
      <bibkey>moore-barnes-2021-multi</bibkey>
      <video href="2021.naacl-main.227.mp4"/>
      <pwccode url="https://github.com/jerbarnes/multitask_negation_for_targeted_sentiment" additional="true">jerbarnes/multitask_negation_for_targeted_sentiment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
    </paper>
    <paper id="228">
      <title>A Disentangled Adversarial Neural Topic Model for Separating Opinions from Plots in User Reviews</title>
      <author><first>Gabriele</first><last>Pergola</last></author>
      <author><first>Lin</first><last>Gui</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>2870–2883</pages>
      <abstract>The flexibility of the inference process in Variational Autoencoders (VAEs) has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models (NTM). Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers’ subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models.</abstract>
      <url hash="7149de0b">2021.naacl-main.228</url>
      <doi>10.18653/v1/2021.naacl-main.228</doi>
      <bibkey>pergola-etal-2021-disentangled</bibkey>
      <video href="2021.naacl-main.228.mp4"/>
      <pwccode url="https://github.com/gabrer/diatom" additional="false">gabrer/diatom</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="229">
      <title>Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification</title>
      <author><first>Xiaochen</first><last>Hou</last></author>
      <author><first>Peng</first><last>Qi</last></author>
      <author><first>Guangtao</first><last>Wang</last></author>
      <author><first>Rex</first><last>Ying</last></author>
      <author><first>Jing</first><last>Huang</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Bowen</first><last>Zhou</last></author>
      <pages>2884–2894</pages>
      <abstract>Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks (GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from different parsers. Instead of assigning one set of model parameters to each dependency tree, we first combine the dependency relations from different parses before applying GNNs over the resulting graph. This allows GNN models to be robust to parse errors at no additional computational cost, and helps avoid overparameterization and overfitting from GNN layer stacking by introducing more connectivity into the ensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter datasets show that our GraphMerge model not only outperforms models with single dependency tree, but also beats other ensemble models without adding model parameters.</abstract>
      <url hash="b0ebff2f">2021.naacl-main.229</url>
      <doi>10.18653/v1/2021.naacl-main.229</doi>
      <bibkey>hou-etal-2021-graph</bibkey>
      <video href="2021.naacl-main.229.mp4"/>
    </paper>
    <paper id="230">
      <title>Emotion-Infused Models for Explainable Psychological Stress Detection</title>
      <author><first>Elsbeth</first><last>Turcan</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>2895–2909</pages>
      <abstract>The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, emotion detection, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of multi-task learning as well as emotion-based language model fine-tuning. With our emotion-infused models, we see comparable results to state-of-the-art BERT. Our analysis of the words used for prediction show that our emotion-infused models mirror psychological components of stress.</abstract>
      <url hash="553ed5de">2021.naacl-main.230</url>
      <doi>10.18653/v1/2021.naacl-main.230</doi>
      <bibkey>turcan-etal-2021-emotion</bibkey>
      <video href="2021.naacl-main.230.mp4"/>
      <pwccode url="https://github.com/eturcan/emotion-infused" additional="false">eturcan/emotion-infused</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dreaddit">Dreaddit</pwcdataset>
    </paper>
    <paper id="231">
      <title>Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble</title>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Guimin</first><last>Chen</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>2910–2922</pages>
      <abstract>It is popular that neural graph-based models are applied in existing aspect-based sentiment analysis (ABSA) studies for utilizing word relations through dependency parses to facilitate the task with better semantic guidance for analyzing context and aspect words. However, most of these studies only leverage dependency relations without considering their dependency types, and are limited in lacking efficient mechanisms to distinguish the important relations as well as learn from different layers of graph based models. To address such limitations, in this paper, we propose an approach to explicitly utilize dependency types for ABSA with type-aware graph convolutional networks (T-GCN), where attention is used in T-GCN to distinguish different edges (relations) in the graph and attentive layer ensemble is proposed to comprehensively learn from different layers of T-GCN. The validity and effectiveness of our approach are demonstrated in the experimental results, where state-of-the-art performance is achieved on six English benchmark datasets. Further experiments are conducted to analyze the contributions of each component in our approach and illustrate how different layers in T-GCN help ABSA with quantitative and qualitative analysis.</abstract>
      <url hash="fba02bd8">2021.naacl-main.231</url>
      <doi>10.18653/v1/2021.naacl-main.231</doi>
      <bibkey>tian-etal-2021-aspect</bibkey>
      <video href="2021.naacl-main.231.mp4"/>
      <pwccode url="https://github.com/cuhksz-nlp/ASA-TGCN" additional="false">cuhksz-nlp/ASA-TGCN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="232">
      <title>Supertagging-based Parsing with Linear Context-free Rewriting Systems</title>
      <author><first>Thomas</first><last>Ruprecht</last></author>
      <author><first>Richard</first><last>Mörbitz</last></author>
      <pages>2923–2935</pages>
      <abstract>We present the first supertagging-based parser for linear context-free rewriting systems (LCFRS). It utilizes neural classifiers and outperforms previous LCFRS-based parsers in both accuracy and parsing speed by a wide margin. Our results keep up with the best (general) discontinuous parsers, particularly the scores for discontinuous constituents establish a new state of the art. The heart of our approach is an efficient lexicalization procedure which induces a lexical LCFRS from any discontinuous treebank. We describe a modification to usual chart-based LCFRS parsing that accounts for supertagging and introduce a procedure that transforms lexical LCFRS derivations into equivalent parse trees of the original treebank. Our approach is evaluated on the English Discontinuous Penn Treebank and the German treebanks Negra and Tiger.</abstract>
      <url hash="ddd2c5cb">2021.naacl-main.232</url>
      <attachment type="OptionalSupplementaryCode" hash="99f466f2">2021.naacl-main.232.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.232</doi>
      <bibkey>ruprecht-morbitz-2021-supertagging</bibkey>
      <video href="2021.naacl-main.232.mp4"/>
    </paper>
    <paper id="233">
      <title>Outside Computation with Superior Functions</title>
      <author><first>Parker</first><last>Riley</last></author>
      <author><first>Daniel</first><last>Gildea</last></author>
      <pages>2936–2940</pages>
      <abstract>We show that a general algorithm for efficient computation of outside values under the minimum of superior functions framework proposed by Knuth (1977) would yield a sub-exponential time algorithm for SAT, violating the Strong Exponential Time Hypothesis (SETH).</abstract>
      <url hash="386b26a5">2021.naacl-main.233</url>
      <doi>10.18653/v1/2021.naacl-main.233</doi>
      <bibkey>riley-gildea-2021-outside</bibkey>
      <video href="2021.naacl-main.233.mp4"/>
    </paper>
    <paper id="234">
      <title>Learning Syntax from Naturally-Occurring Bracketings</title>
      <author><first>Tianze</first><last>Shi</last></author>
      <author><first>Ozan</first><last>İrsoy</last></author>
      <author><first>Igor</first><last>Malioutov</last></author>
      <author><first>Lillian</first><last>Lee</last></author>
      <pages>2941–2949</pages>
      <abstract>Naturally-occurring bracketings, such as answer fragments to natural language questions and hyperlinks on webpages, can reflect human syntactic intuition regarding phrasal boundaries. Their availability and approximate correspondence to syntax make them appealing as distant information sources to incorporate into unsupervised constituency parsing. But they are noisy and incomplete; to address this challenge, we develop a partial-brackets-aware structured ramp loss in learning. Experiments demonstrate that our distantly-supervised models trained on naturally-occurring bracketing data are more accurate in inducing syntactic structures than competing unsupervised systems. On the English WSJ corpus, our models achieve an unlabeled F1 score of 68.9 for constituency parsing.</abstract>
      <url hash="2f75ff8b">2021.naacl-main.234</url>
      <doi>10.18653/v1/2021.naacl-main.234</doi>
      <bibkey>shi-etal-2021-learning</bibkey>
      <video href="2021.naacl-main.234.mp4"/>
      <pwccode url="https://github.com/tzshi/nob-naacl21" additional="false">tzshi/nob-naacl21</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl">QA-SRL</pwcdataset>
    </paper>
    <paper id="235">
      <title>Bot-Adversarial Dialogue for Safe Conversational Agents</title>
      <author><first>Jing</first><last>Xu</last></author>
      <author><first>Da</first><last>Ju</last></author>
      <author><first>Margaret</first><last>Li</last></author>
      <author><first>Y-Lan</first><last>Boureau</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <author><first>Emily</first><last>Dinan</last></author>
      <pages>2950–2968</pages>
      <abstract>Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-the-loop framework in a two-stage system, or ”baking-in” safety to the generative model itself. We find our new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, we expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot.</abstract>
      <url hash="8b683ae3">2021.naacl-main.235</url>
      <doi>10.18653/v1/2021.naacl-main.235</doi>
      <bibkey>xu-etal-2021-bot</bibkey>
      <video href="2021.naacl-main.235.mp4"/>
    </paper>
    <paper id="236">
      <title>Non-Autoregressive Semantic Parsing for Compositional Task-Oriented Dialog</title>
      <author><first>Arun</first><last>Babu</last></author>
      <author><first>Akshat</first><last>Shrivastava</last></author>
      <author><first>Armen</first><last>Aghajanyan</last></author>
      <author><first>Ahmed</first><last>Aly</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Marjan</first><last>Ghazvininejad</last></author>
      <pages>2969–2978</pages>
      <abstract>Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models. In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency. In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture. By combining non-autoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models. Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets.</abstract>
      <url hash="4f976089">2021.naacl-main.236</url>
      <doi>10.18653/v1/2021.naacl-main.236</doi>
      <bibkey>babu-etal-2021-non</bibkey>
      <video href="2021.naacl-main.236.mp4"/>
      <pwccode url="https://github.com/facebookresearch/pytext" additional="false">facebookresearch/pytext</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="237">
      <title>Example-Driven Intent Prediction with Observers</title>
      <author><first>Shikib</first><last>Mehri</last></author>
      <author><first>Mihail</first><last>Eric</last></author>
      <pages>2979–2992</pages>
      <abstract>A key challenge of dialog systems research is to effectively and efficiently adapt to new domains. A scalable paradigm for adaptation necessitates the development of generalizable models that perform well in few-shot settings. In this paper, we focus on the intent classification problem which aims to identify user intents given utterances addressed to the dialog system. We propose two approaches for improving the generalizability of utterance classification models: (1) observers and (2) example-driven training. Prior work has shown that BERT-like models tend to attribute a significant amount of attention to the [CLS] token, which we hypothesize results in diluted representations. Observers are tokens that are not attended to, and are an alternative to the [CLS] token as a semantic representation of utterances. Example-driven training learns to classify utterances by comparing to examples, thereby using the underlying encoder as a sentence similarity model. These methods are complementary; improving the representation through observers allows the example-driven model to better measure sentence similarities. When combined, the proposed methods attain state-of-the-art results on three intent prediction datasets (banking77, clinc150, hwu64) in both the full data and few-shot (10 examples per intent) settings. Furthermore, we demonstrate that the proposed approach can transfer to new intents and across datasets without any additional training.</abstract>
      <url hash="372e6cf5">2021.naacl-main.237</url>
      <doi>10.18653/v1/2021.naacl-main.237</doi>
      <bibkey>mehri-eric-2021-example</bibkey>
      <video href="2021.naacl-main.237.mp4"/>
      <pwccode url="https://github.com/alexa/dialoglue" additional="false">alexa/dialoglue</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hwu64">HWU64</pwcdataset>
    </paper>
    <paper id="238">
      <title>Imperfect also Deserves Reward: Multi-Level and Sequential Reward Modeling for Better Dialog Management</title>
      <author><first>Zhengxu</first><last>Hou</last></author>
      <author><first>Bang</first><last>Liu</last></author>
      <author><first>Ruihui</first><last>Zhao</last></author>
      <author><first>Zijing</first><last>Ou</last></author>
      <author><first>Yafei</first><last>Liu</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Yefeng</first><last>Zheng</last></author>
      <pages>2993–3001</pages>
      <abstract>For task-oriented dialog systems, training a Reinforcement Learning (RL) based Dialog Management module suffers from low sample efficiency and slow convergence speed due to the sparse rewards in RL. To solve this problem, many strategies have been proposed to give proper rewards when training RL, but their rewards lack interpretability and cannot accurately estimate the distribution of state-action pairs in real dialogs. In this paper, we propose a multi-level reward modeling approach that factorizes a reward into a three-level hierarchy: domain, act, and slot. Based on inverse adversarial reinforcement learning, our designed reward model can provide more accurate and explainable reward signals for state-action pairs. Extensive evaluations show that our approach can be applied to a wide range of reinforcement learning-based dialog systems and significantly improves both the performance and the speed of convergence.</abstract>
      <url hash="acff4505">2021.naacl-main.238</url>
      <doi>10.18653/v1/2021.naacl-main.238</doi>
      <bibkey>hou-etal-2021-imperfect</bibkey>
      <video href="2021.naacl-main.238.mp4"/>
      <pwccode url="https://github.com/sherlock1987/SeqReward" additional="false">sherlock1987/SeqReward</pwccode>
    </paper>
    <paper id="239">
      <title>Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems</title>
      <author><first>Derek</first><last>Chen</last></author>
      <author><first>Howard</first><last>Chen</last></author>
      <author><first>Yi</first><last>Yang</last></author>
      <author><first>Alexander</first><last>Lin</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>3002–3017</pages>
      <abstract>Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8% absolute accuracy) still exists to reach human-level performance on ABCD.</abstract>
      <url hash="66b9ee55">2021.naacl-main.239</url>
      <doi>10.18653/v1/2021.naacl-main.239</doi>
      <bibkey>chen-etal-2021-action</bibkey>
      <video href="2021.naacl-main.239.mp4"/>
      <pwccode url="https://github.com/asappresearch/abcd" additional="true">asappresearch/abcd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/abcd">ABCD</pwcdataset>
    </paper>
    <paper id="240">
      <title>Controlling Dialogue Generation with Semantic Exemplars</title>
      <author><first>Prakhar</first><last>Gupta</last></author>
      <author><first>Jeffrey</first><last>Bigham</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Amy</first><last>Pavel</last></author>
      <pages>3018–3029</pages>
      <abstract>Dialogue systems pretrained with large language models generate locally coherent responses, but lack fine-grained control over responses necessary to achieve specific goals. A promising method to control response generation is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. We present an Exemplar-based Dialogue Generation model, EDGE, that uses the semantic frames present in exemplar responses to guide response generation. We show that controlling dialogue generation based on the semantic frames of exemplars improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar responses.</abstract>
      <url hash="5cfe9929">2021.naacl-main.240</url>
      <doi>10.18653/v1/2021.naacl-main.240</doi>
      <bibkey>gupta-etal-2021-controlling</bibkey>
      <video href="2021.naacl-main.240.mp4"/>
      <pwccode url="https://github.com/prakharguptaz/EDGE-exemplars" additional="false">prakharguptaz/EDGE-exemplars</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="241">
      <title><fixed-case>COIL</fixed-case>: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List</title>
      <author><first>Luyu</first><last>Gao</last></author>
      <author><first>Zhuyun</first><last>Dai</last></author>
      <author><first>Jamie</first><last>Callan</last></author>
      <pages>3030–3042</pages>
      <abstract>Classical information retrieval systems such as BM25 rely on exact lexical match and can carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture, where scoring is based on overlapping query document tokens’ contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of exact match and the representation power of deep language models. Our experimental results show COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency.</abstract>
      <url hash="ddcb37bb">2021.naacl-main.241</url>
      <doi>10.18653/v1/2021.naacl-main.241</doi>
      <bibkey>gao-etal-2021-coil</bibkey>
      <video href="2021.naacl-main.241.mp4"/>
      <pwccode url="https://github.com/luyug/COIL" additional="false">luyug/COIL</pwccode>
    </paper>
    <paper id="242">
      <title><fixed-case>X</fixed-case>-Class: Text Classification with Extremely Weak Supervision</title>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Dheeraj</first><last>Mekala</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>3043–3053</pages>
      <abstract>In this paper, we explore text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective—ideal document representations should lead to nearly the same results between clustering and the desired classification. In particular, one can classify the same corpus differently (e.g., based on topics and locations), so document representations should be adaptive to the given class names. We propose a novel framework X-Class to realize the adaptive representations. Specifically, we first estimate class representations by incrementally adding the most similar word to each class until inconsistency arises. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized word representations. With the prior of each document assigned to its nearest class, we then cluster and align the documents to classes. Finally, we pick the most confident documents from each cluster to train a text classifier. Extensive experiments demonstrate that X-Class can rival and even outperform seed-driven weakly supervised methods on 7 benchmark datasets.</abstract>
      <url hash="fef2d803">2021.naacl-main.242</url>
      <doi>10.18653/v1/2021.naacl-main.242</doi>
      <bibkey>wang-etal-2021-x</bibkey>
      <video href="2021.naacl-main.242.mp4"/>
      <pwccode url="https://github.com/ZihanWangKi/XClass" additional="true">ZihanWangKi/XClass</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="243">
      <title>Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling</title>
      <author><first>Aaron</first><last>Mueller</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>3054–3068</pages>
      <abstract>Neural topic models can augment or replace bag-of-words inputs with the learned representations of deep pre-trained transformer-based word prediction models. One added benefit when using representations from multilingual models is that they facilitate zero-shot polylingual topic modeling. However, while it has been widely observed that pre-trained embeddings should be fine-tuned to a given task, it is not immediately clear what supervision should look like for an unsupervised task such as topic modeling. Thus, we propose several methods for fine-tuning encoders to improve both monolingual and zero-shot polylingual neural topic modeling. We consider fine-tuning on auxiliary tasks, constructing a new topic classification task, integrating the topic classification objective directly into topic model training, and continued pre-training. We find that fine-tuning encoder representations on topic classification and integrating the topic classification task directly into topic modeling improves topic quality, and that fine-tuning encoder representations on any task is the most important factor for facilitating cross-lingual transfer.</abstract>
      <url hash="af73dfe3">2021.naacl-main.243</url>
      <doi>10.18653/v1/2021.naacl-main.243</doi>
      <bibkey>mueller-dredze-2021-fine</bibkey>
      <video href="2021.naacl-main.243.mp4"/>
      <pwccode url="https://github.com/aaronmueller/contextualized-topic-models" additional="false">aaronmueller/contextualized-topic-models</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mldoc">MLDoc</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="244">
      <title>Exploring the Relationship Between Algorithm Performance, Vocabulary, and Run-Time in Text Classification</title>
      <author><first>Wilson</first><last>Fearn</last></author>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <pages>3069–3082</pages>
      <abstract>Text classification is a significant branch of natural language processing, and has many applications including document classification and sentiment analysis. Unsurprisingly, those who do text classification are concerned with the run-time of their algorithms, many of which depend on the size of the corpus’ vocabulary due to their bag-of-words representation. Although many studies have examined the effect of preprocessing techniques on vocabulary size and accuracy, none have examined how these methods affect a model’s run-time. To fill this gap, we provide a comprehensive study that examines how preprocessing techniques affect the vocabulary size, model performance, and model run-time, evaluating ten techniques over four models and two datasets. We show that some individual methods can reduce run-time with no loss of accuracy, while some combinations of methods can trade 2-5% of the accuracy for up to a 65% reduction of run-time. Furthermore, some combinations of preprocessing techniques can even provide a 15% reduction in run-time while simultaneously improving model accuracy.</abstract>
      <url hash="4cec24da">2021.naacl-main.244</url>
      <doi>10.18653/v1/2021.naacl-main.244</doi>
      <bibkey>fearn-etal-2021-exploring</bibkey>
      <video href="2021.naacl-main.244.mp4"/>
      <pwccode url="https://github.com/wfearn/preprocessing-paper" additional="false">wfearn/preprocessing-paper</pwccode>
    </paper>
    <paper id="245">
      <title>Faithfully Explainable Recommendation via Neural Logic Reasoning</title>
      <author><first>Yaxin</first><last>Zhu</last></author>
      <author><first>Yikun</first><last>Xian</last></author>
      <author><first>Zuohui</first><last>Fu</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <author><first>Yongfeng</first><last>Zhang</last></author>
      <pages>3083–3090</pages>
      <abstract>Knowledge graphs (KG) have become increasingly important to endow modern recommender systems with the ability to generate traceable reasoning paths to explain the recommendation process. However, prior research rarely considers the faithfulness of the derived explanations to justify the decision-making process. To the best of our knowledge, this is the first work that models and evaluates faithfully explainable recommendation under the framework of KG reasoning. Specifically, we propose neural logic reasoning for explainable recommendation (LOGER) by drawing on interpretable logical rules to guide the path-reasoning process for explanation generation. We experiment on three large-scale datasets in the e-commerce domain, demonstrating the effectiveness of our method in delivering high-quality recommendations as well as ascertaining the faithfulness of the derived explanation.</abstract>
      <url hash="678c78b2">2021.naacl-main.245</url>
      <doi>10.18653/v1/2021.naacl-main.245</doi>
      <bibkey>zhu-etal-2021-faithfully</bibkey>
      <video href="2021.naacl-main.245.mp4"/>
      <pwccode url="https://github.com/orcax/LOGER" additional="false">orcax/LOGER</pwccode>
    </paper>
    <paper id="246">
      <title>You Sound Like Someone Who Watches Drama Movies: Towards Predicting Movie Preferences from Conversational Interactions</title>
      <author><first>Sergey</first><last>Volokhin</last></author>
      <author><first>Joyce</first><last>Ho</last></author>
      <author><first>Oleg</first><last>Rokhlenko</last></author>
      <author><first>Eugene</first><last>Agichtein</last></author>
      <pages>3091–3096</pages>
      <abstract>The increasing popularity of voice-based personal assistants provides new opportunities for conversational recommendation. One particularly interesting area is movie recommendation, which can benefit from an open-ended interaction with the user, through a natural conversation. We explore one promising direction for conversational recommendation: mapping a conversational user, for whom there is limited or no data available, to most similar external reviewers, whose preferences are known, by representing the conversation as a user’s interest vector, and adapting collaborative filtering techniques to estimate the current user’s preferences for new movies. We call our proposed method ConvExtr (Conversational Collaborative Filtering using External Data), which 1) infers a user’s sentiment towards an entity from the conversation context, and 2) transforms the ratings of “similar” external reviewers to predict the current user’s preferences. We implement these steps by adapting contextual sentiment prediction techniques, and domain adaptation, respectively. To evaluate our method, we develop and make available a finely annotated dataset of movie recommendation conversations, which we call MovieSent. Our results demonstrate that ConvExtr can improve the accuracy of predicting users’ ratings for new movies by exploiting conversation content and external data.</abstract>
      <url hash="ef2f9cae">2021.naacl-main.246</url>
      <doi>10.18653/v1/2021.naacl-main.246</doi>
      <bibkey>volokhin-etal-2021-sound</bibkey>
      <video href="2021.naacl-main.246.mp4"/>
      <pwccode url="https://github.com/sergey-volokhin/conversational-movies" additional="false">sergey-volokhin/conversational-movies</pwccode>
    </paper>
    <paper id="247">
      <title>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</title>
      <author><first>Shunyu</first><last>Yao</last></author>
      <author><first>Karthik</first><last>Narasimhan</last></author>
      <author><first>Matthew</first><last>Hausknecht</last></author>
      <pages>3097–3102</pages>
      <abstract>Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including Zork I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.</abstract>
      <url hash="b1b7c054">2021.naacl-main.247</url>
      <doi>10.18653/v1/2021.naacl-main.247</doi>
      <bibkey>yao-etal-2021-reading</bibkey>
      <video href="2021.naacl-main.247.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/jericho">Jericho</pwcdataset>
    </paper>
    <paper id="248">
      <title><fixed-case>SO</fixed-case>r<fixed-case>T</fixed-case>-ing <fixed-case>VQA</fixed-case> Models : Contrastive Gradient Learning for Improved Consistency</title>
      <author><first>Sameer</first><last>Dharur</last></author>
      <author><first>Purva</first><last>Tendulkar</last></author>
      <author><first>Dhruv</first><last>Batra</last></author>
      <author><first>Devi</first><last>Parikh</last></author>
      <author><first>Ramprasaath</first><last>R. Selvaraju</last></author>
      <pages>3103–3111</pages>
      <abstract>Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world - they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the reasoning question correctly. To address this, we first present a gradient-based interpretability approach to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach called Sub-question Oriented Tuning (SOrT) which encourages models to rank relevant sub-questions higher than irrelevant questions for an &lt;image, reasoning-question&gt; pair. We show that SOrT improves model consistency by up to 6.5% points over existing approaches, while also improving visual grounding and robustness to rephrasings of questions.</abstract>
      <url hash="6eff714b">2021.naacl-main.248</url>
      <attachment type="OptionalSupplementaryData" hash="279667d1">2021.naacl-main.248.OptionalSupplementaryData.zip</attachment>
      <attachment type="OptionalSupplementaryCode" hash="1925f1d9">2021.naacl-main.248.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.248</doi>
      <bibkey>dharur-etal-2021-sort</bibkey>
      <video href="2021.naacl-main.248.mp4"/>
      <pwccode url="https://github.com/sameerdharur/sorting-vqa" additional="false">sameerdharur/sorting-vqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vqa-hat">VQA-HAT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="249">
      <title>Semi-Supervised Policy Initialization for Playing Games with Language Hints</title>
      <author><first>Tsu-Jui</first><last>Fu</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>3112–3116</pages>
      <abstract>Using natural language as a hint can supply an additional reward for playing sparse-reward games. Achieving a goal should involve several different hints, while the given hints are usually incomplete. Those unmentioned latent hints still rely on the sparse reward signal, and make the learning process difficult. In this paper, we propose semi-supervised initialization (SSI) that allows the agent to learn from various possible hints before training under different tasks. Experiments show that SSI not only helps to learn faster (<b>1.2x</b>) but also has a higher success rate (<b>11%</b> relative improvement) of the final policy.</abstract>
      <url hash="516fa0e8">2021.naacl-main.249</url>
      <doi>10.18653/v1/2021.naacl-main.249</doi>
      <bibkey>fu-wang-2021-semi</bibkey>
      <video href="2021.naacl-main.249.mp4"/>
      <pwccode url="https://github.com/tsujuifu/code_ssi" additional="false">tsujuifu/code_ssi</pwccode>
    </paper>
    <paper id="250">
      <title>Revisiting Document Representations for Large-Scale Zero-Shot Learning</title>
      <author><first>Jihyung</first><last>Kil</last></author>
      <author><first>Wei-Lun</first><last>Chao</last></author>
      <pages>3117–3128</pages>
      <abstract>Zero-shot learning aims to recognize unseen objects using their semantic representations. Most existing works use visual attributes labeled by humans, not suitable for large-scale applications. In this paper, we revisit the use of documents as semantic representations. We argue that documents like Wikipedia pages contain rich visual information, which however can easily be buried by the vast amount of non-visual sentences. To address this issue, we propose a semi-automatic mechanism for visual sentence extraction that leverages the document section headers and the clustering structure of visual sentences. The extracted visual sentences, after a novel weighting scheme to distinguish similar classes, essentially form semantic representations like visual attributes but need much less human effort. On the ImageNet dataset with over 10,000 unseen classes, our representations lead to a 64% relative improvement against the commonly used ones.</abstract>
      <url hash="d832c624">2021.naacl-main.250</url>
      <doi>10.18653/v1/2021.naacl-main.250</doi>
      <bibkey>kil-chao-2021-revisiting</bibkey>
      <video href="2021.naacl-main.250.mp4"/>
      <pwccode url="https://github.com/heendung/vs-zsl" additional="false">heendung/vs-zsl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/awa-1">AwA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/awa2-1">AwA2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/apy">aPY</pwcdataset>
    </paper>
    <paper id="251">
      <title>Negative language transfer in learner <fixed-case>E</fixed-case>nglish: A new dataset</title>
      <author><first>Leticia</first><last>Farias Wanderley</last></author>
      <author><first>Nicole</first><last>Zhao</last></author>
      <author><first>Carrie</first><last>Demmans Epp</last></author>
      <pages>3129–3142</pages>
      <abstract>Automatic personalized corrective feedback can help language learners from different backgrounds better acquire a new language. This paper introduces a learner English dataset in which learner errors are accompanied by information about possible error sources. This dataset contains manually annotated error causes for learner writing errors. These causes tie learner mistakes to structures from their first languages, when the rules in English and in the first language diverge. This new dataset will enable second language acquisition researchers to computationally analyze a large quantity of learner errors that are related to language transfer from the learners’ first language. The dataset can also be applied in personalizing grammatical error correction systems according to the learners’ first language and in providing feedback that is informed by the cause of an error.</abstract>
      <url hash="99274768">2021.naacl-main.251</url>
      <doi>10.18653/v1/2021.naacl-main.251</doi>
      <bibkey>farias-wanderley-etal-2021-negative</bibkey>
      <video href="2021.naacl-main.251.mp4"/>
      <pwccode url="https://github.com/EdTeKLA/LanguageTransfer" additional="false">EdTeKLA/LanguageTransfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="252">
      <title><fixed-case>S</fixed-case>ent<fixed-case>S</fixed-case>im: Crosslingual Semantic Evaluation of Machine Translation</title>
      <author><first>Yurun</first><last>Song</last></author>
      <author><first>Junchen</first><last>Zhao</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>3143–3156</pages>
      <abstract>Machine translation (MT) is currently evaluated in one of two ways: in a monolingual fashion, by comparison with the system output to one or more human reference translations, or in a trained crosslingual fashion, by building a supervised model to predict quality scores from human-labeled data. In this paper, we propose a more cost-effective, yet well performing unsupervised alternative SentSim: relying on strong pretrained multilingual word and sentence representations, we directly compare the source with the machine translated sentence, thus avoiding the need for both reference translations and labelled training data. The metric builds on state-of-the-art embedding-based approaches – namely BERTScore and Word Mover’s Distance – by incorporating a notion of sentence semantic similarity. By doing so, it achieves better correlation with human scores on different datasets. We show that it outperforms these and other metrics in the standard monolingual setting (MT-reference translation), a well as in the source-MT bilingual setting, where it performs on par with glass-box approaches to quality estimation that rely on MT model information.</abstract>
      <url hash="fd0fe40c">2021.naacl-main.252</url>
      <attachment type="OptionalSupplementaryData" hash="cc6a8b52">2021.naacl-main.252.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.252</doi>
      <bibkey>song-etal-2021-sentsim</bibkey>
      <video href="2021.naacl-main.252.mp4"/>
    </paper>
    <paper id="253">
      <title>Quality Estimation for Image Captions Based on Large-scale Human Evaluations</title>
      <author><first>Tomer</first><last>Levinboim</last></author>
      <author><first>Ashish V.</first><last>Thapliyal</last></author>
      <author><first>Piyush</first><last>Sharma</last></author>
      <author><first>Radu</first><last>Soricut</last></author>
      <pages>3157–3166</pages>
      <abstract>Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of Quality Estimation (QE) for image captions, which attempts to model the caption quality from a human perspective and *without* access to ground-truth references, so that it can be applied at prediction time to detect low-quality captions produced on *previously unseen images*. For this task, we develop a human evaluation process that collects coarse-grained caption annotations from crowdsourced users, which is then used to collect a large scale dataset spanning more than 600k caption quality ratings. We then carefully validate the quality of the collected ratings and establish baseline models for this new QE task. Finally, we further collect fine-grained caption quality annotations from trained raters, and use them to demonstrate that QE models trained over the coarse ratings can effectively detect and filter out low-quality image captions, thereby improving the user experience from captioning systems.</abstract>
      <url hash="70a5bd23">2021.naacl-main.253</url>
      <doi>10.18653/v1/2021.naacl-main.253</doi>
      <bibkey>levinboim-etal-2021-quality</bibkey>
      <video href="2021.naacl-main.253.mp4"/>
      <pwccode url="https://github.com/google-research-datasets/Image-Caption-Quality-Dataset" additional="false">google-research-datasets/Image-Caption-Quality-Dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/image-caption-quality-dataset">Image Caption Quality Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
    </paper>
    <paper id="254">
      <title><fixed-case>C</fixed-case>a<fixed-case>S</fixed-case>i<fixed-case>N</fixed-case>o: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems</title>
      <author><first>Kushal</first><last>Chawla</last></author>
      <author><first>Jaysa</first><last>Ramirez</last></author>
      <author><first>Rene</first><last>Clever</last></author>
      <author><first>Gale</first><last>Lucas</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Jonathan</first><last>Gratch</last></author>
      <pages>3167–3185</pages>
      <abstract>Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https://github.com/kushalchawla/CaSiNo</abstract>
      <url hash="9232b1e3">2021.naacl-main.254</url>
      <doi>10.18653/v1/2021.naacl-main.254</doi>
      <bibkey>chawla-etal-2021-casino</bibkey>
      <video href="2021.naacl-main.254.mp4"/>
      <pwccode url="https://github.com/kushalchawla/CaSiNo" additional="false">kushalchawla/CaSiNo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/casino">CaSiNo</pwcdataset>
    </paper>
    <paper id="255">
      <title>News Headline Grouping as a Challenging <fixed-case>NLU</fixed-case> Task</title>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Lucas</first><last>Bandarkar</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <pages>3186–3198</pages>
      <abstract>Recent progress in Natural Language Understanding (NLU) has seen the latest models outperform human performance on many standard tasks. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of HeadLine Grouping (HLG) and a corresponding dataset (HLGD) consisting of 20,056 pairs of news headlines, each labeled with a binary judgement as to whether the pair belongs within the same group. On HLGD, human annotators achieve high performance of around 0.9 F-1, while current state-of-the art Transformer models only reach 0.75 F-1, opening the path for further improvements. We further propose a novel unsupervised Headline Generator Swap model for the task of HeadLine Grouping that achieves within 3 F-1 of the best supervised model. Finally, we analyze high-performing models with consistency tests, and find that models are not consistent in their predictions, revealing modeling limits of current architectures.</abstract>
      <url hash="800ae2c2">2021.naacl-main.255</url>
      <doi>10.18653/v1/2021.naacl-main.255</doi>
      <bibkey>laban-etal-2021-news</bibkey>
      <video href="2021.naacl-main.255.mp4"/>
      <pwccode url="https://github.com/tingofurro/headline_grouping" additional="false">tingofurro/headline_grouping</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hlgd">HLGD</pwcdataset>
    </paper>
    <paper id="256">
      <title>Olá, Bonjour, Salve! <fixed-case>XFORMAL</fixed-case>: A Benchmark for Multilingual Formality Style Transfer</title>
      <author><first>Eleftheria</first><last>Briakou</last></author>
      <author><first>Di</first><last>Lu</last></author>
      <author><first>Ke</first><last>Zhang</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <pages>3199–3216</pages>
      <abstract>We take the first step towards multilingual style transfer by creating and releasing XFORMAL, a benchmark of multiple formal reformulations of informal text in Brazilian Portuguese, French, and Italian. Results on XFORMAL suggest that state-of-the-art style transfer approaches perform close to simple baselines, indicating that style transfer is even more challenging when moving multilingual.</abstract>
      <url hash="e5d15c00">2021.naacl-main.256</url>
      <doi>10.18653/v1/2021.naacl-main.256</doi>
      <bibkey>briakou-etal-2021-ola</bibkey>
      <video href="2021.naacl-main.256.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="257">
      <title>Grouping Words with Semantic Diversity</title>
      <author><first>Karine</first><last>Chubarian</last></author>
      <author><first>Abdul Rafae</first><last>Khan</last></author>
      <author><first>Anastasios</first><last>Sidiropoulos</last></author>
      <author><first>Jia</first><last>Xu</last></author>
      <pages>3217–3228</pages>
      <abstract>Deep Learning-based NLP systems can be sensitive to unseen tokens and hard to learn with high-dimensional inputs, which critically hinder learning generalization. We introduce an approach by grouping input words based on their semantic diversity to simplify input language representation with low ambiguity. Since the semantically diverse words reside in different contexts, we are able to substitute words with their groups and still distinguish word meanings relying on their contexts. We design several algorithms that compute diverse groupings based on random sampling, geometric distances, and entropy maximization, and we prove formal guarantees for the entropy-based algorithms. Experimental results show that our methods generalize NLP models and demonstrate enhanced accuracy on POS tagging and LM tasks and significant improvements on medium-scale machine translation tasks, up to +6.5 BLEU points. Our source code is available at https://github.com/abdulrafae/dg.</abstract>
      <url hash="5b4e5e6a">2021.naacl-main.257</url>
      <attachment type="OptionalSupplementaryData" hash="748a89a8">2021.naacl-main.257.OptionalSupplementaryData.zip</attachment>
      <attachment type="OptionalSupplementaryCode" hash="c7dee13a">2021.naacl-main.257.OptionalSupplementaryCode.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.257</doi>
      <bibkey>chubarian-etal-2021-grouping</bibkey>
      <video href="2021.naacl-main.257.mp4"/>
      <pwccode url="https://github.com/abdulrafae/dg" additional="false">abdulrafae/dg</pwccode>
    </paper>
    <paper id="258">
      <title>Noise Stability Regularization for Improving <fixed-case>BERT</fixed-case> Fine-tuning</title>
      <author><first>Hang</first><last>Hua</last></author>
      <author><first>Xingjian</first><last>Li</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <author><first>Chengzhong</first><last>Xu</last></author>
      <author><first>Jiebo</first><last>Luo</last></author>
      <pages>3229–3241</pages>
      <abstract>Fine-tuning pre-trained language models suchas BERT has become a common practice dom-inating leaderboards across various NLP tasks.Despite its recent success and wide adoption,this process is unstable when there are onlya small number of training samples available.The brittleness of this process is often reflectedby the sensitivity to random seeds. In this pa-per, we propose to tackle this problem basedon the noise stability property of deep nets,which is investigated in recent literature (Aroraet al., 2018; Sanyal et al., 2020). Specifically,we introduce a novel and effective regulariza-tion method to improve fine-tuning on NLPtasks, referred to asLayer-wiseNoiseStabilityRegularization (LNSR). We extend the theo-ries about adding noise to the input and provethat our method gives a stabler regularizationeffect. We provide supportive evidence by ex-perimentally confirming that well-performingmodels show a low sensitivity to noise andfine-tuning with LNSR exhibits clearly bet-ter generalizability and stability. Furthermore,our method also demonstrates advantages overother state-of-the-art algorithms including L2-SP (Li et al., 2018), Mixout (Lee et al., 2020)and SMART (Jiang et al., 20)</abstract>
      <url hash="db10efc2">2021.naacl-main.258</url>
      <doi>10.18653/v1/2021.naacl-main.258</doi>
      <bibkey>hua-etal-2021-noise</bibkey>
      <video href="2021.naacl-main.258.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="259">
      <title><fixed-case>F</fixed-case>low<fixed-case>P</fixed-case>rior: Learning Expressive Priors for Latent Variable Sentence Models</title>
      <author><first>Xiaoan</first><last>Ding</last></author>
      <author><first>Kevin</first><last>Gimpel</last></author>
      <pages>3242–3258</pages>
      <abstract>Variational autoencoders (VAEs) are widely used for latent variable modeling of text. We focus on variations that learn expressive prior distributions over the latent variable. We find that existing training strategies are not effective for learning rich priors, so we propose adding the importance-sampled log marginal likelihood as a second term to the standard VAE objective to help when learning the prior. Doing so improves results for all priors evaluated, including a novel choice for sentence VAEs based on normalizing flows (NF). Priors parameterized with NF are no longer constrained to a specific distribution family, allowing a more flexible way to encode the data distribution. Our model, which we call FlowPrior, shows a substantial improvement in language modeling tasks compared to strong baselines. We demonstrate that FlowPrior learns an expressive prior with analysis and several forms of evaluation involving generation.</abstract>
      <url hash="a8065e6e">2021.naacl-main.259</url>
      <doi>10.18653/v1/2021.naacl-main.259</doi>
      <bibkey>ding-gimpel-2021-flowprior</bibkey>
      <video href="2021.naacl-main.259.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="260">
      <title><fixed-case>HTCI</fixed-case>nfo<fixed-case>M</fixed-case>ax: A Global Model for Hierarchical Text Classification via Information Maximization</title>
      <author><first>Zhongfen</first><last>Deng</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Dongxiao</first><last>He</last></author>
      <author><first>Jianxin</first><last>Li</last></author>
      <author><first>Philip</first><last>Yu</last></author>
      <pages>3259–3265</pages>
      <abstract>The current state-of-the-art model HiAGM for hierarchical text classification has two limitations. First, it correlates each text sample with all labels in the dataset which contains irrelevant information. Second, it does not consider any statistical constraint on the label representations learned by the structure encoder, while constraints for representation learning are proved to be helpful in previous work. In this paper, we propose HTCInfoMax to address these issues by introducing information maximization which includes two modules: text-label mutual information maximization and label prior matching. The first module can model the interaction between each text sample and its ground truth labels explicitly which filters out irrelevant information. The second one encourages the structure encoder to learn better representations with desired characteristics for all labels which can better handle label imbalance in hierarchical text classification. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed HTCInfoMax.</abstract>
      <url hash="a5b3e9bd">2021.naacl-main.260</url>
      <doi>10.18653/v1/2021.naacl-main.260</doi>
      <bibkey>deng-etal-2021-htcinfomax</bibkey>
      <video href="2021.naacl-main.260.mp4"/>
      <pwccode url="https://github.com/RingBDStack/HTCInfoMax" additional="false">RingBDStack/HTCInfoMax</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rcv1">RCV1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/web-of-science-dataset">WOS</pwcdataset>
    </paper>
    <paper id="261">
      <title>Knowledge Guided Metric Learning for Few-Shot Text Classification</title>
      <author><first>Dianbo</first><last>Sui</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Binjie</first><last>Mao</last></author>
      <author><first>Delai</first><last>Qiu</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>3266–3271</pages>
      <abstract>Humans can distinguish new categories very efficiently with few examples, largely due to the fact that human beings can leverage knowledge obtained from relevant tasks. However, deep learning based text classification model tends to struggle to achieve satisfactory performance when labeled data are scarce. Inspired by human intelligence, we propose to introduce external knowledge into few-shot learning to imitate human knowledge. A novel parameter generator network is investigated to this end, which is able to use the external knowledge to generate different metrics for different tasks. Armed with this network, similar tasks can use similar metrics while different tasks use different metrics. Through experiments, we demonstrate that our method outperforms the SoTA few-shot text classification models.</abstract>
      <url hash="58b0f698">2021.naacl-main.261</url>
      <doi>10.18653/v1/2021.naacl-main.261</doi>
      <bibkey>sui-etal-2021-knowledge</bibkey>
      <video href="2021.naacl-main.261.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/nell">NELL</pwcdataset>
    </paper>
    <paper id="262">
      <title>Ensemble of <fixed-case>MRR</fixed-case> and <fixed-case>NDCG</fixed-case> models for Visual Dialog</title>
      <author><first>Idan</first><last>Schwartz</last></author>
      <pages>3272–3363</pages>
      <abstract>Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., ‘yeah’ and ‘yes’). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as ‘I don’t know.’ Crafting a model that excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https://github.com/idansc/mrr-ndcg.</abstract>
      <url hash="9b91aca2">2021.naacl-main.262</url>
      <doi>10.18653/v1/2021.naacl-main.262</doi>
      <bibkey>schwartz-2021-ensemble</bibkey>
      <video href="2021.naacl-main.262.mp4"/>
      <pwccode url="https://github.com/idansc/mrr-ndcg" additional="false">idansc/mrr-ndcg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
    </paper>
    <paper id="263">
      <title>Supervised Neural Clustering via Latent Structured Output Learning: Application to Question Intents</title>
      <author><first>Iryna</first><last>Haponchyk</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>3364–3374</pages>
      <abstract>Previous pre-neural work on structured prediction has produced very effective supervised clustering algorithms using linear classifiers, e.g., structured SVM or perceptron. However, these cannot exploit the representation learning ability of neural networks, which would make supervised clustering even more powerful, i.e., general clustering patterns can be learned automatically. In this paper, we design neural networks based on latent structured prediction loss and Transformer models to approach supervised clustering. We tested our methods on the task of automatically recreating categories of intents from publicly available question intent corpora. The results show that our approach delivers 95.65% of F1, outperforming the state of the art by 17.24%.</abstract>
      <url hash="019e73cd">2021.naacl-main.263</url>
      <doi>10.18653/v1/2021.naacl-main.263</doi>
      <bibkey>haponchyk-moschitti-2021-supervised</bibkey>
      <video href="2021.naacl-main.263.mp4"/>
      <pwccode url="https://github.com/ikernels/intent-qa" additional="false">ikernels/intent-qa</pwccode>
    </paper>
    <paper id="264">
      <title><fixed-case>ConVEx</fixed-case>: Data-Efficient and Few-Shot Slot Labeling</title>
      <author><first>Matthew</first><last>Henderson</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>3375–3389</pages>
      <abstract>We propose ConVEx (Conversational Value Extractor), an efficient pretraining and fine-tuning neural approach for slot-labeling dialog tasks. Instead of relying on more general pretraining objectives from prior work (e.g., language modeling, response selection), ConVEx’s pretraining objective, a novel pairwise cloze task using Reddit data, is well aligned with its intended usage on sequence labeling tasks. This enables learning domain-specific slot labelers by simply fine-tuning decoding layers of the pretrained general-purpose sequence labeling model, while the majority of the pretrained model’s parameters are kept frozen. We report state-of-the-art performance of ConVEx across a range of diverse domains and data sets for dialog slot-labeling, with the largest gains in the most challenging, few-shot setups. We believe that ConVEx’s reduced pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its efficient fine-tuning and strong performance, promise wider portability and scalability for data-efficient sequence-labeling tasks in general.</abstract>
      <url hash="554cc226">2021.naacl-main.264</url>
      <doi>10.18653/v1/2021.naacl-main.264</doi>
      <bibkey>henderson-vulic-2021-convex</bibkey>
      <video href="2021.naacl-main.264.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="265">
      <title><fixed-case>CREAD</fixed-case>: Combined Resolution of Ellipses and Anaphora in Dialogues</title>
      <author><first>Bo-Hsiang</first><last>Tseng</last></author>
      <author><first>Shruti</first><last>Bhargava</last></author>
      <author><first>Jiarui</first><last>Lu</last></author>
      <author><first>Joel Ruben Antony</first><last>Moniz</last></author>
      <author><first>Dhivya</first><last>Piraviperumal</last></author>
      <author><first>Lin</first><last>Li</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>3390–3406</pages>
      <abstract>Anaphora and ellipses are two common phenomena in dialogues. Without resolving referring expressions and information omission, dialogue systems may fail to generate consistent and coherent responses. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a self-contained rewritten user query. To evaluate our model, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3% F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2% F1) on this dataset.</abstract>
      <url hash="c4f5d834">2021.naacl-main.265</url>
      <doi>10.18653/v1/2021.naacl-main.265</doi>
      <bibkey>tseng-etal-2021-cread</bibkey>
      <video href="2021.naacl-main.265.mp4"/>
      <pwccode url="https://github.com/apple/ml-cread" additional="false">apple/ml-cread</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mudoco-queryrewrite">MuDoCo_QueryRewrite</pwcdataset>
    </paper>
    <paper id="266">
      <title>Knowledge-Driven Slot Constraints for Goal-Oriented Dialogue Systems</title>
      <author><first>Piyawat</first><last>Lertvittayakumjorn</last></author>
      <author><first>Daniele</first><last>Bonadiman</last></author>
      <author><first>Saab</first><last>Mansour</last></author>
      <pages>3407–3419</pages>
      <abstract>In goal-oriented dialogue systems, users provide information through slot values to achieve specific goals. Practically, some combinations of slot values can be invalid according to external knowledge. For example, a combination of “cheese pizza” (a menu item) and “oreo cookies” (a topping) from an input utterance “Can I order a cheese pizza with oreo cookies on top?” exemplifies such invalid combinations according to the menu of a restaurant business. Traditional dialogue systems allow execution of validation rules as a post-processing step after slots have been filled which can lead to error accumulation. In this paper, we formalize knowledge-driven slot constraints and present a new task of constraint violation detection accompanied with benchmarking data. Then, we propose methods to integrate the external knowledge into the system and model constraint violation detection as an end-to-end classification task and compare it to the traditional rule-based pipeline approach. Experiments on two domains of the MultiDoGO dataset reveal challenges of constraint violation detection and sets the stage for future work and improvements.</abstract>
      <url hash="b0ec24f5">2021.naacl-main.266</url>
      <doi>10.18653/v1/2021.naacl-main.266</doi>
      <bibkey>lertvittayakumjorn-etal-2021-knowledge</bibkey>
      <video href="2021.naacl-main.266.mp4"/>
      <pwccode url="https://github.com/amazon-research/nlu-slot-constraints" additional="false">amazon-research/nlu-slot-constraints</pwccode>
    </paper>
    <paper id="267">
      <title>Clipping Loops for Sample-Efficient Dialogue Policy Optimisation</title>
      <author><first>Yen-Chen</first><last>Wu</last></author>
      <author><first>Carl Edward</first><last>Rasmussen</last></author>
      <pages>3420–3428</pages>
      <abstract>Training dialogue agents requires a large number of interactions with users: agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages: loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguish useless actions from others and reduce the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80% success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in human evaluation where the agent interactively collects 100 real-user dialogues in training phase.</abstract>
      <url hash="c4316387">2021.naacl-main.267</url>
      <doi>10.18653/v1/2021.naacl-main.267</doi>
      <bibkey>wu-rasmussen-2021-clipping</bibkey>
      <video href="2021.naacl-main.267.mp4"/>
    </paper>
    <paper id="268">
      <title>Integrating Lexical Information into Entity Neighbourhood Representations for Relation Prediction</title>
      <author><first>Ian</first><last>Wood</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <author><first>Stephen</first><last>Wan</last></author>
      <pages>3429–3436</pages>
      <abstract>Relation prediction informed from a combination of text corpora and curated knowledge bases, combining knowledge graph completion with relation extraction, is a relatively little studied task. A system that can perform this task has the ability to extend an arbitrary set of relational database tables with information extracted from a document corpus. OpenKi[1] addresses this task through extraction of named entities and predicates via OpenIE tools then learning relation embeddings from the resulting entity-relation graph for relation prediction, outperforming previous approaches. We present an extension of OpenKi that incorporates embeddings of text-based representations of the entities and the relations. We demonstrate that this results in a substantial performance increase over a system without this information.</abstract>
      <url hash="5a78dc10">2021.naacl-main.268</url>
      <doi>10.18653/v1/2021.naacl-main.268</doi>
      <bibkey>wood-etal-2021-integrating</bibkey>
      <video href="2021.naacl-main.268.mp4"/>
      <pwccode url="https://github.com/drevicko/openki" additional="false">drevicko/openki</pwccode>
    </paper>
    <paper id="269">
      <title>Noisy-Labeled <fixed-case>NER</fixed-case> with Confidence Estimation</title>
      <author><first>Kun</first><last>Liu</last></author>
      <author><first>Yao</first><last>Fu</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Mosha</first><last>Chen</last></author>
      <author><first>Ningyu</first><last>Zhang</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Sheng</first><last>Gao</last></author>
      <pages>3437–3445</pages>
      <abstract>Recent studies in deep learning have shown significant progress in named entity recognition (NER). However, most existing works assume clean data annotation, while real-world scenarios typically involve a large amount of noises from a variety of sources (e.g., pseudo, weak, or distant annotations). This work studies NER under a noisy labeled setting with calibrated confidence estimation. Based on empirical observations of different training dynamics of noisy and clean labels, we propose strategies for estimating confidence scores based on local and global independence assumptions. We partially marginalize out labels of low confidence with a CRF model. We further propose a calibration method for confidence scores based on the structure of entity labels. We integrate our approach into a self-training framework for boosting performance. Experiments in general noisy settings with four languages and distantly labeled settings demonstrate the effectiveness of our method.</abstract>
      <url hash="7442f734">2021.naacl-main.269</url>
      <doi>10.18653/v1/2021.naacl-main.269</doi>
      <bibkey>liu-etal-2021-noisy-labeled</bibkey>
      <video href="2021.naacl-main.269.mp4"/>
      <pwccode url="https://github.com/liukun95/Noisy-NER-Confidence-Estimation" additional="false">liukun95/Noisy-NER-Confidence-Estimation</pwccode>
    </paper>
    <paper id="270">
      <title><fixed-case>TABBIE</fixed-case>: Pretrained Representations of Tabular Data</title>
      <author><first>Hiroshi</first><last>Iida</last></author>
      <author><first>Dung</first><last>Thai</last></author>
      <author><first>Varun</first><last>Manjunatha</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>3446–3456</pages>
      <abstract>Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model’s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.</abstract>
      <url hash="0bf6dd05">2021.naacl-main.270</url>
      <doi>10.18653/v1/2021.naacl-main.270</doi>
      <bibkey>iida-etal-2021-tabbie</bibkey>
      <video href="2021.naacl-main.270.mp4"/>
      <pwccode url="https://github.com/SFIG611/tabbie" additional="false">SFIG611/tabbie</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/viznet-sato">VizNet-Sato</pwcdataset>
    </paper>
    <paper id="271">
      <title>Better Feature Integration for Named Entity Recognition</title>
      <author><first>Lu</first><last>Xu</last></author>
      <author><first>Zhanming</first><last>Jie</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <pages>3457–3469</pages>
      <abstract>It has been shown that named entity recognition (NER) could benefit from incorporating the long-distance structured information captured by dependency trees. We believe this is because both types of features - the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other. However, existing approaches largely focused on stacking the LSTM and graph neural networks such as graph convolutional networks (GCNs) for building improved NER models, where the exact interaction mechanism between the two types of features is not very clear, and the performance gain does not appear to be significant. In this work, we propose a simple and robust solution to incorporate both types of features with our Synergized-LSTM (Syn-LSTM), which clearly captures how the two types of features interact. We conduct extensive experiments on several standard datasets across four languages. The results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters. Our further analysis demonstrates that our model can capture longer dependencies compared with strong baselines.</abstract>
      <url hash="f81a1a76">2021.naacl-main.271</url>
      <doi>10.18653/v1/2021.naacl-main.271</doi>
      <bibkey>xu-etal-2021-better</bibkey>
      <video href="2021.naacl-main.271.mp4"/>
      <pwccode url="https://github.com/xuuuluuu/SynLSTM-for-NER" additional="false">xuuuluuu/SynLSTM-for-NER</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="272">
      <title><fixed-case>ZS</fixed-case>-<fixed-case>BERT</fixed-case>: Towards Zero-Shot Relation Extraction with Attribute Representation Learning</title>
      <author><first>Chih-Yao</first><last>Chen</last></author>
      <author><first>Cheng-Te</first><last>Li</last></author>
      <pages>3470–3479</pages>
      <abstract>While relation extraction is an essential task in knowledge acquisition and representation, and new-generated relations are common in the real world, less effort is made to predict unseen relations that cannot be observed at the training stage. In this paper, we formulate the zero-shot relation extraction problem by incorporating the text description of seen and unseen relations. We propose a novel multi-task learning model, Zero-Shot BERT (ZS-BERT), to directly predict unseen relations without hand-crafted attribute labeling and multiple pairwise classifications. Given training instances consisting of input sentences and the descriptions of their seen relations, ZS-BERT learns two functions that project sentences and relations into an embedding space by jointly minimizing the distances between them and classifying seen relations. By generating the embeddings of unseen relations and new-coming sentences based on such two functions, we use nearest neighbor search to obtain the prediction of unseen relations. Experiments conducted on two well-known datasets exhibit that ZS-BERT can outperform existing methods by at least 13.54% improvement on F1 score.</abstract>
      <url hash="8c77e646">2021.naacl-main.272</url>
      <doi>10.18653/v1/2021.naacl-main.272</doi>
      <bibkey>chen-li-2021-zs</bibkey>
      <video href="2021.naacl-main.272.mp4"/>
      <pwccode url="https://github.com/dinobby/ZS-BERT" additional="false">dinobby/ZS-BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wiki-zsl">Wiki-ZSL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="273">
      <title>Graph Convolutional Networks for Event Causality Identification with Rich Document-level Structures</title>
      <author><first>Minh</first><last>Tran Phu</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>3480–3490</pages>
      <abstract>We study the problem of Event Causality Identification (ECI) to detect causal relation between event mention pairs in text. Although deep learning models have recently shown state-of-the-art performance for ECI, they are limited to the intra-sentence setting where event mention pairs are presented in the same sentences. This work addresses this issue by developing a novel deep learning model for document-level ECI (DECI) to accept inter-sentence event mention pairs. As such, we propose a graph-based model that constructs interaction graphs to capture relevant connections between important objects for DECI in input documents. Such interaction graphs are then consumed by graph convolutional networks to learn document context-augmented representations for causality prediction between events. Various information sources are introduced to enrich the interaction graphs for DECI, featuring discourse, syntax, and semantic information. Our extensive experiments show that the proposed model achieves state-of-the-art performance on two benchmark datasets.</abstract>
      <url hash="ee818809">2021.naacl-main.273</url>
      <doi>10.18653/v1/2021.naacl-main.273</doi>
      <bibkey>tran-phu-nguyen-2021-graph</bibkey>
      <video href="2021.naacl-main.273.mp4"/>
    </paper>
    <paper id="274">
      <title>A Context-Dependent Gated Module for Incorporating Symbolic Semantics into Event Coreference Resolution</title>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Quan Hung</first><last>Tran</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Walter</first><last>Chang</last></author>
      <pages>3491–3499</pages>
      <abstract>Event coreference resolution is an important research problem with many applications. Despite the recent remarkable success of pre-trained language models, we argue that it is still highly beneficial to utilize symbolic features for the task. However, as the input for coreference resolution typically comes from upstream components in the information extraction pipeline, the automatically extracted symbolic features can be noisy and contain errors. Also, depending on the specific context, some features can be more informative than others. Motivated by these observations, we propose a novel context-dependent gated module to adaptively control the information flows from the input symbolic features. Combined with a simple noisy training method, our best models achieve state-of-the-art results on two datasets: ACE 2005 and KBP 2016.</abstract>
      <url hash="2053025f">2021.naacl-main.274</url>
      <doi>10.18653/v1/2021.naacl-main.274</doi>
      <bibkey>lai-etal-2021-context</bibkey>
      <video href="2021.naacl-main.274.mp4"/>
      <pwccode url="https://github.com/laituan245/eventcoref" additional="false">laituan245/eventcoref</pwccode>
    </paper>
    <paper id="275">
      <title>Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus</title>
      <author><first>Navita</first><last>Goyal</last></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last></author>
      <author><first>Anandhavelu</first><last>N</last></author>
      <author><first>Abhilasha</first><last>Sancheti</last></author>
      <pages>3500–3510</pages>
      <abstract>Style transfer has been widely explored in natural language generation with non-parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration. Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models across multiple styles is a possibility, it suffers from content loss, especially when the style dimensions are not completely independent of each other. In our work, we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations. We initialize an encoder-decoder setup with transformer-based language model pre-trained on a generic corpus and enhance its re-writing capability to multiple target style dimensions by employing multiple style-aware language models as discriminators. Through quantitative and qualitative evaluation, we show the ability of our model to control styles across multiple style dimensions while preserving content of the input text. We compare it against baselines involving cascaded state-of-the-art uni-dimensional style transfer models.</abstract>
      <url hash="b8e9565c">2021.naacl-main.275</url>
      <doi>10.18653/v1/2021.naacl-main.275</doi>
      <bibkey>goyal-etal-2021-multi</bibkey>
      <video href="2021.naacl-main.275.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="276">
      <title><fixed-case>FUDGE</fixed-case>: Controlled Text Generation With Future Discriminators</title>
      <author><first>Kevin</first><last>Yang</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>3511–3535</pages>
      <abstract>We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G’s output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor’s outputs to adjust G’s original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks — couplet completion in poetry, topic control in language generation, and formality change in machine translation — and observe gains in all three tasks.</abstract>
      <url hash="30c68c02">2021.naacl-main.276</url>
      <doi>10.18653/v1/2021.naacl-main.276</doi>
      <bibkey>yang-klein-2021-fudge</bibkey>
      <video href="2021.naacl-main.276.mp4"/>
      <pwccode url="https://github.com/yangkevin2/naacl-2021-fudge-controlled-generation" additional="true">yangkevin2/naacl-2021-fudge-controlled-generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="277">
      <title>Controllable Text Simplification with Explicit Paraphrasing</title>
      <author><first>Mounica</first><last>Maddela</last></author>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <pages>3536–3553</pages>
      <abstract>Text Simplification improves the readability of sentences through several rewriting transformations, such as lexical paraphrasing, deletion, and splitting. Current simplification systems are predominantly sequence-to-sequence models that are trained end-to-end to perform all these operations simultaneously. However, such systems limit themselves to mostly deleting words and cannot easily adapt to the requirements of different target audiences. In this paper, we propose a novel hybrid approach that leverages linguistically-motivated rules for splitting and deletion, and couples them with a neural paraphrasing model to produce varied rewriting styles. We introduce a new data augmentation method to improve the paraphrasing capability of our model. Through automatic and manual evaluations, we show that our proposed model establishes a new state-of-the-art for the task, paraphrasing more often than the existing systems, and can control the degree of each simplification operation applied to the input texts.</abstract>
      <url hash="57919713">2021.naacl-main.277</url>
      <doi>10.18653/v1/2021.naacl-main.277</doi>
      <bibkey>maddela-etal-2021-controllable</bibkey>
      <video href="2021.naacl-main.277.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="278">
      <title>Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training</title>
      <author><first>Oshin</first><last>Agarwal</last></author>
      <author><first>Heming</first><last>Ge</last></author>
      <author><first>Siamak</first><last>Shakeri</last></author>
      <author><first>Rami</first><last>Al-Rfou</last></author>
      <pages>3554–3565</pages>
      <abstract>Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.</abstract>
      <url hash="cf99df86">2021.naacl-main.278</url>
      <doi>10.18653/v1/2021.naacl-main.278</doi>
      <bibkey>agarwal-etal-2021-knowledge</bibkey>
      <video href="2021.naacl-main.278.mp4"/>
      <pwccode url="https://github.com/google-research-datasets/KELM-corpus" additional="false">google-research-datasets/KELM-corpus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kelm">KELM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tekgen">TekGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="279">
      <title>Choose Your Own Adventure: Paired Suggestions in Collaborative Writing for Evaluating Story Generation Models</title>
      <author><first>Elizabeth</first><last>Clark</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>3566–3575</pages>
      <abstract>Story generation is an open-ended and subjective task, which poses a challenge for evaluating story generation models. We present Choose Your Own Adventure, a collaborative writing setup for pairwise model evaluation. Two models generate suggestions to people as they write a short story; we ask writers to choose one of the two suggestions, and we observe which model’s suggestions they prefer. The setup also allows further analysis based on the revisions people make to the suggestions. We show that these measures, combined with automatic metrics, provide an informative picture of the models’ performance, both in cases where the differences in generation methods are small (nucleus vs. top-k sampling) and large (GPT2 vs. Fusion models).</abstract>
      <url hash="de113163">2021.naacl-main.279</url>
      <doi>10.18653/v1/2021.naacl-main.279</doi>
      <bibkey>clark-smith-2021-choose</bibkey>
      <video href="2021.naacl-main.279.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="280">
      <title><fixed-case>I</fixed-case>nfo<fixed-case>XLM</fixed-case>: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training</title>
      <author><first>Zewen</first><last>Chi</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Nan</first><last>Yang</last></author>
      <author><first>Saksham</first><last>Singhal</last></author>
      <author><first>Wenhui</first><last>Wang</last></author>
      <author><first>Xia</first><last>Song</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>3576–3588</pages>
      <abstract>In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm.</abstract>
      <url hash="c7fdf3ea">2021.naacl-main.280</url>
      <doi>10.18653/v1/2021.naacl-main.280</doi>
      <bibkey>chi-etal-2021-infoxlm</bibkey>
      <video href="2021.naacl-main.280.mp4"/>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xtreme">XTREME</pwcdataset>
    </paper>
    <paper id="281">
      <title>Context-Interactive Pre-Training for Document Machine Translation</title>
      <author><first>Pengcheng</first><last>Yang</last></author>
      <author><first>Pei</first><last>Zhang</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Weihua</first><last>Luo</last></author>
      <pages>3589–3595</pages>
      <abstract>Document machine translation aims to translate the source sentence into the target language in the presence of additional contextual information. However, it typically suffers from a lack of doc-level bilingual data. To remedy this, here we propose a simple yet effective context-interactive pre-training approach, which targets benefiting from external large-scale corpora. The proposed model performs inter sentence generation to capture the cross-sentence dependency within the target document, and cross sentence translation to make better use of valuable contextual information. Comprehensive experiments illustrate that our approach can achieve state-of-the-art performance on three benchmark datasets, which significantly outperforms a variety of baselines.</abstract>
      <url hash="a81d1c7a">2021.naacl-main.281</url>
      <doi>10.18653/v1/2021.naacl-main.281</doi>
      <bibkey>yang-etal-2021-context</bibkey>
    </paper>
    <paper id="282">
      <title>Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots</title>
      <author><first>Samson</first><last>Tan</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <pages>3596–3616</pages>
      <abstract>Multilingual models have demonstrated impressive cross-lingual transfer performance. However, test sets like XNLI are monolingual at the example level. In multilingual communities, it is common for polyglots to code-mix when conversing with each other. Inspired by this phenomenon, we present two strong black-box adversarial attacks (one word-level, one phrase-level) for multilingual models that push their ability to handle code-mixed sentences to the limit. The former uses bilingual dictionaries to propose perturbations and translations of the clean example for sense disambiguation. The latter directly aligns the clean example with its translations before extracting phrases as perturbations. Our phrase-level attack has a success rate of 89.75% against XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI. Finally, we propose an efficient adversarial training scheme that trains in the same number of steps as the original model and show that it creates more language-invariant representations, improving clean and robust accuracy in the absence of lexical overlap without degrading performance on the original examples.</abstract>
      <url hash="6f01806e">2021.naacl-main.282</url>
      <doi>10.18653/v1/2021.naacl-main.282</doi>
      <bibkey>tan-joty-2021-code</bibkey>
      <revision id="1" href="2021.naacl-main.282v1" hash="14ca3ca7"/>
      <revision id="2" href="2021.naacl-main.282v2" hash="6f01806e" date="2021-06-28">Fixed typo in Table 6</revision>
      <video href="2021.naacl-main.282.mp4"/>
    </paper>
    <paper id="283">
      <title><fixed-case>X</fixed-case>-<fixed-case>METRA</fixed-case>-<fixed-case>ADA</fixed-case>: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</title>
      <author><first>Meryem</first><last>M’hamdi</last></author>
      <author><first>Doo Soon</first><last>Kim</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <pages>3617–3632</pages>
      <abstract>Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</abstract>
      <url hash="adc0d29d">2021.naacl-main.283</url>
      <doi>10.18653/v1/2021.naacl-main.283</doi>
      <bibkey>mhamdi-etal-2021-x</bibkey>
      <video href="2021.naacl-main.283.mp4"/>
      <pwccode url="https://github.com/meryemmhamdi1/meta_cross_nlu_qa" additional="false">meryemmhamdi1/meta_cross_nlu_qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydiqa-goldp">TyDiQA-GoldP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="284">
      <title>Explicit Alignment Objectives for Multilingual Bidirectional Encoders</title>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Melvin</first><last>Johnson</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Aditya</first><last>Siddhant</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>3633–3643</pages>
      <abstract>Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zero-shot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results on the tasks in the XTREME benchmark (Hu et al., 2020) show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLM-R-large model which has 3.2x the parameters of AMBER. Our code and models are available at http://github.com/junjiehu/amber.</abstract>
      <url hash="079276c9">2021.naacl-main.284</url>
      <doi>10.18653/v1/2021.naacl-main.284</doi>
      <bibkey>hu-etal-2021-explicit</bibkey>
      <video href="2021.naacl-main.284.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tatoeba">Tatoeba</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="285">
      <title>Cross-lingual Cross-modal Pretraining for Multimodal Retrieval</title>
      <author><first>Hongliang</first><last>Fei</last></author>
      <author><first>Tan</first><last>Yu</last></author>
      <author><first>Ping</first><last>Li</last></author>
      <pages>3644–3650</pages>
      <abstract>Recent pretrained vision-language models have achieved impressive performance on cross-modal retrieval tasks in English. Their success, however, heavily depends on the availability of many annotated image-caption datasets for pretraining, where the texts are not necessarily in English. Although we can utilize machine translation (MT) tools to translate non-English text to English, the performance still largely relies on MT’s quality and may suffer from high latency problems in real-world applications. This paper proposes a new approach to learn cross-lingual cross-modal representations for matching images and their relevant captions in multiple languages. We seamlessly combine cross-lingual pretraining objectives and cross-modal pretraining objectives in a unified framework to learn image and text in a joint embedding space from available English image-caption data, monolingual and parallel corpus. We show that our approach achieves SOTA performance in retrieval tasks on two multimodal multilingual image caption benchmarks: Multi30k with German captions and MSCOCO with Japanese captions.</abstract>
      <url hash="f8eba6c3">2021.naacl-main.285</url>
      <doi>10.18653/v1/2021.naacl-main.285</doi>
      <bibkey>fei-etal-2021-cross</bibkey>
      <video href="2021.naacl-main.285.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="286">
      <title><fixed-case>W</fixed-case>ikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting <fixed-case>W</fixed-case>ikipedia Hyperlinks</title>
      <author><first>Iacer</first><last>Calixto</last></author>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Tommaso</first><last>Pasini</last></author>
      <pages>3651–3661</pages>
      <abstract>Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexical-semantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages.</abstract>
      <url hash="c31f30e1">2021.naacl-main.286</url>
      <doi>10.18653/v1/2021.naacl-main.286</doi>
      <bibkey>calixto-etal-2021-wikipedia</bibkey>
      <video href="2021.naacl-main.286.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xl-wic">XL-WiC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="287">
      <title>multi<fixed-case>PR</fixed-case>over: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning</title>
      <author><first>Swarnadeep</first><last>Saha</last></author>
      <author><first>Prateek</first><last>Yadav</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>3662–3677</pages>
      <abstract>We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent.</abstract>
      <url hash="fb1552c8">2021.naacl-main.287</url>
      <doi>10.18653/v1/2021.naacl-main.287</doi>
      <bibkey>saha-etal-2021-multiprover</bibkey>
      <video href="2021.naacl-main.287.mp4"/>
      <pwccode url="https://github.com/swarnaHub/multiPRover" additional="false">swarnaHub/multiPRover</pwccode>
    </paper>
    <paper id="288">
      <title>Adaptable and Interpretable Neural <fixed-case>M</fixed-case>emory<fixed-case>O</fixed-case>ver Symbolic Knowledge</title>
      <author><first>Pat</first><last>Verga</last></author>
      <author><first>Haitian</first><last>Sun</last></author>
      <author><first>Livio</first><last>Baldini Soares</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <pages>3678–3691</pages>
      <abstract>Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a “fact memory”. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5% of the parameters. Most interestingly, we demonstrate that the model can be modified, without <i>any</i> re-training, by updating the fact memory.</abstract>
      <url hash="bb08d5c1">2021.naacl-main.288</url>
      <doi>10.18653/v1/2021.naacl-main.288</doi>
      <bibkey>verga-etal-2021-adaptable</bibkey>
      <video href="2021.naacl-main.288.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="289">
      <title><fixed-case>CLEVR</fixed-case>_<fixed-case>HYP</fixed-case>: A Challenge Dataset and Baselines for Visual Question Answering with Hypothetical Actions over Images</title>
      <author><first>Shailaja Keyur</first><last>Sampat</last></author>
      <author><first>Akshay</first><last>Kumar</last></author>
      <author><first>Yezhou</first><last>Yang</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>3692–3709</pages>
      <abstract>Most existing research on visual question answering (VQA) is limited to information explicitly present in an image or a video. In this paper, we take visual understanding to a higher level where systems are challenged to answer questions that involve mentally simulating the hypothetical consequences of performing specific actions in a given scenario. Towards that end, we formulate a vision-language question answering task based on the CLEVR (Johnson et. al., 2017) dataset. We then modify the best existing VQA methods and propose baseline solvers for this task. Finally, we motivate the development of better vision-language models by providing insights about the capability of diverse architectures to perform joint reasoning over image-text modality. Our dataset setup scripts and codes will be made publicly available at https://github.com/shailaja183/clevr_hyp.</abstract>
      <url hash="076d7b41">2021.naacl-main.289</url>
      <attachment type="OptionalSupplementaryData" hash="8e83d44d">2021.naacl-main.289.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.289</doi>
      <bibkey>sampat-etal-2021-clevr</bibkey>
      <pwccode url="https://github.com/shailaja183/clevr_hyp" additional="false">shailaja183/clevr_hyp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/shapes-1">SHAPES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tqa">TQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="290">
      <title>Refining Targeted Syntactic Evaluation of Language Models</title>
      <author><first>Benjamin</first><last>Newman</last></author>
      <author><first>Kai-Siang</first><last>Ang</last></author>
      <author><first>Julia</first><last>Gong</last></author>
      <author><first>John</first><last>Hewitt</last></author>
      <pages>3710–3723</pages>
      <abstract>Targeted syntactic evaluation of subject-verb number agreement in English (TSE) evaluates language models’ syntactic knowledge using hand-crafted minimal pairs of sentences that differ only in the main verb’s conjugation. The method evaluates whether language models rate each grammatical sentence as more likely than its ungrammatical counterpart. We identify two distinct goals for TSE. First, evaluating the systematicity of a language model’s syntactic knowledge: given a sentence, can it conjugate arbitrary verbs correctly? Second, evaluating a model’s likely behavior: given a sentence, does the model concentrate its probability mass on correctly conjugated verbs, even if only on a subset of the possible verbs? We argue that current implementations of TSE do not directly capture either of these goals, and propose new metrics to capture each goal separately. Under our metrics, we find that TSE overestimates systematicity of language models, but that models score up to 40% better on verbs that they predict are likely in context.</abstract>
      <url hash="38d5fdda">2021.naacl-main.290</url>
      <doi>10.18653/v1/2021.naacl-main.290</doi>
      <bibkey>newman-etal-2021-refining</bibkey>
      <video href="2021.naacl-main.290.mp4"/>
      <pwccode url="https://github.com/bnewm0609/refining-tse" additional="false">bnewm0609/refining-tse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blimp">BLiMP</pwcdataset>
    </paper>
    <paper id="291">
      <title>Universal Adversarial Attacks with Natural Triggers for Text Classification</title>
      <author><first>Liwei</first><last>Song</last></author>
      <author><first>Xinwei</first><last>Yu</last></author>
      <author><first>Hsuan-Tung</first><last>Peng</last></author>
      <author><first>Karthik</first><last>Narasimhan</last></author>
      <pages>3724–3733</pages>
      <abstract>Recent work has demonstrated the vulnerability of modern text classifiers to universal adversarial attacks, which are input-agnostic sequences of words added to text processed by classifiers. Despite being successful, the word sequences produced in such attacks are often ungrammatical and can be easily distinguished from natural text. We develop adversarial attacks that appear closer to natural English phrases and yet confuse classification systems when added to benign inputs. We leverage an adversarially regularized autoencoder (ARAE) to generate triggers and propose a gradient-based search that aims to maximize the downstream classifier’s prediction loss. Our attacks effectively reduce model accuracy on classification tasks while being less identifiable than prior models as per automatic detection metrics and human-subject studies. Our aim is to demonstrate that adversarial attacks can be made harder to detect than previously thought and to enable the development of appropriate defenses.</abstract>
      <url hash="1dc67b3a">2021.naacl-main.291</url>
      <doi>10.18653/v1/2021.naacl-main.291</doi>
      <bibkey>song-etal-2021-universal</bibkey>
      <video href="2021.naacl-main.291.mp4"/>
      <pwccode url="https://github.com/Hsuan-Tung/universal_attack_natural_trigger" additional="false">Hsuan-Tung/universal_attack_natural_trigger</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="292">
      <title><fixed-case>Q</fixed-case>uadruplet<fixed-case>BERT</fixed-case>: An Efficient Model For Embedding-Based Large-Scale Retrieval</title>
      <author><first>Peiyang</first><last>Liu</last></author>
      <author><first>Sen</first><last>Wang</last></author>
      <author><first>Xi</first><last>Wang</last></author>
      <author><first>Wei</first><last>Ye</last></author>
      <author><first>Shikun</first><last>Zhang</last></author>
      <pages>3734–3739</pages>
      <abstract>The embedding-based large-scale query-document retrieval problem is a hot topic in the information retrieval (IR) field. Considering that pre-trained language models like BERT have achieved great success in a wide variety of NLP tasks, we present a QuadrupletBERT model for effective and efficient retrieval in this paper. Unlike most existing BERT-style retrieval models, which only focus on the ranking phase in retrieval systems, our model makes considerable improvements to the retrieval phase and leverages the distances between simple negative and hard negative instances to obtaining better embeddings. Experimental results demonstrate that our QuadrupletBERT achieves state-of-the-art results in embedding-based large-scale retrieval tasks.</abstract>
      <url hash="a622b285">2021.naacl-main.292</url>
      <doi>10.18653/v1/2021.naacl-main.292</doi>
      <bibkey>liu-etal-2021-quadrupletbert</bibkey>
      <video href="2021.naacl-main.292.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/reqa">ReQA</pwcdataset>
    </paper>
    <paper id="293">
      <title>Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack</title>
      <author><first>Liwen</first><last>Wang</last></author>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>3740–3750</pages>
      <abstract>Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of debiasing and the main task performance.</abstract>
      <url hash="17ba7069">2021.naacl-main.293</url>
      <doi>10.18653/v1/2021.naacl-main.293</doi>
      <bibkey>wang-etal-2021-dynamically</bibkey>
      <video href="2021.naacl-main.293.mp4"/>
      <pwccode url="https://github.com/w-lw/debias_adv" additional="false">w-lw/debias_adv</pwccode>
    </paper>
    <paper id="294">
      <title>An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Arshiya</first><last>Aggarwal</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>3751–3757</pages>
      <abstract>Volatility prediction is complex due to the stock market’s stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives’ speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous state-of-the-art approaches demonstrating the benefits of multimodality and speech. However, the financial realm is still plagued with a severe underrepresentation of various communities spanning diverse demographics, gender, and native speech. While multimodal models are better risk forecasters, it is imperative to also investigate the potential bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal volatility prediction due to gender-sensitive audio features and fewer female executives in earnings calls of one of the world’s biggest stock indexes, the S&amp;P 500 index. We quantitatively analyze bias as error disparity and investigate the sources of this bias. Our results suggest that multimodal neural financial models accentuate gender-based stereotypes.</abstract>
      <url hash="6944fd3e">2021.naacl-main.294</url>
      <doi>10.18653/v1/2021.naacl-main.294</doi>
      <bibkey>sawhney-etal-2021-empirical</bibkey>
      <video href="2021.naacl-main.294.mp4"/>
      <pwccode url="https://github.com/midas-research/multimodal-bias-naacl" additional="false">midas-research/multimodal-bias-naacl</pwccode>
    </paper>
    <paper id="295">
      <title>Beyond Fair Pay: Ethical Implications of <fixed-case>NLP</fixed-case> Crowdsourcing</title>
      <author><first>Boaz</first><last>Shmueli</last></author>
      <author><first>Jan</first><last>Fell</last></author>
      <author><first>Soumya</first><last>Ray</last></author>
      <author><first>Lun-Wei</first><last>Ku</last></author>
      <pages>3758–3769</pages>
      <abstract>The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers.</abstract>
      <url hash="0a803eaf">2021.naacl-main.295</url>
      <doi>10.18653/v1/2021.naacl-main.295</doi>
      <bibkey>shmueli-etal-2021-beyond</bibkey>
      <video href="2021.naacl-main.295.mp4"/>
    </paper>
    <paper id="296">
      <title>On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning</title>
      <author><first>Xisen</first><last>Jin</last></author>
      <author><first>Francesco</first><last>Barbieri</last></author>
      <author><first>Brendan</first><last>Kennedy</last></author>
      <author><first>Aida</first><last>Mostafazadeh Davani</last></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>3770–3783</pages>
      <abstract>Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task and domain at hand, the effects of bias mitigation may not directly transfer to new tasks, requiring additional data collection and customized annotation of sensitive attributes, and re-evaluation of appropriate fairness metrics. We explore the feasibility and benefits of upstream bias mitigation (UBM) for reducing bias on downstream tasks, by first applying bias mitigation to an upstream model through fine-tuning and subsequently using it for downstream fine-tuning. We find, in extensive experiments across hate speech detection, toxicity detection and coreference resolution tasks over various bias factors, that the effects of UBM are indeed transferable to new downstream tasks or domains via fine-tuning, creating less biased downstream models than directly fine-tuning on the downstream task or transferring from a vanilla upstream model. Though challenges remain, we show that UBM promises more efficient and accessible bias mitigation in LM fine-tuning.</abstract>
      <url hash="195089a3">2021.naacl-main.296</url>
      <doi>10.18653/v1/2021.naacl-main.296</doi>
      <bibkey>jin-etal-2021-transferability</bibkey>
      <video href="2021.naacl-main.296.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
    <paper id="297">
      <title>Case Study: Deontological Ethics in <fixed-case>NLP</fixed-case></title>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>Brendon</first><last>Boldt</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <pages>3784–3798</pages>
      <abstract>Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.</abstract>
      <url hash="387fce63">2021.naacl-main.297</url>
      <doi>10.18653/v1/2021.naacl-main.297</doi>
      <bibkey>prabhumoye-etal-2021-case</bibkey>
      <video href="2021.naacl-main.297.mp4"/>
    </paper>
    <paper id="298">
      <title>Privacy Regularization: Joint Privacy-Utility Optimization in <fixed-case>L</fixed-case>anguage<fixed-case>M</fixed-case>odels</title>
      <author><first>Fatemehsadat</first><last>Mireshghallah</last></author>
      <author><first>Huseyin</first><last>Inan</last></author>
      <author><first>Marcello</first><last>Hasegawa</last></author>
      <author><first>Victor</first><last>Rühle</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <author><first>Robert</first><last>Sim</last></author>
      <pages>3799–3807</pages>
      <abstract>Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy im- plications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacy-preserving regularization methods for training language models that enable joint optimization of utility and privacy through (1) the use of a discriminator and (2) the inclusion of a novel triplet-loss term. We compare our methods with DP through extensive evaluation. We show the advantages of our regularizers with favorable utility-privacy trade-off, faster training with the ability to tap into existing optimization approaches, and ensuring uniform treatment of under-represented subgroups.</abstract>
      <url hash="67b3de54">2021.naacl-main.298</url>
      <doi>10.18653/v1/2021.naacl-main.298</doi>
      <bibkey>mireshghallah-etal-2021-privacy</bibkey>
      <video href="2021.naacl-main.298.mp4"/>
    </paper>
    <paper id="299">
      <title>On the Impact of Random Seeds on the Fairness of Clinical Classifiers</title>
      <author><first>Silvio</first><last>Amir</last></author>
      <author><first>Jan-Willem</first><last>van de Meent</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>3808–3823</pages>
      <abstract>Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III —— the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of minority groups and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from stochasticity and small sample sizes.</abstract>
      <url hash="3a511f1a">2021.naacl-main.299</url>
      <doi>10.18653/v1/2021.naacl-main.299</doi>
      <bibkey>amir-etal-2021-impact</bibkey>
      <video href="2021.naacl-main.299.mp4"/>
    </paper>
    <paper id="300">
      <title>Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures</title>
      <author><first>Caitlin</first><last>Doogan</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <pages>3824–3848</pages>
      <abstract>When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models’ generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of interpretability. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using open labeling, typical of applied research. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data.</abstract>
      <url hash="93c50f31">2021.naacl-main.300</url>
      <attachment type="OptionalSupplementaryData" hash="3527f5e9">2021.naacl-main.300.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.300</doi>
      <bibkey>doogan-buntine-2021-topic</bibkey>
      <video href="2021.naacl-main.300.mp4"/>
    </paper>
    <paper id="301">
      <title>Discourse Probing of Pretrained Language Models</title>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>3849–3864</pages>
      <abstract>Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse — but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models.</abstract>
      <url hash="6622a4a5">2021.naacl-main.301</url>
      <doi>10.18653/v1/2021.naacl-main.301</doi>
      <bibkey>koto-etal-2021-discourse</bibkey>
      <video href="2021.naacl-main.301.mp4"/>
      <pwccode url="https://github.com/fajri91/discourse_probing" additional="false">fajri91/discourse_probing</pwccode>
    </paper>
    <paper id="302">
      <title><fixed-case>U</fixed-case>ni<fixed-case>D</fixed-case>rop: A Simple yet Effective Technique to Improve Transformer without Extra Cost</title>
      <author><first>Zhen</first><last>Wu</last></author>
      <author><first>Lijun</first><last>Wu</last></author>
      <author><first>Qi</first><last>Meng</last></author>
      <author><first>Yingce</first><last>Xia</last></author>
      <author><first>Shufang</first><last>Xie</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <pages>3865–3878</pages>
      <abstract>Transformer architecture achieves great success in abundant natural language processing tasks. The over-parameterization of the Transformer model has motivated plenty of works to alleviate its overfitting for superior performances. With some explorations, we find simple techniques such as dropout, can greatly boost model performance with a careful design. Therefore, in this paper, we integrate different dropout techniques into the training of Transformer models. Specifically, we propose an approach named UniDrop to unites three different dropout techniques from fine-grain to coarse-grain, i.e., feature dropout, structure dropout, and data dropout. Theoretically, we demonstrate that these three dropouts play different roles from regularization perspectives. Empirically, we conduct experiments on both neural machine translation and text classification benchmark datasets. Extensive results indicate that Transformer with UniDrop can achieve around 1.5 BLEU improvement on IWSLT14 translation tasks, and better accuracy for the classification even using strong pre-trained RoBERTa as backbone.</abstract>
      <url hash="bb3168c2">2021.naacl-main.302</url>
      <doi>10.18653/v1/2021.naacl-main.302</doi>
      <bibkey>wu-etal-2021-unidrop</bibkey>
      <video href="2021.naacl-main.302.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="303">
      <title>t<fixed-case>WT</fixed-case>–<fixed-case>WT</fixed-case>: A Dataset to Assert the Role of Target Entities for Detecting Stance of Tweets</title>
      <author><first>Ayush</first><last>Kaushal</last></author>
      <author><first>Avirup</first><last>Saha</last></author>
      <author><first>Niloy</first><last>Ganguly</last></author>
      <pages>3879–3889</pages>
      <abstract>The stance detection task aims at detecting the stance of a tweet or a text for a target. These targets can be named entities or free-form sentences (claims). Though the task involves reasoning of the tweet with respect to a target, we find that it is possible to achieve high accuracy on several publicly available Twitter stance detection datasets without looking at the target sentence. Specifically, a simple tweet classification model achieved human-level performance on the WT–WT dataset and more than two-third accuracy on various other datasets. We investigate the existence of biases in such datasets to find the potential spurious correlations of sentiment-stance relations and lexical choice associated with the stance category. Furthermore, we propose a new large dataset free of such biases and demonstrate its aptness on the existing stance detection systems. Our empirical findings show much scope for research on the stance detection task and proposes several considerations for creating future stance detection datasets.</abstract>
      <url hash="125a58a8">2021.naacl-main.303</url>
      <attachment type="OptionalSupplementaryCode" hash="5eec7ccd">2021.naacl-main.303.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="7ed02db9">2021.naacl-main.303.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.303</doi>
      <bibkey>kaushal-etal-2021-twt</bibkey>
      <video href="2021.naacl-main.303.mp4"/>
      <pwccode url="https://github.com/Ayushk4/bias-stance" additional="true">Ayushk4/bias-stance</pwccode>
    </paper>
    <paper id="304">
      <title>Learning to Learn to be Right for the Right Reasons</title>
      <author><first>Pride</first><last>Kavumba</last></author>
      <author><first>Benjamin</first><last>Heinzerling</last></author>
      <author><first>Ana</first><last>Brassard</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>3890–3898</pages>
      <abstract>Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the methods have improved performance on hard instances, they also lead to degraded performance on easy in- stances. Here, we propose to explicitly learn a model that does well on both the easy test set with superficial cues and the hard test set without superficial cues. Using a meta-learning objective, we learn such a model that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline.</abstract>
      <url hash="1ef72762">2021.naacl-main.304</url>
      <doi>10.18653/v1/2021.naacl-main.304</doi>
      <bibkey>kavumba-etal-2021-learning</bibkey>
      <video href="2021.naacl-main.304.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="305">
      <title>Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation</title>
      <author><first>Chong</first><last>Zhang</last></author>
      <author><first>Jieyu</first><last>Zhao</last></author>
      <author><first>Huan</first><last>Zhang</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <pages>3899–3916</pages>
      <abstract>Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a “double perturbation” framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models’ robustness and counterfactual bias in English. (1) For robustness, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed attack attains high success rates (96.0%-99.8%) in finding vulnerable examples on both original and robustly trained CNNs and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/nlp-second-order-attack.</abstract>
      <url hash="0fa9f3a2">2021.naacl-main.305</url>
      <doi>10.18653/v1/2021.naacl-main.305</doi>
      <bibkey>zhang-etal-2021-double</bibkey>
      <video href="2021.naacl-main.305.mp4"/>
      <pwccode url="https://github.com/chong-z/nlp-second-order-attack" additional="false">chong-z/nlp-second-order-attack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="306">
      <title>Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks</title>
      <author><first>Hanjie</first><last>Chen</last></author>
      <author><first>Song</first><last>Feng</last></author>
      <author><first>Jatin</first><last>Ganhotra</last></author>
      <author><first>Hui</first><last>Wan</last></author>
      <author><first>Chulaka</first><last>Gunasekara</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <pages>3917–3930</pages>
      <abstract>Explaining neural network models is important for increasing their trustworthiness in real-world applications. Most existing methods generate post-hoc explanations for neural network models by identifying individual feature attributions or detecting interactions between adjacent features. However, for models with text pairs as inputs (e.g., paraphrase identification), existing methods are not sufficient to capture feature interactions between two texts and their simple extension of computing all word-pair interactions between two texts is computationally inefficient. In this work, we propose the Group Mask (GMASK) method to implicitly detect word correlations by grouping correlated words from the input text pair together and measure their contribution to the corresponding NLP tasks as a whole. The proposed method is evaluated with two different model architectures (decomposable attention model and BERT) across four datasets, including natural language inference and paraphrase identification tasks. Experiments show the effectiveness of GMASK in providing faithful explanations to these models.</abstract>
      <url hash="ed1975ed">2021.naacl-main.306</url>
      <doi>10.18653/v1/2021.naacl-main.306</doi>
      <bibkey>chen-etal-2021-explaining</bibkey>
      <video href="2021.naacl-main.306.mp4"/>
      <pwccode url="https://github.com/UVa-NLP/GMASK" additional="false">UVa-NLP/GMASK</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    </paper>
    <paper id="307">
      <title>Almost Free Semantic Draft for Neural Machine Translation</title>
      <author><first>Xi</first><last>Ai</last></author>
      <author><first>Bin</first><last>Fang</last></author>
      <pages>3931–3941</pages>
      <abstract>Translation quality can be improved by global information from the required target sentence because the decoder can understand both past and future information. However, the model needs additional cost to produce and consider such global information. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from semantic space for decoding with almost free of cost. Unlike other successful adaptations, we do not have to perform an EM-like process that repeatedly samples a possible semantic from the semantic space. Empirical experiments show that the presented method can achieve competitive performance in common language pairs with a clear advantage in inference efficiency. We will open all our source code on GitHub.</abstract>
      <url hash="4ae1d23c">2021.naacl-main.307</url>
      <doi>10.18653/v1/2021.naacl-main.307</doi>
      <bibkey>ai-fang-2021-almost</bibkey>
      <video href="2021.naacl-main.307.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="308">
      <title>Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation</title>
      <author><first>Shuhao</first><last>Gu</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Wanying</first><last>Xie</last></author>
      <pages>3942–3952</pages>
      <abstract>Domain Adaptation is widely used in practical applications of neural machine translation, which aims to achieve good performance on both general domain and in-domain data. However, the existing methods for domain adaptation usually suffer from catastrophic forgetting, large domain divergence, and model explosion. To address these three problems, we propose a method of “divide and conquer” which is based on the importance of neurons or parameters for the translation model. In this method, we first prune the model and only keep the important neurons or parameters, making them responsible for both general-domain and in-domain translation. Then we further train the pruned model supervised by the original whole model with knowledge distillation. Last we expand the model to the original size and fine-tune the added parameters for the in-domain translation. We conducted experiments on different language pairs and domains and the results show that our method can achieve significant improvements compared with several strong baselines.</abstract>
      <url hash="a6a9ddd6">2021.naacl-main.308</url>
      <doi>10.18653/v1/2021.naacl-main.308</doi>
      <bibkey>gu-etal-2021-pruning</bibkey>
      <video href="2021.naacl-main.308.mp4"/>
      <pwccode url="https://github.com/ictnlp/PTE-NMT" additional="false">ictnlp/PTE-NMT</pwccode>
    </paper>
    <paper id="309">
      <title>Multi-Hop Transformer for Document-Level Machine Translation</title>
      <author><first>Long</first><last>Zhang</last></author>
      <author><first>Tong</first><last>Zhang</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Wei</first><last>Ye</last></author>
      <author><first>Shikun</first><last>Zhang</last></author>
      <pages>3953–3963</pages>
      <abstract>Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior – human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end, we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically, our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve document-level translation performance and can tackle discourse phenomena, such as coreference error and the problem of polysemy.</abstract>
      <url hash="d7e2d040">2021.naacl-main.309</url>
      <doi>10.18653/v1/2021.naacl-main.309</doi>
      <bibkey>zhang-etal-2021-multi</bibkey>
      <video href="2021.naacl-main.309.mp4"/>
    </paper>
    <paper id="310">
      <title>Continual Learning for Neural Machine Translation</title>
      <author><first>Yue</first><last>Cao</last></author>
      <author><first>Hao-Ran</first><last>Wei</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>3964–3974</pages>
      <abstract>Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the general domain is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings.</abstract>
      <url hash="9ea31e51">2021.naacl-main.310</url>
      <doi>10.18653/v1/2021.naacl-main.310</doi>
      <bibkey>cao-etal-2021-continual</bibkey>
      <video href="2021.naacl-main.310.mp4"/>
    </paper>
    <paper id="311">
      <title>Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios</title>
      <author><first>Haipeng</first><last>Sun</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Kehai</first><last>Chen</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <pages>3975–3981</pages>
      <abstract>Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems.</abstract>
      <url hash="8eb1b8db">2021.naacl-main.311</url>
      <doi>10.18653/v1/2021.naacl-main.311</doi>
      <bibkey>sun-etal-2021-self</bibkey>
      <video href="2021.naacl-main.311.mp4"/>
    </paper>
    <paper id="312">
      <title>Smart-Start Decoding for Neural Machine Translation</title>
      <author><first>Jian</first><last>Yang</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Dongdong</first><last>Zhang</last></author>
      <author><first>Juncheng</first><last>Wan</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>3982–3988</pages>
      <abstract>Most current neural machine translation models adopt a monotonic decoding order of either left-to-right or right-to-left. In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. More specifically, our method first predicts a median word. It starts to decode the words on the right side of the median word and then generates words on the left. We evaluate the proposed Smart-Start decoding method on three datasets. Experimental results show that the proposed method can significantly outperform strong baseline models.</abstract>
      <url hash="24aed096">2021.naacl-main.312</url>
      <doi>10.18653/v1/2021.naacl-main.312</doi>
      <bibkey>yang-etal-2021-smart</bibkey>
      <video href="2021.naacl-main.312.mp4"/>
    </paper>
    <paper id="313">
      <title>Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation</title>
      <author><first>Yongchang</first><last>Hao</last></author>
      <author><first>Shilin</first><last>He</last></author>
      <author><first>Wenxiang</first><last>Jiao</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Michael</first><last>Lyu</last></author>
      <author><first>Xing</first><last>Wang</last></author>
      <pages>3989–3996</pages>
      <abstract>Non-Autoregressive machine Translation (NAT) models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation (AT) knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties of source sentences. Therefore, we propose to adopt multi-task learning to transfer the AT knowledge to NAT models through encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 En-De and WMT16 En-Ro datasets show that the proposed Multi-Task NAT achieves significant improvements over the baseline NAT models. Furthermore, the performance on large-scale WMT19 and WMT20 En-De datasets confirm the consistency of our proposed method. In addition, experimental results demonstrate that our Multi-Task NAT is complementary to knowledge distillation, the standard knowledge transfer method for NAT.</abstract>
      <url hash="3f69e983">2021.naacl-main.313</url>
      <doi>10.18653/v1/2021.naacl-main.313</doi>
      <bibkey>hao-etal-2021-multi</bibkey>
      <video href="2021.naacl-main.313.mp4"/>
      <pwccode url="https://github.com/yongchanghao/multi-task-nat" additional="false">yongchanghao/multi-task-nat</pwccode>
    </paper>
    <paper id="314">
      <title><fixed-case>ER</fixed-case>-<fixed-case>AE</fixed-case>: Differentially Private Text Generation for Authorship Anonymization</title>
      <author><first>Haohan</first><last>Bo</last></author>
      <author><first>Steven H. H.</first><last>Ding</last></author>
      <author><first>Benjamin C. M.</first><last>Fung</last></author>
      <author><first>Farkhund</first><last>Iqbal</last></author>
      <pages>3997–4007</pages>
      <abstract>Most of privacy protection studies for textual data focus on removing explicit sensitive identifiers. However, personal writing style, as a strong indicator of the authorship, is often neglected. Recent studies, such as SynTF, have shown promising results on privacy-preserving text mining. However, their anonymization algorithm can only output numeric term vectors which are difficult for the recipients to interpret. We propose a novel text generation model with a two-set exponential mechanism for authorship anonymization. By augmenting the semantic information through a REINFORCE training reward function, the model can generate differentially private text that has a close semantic and similar grammatical structure to the original text while removing personal traits of the writing style. It does not assume any conditioned labels or paralleled text data for training. We evaluate the performance of the proposed model on the real-life peer reviews dataset and the Yelp review dataset. The result suggests that our model outperforms the state-of-the-art on semantic preservation, authorship obfuscation, and stylometric transformation.</abstract>
      <url hash="c60aee82">2021.naacl-main.314</url>
      <doi>10.18653/v1/2021.naacl-main.314</doi>
      <bibkey>bo-etal-2021-er</bibkey>
      <video href="2021.naacl-main.314.mp4"/>
      <pwccode url="https://github.com/McGill-DMaS/AuthorshipAnonymization" additional="true">McGill-DMaS/AuthorshipAnonymization</pwccode>
    </paper>
    <paper id="315">
      <title>Distantly Supervised Transformers For <fixed-case>E</fixed-case>-Commerce Product <fixed-case>QA</fixed-case></title>
      <author><first>Happy</first><last>Mittal</last></author>
      <author><first>Aniket</first><last>Chakrabarti</last></author>
      <author><first>Belhassen</first><last>Bayar</last></author>
      <author><first>Animesh Anant</first><last>Sharma</last></author>
      <author><first>Nikhil</first><last>Rasiwasia</last></author>
      <pages>4008–4017</pages>
      <abstract>We propose a practical instant question answering (QA) system on product pages of e-commerce services, where for each user query, relevant community question answer (CQA) pairs are retrieved. User queries and CQA pairs differ significantly in language characteristics making relevance learning difficult. Our proposed transformer-based model learns a robust relevance function by jointly learning unified syntactic and semantic representations without the need for human labeled data. This is achieved by distantly supervising our model by distilling from predictions of a syntactic matching system on user queries and simultaneously training with CQA pairs. Training with CQA pairs helps our model learning semantic QA relevance and distant supervision enables learning of syntactic features as well as the nuances of user querying language. Additionally, our model encodes queries and candidate responses independently allowing offline candidate embedding generation thereby minimizing the need for real-time transformer model execution. Consequently, our framework is able to scale to large e-commerce QA traffic. Extensive evaluation on user queries shows that our framework significantly outperforms both syntactic and semantic baselines in offline as well as large scale online A/B setups of a popular e-commerce service.</abstract>
      <url hash="48c7bd20">2021.naacl-main.315</url>
      <doi>10.18653/v1/2021.naacl-main.315</doi>
      <bibkey>mittal-etal-2021-distantly</bibkey>
      <video href="2021.naacl-main.315.mp4"/>
    </paper>
    <paper id="316">
      <title>Quantitative Day Trading from Natural Language using Reinforcement Learning</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Arnav</first><last>Wadhwa</last></author>
      <author><first>Shivam</first><last>Agarwal</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>4018–4030</pages>
      <abstract>It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market responds quickly. Building on these shortcomings, we propose a deep reinforcement learning approach that makes time-aware decisions to trade stocks while optimizing profit using textual data. Our method outperforms state-of-the-art in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets (English) and financial news (Chinese) pertaining to two major indexes and four global stock markets. Through extensive experiments and studies, we build the case for our method as a tool for quantitative trading.</abstract>
      <url hash="cf80d8f0">2021.naacl-main.316</url>
      <doi>10.18653/v1/2021.naacl-main.316</doi>
      <bibkey>sawhney-etal-2021-quantitative</bibkey>
      <video href="2021.naacl-main.316.mp4"/>
    </paper>
    <paper id="317">
      <title>Restoring and Mining the Records of the <fixed-case>J</fixed-case>oseon Dynasty via Neural Language Modeling and Machine Translation</title>
      <author><first>Kyeongpil</first><last>Kang</last></author>
      <author><first>Kyohoon</first><last>Jin</last></author>
      <author><first>Soyoung</first><last>Yang</last></author>
      <author><first>Soojin</first><last>Jang</last></author>
      <author><first>Jaegul</first><last>Choo</last></author>
      <author><first>Youngbin</first><last>Kim</last></author>
      <pages>4031–4042</pages>
      <abstract>Understanding voluminous historical records provides clues on the past in various aspects, such as social and political issues and even natural science facts. However, it is generally difficult to fully utilize the historical records, since most of the documents are not written in a modern language and part of the contents are damaged over time. As a result, restoring the damaged or unrecognizable parts as well as translating the records into modern languages are crucial tasks. In response, we present a multi-task learning approach to restore and translate historical documents based on a self-attention mechanism, specifically utilizing two Korean historical records, ones of the most voluminous historical records in the world. Experimental results show that our approach significantly improves the accuracy of the translation task than baselines without multi-task learning. In addition, we present an in-depth exploratory analysis on our translated results via topic modeling, uncovering several significant historical events.</abstract>
      <url hash="bb734073">2021.naacl-main.317</url>
      <doi>10.18653/v1/2021.naacl-main.317</doi>
      <bibkey>kang-etal-2021-restoring</bibkey>
      <video href="2021.naacl-main.317.mp4"/>
    </paper>
    <paper id="318">
      <title>Modeling Diagnostic Label Correlation for Automatic <fixed-case>ICD</fixed-case> Coding</title>
      <author><first>Shang-Chi</first><last>Tsai</last></author>
      <author><first>Chao-Wei</first><last>Huang</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>4043–4052</pages>
      <abstract>Given the clinical notes written in electronic health records (EHRs), it is challenging to predict the diagnostic codes which is formulated as a multi-label classification task. The large set of labels, the hierarchical dependency, and the imbalanced data make this prediction task extremely hard. Most existing work built a binary prediction for each label independently, ignoring the dependencies between labels. To address this problem, we propose a two-stage framework to improve automatic ICD coding by capturing the label correlation. Specifically, we train a label set distribution estimator to rescore the probability of each label set candidate generated by a base predictor. This paper is the first attempt at learning the label set distribution as a reranking module for ICD coding. In the experiments, our proposed framework is able to improve upon best-performing predictors for medical code prediction on the benchmark MIMIC datasets.</abstract>
      <url hash="0f9eb258">2021.naacl-main.318</url>
      <doi>10.18653/v1/2021.naacl-main.318</doi>
      <bibkey>tsai-etal-2021-modeling</bibkey>
      <video href="2021.naacl-main.318.mp4"/>
      <pwccode url="https://github.com/MiuLab/ICD-Correlation" additional="false">MiuLab/ICD-Correlation</pwccode>
    </paper>
    <paper id="319">
      <title>Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents</title>
      <author><first>Mohammad</first><last>Kachuee</last></author>
      <author><first>Hao</first><last>Yuan</last></author>
      <author><first>Young-Bum</first><last>Kim</last></author>
      <author><first>Sungjin</first><last>Lee</last></author>
      <pages>4053–4064</pages>
      <abstract>Turn-level user satisfaction is one of the most important performance metrics for conversational agents. It can be used to monitor the agent’s performance and provide insights about defective user experiences. While end-to-end deep learning has shown promising results, having access to a large number of reliable annotated samples required by these methods remains challenging. In a large-scale conversational system, there is a growing number of newly developed skills, making the traditional data collection, annotation, and modeling process impractical due to the required annotation costs and the turnaround times. In this paper, we suggest a self-supervised contrastive learning approach that leverages the pool of unlabeled data to learn user-agent interactions. We show that the pre-trained models using the self-supervised objective are transferable to the user satisfaction prediction. In addition, we propose a novel few-shot transfer learning approach that ensures better transferability for very small sample sizes. The suggested few-shot method does not require any inner loop optimization process and is scalable to very large datasets and complex models. Based on our experiments using real data from a large-scale commercial system, the suggested approach is able to significantly reduce the required number of annotations, while improving the generalization on unseen skills.</abstract>
      <url hash="a0c496dd">2021.naacl-main.319</url>
      <doi>10.18653/v1/2021.naacl-main.319</doi>
      <bibkey>kachuee-etal-2021-self</bibkey>
      <video href="2021.naacl-main.319.mp4"/>
    </paper>
    <paper id="320">
      <title>A recipe for annotating grounded clarifications</title>
      <author><first>Luciana</first><last>Benotti</last></author>
      <author><first>Patrick</first><last>Blackburn</last></author>
      <pages>4065–4077</pages>
      <abstract>In order to interpret the communicative intents of an utterance, it needs to be grounded in something that is outside of language; that is, grounded in world modalities. In this paper, we argue that dialogue clarification mechanisms make explicit the process of interpreting the communicative intents of the speaker’s utterances by grounding them in the various modalities in which the dialogue is situated. This paper frames dialogue clarification mechanisms as an understudied research problem and a key missing piece in the giant jigsaw puzzle of natural language understanding. We discuss both the theoretical background and practical challenges posed by this problem and propose a recipe for obtaining grounding annotations. We conclude by highlighting ethical issues that need to be addressed in future work.</abstract>
      <url hash="08bff57c">2021.naacl-main.320</url>
      <doi>10.18653/v1/2021.naacl-main.320</doi>
      <bibkey>benotti-blackburn-2021-recipe</bibkey>
      <video href="2021.naacl-main.320.mp4"/>
    </paper>
    <paper id="321">
      <title>Grey-box Adversarial Attack And Defence For Sentiment Classification</title>
      <author><first>Ying</first><last>Xu</last></author>
      <author><first>Xu</first><last>Zhong</last></author>
      <author><first>Antonio</first><last>Jimeno Yepes</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <pages>4078–4087</pages>
      <abstract>We introduce a grey-box adversarial attack and defence framework for sentiment classification. We address the issues of differentiability, label preservation and input reconstruction for adversarial attack and defence in one unified framework. Our results show that once trained, the attacking model is capable of generating high-quality adversarial examples substantially faster (one order of magnitude less in time) than state-of-the-art attacking methods. These examples also preserve the original sentiment according to human evaluation. Additionally, our framework produces an improved classifier that is robust in defending against multiple adversarial attacking methods. Code is available at: https://github.com/ibm-aur-nlp/adv-def-text-dist.</abstract>
      <url hash="8c3d5e4a">2021.naacl-main.321</url>
      <attachment type="OptionalSupplementaryData" hash="13ab1e9f">2021.naacl-main.321.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.321</doi>
      <bibkey>xu-etal-2021-grey</bibkey>
      <video href="2021.naacl-main.321.mp4"/>
    </paper>
    <paper id="322">
      <title>How low is too low? A monolingual take on lemmatisation in <fixed-case>I</fixed-case>ndian languages</title>
      <author><first>Kumar</first><last>Saunack</last></author>
      <author><first>Kumar</first><last>Saurav</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>4088–4094</pages>
      <abstract>Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form. Most prior work on ML based lemmatization has focused on high resource languages, where data sets (word forms) are readily available. For languages which have no linguistic work available, especially on morphology or in languages where the computational realization of linguistic rules is complex and cumbersome, machine learning based lemmatizers are the way togo. In this paper, we devote our attention to lemmatisation for low resource, morphologically rich scheduled Indian languages using neural methods. Here, low resource means only a small number of word forms are available. We perform tests to analyse the variance in monolingual models’ performance on varying the corpus size and contextual morphological tag data for training. We show that monolingual approaches with data augmentation can give competitive accuracy even in the low resource setting, which augurs well for NLP in low resource setting.</abstract>
      <url hash="a23d934d">2021.naacl-main.322</url>
      <doi>10.18653/v1/2021.naacl-main.322</doi>
      <bibkey>saunack-etal-2021-low</bibkey>
      <video href="2021.naacl-main.322.mp4"/>
    </paper>
    <paper id="323">
      <title>Causal Effects of Linguistic Properties</title>
      <author><first>Reid</first><last>Pryzant</last></author>
      <author><first>Dallas</first><last>Card</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <author><first>Victor</first><last>Veitch</last></author>
      <author><first>Dhanya</first><last>Sridhar</last></author>
      <pages>4095–4109</pages>
      <abstract>We consider the problem of using observational data to estimate the causal effects of linguistic properties. For example, does writing a complaint politely lead to a faster response time? How much will a positive product review increase sales? This paper addresses two technical challenges related to the problem before developing a practical method. First, we formalize the causal quantity of interest as the effect of a writer’s intent, and establish the assumptions necessary to identify this from observational data. Second, in practice, we only have access to noisy proxies for the linguistic properties of interest—e.g., predictions from classifiers and lexicons. We propose an estimator for this setting and prove that its bias is bounded when we perform an adjustment for the text. Based on these results, we introduce TextCause, an algorithm for estimating causal effects of linguistic properties. The method leverages (1) distant supervision to improve the quality of noisy proxies, and (2) a pre-trained language model (BERT) to adjust for the text. We show that the proposed method outperforms related approaches when estimating the effect of Amazon review sentiment on semi-simulated sales figures. Finally, we present an applied case study investigating the effects of complaint politeness on bureaucratic response times.</abstract>
      <url hash="2d477d65">2021.naacl-main.323</url>
      <doi>10.18653/v1/2021.naacl-main.323</doi>
      <bibkey>pryzant-etal-2021-causal</bibkey>
      <video href="2021.naacl-main.323.mp4"/>
      <pwccode url="https://github.com/rpryzant/causal-text" additional="false">rpryzant/causal-text</pwccode>
    </paper>
    <paper id="324">
      <title>Dynabench: Rethinking Benchmarking in <fixed-case>NLP</fixed-case></title>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Max</first><last>Bartolo</last></author>
      <author><first>Yixin</first><last>Nie</last></author>
      <author><first>Divyansh</first><last>Kaushik</last></author>
      <author><first>Atticus</first><last>Geiger</last></author>
      <author><first>Zhengxuan</first><last>Wu</last></author>
      <author><first>Bertie</first><last>Vidgen</last></author>
      <author><first>Grusha</first><last>Prasad</last></author>
      <author><first>Amanpreet</first><last>Singh</last></author>
      <author><first>Pratik</first><last>Ringshia</last></author>
      <author><first>Zhiyi</first><last>Ma</last></author>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Zeerak</first><last>Waseem</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <pages>4110–4124</pages>
      <abstract>We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.</abstract>
      <url hash="a980f5b4">2021.naacl-main.324</url>
      <doi>10.18653/v1/2021.naacl-main.324</doi>
      <bibkey>kiela-etal-2021-dynabench</bibkey>
      <video href="2021.naacl-main.324.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="325">
      <title>Translational <fixed-case>NLP</fixed-case>: A New Paradigm and General Principles for Natural Language Processing Research</title>
      <author><first>Denis</first><last>Newman-Griffis</last></author>
      <author><first>Jill Fain</first><last>Lehman</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <author><first>Harry</first><last>Hochheiser</last></author>
      <pages>4125–4138</pages>
      <abstract>Natural language processing (NLP) research combines the study of universal principles, through basic science, with applied science targeting specific use cases and settings. However, the process of exchange between basic NLP and applications is often assumed to emerge naturally, resulting in many innovations going unapplied and many important questions left unstudied. We describe a new paradigm of Translational NLP, which aims to structure and facilitate the processes by which basic and applied NLP research inform one another. Translational NLP thus presents a third research paradigm, focused on understanding the challenges posed by application needs and how these challenges can drive innovation in basic science and technology design. We show that many significant advances in NLP research have emerged from the intersection of basic principles with application needs, and present a conceptual framework outlining the stakeholders and key questions in translational research. Our framework provides a roadmap for developing Translational NLP as a dedicated research area, and identifies general translational principles to facilitate exchange between basic and applied research.</abstract>
      <url hash="ee886071">2021.naacl-main.325</url>
      <doi>10.18653/v1/2021.naacl-main.325</doi>
      <bibkey>newman-griffis-etal-2021-translational</bibkey>
      <video href="2021.naacl-main.325.mp4"/>
    </paper>
    <paper id="326">
      <title>Predicting Discourse Trees from Transformer-based Neural Summarizers</title>
      <author><first>Wen</first><last>Xiao</last></author>
      <author><first>Patrick</first><last>Huber</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>4139–4152</pages>
      <abstract>Previous work indicates that discourse information benefits summarization. In this paper, we explore whether this synergy between discourse and summarization is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency- and constituency-style discourse information, which is typically encoded in a single head, covering long- and short-distance discourse dependencies. Overall, the experimental results suggest that the learned discourse information is general and transferable inter-domain.</abstract>
      <url hash="41f96046">2021.naacl-main.326</url>
      <doi>10.18653/v1/2021.naacl-main.326</doi>
      <bibkey>xiao-etal-2021-predicting</bibkey>
      <video href="2021.naacl-main.326.mp4"/>
      <pwccode url="https://github.com/Wendy-Xiao/summ_guided_disco_parser" additional="false">Wendy-Xiao/summ_guided_disco_parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="327">
      <title>Probing for Bridging Inference in Transformer Language Models</title>
      <author><first>Onkar</first><last>Pandit</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <pages>4153–4163</pages>
      <abstract>We probe pre-trained transformer language models for bridging inference. We first investigate individual attention heads in BERT and observe that attention heads at higher layers prominently focus on bridging relations in-comparison with the lower and middle layers, also, few specific attention heads concentrate consistently on bridging. More importantly, we consider language models as a whole in our second approach where bridging anaphora resolution is formulated as a masked token prediction task (Of-Cloze test). Our formulation produces optimistic results without any fine-tuning, which indicates that pre-trained language models substantially capture bridging inference. Our further investigation shows that the distance between anaphor-antecedent and the context provided to language models play an important role in the inference.</abstract>
      <url hash="974d4e4c">2021.naacl-main.327</url>
      <doi>10.18653/v1/2021.naacl-main.327</doi>
      <bibkey>pandit-hou-2021-probing</bibkey>
      <video href="2021.naacl-main.327.mp4"/>
      <pwccode url="https://github.com/oapandit/probBertForbridging" additional="false">oapandit/probBertForbridging</pwccode>
    </paper>
    <paper id="328">
      <title>Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models</title>
      <author><first>Anne</first><last>Beyer</last></author>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>4164–4173</pages>
      <abstract>Coherent discourse is distinguished from a mere collection of utterances by the satisfaction of a diverse set of constraints, for example choice of expression, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices beyond sentence order perturbations, which allow for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective are capable of encoding. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence.</abstract>
      <url hash="3c7061ad">2021.naacl-main.328</url>
      <doi>10.18653/v1/2021.naacl-main.328</doi>
      <bibkey>beyer-etal-2021-incoherence</bibkey>
      <video href="2021.naacl-main.328.mp4"/>
      <pwccode url="https://github.com/AnneBeyer/coherencegym" additional="false">AnneBeyer/coherencegym</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="329">
      <title>Stay Together: A System for Single and Split-antecedent Anaphora Resolution</title>
      <author><first>Juntao</first><last>Yu</last></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last></author>
      <author><first>Silviu</first><last>Paun</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <pages>4174–4184</pages>
      <abstract>The state-of-the-art on basic, single-antecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of anaphora such as split-antecedent anaphora, as in “Time-Warner is considering a legal challenge to Telecommunications Inc’s plan to buy half of Showtime Networks Inc–a move that could lead to all-out war between the two powerful companies”. Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora; as a result, it is not annotated in many datasets designed to test coreference, and previous work on resolving this type of anaphora was carried out in unrealistic conditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.</abstract>
      <url hash="54e69f35">2021.naacl-main.329</url>
      <doi>10.18653/v1/2021.naacl-main.329</doi>
      <bibkey>yu-etal-2021-stay</bibkey>
      <video href="2021.naacl-main.329.mp4"/>
      <pwccode url="https://github.com/juntaoy/dali-full-anaphora" additional="false">juntaoy/dali-full-anaphora</pwccode>
    </paper>
    <paper id="330">
      <title>Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness</title>
      <author><first>Florian</first><last>Boudin</last></author>
      <author><first>Ygor</first><last>Gallina</last></author>
      <pages>4185–4193</pages>
      <abstract>Neural keyphrase generation models have recently attracted much interest due to their ability to output absent keyphrases, that is, keyphrases that do not appear in the source text. In this paper, we discuss the usefulness of absent keyphrases from an Information Retrieval (IR) perspective, and show that the commonly drawn distinction between present and absent keyphrases is not made explicit enough. We introduce a finer-grained categorization scheme that sheds more light on the impact of absent keyphrases on scientific document retrieval. Under this scheme, we find that only a fraction (around 20%) of the words that make up keyphrases actually serves as document expansion, but that this small fraction of words is behind much of the gains observed in retrieval effectiveness. We also discuss how the proposed scheme can offer a new angle to evaluate the output of neural keyphrase generation models.</abstract>
      <url hash="bf8ad20e">2021.naacl-main.330</url>
      <doi>10.18653/v1/2021.naacl-main.330</doi>
      <bibkey>boudin-gallina-2021-redefining</bibkey>
      <video href="2021.naacl-main.330.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="331">
      <title><fixed-case>C</fixed-case>o<fixed-case>RT</fixed-case>: Complementary Rankings from Transformers</title>
      <author><first>Marco</first><last>Wrzalik</last></author>
      <author><first>Dirk</first><last>Krechel</last></author>
      <pages>4194–4204</pages>
      <abstract>Many recent approaches towards neural information retrieval mitigate their computational costs by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as BM25. Although BM25 has proven decent performance as a first-stage ranker, it tends to miss relevant passages. In this context we propose CoRT, a simple neural first-stage ranking model that leverages contextual representations from pretrained language models such as BERT to complement term-based ranking functions while causing no significant delay at query time. Using the MS MARCO dataset, we show that CoRT significantly increases the candidate recall by complementing BM25 with missing candidates. Consequently, we find subsequent re-rankers achieve superior results with less candidates. We further demonstrate that passage retrieval using CoRT can be realized with surprisingly low latencies.</abstract>
      <url hash="65b0ff95">2021.naacl-main.331</url>
      <doi>10.18653/v1/2021.naacl-main.331</doi>
      <bibkey>wrzalik-krechel-2021-cort</bibkey>
      <video href="2021.naacl-main.331.mp4"/>
      <pwccode url="https://github.com/lavis-nlp/CoRT" additional="false">lavis-nlp/CoRT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="332">
      <title>Multi-source Neural Topic Modeling in Multi-view Embedding Spaces</title>
      <author><first>Pankaj</first><last>Gupta</last></author>
      <author><first>Yatin</first><last>Chaudhary</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>4205–4217</pages>
      <abstract>Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora.</abstract>
      <url hash="993d4961">2021.naacl-main.332</url>
      <doi>10.18653/v1/2021.naacl-main.332</doi>
      <bibkey>gupta-etal-2021-multi</bibkey>
      <video href="2021.naacl-main.332.mp4"/>
      <pwccode url="https://github.com/YatinChaudhary/Multi-view-Multi-source-Topic-Modeling" additional="false">YatinChaudhary/Multi-view-Multi-source-Topic-Modeling</pwccode>
    </paper>
    <paper id="333">
      <title>Inductive Topic Variational Graph Auto-Encoder for Text Classification</title>
      <author><first>Qianqian</first><last>Xie</last></author>
      <author><first>Jimin</first><last>Huang</last></author>
      <author><first>Pan</first><last>Du</last></author>
      <author><first>Min</first><last>Peng</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <pages>4218–4227</pages>
      <abstract>Graph convolutional networks (GCNs) have been applied recently to text classification and produced an excellent performance. However, existing GCN-based methods do not assume an explicit latent semantic structure of documents, making learned representations less effective and difficult to interpret. They are also transductive in nature, thus cannot handle out-of-graph documents. To address these issues, we propose a novel model named inductive Topic Variational Graph Auto-Encoder (T-VGAE), which incorporates a topic model into variational graph-auto-encoder (VGAE) to capture the hidden semantic information between documents and words. T-VGAE inherits the interpretability of the topic model and the efficient information propagation mechanism of VGAE. It learns probabilistic representations of words and documents by jointly encoding and reconstructing the global word-level graph and bipartite graphs of documents, where each document is considered individually and decoupled from the global correlation graph so as to enable inductive learning. Our experiments on several benchmark datasets show that our method outperforms the existing competitive models on supervised and semi-supervised text classification, as well as unsupervised text representation learning. In addition, it has higher interpretability and is able to deal with unseen documents.</abstract>
      <url hash="657546ce">2021.naacl-main.333</url>
      <doi>10.18653/v1/2021.naacl-main.333</doi>
      <bibkey>xie-etal-2021-inductive</bibkey>
      <video href="2021.naacl-main.333.mp4"/>
    </paper>
    <paper id="334">
      <title>Self-Alignment Pretraining for Biomedical Entity Representations</title>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <author><first>Zaiqiao</first><last>Meng</last></author>
      <author><first>Marco</first><last>Basaldella</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>4228–4238</pages>
      <abstract>Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust.</abstract>
      <url hash="531398d7">2021.naacl-main.334</url>
      <doi>10.18653/v1/2021.naacl-main.334</doi>
      <bibkey>liu-etal-2021-self</bibkey>
      <video href="2021.naacl-main.334.mp4"/>
      <pwccode url="https://github.com/cambridgeltl/sapbert" additional="false">cambridgeltl/sapbert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cometa">COMETA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
    </paper>
    <paper id="335">
      <title><fixed-case>T</fixed-case>axo<fixed-case>C</fixed-case>lass: Hierarchical Multi-Label Text Classification Using Only Class Names</title>
      <author><first>Jiaming</first><last>Shen</last></author>
      <author><first>Wenda</first><last>Qiu</last></author>
      <author><first>Yu</first><last>Meng</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>4239–4249</pages>
      <abstract>Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a taxonomic class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its “core classes”, and then check core classes’ ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document’s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25%.</abstract>
      <url hash="09e34dac">2021.naacl-main.335</url>
      <doi>10.18653/v1/2021.naacl-main.335</doi>
      <bibkey>shen-etal-2021-taxoclass</bibkey>
      <video href="2021.naacl-main.335.mp4"/>
    </paper>
    <paper id="336">
      <title><fixed-case>MERMAID</fixed-case>: Metaphor Generation with Symbolism and Discriminative Decoding</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Xurui</first><last>Zhang</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>4250–4261</pages>
      <abstract>Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66% of the time on average. A task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68% of the time compared to poems without metaphors.</abstract>
      <url hash="ea2cd7b4">2021.naacl-main.336</url>
      <doi>10.18653/v1/2021.naacl-main.336</doi>
      <bibkey>chakrabarty-etal-2021-mermaid</bibkey>
      <video href="2021.naacl-main.336.mp4"/>
      <pwccode url="https://github.com/tuhinjubcse/MetaphorGenNAACL2021" additional="false">tuhinjubcse/MetaphorGenNAACL2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="337">
      <title>On Learning Text Style Transfer with Direct Rewards</title>
      <author><first>Yixin</first><last>Liu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>John</first><last>Wieting</last></author>
      <pages>4262–4273</pages>
      <abstract>In most cases, the lack of parallel corpora makes it impossible to directly train supervised models for the text style transfer task. In this paper, we explore training algorithms that instead optimize reward functions that explicitly consider different aspects of the style-transferred outputs. In particular, we leverage semantic similarity metrics originally used for fine-tuning neural machine translation models to explicitly assess the preservation of content between system outputs and input texts. We also investigate the potential weaknesses of the existing automatic metrics and propose efficient strategies of using these metrics for training. The experimental results show that our model provides significant gains in both automatic and human evaluation over strong baselines, indicating the effectiveness of our proposed methods and training strategies.</abstract>
      <url hash="77107d4f">2021.naacl-main.337</url>
      <doi>10.18653/v1/2021.naacl-main.337</doi>
      <bibkey>liu-etal-2021-learning</bibkey>
      <video href="2021.naacl-main.337.mp4"/>
      <pwccode url="https://github.com/yixinL7/Direct-Style-Transfer" additional="false">yixinL7/Direct-Style-Transfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="338">
      <title>Focused Attention Improves Document-Grounded Generation</title>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <author><first>Alan W</first><last>Black</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <pages>4274–4287</pages>
      <abstract>Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally, we provide a stronger BART baseline for these tasks. Our proposed techniques outperform existing methods on both automated (at least 48% increase in BLEU-4 points) and human evaluation for closeness to reference and relevance to the document. Furthermore, we perform comprehensive manual inspection of the generated output and categorize errors to provide insights into future directions in modeling these tasks.</abstract>
      <url hash="d554e4fc">2021.naacl-main.338</url>
      <doi>10.18653/v1/2021.naacl-main.338</doi>
      <bibkey>prabhumoye-etal-2021-focused</bibkey>
      <video href="2021.naacl-main.338.mp4"/>
      <pwccode url="https://github.com/shrimai/Focused-Attention-Improves-Document-Grounded-Generation" additional="false">shrimai/Focused-Attention-Improves-Document-Grounded-Generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="339">
      <title><fixed-case>N</fixed-case>euro<fixed-case>L</fixed-case>ogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints</title>
      <author><first>Ximing</first><last>Lu</last></author>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Rowan</first><last>Zellers</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Chandra</first><last>Bhagavatula</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>4288–4299</pages>
      <abstract>Conditional text generation often requires lexical constraints, i.e., which words should or shouldn’t be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models – supervised or not – to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.</abstract>
      <url hash="9227069d">2021.naacl-main.339</url>
      <doi>10.18653/v1/2021.naacl-main.339</doi>
      <bibkey>lu-etal-2021-neurologic</bibkey>
      <video href="2021.naacl-main.339.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/commongen">CommonGen</pwcdataset>
    </paper>
    <paper id="340">
      <title>Ask what’s missing and what’s useful: Improving Clarification Question Generation using Global Knowledge</title>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last></author>
      <author><first>Sudha</first><last>Rao</last></author>
      <author><first>Michel</first><last>Galley</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>4300–4312</pages>
      <abstract>The ability to generate clarification questions i.e., questions that identify useful missing information in a given context, is important in reducing ambiguity. Humans use previous experience with similar contexts to form a global view and compare it to the given context to ascertain what is missing and what is useful in the context. Inspired by this, we propose a model for clarification question generation where we first identify what is missing by taking a difference between the global and the local view and then train a model to identify what is useful and generate a question about it. Our model outperforms several baselines as judged by both automatic metrics and humans.</abstract>
      <url hash="c019b5d5">2021.naacl-main.340</url>
      <doi>10.18653/v1/2021.naacl-main.340</doi>
      <bibkey>majumder-etal-2021-ask</bibkey>
      <video href="2021.naacl-main.340.mp4"/>
      <pwccode url="https://github.com/microsoft/clarification-qgen-globalinfo" additional="false">microsoft/clarification-qgen-globalinfo</pwccode>
    </paper>
    <paper id="341">
      <title>Progressive Generation of Long Text with Pretrained Language Models</title>
      <author><first>Bowen</first><last>Tan</last></author>
      <author><first>Zichao</first><last>Yang</last></author>
      <author><first>Maruan</first><last>Al-Shedivat</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <pages>4313–4324</pages>
      <abstract>Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent.</abstract>
      <url hash="cab8615e">2021.naacl-main.341</url>
      <doi>10.18653/v1/2021.naacl-main.341</doi>
      <bibkey>tan-etal-2021-progressive</bibkey>
      <video href="2021.naacl-main.341.mp4"/>
      <pwccode url="https://github.com/tanyuqian/progressive-generation" additional="false">tanyuqian/progressive-generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="342">
      <title><fixed-case>SOCCER</fixed-case>: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain</title>
      <author><first>Ruochen</first><last>Zhang</last></author>
      <author><first>Carsten</first><last>Eickhoff</last></author>
      <pages>4325–4333</pages>
      <abstract>In the pursuit of natural language understanding, there has been a long standing interest in tracking state changes throughout narratives. Impressive progress has been made in modeling the state of transaction-centric dialogues and procedural texts. However, this problem has been less intensively studied in the realm of general discourse where ground truth descriptions of states may be loosely defined and state changes are less densely distributed over utterances. This paper proposes to turn to simplified, fully observable systems that show some of these properties: Sports events. We curated 2,263 soccer matches including time-stamped natural language commentary accompanied by discrete events such as a team scoring goals, switching players or being penalized with cards. We propose a new task formulation where, given paragraphs of commentary of a game at different timestamps, the system is asked to recognize the occurrence of in-game events. This domain allows for rich descriptions of state while avoiding the complexities of many other real-world settings. As an initial point of performance measurement, we include two baseline methods from the perspectives of sentence classification with temporal dependence and current state-of-the-art generative model, respectively, and demonstrate that even sophisticated existing methods struggle on the state tracking task when the definition of state broadens or non-event chatter becomes prevalent.</abstract>
      <url hash="dc9258a0">2021.naacl-main.342</url>
      <doi>10.18653/v1/2021.naacl-main.342</doi>
      <bibkey>zhang-eickhoff-2021-soccer</bibkey>
      <video href="2021.naacl-main.342.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/open-pi">Open PI</pwcdataset>
    </paper>
    <paper id="343">
      <title>Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation</title>
      <author><first>Sarik</first><last>Ghazarian</last></author>
      <author><first>Zixi</first><last>Liu</last></author>
      <author><first>Akash</first><last>S M</last></author>
      <author><first>Ralph</first><last>Weischedel</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>4334–4344</pages>
      <abstract>With the recent advances of open-domain story generation, the lack of reliable automatic evaluation metrics becomes an increasingly imperative issue that hinders the fast development of story generation. According to conducted researches in this regard, learnable evaluation metrics have promised more accurate assessments by having higher correlations with human judgments. A critical bottleneck of obtaining a reliable learnable evaluation metric is the lack of high-quality training data for classifiers to efficiently distinguish plausible and implausible machine-generated stories. Previous works relied on <i>heuristically manipulated</i> plausible examples to mimic possible system drawbacks such as repetition, contradiction, or irrelevant content in the text level, which can be <i>unnatural</i> and <i>oversimplify</i> the characteristics of implausible machine-generated stories. We propose to tackle these issues by generating a more comprehensive set of implausible stories using <i>plots</i>, which are structured representations of controllable factors used to generate stories. Since these plots are compact and structured, it is easier to manipulate them to generate text with targeted undesirable properties, while at the same time maintain the grammatical correctness and naturalness of the generated sentences. To improve the quality of generated implausible stories, we further apply the adversarial filtering procedure presented by (CITATION) to select a more nuanced set of implausible texts. Experiments show that the evaluation metrics trained on our generated data result in more reliable automatic assessments that correlate remarkably better with human judgments compared to the baselines.</abstract>
      <url hash="9436c6d1">2021.naacl-main.343</url>
      <revision id="1" href="2021.naacl-main.343v1" hash="d09059b4"/>
      <revision id="2" href="2021.naacl-main.343v2" hash="9436c6d1" date="2021-05-25">Updated a citation.</revision>
      <doi>10.18653/v1/2021.naacl-main.343</doi>
      <bibkey>ghazarian-etal-2021-plot</bibkey>
      <video href="2021.naacl-main.343.mp4"/>
      <pwccode url="https://github.com/PlusLabNLP/Plot-guided-Coherence-Evaluation" additional="false">PlusLabNLP/Plot-guided-Coherence-Evaluation</pwccode>
    </paper>
    <paper id="344">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>O</fixed-case>p<fixed-case>E</fixed-case>d: A Corpus of Multi-Perspective News Editorials</title>
      <author><first>Siyi</first><last>Liu</last></author>
      <author><first>Sihao</first><last>Chen</last></author>
      <author><first>Xander</first><last>Uyttendaele</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>4345–4361</pages>
      <abstract>We propose MultiOpEd, an open-domain news editorial corpus that supports various tasks pertaining to the argumentation structure in news editorials, focusing on automatic perspective discovery. News editorial is a genre of persuasive text, where the argumentation structure is usually implicit. However, the arguments presented in an editorial typically center around a concise, focused thesis, which we refer to as their perspective. MultiOpEd aims at supporting the study of multiple tasks relevant to automatic perspective discovery, where a system is expected to produce a single-sentence thesis statement summarizing the arguments presented. We argue that identifying and abstracting such natural language perspectives from editorials is a crucial step toward studying the implicit argumentation structure in news editorials. We first discuss the challenges and define a few conceptual tasks towards our goal. To demonstrate the utility of MultiOpEd and the induced tasks, we study the problem of perspective summarization in a multi-task learning setting, as a case study. We show that, with the induced tasks as auxiliary tasks, we can improve the quality of the perspective summary generated. We hope that MultiOpEd will be a useful resource for future studies on argumentation in the news editorial domain.</abstract>
      <url hash="c30b1da6">2021.naacl-main.344</url>
      <attachment type="OptionalSupplementaryData" hash="114c349e">2021.naacl-main.344.OptionalSupplementaryData.zip</attachment>
      <attachment type="OptionalSupplementaryCode" hash="c5673a7e">2021.naacl-main.344.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.344</doi>
      <bibkey>liu-etal-2021-multioped</bibkey>
      <video href="2021.naacl-main.344.mp4"/>
      <pwccode url="https://github.com/CogComp/MultiOpEd" additional="false">CogComp/MultiOpEd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multioped">MultiOpEd</pwcdataset>
    </paper>
    <paper id="345">
      <title>Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality</title>
      <author><first>Mina</first><last>Lee</last></author>
      <author><first>Chris</first><last>Donahue</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Alexander</first><last>Iyabor</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <pages>4362–4379</pages>
      <abstract>We release a new benchmark for lexical substitution, the task of finding appropriate substitutes for a target word in a context. For writing, lexical substitution systems can assist humans by suggesting words that humans cannot easily think of. However, existing benchmarks depend on human recall as the only source of data, and therefore lack coverage of the substitutes that would be most helpful to humans. Furthermore, annotators often provide substitutes of low quality, which are not actually appropriate in the given context. We collect higher-coverage and higher-quality data by framing lexical substitution as a classification problem, guided by the intuition that it is easier for humans to judge the appropriateness of candidate substitutes than conjure them from memory. To this end, we use a context-free thesaurus to produce candidates and rely on human judgement to determine contextual appropriateness. Compared to the previous largest benchmark, our Swords benchmark has 3x as many substitutes per target word for the same level of quality, and its substitutes are 1.4x more appropriate (based on human judgement) for the same number of substitutes.</abstract>
      <url hash="14770f81">2021.naacl-main.345</url>
      <doi>10.18653/v1/2021.naacl-main.345</doi>
      <bibkey>lee-etal-2021-swords</bibkey>
      <video href="2021.naacl-main.345.mp4"/>
      <pwccode url="https://github.com/p-lambda/swords" additional="false">p-lambda/swords</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/swords">Swords</pwcdataset>
    </paper>
    <paper id="346">
      <title>“<fixed-case>I</fixed-case>’m Not Mad”: Commonsense Implications of Negation and Contradiction</title>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <author><first>Chandra</first><last>Bhagavatula</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>4380–4397</pages>
      <abstract>Natural language inference requires reasoning about contradictions, negations, and their commonsense implications. Given a simple premise (e.g., “I’m mad at you”), humans can reason about the varying shades of contradictory statements ranging from straightforward negations (“I’m not mad at you”) to commonsense contradictions (“I’m happy”). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For example, while “I’m mad” implies “I’m unhappy about something,” negating the premise does not necessarily negate the corresponding commonsense implications. In this paper, we present the first comprehensive study focusing on commonsense implications of negated statements and contradictions. We introduce ANION, a new commonsense knowledge graph with 624K if-then rules focusing on negated and contradictory events. We then present joint generative and discriminative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises.</abstract>
      <url hash="31b513c7">2021.naacl-main.346</url>
      <doi>10.18653/v1/2021.naacl-main.346</doi>
      <bibkey>jiang-etal-2021-im</bibkey>
      <video href="2021.naacl-main.346.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="347">
      <title>Identifying Medical Self-Disclosure in Online Communities</title>
      <author><first>Mina</first><last>Valizadeh</last></author>
      <author><first>Pardis</first><last>Ranjbar-Noiey</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>4398–4408</pages>
      <abstract>Self-disclosure in online health conversations may offer a host of benefits, including earlier detection and treatment of medical issues that may have otherwise gone unaddressed. However, research analyzing medical self-disclosure in online communities is limited. We address this shortcoming by introducing a new dataset of health-related posts collected from online social platforms, categorized into three groups (No Self-Disclosure, Possible Self-Disclosure, and Clear Self-Disclosure) with high inter-annotator agreement (_k_=0.88). We make this data available to the research community. We also release a predictive model trained on this dataset that achieves an accuracy of 81.02%, establishing a strong performance benchmark for this task.</abstract>
      <url hash="1dd9229a">2021.naacl-main.347</url>
      <doi>10.18653/v1/2021.naacl-main.347</doi>
      <bibkey>valizadeh-etal-2021-identifying</bibkey>
      <video href="2021.naacl-main.347.mp4"/>
    </paper>
    <paper id="348">
      <title>Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction</title>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Ciro</first><last>Greco</last></author>
      <author><first>Jacopo</first><last>Tagliabue</last></author>
      <pages>4409–4415</pages>
      <abstract>We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a composition function are learned from user data only. We show how the resulting semantics for noun phrases exhibits compositional properties while being fully learnable without any explicit labelling. We benchmark our grounded semantics on compositionality and zero-shot inference tasks, and we show that it provides better results and better generalizations than SOTA non-grounded models, such as word2vec and BERT.</abstract>
      <url hash="88d66e01">2021.naacl-main.348</url>
      <doi>10.18653/v1/2021.naacl-main.348</doi>
      <bibkey>bianchi-etal-2021-language</bibkey>
      <video href="2021.naacl-main.348.mp4"/>
    </paper>
    <paper id="349">
      <title>Finding Concept-specific Biases in Form–Meaning Associations</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Søren</first><last>Wichmann</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Damián</first><last>Blasi</last></author>
      <pages>4416–4425</pages>
      <abstract>This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words. For instance, it has been claimed (Blasi et al., 2016) that the word for “tongue” is more likely than chance to contain the phone [l]. By controlling for the influence of language family and geographic proximity within a very large concept-aligned, cross-lingual lexicon, we extend methods previously used to detect within language non-arbitrariness (Pimentel et al., 2019) to measure cross-linguistic associations. We find that there is a significant effect of non-arbitrariness, but it is unsurprisingly small (less than 0.5% on average according to our information-theoretic estimate). We also provide a concept-level analysis which shows that a quarter of the concepts considered in our work exhibit a significant level of cross-linguistic non-arbitrariness. In sum, the paper provides new methods to detect cross-linguistic associations at scale, and confirms their effects are minor.</abstract>
      <url hash="e918e632">2021.naacl-main.349</url>
      <doi>10.18653/v1/2021.naacl-main.349</doi>
      <bibkey>pimentel-etal-2021-finding</bibkey>
      <video href="2021.naacl-main.349.mp4"/>
      <pwccode url="https://github.com/tpimentelms/form-meaning-associations" additional="true">tpimentelms/form-meaning-associations</pwccode>
    </paper>
    <paper id="350">
      <title>How (Non-)Optimal is the Lexicon?</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Irene</first><last>Nikkarinen</last></author>
      <author><first>Kyle</first><last>Mahowald</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Damián</first><last>Blasi</last></author>
      <pages>4426–4438</pages>
      <abstract>The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf’s law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world’s languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon’s optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes—as measured by code length.</abstract>
      <url hash="3ce78805">2021.naacl-main.350</url>
      <doi>10.18653/v1/2021.naacl-main.350</doi>
      <bibkey>pimentel-etal-2021-non</bibkey>
      <video href="2021.naacl-main.350.mp4"/>
    </paper>
    <paper id="351">
      <title>Word Complexity is in the Eye of the Beholder</title>
      <author><first>Sian</first><last>Gooding</last></author>
      <author><first>Ekaterina</first><last>Kochmar</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>4439–4449</pages>
      <abstract>Lexical complexity is a highly subjective notion, yet this factor is often neglected in lexical simplification and readability systems which use a ”one-size-fits-all” approach. In this paper, we investigate which aspects contribute to the notion of lexical complexity in various groups of readers, focusing on native and non-native speakers of English, and how the notion of complexity changes depending on the proficiency level of a non-native reader. To facilitate reproducibility of our approach and foster further research into these aspects, we release a dataset of complex words annotated by readers with different backgrounds.</abstract>
      <url hash="9b793814">2021.naacl-main.351</url>
      <doi>10.18653/v1/2021.naacl-main.351</doi>
      <bibkey>gooding-etal-2021-word</bibkey>
      <video href="2021.naacl-main.351.mp4"/>
    </paper>
    <paper id="352">
      <title>Linguistic Complexity Loss in Text-Based Therapy</title>
      <author><first>Jason</first><last>Wei</last></author>
      <author><first>Kelly</first><last>Finn</last></author>
      <author><first>Emma</first><last>Templeton</last></author>
      <author><first>Thalia</first><last>Wheatley</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>4450–4459</pages>
      <abstract>The complexity loss paradox, which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics, has been observed in a variety of both human and animal physiological systems. The recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel operationalization: linguistic complexity loss in text-based therapy conversations. In this paper, we analyze linguistic complexity correlates of mental health in the online therapy messages sent between therapists and 7,170 clients who provided 30,437 corresponding survey responses on their anxiety. We found that when clients reported more anxiety, they showed reduced lexical diversity as estimated by the moving average type-token ratio. Therapists, on the other hand, used language of higher reading difficulty, syntactic complexity, and age of acquisition when clients were more anxious. Finally, we found that clients, and to an even greater extent, therapists, exhibited consistent levels of many linguistic complexity measures. These results demonstrate how linguistic analysis of text-based communication can be leveraged as a marker for anxiety, an exciting prospect in a time of both increased online communication and increased mental health issues.</abstract>
      <url hash="05cd8b97">2021.naacl-main.352</url>
      <doi>10.18653/v1/2021.naacl-main.352</doi>
      <bibkey>wei-etal-2021-linguistic</bibkey>
      <video href="2021.naacl-main.352.mp4"/>
    </paper>
    <paper id="353">
      <title>Ab Antiquo: Neural Proto-language Reconstruction</title>
      <author><first>Carlo</first><last>Meloni</last></author>
      <author><first>Shauli</first><last>Ravfogel</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>4460–4473</pages>
      <abstract>Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in daughter languages. Can this process be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel dataset for this task, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this task so far. Error analysis reveals a variability in the ability of neural model to capture different phonological changes, correlating with the complexity of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by historical linguistics.</abstract>
      <url hash="9e8f4679">2021.naacl-main.353</url>
      <doi>10.18653/v1/2021.naacl-main.353</doi>
      <bibkey>meloni-etal-2021-ab</bibkey>
      <video href="2021.naacl-main.353.mp4"/>
      <pwccode url="https://github.com/shauli-ravfogel/Latin-Reconstruction-NAACL" additional="true">shauli-ravfogel/Latin-Reconstruction-NAACL</pwccode>
    </paper>
    <paper id="354">
      <title>On Biasing Transformer Attention Towards Monotonicity</title>
      <author><first>Annette</first><last>Rios</last></author>
      <author><first>Chantal</first><last>Amrhein</last></author>
      <author><first>Noëmi</first><last>Aepli</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>4474–4488</pages>
      <abstract>Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target sequence, and previous work has facilitated or enforced learning of monotonic attention behavior via specialized attention functions or pretraining. In this work, we introduce a monotonicity loss function that is compatible with standard attention mechanisms and test it on several sequence-to-sequence tasks: grapheme-to-phoneme conversion, morphological inflection, transliteration, and dialect normalization. Experiments show that we can achieve largely monotonic behavior. Performance is mixed, with larger gains on top of RNN baselines. General monotonicity does not benefit transformer multihead attention, however, we see isolated improvements when only a subset of heads is biased towards monotonic behavior.</abstract>
      <url hash="fabb640f">2021.naacl-main.354</url>
      <doi>10.18653/v1/2021.naacl-main.354</doi>
      <bibkey>rios-etal-2021-biasing</bibkey>
      <video href="2021.naacl-main.354.mp4"/>
      <pwccode url="https://github.com/ZurichNLP/monotonicity_loss" additional="false">ZurichNLP/monotonicity_loss</pwccode>
    </paper>
    <paper id="355">
      <title>Extracting a Knowledge Base of Mechanisms from <fixed-case>COVID</fixed-case>-19 Papers</title>
      <author><first>Tom</first><last>Hope</last></author>
      <author><first>Aida</first><last>Amini</last></author>
      <author><first>David</first><last>Wadden</last></author>
      <author><first>Madeleine</first><last>van Zuylen</last></author>
      <author><first>Sravanthi</first><last>Parasa</last></author>
      <author><first>Eric</first><last>Horvitz</last></author>
      <author><first>Daniel</first><last>Weld</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>4489–4503</pages>
      <abstract>The COVID-19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. We pursue the construction of a knowledge base (KB) of mechanisms—a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. We extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth. We annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers. Our experiments demonstrate the utility of our KB in supporting interdisciplinary scientific search over COVID-19 literature, outperforming the prominent PubMed search in a study with clinical experts. Our search engine, dataset and code are publicly available.</abstract>
      <url hash="d9e9c175">2021.naacl-main.355</url>
      <doi>10.18653/v1/2021.naacl-main.355</doi>
      <bibkey>hope-etal-2021-extracting</bibkey>
      <video href="2021.naacl-main.355.mp4"/>
      <pwccode url="https://github.com/dwadden/dygiepp" additional="true">dwadden/dygiepp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="356">
      <title>Constrained Multi-Task Learning for Event Coreference Resolution</title>
      <author><first>Jing</first><last>Lu</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>4504–4514</pages>
      <abstract>We propose a neural event coreference model in which event coreference is jointly trained with five tasks: trigger detection, entity coreference, anaphoricity determination, realis detection, and argument extraction. To guide the learning of this complex model, we incorporate cross-task consistency constraints into the learning process as soft constraints via designing penalty functions. In addition, we propose the novel idea of viewing entity coreference and event coreference as a single coreference task, which we believe is a step towards a unified model of coreference resolution. The resulting model achieves state-of-the-art results on the KBP 2017 event coreference dataset.</abstract>
      <url hash="44a1a675">2021.naacl-main.356</url>
      <doi>10.18653/v1/2021.naacl-main.356</doi>
      <bibkey>lu-ng-2021-constrained</bibkey>
      <video href="2021.naacl-main.356.mp4"/>
      <pwccode url="https://github.com/samlee946/cmtl-event-coref" additional="false">samlee946/cmtl-event-coref</pwccode>
    </paper>
    <paper id="357">
      <title>Empirical Evaluation of Pre-trained Transformers for Human-Level <fixed-case>NLP</fixed-case>: The Role of Sample Size and Dimensionality</title>
      <author><first>Adithya</first><last>V Ganesan</last></author>
      <author><first>Matthew</first><last>Matero</last></author>
      <author><first>Aravind Reddy</first><last>Ravula</last></author>
      <author><first>Huy</first><last>Vu</last></author>
      <author><first>H. Andrew</first><last>Schwartz</last></author>
      <pages>4515–4532</pages>
      <abstract>In human-level NLP tasks, such as predicting mental health, personality, or demographics, the number of observations is often smaller than the standard 768+ hidden state sizes of each layer within modern transformer-based language models, limiting the ability to effectively leverage transformers. Here, we provide a systematic study on the role of dimension reduction methods (principal components analysis, factorization techniques, or multi-layer auto-encoders) as well as the dimensionality of embedding vectors and sample sizes as a function of predictive performance. We first find that fine-tuning large models with a limited amount of data pose a significant difficulty which can be overcome with a pre-trained dimension reduction regime. RoBERTa consistently achieves top performance in human-level tasks, with PCA giving benefit over other reduction methods in better handling users that write longer texts. Finally, we observe that a majority of the tasks achieve results comparable to the best performance with just 1/12 of the embedding dimensions.</abstract>
      <url hash="c1b9bbd9">2021.naacl-main.357</url>
      <doi>10.18653/v1/2021.naacl-main.357</doi>
      <bibkey>v-ganesan-etal-2021-empirical</bibkey>
      <video href="2021.naacl-main.357.mp4"/>
      <pwccode url="https://github.com/adithya8/ContextualEmbeddingDR" additional="false">adithya8/ContextualEmbeddingDR</pwccode>
    </paper>
    <paper id="358">
      <title>Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality</title>
      <author><first>Hyun Gi</first><last>Lee</last></author>
      <author><first>Evan</first><last>Sholle</last></author>
      <author><first>Ashley</first><last>Beecy</last></author>
      <author><first>Subhi</first><last>Al’Aref</last></author>
      <author><first>Yifan</first><last>Peng</last></author>
      <pages>4533–4538</pages>
      <abstract>Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Furthermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict patient survival outcomes. We show that hidden layers yield notably more accurate predictions than predefined features, outperforming the previous baseline model by 5.7% on average across C-index and time-dependent AUC. We make our work publicly available at https://github.com/bionlplab/heart_failure_mortality.</abstract>
      <url hash="39b07bce">2021.naacl-main.358</url>
      <doi>10.18653/v1/2021.naacl-main.358</doi>
      <bibkey>lee-etal-2021-leveraging</bibkey>
      <video href="2021.naacl-main.358.mp4"/>
      <pwccode url="https://github.com/bionlplab/heart_failure_mortality" additional="false">bionlplab/heart_failure_mortality</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/chexpert">CheXpert</pwcdataset>
    </paper>
    <paper id="359">
      <title>On the Use of Context for Predicting Citation Worthiness of Sentences in Scholarly Articles</title>
      <author><first>Rakesh</first><last>Gosangi</last></author>
      <author><first>Ravneet</first><last>Arora</last></author>
      <author><first>Mohsen</first><last>Gheisarieha</last></author>
      <author><first>Debanjan</first><last>Mahata</last></author>
      <author><first>Haimin</first><last>Zhang</last></author>
      <pages>4539–4545</pages>
      <abstract>In this paper, we study the importance of context in predicting the citation worthiness of sentences in scholarly articles. We formulate this problem as a sequence labeling task solved using a hierarchical BiLSTM model. We contribute a new benchmark dataset containing over two million sentences and their corresponding labels. We preserve the sentence order in this dataset and perform document-level train/test splits, which importantly allows incorporating contextual information in the modeling process. We evaluate the proposed approach on three benchmark datasets. Our results quantify the benefits of using context and contextual embeddings for citation worthiness. Lastly, through error analysis, we provide insights into cases where context plays an essential role in predicting citation worthiness.</abstract>
      <url hash="f6400587">2021.naacl-main.359</url>
      <attachment type="OptionalSupplementaryData" hash="fce9f82a">2021.naacl-main.359.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.359</doi>
      <bibkey>gosangi-etal-2021-use</bibkey>
      <video href="2021.naacl-main.359.mp4"/>
    </paper>
    <paper id="360">
      <title>Data and Model Distillation as a Solution for Domain-transferable Fact Verification</title>
      <author><first>Mitch Paul</first><last>Mithun</last></author>
      <author><first>Sandeep</first><last>Suntwal</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>4546–4552</pages>
      <abstract>While neural networks produce state-of-the-art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. We present a combination of two strategies to mitigate this dependence on lexicalized information in fact verification tasks. We present a data distillation technique for delexicalization, which we then combine with a model distillation method to prevent aggressive data distillation. We show that by using our solution, not only does the performance of an existing state-of-the-art model remain at par with that of the model trained on a fully lexicalized data, but it also performs better than it when tested out of domain. We show that the technique we present encourages models to extract transferable facts from a given fact verification dataset.</abstract>
      <url hash="002c4f13">2021.naacl-main.360</url>
      <doi>10.18653/v1/2021.naacl-main.360</doi>
      <bibkey>mithun-etal-2021-data</bibkey>
      <video href="2021.naacl-main.360.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="361">
      <title>Adapting Coreference Resolution for Processing Violent Death Narratives</title>
      <author><first>Ankith</first><last>Uppunda</last></author>
      <author><first>Susan</first><last>Cochran</last></author>
      <author><first>Jacob</first><last>Foster</last></author>
      <author><first>Alina</first><last>Arseniev-Koehler</last></author>
      <author><first>Vickie</first><last>Mays</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>4553–4559</pages>
      <abstract>Coreference resolution is an important compo-nent in analyzing narrative text from admin-istrative data (e.g., clinical or police sources).However, existing coreference models trainedon general language corpora suffer from poortransferability due to domain gaps, especiallywhen they are applied to gender-inclusive datawith lesbian, gay, bisexual, and transgender(LGBT) individuals.In this paper, we an-alyzed the challenges of coreference resolu-tion in an exemplary form of administrativetext written in English: violent death nar-ratives from the USA’s Centers for DiseaseControl’s (CDC) National Violent Death Re-porting System. We developed a set of dataaugmentation rules to improve model perfor-mance using a probabilistic data programmingframework. Experiments on narratives froman administrative database, as well as existinggender-inclusive coreference datasets, demon-strate the effectiveness of data augmentationin training coreference models that can betterhandle text data about LGBT individuals.</abstract>
      <url hash="00bf1116">2021.naacl-main.361</url>
      <doi>10.18653/v1/2021.naacl-main.361</doi>
      <bibkey>uppunda-etal-2021-adapting</bibkey>
      <video href="2021.naacl-main.361.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gicoref">GICoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/map">MAP</pwcdataset>
    </paper>
    <paper id="362">
      <title>Time-Stamped Language Model: Teaching Language Models to Understand The Flow of Events</title>
      <author><first>Hossein</first><last>Rajaby Faghihi</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>4560–4570</pages>
      <abstract>Tracking entities throughout a procedure described in a text is challenging due to the dynamic nature of the world described in the process. Firstly, we propose to formulate this task as a question answering problem. This enables us to use pre-trained transformer-based language models on other QA benchmarks by adapting those to the procedural text understanding. Secondly, since the transformer-based language models cannot encode the flow of events by themselves, we propose a Time-Stamped Language Model (TSLM) to encode event information in LMs architecture by introducing the timestamp encoding. Our model evaluated on the Propara dataset shows improvements on the published state-of-the-art results with a 3.1% increase in F1 score. Moreover, our model yields better results on the location prediction task on the NPN-Cooking dataset. This result indicates that our approach is effective for procedural text understanding in general.</abstract>
      <url hash="f75557eb">2021.naacl-main.362</url>
      <doi>10.18653/v1/2021.naacl-main.362</doi>
      <bibkey>rajaby-faghihi-kordjamshidi-2021-time</bibkey>
      <video href="2021.naacl-main.362.mp4"/>
      <pwccode url="https://github.com/HLR/TSLM" additional="false">HLR/TSLM</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/propara">ProPara</pwcdataset>
    </paper>
    <paper id="363">
      <title>If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering</title>
      <author><first>Vikas</first><last>Yadav</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>4571–4581</pages>
      <abstract>Multi-hop reasoning requires aggregation and inference from multiple facts. To retrieve such facts, we propose a simple approach that retrieves and reranks set of evidence facts jointly. Our approach first generates unsupervised clusters of sentences as candidate evidence by accounting links between sentences and coverage with the given query. Then, a RoBERTa-based reranker is trained to bring the most representative evidence cluster to the top. We specifically emphasize on the importance of retrieving evidence jointly by showing several comparative analyses to other methods that retrieve and rerank evidence sentences individually. First, we introduce several attention- and embedding-based analyses, which indicate that jointly retrieving and reranking approaches can learn compositional knowledge required for multi-hop reasoning. Second, our experiments show that jointly retrieving candidate evidence leads to substantially higher evidence retrieval performance when fed to the same supervised reranker. In particular, our joint retrieval and then reranking approach achieves new state-of-the-art evidence retrieval performance on two multi-hop question answering (QA) datasets: 30.5 Recall@2 on QASC, and 67.6% F1 on MultiRC. When the evidence text from our joint retrieval approach is fed to a RoBERTa-based answer selection classifier, we achieve new state-of-the-art QA performance on MultiRC and second best result on QASC.</abstract>
      <url hash="2c07eff9">2021.naacl-main.363</url>
      <doi>10.18653/v1/2021.naacl-main.363</doi>
      <bibkey>yadav-etal-2021-want</bibkey>
      <video href="2021.naacl-main.363.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="364">
      <title><fixed-case>SPARTQA</fixed-case>: A Textual Question Answering Benchmark for Spatial Reasoning</title>
      <author><first>Roshanak</first><last>Mirzaee</last></author>
      <author><first>Hossein</first><last>Rajaby Faghihi</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>4582–4598</pages>
      <abstract>This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs’ capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.</abstract>
      <url hash="474d2b28">2021.naacl-main.364</url>
      <doi>10.18653/v1/2021.naacl-main.364</doi>
      <bibkey>mirzaee-etal-2021-spartqa</bibkey>
      <video href="2021.naacl-main.364.mp4"/>
      <pwccode url="https://github.com/HLR/SpartQA-baselines" additional="true">HLR/SpartQA-baselines</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/spartqa-1">SPARTQA -</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nlvr">NLVR</pwcdataset>
    </paper>
    <paper id="365">
      <title>A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers</title>
      <author><first>Pradeep</first><last>Dasigi</last></author>
      <author><first>Kyle</first><last>Lo</last></author>
      <author><first>Iz</first><last>Beltagy</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>4599–4610</pages>
      <abstract>Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.</abstract>
      <url hash="fe6e9970">2021.naacl-main.365</url>
      <doi>10.18653/v1/2021.naacl-main.365</doi>
      <bibkey>dasigi-etal-2021-dataset</bibkey>
      <video href="2021.naacl-main.365.mp4"/>
      <pwccode url="https://github.com/allenai/qasper-led-baseline" additional="false">allenai/qasper-led-baseline</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/qasper">QASPER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="366">
      <title>Differentiable Open-Ended Commonsense Reasoning</title>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Haitian</first><last>Sun</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Manzil</first><last>Zaheer</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <pages>4611–4625</pages>
      <abstract>Current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. As a step towards making commonsense reasoning research more realistic, we propose to study open-ended commonsense reasoning (OpenCSR) — the task of answering a commonsense question without any pre-defined choices — using as a resource only a corpus of commonsense facts written in natural language. OpenCSR is challenging due to a large decision space, and because many questions require implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To evaluate OpenCSR methods, we adapt several popular commonsense reasoning benchmarks, and collect multiple new answers for each test question via crowd-sourcing. Experiments show that DrFact outperforms strong baseline methods by a large margin.</abstract>
      <url hash="a3985c3c">2021.naacl-main.366</url>
      <doi>10.18653/v1/2021.naacl-main.366</doi>
      <bibkey>lin-etal-2021-differentiable</bibkey>
      <video href="2021.naacl-main.366.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/genericskb">GenericsKB</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
    </paper>
    <paper id="367">
      <title>Does Structure Matter? Encoding Documents for Machine Reading Comprehension</title>
      <author><first>Hui</first><last>Wan</last></author>
      <author><first>Song</first><last>Feng</last></author>
      <author><first>Chulaka</first><last>Gunasekara</last></author>
      <author><first>Siva Sankalp</first><last>Patel</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <author><first>Luis</first><last>Lastras</last></author>
      <pages>4626–4634</pages>
      <abstract>Machine reading comprehension is a challenging task especially for querying documents with deep and interconnected contexts. Transformer-based methods have shown advanced performances on this task; however, most of them still treat documents as a flat sequence of tokens. This work proposes a new Transformer-based method that reads a document as tree slices. It contains two modules for identifying more relevant text passage and the best answer span respectively, which are not only jointly trained but also jointly consulted at inference time. Our evaluation results show that our proposed method outperforms several competitive baseline approaches on two datasets from varied domains.</abstract>
      <url hash="23216f3b">2021.naacl-main.367</url>
      <doi>10.18653/v1/2021.naacl-main.367</doi>
      <bibkey>wan-etal-2021-structure</bibkey>
      <video href="2021.naacl-main.367.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial-1">Doc2Dial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial">doc2dial</pwcdataset>
    </paper>
    <paper id="368">
      <title>Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval</title>
      <author><first>Chen</first><last>Zhao</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <pages>4635–4641</pages>
      <abstract>Complex question answering often requires finding a reasoning chain that consists of multiple evidence pieces. Current approaches incorporate the strengths of structured knowledge and unstructured text, assuming text corpora is semi-structured. Building on dense retrieval methods, we propose a new multi-step retrieval approach (BeamDR) that iteratively forms an evidence chain through beam search in dense representations. When evaluated on multi-hop question answering, BeamDR is competitive to state-of-the-art systems, without using any semi-structured information. Through query composition in dense space, BeamDR captures the implicit relationships between evidence in the reasoning chain. The code is available at https://github.com/ henryzhao5852/BeamDR.</abstract>
      <url hash="7eb4993c">2021.naacl-main.368</url>
      <doi>10.18653/v1/2021.naacl-main.368</doi>
      <bibkey>zhao-etal-2021-multi-step</bibkey>
      <video href="2021.naacl-main.368.mp4"/>
      <pwccode url="https://github.com/henryzhao5852/BeamDR" additional="false">henryzhao5852/BeamDR</pwccode>
    </paper>
    <paper id="369">
      <title>Scalable and Interpretable Semantic Change Detection</title>
      <author><first>Syrielle</first><last>Montariol</last></author>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Lidia</first><last>Pivovarova</last></author>
      <pages>4642–4652</pages>
      <abstract>Several cluster-based methods for semantic change detection with contextual embeddings emerged recently. They allow a fine-grained analysis of word use change by aggregating embeddings into clusters that reflect the different usages of the word. However, these methods are unscalable in terms of memory consumption and computation time. Therefore, they require a limited set of target words to be picked in advance. This drastically limits the usability of these methods in open exploratory tasks, where each word from the vocabulary can be considered as a potential target. We propose a novel scalable method for word usage-change detection that offers large gains in processing time and significant memory savings while offering the same interpretability and better performance than unscalable methods. We demonstrate the applicability of the proposed method by analysing a large corpus of news articles about COVID-19.</abstract>
      <url hash="a54bb7be">2021.naacl-main.369</url>
      <doi>10.18653/v1/2021.naacl-main.369</doi>
      <bibkey>montariol-etal-2021-scalable</bibkey>
      <video href="2021.naacl-main.369.mp4"/>
      <pwccode url="https://github.com/matejmartinc/scalable_semantic_shift" additional="false">matejmartinc/scalable_semantic_shift</pwccode>
    </paper>
    <paper id="370">
      <title>Scalar Adjective Identification and Multilingual Ranking</title>
      <author><first>Aina</first><last>Garí Soler</last></author>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <pages>4653–4660</pages>
      <abstract>The intensity relationship that holds between scalar adjectives (e.g., nice &lt; great &lt; wonderful) is highly relevant for natural language inference and common-sense reasoning. Previous research on scalar adjective ranking has focused on English, mainly due to the availability of datasets for evaluation. We introduce a new multilingual dataset in order to promote research on scalar adjectives in new languages. We perform a series of experiments and set performance baselines on this dataset, using monolingual and multilingual contextual language models. Additionally, we introduce a new binary classification task for English scalar adjective identification which examines the models’ ability to distinguish scalar from relational adjectives. We probe contextualised representations and report baseline results for future comparison on this task.</abstract>
      <url hash="f09e90d1">2021.naacl-main.370</url>
      <doi>10.18653/v1/2021.naacl-main.370</doi>
      <bibkey>gari-soler-apidianaki-2021-scalar</bibkey>
      <video href="2021.naacl-main.370.mp4"/>
    </paper>
    <paper id="371">
      <title><fixed-case>ESC</fixed-case>: Redesigning <fixed-case>WSD</fixed-case> with Extractive Sense Comprehension</title>
      <author><first>Edoardo</first><last>Barba</last></author>
      <author><first>Tommaso</first><last>Pasini</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>4661–4672</pages>
      <abstract>Word Sense Disambiguation (WSD) is a historical NLP task aimed at linking words in contexts to discrete sense inventories and it is usually cast as a multi-label classification task. Recently, several neural approaches have employed sense definitions to better represent word meanings. Yet, these approaches do not observe the input sentence and the sense definition candidates all at once, thus potentially reducing the model performance and generalization power. We cope with this issue by reframing WSD as a span extraction problem — which we called Extractive Sense Comprehension (ESC) — and propose ESCHER, a transformer-based neural architecture for this new formulation. By means of an extensive array of experiments, we show that ESC unleashes the full potential of our model, leading it to outdo all of its competitors and to set a new state of the art on the English WSD task. In the few-shot scenario, ESCHER proves to exploit training data efficiently, attaining the same performance as its closest competitor while relying on almost three times fewer annotations. Furthermore, ESCHER can nimbly combine data annotated with senses from different lexical resources, achieving performances that were previously out of everyone’s reach. The model along with data is available at https://github.com/SapienzaNLP/esc.</abstract>
      <url hash="c245a034">2021.naacl-main.371</url>
      <doi>10.18653/v1/2021.naacl-main.371</doi>
      <bibkey>barba-etal-2021-esc</bibkey>
      <video href="2021.naacl-main.371.mp4"/>
      <pwccode url="https://github.com/SapienzaNLP/esc" additional="false">SapienzaNLP/esc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="372">
      <title>Recent advances in neural metaphor processing: A linguistic, cognitive and social perspective</title>
      <author><first>Xiaoyu</first><last>Tong</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <author><first>Martha</first><last>Lewis</last></author>
      <pages>4673–4686</pages>
      <abstract>Metaphor is an indispensable part of human cognition and everyday communication. Much research has been conducted elucidating metaphor processing in the mind/brain and the role it plays in communication. in recent years, metaphor processing systems have benefited greatly from these studies, as well as the rapid advances in deep learning for natural language processing (NLP). This paper provides a comprehensive review and discussion of recent developments in automated metaphor processing, in light of the findings about metaphor in the mind, language, and communication, and from the perspective of downstream NLP tasks.</abstract>
      <url hash="25956e9c">2021.naacl-main.372</url>
      <doi>10.18653/v1/2021.naacl-main.372</doi>
      <bibkey>tong-etal-2021-recent</bibkey>
    </paper>
    <paper id="373">
      <title>Constructing Taxonomies from Pretrained Language Models</title>
      <author><first>Catherine</first><last>Chen</last></author>
      <author><first>Kevin</first><last>Lin</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>4687–4700</pages>
      <abstract>We present a method for constructing taxonomic trees (e.g., WordNet) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those pairwise predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WordNet, and test on nonoverlapping WordNet subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WordNet, the model achieves 66.7 ancestor F1, a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using Open Multilingual WordNet and extend our results across these languages.</abstract>
      <url hash="e013e45b">2021.naacl-main.373</url>
      <doi>10.18653/v1/2021.naacl-main.373</doi>
      <bibkey>chen-etal-2021-constructing</bibkey>
      <video href="2021.naacl-main.373.mp4"/>
    </paper>
    <paper id="374">
      <title>Event Representation with Sequential, Semi-Supervised Discrete Variables</title>
      <author><first>Mehdi</first><last>Rezaee</last></author>
      <author><first>Francis</first><last>Ferraro</last></author>
      <pages>4701–4716</pages>
      <abstract>Within the context of event modeling and understanding, we propose a new method for neural sequence modeling that takes partially-observed sequences of discrete, external knowledge into account. We construct a sequential neural variational autoencoder, which uses Gumbel-Softmax reparametrization within a carefully defined encoder, to allow for successful backpropagation during training. The core idea is to allow semi-supervised external discrete knowledge to guide, but not restrict, the variational latent parameters during training. Our experiments indicate that our approach not only outperforms multiple baselines and the state-of-the-art in narrative script induction, but also converges more quickly.</abstract>
      <url hash="7da81efe">2021.naacl-main.374</url>
      <doi>10.18653/v1/2021.naacl-main.374</doi>
      <bibkey>rezaee-ferraro-2021-event</bibkey>
      <video href="2021.naacl-main.374.mp4"/>
    </paper>
    <paper id="375">
      <title><fixed-case>S</fixed-case>eq2<fixed-case>E</fixed-case>mo: A Sequence to Multi-Label Emotion Classification Model</title>
      <author><first>Chenyang</first><last>Huang</last></author>
      <author><first>Amine</first><last>Trabelsi</last></author>
      <author><first>Xuebin</first><last>Qin</last></author>
      <author><first>Nawshad</first><last>Farruque</last></author>
      <author><first>Lili</first><last>Mou</last></author>
      <author><first>Osmar</first><last>Zaïane</last></author>
      <pages>4717–4724</pages>
      <abstract>Multi-label emotion classification is an important task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval’18 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) approaches in a fair setting.</abstract>
      <url hash="f0e16946">2021.naacl-main.375</url>
      <doi>10.18653/v1/2021.naacl-main.375</doi>
      <bibkey>huang-etal-2021-seq2emo</bibkey>
      <video href="2021.naacl-main.375.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/goemotions">GoEmotions</pwcdataset>
    </paper>
    <paper id="376">
      <title>Knowledge Enhanced Masked Language Model for Stance Detection</title>
      <author><first>Kornraphop</first><last>Kawintiranon</last></author>
      <author><first>Lisa</first><last>Singh</last></author>
      <pages>4725–4735</pages>
      <abstract>Detecting stance on Twitter is especially challenging because of the short length of each tweet, the continuous coinage of new terminology and hashtags, and the deviation of sentence structure from standard prose. Fine-tuned language models using large-scale in-domain data have been shown to be the new state-of-the-art for many NLP tasks, including stance detection. In this paper, we propose a novel BERT-based fine-tuning method that enhances the masked language model for stance detection. Instead of random token masking, we propose using a weighted log-odds-ratio to identify words with high stance distinguishability and then model an attention mechanism that focuses on these words. We show that our proposed approach outperforms the state of the art for stance detection on Twitter data about the 2020 US Presidential election.</abstract>
      <url hash="8e4dfafe">2021.naacl-main.376</url>
      <doi>10.18653/v1/2021.naacl-main.376</doi>
      <bibkey>kawintiranon-singh-2021-knowledge</bibkey>
      <video href="2021.naacl-main.376.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/twitter-stance-election-2020">Twitter Stance Election 2020</pwcdataset>
    </paper>
    <paper id="377">
      <title>Learning Paralinguistic Features from Audiobooks through Style Voice Conversion</title>
      <author><first>Zakaria</first><last>Aldeneh</last></author>
      <author><first>Matthew</first><last>Perez</last></author>
      <author><first>Emily</first><last>Mower Provost</last></author>
      <pages>4736–4745</pages>
      <abstract>Paralinguistics, the non-lexical components of speech, play a crucial role in human-human interaction. Models designed to recognize paralinguistic information, particularly speech emotion and style, are difficult to train because of the limited labeled datasets available. In this work, we present a new framework that enables a neural network to learn to extract paralinguistic attributes from speech using data that are not annotated for emotion. We assess the utility of the learned embeddings on the downstream tasks of emotion recognition and speaking style detection, demonstrating significant improvements over surface acoustic features as well as over embeddings extracted from other unsupervised approaches. Our work enables future systems to leverage the learned embedding extractor as a separate component capable of highlighting the paralinguistic components of speech.</abstract>
      <url hash="25df5c7a">2021.naacl-main.377</url>
      <doi>10.18653/v1/2021.naacl-main.377</doi>
      <bibkey>aldeneh-etal-2021-learning</bibkey>
      <video href="2021.naacl-main.377.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="378">
      <title>Adapting <fixed-case>BERT</fixed-case> for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks</title>
      <author><first>Zixuan</first><last>Ke</last></author>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <pages>4746–4755</pages>
      <abstract>This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues: (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments.</abstract>
      <url hash="f17af448">2021.naacl-main.378</url>
      <doi>10.18653/v1/2021.naacl-main.378</doi>
      <bibkey>ke-etal-2021-adapting</bibkey>
      <video href="2021.naacl-main.378.mp4"/>
      <pwccode url="https://github.com/zixuanke/pycontinual" additional="false">zixuanke/pycontinual</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asc-til-19-tasks">ASC (TIL, 19 tasks)</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/20newsgroup-10-tasks">20Newsgroup (10 tasks)</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dsc-10-tasks">DSC (10 tasks)</pwcdataset>
    </paper>
    <paper id="379">
      <title>Adversarial Learning for Zero-Shot Stance Detection on Social Media</title>
      <author><first>Emily</first><last>Allaway</last></author>
      <author><first>Malavika</first><last>Srikanth</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>4756–4767</pages>
      <abstract>Stance detection on social media can help to identify and understand slanted news or commentary in everyday life. In this work, we propose a new model for zero-shot stance detection on Twitter that uses adversarial learning to generalize across topics. Our model achieves state-of-the-art performance on a number of unseen test topics with minimal computational costs. In addition, we extend zero-shot stance detection to topics not previously considered, highlighting future directions for zero-shot transfer.</abstract>
      <url hash="a1d5a465">2021.naacl-main.379</url>
      <doi>10.18653/v1/2021.naacl-main.379</doi>
      <bibkey>allaway-etal-2021-adversarial</bibkey>
      <video href="2021.naacl-main.379.mp4"/>
      <pwccode url="https://github.com/MalavikaSrikanth16/adversarial-learning-for-stance" additional="false">MalavikaSrikanth16/adversarial-learning-for-stance</pwccode>
    </paper>
    <paper id="380">
      <title>Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters</title>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Mengwen</first><last>Liu</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Sujith</first><last>Ravi</last></author>
      <author><first>Markus</first><last>Dreyer</last></author>
      <pages>4768–4779</pages>
      <abstract>This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.</abstract>
      <url hash="10ad2f63">2021.naacl-main.380</url>
      <doi>10.18653/v1/2021.naacl-main.380</doi>
      <bibkey>pasunuru-etal-2021-efficiently</bibkey>
      <video href="2021.naacl-main.380.mp4"/>
      <pwccode url="https://github.com/amazon-research/bartgraphsumm" additional="false">amazon-research/bartgraphsumm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
    </paper>
    <paper id="381">
      <title>Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization</title>
      <author><first>Yichen</first><last>Jiang</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author><first>Paul</first><last>Smolensky</last></author>
      <author><first>Paul</first><last>Soulos</last></author>
      <author><first>Sudha</first><last>Rao</last></author>
      <author><first>Hamid</first><last>Palangi</last></author>
      <author><first>Roland</first><last>Fernandez</last></author>
      <author><first>Caitlin</first><last>Smith</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>4780–4793</pages>
      <abstract>Abstractive summarization, the task of generating a concise summary of input documents, requires: (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The model then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-Transformer outperforms the Transformer and the original TP-Transformer significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and the performance gain by information specificity of the role vectors and improved syntactic interpretability in the TPR layer outputs.(Code and models are available at https://github.com/jiangycTarheel/TPT-Summ)</abstract>
      <url hash="f06a0ec4">2021.naacl-main.381</url>
      <doi>10.18653/v1/2021.naacl-main.381</doi>
      <bibkey>jiang-etal-2021-enriching</bibkey>
      <video href="2021.naacl-main.381.mp4"/>
      <pwccode url="https://github.com/jiangycTarheel/TPT-Summ" additional="false">jiangycTarheel/TPT-Summ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="382">
      <title>What’s in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization</title>
      <author><first>Griffin</first><last>Adams</last></author>
      <author><first>Emily</first><last>Alsentzer</last></author>
      <author><first>Mert</first><last>Ketenci</last></author>
      <author><first>Jason</first><last>Zucker</last></author>
      <author><first>Noémie</first><last>Elhadad</last></author>
      <pages>4794–4811</pages>
      <abstract>Summarization of clinical narratives is a long-standing research problem. Here, we introduce the task of hospital-course summarization. Given the documentation authored throughout a patient’s hospitalization, generate a paragraph that tells the story of the patient admission. We construct an English, text-to-text dataset of 109,000 hospitalizations (2M source notes) and their corresponding summary proxy: the clinician-authored “Brief Hospital Course” paragraph written as part of a discharge note. Exploratory analyses reveal that the BHC paragraphs are highly abstractive with some long extracted fragments; are concise yet comprehensive; differ in style and content organization from the source notes; exhibit minimal lexical cohesion; and represent silver-standard references. Our analysis identifies multiple implications for modeling this complex, multi-document summarization task.</abstract>
      <url hash="9a4381c2">2021.naacl-main.382</url>
      <doi>10.18653/v1/2021.naacl-main.382</doi>
      <bibkey>adams-etal-2021-whats</bibkey>
      <video href="2021.naacl-main.382.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="383">
      <title>Understanding Factuality in Abstractive Summarization with <fixed-case>FRANK</fixed-case>: A Benchmark for Factuality Metrics</title>
      <author><first>Artidoro</first><last>Pagnoni</last></author>
      <author><first>Vidhisha</first><last>Balachandran</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>4812–4829</pages>
      <abstract>Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.</abstract>
      <url hash="0cb57592">2021.naacl-main.383</url>
      <doi>10.18653/v1/2021.naacl-main.383</doi>
      <bibkey>pagnoni-etal-2021-understanding</bibkey>
      <video href="2021.naacl-main.383.mp4"/>
      <pwccode url="https://github.com/artidoro/frank" additional="false">artidoro/frank</pwccode>
    </paper>
    <paper id="384">
      <title><fixed-case>GS</fixed-case>um: A General Framework for Guided Neural Abstractive Summarization</title>
      <author><first>Zi-Yi</first><last>Dou</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Hiroaki</first><last>Hayashi</last></author>
      <author><first>Zhengbao</first><last>Jiang</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>4830–4842</pages>
      <abstract>Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (<b>GSum</b>) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models.</abstract>
      <url hash="8bfb84c8">2021.naacl-main.384</url>
      <doi>10.18653/v1/2021.naacl-main.384</doi>
      <bibkey>dou-etal-2021-gsum</bibkey>
      <video href="2021.naacl-main.384.mp4"/>
      <pwccode url="https://github.com/neulab/guided_summarization" additional="false">neulab/guided_summarization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow">WikiHow</pwcdataset>
    </paper>
    <paper id="385">
      <title>What Will it Take to Fix Benchmarking in Natural Language Understanding?</title>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <author><first>George</first><last>Dahl</last></author>
      <pages>4843–4855</pages>
      <abstract>Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.</abstract>
      <url hash="759a9457">2021.naacl-main.385</url>
      <doi>10.18653/v1/2021.naacl-main.385</doi>
      <bibkey>bowman-dahl-2021-will</bibkey>
      <video href="2021.naacl-main.385.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="386">
      <title><fixed-case>T</fixed-case>uring<fixed-case>A</fixed-case>dvice: A Generative and Dynamic Evaluation of Language Use</title>
      <author><first>Rowan</first><last>Zellers</last></author>
      <author><first>Ari</first><last>Holtzman</last></author>
      <author><first>Elizabeth</first><last>Clark</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Ali</first><last>Farhadi</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>4856–4880</pages>
      <abstract>We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today’s models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.</abstract>
      <url hash="1b9451ed">2021.naacl-main.386</url>
      <doi>10.18653/v1/2021.naacl-main.386</doi>
      <bibkey>zellers-etal-2021-turingadvice</bibkey>
      <video href="2021.naacl-main.386.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="387">
      <title>Multitask Learning for Emotionally Analyzing Sexual Abuse Disclosures</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Puneet</first><last>Mathur</last></author>
      <author><first>Taru</first><last>Jain</last></author>
      <author><first>Akash Kumar</first><last>Gautam</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>4881–4892</pages>
      <abstract>The #MeToo movement on social media platforms initiated discussions over several facets of sexual harassment in our society. Prior work by the NLP community for automated identification of the narratives related to sexual abuse disclosures barely explored this social phenomenon as an independent task. However, emotional attributes associated with textual conversations related to the #MeToo social movement are complexly intertwined with such narratives. We formulate the task of identifying narratives related to the sexual abuse disclosures in online posts as a joint modeling task that leverages their emotional attributes through multitask learning. Our results demonstrate that positive knowledge transfer via context-specific shared representations of a flexible cross-stitched parameter sharing model helps establish the inherent benefit of jointly modeling tasks related to sexual abuse disclosures with emotion classification from the text in homogeneous and heterogeneous settings. We show how for more domain-specific tasks related to sexual abuse disclosures such as sarcasm identification and dialogue act (refutation, justification, allegation) classification, homogeneous multitask learning is helpful, whereas for more general tasks such as stance and hate speech detection, heterogeneous multitask learning with emotion classification works better.</abstract>
      <url hash="d35db9e4">2021.naacl-main.387</url>
      <doi>10.18653/v1/2021.naacl-main.387</doi>
      <bibkey>sawhney-etal-2021-multitask</bibkey>
      <video href="2021.naacl-main.387.mp4"/>
      <pwccode url="https://github.com/midas-research/metoo-mtl-naacl" additional="false">midas-research/metoo-mtl-naacl</pwccode>
    </paper>
    <paper id="388">
      <title>Self Promotion in <fixed-case>US</fixed-case> Congressional Tweets</title>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Kelly</first><last>Cui</last></author>
      <author><first>Bei</first><last>Yu</last></author>
      <pages>4893–4899</pages>
      <abstract>Prior studies have found that women self-promote less than men due to gender stereotypes. In this study we built a BERT-based NLP model to predict whether a Congressional tweet shows self-promotion or not and then used this model to examine whether a gender gap in self-promotion exists among Congressional tweets. After analyzing 2 million Congressional tweets from July 2017 to March 2021, controlling for a number of factors that include political party, chamber, age, number of terms in Congress, number of daily tweets, and number of followers, we found that women in Congress actually perform more self-promotion on Twitter, indicating a reversal of traditional gender norms where women self-promote less than men.</abstract>
      <url hash="b851d9d2">2021.naacl-main.388</url>
      <doi>10.18653/v1/2021.naacl-main.388</doi>
      <bibkey>wang-etal-2021-self</bibkey>
      <video href="2021.naacl-main.388.mp4"/>
      <pwccode url="https://github.com/junwang4/self-promotion-in-congress-tweets" additional="false">junwang4/self-promotion-in-congress-tweets</pwccode>
    </paper>
    <paper id="389">
      <title>Profiling of Intertextuality in <fixed-case>L</fixed-case>atin Literature Using Word Embeddings</title>
      <author><first>Patrick J.</first><last>Burns</last></author>
      <author><first>James A.</first><last>Brofos</last></author>
      <author><first>Kyle</first><last>Li</last></author>
      <author><first>Pramit</first><last>Chaudhuri</last></author>
      <author><first>Joseph P.</first><last>Dexter</last></author>
      <pages>4900–4907</pages>
      <abstract>Identifying intertextual relationships between authors is of central importance to the study of literature. We report an empirical analysis of intertextuality in classical Latin literature using word embedding models. To enable quantitative evaluation of intertextual search methods, we curate a new dataset of 945 known parallels drawn from traditional scholarship on Latin epic poetry. We train an optimized word2vec model on a large corpus of lemmatized Latin, which achieves state-of-the-art performance for synonym detection and outperforms a widely used lexical method for intertextual search. We then demonstrate that training embeddings on very small corpora can capture salient aspects of literary style and apply this approach to replicate a previous intertextual study of the Roman historian Livy, which relied on hand-crafted stylometric features. Our results advance the development of core computational resources for a major premodern language and highlight a productive avenue for cross-disciplinary collaboration between the study of literature and NLP.</abstract>
      <url hash="6fa756fb">2021.naacl-main.389</url>
      <doi>10.18653/v1/2021.naacl-main.389</doi>
      <bibkey>burns-etal-2021-profiling</bibkey>
      <video href="2021.naacl-main.389.mp4"/>
      <pwccode url="https://github.com/quantitativecriticismlab/naacl-hlt-2021-latin-intertextuality" additional="false">quantitativecriticismlab/naacl-hlt-2021-latin-intertextuality</pwccode>
    </paper>
    <paper id="390">
      <title>Identifying inherent disagreement in natural language inference</title>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <pages>4908–4915</pages>
      <abstract>Natural language inference (NLI) is the task of determining whether a piece of text is entailed, contradicted by or unrelated to another piece of text. In this paper, we investigate how to tease systematic inferences (i.e., items for which people agree on the NLI label) apart from disagreement items (i.e., items which lead to different annotations), which most prior work has overlooked. To distinguish systematic inferences from disagreement items, we propose Artificial Annotators (AAs) to simulate the uncertainty in the annotation process by capturing the modes in annotations. Results on the CommitmentBank, a corpus of naturally occurring discourses in English, confirm that our approach performs statistically significantly better than all baselines. We further show that AAs learn linguistic patterns and context-dependent reasoning.</abstract>
      <url hash="7ddaf53b">2021.naacl-main.390</url>
      <doi>10.18653/v1/2021.naacl-main.390</doi>
      <bibkey>zhang-de-marneffe-2021-identifying</bibkey>
      <video href="2021.naacl-main.390.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="391">
      <title>Modeling Human Mental States with an Entity-based Narrative Graph</title>
      <author><first>I-Ta</first><last>Lee</last></author>
      <author><first>Maria Leonor</first><last>Pacheco</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>4916–4926</pages>
      <abstract>Understanding narrative text requires capturing characters’ motivations, goals, and mental states. This paper proposes an Entity-based Narrative Graph (ENG) to model the internal- states of characters in a story. We explicitly model entities, their interactions and the context in which they appear, and learn rich representations for them. We experiment with different task-adaptive pre-training objectives, in-domain training, and symbolic inference to capture dependencies between different decisions in the output space. We evaluate our model on two narrative understanding tasks: predicting character mental states, and desire fulfillment, and conduct a qualitative analysis.</abstract>
      <url hash="e6036ba8">2021.naacl-main.391</url>
      <doi>10.18653/v1/2021.naacl-main.391</doi>
      <bibkey>lee-etal-2021-modeling</bibkey>
      <video href="2021.naacl-main.391.mp4"/>
      <pwccode url="https://github.com/doug919/entity_based_narrative_graph" additional="false">doug919/entity_based_narrative_graph</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/desiredb">DesireDB</pwcdataset>
    </paper>
    <paper id="392">
      <title>A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation</title>
      <author><first>Yan</first><last>Zeng</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <pages>4927–4939</pages>
      <abstract>Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer – conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on the labeled text data. Experimental results show that our approach outperforms the state-of-the-art models by leveraging the labeled texts, and it also obtains larger improvement in performance comparing to the previous methods to leverage text data.</abstract>
      <url hash="ff9b00c5">2021.naacl-main.392</url>
      <doi>10.18653/v1/2021.naacl-main.392</doi>
      <bibkey>zeng-nie-2021-simple</bibkey>
      <video href="2021.naacl-main.392.mp4"/>
    </paper>
    <paper id="393">
      <title>Hurdles to Progress in Long-form Question Answering</title>
      <author><first>Kalpesh</first><last>Krishna</last></author>
      <author><first>Aurko</first><last>Roy</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>4940–4957</pages>
      <abstract>The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system’s generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.</abstract>
      <url hash="4cc7c7fb">2021.naacl-main.393</url>
      <attachment type="OptionalSupplementaryData" hash="031c8096">2021.naacl-main.393.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.393</doi>
      <bibkey>krishna-etal-2021-hurdles</bibkey>
      <video href="2021.naacl-main.393.mp4"/>
      <pwccode url="https://github.com/martiansideofthemoon/hurdles-longform-qa" additional="false">martiansideofthemoon/hurdles-longform-qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/eli5">ELI5</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pg-19">PG-19</pwcdataset>
    </paper>
    <paper id="394">
      <title><fixed-case>ENTRUST</fixed-case>: Argument Reframing with Language Models and Entailment</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Christopher</first><last>Hidey</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>4958–4971</pages>
      <abstract>Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker. Differences in lexical framing, the focus of our work, can have large effects on peoples’ opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for “connotations” to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a post-decoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear.</abstract>
      <url hash="2e42eeb8">2021.naacl-main.394</url>
      <doi>10.18653/v1/2021.naacl-main.394</doi>
      <bibkey>chakrabarty-etal-2021-entrust</bibkey>
      <video href="2021.naacl-main.394.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="395">
      <title>Paragraph-level Simplification of Medical Texts</title>
      <author><first>Ashwin</first><last>Devaraj</last></author>
      <author><first>Iain</first><last>Marshall</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <pages>4972–4984</pages>
      <abstract>We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing “jargon” terms; we find that this yields improvements over baselines in terms of readability.</abstract>
      <url hash="18892798">2021.naacl-main.395</url>
      <doi>10.18653/v1/2021.naacl-main.395</doi>
      <bibkey>devaraj-etal-2021-paragraph</bibkey>
      <video href="2021.naacl-main.395.mp4"/>
      <pwccode url="https://github.com/AshOlogn/Paragraph-level-Simplification-of-Medical-Texts" additional="false">AshOlogn/Paragraph-level-Simplification-of-Medical-Texts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="396">
      <title>An Empirical Study on Neural Keyphrase Generation</title>
      <author><first>Rui</first><last>Meng</last></author>
      <author><first>Xingdi</first><last>Yuan</last></author>
      <author><first>Tong</first><last>Wang</last></author>
      <author><first>Sanqiang</first><last>Zhao</last></author>
      <author><first>Adam</first><last>Trischler</last></author>
      <author><first>Daqing</first><last>He</last></author>
      <pages>4985–5007</pages>
      <abstract>Recent years have seen a flourishing of neural keyphrase generation (KPG) works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system’s generalization performance. In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models. We hope this study can help clarify some of the uncertainties surrounding the KPG task and facilitate future research on this topic.</abstract>
      <url hash="29fd6db8">2021.naacl-main.396</url>
      <doi>10.18653/v1/2021.naacl-main.396</doi>
      <bibkey>meng-etal-2021-empirical</bibkey>
      <video href="2021.naacl-main.396.mp4"/>
      <pwccode url="https://github.com/memray/OpenNMT-kpg-release" additional="false">memray/OpenNMT-kpg-release</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="397">
      <title>Attention Head Masking for Inference Time Content Selection in Abstractive Summarization</title>
      <author><first>Shuyang</first><last>Cao</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>5008–5016</pages>
      <abstract>How can we effectively inform content selection in Transformer-based abstractive summarization models? In this work, we present a simple-yet-effective attention head masking technique, which is applied on encoder-decoder attentions to pinpoint salient content at inference time. Using attention head masking, we are able to reveal the relation between encoder-decoder attentions and content selection behaviors of summarization models. We then demonstrate its effectiveness on three document summarization datasets based on both in-domain and cross-domain settings. Importantly, our models outperform prior state-of-the-art models on CNN/Daily Mail and New York Times datasets. Moreover, our inference-time masking technique is also data-efficient, requiring only 20% of the training samples to outperform BART fine-tuned on the full CNN/DailyMail dataset.</abstract>
      <url hash="37b9dbe7">2021.naacl-main.397</url>
      <doi>10.18653/v1/2021.naacl-main.397</doi>
      <bibkey>cao-wang-2021-attention</bibkey>
      <video href="2021.naacl-main.397.mp4"/>
    </paper>
    <paper id="398">
      <title>Factual Probing Is [<fixed-case>MASK</fixed-case>]: Learning vs. Learning to Recall</title>
      <author><first>Zexuan</first><last>Zhong</last></author>
      <author><first>Dan</first><last>Friedman</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>5017–5033</pages>
      <abstract>Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the model’s prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OptiPrompt, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle “learning” from “learning to recall”, providing a more detailed picture of what different prompts can reveal about pre-trained language models.</abstract>
      <url hash="55e75ce4">2021.naacl-main.398</url>
      <revision id="1" href="2021.naacl-main.398v1" hash="bf4b6ce3"/>
      <revision id="2" href="2021.naacl-main.398v2" hash="55e75ce4" date="2021-05-25">Updated a citation.</revision>
      <doi>10.18653/v1/2021.naacl-main.398</doi>
      <bibkey>zhong-etal-2021-factual</bibkey>
      <video href="2021.naacl-main.398.mp4"/>
      <pwccode url="https://github.com/princeton-nlp/OptiPrompt" additional="true">princeton-nlp/OptiPrompt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="399">
      <title>Evaluating Saliency Methods for Neural Language Models</title>
      <author><first>Shuoyang</first><last>Ding</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>5034–5052</pages>
      <abstract>Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentence-level and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights.</abstract>
      <url hash="07dd3b47">2021.naacl-main.399</url>
      <doi>10.18653/v1/2021.naacl-main.399</doi>
      <bibkey>ding-koehn-2021-evaluating</bibkey>
      <video href="2021.naacl-main.399.mp4"/>
      <pwccode url="https://github.com/shuoyangd/tarsius" additional="false">shuoyangd/tarsius</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="400">
      <title>Contextualized Perturbation for Textual Adversarial Attack</title>
      <author><first>Dianqi</first><last>Li</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Liqun</first><last>Chen</last></author>
      <author><first>Chris</first><last>Brockett</last></author>
      <author><first>Ming-Ting</first><last>Sun</last></author>
      <author><first>Bill</first><last>Dolan</last></author>
      <pages>5053–5069</pages>
      <abstract>Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.</abstract>
      <url hash="5aa24760">2021.naacl-main.400</url>
      <doi>10.18653/v1/2021.naacl-main.400</doi>
      <bibkey>li-etal-2021-contextualized</bibkey>
      <video href="2021.naacl-main.400.mp4"/>
      <pwccode url="https://github.com/cookielee77/CLARE" additional="false">cookielee77/CLARE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="401">
      <title><fixed-case>D</fixed-case>irect<fixed-case>P</fixed-case>robe: Studying Representations without Classifiers</title>
      <author><first>Yichu</first><last>Zhou</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <pages>5070–5083</pages>
      <abstract>Understanding how linguistic structure is encoded in contextualized embedding could help explain their impressive performance across NLP. Existing approaches for probing them usually call for training classifiers and use the accuracy, mutual information, or complexity as a proxy for the representation’s goodness. In this work, we argue that doing so can be unreliable because different representations may need different classifiers. We develop a heuristic, DirectProbe, that directly studies the geometry of a representation by building upon the notion of a version space for a task. Experiments with several linguistic tasks and contextualized embeddings show that, even without training classifiers, DirectProbe can shine lights on how an embedding space represents labels and also anticipate the classifier performance for the representation.</abstract>
      <url hash="49a6f439">2021.naacl-main.401</url>
      <doi>10.18653/v1/2021.naacl-main.401</doi>
      <bibkey>zhou-srikumar-2021-directprobe</bibkey>
      <video href="2021.naacl-main.401.mp4"/>
      <revision id="1" href="2021.naacl-main.401v1" hash="89156f56"/>
      <revision id="2" href="2021.naacl-main.401v2" hash="49a6f439" date="2023-03-03">Corrected paper results due to code error.</revision>
      <pwccode url="https://github.com/utahnlp/DirectProbe" additional="false">utahnlp/DirectProbe</pwccode>
    </paper>
    <paper id="402">
      <title>Evaluating the Values of Sources in Transfer Learning</title>
      <author><first>Md Rizwan</first><last>Parvez</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>5084–5116</pages>
      <abstract>Transfer learning that adapts a model trained on data-rich sources to low-resource targets has been widely applied in natural language processing (NLP). However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a model, it is essential to understand the values of the sources. In this paper, we develop , an efficient source valuation framework for quantifying the usefulness of the sources (e.g., ) in transfer learning based on the Shapley value method. Experiments and comprehensive analyses on both cross-domain and cross-lingual transfers demonstrate that our framework is not only effective in choosing useful transfer sources but also the source values match the intuitive source-target similarity.</abstract>
      <url hash="8bf9997f">2021.naacl-main.402</url>
      <doi>10.18653/v1/2021.naacl-main.402</doi>
      <bibkey>parvez-chang-2021-evaluating</bibkey>
      <video href="2021.naacl-main.402.mp4"/>
      <pwccode url="https://github.com/rizwan09/NLPDV" additional="false">rizwan09/NLPDV</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="403">
      <title>Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications</title>
      <author><first>Daniel</first><last>Biś</last></author>
      <author><first>Maksim</first><last>Podkorytov</last></author>
      <author><first>Xiuwen</first><last>Liu</last></author>
      <pages>5117–5130</pages>
      <abstract>The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set, the embeddings drift and share common components, manifested in their shape in all the models we have empirically tested. Our experiments show that isotropy can be restored using a simple transformation.</abstract>
      <url hash="c5528f55">2021.naacl-main.403</url>
      <doi>10.18653/v1/2021.naacl-main.403</doi>
      <bibkey>bis-etal-2021-much</bibkey>
      <video href="2021.naacl-main.403.mp4"/>
      <pwccode url="https://github.com/danielbis/toomuchincommon" additional="false">danielbis/toomuchincommon</pwccode>
    </paper>
    <paper id="404">
      <title>On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies</title>
      <author><first>Tianyi</first><last>Zhang</last></author>
      <author><first>Tatsunori B.</first><last>Hashimoto</last></author>
      <pages>5131–5146</pages>
      <abstract>We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).</abstract>
      <url hash="e38eece1">2021.naacl-main.404</url>
      <doi>10.18653/v1/2021.naacl-main.404</doi>
      <bibkey>zhang-hashimoto-2021-inductive</bibkey>
      <video href="2021.naacl-main.404.mp4"/>
      <pwccode url="https://github.com/tatsu-lab/mlm_inductive_bias" additional="false">tatsu-lab/mlm_inductive_bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="405">
      <title>Limitations of Autoregressive Models and Their Alternatives</title>
      <author><first>Chu-Cheng</first><last>Lin</last></author>
      <author><first>Aaron</first><last>Jaech</last></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Matthew R.</first><last>Gormley</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <pages>5147–5173</pages>
      <abstract>Standard autoregressive language models perform only polynomial-time computation to compute the probability of the next symbol. While this is attractive, it means they cannot model distributions whose next-symbol probability is <i>hard</i> to compute. Indeed, they cannot even model them well enough to solve associated <i>easy</i> decision problems for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless the model is given access to oracle parameters that grow <i>superpolynomially</i> in sequence length. Thus, simply training larger autoregressive language models is not a panacea for NLP. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.</abstract>
      <url hash="49e12cc6">2021.naacl-main.405</url>
      <revision id="1" href="2021.naacl-main.405v1" hash="93f6fe2b"/>
      <revision id="2" href="2021.naacl-main.405v2" hash="49e12cc6" date="2021-05-31">Various corrections through the paper</revision>
      <doi>10.18653/v1/2021.naacl-main.405</doi>
      <bibkey>lin-etal-2021-limitations</bibkey>
      <video href="2021.naacl-main.405.mp4"/>
    </paper>
    <paper id="406">
      <title>On the Transformer Growth for Progressive <fixed-case>BERT</fixed-case> Training</title>
      <author><first>Xiaotao</first><last>Gu</last></author>
      <author><first>Liyuan</first><last>Liu</last></author>
      <author><first>Hongkun</first><last>Yu</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>5174–5180</pages>
      <abstract>As the excessive pre-training cost arouses the need to improve efficiency, considerable efforts have been made to train BERT progressively–start from an inferior but low-cost model and gradually increase the computational complexity. Our objective is to help advance the understanding of such Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture selection, Transformer growth also favors compound scaling. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give practical guidance for operator selection. In light of our analyses, the proposed method CompoundGrow speeds up BERT pre-training by 73.6% and 82.2% for the base and large models respectively while achieving comparable performances.</abstract>
      <url hash="4e971e21">2021.naacl-main.406</url>
      <doi>10.18653/v1/2021.naacl-main.406</doi>
      <bibkey>gu-etal-2021-transformer</bibkey>
      <video href="2021.naacl-main.406.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="407">
      <title>Revisiting Simple Neural Probabilistic Language Models</title>
      <author><first>Simeng</first><last>Sun</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>5181–5188</pages>
      <abstract>Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of Bengio et al. (2003), which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a baseline Transformer with short input contexts but struggles to handle long-term dependencies. Inspired by this result, we modify the Transformer by replacing its first self-attention layer with the NPLM’s local concatenation layer, which results in small but consistent perplexity decreases across three word-level language modeling datasets.</abstract>
      <url hash="6e0306af">2021.naacl-main.407</url>
      <doi>10.18653/v1/2021.naacl-main.407</doi>
      <bibkey>sun-iyyer-2021-revisiting</bibkey>
      <video href="2021.naacl-main.407.mp4"/>
      <pwccode url="https://github.com/SimengSun/revisit-nplm" additional="false">SimengSun/revisit-nplm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="408">
      <title><fixed-case>R</fixed-case>ead<fixed-case>T</fixed-case>wice: Reading Very Large Documents with Memories</title>
      <author><first>Yury</first><last>Zemlyanskiy</last></author>
      <author><first>Joshua</first><last>Ainslie</last></author>
      <author><first>Michiel</first><last>de Jong</last></author>
      <author><first>Philip</first><last>Pham</last></author>
      <author><first>Ilya</first><last>Eckstein</last></author>
      <author><first>Fei</first><last>Sha</last></author>
      <pages>5189–5195</pages>
      <abstract>Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose ReadTwice, a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books.</abstract>
      <url hash="7995febf">2021.naacl-main.408</url>
      <doi>10.18653/v1/2021.naacl-main.408</doi>
      <bibkey>zemlyanskiy-etal-2021-readtwice</bibkey>
      <video href="2021.naacl-main.408.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="409">
      <title><fixed-case>SCRIPT</fixed-case>: Self-Critic <fixed-case>P</fixed-case>re<fixed-case>T</fixed-case>raining of Transformers</title>
      <author><first>Erik</first><last>Nijkamp</last></author>
      <author><first>Bo</first><last>Pang</last></author>
      <author><first>Ying Nian</first><last>Wu</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>5196–5202</pages>
      <abstract>We introduce Self-CRItic Pretraining Transformers (SCRIPT) for representation learning of text. The popular masked language modeling (MLM) pretraining methods like BERT replace some tokens with [MASK] and an encoder is trained to recover them, while ELECTRA trains a discriminator to detect replaced tokens proposed by a generator. In contrast, we train a language model as in MLM and further derive a discriminator or critic on top of the encoder without using any additional parameters. That is, the model itself is a critic. SCRIPT combines MLM training and discriminative training for learning rich representations and compute- and sample-efficiency. We demonstrate improved sample-efficiency in pretraining and enhanced representations evidenced by improved downstream task performance on GLUE and SQuAD over strong baselines. Also, the self-critic scores can be directly used as pseudo-log-likelihood for efficient scoring.</abstract>
      <url hash="54cf3f04">2021.naacl-main.409</url>
      <doi>10.18653/v1/2021.naacl-main.409</doi>
      <bibkey>nijkamp-etal-2021-script</bibkey>
      <video href="2021.naacl-main.409.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openwebtext">OpenWebText</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="410">
      <title>Learning How to Ask: Querying <fixed-case>LM</fixed-case>s with Mixtures of Soft Prompts</title>
      <author><first>Guanghui</first><last>Qin</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <pages>5203–5212</pages>
      <abstract>Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to “fill in the blank” in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent—either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of “soft words,” i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.</abstract>
      <url hash="eb8f3e6e">2021.naacl-main.410</url>
      <attachment type="OptionalSupplementaryCode" hash="83db5959">2021.naacl-main.410.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="ab2eafc7">2021.naacl-main.410.OptionalSupplementaryData.zip</attachment>
      <award>Best Short Paper</award>
      <doi>10.18653/v1/2021.naacl-main.410</doi>
      <bibkey>qin-eisner-2021-learning</bibkey>
      <video href="2021.naacl-main.410.mp4"/>
      <pwccode url="https://github.com/hiaoxui/soft-prompts" additional="true">hiaoxui/soft-prompts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="411">
      <title>Nutri-bullets Hybrid: Consensual Multi-document Summarization</title>
      <author><first>Darsh</first><last>Shah</last></author>
      <author><first>Lili</first><last>Yu</last></author>
      <author><first>Tao</first><last>Lei</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <pages>5213–5222</pages>
      <abstract>We present a method for generating comparative summaries that highlight similarities and contradictions in input documents. The key challenge in creating such summaries is the lack of large parallel training data required for training typical summarization systems. To this end, we introduce a hybrid generation approach inspired by traditional concept-to-text systems. To enable accurate comparison between different sources, the model first learns to extract pertinent relations from input documents. The content planning component uses deterministic operators to aggregate these relations after identifying a subset for inclusion into a summary. The surface realization component lexicalizes this information using a text-infilling language model. By separately modeling content selection and realization, we can effectively train them with limited annotations. We implemented and tested the model in the domain of nutrition and health – rife with inconsistencies. Compared to conventional methods, our framework leads to more faithful, relevant and aggregation-sensitive summarization – while being equally fluent.</abstract>
      <url hash="b96b7717">2021.naacl-main.411</url>
      <attachment type="OptionalSupplementaryCode" hash="9b37be1e">2021.naacl-main.411.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="94229b49">2021.naacl-main.411.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.411</doi>
      <bibkey>shah-etal-2021-nutri</bibkey>
      <video href="2021.naacl-main.411.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/healthline">Healthline</pwcdataset>
    </paper>
    <paper id="412">
      <title><fixed-case>AVA</fixed-case>: an Automatic e<fixed-case>V</fixed-case>aluation Approach for Question Answering Systems</title>
      <author><first>Thuy</first><last>Vu</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>5223–5233</pages>
      <abstract>We introduce AVA, an automatic evaluation approach for Question Answering, which given a set of questions associated with Gold Standard answers (references), can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference texts. This allows for effectively assessing answer correctness using similarity between the reference and an automatic answer, biased towards the question semantics. To design, train, and test AVA, we built multiple large training, development, and test sets on public and industrial benchmarks. Our innovative solutions achieve up to 74.7% F1 score in predicting human judgment for single answers. Additionally, AVA can be used to evaluate the overall system Accuracy with an error lower than 7% at 95% of confidence when measured on several QA systems.</abstract>
      <url hash="aaebf168">2021.naacl-main.412</url>
      <doi>10.18653/v1/2021.naacl-main.412</doi>
      <bibkey>vu-moschitti-2021-ava</bibkey>
      <video href="2021.naacl-main.412.mp4"/>
    </paper>
    <paper id="413">
      <title><fixed-case>S</fixed-case>pan<fixed-case>P</fixed-case>redict: Extraction of Predictive Document Spans with Neural Attention</title>
      <author><first>Vivek</first><last>Subramanian</last></author>
      <author><first>Matthew</first><last>Engelhard</last></author>
      <author><first>Sam</first><last>Berchuck</last></author>
      <author><first>Liqun</first><last>Chen</last></author>
      <author><first>Ricardo</first><last>Henao</last></author>
      <author><first>Lawrence</first><last>Carin</last></author>
      <pages>5234–5258</pages>
      <abstract>In many natural language processing applications, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in clinical notes not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves differentiability, allowing scalable inference via stochastic gradient descent. Further, the model decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only document labels, not ground-truth spans. Results show that our model identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving classification performance.</abstract>
      <url hash="13a4d027">2021.naacl-main.413</url>
      <attachment type="OptionalSupplementaryCode" hash="65e3c865">2021.naacl-main.413.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.413</doi>
      <bibkey>subramanian-etal-2021-spanpredict</bibkey>
      <video href="2021.naacl-main.413.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="414">
      <title>Text Editing by Command</title>
      <author><first>Felix</first><last>Faltings</last></author>
      <author><first>Michel</first><last>Galley</last></author>
      <author><first>Gerold</first><last>Hintz</last></author>
      <author><first>Chris</first><last>Brockett</last></author>
      <author><first>Chris</first><last>Quirk</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Bill</first><last>Dolan</last></author>
      <pages>5259–5274</pages>
      <abstract>A prevailing paradigm in neural text generation is one-shot generation, where text is produced in a single step. The one-shot setting is inadequate, however, when the constraints the user wishes to impose on the generated text are dynamic, especially when authoring longer documents. We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text. To this end, we propose a novel text editing task, and introduce WikiDocEdits, a dataset of single-sentence edits crawled from Wikipedia. We show that our Interactive Editor, a transformer-based model trained on this dataset, outperforms baselines and obtains positive results in both automatic and human evaluations. We present empirical and qualitative analyses of this model’s performance.</abstract>
      <url hash="5ea5f670">2021.naacl-main.414</url>
      <doi>10.18653/v1/2021.naacl-main.414</doi>
      <bibkey>faltings-etal-2021-text</bibkey>
      <video href="2021.naacl-main.414.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wikidocedits">WikiDocEdits</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiatomicedits">WikiAtomicEdits</pwcdataset>
    </paper>
    <paper id="415">
      <title>A Deep Metric Learning Approach to Account Linking</title>
      <author><first>Aleem</first><last>Khan</last></author>
      <author><first>Elizabeth</first><last>Fleming</last></author>
      <author><first>Noah</first><last>Schofield</last></author>
      <author><first>Marcus</first><last>Bishop</last></author>
      <author><first>Nicholas</first><last>Andrews</last></author>
      <pages>5275–5287</pages>
      <abstract>We consider the task of linking social media accounts that belong to the same author in an automated fashion on the basis of the content and meta-data of the corresponding document streams. We focus on learning an embedding that maps variable-sized samples of user activity–ranging from single posts to entire months of activity–to a vector space, where samples by the same author map to nearby points. Our approach does not require human-annotated data for training purposes, which allows us to leverage large amounts of social media content. The proposed model outperforms several competitive baselines under a novel evaluation framework modeled after established recognition benchmarks in other domains. Our method achieves high linking accuracy, even with small samples from accounts not seen at training time, a prerequisite for practical applications of the proposed linking framework.</abstract>
      <url hash="cce73231">2021.naacl-main.415</url>
      <doi>10.18653/v1/2021.naacl-main.415</doi>
      <bibkey>khan-etal-2021-deep</bibkey>
      <video href="2021.naacl-main.415.mp4"/>
      <pwccode url="https://github.com/noa/naacl2021" additional="true">noa/naacl2021</pwccode>
    </paper>
    <paper id="416">
      <title>Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation</title>
      <author><first>Yasuhide</first><last>Miura</last></author>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Emily</first><last>Tsai</last></author>
      <author><first>Curtis</first><last>Langlotz</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>5288–5304</pages>
      <abstract>Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or BLEU, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and consistent radiology reports: one that encourages the system to generate radiology domain entities consistent with the reference, and one that uses natural language inference to encourage these entities to be described in inferentially consistent ways. We combine these with the novel use of an existing semantic equivalence metric (BERTScore). We further propose a report generation system that optimizes these rewards via reinforcement learning. On two open radiology report datasets, our system substantially improved the F1 score of a clinical information extraction performance by +22.1 (Delta +63.9%). We further show via a human evaluation and a qualitative analysis that our system leads to generations that are more factually complete and consistent compared to the baselines.</abstract>
      <url hash="e4e72a3e">2021.naacl-main.416</url>
      <doi>10.18653/v1/2021.naacl-main.416</doi>
      <bibkey>miura-etal-2021-improving</bibkey>
      <video href="2021.naacl-main.416.mp4"/>
      <pwccode url="https://github.com/ysmiura/ifcc" additional="true">ysmiura/ifcc</pwccode>
    </paper>
    <paper id="417">
      <title>Multimodal End-to-End Sparse Model for Emotion Recognition</title>
      <author><first>Wenliang</first><last>Dai</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>5305–5316</pages>
      <abstract>Existing works in multimodal affective computing tasks, such as emotion recognition and personality recognition, generally adopt a two-phase pipeline by first extracting feature representations for each single modality with hand crafted algorithms, and then performing end-to-end learning with extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extracting algorithms does not generalize or scale well to different tasks, which can lead to sub-optimal performance. In this paper, we develop a fully end-to-end model that connects the two phases and optimizes them jointly. In addition, we restructure the current datasets to enable the fully end-to-end training. Furthermore, to reduce the computational overhead brought by the end-to-end model, we introduce a sparse cross-modal attention mechanism for the feature extraction. Experimental results show that our fully end-to-end model significantly surpasses the current state-of-the-art models based on the two-phase pipeline. Moreover, by adding the sparse cross-modal attention, our model can maintain the performance with around half less computation in the feature extraction part of the model.</abstract>
      <url hash="1166f180">2021.naacl-main.417</url>
      <doi>10.18653/v1/2021.naacl-main.417</doi>
      <bibkey>dai-etal-2021-multimodal</bibkey>
      <video href="2021.naacl-main.417.mp4"/>
      <pwccode url="https://github.com/wenliangdai/Multimodal-End2end-Sparse" additional="false">wenliangdai/Multimodal-End2end-Sparse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
    </paper>
    <paper id="418">
      <title><fixed-case>MIMOQA</fixed-case>: Multimodal Input Multimodal Output Question Answering</title>
      <author><first>Hrituraj</first><last>Singh</last></author>
      <author><first>Anshul</first><last>Nasery</last></author>
      <author><first>Denil</first><last>Mehta</last></author>
      <author><first>Aishwarya</first><last>Agarwal</last></author>
      <author><first>Jatin</first><last>Lamba</last></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last></author>
      <pages>5317–5332</pages>
      <abstract>Multimodal research has picked up significantly in the space of question answering with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task - MIMOQA - Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics.</abstract>
      <url hash="9da2260b">2021.naacl-main.418</url>
      <doi>10.18653/v1/2021.naacl-main.418</doi>
      <bibkey>singh-etal-2021-mimoqa</bibkey>
      <video href="2021.naacl-main.418.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/manymodalqa">ManyModalQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tvqa">TVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="419">
      <title><fixed-case>OCID</fixed-case>-Ref: A 3<fixed-case>D</fixed-case> Robotic Dataset With Embodied Language For Clutter Scene Grounding</title>
      <author><first>Ke-Jyun</first><last>Wang</last></author>
      <author><first>Yun-Hsuan</first><last>Liu</last></author>
      <author><first>Hung-Ting</first><last>Su</last></author>
      <author><first>Jen-Wei</first><last>Wang</last></author>
      <author><first>Yu-Siang</first><last>Wang</last></author>
      <author><first>Winston</first><last>Hsu</last></author>
      <author><first>Wen-Chin</first><last>Chen</last></author>
      <pages>5333–5338</pages>
      <abstract>To effectively apply robots in working environments and assist humans, it is essential to develop and evaluate how visual grounding (VG) can affect machine performance on occluded objects. However, current VG works are limited in working environments, such as offices and warehouses, where objects are usually occluded due to space utilization issues. In our work, we propose a novel OCID-Ref dataset featuring a referring expression segmentation task with referring expressions of occluded objects. OCID-Ref consists of 305,694 referring expressions from 2,300 scenes with providing RGB image and point cloud inputs. To resolve challenging occlusion issues, we argue that it’s crucial to take advantage of both 2D and 3D signals to resolve challenging occlusion issues. Our experimental results demonstrate the effectiveness of aggregating 2D and 3D signals but referring to occluded objects still remains challenging for the modern visual grounding systems. OCID-Ref is publicly available at https://github.com/lluma/OCID-Ref</abstract>
      <url hash="ed094776">2021.naacl-main.419</url>
      <attachment type="OptionalSupplementaryData" hash="be7e740e">2021.naacl-main.419.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.419</doi>
      <bibkey>wang-etal-2021-ocid</bibkey>
      <video href="2021.naacl-main.419.mp4"/>
      <pwccode url="https://github.com/lluma/OCID-Ref" additional="false">lluma/OCID-Ref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cops-ref">Cops-Ref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ocid">OCID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
    </paper>
    <paper id="420">
      <title>Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions</title>
      <author><first>Liunian Harold</first><last>Li</last></author>
      <author><first>Haoxuan</first><last>You</last></author>
      <author><first>Zhecan</first><last>Wang</last></author>
      <author><first>Alireza</first><last>Zareian</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>5339–5350</pages>
      <abstract>Pre-trained contextual vision-and-language (V&amp;L) models have achieved impressive performance on various benchmarks. However, existing models require a large amount of parallel image-caption data for pre-training. Such data are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V&amp;L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct “mask-and-predict” pre-training on text-only and image-only corpora and introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. We find that such a simple approach achieves performance close to a model pre-trained with aligned data, on four English V&amp;L benchmarks. Our work challenges the widely held notion that aligned data is necessary for V&amp;L pre-training, while significantly reducing the amount of supervision needed for V&amp;L models.</abstract>
      <url hash="43881f38">2021.naacl-main.420</url>
      <attachment type="OptionalSupplementaryData" hash="1fa89178">2021.naacl-main.420.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.420</doi>
      <bibkey>li-etal-2021-unsupervised</bibkey>
      <video href="2021.naacl-main.420.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/refcoco">RefCOCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="421">
      <title>Multitasking Inhibits Semantic Drift</title>
      <author><first>Athul Paul</first><last>Jacob</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <pages>5351–5366</pages>
      <abstract>When intelligent agents communicate to accomplish shared goals, how do these goals shape the agents’ language? We study the dynamics of learning in latent language policies (LLPs), in which instructor agents generate natural-language subgoal descriptions and executor agents map these descriptions to low-level actions. LLPs can solve challenging long-horizon reinforcement learning problems and provide a rich model for studying task-oriented language use. But previous work has found that LLP training is prone to semantic drift (use of messages in ways inconsistent with their original natural language meanings). Here, we demonstrate theoretically and empirically that multitask training is an effective counter to this problem: we prove that multitask training eliminates semantic drift in a well-studied family of signaling games, and show that multitask training of neural LLPs in a complex strategy game reduces drift and while improving sample efficiency.</abstract>
      <url hash="6428e4ba">2021.naacl-main.421</url>
      <doi>10.18653/v1/2021.naacl-main.421</doi>
      <bibkey>jacob-etal-2021-multitasking</bibkey>
      <video href="2021.naacl-main.421.mp4"/>
    </paper>
    <paper id="422">
      <title>Probing Contextual Language Models for Common Ground with Visual Representations</title>
      <author><first>Gabriel</first><last>Ilharco</last></author>
      <author><first>Rowan</first><last>Zellers</last></author>
      <author><first>Ali</first><last>Farhadi</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>5367–5377</pages>
      <abstract>The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.</abstract>
      <url hash="81ce6752">2021.naacl-main.422</url>
      <doi>10.18653/v1/2021.naacl-main.422</doi>
      <bibkey>ilharco-etal-2021-probing</bibkey>
      <video href="2021.naacl-main.422.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="423">
      <title><fixed-case>BBAEG</fixed-case>: Towards <fixed-case>BERT</fixed-case>-based Biomedical Adversarial Example Generation for Text Classification</title>
      <author><first>Ishani</first><last>Mondal</last></author>
      <pages>5378–5384</pages>
      <abstract>Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the ever-increasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERT-MLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work.</abstract>
      <url hash="c88e1673">2021.naacl-main.423</url>
      <attachment type="OptionalSupplementaryData" hash="242b095d">2021.naacl-main.423.OptionalSupplementaryData.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.423</doi>
      <bibkey>mondal-2021-bbaeg</bibkey>
      <video href="2021.naacl-main.423.mp4"/>
      <pwccode url="https://github.com/Ishani-Mondal/BBAEG" additional="false">Ishani-Mondal/BBAEG</pwccode>
    </paper>
    <paper id="424">
      <title>Targeted Adversarial Training for Natural Language Understanding</title>
      <author><first>Lis</first><last>Pereira</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Hoifung</first><last>Poon</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <pages>5385–5393</pages>
      <abstract>We present a simple yet effective Targeted Adversarial Training (TAT) algorithm to improve adversarial training for natural language understanding. The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released upon acceptance of the paper.</abstract>
      <url hash="25498895">2021.naacl-main.424</url>
      <doi>10.18653/v1/2021.naacl-main.424</doi>
      <bibkey>pereira-etal-2021-targeted</bibkey>
      <video href="2021.naacl-main.424.mp4"/>
      <pwccode url="https://github.com/namisan/mt-dnn" additional="false">namisan/mt-dnn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="425">
      <title>Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection</title>
      <author><first>Xu</first><last>Guo</last></author>
      <author><first>Boyang</first><last>Li</last></author>
      <author><first>Han</first><last>Yu</last></author>
      <author><first>Chunyan</first><last>Miao</last></author>
      <pages>5394–5407</pages>
      <abstract>The existence of multiple datasets for sarcasm detection prompts us to apply transfer learning to exploit their commonality. The adversarial neural transfer (ANT) framework utilizes multiple loss terms that encourage the source-domain and the target-domain feature distributions to be similar while optimizing for domain-specific performance. However, these objectives may be in conflict, which can lead to optimization difficulties and sometimes diminished transfer. We propose a generalized latent optimization strategy that allows different losses to accommodate each other and improves training dynamics. The proposed method outperforms transfer learning and meta-learning baselines. In particular, we achieve 10.02% absolute performance gain over the previous state of the art on the iSarcasm dataset.</abstract>
      <url hash="b4a7bf64">2021.naacl-main.425</url>
      <doi>10.18653/v1/2021.naacl-main.425</doi>
      <bibkey>guo-etal-2021-latent</bibkey>
      <video href="2021.naacl-main.425.mp4"/>
      <pwccode url="https://github.com/guoxuxu/LOANT" additional="false">guoxuxu/LOANT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/isarcasm">iSarcasm</pwcdataset>
    </paper>
    <paper id="426">
      <title>Self-training Improves Pre-training for Natural Language Understanding</title>
      <author><first>Jingfei</first><last>Du</last></author>
      <author><first>Edouard</first><last>Grave</last></author>
      <author><first>Beliz</first><last>Gunel</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Onur</first><last>Celebi</last></author>
      <author><first>Michael</first><last>Auli</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <author><first>Alexis</first><last>Conneau</last></author>
      <pages>5408–5418</pages>
      <abstract>Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.</abstract>
      <url hash="a828c0d6">2021.naacl-main.426</url>
      <doi>10.18653/v1/2021.naacl-main.426</doi>
      <bibkey>du-etal-2021-self</bibkey>
      <pwccode url="https://github.com/facebookresearch/SentAugment" additional="false">facebookresearch/SentAugment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="427">
      <title>Supporting Clustering with Contrastive Learning</title>
      <author><first>Dejiao</first><last>Zhang</last></author>
      <author><first>Feng</first><last>Nan</last></author>
      <author><first>Xiaokai</first><last>Wei</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Andrew O.</first><last>Arnold</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <pages>5419–5430</pages>
      <abstract>Unsupervised clustering aims at discovering the semantic categories of data according to some distance measured in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) – a novel framework to leverage contrastive learning to promote better separation. We assess the performance of SCCL on short text clustering and show that SCCL significantly advances the state-of-the-art results on most benchmark datasets with 3%-11% improvement on Accuracy and 4%-15% improvement on Normalized Mutual Information. Furthermore, our quantitative analysis demonstrates the effectiveness of SCCL in leveraging the strengths of both bottom-up instance discrimination and top-down clustering to achieve better intra-cluster and inter-cluster distances when evaluated with the ground truth cluster labels.</abstract>
      <url hash="86a3ca4d">2021.naacl-main.427</url>
      <doi>10.18653/v1/2021.naacl-main.427</doi>
      <bibkey>zhang-etal-2021-supporting</bibkey>
      <video href="2021.naacl-main.427.mp4"/>
      <pwccode url="https://github.com/amazon-research/sccl" additional="true">amazon-research/sccl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
    </paper>
    <paper id="428">
      <title><fixed-case>TITA</fixed-case>: A Two-stage Interaction and Topic-Aware Text Matching Model</title>
      <author><first>Xingwu</first><last>Sun</last></author>
      <author><first>Yanling</first><last>Cui</last></author>
      <author><first>Hongyin</first><last>Tang</last></author>
      <author><first>Qiuyu</first><last>Zhu</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Beihong</first><last>Jin</last></author>
      <pages>5431–5440</pages>
      <abstract>In this paper, we focus on the problem of keyword and document matching by considering different relevance levels. In our recommendation system, different people follow different hot keywords with interest. We need to attach documents to each keyword and then distribute the documents to people who follow these keywords. The ideal documents should have the same topic with the keyword, which we call topic-aware relevance. In other words, topic-aware relevance documents are better than partially-relevance ones in this application. However, previous tasks never define topic-aware relevance clearly. To tackle this problem, we define a three-level relevance in keyword-document matching task: topic-aware relevance, partially-relevance and irrelevance. To capture the relevance between the short keyword and the document at above-mentioned three levels, we should not only combine the latent topic of the document with its deep neural representation, but also model complex interactions between the keyword and the document. To this end, we propose a Two-stage Interaction and Topic-Aware text matching model (TITA). In terms of “topic-aware”, we introduce neural topic model to analyze the topic of the document and then use it to further encode the document. In terms of “two-stage interaction”, we propose two successive stages to model complex interactions between the keyword and the document. Extensive experiments reveal that TITA outperforms other well-designed baselines and shows excellent performance in our recommendation system.</abstract>
      <url hash="d061f2a3">2021.naacl-main.428</url>
      <doi>10.18653/v1/2021.naacl-main.428</doi>
      <bibkey>sun-etal-2021-tita</bibkey>
      <video href="2021.naacl-main.428.mp4"/>
    </paper>
    <paper id="429">
      <title>Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction</title>
      <author><first>Zhenghao</first><last>Liu</last></author>
      <author><first>Xiaoyuan</first><last>Yi</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Liner</first><last>Yang</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>5441–5452</pages>
      <abstract>Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/thunlp/VERNet.</abstract>
      <url hash="4b3cb79b">2021.naacl-main.429</url>
      <doi>10.18653/v1/2021.naacl-main.429</doi>
      <bibkey>liu-etal-2021-neural</bibkey>
      <video href="2021.naacl-main.429.mp4"/>
      <pwccode url="https://github.com/thunlp/VERNet" additional="false">thunlp/VERNet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
    </paper>
    <paper id="430">
      <title>Neural Network Surgery: Injecting Data Patterns into Pre-trained Models with Minimal Instance-wise Side Effects</title>
      <author><first>Zhiyuan</first><last>Zhang</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Bin</first><last>He</last></author>
      <pages>5453–5466</pages>
      <abstract>Side effects during neural network tuning are typically measured by overall accuracy changes. However, we find that even with similar overall accuracy, existing tuning methods result in non-negligible instance-wise side effects. Motivated by neuroscientific evidence and theoretical results, we demonstrate that side effects can be controlled by the number of changed parameters and thus, we propose to conduct <i>neural network surgery</i> by only modifying a limited number of parameters. Neural network surgery can be realized using diverse techniques and we investigate three lines of methods. Experimental results on representative tuning problems validate the effectiveness of the surgery approach. The dynamic selecting method achieves the best overall performance that not only satisfies the tuning goal but also induces fewer instance-wise side effects by changing only <tex-math>10^{-5}</tex-math> of the parameters.</abstract>
      <url hash="40c9a728">2021.naacl-main.430</url>
      <doi>10.18653/v1/2021.naacl-main.430</doi>
      <bibkey>zhang-etal-2021-neural</bibkey>
      <video href="2021.naacl-main.430.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="431">
      <title>Discrete Argument Representation Learning for Interactive Argument Pair Identification</title>
      <author><first>Lu</first><last>Ji</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>5467–5478</pages>
      <abstract>In this paper, we focus on identifying interactive argument pairs from two posts with opposite stances to a certain topic. Considering opinions are exchanged from different perspectives of the discussing topic, we study the discrete representations for arguments to capture varying aspects in argumentation languages (e.g., the debate focus and the participant behavior). Moreover, we utilize hierarchical structure to model post-wise information incorporating contextual knowledge. Experimental results on the large-scale dataset collected from CMV show that our proposed framework can significantly outperform the competitive baselines. Further analyses reveal why our model yields superior performance and prove the usefulness of our learned representations.</abstract>
      <url hash="ee0713b6">2021.naacl-main.431</url>
      <doi>10.18653/v1/2021.naacl-main.431</doi>
      <bibkey>ji-etal-2021-discrete</bibkey>
      <video href="2021.naacl-main.431.mp4"/>
    </paper>
    <paper id="432">
      <title>On Unifying Misinformation Detection</title>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Belinda Z.</first><last>Li</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <author><first>Hao</first><last>Ma</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <author><first>Madian</first><last>Khabsa</last></author>
      <pages>5479–5485</pages>
      <abstract>In this paper, we introduce UnifiedM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news, and verifying rumors. By grouping these tasks together, UnifiedM2 learns a richer representation of misinformation, which leads to state-of-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UnifiedM2’s learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and the model’s generalizability to unseen events.</abstract>
      <url hash="41a63720">2021.naacl-main.432</url>
      <doi>10.18653/v1/2021.naacl-main.432</doi>
      <bibkey>lee-etal-2021-unifying</bibkey>
      <video href="2021.naacl-main.432.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/basil">BASIL</pwcdataset>
    </paper>
    <paper id="433">
      <title>Frustratingly Easy Edit-based Linguistic Steganography with a Masked Language Model</title>
      <author><first>Honai</first><last>Ueoka</last></author>
      <author><first>Yugo</first><last>Murawaki</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>5486–5492</pages>
      <abstract>With advances in neural language models, the focus of linguistic steganography has shifted from edit-based approaches to generation-based ones. While the latter’s payload capacity is impressive, generating genuine-looking texts remains challenging. In this paper, we revisit edit-based linguistic steganography, with the idea that a masked language model offers an off-the-shelf solution. The proposed method eliminates painstaking rule construction and has a high payload capacity for an edit-based model. It is also shown to be more secure against automatic detection than a generation-based method while offering better control of the security/payload capacity trade-off.</abstract>
      <url hash="bd03723f">2021.naacl-main.433</url>
      <doi>10.18653/v1/2021.naacl-main.433</doi>
      <bibkey>ueoka-etal-2021-frustratingly</bibkey>
      <video href="2021.naacl-main.433.mp4"/>
      <pwccode url="https://github.com/ku-nlp/steganography-with-masked-lm" additional="false">ku-nlp/steganography-with-masked-lm</pwccode>
    </paper>
    <paper id="434">
      <title>Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning</title>
      <author><first>Jason</first><last>Wei</last></author>
      <author><first>Chengyu</first><last>Huang</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <author><first>Yu</first><last>Cheng</last></author>
      <author><first>Shiqi</first><last>Xu</last></author>
      <pages>5493–5500</pages>
      <abstract>Few-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation—a technique particularly suitable for training with limited data—for this few-shot, highly-multiclass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to 3.0% on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a two-stage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.</abstract>
      <url hash="af60bf89">2021.naacl-main.434</url>
      <doi>10.18653/v1/2021.naacl-main.434</doi>
      <bibkey>wei-etal-2021-shot</bibkey>
      <video href="2021.naacl-main.434.mp4"/>
      <pwccode url="https://github.com/jasonwei20/triplet-loss" additional="false">jasonwei20/triplet-loss</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="435">
      <title>Do <fixed-case>RNN</fixed-case> States Encode Abstract Phonological Alternations?</title>
      <author><first>Miikka</first><last>Silfverberg</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <author><first>Garrett</first><last>Nicolai</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>5501–5513</pages>
      <abstract>Sequence-to-sequence models have delivered impressive results in word formation tasks such as morphological inflection, often learning to model subtle morphophonological details with limited training data. Despite the performance, the opacity of neural models makes it difficult to determine whether complex generalizations are learned, or whether a kind of separate rote memorization of each morphophonological process takes place. To investigate whether complex alternations are simply memorized or whether there is some level of generalization across related sound changes in a sequence-to-sequence model, we perform several experiments on Finnish consonant gradation—a complex set of sound changes triggered in some words by certain suffixes. We find that our models often—though not always—encode 17 different consonant gradation processes in a handful of dimensions in the RNN. We also show that by scaling the activations in these dimensions we can control whether consonant gradation occurs and the direction of the gradation.</abstract>
      <url hash="73bfefed">2021.naacl-main.435</url>
      <doi>10.18653/v1/2021.naacl-main.435</doi>
      <bibkey>silfverberg-etal-2021-rnn</bibkey>
      <video href="2021.naacl-main.435.mp4"/>
    </paper>
    <paper id="436">
      <title>Pre-training with Meta Learning for <fixed-case>C</fixed-case>hinese Word Segmentation</title>
      <author><first>Zhen</first><last>Ke</last></author>
      <author><first>Liang</first><last>Shi</last></author>
      <author><first>Songtao</first><last>Sun</last></author>
      <author><first>Erli</first><last>Meng</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>5514–5523</pages>
      <abstract>Recent researches show that pre-trained models (PTMs) are beneficial to Chinese Word Segmentation (CWS). However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task. Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks. Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings.</abstract>
      <url hash="d50b60f7">2021.naacl-main.436</url>
      <doi>10.18653/v1/2021.naacl-main.436</doi>
      <bibkey>ke-etal-2021-pre</bibkey>
      <video href="2021.naacl-main.436.mp4"/>
    </paper>
    <paper id="437">
      <title>Decompose, Fuse and Generate: A Formation-Informed Method for <fixed-case>C</fixed-case>hinese Definition Generation</title>
      <author><first>Hua</first><last>Zheng</last></author>
      <author><first>Damai</first><last>Dai</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>5524–5531</pages>
      <abstract>In this paper, we tackle the task of Definition Generation (DG) in Chinese, which aims at automatically generating a definition for a word. Most existing methods take the source word as an indecomposable semantic unit. However, in parataxis languages like Chinese, word meanings can be composed using the word formation process, where a word (“桃花”, peach-blossom) is formed by formation components (“桃”, peach; “花”, flower) using a formation rule (Modifier-Head). Inspired by this process, we propose to enhance DG with word formation features. We build a formation-informed dataset, and propose a model DeFT, which Decomposes words into formation features, dynamically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust.</abstract>
      <url hash="e5413c3d">2021.naacl-main.437</url>
      <doi>10.18653/v1/2021.naacl-main.437</doi>
      <bibkey>zheng-etal-2021-decompose</bibkey>
      <video href="2021.naacl-main.437.mp4"/>
    </paper>
    <paper id="438">
      <title>User-Generated Text Corpus for Evaluating <fixed-case>J</fixed-case>apanese Morphological Analysis and Lexical Normalization</title>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>5532–5541</pages>
      <abstract>Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA/LN systems, we have constructed a publicly available Japanese UGT corpus. Our corpus comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGT-specific phenomena. Experiments on the corpus demonstrated the low performance of existing MA/LN methods for non-general words and non-standard forms, indicating that the corpus would be a challenging benchmark for further research on UGT.</abstract>
      <url hash="f1141938">2021.naacl-main.438</url>
      <doi>10.18653/v1/2021.naacl-main.438</doi>
      <bibkey>higashiyama-etal-2021-user</bibkey>
      <video href="2021.naacl-main.438.mp4"/>
      <pwccode url="https://github.com/shigashiyama/jlexnorm" additional="false">shigashiyama/jlexnorm</pwccode>
    </paper>
    <paper id="439">
      <title><fixed-case>GPT</fixed-case> Perdetry Test: Generating new meanings for new words</title>
      <author><first>Nikolay</first><last>Malkin</last></author>
      <author><first>Sameera</first><last>Lanka</last></author>
      <author><first>Pranav</first><last>Goel</last></author>
      <author><first>Sudha</first><last>Rao</last></author>
      <author><first>Nebojsa</first><last>Jojic</last></author>
      <pages>5542–5553</pages>
      <abstract>Human innovation in language, such as inventing new words, is a challenge for pretrained language models. We assess the ability of one large model, GPT-3, to process new words and decide on their meaning. We create a set of nonce words and prompt GPT-3 to generate their dictionary definitions. We find GPT-3 produces plausible definitions that align with human judgments. Moreover, GPT-3’s definitions are sometimes preferred to those invented by humans, signaling its intriguing ability not just to adapt, but to add to the evolving vocabulary of the English language.</abstract>
      <url hash="589f8485">2021.naacl-main.439</url>
      <doi>10.18653/v1/2021.naacl-main.439</doi>
      <bibkey>malkin-etal-2021-gpt</bibkey>
      <video href="2021.naacl-main.439.mp4"/>
    </paper>
    <paper id="440">
      <title>Universal Semantic Tagging for <fixed-case>E</fixed-case>nglish and <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese</title>
      <author><first>Wenxi</first><last>Li</last></author>
      <author><first>Yiyang</first><last>Hou</last></author>
      <author><first>Yajie</first><last>Ye</last></author>
      <author><first>Li</first><last>Liang</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <pages>5554–5566</pages>
      <abstract>Universal Semantic Tagging aims to provide lightweight unified analysis for all languages at the word level. Though the proposed annotation scheme is conceptually promising, the feasibility is only examined in four Indo–European languages. This paper is concerned with extending the annotation scheme to handle Mandarin Chinese and empirically study the plausibility of unifying meaning representations for multiple languages. We discuss a set of language-specific semantic phenomena, propose new annotation specifications and build a richly annotated corpus. The corpus consists of 1100 English–Chinese parallel sentences, where compositional semantic analysis is available for English, and another 1000 Chinese sentences which has enriched syntactic analysis. By means of the new annotations, we also evaluate a series of neural tagging models to gauge how successful semantic tagging can be: accuracies of 92.7% and 94.6% are obtained for Chinese and English respectively. The English tagging performance is remarkably better than the state-of-the-art by 7.7%.</abstract>
      <url hash="63148334">2021.naacl-main.440</url>
      <attachment type="OptionalSupplementaryData" hash="7bb47dd3">2021.naacl-main.440.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.440</doi>
      <bibkey>li-etal-2021-universal</bibkey>
      <video href="2021.naacl-main.440.mp4"/>
      <pwccode url="https://github.com/pkucoli/ust" additional="false">pkucoli/ust</pwccode>
    </paper>
    <paper id="441">
      <title><fixed-case>S</fixed-case>hadow<fixed-case>GNN</fixed-case>: Graph Projection Neural Network for Text-to-<fixed-case>SQL</fixed-case> Parser</title>
      <author><first>Zhi</first><last>Chen</last></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>Yanbin</first><last>Zhao</last></author>
      <author><first>Ruisheng</first><last>Cao</last></author>
      <author><first>Zihan</first><last>Xu</last></author>
      <author><first>Su</first><last>Zhu</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <pages>5567–5577</pages>
      <abstract>Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema. Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema. Finally, a SQL decoder with context-free grammar is applied. On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models. When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability. Our implementation will be open-sourced at <url>https://github.com/WowCZ/shadowgnn</url>
      </abstract>
      <url hash="d3fef69d">2021.naacl-main.441</url>
      <doi>10.18653/v1/2021.naacl-main.441</doi>
      <bibkey>chen-etal-2021-shadowgnn</bibkey>
      <video href="2021.naacl-main.441.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="442">
      <title>Contextualized and Generalized Sentence Representations by Contrastive Self-Supervised Learning: A Case Study on Discourse Relation Analysis</title>
      <author><first>Hirokazu</first><last>Kiyomaru</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <pages>5578–5584</pages>
      <abstract>We propose a method to learn contextualized and generalized sentence representations using contrastive self-supervised learning. In the proposed method, a model is given a text consisting of multiple sentences. One sentence is randomly selected as a target sentence. The model is trained to maximize the similarity between the representation of the target sentence with its context and that of the masked target sentence with the same context. Simultaneously, the model minimizes the similarity between the latter representation and the representation of a random sentence with the same context. We apply our method to discourse relation analysis in English and Japanese and show that it outperforms strong baseline methods based on BERT, XLNet, and RoBERTa.</abstract>
      <url hash="76a6090e">2021.naacl-main.442</url>
      <doi>10.18653/v1/2021.naacl-main.442</doi>
      <bibkey>kiyomaru-kurohashi-2021-contextualized</bibkey>
      <video href="2021.naacl-main.442.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
    </paper>
    <paper id="443">
      <title><fixed-case>AMR</fixed-case> Parsing with Action-Pointer Transformer</title>
      <author><first>Jiawei</first><last>Zhou</last></author>
      <author><first>Tahira</first><last>Naseem</last></author>
      <author><first>Ramón</first><last>Fernandez Astudillo</last></author>
      <author><first>Radu</first><last>Florian</last></author>
      <pages>5585–5598</pages>
      <abstract>Abstract Meaning Representation parsing is a sentence-to-graph prediction task where target nodes are not explicitly aligned to sentence tokens. However, since graph nodes are semantically based on one or more sentence tokens, implicit alignments can be derived. Transition-based parsers operate over the sentence from left to right, capturing this inductive bias via alignments at the cost of limited expressiveness. In this work, we propose a transition-based system that combines hard-attention over sentences with a target-side action pointer mechanism to decouple source tokens from node representations and address alignments. We model the transitions as well as the pointer mechanism through straightforward modifications within a single Transformer architecture. Parser state and graph structure information are efficiently encoded using attention heads. We show that our action-pointer approach leads to increased expressiveness and attains large gains (+1.6 points) against the best transition-based AMR parser in very similar conditions. While using no graph re-categorization, our single model yields the second best Smatch score on AMR 2.0 (81.8), which is further improved to 83.4 with silver data and ensemble decoding.</abstract>
      <url hash="6f18c4ee">2021.naacl-main.443</url>
      <doi>10.18653/v1/2021.naacl-main.443</doi>
      <bibkey>zhou-etal-2021-amr</bibkey>
      <video href="2021.naacl-main.443.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2020t02">LDC2020T02</pwcdataset>
    </paper>
    <paper id="444">
      <title><fixed-case>NL</fixed-case>-<fixed-case>EDIT</fixed-case>: Correcting Semantic Parse Errors through Natural Language Interaction</title>
      <author><first>Ahmed</first><last>Elgohary</last></author>
      <author><first>Christopher</first><last>Meek</last></author>
      <author><first>Matthew</first><last>Richardson</last></author>
      <author><first>Adam</first><last>Fourney</last></author>
      <author><first>Gonzalo</first><last>Ramos</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <pages>5599–5610</pages>
      <abstract>We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present NL-EDIT, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL parsers by up to 20% with only one turn of correction. We analyze the limitations of the model and discuss directions for improvement and evaluation. The code and datasets used in this paper are publicly available at http://aka.ms/NLEdit.</abstract>
      <url hash="623a4ff3">2021.naacl-main.444</url>
      <doi>10.18653/v1/2021.naacl-main.444</doi>
      <bibkey>elgohary-etal-2021-nl</bibkey>
      <video href="2021.naacl-main.444.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/splash">SPLASH</pwcdataset>
    </paper>
    <paper id="445">
      <title>Unsupervised Concept Representation Learning for Length-Varying Text Similarity</title>
      <author><first>Xuchao</first><last>Zhang</last></author>
      <author><first>Bo</first><last>Zong</last></author>
      <author><first>Wei</first><last>Cheng</last></author>
      <author><first>Jingchao</first><last>Ni</last></author>
      <author><first>Yanchi</first><last>Liu</last></author>
      <author><first>Haifeng</first><last>Chen</last></author>
      <pages>5611–5620</pages>
      <abstract>Measuring document similarity plays an important role in natural language processing tasks. Most existing document similarity approaches suffer from the information gap caused by context and vocabulary mismatches when comparing varying-length texts. In this paper, we propose an unsupervised concept representation learning approach to address the above issues. Specifically, we propose a novel Concept Generation Network (CGNet) to learn concept representations from the perspective of the entire text corpus. Moreover, a concept-based document matching method is proposed to leverage advances in the recognition of local phrase features and corpus-level concept features. Extensive experiments on real-world data sets demonstrate that new method can achieve a considerable improvement in comparing length-varying texts. In particular, our model achieved 6.5% better F1 Score compared to the best of the baseline models for a concept-project benchmark dataset.</abstract>
      <url hash="9fca36ac">2021.naacl-main.445</url>
      <doi>10.18653/v1/2021.naacl-main.445</doi>
      <bibkey>zhang-etal-2021-unsupervised</bibkey>
      <video href="2021.naacl-main.445.mp4"/>
    </paper>
    <paper id="446">
      <title>Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition</title>
      <author><first>Haolan</first><last>Zhan</last></author>
      <author><first>Hainan</first><last>Zhang</last></author>
      <author><first>Hongshen</first><last>Chen</last></author>
      <author><first>Zhuoye</first><last>Ding</last></author>
      <author><first>Yongjun</first><last>Bao</last></author>
      <author><first>Yanyan</first><last>Lan</last></author>
      <pages>5621–5630</pages>
      <abstract>Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2) the models often over-fit during training, resulting with incoherent response by referring to unrelated tokens from specific knowledge content in the testing phase; 3) although response is generated upon the dialogue history and knowledge, the models often tend to overlook the selected knowledge, and hence generates knowledge-irrelevant response. To address these problems, we proposed to explicitly model the knowledge transition in sequential multi-turn conversations by abstracting knowledge into topic tags. Besides, to fully utilizing the selected knowledge in generative process, we propose pre-training a knowledge-aware response generator to pay more attention on the selected knowledge. In particular, a sequential knowledge transition model equipped with a pre-trained knowledge-aware response generator (SKT-KG) formulates the high-level knowledge transition and fully utilizes the limited knowledge data. Experimental results on both structured and unstructured knowledge-grounded dialogue benchmarks indicate that our model achieves better performance over baseline models.</abstract>
      <url hash="78632b55">2021.naacl-main.446</url>
      <doi>10.18653/v1/2021.naacl-main.446</doi>
      <bibkey>zhan-etal-2021-augmenting</bibkey>
      <video href="2021.naacl-main.446.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="447">
      <title>Adversarial Self-Supervised Learning for Out-of-Domain Detection</title>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Hong</first><last>Xu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>5631–5639</pages>
      <abstract>Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require extensive labeled OOD data. To combine the benefits of both types, we propose a self-supervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data. Besides, we introduce an adversarial augmentation neural module to improve the efficiency and robustness of contrastive learning. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin.</abstract>
      <url hash="a04ad55d">2021.naacl-main.447</url>
      <doi>10.18653/v1/2021.naacl-main.447</doi>
      <bibkey>zeng-etal-2021-adversarial</bibkey>
      <video href="2021.naacl-main.447.mp4"/>
      <pwccode url="https://github.com/parzival27/adversarial-self-supervised-out-of-domain-detection" additional="false">parzival27/adversarial-self-supervised-out-of-domain-detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
    </paper>
    <paper id="448">
      <title>Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue <fixed-case>S</fixed-case>tate<fixed-case>T</fixed-case>racking</title>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>Zhenpeng</first><last>Zhou</last></author>
      <author><first>Zhiguang</first><last>Wang</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Eunjoon</first><last>Cho</last></author>
      <author><first>Rajen</first><last>Subba</last></author>
      <pages>5640–5648</pages>
      <abstract>Zero-shot cross-domain dialogue state tracking (DST) enables us to handle unseen domains without the expense of collecting in-domain data. In this paper, we propose a slot descriptions enhanced generative approach for zero-shot cross-domain DST. Specifically, our model first encodes a dialogue context and a slot with a pre-trained self-attentive encoder, and generates slot value in auto-regressive manner. In addition, we incorporate Slot Type Informed Descriptions that capture the shared information of different slots to facilitates the cross-domain knowledge transfer. Experimental results on MultiWOZ shows that our model significantly improve existing state-of-the-art results in zero-shot cross-domain setting.</abstract>
      <url hash="ce6a4474">2021.naacl-main.448</url>
      <doi>10.18653/v1/2021.naacl-main.448</doi>
      <bibkey>lin-etal-2021-leveraging</bibkey>
      <video href="2021.naacl-main.448.mp4"/>
      <pwccode url="https://github.com/facebookresearch/Zero-Shot-DST" additional="false">facebookresearch/Zero-Shot-DST</pwccode>
    </paper>
    <paper id="449">
      <title>Hierarchical Transformer for Task Oriented Dialog Systems</title>
      <author><first>Bishal</first><last>Santra</last></author>
      <author><first>Potnuru</first><last>Anusha</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>5649–5658</pages>
      <abstract>Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like question answering and summarization. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the model learn meaningful utterance and conversation level features, Sordoni et al. (2015b), Serban et al. (2016) proposed Hierarchical RNN architecture, which was later adopted by several other RNN based dialog systems. With the transformer-based models dominating the seq2seq problems lately, the natural question to ask is the applicability of the notion of hierarchy in transformer-based dialog systems. In this paper, we propose a generalized framework for Hierarchical Transformer Encoders and show how a standard transformer can be morphed into any hierarchical encoder, including HRED and HIBERT like models, by using specially designed attention masks and positional encodings. We demonstrate that Hierarchical Encoding helps achieve better natural language understanding of the contexts in transformer-based models for task-oriented dialog systems through a wide range of experiments.</abstract>
      <url hash="c615c674">2021.naacl-main.449</url>
      <doi>10.18653/v1/2021.naacl-main.449</doi>
      <bibkey>santra-etal-2021-hierarchical</bibkey>
      <video href="2021.naacl-main.449.mp4"/>
      <pwccode url="https://github.com/bsantraigi/hier-transformer-pytorch" additional="true">bsantraigi/hier-transformer-pytorch</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="450">
      <title>Measuring the ‘<fixed-case>I</fixed-case> don’t know’ Problem through the Lens of <fixed-case>G</fixed-case>ricean Quantity</title>
      <author><first>Huda</first><last>Khayrallah</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <pages>5659–5670</pages>
      <abstract>We consider the intrinsic evaluation of neural generative dialog models through the lens of Grice’s Maxims of Conversation (1975). Based on the maxim of Quantity (be informative), we propose Relative Utterance Quantity (RUQ) to diagnose the ‘I don’t know’ problem, in which a dialog system produces generic responses. The linguistically motivated RUQ diagnostic compares the model score of a generic response to that of the reference response. We find that for reasonable baseline models, ‘I don’t know’ is preferred over the reference the majority of the time, but this can be reduced to less than 5% with hyperparameter tuning. RUQ allows for the direct analysis of the ‘I don’t know’ problem, which has been addressed but not analyzed by prior work.</abstract>
      <url hash="d0d2e04d">2021.naacl-main.450</url>
      <doi>10.18653/v1/2021.naacl-main.450</doi>
      <bibkey>khayrallah-sedoc-2021-measuring</bibkey>
      <video href="2021.naacl-main.450.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
    </paper>
    <paper id="451">
      <title><fixed-case>RTFE</fixed-case>: A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion</title>
      <author><first>Youri</first><last>Xu</last></author>
      <author><first>Haihong</first><last>E</last></author>
      <author><first>Meina</first><last>Song</last></author>
      <author><first>Wenyu</first><last>Song</last></author>
      <author><first>Xiaodong</first><last>Lv</last></author>
      <author><first>Wang</first><last>Haotian</last></author>
      <author><first>Yang</first><last>Jinrui</last></author>
      <pages>5671–5681</pages>
      <abstract>Static knowledge graph (SKG) embedding (SKGE) has been studied intensively in the past years. Recently, temporal knowledge graph (TKG) embedding (TKGE) has emerged. In this paper, we propose a Recursive Temporal Fact Embedding (RTFE) framework to transplant SKGE models to TKGs and to enhance the performance of existing TKGE models for TKG completion. Different from previous work which ignores the continuity of states of TKG in time evolution, we treat the sequence of graphs as a Markov chain, which transitions from the previous state to the next state. RTFE takes the SKGE to initialize the embeddings of TKG. Then it recursively tracks the state transition of TKG by passing updated parameters/features between timestamps. Specifically, at each timestamp, we approximate the state transition as the gradient update process. Since RTFE learns each timestamp recursively, it can naturally transit to future timestamps. Experiments on five TKG datasets show the effectiveness of RTFE.</abstract>
      <url hash="232743f8">2021.naacl-main.451</url>
      <doi>10.18653/v1/2021.naacl-main.451</doi>
      <bibkey>xu-etal-2021-rtfe</bibkey>
      <video href="2021.naacl-main.451.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/icews">ICEWS</pwcdataset>
    </paper>
    <paper id="452">
      <title>Open Hierarchical Relation Extraction</title>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Yuan</first><last>Yao</last></author>
      <author><first>Ruobing</first><last>Xie</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Fen</first><last>Lin</last></author>
      <author><first>Leyu</first><last>Lin</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>5682–5693</pages>
      <abstract>Open relation extraction (OpenRE) aims to extract novel relation types from open-domain corpora, which plays an important role in completing the relation schemes of knowledge bases (KBs). Most OpenRE methods cast different relation types in isolation without considering their hierarchical dependency. We argue that OpenRE is inherently in close connection with relation hierarchies. To establish the bidirectional connections between OpenRE and relation hierarchy, we propose the task of open hierarchical relation extraction and present a novel OHRE framework for the task. We propose a dynamic hierarchical triplet objective and hierarchical curriculum training paradigm, to effectively integrate hierarchy information into relation representations for better novel relation extraction. We also present a top-down hierarchy expansion algorithm to add the extracted relations into existing hierarchies with reasonable interpretability. Comprehensive experiments show that OHRE outperforms state-of-the-art models by a large margin on both relation clustering and hierarchy expansion.</abstract>
      <url hash="7ce61d5c">2021.naacl-main.452</url>
      <doi>10.18653/v1/2021.naacl-main.452</doi>
      <bibkey>zhang-etal-2021-open</bibkey>
      <video href="2021.naacl-main.452.mp4"/>
      <pwccode url="https://github.com/thunlp/ohre" additional="false">thunlp/ohre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="453">
      <title>Jointly Extracting Explicit and Implicit Relational Triples with Reasoning Pattern Enhanced Binary Pointer Network</title>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Yunqi</first><last>Zhang</last></author>
      <author><first>Changran</first><last>Hu</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>5694–5703</pages>
      <abstract>Relational triple extraction is a crucial task for knowledge graph construction. Existing methods mainly focused on explicit relational triples that are directly expressed, but usually suffer from ignoring implicit triples that lack explicit expressions. This will lead to serious incompleteness of the constructed knowledge graphs. Fortunately, other triples in the sentence provide supplementary information for discovering entity pairs that may have implicit relations. Also, the relation types between the implicitly connected entity pairs can be identified with relational reasoning patterns in the real world. In this paper, we propose a unified framework to jointly extract explicit and implicit relational triples. To explore entity pairs that may be implicitly connected by relations, we propose a binary pointer network to extract overlapping relational triples relevant to each word sequentially and retain the information of previously extracted triples in an external memory. To infer the relation types of implicit relational triples, we propose to introduce real-world relational reasoning patterns in our model and capture these patterns with a relation network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method.</abstract>
      <url hash="1652ef56">2021.naacl-main.453</url>
      <doi>10.18653/v1/2021.naacl-main.453</doi>
      <bibkey>chen-etal-2021-jointly</bibkey>
      <video href="2021.naacl-main.453.mp4"/>
    </paper>
    <paper id="454">
      <title>Multi-Grained Knowledge Distillation for Named Entity Recognition</title>
      <author><first>Xuan</first><last>Zhou</last></author>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Chenyang</first><last>Tao</last></author>
      <author><first>Junya</first><last>Chen</last></author>
      <author><first>Bing</first><last>Xu</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Jing</first><last>Xiao</last></author>
      <pages>5704–5716</pages>
      <abstract>Although pre-trained big models (e.g., BERT, ERNIE, XLNet, GPT3 etc.) have delivered top performance in Seq2seq modeling, their deployments in real-world applications are often hindered by the excessive computations and memory demand involved. For many applications, including named entity recognition (NER), matching the state-of-the-art result under budget has attracted considerable attention. Drawing power from the recent advance in knowledge distillation (KD), this work presents a novel distillation scheme to efficiently transfer the knowledge learned from big models to their more affordable counterpart. Our solution highlights the construction of surrogate labels through the k-best Viterbi algorithm to distill knowledge from the teacher model. To maximally assimilate knowledge into the student model, we propose a multi-grained distillation scheme, which integrates cross entropy involved in conditional random field (CRF) and fuzzy learning.To validate the effectiveness of our proposal, we conducted a comprehensive evaluation on five NER benchmarks, reporting cross-the-board performance gains relative to competing prior-arts. We further discuss ablation results to dissect our gains.</abstract>
      <url hash="80a493cf">2021.naacl-main.454</url>
      <doi>10.18653/v1/2021.naacl-main.454</doi>
      <bibkey>zhou-etal-2021-multi</bibkey>
      <video href="2021.naacl-main.454.mp4"/>
      <pwccode url="https://github.com/11zhouxuan/multi_grained_kd_ner" additional="false">11zhouxuan/multi_grained_kd_ner</pwccode>
    </paper>
    <paper id="455">
      <title><fixed-case>SGG</fixed-case>: Learning to Select, Guide, and Generate for Keyphrase Generation</title>
      <author><first>Jing</first><last>Zhao</last></author>
      <author><first>Junwei</first><last>Bao</last></author>
      <author><first>Yifan</first><last>Wang</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Bowen</first><last>Zhou</last></author>
      <pages>5717–5726</pages>
      <abstract>Keyphrases, that concisely summarize the high-level topics discussed in a document, can be categorized into present keyphrase which explicitly appears in the source text and absent keyphrase which does not match any contiguous subsequence but is highly semantically related to the source. Most existing keyphrase generation approaches synchronously generate present and absent keyphrases without explicitly distinguishing these two categories. In this paper, a Select-Guide-Generate (SGG) approach is proposed to deal with present and absent keyphrases generation separately with different mechanisms. Specifically, SGG is a hierarchical neural network which consists of a pointing-based selector at low layer concentrated on present keyphrase generation, a selection-guided generator at high layer dedicated to absent keyphrase generation, and a guider in the middle to transfer information from selector to generator. Experimental results on four keyphrase generation benchmarks demonstrate the effectiveness of our model, which significantly outperforms the strong baselines for both present and absent keyphrases generation. Furthermore, we extend SGG to a title generation task which indicates its extensibility in natural language generation tasks.</abstract>
      <url hash="853573e7">2021.naacl-main.455</url>
      <doi>10.18653/v1/2021.naacl-main.455</doi>
      <bibkey>zhao-etal-2021-sgg</bibkey>
      <video href="2021.naacl-main.455.mp4"/>
      <pwccode url="https://github.com/jd-ai-research-nlp/sgg" additional="false">jd-ai-research-nlp/sgg</pwccode>
    </paper>
    <paper id="456">
      <title>Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in <fixed-case>T</fixed-case>witter</title>
      <author><first>Tulika</first><last>Saha</last></author>
      <author><first>Apoorva</first><last>Upadhyaya</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>5727–5737</pages>
      <abstract>Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA (‘TA’ means tweet act, i.e., speech act in Twitter) dataset called <i>EmoTA</i> collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants.</abstract>
      <url hash="c6ca1a45">2021.naacl-main.456</url>
      <doi>10.18653/v1/2021.naacl-main.456</doi>
      <bibkey>saha-etal-2021-towards</bibkey>
      <video href="2021.naacl-main.456.mp4"/>
    </paper>
    <paper id="457">
      <title>Generative Imagination Elevates Machine Translation</title>
      <author><first>Quanyu</first><last>Long</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>5738–5748</pages>
      <abstract>There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence - image for training and tuples of source sentence - image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the “imagined representation” to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy.</abstract>
      <url hash="2a5a5564">2021.naacl-main.457</url>
      <doi>10.18653/v1/2021.naacl-main.457</doi>
      <bibkey>long-etal-2021-generative</bibkey>
      <video href="2021.naacl-main.457.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="458">
      <title>Non-Autoregressive Translation by Learning Target Categorical Codes</title>
      <author><first>Yu</first><last>Bao</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Dongqi</first><last>Wang</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>5749–5759</pages>
      <abstract>Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks than several strong baselines.</abstract>
      <url hash="f1787812">2021.naacl-main.458</url>
      <doi>10.18653/v1/2021.naacl-main.458</doi>
      <bibkey>bao-etal-2021-non</bibkey>
      <video href="2021.naacl-main.458.mp4"/>
      <pwccode url="https://github.com/baoy-nlp/CNAT" additional="false">baoy-nlp/CNAT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="459">
      <title>Training Data Augmentation for Code-Mixed Translation</title>
      <author><first>Abhirut</first><last>Gupta</last></author>
      <author><first>Aditya</first><last>Vavre</last></author>
      <author><first>Sunita</first><last>Sarawagi</last></author>
      <pages>5760–5766</pages>
      <abstract>Machine translation of user-generated code-mixed inputs to English is of crucial importance in applications like web search and targeted advertising. We address the scarcity of parallel training data for training such models by designing a strategy of converting existing non-code-mixed parallel data sources to code-mixed parallel data. We present an m-BERT based procedure whose core learnable component is a ternary sequence labeling model, that can be trained with a limited code-mixed corpus alone. We show a 5.8 point increase in BLEU on heavily code-mixed sentences by training a translation model using our data augmentation strategy on an Hindi-English code-mixed translation task.</abstract>
      <url hash="f7ca05bd">2021.naacl-main.459</url>
      <doi>10.18653/v1/2021.naacl-main.459</doi>
      <bibkey>gupta-etal-2021-training</bibkey>
      <video href="2021.naacl-main.459.mp4"/>
      <pwccode url="https://github.com/shruikan20/spoken-tutorial-dataset" additional="false">shruikan20/spoken-tutorial-dataset</pwccode>
    </paper>
    <paper id="460">
      <title>Rethinking Perturbations in Encoder-Decoders for Fast Training</title>
      <author><first>Sho</first><last>Takase</last></author>
      <author><first>Shun</first><last>Kiyono</last></author>
      <pages>5767–5780</pages>
      <abstract>We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling (Bengio et al., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time. Experimental results show that the simple techniques such as word dropout (Gal and Ghahramani, 2016) and random replacement of input tokens achieve comparable (or better) scores to the recently proposed perturbations, even though these simple methods are faster.</abstract>
      <url hash="daef1039">2021.naacl-main.460</url>
      <doi>10.18653/v1/2021.naacl-main.460</doi>
      <bibkey>takase-kiyono-2021-rethinking</bibkey>
      <video href="2021.naacl-main.460.mp4"/>
      <pwccode url="https://github.com/takase/rethink_perturbations" additional="false">takase/rethink_perturbations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/duc-2004">DUC 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="461">
      <title>Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model</title>
      <author><first>Amane</first><last>Sugiyama</last></author>
      <author><first>Naoki</first><last>Yoshinaga</last></author>
      <pages>5781–5791</pages>
      <abstract>Although many end-to-end context-aware neural machine translation models have been proposed to incorporate inter-sentential contexts in translation, these models can be trained only in domains where parallel documents with sentential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and target-side document-level monolingual data. From a theoretical viewpoint, our core contribution is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We demonstrate the effectiveness of our method on English to Russian translation, by evaluating with BLEU and contrastive tests for context-aware translation.</abstract>
      <url hash="4d9d0416">2021.naacl-main.461</url>
      <doi>10.18653/v1/2021.naacl-main.461</doi>
      <bibkey>sugiyama-yoshinaga-2021-context</bibkey>
      <video href="2021.naacl-main.461.mp4"/>
    </paper>
    <paper id="462">
      <title>Machine Translated Text Detection Through Text Similarity with Round-Trip Translation</title>
      <author><first>Hoang-Quoc</first><last>Nguyen-Son</last></author>
      <author><first>Tran</first><last>Thao</last></author>
      <author><first>Seira</first><last>Hidano</last></author>
      <author><first>Ishita</first><last>Gupta</last></author>
      <author><first>Shinsaku</first><last>Kiyomoto</last></author>
      <pages>5792–5797</pages>
      <abstract>Translated texts have been used for malicious purposes, i.e., plagiarism or fake reviews. Existing detectors have been built around a specific translator (e.g., Google) but fail to detect a translated text from a strange translator. If we use the same translator, the translated text is similar to its round-trip translation, which is when text is translated into another language and translated back into the original language. However, a round-trip translated text is significantly different from the original text or a translated text using a strange translator. Hence, we propose a detector using text similarity with round-trip translation (TSRT). TSRT achieves 86.9% accuracy in detecting a translated text from a strange translator. It outperforms existing detectors (77.9%) and human recognition (53.3%).</abstract>
      <url hash="065edd72">2021.naacl-main.462</url>
      <doi>10.18653/v1/2021.naacl-main.462</doi>
      <bibkey>nguyen-son-etal-2021-machine</bibkey>
      <video href="2021.naacl-main.462.mp4"/>
      <pwccode url="https://github.com/quocnsh/machine_translation_detection" additional="false">quocnsh/machine_translation_detection</pwccode>
    </paper>
    <paper id="463">
      <title><fixed-case>TR</fixed-case>-<fixed-case>BERT</fixed-case>: Dynamic Token Reduction for Accelerating <fixed-case>BERT</fixed-case> Inference</title>
      <author><first>Deming</first><last>Ye</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Yufei</first><last>Huang</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>5798–5809</pages>
      <abstract>Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs’ inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/TR-BERT.</abstract>
      <url hash="e279e6ef">2021.naacl-main.463</url>
      <doi>10.18653/v1/2021.naacl-main.463</doi>
      <bibkey>ye-etal-2021-tr</bibkey>
      <video href="2021.naacl-main.463.mp4"/>
      <pwccode url="https://github.com/thunlp/TR-BERT" additional="false">thunlp/TR-BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="464">
      <title>Breadth First Reasoning Graph for Multi-hop Question Answering</title>
      <author><first>Yongjie</first><last>Huang</last></author>
      <author><first>Meng</first><last>Yang</last></author>
      <pages>5810–5821</pages>
      <abstract>Recently Graph Neural Network (GNN) has been used as a promising tool in multi-hop question answering task. However, the unnecessary updations and simple edge constructions prevent an accurate answer span extraction in a more direct and interpretable way. In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. In BFR-Graph, the reasoning message is required to start from the question node and pass to the next sentences node hop by hop until all the edges have been passed, which can effectively prevent each node from over-smoothing or being updated multiple times unnecessarily. To introduce more semantics, we also define the reasoning graph as a weighted graph with considering the number of co-occurrence entities and the distance between sentences. Then we present a more direct and interpretable way to aggregate scores from different levels of granularity based on the GNN. On HotpotQA leaderboard, the proposed BFR-Graph achieves state-of-the-art on answer span prediction.</abstract>
      <url hash="3dc83a3f">2021.naacl-main.464</url>
      <doi>10.18653/v1/2021.naacl-main.464</doi>
      <bibkey>huang-yang-2021-breadth</bibkey>
      <video href="2021.naacl-main.464.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="465">
      <title>Improving Zero-Shot Cross-lingual Transfer for Multilingual Question Answering over Knowledge Graph</title>
      <author><first>Yucheng</first><last>Zhou</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Tao</first><last>Shen</last></author>
      <author><first>Wenqiang</first><last>Zhang</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>5822–5834</pages>
      <abstract>Multilingual question answering over knowledge graph (KGQA) aims to derive answers from a knowledge graph (KG) for questions in multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That is, we can only access training data in a high-resource language, while need to answer multilingual questions without any labeled data in target languages. A straightforward approach is resorting to pre-trained multilingual models (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA performance between source and target languages. In this paper, we exploit unsupervised bilingual lexicon induction (BLI) to map training questions in source language into those in target language as augmented training data, which circumvents language inconsistency between training and inference. Furthermore, we propose an adversarial learning strategy to alleviate syntax-disorder of the augmented data, making the model incline to both language- and syntax-independence. Consequently, our model narrows the gap in zero-shot cross-lingual transfer. Experiments on two multilingual KGQA datasets with 11 zero-resource languages verify its effectiveness.</abstract>
      <url hash="3d82cf2e">2021.naacl-main.465</url>
      <doi>10.18653/v1/2021.naacl-main.465</doi>
      <bibkey>zhou-etal-2021-improving</bibkey>
      <video href="2021.naacl-main.465.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
    </paper>
    <paper id="466">
      <title><fixed-case>R</fixed-case>ocket<fixed-case>QA</fixed-case>: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering</title>
      <author><first>Yingqi</first><last>Qu</last></author>
      <author><first>Yuchen</first><last>Ding</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Kai</first><last>Liu</last></author>
      <author><first>Ruiyang</first><last>Ren</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Daxiang</first><last>Dong</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>5835–5847</pages>
      <abstract>In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.</abstract>
      <url hash="4f7c8b53">2021.naacl-main.466</url>
      <doi>10.18653/v1/2021.naacl-main.466</doi>
      <bibkey>qu-etal-2021-rocketqa</bibkey>
      <video href="2021.naacl-main.466.mp4"/>
      <pwccode url="https://github.com/paddlepaddle/rocketqa" additional="false">paddlepaddle/rocketqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="467">
      <title><fixed-case>DAGN</fixed-case>: Discourse-Aware Graph Network for Logical Reasoning</title>
      <author><first>Yinya</first><last>Huang</last></author>
      <author><first>Meng</first><last>Fang</last></author>
      <author><first>Yu</first><last>Cao</last></author>
      <author><first>Liwei</first><last>Wang</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <pages>5848–5855</pages>
      <abstract>Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourse-aware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, ReClor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/Eleanor-H/DAGN.</abstract>
      <url hash="32c3fcb0">2021.naacl-main.467</url>
      <doi>10.18653/v1/2021.naacl-main.467</doi>
      <bibkey>huang-etal-2021-dagn</bibkey>
      <video href="2021.naacl-main.467.mp4"/>
      <pwccode url="https://github.com/Eleanor-H/DAGN" additional="true">Eleanor-H/DAGN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/logiqa">LogiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="468">
      <title>Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering</title>
      <author><first>Sohee</first><last>Yang</last></author>
      <author><first>Minjoon</first><last>Seo</last></author>
      <pages>5856–5865</pages>
      <abstract>In open-domain question answering (QA), retrieve-and-read mechanism has the inherent benefit of interpretability and the easiness of adding, removing, or editing knowledge compared to the parametric approaches of closed-book QA models. However, it is also known to suffer from its large storage footprint due to its document corpus and index. Here, we discuss several orthogonal strategies to drastically reduce the footprint of a retrieve-and-read open-domain QA system by up to 160x. Our results indicate that retrieve-and-read can be a viable option even in a highly constrained serving environment such as edge devices, as we show that it can achieve better accuracy than a purely parametric model with comparable docker-level system size.</abstract>
      <url hash="cfca1071">2021.naacl-main.468</url>
      <doi>10.18653/v1/2021.naacl-main.468</doi>
      <bibkey>yang-seo-2021-designing</bibkey>
      <video href="2021.naacl-main.468.mp4"/>
      <pwccode url="https://github.com/clovaai/minimal-rnr-qa" additional="false">clovaai/minimal-rnr-qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="469">
      <title>Unsupervised Multi-hop Question Answering by Question Generation</title>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>Wenhan</first><last>Xiong</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>5866–5880</pages>
      <abstract>Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61% and 83% of the supervised learning performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA.</abstract>
      <url hash="d59cda74">2021.naacl-main.469</url>
      <doi>10.18653/v1/2021.naacl-main.469</doi>
      <bibkey>pan-etal-2021-unsupervised</bibkey>
      <video href="2021.naacl-main.469.mp4"/>
      <pwccode url="https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA" additional="false">teacherpeterpan/Unsupervised-Multi-hop-QA</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
    </paper>
    <paper id="470">
      <title>Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents</title>
      <author><first>Peng</first><last>Cui</last></author>
      <author><first>Le</first><last>Hu</last></author>
      <pages>5881–5891</pages>
      <abstract>Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides, we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from.</abstract>
      <url hash="9077b950">2021.naacl-main.470</url>
      <doi>10.18653/v1/2021.naacl-main.470</doi>
      <bibkey>cui-hu-2021-sliding</bibkey>
      <video href="2021.naacl-main.470.mp4"/>
      <pwccode url="https://github.com/pcui-nlp/ssn_dm" additional="false">pcui-nlp/ssn_dm</pwccode>
    </paper>
    <paper id="471">
      <title><fixed-case>A</fixed-case>dapt<fixed-case>S</fixed-case>um: Towards Low-Resource Domain Adaptation for Abstractive Summarization</title>
      <author><first>Tiezheng</first><last>Yu</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>5892–5904</pages>
      <abstract>State-of-the-art abstractive summarization models generally rely on extensive labeled data, which lowers their generalization ability on domains where such data are not available. In this paper, we present a study of domain adaptation for the abstractive summarization task across six diverse target domains in a low-resource setting. Specifically, we investigate the second phase of pre-training on large-scale generative models under three different settings: 1) source domain pre-training; 2) domain-adaptive pre-training; and 3) task-adaptive pre-training. Experiments show that the effectiveness of pre-training is correlated with the similarity between the pre-training data and the target domain task. Moreover, we find that continuing pre-training could lead to the pre-trained model’s catastrophic forgetting, and a learning method with less forgetting can alleviate this issue. Furthermore, results illustrate that a huge gap still exists between the low-resource and high-resource settings, which highlights the need for more advanced domain adaptation methods for the abstractive summarization task.</abstract>
      <url hash="74de3698">2021.naacl-main.471</url>
      <attachment type="OptionalSupplementaryCode" hash="45a145ff">2021.naacl-main.471.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.471</doi>
      <bibkey>yu-etal-2021-adaptsum</bibkey>
      <video href="2021.naacl-main.471.mp4"/>
      <pwccode url="https://github.com/TysonYu/AdaptSum" additional="false">TysonYu/AdaptSum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
    </paper>
    <paper id="472">
      <title><fixed-case>QMS</fixed-case>um: A New Benchmark for Query-based Multi-domain Meeting Summarization</title>
      <author><first>Ming</first><last>Zhong</last></author>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Ahmad</first><last>Zaidi</last></author>
      <author><first>Mutethia</first><last>Mutuma</last></author>
      <author><first>Rahul</first><last>Jha</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author id="yang-liu-edinburgh"><first>Yang</first><last>Liu</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>5905–5921</pages>
      <abstract>Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at <url>https://github.com/Yale-LILY/QMSum</url>.</abstract>
      <url hash="7ab7f678">2021.naacl-main.472</url>
      <attachment type="OptionalSupplementaryData" hash="b6b58dbe">2021.naacl-main.472.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.naacl-main.472</doi>
      <bibkey>zhong-etal-2021-qmsum</bibkey>
      <video href="2021.naacl-main.472.mp4"/>
      <pwccode url="https://github.com/Yale-LILY/QMSum" additional="false">Yale-LILY/QMSum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/qmsum">QMSum</pwcdataset>
    </paper>
    <paper id="473">
      <title><fixed-case>MM</fixed-case>-<fixed-case>AVS</fixed-case>: A Full-Scale Dataset for Multi-modal Summarization</title>
      <author><first>Xiyan</first><last>Fu</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Zhenglu</first><last>Yang</last></author>
      <pages>5922–5926</pages>
      <abstract>Multimodal summarization becomes increasingly significant as it is the basis for question answering, Web search, and many other downstream tasks. However, its learning materials have been lacking a holistic organization by integrating resources from various modalities, thereby lagging behind the research progress of this field. In this study, we release a full-scale multimodal dataset comprehensively gathering documents, summaries, images, captions, videos, audios, transcripts, and titles in English from CNN and Daily Mail. To our best knowledge, this is the first collection that spans all modalities and nearly comprises all types of materials available in this community. In addition, we devise a baseline model based on the novel dataset, which employs a newly proposed Jump-Attention mechanism based on transcripts. The experimental results validate the important assistance role of the external information for multimodal summarization.</abstract>
      <url hash="2e840649">2021.naacl-main.473</url>
      <doi>10.18653/v1/2021.naacl-main.473</doi>
      <bibkey>fu-etal-2021-mm</bibkey>
      <video href="2021.naacl-main.473.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
    </paper>
    <paper id="474">
      <title><fixed-case>M</fixed-case>edia<fixed-case>S</fixed-case>um: A Large-scale Media Interview Dataset for Dialogue Summarization</title>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author id="yang-liu-edinburgh"><first>Yang</first><last>Liu</last></author>
      <author><first>Jie</first><last>Mei</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>5927–5934</pages>
      <abstract>This paper introduces MediaSum, a large-scale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MediaSum can be used in transfer learning to improve a model’s performance on other dialogue summarization tasks.</abstract>
      <url hash="01c78e74">2021.naacl-main.474</url>
      <attachment type="OptionalSupplementaryMaterial" hash="a0ef8c06">2021.naacl-main.474.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-main.474</doi>
      <bibkey>zhu-etal-2021-mediasum</bibkey>
      <video href="2021.naacl-main.474.mp4"/>
      <pwccode url="https://github.com/zcgzcgzcg1/MediaSum" additional="false">zcgzcgzcg1/MediaSum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/crd3">CRD3</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/interview">Interview</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="475">
      <title>Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection</title>
      <author><first>Sihao</first><last>Chen</last></author>
      <author><first>Fan</first><last>Zhang</last></author>
      <author><first>Kazoo</first><last>Sone</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>5935–5941</pages>
      <abstract>Despite significant progress in neural abstractive summarization, recent studies have shown that the current models are prone to generating summaries that are unfaithful to the original context. To address the issue, we study contrast candidate generation and selection as a model-agnostic post-processing technique to correct the extrinsic hallucinations (i.e. information not present in the source text) in unfaithful summaries. We learn a discriminative correction model by generating alternative candidate summaries where named entities and quantities in the generated summary are replaced with ones with compatible semantic types from the source document. This model is then used to select the best candidate as the final output summary. Our experiments and analysis across a number of neural summarization systems show that our proposed method is effective in identifying and correcting extrinsic hallucinations. We analyze the typical hallucination phenomenon by different types of neural summarization systems, in hope to provide insights for future work on the direction.</abstract>
      <url hash="6e70d8ab">2021.naacl-main.475</url>
      <doi>10.18653/v1/2021.naacl-main.475</doi>
      <bibkey>chen-etal-2021-improving</bibkey>
      <video href="2021.naacl-main.475.mp4"/>
    </paper>
    <paper id="476">
      <title>Inference Time Style Control for Summarization</title>
      <author><first>Shuyang</first><last>Cao</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>5942–5953</pages>
      <abstract>How to generate summaries of different styles without requiring corpora in the target styles, or training separate models? We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. (1) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical control during generation. In experiments of summarizing with simplicity control, automatic evaluation and human judges both find our models producing outputs in simpler languages while still informative. We also generate news headlines with various ideological leanings, which can be distinguished by humans with a reasonable probability.</abstract>
      <url hash="ed6bc297">2021.naacl-main.476</url>
      <doi>10.18653/v1/2021.naacl-main.476</doi>
      <bibkey>cao-wang-2021-inference</bibkey>
      <video href="2021.naacl-main.476.mp4"/>
    </paper>
    <paper id="477">
      <title>ReinforceBug: A Framework to Generate Adversarial Textual Examples</title>
      <author><first>Bushra</first><last>Sabir</last></author>
      <author><first>Muhammad Ali</first><last>Babar</last></author>
      <author><first>Raj</first><last>Gaire</last></author>
      <pages>5954–5964</pages>
      <abstract>Adversarial Examples (AEs) generated by perturbingining examples are useful in improving the robustness of Deep Learning (DL) based models. Most prior works generate AEs that are either unconscionable due to lexical errors or semantically and functionally deviant from original examples. In this paper, we present ReinforceBug, a reinforcement learning framework, that learns a policy that is transferable on unseen datasets and generates utility-preserving and transferable (on other models) AEs. Our experiments show that ReinforceBug is on average 10% more successful as compared to the state-of the-art attack TextFooler. Moreover, the target models have on average 73.64% confidence in wrong prediction, the generated AEs preserve the functional equivalence and semantic similarity (83.38%) to their original counterparts, and are transferable on other models with an average success rate of 46%</abstract>
      <url hash="9ec3df5d">2021.naacl-main.477</url>
      <doi>10.18653/v1/2021.naacl-main.477</doi>
      <bibkey>sabir-etal-2021-reinforcebug</bibkey>
      <attachment type="OptionalSupplementaryCode" hash="d3a806e3">2021.naacl-main.477.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="061d3b2a">2021.naacl-main.477.OptionalSupplementaryData.zip</attachment>
      <video href="2021.naacl-main.477.mp4"/>
    </paper>
  </volume>
  <volume id="demos" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations</booktitle>
      <editor><first>Avi</first><last>Sil</last></editor>
      <editor><first>Xi Victoria</first><last>Lin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.naacl-demos</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="15c022e7">2021.naacl-demos.0</url>
      <bibkey>naacl-2021-2021-north</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>P</fixed-case>ho<fixed-case>NLP</fixed-case>: A joint multi-task learning model for <fixed-case>V</fixed-case>ietnamese part-of-speech tagging, named entity recognition and dependency parsing</title>
      <author><first>Linh The</first><last>Nguyen</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <pages>1–7</pages>
      <abstract>We present the first multi-task learning model – named PhoNLP – for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the Apache License 2.0. Although we specify PhoNLP for Vietnamese, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only Vietnamese but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP</abstract>
      <url hash="478af461">2021.naacl-demos.1</url>
      <doi>10.18653/v1/2021.naacl-demos.1</doi>
      <bibkey>nguyen-nguyen-2021-phonlp</bibkey>
      <video href="2021.naacl-demos.1.mp4"/>
      <pwccode url="https://github.com/VinAIResearch/PhoNLP" additional="false">VinAIResearch/PhoNLP</pwccode>
    </paper>
    <paper id="2">
      <title>Machine-Assisted Script Curation</title>
      <author><first>Manuel</first><last>Ciosici</last></author>
      <author><first>Joseph</first><last>Cummings</last></author>
      <author><first>Mitchell</first><last>DeHaven</last></author>
      <author><first>Alex</first><last>Hedges</last></author>
      <author><first>Yash</first><last>Kankanampati</last></author>
      <author><first>Dong-Ho</first><last>Lee</last></author>
      <author><first>Ralph</first><last>Weischedel</last></author>
      <author><first>Marjorie</first><last>Freedman</last></author>
      <pages>8–17</pages>
      <abstract>We describe Machine-Aided Script Curator (MASC), a system for human-machine collaborative script authoring. Scripts produced with MASC include (1) English descriptions of sub-events that comprise a larger, complex event; (2) event types for each of those events; (3) a record of entities expected to participate in multiple sub-events; and (4) temporal sequencing between the sub-events. MASC automates portions of the script creation process with suggestions for event types, links to Wikidata, and sub-events that may have been forgotten. We illustrate how these automations are useful to the script writer with a few case-study scripts.</abstract>
      <url hash="897e5761">2021.naacl-demos.2</url>
      <attachment type="Supplementary" hash="32713ecb">2021.naacl-demos.2.Supplementary.zip</attachment>
      <doi>10.18653/v1/2021.naacl-demos.2</doi>
      <bibkey>ciosici-etal-2021-machine</bibkey>
      <video href="2021.naacl-demos.2.mp4"/>
      <pwccode url="https://github.com/isi-vista/MASC" additional="false">isi-vista/MASC</pwccode>
    </paper>
    <paper id="3">
      <title><fixed-case>NAMER</fixed-case>: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering</title>
      <author><first>Minhao</first><last>Zhang</last></author>
      <author><first>Ruoyu</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Zou</last></author>
      <author><first>Yinnian</first><last>Lin</last></author>
      <author><first>Sen</first><last>Hu</last></author>
      <pages>18–25</pages>
      <abstract>We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding mentions in question. Equipped with techniques including data augmentation and multitasking, we show that the proposed framework outperforms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset (https://github.com/ridiculouz/CKBQA) with such strategy is also published to promote further research. An online demo of NAMER (http://kbqademo.gstore.cn) is provided to visualize our framework and supply extra information for users, a video illustration (https://youtu.be/yetnVye_hg4) of NAMER is also available.</abstract>
      <url hash="ea05fc80">2021.naacl-demos.3</url>
      <doi>10.18653/v1/2021.naacl-demos.3</doi>
      <bibkey>zhang-etal-2021-namer</bibkey>
      <video href="2021.naacl-demos.3.mp4"/>
    </paper>
    <paper id="4">
      <title><fixed-case>D</fixed-case>i<fixed-case>SC</fixed-case>o<fixed-case>L</fixed-case>: Toward Engaging Dialogue Systems through Conversational Line Guided Response Generation</title>
      <author><first>Sarik</first><last>Ghazarian</last></author>
      <author><first>Zixi</first><last>Liu</last></author>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>26–34</pages>
      <abstract>Having engaging and informative conversations with users is the utmost goal for open-domain conversational systems. Recent advances in transformer-based language models and their applications to dialogue systems have succeeded to generate fluent and human-like responses. However, they still lack control over the generation process towards producing contentful responses and achieving engaging conversations. To achieve this goal, we present <b>DiSCoL</b> (<b>Di</b>alogue <b>S</b>ystems through <b>Co</b>versational <b>L</b>ine guided response generation). DiSCoL is an open-domain dialogue system that leverages conversational lines (briefly <b>convlines</b>) as controllable and informative content-planning elements to guide the generation model produce engaging and informative responses. Two primary modules in DiSCoL’s pipeline are conditional generators trained for 1) predicting relevant and informative convlines for dialogue contexts and 2) generating high-quality responses conditioned on the predicted convlines. Users can also change the returned convlines to <i>control</i> the direction of the conversations towards topics that are more interesting for them. Through automatic and human evaluations, we demonstrate the efficiency of the convlines in producing engaging conversations.</abstract>
      <url hash="30350930">2021.naacl-demos.4</url>
      <attachment type="Supplementary" hash="a04bdb7a">2021.naacl-demos.4.Supplementary.mp4</attachment>
      <doi>10.18653/v1/2021.naacl-demos.4</doi>
      <bibkey>ghazarian-etal-2021-discol</bibkey>
      <video href="2021.naacl-demos.4.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/topical-chat">Topical-Chat</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>FITA</fixed-case>nnotator: A Flexible and Intelligent Text Annotation System</title>
      <author><first>Yanzeng</first><last>Li</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Li</first><last>Quangang</last></author>
      <author><first>Tingwen</first><last>Liu</last></author>
      <pages>35–41</pages>
      <abstract>In this paper, we introduce FITAnnotator, a generic web-based tool for efficient text annotation. Benefiting from the fully modular architecture design, FITAnnotator provides a systematic solution for the annotation of a variety of natural language processing tasks, including classification, sequence tagging and semantic role annotation, regardless of the language. Three kinds of interfaces are developed to annotate instances, evaluate annotation quality and manage the annotation task for annotators, reviewers and managers, respectively. FITAnnotator also gives intelligent annotations by introducing task-specific assistant to support and guide the annotators based on active learning and incremental learning strategies. This assistant is able to effectively update from the annotator feedbacks and easily handle the incremental labeling scenarios.</abstract>
      <url hash="730d19d5">2021.naacl-demos.5</url>
      <doi>10.18653/v1/2021.naacl-demos.5</doi>
      <bibkey>li-etal-2021-fitannotator</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="6">
      <title>Robustness Gym: Unifying the <fixed-case>NLP</fixed-case> Evaluation Landscape</title>
      <author><first>Karan</first><last>Goel</last></author>
      <author><first>Nazneen Fatema</first><last>Rajani</last></author>
      <author><first>Jesse</first><last>Vig</last></author>
      <author><first>Zachary</first><last>Taschdjian</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Christopher</first><last>Ré</last></author>
      <pages>42–55</pages>
      <abstract>Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback &amp; contributions from the community.</abstract>
      <url hash="993be04c">2021.naacl-demos.6</url>
      <doi>10.18653/v1/2021.naacl-demos.6</doi>
      <bibkey>goel-etal-2021-robustness</bibkey>
      <pwccode url="https://github.com/robustness-gym/robustness-gym" additional="true">robustness-gym/robustness-gym</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>E</fixed-case>vent<fixed-case>P</fixed-case>lus: A Temporal Event Understanding Pipeline</title>
      <author><first>Mingyu Derek</first><last>Ma</last></author>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Mu</first><last>Yang</last></author>
      <author><first>Kung-Hsiang</first><last>Huang</last></author>
      <author><first>Nuan</first><last>Wen</last></author>
      <author><first>Shikhar</first><last>Singh</last></author>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>56–65</pages>
      <abstract>We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications.</abstract>
      <url hash="cf0c5341">2021.naacl-demos.7</url>
      <doi>10.18653/v1/2021.naacl-demos.7</doi>
      <bibkey>ma-etal-2021-eventplus</bibkey>
      <video href="2021.naacl-demos.7.mp4"/>
      <pwccode url="https://github.com/PlusLabNLP/EventPlus" additional="false">PlusLabNLP/EventPlus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/matres">MATRES</pwcdataset>
    </paper>
    <paper id="8">
      <title><fixed-case>COVID</fixed-case>-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation</title>
      <author><first>Qingyun</first><last>Wang</last></author>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Xuan</first><last>Wang</last></author>
      <author><first>Nikolaus</first><last>Parulian</last></author>
      <author><first>Guangxing</first><last>Han</last></author>
      <author><first>Jiawei</first><last>Ma</last></author>
      <author><first>Jingxuan</first><last>Tu</last></author>
      <author><first>Ying</first><last>Lin</last></author>
      <author><first>Ranran Haoran</first><last>Zhang</last></author>
      <author><first>Weili</first><last>Liu</last></author>
      <author><first>Aabhas</first><last>Chauhan</last></author>
      <author><first>Yingjun</first><last>Guan</last></author>
      <author><first>Bangzheng</first><last>Li</last></author>
      <author><first>Ruisong</first><last>Li</last></author>
      <author><first>Xiangchen</first><last>Song</last></author>
      <author><first>Yi</first><last>Fung</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Jasmine</first><last>Rah</last></author>
      <author><first>David</first><last>Liem</last></author>
      <author><first>Ahmed</first><last>ELsayed</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <author><first>Cynthia</first><last>Schneider</last></author>
      <author><first>Boyan</first><last>Onyshkevych</last></author>
      <pages>66–77</pages>
      <abstract>To combat COVID-19, both clinicians and scientists need to digest the vast amount of relevant biomedical knowledge in literature to understand the disease mechanism and the related biological functions. We have developed a novel and comprehensive knowledge discovery framework, COVID-KG to extract fine-grained multimedia knowledge elements (entities, relations and events) from scientific literature. We then exploit the constructed multimedia knowledge graphs (KGs) for question answering and report generation, using drug repurposing as a case study. Our framework also provides detailed contextual sentences, subfigures, and knowledge subgraphs as evidence. All of the data, KGs, reports.</abstract>
      <url hash="db194589">2021.naacl-demos.8</url>
      <doi>10.18653/v1/2021.naacl-demos.8</doi>
      <award>Best Demo Paper</award>
      <bibkey>wang-etal-2021-covid</bibkey>
      <video href="2021.naacl-demos.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Multifaceted Domain-Specific Document Embeddings</title>
      <author><first>Julian</first><last>Risch</last></author>
      <author><first>Philipp</first><last>Hager</last></author>
      <author><first>Ralf</first><last>Krestel</last></author>
      <pages>78–83</pages>
      <abstract>Current document embeddings require large training corpora but fail to learn high-quality representations when confronted with a small number of domain-specific documents and rare terms. Further, they transform each document into a single embedding vector, making it hard to capture different notions of document similarity or explain why two documents are considered similar. In this work, we propose our Faceted Domain Encoder, a novel approach to learn multifaceted embeddings for domain-specific documents. It is based on a Siamese neural network architecture and leverages knowledge graphs to further enhance the embeddings even if only a few training samples are available. The model identifies different types of domain knowledge and encodes them into separate dimensions of the embedding, thereby enabling multiple ways of finding and comparing related documents in the vector space. We evaluate our approach on two benchmark datasets and find that it achieves the same embedding quality as state-of-the-art models while requiring only a tiny fraction of their training data. An interactive demo, our source code, and the evaluation datasets are available online: https://hpi.de/naumann/s/multifaceted-embeddings and a screencast is available on YouTube: https://youtu.be/HHcsX2clEwg</abstract>
      <url hash="18a45b63">2021.naacl-demos.9</url>
      <doi>10.18653/v1/2021.naacl-demos.9</doi>
      <bibkey>risch-etal-2021-multifaceted</bibkey>
      <video href="2021.naacl-demos.9.mp4"/>
      <pwccode url="https://github.com/philipphager/faceted-domain-encoder" additional="false">philipphager/faceted-domain-encoder</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/biosses">BIOSSES</pwcdataset>
    </paper>
    <paper id="10">
      <title>Improving Evidence Retrieval for Automated Explainable Fact-Checking</title>
      <author><first>Chris</first><last>Samarinas</last></author>
      <author><first>Wynne</first><last>Hsu</last></author>
      <author><first>Mong Li</first><last>Lee</last></author>
      <pages>84–91</pages>
      <abstract>Automated fact-checking on a large-scale is a challenging task that has not been studied systematically until recently. Large noisy document collections like the web or news articles make the task more difficult. We describe a three-stage automated fact-checking system, named Quin+, using evidence retrieval and selection methods. We demonstrate that using dense passage representations leads to much higher evidence recall in a noisy setting. We also propose two sentence selection approaches, an embedding-based selection using a dense retrieval model, and a sequence labeling approach for context-aware selection. Quin+ is able to verify open-domain claims using results from web search engines.</abstract>
      <url hash="49221f38">2021.naacl-demos.10</url>
      <attachment type="Supplementary" hash="6df7b8e1">2021.naacl-demos.10.Supplementary.txt</attachment>
      <doi>10.18653/v1/2021.naacl-demos.10</doi>
      <bibkey>samarinas-etal-2021-improving</bibkey>
      <video href="2021.naacl-demos.10.mp4"/>
      <pwccode url="https://github.com/algoprog/Quin" additional="false">algoprog/Quin</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scifact">SciFact</pwcdataset>
    </paper>
    <paper id="11">
      <title>Interactive Plot Manipulation using Natural Language</title>
      <author><first>Yihan</first><last>Wang</last></author>
      <author><first>Yutong</first><last>Shao</last></author>
      <author><first>Ndapa</first><last>Nakashole</last></author>
      <pages>92–98</pages>
      <abstract>We present an interactive Plotting Agent, a system that enables users to directly manipulate plots using natural language instructions within an interactive programming environment. The Plotting Agent maps language to plot updates. We formulate this problem as a slot-based task-oriented dialog problem, which we tackle with a sequence-to-sequence model. This plotting model while accurate in most cases, still makes errors, therefore, the system allows a feedback mode, wherein the user is presented with a top-k list of plots, among which the user can pick the desired one. From this kind of feedback, we can then, in principle, continuously learn and improve the system. Given that plotting is widely used across data-driven fields, we believe our demonstration will be of interest to both practitioners such as data scientists broadly defined, and researchers interested in natural language interfaces.</abstract>
      <url hash="c4181900">2021.naacl-demos.11</url>
      <doi>10.18653/v1/2021.naacl-demos.11</doi>
      <bibkey>wang-etal-2021-interactive</bibkey>
      <video href="2021.naacl-demos.11.mp4"/>
    </paper>
    <paper id="12">
      <title><fixed-case>A</fixed-case>ctive<fixed-case>A</fixed-case>nno: General-Purpose Document-Level Annotation Tool with Active Learning Integration</title>
      <author><first>Max</first><last>Wiechmann</last></author>
      <author><first>Seid Muhie</first><last>Yimam</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>99–105</pages>
      <abstract>ActiveAnno is an annotation tool focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive web UI for creating annotation projects, conducting annotations, adjudicating disagreements, and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables integration into other software systems, including an API for machine learning integration. ActiveAnno is built with extensible design and easy deployment in mind, all to enable users to perform annotation tasks with high efficiency and high-quality annotation results.</abstract>
      <url hash="620fb7be">2021.naacl-demos.12</url>
      <attachment type="Supplementary" hash="f4feeea6">2021.naacl-demos.12.Supplementary.pdf</attachment>
      <doi>10.18653/v1/2021.naacl-demos.12</doi>
      <bibkey>wiechmann-etal-2021-activeanno</bibkey>
      <video href="2021.naacl-demos.12.mp4"/>
    </paper>
    <paper id="13">
      <title><fixed-case>T</fixed-case>ext<fixed-case>E</fixed-case>ssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora</title>
      <author><first>Denis</first><last>Newman-Griffis</last></author>
      <author><first>Venkatesh</first><last>Sivaraman</last></author>
      <author><first>Adam</first><last>Perer</last></author>
      <author><first>Eric</first><last>Fosler-Lussier</last></author>
      <author><first>Harry</first><last>Hochheiser</last></author>
      <pages>106–115</pages>
      <abstract>Embeddings of words and concepts capture syntactic and semantic regularities of language; however, they have seen limited use as tools to study characteristics of different corpora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of corpora using embeddings. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality embeddings for corpus analysis. A case study on COVID-19 scientific literature illustrates the utility of the system. TextEssence can be found at https://textessence.github.io.</abstract>
      <url hash="4f44e392">2021.naacl-demos.13</url>
      <doi>10.18653/v1/2021.naacl-demos.13</doi>
      <bibkey>newman-griffis-etal-2021-textessence</bibkey>
      <video href="2021.naacl-demos.13.mp4"/>
      <pwccode url="https://github.com/drgriffis/text-essence" additional="false">drgriffis/text-essence</pwccode>
    </paper>
    <paper id="14">
      <title>Supporting <fixed-case>S</fixed-case>panish Writers using Automated Feedback</title>
      <author><first>Aoife</first><last>Cahill</last></author>
      <author><first>James</first><last>Bruno</last></author>
      <author><first>James</first><last>Ramey</last></author>
      <author><first>Gilmar</first><last>Ayala Meneses</last></author>
      <author><first>Ian</first><last>Blood</last></author>
      <author><first>Florencia</first><last>Tolentino</last></author>
      <author><first>Tamar</first><last>Lavee</last></author>
      <author><first>Slava</first><last>Andreyev</last></author>
      <pages>116–124</pages>
      <abstract>We present a tool that provides automated feedback to students studying Spanish writing. The feedback is given for four categories: topic development, coherence, writing conventions, and essay organization. The tool is made freely available via a Google Docs add-on. A small user study with third-level students in Mexico shows that students found the tool generally helpful and that most of them plan to continue using it as they work to improve their writing skills.</abstract>
      <url hash="01182553">2021.naacl-demos.14</url>
      <doi>10.18653/v1/2021.naacl-demos.14</doi>
      <bibkey>cahill-etal-2021-supporting</bibkey>
      <video href="2021.naacl-demos.14.mp4"/>
    </paper>
    <paper id="15">
      <title><fixed-case>A</fixed-case>lexa Conversations: An Extensible Data-driven Approach for Building Task-oriented Dialogue Systems</title>
      <author><first>Anish</first><last>Acharya</last></author>
      <author><first>Suranjit</first><last>Adhikari</last></author>
      <author><first>Sanchit</first><last>Agarwal</last></author>
      <author><first>Vincent</first><last>Auvray</last></author>
      <author><first>Nehal</first><last>Belgamwar</last></author>
      <author><first>Arijit</first><last>Biswas</last></author>
      <author><first>Shubhra</first><last>Chandra</last></author>
      <author><first>Tagyoung</first><last>Chung</last></author>
      <author><first>Maryam</first><last>Fazel-Zarandi</last></author>
      <author><first>Raefer</first><last>Gabriel</last></author>
      <author><first>Shuyang</first><last>Gao</last></author>
      <author><first>Rahul</first><last>Goel</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <author><first>Jan</first><last>Jezabek</last></author>
      <author><first>Abhay</first><last>Jha</last></author>
      <author><first>Jiun-Yu</first><last>Kao</last></author>
      <author><first>Prakash</first><last>Krishnan</last></author>
      <author><first>Peter</first><last>Ku</last></author>
      <author><first>Anuj</first><last>Goyal</last></author>
      <author><first>Chien-Wei</first><last>Lin</last></author>
      <author><first>Qing</first><last>Liu</last></author>
      <author><first>Arindam</first><last>Mandal</last></author>
      <author><first>Angeliki</first><last>Metallinou</last></author>
      <author><first>Vishal</first><last>Naik</last></author>
      <author><first>Yi</first><last>Pan</last></author>
      <author><first>Shachi</first><last>Paul</last></author>
      <author><first>Vittorio</first><last>Perera</last></author>
      <author><first>Abhishek</first><last>Sethi</last></author>
      <author><first>Minmin</first><last>Shen</last></author>
      <author><first>Nikko</first><last>Strom</last></author>
      <author><first>Eddie</first><last>Wang</last></author>
      <pages>125–132</pages>
      <abstract>Traditional goal-oriented dialogue systems rely on various components such as natural language understanding, dialogue state tracking, policy learning and response generation. Training each component requires annotations which are hard to obtain for every new domain, limiting scalability of such systems. Similarly, rule-based dialogue systems require extensive writing and maintenance of rules and do not scale either. End-to-End dialogue systems, on the other hand, do not require module-specific annotations but need a large amount of data for training. To overcome these problems, in this demo, we present Alexa Conversations, a new approach for building goal-oriented dialogue systems that is scalable, extensible as well as data efficient. The components of this system are trained in a data-driven manner, but instead of collecting annotated conversations for training, we generate them using a novel dialogue simulator based on a few seed dialogues and specifications of APIs and entities provided by the developer. Our approach provides out-of-the-box support for natural conversational phenomenon like entity sharing across turns or users changing their mind during conversation without requiring developers to provide any such dialogue flows. We exemplify our approach using a simple pizza ordering task and showcase its value in reducing the developer burden for creating a robust experience. Finally, we evaluate our system using a typical movie ticket booking task integrated with live APIs and show that the dialogue simulator is an essential component of the system that leads to over 50% improvement in turn-level action signature prediction accuracy.</abstract>
      <url hash="973a348c">2021.naacl-demos.15</url>
      <doi>10.18653/v1/2021.naacl-demos.15</doi>
      <bibkey>acharya-etal-2021-alexa</bibkey>
      <video href="2021.naacl-demos.15.mp4"/>
    </paper>
    <paper id="16">
      <title><fixed-case>RESIN</fixed-case>: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System</title>
      <author><first>Haoyang</first><last>Wen</last></author>
      <author><first>Ying</first><last>Lin</last></author>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Xiaoman</first><last>Pan</last></author>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Xudong</first><last>Lin</last></author>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Xiaodong</first><last>Yu</last></author>
      <author><first>Alexander</first><last>Dong</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Yi</first><last>Fung</last></author>
      <author><first>Piyush</first><last>Mishra</last></author>
      <author><first>Qing</first><last>Lyu</last></author>
      <author><first>Dídac</first><last>Surís</last></author>
      <author><first>Brian</first><last>Chen</last></author>
      <author><first>Susan Windisch</first><last>Brown</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Carl</first><last>Vondrick</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>133–143</pages>
      <abstract>We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.</abstract>
      <url hash="41c10f7c">2021.naacl-demos.16</url>
      <doi>10.18653/v1/2021.naacl-demos.16</doi>
      <bibkey>wen-etal-2021-resin</bibkey>
      <video href="2021.naacl-demos.16.mp4"/>
      <pwccode url="https://github.com/resin-kairos/resin-pipeline-public" additional="false">resin-kairos/resin-pipeline-public</pwccode>
    </paper>
    <paper id="17">
      <title><fixed-case>MUDES</fixed-case>: Multilingual Detection of Offensive Spans</title>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>144–152</pages>
      <abstract>The interest in offensive content identification in social media has grown substantially in recent years. Previous work has dealt mostly with post level annotations. However, identifying offensive spans is useful in many ways. To help coping with this important challenge, we present MUDES, a multilingual system to detect offensive spans in texts. MUDES features pre-trained models, a Python API for developers, and a user-friendly web-based interface. A detailed description of MUDES’ components is presented in this paper.</abstract>
      <url hash="8bbcbf4b">2021.naacl-demos.17</url>
      <doi>10.18653/v1/2021.naacl-demos.17</doi>
      <bibkey>ranasinghe-zampieri-2021-mudes</bibkey>
      <video href="2021.naacl-demos.17.mp4"/>
      <pwccode url="https://github.com/tharindudr/MUDES" additional="false">tharindudr/MUDES</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
  </volume>
  <volume id="srw" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</booktitle>
      <editor><first>Esin</first><last>Durmus</last></editor>
      <editor><first>Vivek</first><last>Gupta</last></editor>
      <editor><first>Nelson</first><last>Liu</last></editor>
      <editor><first>Nanyun</first><last>Peng</last></editor>
      <editor><first>Yu</first><last>Su</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.naacl-srw</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="ae6ac8a7">2021.naacl-srw.0</url>
      <bibkey>naacl-2021-2021-north-american</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Sampling and Filtering of Neural Machine Translation Distillation Data</title>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <pages>1–8</pages>
      <abstract>In most of neural machine translation distillation or stealing scenarios, the highest-scoring hypothesis of the target model (teacher) is used to train a new model (student). If reference translations are also available, then better hypotheses (with respect to the references) can be oversampled and poor hypotheses either removed or undersampled. This paper explores the sampling method landscape (pruning, hypothesis oversampling and undersampling, deduplication and their combination) with English to Czech and English to German MT models using standard MT evaluation metrics. We show that careful oversampling and combination with the original data leads to better performance when compared to training only on the original or synthesized data or their direct combination.</abstract>
      <url hash="e1e7c102">2021.naacl-srw.1</url>
      <doi>10.18653/v1/2021.naacl-srw.1</doi>
      <bibkey>zouhar-2021-sampling</bibkey>
      <video href="2021.naacl-srw.1.mp4"/>
      <pwccode url="https://github.com/zouharvi/reference-mt-distill" additional="false">zouharvi/reference-mt-distill</pwccode>
    </paper>
    <paper id="2">
      <title><fixed-case>I</fixed-case>ce<fixed-case>S</fixed-case>um: An <fixed-case>I</fixed-case>celandic Text Summarization Corpus</title>
      <author><first>Jón</first><last>Daðason</last></author>
      <author><first>Hrafn</first><last>Loftsson</last></author>
      <author><first>Salome</first><last>Sigurðardóttir</last></author>
      <author><first>Þorsteinn</first><last>Björnsson</last></author>
      <pages>9–14</pages>
      <abstract>Automatic Text Summarization (ATS) is the task of generating concise and fluent summaries from one or more documents. In this paper, we present IceSum, the first Icelandic corpus annotated with human-generated summaries. IceSum consists of 1,000 online news articles and their extractive summaries. We train and evaluate several neural network-based models on this dataset, comparing them against a selection of baseline methods. We find that an encoder-decoder model with a sequence-to-sequence based extractor obtains the best results, outperforming all baseline methods. Furthermore, we evaluate how the size of the training corpus affects the quality of the generated summaries. We release the corpus and the models with an open license.</abstract>
      <url hash="19e5f161">2021.naacl-srw.2</url>
      <doi>10.18653/v1/2021.naacl-srw.2</doi>
      <bibkey>dadason-etal-2021-icesum</bibkey>
      <video href="2021.naacl-srw.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Negation typology and general representation models for cross-lingual zero-shot negation scope resolution in <fixed-case>R</fixed-case>ussian, <fixed-case>F</fixed-case>rench, and <fixed-case>S</fixed-case>panish.</title>
      <author><first>Anastassia</first><last>Shaitarova</last></author>
      <author><first>Fabio</first><last>Rinaldi</last></author>
      <pages>15–23</pages>
      <abstract>Negation is a linguistic universal that poses difficulties for cognitive and computational processing. Despite many advances in text analytics, negation resolution remains an acute and continuously researched question in Natural Language Processing. Reliable negation parsing affects results in biomedical text mining, sentiment analysis, machine translation, and many other fields. The availability of multilingual pre-trained general representation models makes it possible to experiment with negation detection in languages that lack annotated data. In this work we test the performance of two state-of-the-art contextual representation models, Multilingual BERT and XLM-RoBERTa. We resolve negation scope by conducting zero-shot transfer between English, Spanish, French, and Russian. Our best result amounts to a token-level F1-score of 86.86% between Spanish and Russian. We correlate these results with a linguistic negation typology and lexical capacity of the models.</abstract>
      <url hash="73be1de7">2021.naacl-srw.3</url>
      <doi>10.18653/v1/2021.naacl-srw.3</doi>
      <bibkey>shaitarova-rinaldi-2021-negation</bibkey>
      <video href="2021.naacl-srw.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Representations of Meaning in Neural Networks for <fixed-case>NLP</fixed-case>: a Thesis Proposal</title>
      <author><first>Tomáš</first><last>Musil</last></author>
      <pages>24–31</pages>
      <abstract>Neural networks are the state-of-the-art method of machine learning for many problems in NLP. Their success in machine translation and other NLP tasks is phenomenal, but their interpretability is challenging. We want to find out how neural networks represent meaning. In order to do this, we propose to examine the distribution of meaning in the vector space representation of words in neural networks trained for NLP tasks. Furthermore, we propose to consider various theories of meaning in the philosophy of language and to find a methodology that would enable us to connect these areas.</abstract>
      <url hash="064df53d">2021.naacl-srw.4</url>
      <doi>10.18653/v1/2021.naacl-srw.4</doi>
      <bibkey>musil-2021-representations</bibkey>
      <video href="2021.naacl-srw.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Towards Layered Events and Schema Representations in Long Documents</title>
      <author><first>Hans Ole</first><last>Hatzel</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>32–39</pages>
      <abstract>In this thesis proposal, we explore the application of event extraction to literary texts. Considering the lengths of literary documents modeling events in different granularities may be more adequate to extract meaningful information, as individual elements contribute little to the overall semantics. We adapt the concept of schemas as sequences of events all describing a single process, connected through shared participants extending it to for multiple schemas in a document. Segmentation of event sequences into schemas is approached by modeling event sequences, on such task as the narrative cloze task, the prediction of missing events in sequences. We propose building on sequences of event embeddings to form schema embeddings, thereby summarizing sections of documents using a single representation. This approach will allow for the comparisons of different sections of documents and entire literary works. Literature is a challenging domain based on its variety of genres, yet the representation of literary content has received relatively little attention.</abstract>
      <url hash="a71ba858">2021.naacl-srw.5</url>
      <doi>10.18653/v1/2021.naacl-srw.5</doi>
      <bibkey>hatzel-biemann-2021-towards</bibkey>
      <video href="2021.naacl-srw.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Parallel Text Alignment and Monolingual Parallel Corpus Creation from Philosophical Texts for Text Simplification</title>
      <author><first>Stefan</first><last>Paun</last></author>
      <pages>40–46</pages>
      <abstract>Text simplification is a growing field with many potential useful applications. Training text simplification algorithms generally requires a lot of annotated data, however there are not many corpora suitable for this task. We propose a new unsupervised method for aligning text based on Doc2Vec embeddings and a new alignment algorithm, capable of aligning texts at different levels. Initial evaluation shows promising results for the new approach. We used the newly developed approach to create a new monolingual parallel corpus composed of the works of English early modern philosophers and their corresponding simplified versions.</abstract>
      <url hash="05d13951">2021.naacl-srw.6</url>
      <doi>10.18653/v1/2021.naacl-srw.6</doi>
      <bibkey>paun-2021-parallel</bibkey>
      <video href="2021.naacl-srw.6.mp4"/>
      <pwccode url="https://github.com/stefanpaun/massalign" additional="false">stefanpaun/massalign</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="7">
      <title>Syntax-Based Attention Masking for Neural Machine Translation</title>
      <author><first>Colin</first><last>McDonald</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <pages>47–52</pages>
      <abstract>We present a simple method for extending transformers to source-side trees. We define a number of masks that limit self-attention based on relationships among tree nodes, and we allow each attention head to learn which mask or masks to use. On translation from English to various low-resource languages, and translation in both directions between English and German, our method always improves over simple linearization of the source-side parse tree and almost always improves over a sequence-to-sequence baseline, by up to +2.1 BLEU.</abstract>
      <url hash="47c9a9c8">2021.naacl-srw.7</url>
      <doi>10.18653/v1/2021.naacl-srw.7</doi>
      <bibkey>mcdonald-chiang-2021-syntax</bibkey>
      <video href="2021.naacl-srw.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Multi-Modal Image Captioning for the Visually Impaired</title>
      <author><first>Hiba</first><last>Ahsan</last></author>
      <author><first>Daivat</first><last>Bhatt</last></author>
      <author><first>Kaivan</first><last>Shah</last></author>
      <author><first>Nikita</first><last>Bhalla</last></author>
      <pages>53–60</pages>
      <abstract>One of the ways blind people understand their surroundings is by clicking images and relying on descriptions generated by image-captioning systems. Current work on captioning images for the visually impaired do not use the textual data present in the image when generating captions. This problem is critical as many visual scenes contain text, and 21% of the questions asked by blind people about the images they click pertain to the text present in them. In this work, we propose altering AoANet, a state-of-the-art image-captioning system, to leverage text detected in the image as an input feature. In addition, we use a pointer-generator network to copy detected text to the caption when tokens need to be reproduced accurately. Our model outperforms AoANet on the benchmark dataset VizWiz, giving a 35% and 16.2% performance improvement on CIDEr and SPICE scores, respectively.</abstract>
      <url hash="e5d95593">2021.naacl-srw.8</url>
      <doi>10.18653/v1/2021.naacl-srw.8</doi>
      <bibkey>ahsan-etal-2021-multi</bibkey>
      <video href="2021.naacl-srw.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Open-Domain Question Answering with Pre-Constructed Question Spaces</title>
      <author><first>Jinfeng</first><last>Xiao</last></author>
      <author><first>Lidan</first><last>Wang</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Tong</first><last>Sun</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>61–67</pages>
      <abstract>Open-domain question answering aims at locating the answers to user-generated questions in massive collections of documents. Retriever-readers and knowledge graph approaches are two big families of solutions to this task. A retriever-reader first applies information retrieval techniques to locate a few passages that are likely to be relevant, and then feeds the retrieved text to a neural network reader to extract the answer. Alternatively, knowledge graphs can be constructed and queried to answer users’ questions. We propose an algorithm with a novel reader-retriever design that differs from both families. Our reader-retriever first uses an offline reader to read the corpus and generate collections of all answerable questions associated with their answers, and then uses an online retriever to respond to user queries by searching the pre-constructed question spaces for answers that are most likely to be asked in the given way. We further combine one retriever-reader and two reader-retrievers into a hybrid model called R6 for the best performance. Experiments with two large-scale public datasets show that R6 achieves state-of-the-art accuracy.</abstract>
      <url hash="613a2a28">2021.naacl-srw.9</url>
      <doi>10.18653/v1/2021.naacl-srw.9</doi>
      <bibkey>xiao-etal-2021-open</bibkey>
      <video href="2021.naacl-srw.9.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="10">
      <title>A Sliding-Window Approach to Automatic Creation of Meeting Minutes</title>
      <author><first>Jia Jin</first><last>Koay</last></author>
      <author><first>Alexander</first><last>Roustai</last></author>
      <author><first>Xiaojin</first><last>Dai</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>68–75</pages>
      <abstract>Meeting minutes record any subject matter discussed, decisions reached and actions taken at the meeting. The importance of automatic minuting cannot be overstated. In this paper, we present a sliding window approach to automatic generation of meeting minutes. It aims at addressing issues pertaining to the nature of spoken text, including the lengthy transcript and lack of document structure, which make it difficult to identify salient content to be included in meeting minutes. Our approach combines a sliding-window approach and a neural abstractive summarizer to navigate through the raw transcript to find salient content. The approach is evaluated on transcripts of natural meeting conversations, where we compare results obtained for human transcripts and two versions of automatic transcripts and discuss how and to what extent the summarizer succeeds at capturing salient content.</abstract>
      <url hash="9665baad">2021.naacl-srw.10</url>
      <doi>10.18653/v1/2021.naacl-srw.10</doi>
      <bibkey>koay-etal-2021-sliding</bibkey>
      <video href="2021.naacl-srw.10.mp4"/>
      <pwccode url="https://github.com/ucfnlp/meeting-sliding-window" additional="false">ucfnlp/meeting-sliding-window</pwccode>
    </paper>
    <paper id="11">
      <title>Exploration and Discovery of the <fixed-case>COVID</fixed-case>-19 Literature through Semantic Visualization</title>
      <author><first>Jingxuan</first><last>Tu</last></author>
      <author><first>Marc</first><last>Verhagen</last></author>
      <author><first>Brent</first><last>Cochran</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <pages>76–87</pages>
      <abstract>We propose semantic visualization as a linguistic visual analytic method. It can enable exploration and discovery over large datasets of complex networks by exploiting the semantics of the relations in them. This involves extracting information, applying parameter reduction operations, building hierarchical data representation and designing visualization. We also present the accompanying COVID-SemViz a searchable and interactive visualization system for knowledge exploration of COVID-19 data to demonstrate the application of our proposed method. In the user studies, users found that semantic visualization-powered COVID-SemViz is helpful in terms of finding relevant information and discovering unknown associations.</abstract>
      <url hash="2e0108f5">2021.naacl-srw.11</url>
      <doi>10.18653/v1/2021.naacl-srw.11</doi>
      <bibkey>tu-etal-2021-exploration</bibkey>
      <revision id="1" href="2021.naacl-srw.11v1" hash="964dc194"/>
      <revision id="2" href="2021.naacl-srw.11v2" hash="2e0108f5" date="2021-06-28">Added a sponsor to Acknowledgments section</revision>
      <video href="2021.naacl-srw.11.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
    </paper>
    <paper id="12">
      <title>Shuffled-token Detection for Refining Pre-trained <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a</title>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Anjali</first><last>Agrawal</last></author>
      <author><first>Jeewon</first><last>Ha</last></author>
      <author><first>Benjamin</first><last>Bloch</last></author>
      <pages>88–93</pages>
      <abstract>State-of-the-art transformer models have achieved robust performance on a variety of NLP tasks. Many of these approaches have employed domain agnostic pre-training tasks to train models that yield highly generalized sentence representations that can be fine-tuned for specific downstream tasks. We propose refining a pre-trained NLP model using the objective of detecting shuffled tokens. We use a sequential approach by starting with the pre-trained RoBERTa model and training it using our approach. Applying random shuffling strategy on the word-level, we found that our approach enables the RoBERTa model achieve better performance on 4 out of 7 GLUE tasks. Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations.</abstract>
      <url hash="6f5d204d">2021.naacl-srw.12</url>
      <doi>10.18653/v1/2021.naacl-srw.12</doi>
      <bibkey>panda-etal-2021-shuffled</bibkey>
      <video href="2021.naacl-srw.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Morphology-Aware Meta-Embeddings for <fixed-case>T</fixed-case>amil</title>
      <author><first>Arjun Sai</first><last>Krishnan</last></author>
      <author><first>Seyoon</first><last>Ragavan</last></author>
      <pages>94–111</pages>
      <abstract>In this work, we explore generating morphologically enhanced word embeddings for Tamil, a highly agglutinative South Indian language with rich morphology that remains low-resource with regards to NLP tasks. We present here the first-ever word analogy dataset for Tamil, consisting of 4499 hand-curated word tetrads across 10 semantic and 13 morphological relation types. Using a rules-based segmenter to capture morphology as well as meta-embedding techniques, we train meta-embeddings that outperform existing baselines by 16% on our analogy task and appear to mitigate a previously observed trade-off between semantic and morphological accuracy.</abstract>
      <url hash="7eaa71ac">2021.naacl-srw.13</url>
      <doi>10.18653/v1/2021.naacl-srw.13</doi>
      <bibkey>krishnan-ragavan-2021-morphology</bibkey>
      <video href="2021.naacl-srw.13.mp4"/>
      <pwccode url="https://github.com/arjun-sai-krishnan/tamil-morpho-embeddings" additional="false">arjun-sai-krishnan/tamil-morpho-embeddings</pwccode>
    </paper>
    <paper id="14">
      <title>Seed Word Selection for Weakly-Supervised Text Classification with Unsupervised Error Estimation</title>
      <author><first>Yiping</first><last>Jin</last></author>
      <author><first>Akshay</first><last>Bhatia</last></author>
      <author><first>Dittaya</first><last>Wanvarie</last></author>
      <pages>112–118</pages>
      <abstract>Weakly-supervised text classification aims to induce text classifiers from only a few user-provided seed words. The vast majority of previous work assumes high-quality seed words are given. However, the expert-annotated seed words are sometimes non-trivial to come up with. Furthermore, in the weakly-supervised learning setting, we do not have any labeled document to measure the seed words’ efficacy, making the seed word selection process “a walk in the dark”. In this work, we remove the need for expert-curated seed words by first mining (noisy) candidate seed words associated with the category names. We then train interim models with individual candidate seed words. Lastly, we estimate the interim models’ error rate in an unsupervised manner. The seed words that yield the lowest estimated error rates are added to the final seed word set. A comprehensive evaluation of six binary classification tasks on four popular datasets demonstrates that the proposed method outperforms a baseline using only category name seed words and obtained comparable performance as a counterpart using expert-annotated seed words.</abstract>
      <url hash="8fd4bbbb">2021.naacl-srw.14</url>
      <doi>10.18653/v1/2021.naacl-srw.14</doi>
      <bibkey>jin-etal-2021-seed</bibkey>
      <video href="2021.naacl-srw.14.mp4"/>
      <pwccode url="https://github.com/YipingNUS/OptimSeed" additional="false">YipingNUS/OptimSeed</pwccode>
    </paper>
    <paper id="15">
      <title>Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation</title>
      <author><first>Tatsuya</first><last>Ide</last></author>
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <pages>119–125</pages>
      <abstract>For a computer to naturally interact with a human, it needs to be human-like. In this paper, we propose a neural response generation model with multi-task learning of generation and classification, focusing on emotion. Our model based on BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model, is trained to generate responses and recognize emotions simultaneously. Furthermore, we weight the losses for the tasks to control the update of parameters. Automatic evaluations and crowdsourced manual evaluations show that the proposed model makes generated responses more emotionally aware.</abstract>
      <url hash="3ca106ae">2021.naacl-srw.15</url>
      <doi>10.18653/v1/2021.naacl-srw.15</doi>
      <bibkey>ide-kawahara-2021-multi</bibkey>
      <video href="2021.naacl-srw.15.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="16">
      <title>Comparison of Grammatical Error Correction Using Back-Translation Models</title>
      <author><first>Aomi</first><last>Koyama</last></author>
      <author><first>Kengo</first><last>Hotate</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>126–135</pages>
      <abstract>Grammatical error correction (GEC) suffers from a lack of sufficient parallel data. Studies on GEC have proposed several methods to generate pseudo data, which comprise pairs of grammatical and artificially produced ungrammatical sentences. Currently, a mainstream approach to generate pseudo data is back-translation (BT). Most previous studies using BT have employed the same architecture for both the GEC and BT models. However, GEC models have different correction tendencies depending on the architecture of their models. Thus, in this study, we compare the correction tendencies of GEC models trained on pseudo data generated by three BT models with different architectures, namely, Transformer, CNN, and LSTM. The results confirm that the correction tendencies for each error type are different for every BT model. In addition, we investigate the correction tendencies when using a combination of pseudo data generated by different BT models. As a result, we find that the combination of different BT models improves or interpolates the performance of each error type compared with using a single BT model with different seeds.</abstract>
      <url hash="b1a75da0">2021.naacl-srw.16</url>
      <doi>10.18653/v1/2021.naacl-srw.16</doi>
      <bibkey>koyama-etal-2021-comparison</bibkey>
      <video href="2021.naacl-srw.16.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="17">
      <title>Parallel sentences mining with transfer learning in an unsupervised setting</title>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Shaolin</first><last>Zhu</last></author>
      <author><first>Feng</first><last>Yifan</last></author>
      <author><first>Chenggang</first><last>Mi</last></author>
      <pages>136–142</pages>
      <abstract>The quality and quantity of parallel sentences are known as very important training data for constructing neural machine translation (NMT) systems. However, these resources are not available for many low-resource language pairs. Many existing methods need strong supervision are not suitable. Although several attempts at developing unsupervised models, they ignore the language-invariant between languages. In this paper, we propose an approach based on transfer learning to mine parallel sentences in the unsupervised setting.With the help of bilingual corpora of rich-resource language pairs, we can mine parallel sentences without bilingual supervision of low-resource language pairs. Experiments show that our approach improves the performance of mined parallel sentences compared with previous methods. In particular, we achieve excellent results at two real-world low-resource language pairs.</abstract>
      <url hash="3f8ad05b">2021.naacl-srw.17</url>
      <doi>10.18653/v1/2021.naacl-srw.17</doi>
      <bibkey>sun-etal-2021-parallel</bibkey>
      <video href="2021.naacl-srw.17.mp4"/>
    </paper>
    <paper id="18">
      <title>Sentence Concatenation Approach to Data Augmentation for Neural Machine Translation</title>
      <author><first>Seiichiro</first><last>Kondo</last></author>
      <author><first>Kengo</first><last>Hotate</last></author>
      <author><first>Tosho</first><last>Hirasawa</last></author>
      <author><first>Masahiro</first><last>Kaneko</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <pages>143–149</pages>
      <abstract>Recently, neural machine translation is widely used for its high translation accuracy, but it is also known to show poor performance at long sentence translation. Besides, this tendency appears prominently for low resource languages. We assume that these problems are caused by long sentences being few in the train data. Therefore, we propose a data augmentation method for handling long sentences. Our method is simple; we only use given parallel corpora as train data and generate long sentences by concatenating two sentences. Based on our experiments, we confirm improvements in long sentence translation by proposed data augmentation despite the simplicity. Moreover, the proposed method improves translation quality more when combined with back-translation.</abstract>
      <url hash="f5d1f20a">2021.naacl-srw.18</url>
      <doi>10.18653/v1/2021.naacl-srw.18</doi>
      <bibkey>kondo-etal-2021-sentence</bibkey>
      <video href="2021.naacl-srw.18.mp4"/>
    </paper>
    <paper id="19">
      <title>Emotion Classification in a Resource Constrained Language Using Transformer-based Approach</title>
      <author><first>Avishek</first><last>Das</last></author>
      <author><first>Omar</first><last>Sharif</last></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last></author>
      <author><first>Iqbal H.</first><last>Sarker</last></author>
      <pages>150–158</pages>
      <abstract>Although research on emotion classification has significantly progressed in high-resource languages, it is still infancy for resource-constrained languages like Bengali. However, unavailability of necessary language processing tools and deficiency of benchmark corpora makes the emotion classification task in Bengali more challenging and complicated. This work proposes a transformer-based technique to classify the Bengali text into one of the six basic emotions: anger, fear, disgust, sadness, joy, and surprise. A Bengali emotion corpus consists of 6243 texts is developed for the classification task. Experimentation carried out using various machine learning (LR, RF, MNB, SVM), deep neural networks (CNN, BiLSTM, CNN+BiLSTM) and transformer (Bangla-BERT, m-BERT, XLM-R) based approaches. Experimental outcomes indicate that XLM-R outdoes all other techniques by achieving the highest weighted f_1-score of 69.73% on the test data.</abstract>
      <url hash="15fc2e51">2021.naacl-srw.19</url>
      <doi>10.18653/v1/2021.naacl-srw.19</doi>
      <bibkey>das-etal-2021-emotion</bibkey>
      <video href="2021.naacl-srw.19.mp4"/>
      <pwccode url="https://github.com/sagorbrur/bangla-bert" additional="true">sagorbrur/bangla-bert</pwccode>
    </paper>
    <paper id="20">
      <title>Hie-<fixed-case>BART</fixed-case>: Document Summarization with Hierarchical <fixed-case>BART</fixed-case></title>
      <author><first>Kazuki</first><last>Akiyama</last></author>
      <author><first>Akihiro</first><last>Tamura</last></author>
      <author><first>Takashi</first><last>Ninomiya</last></author>
      <pages>159–165</pages>
      <abstract>This paper proposes a new abstractive document summarization model, hierarchical BART (Hie-BART), which captures hierarchical structures of a document (i.e., sentence-word structures) in the BART model. Although the existing BART model has achieved a state-of-the-art performance on document summarization tasks, the model does not have the interactions between sentence-level information and word-level information. In machine translation tasks, the performance of neural machine translation models has been improved by incorporating multi-granularity self-attention (MG-SA), which captures the relationships between words and phrases. Inspired by the previous work, the proposed Hie-BART model incorporates MG-SA into the encoder of the BART model for capturing sentence-word structures. Evaluations on the CNN/Daily Mail dataset show that the proposed Hie-BART model outperforms some strong baselines and improves the performance of a non-hierarchical BART model (+0.23 ROUGE-L).</abstract>
      <url hash="0d561395">2021.naacl-srw.20</url>
      <doi>10.18653/v1/2021.naacl-srw.20</doi>
      <bibkey>akiyama-etal-2021-hie</bibkey>
      <video href="2021.naacl-srw.20.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="21">
      <title>Towards Multi-Modal Text-Image Retrieval to improve Human Reading</title>
      <author><first>Florian</first><last>Schneider</last></author>
      <author><first>Özge</first><last>Alaçam</last></author>
      <author><first>Xintong</first><last>Wang</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <abstract>In primary school, children’s books, as well as in modern language learning apps, multi-modal learning strategies like illustrations of terms and phrases are used to support reading comprehension. Also, several studies in educational psychology suggest that integrating cross-modal information will improve reading comprehension. We claim that state-of- he-art multi-modal transformers, which could be used in a language learner context to improve human reading, will perform poorly because of the short and relatively simple textual data those models are trained with. To prove our hypotheses, we collected a new multi-modal image-retrieval dataset based on data from Wikipedia. In an in-depth data analysis, we highlight the differences between our dataset and other popular datasets. Additionally, we evaluate several state-of-the-art multi-modal transformers on text-image retrieval on our dataset and analyze their meager results, which verify our claims.</abstract>
      <url hash="d8aae487">2021.naacl-srw.21</url>
      <bibkey>schneider-etal-2021-towards</bibkey>
      <video href="2021.naacl-srw.21.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikicaps">WikiCaps</pwcdataset>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</booktitle>
      <editor><first>Greg</first><last>Kondrak</last></editor>
      <editor><first>Kalina</first><last>Bontcheva</last></editor>
      <editor><first>Dan</first><last>Gillick</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.naacl-tutorials</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="7699fb95">2021.naacl-tutorials.0</url>
      <bibkey>naacl-2021-2021-north-american-chapter</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Pretrained Transformers for Text Ranking: <fixed-case>BERT</fixed-case> and Beyond</title>
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Rodrigo</first><last>Nogueira</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>1–4</pages>
      <abstract>The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and &amp; Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly.</abstract>
      <url hash="5cc45700">2021.naacl-tutorials.1</url>
      <doi>10.18653/v1/2021.naacl-tutorials.1</doi>
      <bibkey>yates-etal-2021-pretrained</bibkey>
      <video href="2021.naacl-tutorials.1.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/asnq">ASNQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/beir">BEIR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dl-hard">DL-HARD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/trec-covid">TREC-COVID</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="2">
      <title>Fine-grained Interpretation and Causation Analysis in Deep <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>Narine</first><last>Kokhlikyan</last></author>
      <author><first>Fahim</first><last>Dalvi</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <pages>5–10</pages>
      <abstract>Deep neural networks have constantly pushed the state-of-the-art performance in natural language processing and are considered as the de-facto modeling approach in solving complex NLP tasks such as machine translation, summarization and question-answering. Despite the proven efficacy of deep neural networks at-large, their opaqueness is a major cause of concern. In this tutorial, we will present research work on interpreting fine-grained components of a neural network model from two perspectives, i) fine-grained interpretation, and ii) causation analysis. The former is a class of methods to analyze neurons with respect to a desired language concept or a task. The latter studies the role of neurons and input features in explaining the decisions made by the model. We will also discuss how interpretation methods and causation analysis can connect towards better interpretability of model prediction. Finally, we will walk you through various toolkits that facilitate fine-grained interpretation and causation analysis of neural models.</abstract>
      <url hash="7b59c512">2021.naacl-tutorials.2</url>
      <doi>10.18653/v1/2021.naacl-tutorials.2</doi>
      <bibkey>sajjad-etal-2021-fine</bibkey>
      <video href="2021.naacl-tutorials.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Deep Learning on Graphs for Natural Language Processing</title>
      <author><first>Lingfei</first><last>Wu</last></author>
      <author><first>Yu</first><last>Chen</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <pages>11–14</pages>
      <abstract>Due to its great power in modeling non-Euclidean data like graphs or manifolds, deep learning on graph techniques (i.e., Graph Neural Networks (GNNs)) have opened a new door to solving challenging graph-related NLP problems. There has seen a surge of interests in applying deep learning on graph techniques to NLP, and has achieved considerable success in many NLP tasks, ranging from classification tasks like sentence classification, semantic role labeling and relation extraction, to generation tasks like machine translation, question generation and summarization. Despite these successes, deep learning on graphs for NLP still face many challenges, including automatically transforming original text sequence data into highly graph-structured data, and effectively modeling complex data that involves mapping between graph-based inputs and other highly structured output data such as sequences, trees, and graph data with multi-types in both nodes and edges. This tutorial will cover relevant and interesting topics on applying deep learning on graph techniques to NLP, including automatic graph construction for NLP, graph representation learning for NLP, advanced GNN based models (e.g., graph2seq, graph2tree, and graph2graph) for NLP, and the applications of GNNs in various NLP tasks (e.g., machine translation, natural language generation, information extraction and semantic parsing). In addition, hands-on demonstration sessions will be included to help the audience gain practical experience on applying GNNs to solve challenging NLP problems using our recently developed open source library – Graph4NLP, the first library for researchers and practitioners for easy use of GNNs for various NLP tasks.</abstract>
      <url hash="98425d99">2021.naacl-tutorials.3</url>
      <doi>10.18653/v1/2021.naacl-tutorials.3</doi>
      <bibkey>wu-etal-2021-deep</bibkey>
      <video href="2021.naacl-tutorials.3.mp4"/>
    </paper>
    <paper id="4">
      <title>A Tutorial on Evaluation Metrics used in Natural Language Generation</title>
      <author><first>Mitesh M.</first><last>Khapra</last></author>
      <author><first>Ananya B.</first><last>Sai</last></author>
      <pages>15–19</pages>
      <abstract>The advent of Deep Learning and the availability of large scale datasets has accelerated research on Natural Language Generation with a focus on newer tasks and better models. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued the development of automatic evaluation metrics. Especially in the last few years, there has been an increasing focus on evaluation metrics, with several criticisms of existing metrics and proposals for several new metrics. This tutorial presents the evolution of automatic evaluation metrics to their current state along with the emerging trends in this field by specifically addressing the following questions: (i) What makes NLG evaluation challenging? (ii) Why do we need automatic evaluation metrics? (iii) What are the existing automatic evaluation metrics and how can they be organised in a coherent taxonomy? (iv) What are the criticisms and shortcomings of existing metrics? (v) What are the possible future directions of research?</abstract>
      <url hash="459d1f5b">2021.naacl-tutorials.4</url>
      <doi>10.18653/v1/2021.naacl-tutorials.4</doi>
      <bibkey>khapra-sai-2021-tutorial</bibkey>
      <video href="2021.naacl-tutorials.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Beyond Paragraphs: <fixed-case>NLP</fixed-case> for Long Sequences</title>
      <author><first>Iz</first><last>Beltagy</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Matthew E.</first><last>Peters</last></author>
      <pages>20–24</pages>
      <abstract>In this tutorial, we aim at bringing interested NLP researchers up to speed about the recent and ongoing techniques for document-level representation learning. Additionally, our goal is to reveal new research opportunities to the audience, which will hopefully bring us closer to address existing challenges in this domain.</abstract>
      <url hash="6d7a352b">2021.naacl-tutorials.5</url>
      <doi>10.18653/v1/2021.naacl-tutorials.5</doi>
      <bibkey>beltagy-etal-2021-beyond</bibkey>
      <pwccode url="https://github.com/allenai/naacl2021-longdoc-tutorial" additional="false">allenai/naacl2021-longdoc-tutorial</pwccode>
    </paper>
    <paper id="6">
      <title>Crowdsourcing Natural Language Data at Scale: A Hands-On Tutorial</title>
      <author><first>Alexey</first><last>Drutsa</last></author>
      <author><first>Dmitry</first><last>Ustalov</last></author>
      <author><first>Valentina</first><last>Fedorova</last></author>
      <author><first>Olga</first><last>Megorskaya</last></author>
      <author><first>Daria</first><last>Baidakova</last></author>
      <pages>25–30</pages>
      <abstract>In this tutorial, we present a portion of unique industry experience in efficient natural language data annotation via crowdsourcing shared by both leading researchers and engineers from Yandex. We will make an introduction to data labeling via public crowdsourcing marketplaces and will present the key components of efficient label collection. This will be followed by a practical session, where participants address a real-world language resource production task, experiment with selecting settings for the labeling process, and launch their label collection project on one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session and we will present useful quality control techniques and provide the attendees with an opportunity to discuss their own annotation ideas.</abstract>
      <url hash="2a2af5c4">2021.naacl-tutorials.6</url>
      <doi>10.18653/v1/2021.naacl-tutorials.6</doi>
      <bibkey>drutsa-etal-2021-crowdsourcing</bibkey>
      <video href="2021.naacl-tutorials.6.mp4"/>
    </paper>
  </volume>
  <volume id="industry" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</booktitle>
      <editor><first>Young-bum</first><last>Kim</last></editor>
      <editor><first>Yunyao</first><last>Li</last></editor>
      <editor><first>Owen</first><last>Rambow</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.naacl-industry</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="e7988991">2021.naacl-industry.0</url>
      <bibkey>naacl-2021-2021-north-american-chapter-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title>When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages</title>
      <author><first>Stojan</first><last>Trajanovski</last></author>
      <author><first>Chad</first><last>Atalla</last></author>
      <author><first>Kunho</first><last>Kim</last></author>
      <author><first>Vipul</first><last>Agarwal</last></author>
      <author><first>Milad</first><last>Shokouhi</last></author>
      <author><first>Chris</first><last>Quirk</last></author>
      <pages>1–9</pages>
      <abstract>Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical service-oriented text prediction metrics.</abstract>
      <url hash="c107f40e">2021.naacl-industry.1</url>
      <doi>10.18653/v1/2021.naacl-industry.1</doi>
      <bibkey>trajanovski-etal-2021-text</bibkey>
      <video href="2021.naacl-industry.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Identifying and Resolving Annotation Changes for Natural Language Understanding</title>
      <author><first>Jose</first><last>Garrido Ramas</last></author>
      <author><first>Giorgio</first><last>Pessot</last></author>
      <author><first>Abdalghani</first><last>Abujabal</last></author>
      <author><first>Martin</first><last>Rajman</last></author>
      <pages>10–18</pages>
      <abstract>Annotation conflict resolution is crucial towards building machine learning models with acceptable performance. Past work on annotation conflict resolution had assumed that data is collected at once, with a fixed set of annotators and fixed annotation guidelines. Moreover, previous work dealt with atomic labeling tasks. In this paper, we address annotation conflict resolution for Natural Language Understanding (NLU), a structured prediction task, in a real-world setting of commercial voice-controlled personal assistants, where (1) regular data collections are needed to support new and existing functionalities, (2) annotation guidelines evolve over time, and (3) the pool of annotators change across data collections. We devise an approach combining information-theoretic measures and a supervised neural model to resolve conflicts in data annotation. We evaluate our approach both intrinsically and extrinsically on a real-world dataset with 3.5M utterances of a commercial dialog system in German. Our approach leads to dramatic improvements over a majority baseline especially in contentious cases. On the NLU task, our approach achieves 2.75% error reduction over a no-resolution baseline.</abstract>
      <url hash="d6d001a4">2021.naacl-industry.2</url>
      <doi>10.18653/v1/2021.naacl-industry.2</doi>
      <bibkey>garrido-ramas-etal-2021-identifying</bibkey>
    </paper>
    <paper id="3">
      <title>Optimizing <fixed-case>NLU</fixed-case> Reranking Using Entity Resolution Signals in Multi-domain Dialog Systems</title>
      <author><first>Tong</first><last>Wang</last></author>
      <author><first>Jiangning</first><last>Chen</last></author>
      <author><first>Mohsen</first><last>Malmir</last></author>
      <author><first>Shuyan</first><last>Dong</last></author>
      <author><first>Xin</first><last>He</last></author>
      <author><first>Han</first><last>Wang</last></author>
      <author><first>Chengwei</first><last>Su</last></author>
      <author><first>Yue</first><last>Liu</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <pages>19–25</pages>
      <abstract>In dialog systems, the Natural Language Understanding (NLU) component typically makes the interpretation decision (including domain, intent and slots) for an utterance before the mentioned entities are resolved. This may result in intent classification and slot tagging errors. In this work, we propose to leverage Entity Resolution (ER) features in NLU reranking and introduce a novel loss term based on ER signals to better learn model weights in the reranking framework. In addition, for a multi-domain dialog scenario, we propose a score distribution matching method to ensure scores generated by the NLU reranking models for different domains are properly calibrated. In offline experiments, we demonstrate our proposed approach significantly outperforms the baseline model on both single-domain and cross-domain evaluations.</abstract>
      <url hash="669e0672">2021.naacl-industry.3</url>
      <doi>10.18653/v1/2021.naacl-industry.3</doi>
      <bibkey>wang-etal-2021-optimizing</bibkey>
      <video href="2021.naacl-industry.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Entity Resolution in Open-domain Conversations</title>
      <author><first>Mingyue</first><last>Shang</last></author>
      <author><first>Tong</first><last>Wang</last></author>
      <author><first>Mihail</first><last>Eric</last></author>
      <author><first>Jiangning</first><last>Chen</last></author>
      <author><first>Jiyang</first><last>Wang</last></author>
      <author><first>Matthew</first><last>Welch</last></author>
      <author><first>Tiantong</first><last>Deng</last></author>
      <author><first>Akshay</first><last>Grewal</last></author>
      <author><first>Han</first><last>Wang</last></author>
      <author><first>Yue</first><last>Liu</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>26–33</pages>
      <abstract>In recent years, incorporating external knowledge for response generation in open-domain conversation systems has attracted great interest. To improve the relevancy of retrieved knowledge, we propose a neural entity linking (NEL) approach. Different from formal documents, such as news, conversational utterances are informal and multi-turn, which makes it more challenging to disambiguate the entities. Therefore, we present a context-aware named entity recognition model (NER) and entity resolution (ER) model to utilize dialogue context information. We conduct NEL experiments on three open-domain conversation datasets and validate that incorporating context information improves the performance of NER and ER models. The end-to-end NEL approach outperforms the baseline by 62.8% relatively in F1 metric. Furthermore, we verify that using external knowledge based on NEL benefits the neural response generation model.</abstract>
      <url hash="ffdf16fe">2021.naacl-industry.4</url>
      <doi>10.18653/v1/2021.naacl-industry.4</doi>
      <bibkey>shang-etal-2021-entity</bibkey>
      <video href="2021.naacl-industry.4.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="5">
      <title>Pretrain-Finetune Based Training of Task-Oriented Dialogue Systems in a Real-World Setting</title>
      <author><first>Manisha</first><last>Srivastava</last></author>
      <author><first>Yichao</first><last>Lu</last></author>
      <author><first>Riley</first><last>Peschon</last></author>
      <author><first>Chenyang</first><last>Li</last></author>
      <pages>34–40</pages>
      <abstract>One main challenge in building task-oriented dialogue systems is the limited amount of supervised training data available. In this work, we present a method for training retrieval-based dialogue systems using a small amount of high-quality, annotated data and a larger, unlabeled dataset. We show that pretraining using unlabeled data can bring better model performance with a 31% boost in Recall@1 compared with no pretraining. The proposed finetuning technique based on a small amount of high-quality, annotated data resulted in 26% offline and 33% online performance improvement in Recall@1 over the pretrained model. The model is deployed in an agent-support application and evaluated on live customer service contacts, providing additional insights into the real-world implications compared with most other publications in the domain often using asynchronous transcripts (e.g. Reddit data). The high performance of 74% Recall@1 shown in the customer service example demonstrates the effectiveness of this pretrain-finetune approach in dealing with the limited supervised data challenge.</abstract>
      <url hash="943d5e1b">2021.naacl-industry.5</url>
      <doi>10.18653/v1/2021.naacl-industry.5</doi>
      <bibkey>srivastava-etal-2021-pretrain</bibkey>
      <video href="2021.naacl-industry.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Contextual Domain Classification with Temporal Representations</title>
      <author><first>Tzu-Hsiang</first><last>Lin</last></author>
      <author><first>Yipeng</first><last>Shi</last></author>
      <author><first>Chentao</first><last>Ye</last></author>
      <author><first>Yang</first><last>Fan</last></author>
      <author><first>Weitong</first><last>Ruan</last></author>
      <author><first>Emre</first><last>Barut</last></author>
      <author><first>Wael</first><last>Hamza</last></author>
      <author><first>Chengwei</first><last>Su</last></author>
      <pages>41–48</pages>
      <abstract>In commercial dialogue systems, the Spoken Language Understanding (SLU) component tends to have numerous domains thus context is needed to help resolve ambiguities. Previous works that incorporate context for SLU have mostly focused on domains where context is limited to a few minutes. However, there are domains that have related context that could span up to hours and days. In this paper, we propose temporal representations that combine wall-clock second difference and turn order offset information to utilize both recent and distant context in a novel large-scale setup. Experiments on the Contextual Domain Classification (CDC) task with various encoder architectures show that temporal representations combining both information outperforms only one of the two. We further demonstrate that our contextual Transformer is able to reduce 13.04% of classification errors compared to a non-contextual baseline. We also conduct empirical analyses to study recent versus distant context and opportunities to lower deployment costs.</abstract>
      <url hash="02b5c203">2021.naacl-industry.6</url>
      <doi>10.18653/v1/2021.naacl-industry.6</doi>
      <bibkey>lin-etal-2021-contextual</bibkey>
      <video href="2021.naacl-industry.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Bootstrapping a Music Voice Assistant with Weak Supervision</title>
      <author><first>Sergio</first><last>Oramas</last></author>
      <author><first>Massimo</first><last>Quadrana</last></author>
      <author><first>Fabien</first><last>Gouyon</last></author>
      <pages>49–55</pages>
      <abstract>One of the first building blocks to create a voice assistant relates to the task of tagging entities or attributes in user queries. This can be particularly challenging when entities are in the tenth of millions, as is the case of e.g. music catalogs. Training slot tagging models at an industrial scale requires large quantities of accurately labeled user queries, which are often hard and costly to gather. On the other hand, voice assistants typically collect plenty of unlabeled queries that often remain unexploited. This paper presents a weakly-supervised methodology to label large amounts of voice query logs, enhanced with a manual filtering step. Our experimental evaluations show that slot tagging models trained on weakly-supervised data outperform models trained on hand-annotated or synthetic data, at a lower cost. Further, manual filtering of weakly-supervised data leads to a very significant reduction in Sentence Error Rate, while allowing us to drastically reduce human curation efforts from weeks to hours, with respect to hand-annotation of queries. The method is applied to successfully bootstrap a slot tagging system for a major music streaming service that currently serves several tens of thousands of daily voice queries.</abstract>
      <url hash="70a0db59">2021.naacl-industry.7</url>
      <doi>10.18653/v1/2021.naacl-industry.7</doi>
      <bibkey>oramas-etal-2021-bootstrapping</bibkey>
      <video href="2021.naacl-industry.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Continuous Model Improvement for Language Understanding with Machine Translation</title>
      <author><first>Abdalghani</first><last>Abujabal</last></author>
      <author><first>Claudio</first><last>Delli Bovi</last></author>
      <author><first>Sungho</first><last>Ryu</last></author>
      <author><first>Turan</first><last>Gojayev</last></author>
      <author><first>Fabian</first><last>Triefenbach</last></author>
      <author><first>Yannick</first><last>Versley</last></author>
      <pages>56–62</pages>
      <abstract>Scaling conversational personal assistants to a multitude of languages puts high demands on collecting and labelling data, a setting in which cross-lingual learning techniques can help to reconcile the need for well-performing Natural Language Understanding (NLU) with a desideratum to support many languages without incurring unacceptable cost. In this work, we show that automatically annotating unlabeled utterances using Machine Translation in an offline fashion and adding them to the training data can improve performance for existing NLU features for low-resource languages, where a straightforward translate-test approach as considered in existing literature would fail the latency requirements of a live environment. We demonstrate the effectiveness of our method with intrinsic and extrinsic evaluation using a real-world commercial dialog system in German. Beyond an intrinsic evaluation, where 56% of the resulting automatically labeled utterances had a perfect match with ground-truth labels, we see significant performance improvements in an extrinsic evaluation settings when manual labeled data is available in small quantities.</abstract>
      <url hash="4c08a5e0">2021.naacl-industry.8</url>
      <doi>10.18653/v1/2021.naacl-industry.8</doi>
      <bibkey>abujabal-etal-2021-continuous</bibkey>
      <video href="2021.naacl-industry.8.mp4"/>
    </paper>
    <paper id="9">
      <title>A Hybrid Approach to Scalable and Robust Spoken Language Understanding in Enterprise Virtual Agents</title>
      <author><first>Ryan</first><last>Price</last></author>
      <author><first>Mahnoosh</first><last>Mehrabani</last></author>
      <author><first>Narendra</first><last>Gupta</last></author>
      <author><first>Yeon-Jun</first><last>Kim</last></author>
      <author><first>Shahab</first><last>Jalalvand</last></author>
      <author><first>Minhua</first><last>Chen</last></author>
      <author><first>Yanjie</first><last>Zhao</last></author>
      <author><first>Srinivas</first><last>Bangalore</last></author>
      <pages>63–71</pages>
      <abstract>Spoken language understanding (SLU) extracts the intended mean- ing from a user utterance and is a critical component of conversational virtual agents. In enterprise virtual agents (EVAs), language understanding is substantially challenging. First, the users are infrequent callers who are unfamiliar with the expectations of a pre-designed conversation flow. Second, the users are paying customers of an enterprise who demand a reliable, consistent and efficient user experience when resolving their issues. In this work, we describe a general and robust framework for intent and entity extraction utilizing a hybrid of statistical and rule-based approaches. Our framework includes confidence modeling that incorporates information from all components in the SLU pipeline, a critical addition for EVAs to en- sure accuracy. Our focus is on creating accurate and scalable SLU that can be deployed rapidly for a large class of EVA applications with little need for human intervention.</abstract>
      <url hash="0a40ab22">2021.naacl-industry.9</url>
      <doi>10.18653/v1/2021.naacl-industry.9</doi>
      <bibkey>price-etal-2021-hybrid</bibkey>
      <video href="2021.naacl-industry.9.mp4"/>
    </paper>
    <paper id="10">
      <title>Proteno: Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems</title>
      <author><first>Shubhi</first><last>Tyagi</last></author>
      <author><first>Antonio</first><last>Bonafonte</last></author>
      <author><first>Jaime</first><last>Lorenzo-Trueba</last></author>
      <author><first>Javier</first><last>Latorre</last></author>
      <pages>72–79</pages>
      <abstract>Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new languages is hard. We propose a novel architecture to facilitate it for multiple languages while using data less than 3% of the size of the data used by the state of the art results on English. We treat TN as a sequence classification problem and propose a granular tokenization mechanism that enables the system to learn majority of the classes and their normalizations from the training data itself. This is further combined with minimal precoded linguistic knowledge for other classes. We publish the first results on TN for TTS in Spanish and Tamil and also demonstrate that the performance of the approach is comparable with the previous work done on English. All annotated datasets used for experimentation will be released.</abstract>
      <url hash="5bdaa941">2021.naacl-industry.10</url>
      <doi>10.18653/v1/2021.naacl-industry.10</doi>
      <bibkey>tyagi-etal-2021-proteno</bibkey>
      <video href="2021.naacl-industry.10.mp4"/>
      <pwccode url="https://github.com/amazon-research/proteno" additional="false">amazon-research/proteno</pwccode>
    </paper>
    <paper id="11">
      <title>Addressing the Vulnerability of <fixed-case>NMT</fixed-case> in Input Perturbations</title>
      <author><first>Weiwen</first><last>Xu</last></author>
      <author><first>Ai Ti</first><last>Aw</last></author>
      <author><first>Yang</first><last>Ding</last></author>
      <author><first>Kui</first><last>Wu</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <pages>80–88</pages>
      <abstract>Neural Machine Translation (NMT) has achieved significant breakthrough in performance but is known to suffer vulnerability to input perturbations. As real input noise is difficult to predict during training, robustness is a big issue for system deployment. In this paper, we improve the robustness of NMT models by reducing the effect of noisy words through a Context-Enhanced Reconstruction (CER) approach. CER trains the model to resist noise in two steps: (1) perturbation step that breaks the naturalness of input sequence with made-up words; (2) reconstruction step that defends the noise propagation by generating better and more robust contextual representation. Experimental results on Chinese-English (ZH-EN) and French-English (FR-EN) translation tasks demonstrate robustness improvement on both news and social media text. Further fine-tuning experiments on social media text show our approach can converge at a higher position and provide a better adaptation.</abstract>
      <url hash="7ea7a894">2021.naacl-industry.11</url>
      <doi>10.18653/v1/2021.naacl-industry.11</doi>
      <bibkey>xu-etal-2021-addressing</bibkey>
      <video href="2021.naacl-industry.11.mp4"/>
      <pwccode url="https://github.com/wwxu21/CER-MT" additional="false">wwxu21/CER-MT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="12">
      <title>Cross-lingual Supervision Improves Unsupervised Neural Machine Translation</title>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Hongxiao</first><last>Bai</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>89–96</pages>
      <abstract>We propose to improve unsupervised neural machine translation with cross-lingual supervision (), which utilizes supervision signals from high resource language pairs to improve the translation of zero-source languages. Specifically, for training En-Ro system without parallel corpus, we can leverage the corpus from En-Fr and En-De to collectively train the translation from one language into many languages under one model. % is based on multilingual models which require no changes to the standard unsupervised NMT. Simple and effective, significantly improves the translation quality with a big margin in the benchmark unsupervised translation tasks, and even achieves comparable performance to supervised NMT. In particular, on WMT’14 -tasks achieves 37.6 and 35.18 BLEU score, which is very close to the large scale supervised setting and on WMT’16 -tasks achieves 35.09 BLEU score which is even better than the supervised Transformer baseline.</abstract>
      <url hash="83973cc2">2021.naacl-industry.12</url>
      <doi>10.18653/v1/2021.naacl-industry.12</doi>
      <bibkey>wang-etal-2021-cross</bibkey>
      <video href="2021.naacl-industry.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Should we find another model?: Improving Neural Machine Translation Performance with <fixed-case>ONE</fixed-case>-Piece Tokenization Method without Model Modification</title>
      <author><first>Chanjun</first><last>Park</last></author>
      <author><first>Sugyeong</first><last>Eo</last></author>
      <author><first>Hyeonseok</first><last>Moon</last></author>
      <author><first>Heuiseok</first><last>Lim</last></author>
      <pages>97–104</pages>
      <abstract>Most of the recent Natural Language Processing(NLP) studies are based on the Pretrain-Finetuning Approach (PFA), but in small and medium-sized enterprises or companies with insufficient hardware there are many limitations to servicing NLP application software using such technology due to slow speed and insufficient memory. The latest PFA technologies require large amounts of data, especially for low-resource languages, making them much more difficult to work with. We propose a new tokenization method, ONE-Piece, to address this limitation that combines the morphology-considered subword tokenization method and the vocabulary method used after probing for an existing method that has not been carefully considered before. Our proposed method can also be used without modifying the model structure. We experiment by applying ONE-Piece to Korean, a morphologically-rich and low-resource language. We derive an optimal subword tokenization result for Korean-English machine translation by conducting a case study that combines the subword tokenization method, morphological segmentation, and vocabulary method. Through comparative experiments with all the tokenization methods currently used in NLP research, ONE-Piece achieves performance comparable to the current Korean-English machine translation state-of-the-art model.</abstract>
      <url hash="0cd8fd18">2021.naacl-industry.13</url>
      <doi>10.18653/v1/2021.naacl-industry.13</doi>
      <bibkey>park-etal-2021-find</bibkey>
      <video href="2021.naacl-industry.13.mp4"/>
    </paper>
    <paper id="14">
      <title>Autocorrect in the Process of Translation — Multi-task Learning Improves Dialogue Machine Translation</title>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Chengqi</first><last>Zhao</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>105–112</pages>
      <abstract>Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing neural machine translation delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (), punctuation dropping (), and typos (). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize context to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed method improves translation quality by 3.2 BLEU over the baselines. It also elevates the recovery rate of omitted pronouns from 26.09% to 47.16%. We will publish the code and dataset publicly at https://xxx.xx.</abstract>
      <url hash="d3015c87">2021.naacl-industry.14</url>
      <doi>10.18653/v1/2021.naacl-industry.14</doi>
      <bibkey>wang-etal-2021-autocorrect</bibkey>
      <video href="2021.naacl-industry.14.mp4"/>
    </paper>
    <paper id="15">
      <title><fixed-case>L</fixed-case>ight<fixed-case>S</fixed-case>eq: A High Performance Inference Library for Transformers</title>
      <author><first>Xiaohui</first><last>Wang</last></author>
      <author><first>Ying</first><last>Xiong</last></author>
      <author><first>Yang</first><last>Wei</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>113–120</pages>
      <abstract>Transformer and its variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial applications. In this paper, we propose , a highly efficient inference library for models in the Transformer family. includes a series of GPU optimization techniques to both streamline the computation of Transformer layers and reduce memory footprint. supports models trained using PyTorch and Tensorflow. Experimental results on standard machine translation benchmarks show that achieves up to 14x speedup compared with TensorFlow and 1.4x speedup compared with , a concurrent CUDA implementation. The code will be released publicly after the review.</abstract>
      <url hash="288e7385">2021.naacl-industry.15</url>
      <doi>10.18653/v1/2021.naacl-industry.15</doi>
      <bibkey>wang-etal-2021-lightseq</bibkey>
      <video href="2021.naacl-industry.15.mp4"/>
      <pwccode url="https://github.com/bytedance/lightseq" additional="false">bytedance/lightseq</pwccode>
    </paper>
    <paper id="16">
      <title>Practical Transformer-based Multilingual Text Classification</title>
      <author><first>Cindy</first><last>Wang</last></author>
      <author><first>Michele</first><last>Banko</last></author>
      <pages>121–129</pages>
      <abstract>Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.</abstract>
      <url hash="f093073c">2021.naacl-industry.16</url>
      <doi>10.18653/v1/2021.naacl-industry.16</doi>
      <bibkey>wang-banko-2021-practical</bibkey>
      <video href="2021.naacl-industry.16.mp4"/>
      <pwccode url="https://github.com/sentropytechnologies/hateval2019-relabeled" additional="false">sentropytechnologies/hateval2019-relabeled</pwccode>
    </paper>
    <paper id="17">
      <title>An Emotional Comfort Framework for Improving User Satisfaction in <fixed-case>E</fixed-case>-Commerce Customer Service Chatbots</title>
      <author><first>Shuangyong</first><last>Song</last></author>
      <author><first>Chao</first><last>Wang</last></author>
      <author><first>Haiqing</first><last>Chen</last></author>
      <author><first>Huan</first><last>Chen</last></author>
      <pages>130–137</pages>
      <abstract>E-commerce has grown substantially over the last several years, and chatbots for intelligent customer service are concurrently drawing attention. We presented AliMe Assist, a Chinese intelligent assistant designed for creating an innovative online shopping experience in E-commerce. Based on question answering (QA), AliMe Assist offers assistance service, customer service, and chatting service. According to the survey of user studies and the real online testing, emotional comfort of customers’ negative emotions, which make up more than 5% of whole number of customer visits on AliMe, is a key point for providing considerate service. In this paper, we propose a framework to obtain proper answer to customers’ emotional questions. The framework takes emotion classification model as a core, and final answer selection is based on topic classification and text matching. Our experiments on real online systems show that the framework is very promising.</abstract>
      <url hash="7b047c79">2021.naacl-industry.17</url>
      <doi>10.18653/v1/2021.naacl-industry.17</doi>
      <bibkey>song-etal-2021-emotional</bibkey>
      <video href="2021.naacl-industry.17.mp4"/>
    </paper>
    <paper id="18">
      <title>Language Scaling for Universal Suggested Replies Model</title>
      <author><first>Qianlan</first><last>Ying</last></author>
      <author><first>Payal</first><last>Bajaj</last></author>
      <author><first>Budhaditya</first><last>Deb</last></author>
      <author><first>Yu</first><last>Yang</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Bojia</first><last>Lin</last></author>
      <author><first>Milad</first><last>Shokouhi</last></author>
      <author><first>Xia</first><last>Song</last></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>138–145</pages>
      <abstract>We consider the problem of scaling automated suggested replies for a commercial email application to multiple languages. Faced with increased compute requirements and low language resources for language expansion, we build a single universal model for improving the quality and reducing run-time costs of our production system. However, restricted data movement across regional centers prevents joint training across languages. To this end, we propose a multi-lingual multi-task continual learning framework, with auxiliary tasks and language adapters to train universal language representation across regions. The experimental results show positive cross-lingual transfer across languages while reducing catastrophic forgetting across regions. Our online results on real user traffic show significant CTR and Char-saved gain as well as 65% training cost reduction compared with per-language models. As a consequence, we have scaled the feature in multiple languages including low-resource markets.</abstract>
      <url hash="95947fa3">2021.naacl-industry.18</url>
      <doi>10.18653/v1/2021.naacl-industry.18</doi>
      <bibkey>ying-etal-2021-language</bibkey>
      <video href="2021.naacl-industry.18.mp4"/>
    </paper>
    <paper id="19">
      <title>Graph-based Multilingual Product Retrieval in <fixed-case>E</fixed-case>-Commerce Search</title>
      <author><first>Hanqing</first><last>Lu</last></author>
      <author><first>Youna</first><last>Hu</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Tony</first><last>Wu</last></author>
      <author><first>Yiwei</first><last>Song</last></author>
      <author><first>Bing</first><last>Yin</last></author>
      <pages>146–153</pages>
      <abstract>Nowadays, with many e-commerce platforms conducting global business, e-commerce search systems are required to handle product retrieval under multilingual scenarios. Moreover, comparing with maintaining per-country specific e-commerce search systems, having an universal system across countries can further reduce the operational and computational costs, and facilitate business expansion to new countries. In this paper, we introduce an universal end-to-end multilingual retrieval system, and discuss our learnings and technical details when training and deploying the system to serve billion-scale product retrieval for e-commerce search. In particular, we propose a multilingual graph attention based retrieval network by leveraging recent advances in transformer-based multilingual language models and graph neural network architectures to capture the interactions between search queries and items in e-commerce search. Offline experiments on five countries data show that our algorithm outperforms the state-of-the-art baselines by 35% recall and 25% mAP on average. Moreover, the proposed model shows significant increase of conversion/revenue in online A/B experiments and has been deployed in production for multiple countries.</abstract>
      <url hash="99d899bd">2021.naacl-industry.19</url>
      <doi>10.18653/v1/2021.naacl-industry.19</doi>
      <bibkey>lu-etal-2021-graph</bibkey>
      <video href="2021.naacl-industry.19.mp4"/>
    </paper>
    <paper id="20">
      <title><fixed-case>Q</fixed-case>uery2<fixed-case>P</fixed-case>rod2<fixed-case>V</fixed-case>ec: Grounded Word Embeddings for e<fixed-case>C</fixed-case>ommerce</title>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Jacopo</first><last>Tagliabue</last></author>
      <author><first>Bingqing</first><last>Yu</last></author>
      <pages>154–162</pages>
      <abstract>We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings: in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation: our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of data efficiency for product search outside of retail giants, and highlight how Query2Prod2Vec fits with practical constraints faced by most practitioners.</abstract>
      <url hash="c19f4577">2021.naacl-industry.20</url>
      <award>Best Industry Paper</award>
      <doi>10.18653/v1/2021.naacl-industry.20</doi>
      <bibkey>bianchi-etal-2021-query2prod2vec</bibkey>
      <video href="2021.naacl-industry.20.mp4"/>
      <pwccode url="https://github.com/coveooss/ecommerce-query-embeddings" additional="false">coveooss/ecommerce-query-embeddings</pwccode>
    </paper>
    <paper id="21">
      <title>An Architecture for Accelerated Large-Scale Inference of Transformer-Based Language Models</title>
      <author><first>Amir</first><last>Ganiev</last></author>
      <author><first>Colton</first><last>Chapin</last></author>
      <author><first>Anderson</first><last>De Andrade</last></author>
      <author><first>Chen</first><last>Liu</last></author>
      <pages>163–169</pages>
      <abstract>This work demonstrates the development process of a machine learning architecture for inference that can scale to a large volume of requests. We used a BERT model that was fine-tuned for emotion analysis, returning a probability distribution of emotions given a paragraph. The model was deployed as a gRPC service on Kubernetes. Apache Spark was used to perform inference in batches by calling the service. We encountered some performance and concurrency challenges and created solutions to achieve faster running time. Starting with 200 successful inference requests per minute, we were able to achieve as high as 18 thousand successful requests per minute with the same batch job resource allocation. As a result, we successfully stored emotion probabilities for 95 million paragraphs within 96 hours.</abstract>
      <url hash="c32ff7b4">2021.naacl-industry.21</url>
      <doi>10.18653/v1/2021.naacl-industry.21</doi>
      <bibkey>ganiev-etal-2021-architecture</bibkey>
      <video href="2021.naacl-industry.21.mp4"/>
    </paper>
    <paper id="22">
      <title>When and Why a Model Fails? A Human-in-the-loop Error Detection Framework for Sentiment Analysis</title>
      <author><first>Zhe</first><last>Liu</last></author>
      <author><first>Yufan</first><last>Guo</last></author>
      <author><first>Jalal</first><last>Mahmud</last></author>
      <pages>170–177</pages>
      <abstract>Although deep neural networks have been widely employed and proven effective in sentiment analysis tasks, it remains challenging for model developers to assess their models for erroneous predictions that might exist prior to deployment. Once deployed, emergent errors can be hard to identify in prediction run-time and impossible to trace back to their sources. To address such gaps, in this paper we propose an error detection framework for sentiment analysis based on explainable features. We perform global-level feature validation with human-in-the-loop assessment, followed by an integration of global and local-level feature contribution analysis. Experimental results show that, given limited human-in-the-loop intervention, our method is able to identify erroneous model predictions on unseen data with high precision.</abstract>
      <url hash="fb965076">2021.naacl-industry.22</url>
      <doi>10.18653/v1/2021.naacl-industry.22</doi>
      <bibkey>liu-etal-2021-model</bibkey>
      <video href="2021.naacl-industry.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Technical Question Answering across Tasks and Domains</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Lingfei</first><last>Wu</last></author>
      <author><first>Yu</first><last>Deng</last></author>
      <author><first>Qingkai</first><last>Zeng</last></author>
      <author><first>Ruchi</first><last>Mahindru</last></author>
      <author><first>Sinem</first><last>Guven</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>178–186</pages>
      <abstract>Building automatic technical support system is an important yet challenge task. Conceptually, to answer a user question on a technical forum, a human expert has to first retrieve relevant documents, and then read them carefully to identify the answer snippet. Despite huge success the researchers have achieved in coping with general domain question answering (QA), much less attentions have been paid for investigating technical QA. Specifically, existing methods suffer from several unique challenges (i) the question and answer rarely overlaps substantially and (ii) very limited data size. In this paper, we propose a novel framework of deep transfer learning to effectively address technical QA across tasks and domains. To this end, we present an adjustable joint learning approach for document retrieval and reading comprehension tasks. Our experiments on the TechQA demonstrates superior performance compared with state-of-the-art methods.</abstract>
      <url hash="aa975dcc">2021.naacl-industry.23</url>
      <doi>10.18653/v1/2021.naacl-industry.23</doi>
      <bibkey>yu-etal-2021-technical</bibkey>
      <video href="2021.naacl-industry.23.mp4"/>
      <pwccode url="https://github.com/wyu97/TransTD" additional="false">wyu97/TransTD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/techqa">TechQA</pwcdataset>
    </paper>
    <paper id="24">
      <title>Cost-effective Deployment of <fixed-case>BERT</fixed-case> Models in Serverless Environment</title>
      <author><first>Marek</first><last>Suppa</last></author>
      <author><first>Katarína</first><last>Benešová</last></author>
      <author><first>Andrej</first><last>Švec</last></author>
      <pages>187–195</pages>
      <abstract>In this study, we demonstrate the viability of deploying BERT-style models to AWS Lambda in a production environment. Since the freely available pre-trained models are too large to be deployed in this environment, we utilize knowledge distillation and fine-tune the models on proprietary datasets for two real-world tasks: sentiment analysis and semantic textual similarity. As a result, we obtain models that are tuned for a specific domain and deployable in the serverless environment. The subsequent performance analysis shows that this solution does not only report latency levels acceptable for production use but that it is also a cost-effective alternative to small-to-medium size deployments of BERT models, all without any infrastructure overhead.</abstract>
      <url hash="0ceebe9e">2021.naacl-industry.24</url>
      <doi>10.18653/v1/2021.naacl-industry.24</doi>
      <bibkey>suppa-etal-2021-cost</bibkey>
      <video href="2021.naacl-industry.24.mp4"/>
    </paper>
    <paper id="25">
      <title>Noise Robust Named Entity Understanding for Voice Assistants</title>
      <author><first>Deepak</first><last>Muralidharan</last></author>
      <author><first>Joel Ruben Antony</first><last>Moniz</last></author>
      <author><first>Sida</first><last>Gao</last></author>
      <author><first>Xiao</first><last>Yang</last></author>
      <author><first>Justine</first><last>Kao</last></author>
      <author><first>Stephen</first><last>Pulman</last></author>
      <author><first>Atish</first><last>Kothari</last></author>
      <author><first>Ray</first><last>Shen</last></author>
      <author><first>Yinying</first><last>Pan</last></author>
      <author><first>Vivek</first><last>Kaul</last></author>
      <author><first>Mubarak</first><last>Seyed Ibrahim</last></author>
      <author><first>Gang</first><last>Xiang</last></author>
      <author><first>Nan</first><last>Dun</last></author>
      <author><first>Yidan</first><last>Zhou</last></author>
      <author><first>Andy</first><last>O</last></author>
      <author><first>Yuan</first><last>Zhang</last></author>
      <author><first>Pooja</first><last>Chitkara</last></author>
      <author><first>Xuan</first><last>Wang</last></author>
      <author><first>Alkesh</first><last>Patel</last></author>
      <author><first>Kushal</first><last>Tayal</last></author>
      <author><first>Roger</first><last>Zheng</last></author>
      <author><first>Peter</first><last>Grasch</last></author>
      <author><first>Jason D</first><last>Williams</last></author>
      <author><first>Lin</first><last>Li</last></author>
      <pages>196–204</pages>
      <abstract>Named Entity Recognition (NER) and Entity Linking (EL) play an essential role in voice assistant interaction, but are challenging due to the special difficulties associated with spoken user queries. In this paper, we propose a novel architecture that jointly solves the NER and EL tasks by combining them in a joint reranking module. We show that our proposed framework improves NER accuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features used also lead to better accuracies in other natural language understanding tasks, such as domain classification and semantic parsing.</abstract>
      <url hash="eab40b1b">2021.naacl-industry.25</url>
      <doi>10.18653/v1/2021.naacl-industry.25</doi>
      <bibkey>muralidharan-etal-2021-noise</bibkey>
      <video href="2021.naacl-industry.25.mp4"/>
    </paper>
    <paper id="26">
      <title>Goodwill Hunting: Analyzing and Repurposing Off-the-Shelf Named Entity Linking Systems</title>
      <author><first>Karan</first><last>Goel</last></author>
      <author><first>Laurel</first><last>Orr</last></author>
      <author><first>Nazneen Fatema</first><last>Rajani</last></author>
      <author><first>Jesse</first><last>Vig</last></author>
      <author><first>Christopher</first><last>Ré</last></author>
      <pages>205–213</pages>
      <abstract>Named entity linking (NEL) or mapping “strings” to “things” in a knowledge base is a fundamental preprocessing step in systems that require knowledge of entities such as information extraction and question answering. In this work, we lay out and investigate two challenges faced by individuals or organizations building NEL systems. Can they directly use an off-the-shelf system? If not, how easily can such a system be repurposed for their use case? First, we conduct a study of off-the-shelf commercial and academic NEL systems. We find that most systems struggle to link rare entities, with commercial solutions lagging their academic counterparts by 10%+. Second, for a use case where the NEL model is used in a sports question-answering (QA) system, we investigate how to close the loop in our analysis by repurposing the best off-the-shelf model (Bootleg) to correct sport-related errors. We show how tailoring a simple technique for patching models using weak labeling can provide a 25% absolute improvement in accuracy of sport-related errors.</abstract>
      <url hash="ef61c3b3">2021.naacl-industry.26</url>
      <doi>10.18653/v1/2021.naacl-industry.26</doi>
      <bibkey>goel-etal-2021-goodwill</bibkey>
      <video href="2021.naacl-industry.26.mp4"/>
    </paper>
    <paper id="27">
      <title>Intent Features for Rich Natural Language Understanding</title>
      <author><first>Brian</first><last>Lester</last></author>
      <author><first>Sagnik</first><last>Ray Choudhury</last></author>
      <author><first>Rashmi</first><last>Prasad</last></author>
      <author><first>Srinivas</first><last>Bangalore</last></author>
      <pages>214–221</pages>
      <abstract>Complex natural language understanding modules in dialog systems have a richer understanding of user utterances, and thus are critical in providing a better user experience. However, these models are often created from scratch, for specific clients and use cases and require the annotation of large datasets. This encourages the sharing of annotated data across multiple clients. To facilitate this we introduce the idea of <i>intent features</i>: domain and topic agnostic properties of intents that can be learnt from the syntactic cues only, and hence can be shared. We introduce a new neural network architecture, the Global-Local model, that shows significant improvement over strong baselines for identifying these features in a deployed, multi-intent natural language understanding module, and more generally in a classification setting where a part of an utterance has to be classified utilizing the whole context.</abstract>
      <url hash="3e7d08e7">2021.naacl-industry.27</url>
      <doi>10.18653/v1/2021.naacl-industry.27</doi>
      <bibkey>lester-etal-2021-intent</bibkey>
      <video href="2021.naacl-industry.27.mp4"/>
      <pwccode url="https://github.com/blester125/global-local-model" additional="false">blester125/global-local-model</pwccode>
    </paper>
    <paper id="28">
      <title>Development of an Enterprise-Grade Contract Understanding System</title>
      <author><first>Arvind</first><last>Agarwal</last></author>
      <author><first>Laura</first><last>Chiticariu</last></author>
      <author><first>Poornima</first><last>Chozhiyath Raman</last></author>
      <author><first>Marina</first><last>Danilevsky</last></author>
      <author><first>Diman</first><last>Ghazi</last></author>
      <author><first>Ankush</first><last>Gupta</last></author>
      <author><first>Shanmukha</first><last>Guttula</last></author>
      <author><first>Yannis</first><last>Katsis</last></author>
      <author><first>Rajasekar</first><last>Krishnamurthy</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Shubham</first><last>Mudgal</last></author>
      <author><first>Vitobha</first><last>Munigala</last></author>
      <author><first>Nicholas</first><last>Phan</last></author>
      <author><first>Dhaval</first><last>Sonawane</last></author>
      <author><first>Sneha</first><last>Srinivasan</last></author>
      <author><first>Sudarshan R.</first><last>Thitte</last></author>
      <author><first>Mitesh</first><last>Vasa</last></author>
      <author><first>Ramiya</first><last>Venkatachalam</last></author>
      <author><first>Vinitha</first><last>Yaski</last></author>
      <author><first>Huaiyu</first><last>Zhu</last></author>
      <pages>222–229</pages>
      <abstract>Contracts are arguably the most important type of business documents. Despite their significance in business, legal contract review largely remains an arduous, expensive and manual process. In this paper, we describe TECUS: a commercial system designed and deployed for contract understanding and used by a wide range of enterprise users for the past few years. We reflect on the challenges and design decisions when building TECUS. We also summarize the data science life cycle of TECUS and share lessons learned.</abstract>
      <url hash="2c87de63">2021.naacl-industry.28</url>
      <doi>10.18653/v1/2021.naacl-industry.28</doi>
      <bibkey>agarwal-etal-2021-development</bibkey>
      <video href="2021.naacl-industry.28.mp4"/>
    </paper>
    <paper id="29">
      <title>Discovering Better Model Architectures for Medical Query Understanding</title>
      <author><first>Wei</first><last>Zhu</last></author>
      <author><first>Yuan</first><last>Ni</last></author>
      <author><first>Xiaoling</first><last>Wang</last></author>
      <author><first>Guotong</first><last>Xie</last></author>
      <pages>230–237</pages>
      <abstract>In developing an online question-answering system for the medical domains, natural language inference (NLI) models play a central role in question matching and intention detection. However, which models are best for our datasets? Manually selecting or tuning a model is time-consuming. Thus we experiment with automatically optimizing the model architectures on the task at hand via neural architecture search (NAS). First, we formulate a novel architecture search space based on the previous NAS literature, supporting cross-sentence attention (cross-attn) modeling. Second, we propose to modify the ENAS method to accelerate and stabilize the search results. We conduct extensive experiments on our two medical NLI tasks. Results show that our system can easily outperform the classical baseline models. We compare different NAS methods and demonstrate our approach provides the best results.</abstract>
      <url hash="6e29fd7b">2021.naacl-industry.29</url>
      <doi>10.18653/v1/2021.naacl-industry.29</doi>
      <bibkey>zhu-etal-2021-discovering</bibkey>
      <video href="2021.naacl-industry.29.mp4"/>
    </paper>
    <paper id="30">
      <title><fixed-case>O</fixed-case>od<fixed-case>GAN</fixed-case>: Generative Adversarial Network for Out-of-Domain Data Generation</title>
      <author><first>Petr</first><last>Marek</last></author>
      <author><first>Vishal Ishwar</first><last>Naik</last></author>
      <author><first>Anuj</first><last>Goyal</last></author>
      <author><first>Vincent</first><last>Auvray</last></author>
      <pages>238–245</pages>
      <abstract>Detecting an Out-of-Domain (OOD) utterance is crucial for a robust dialog system. Most dialog systems are trained on a pool of annotated OOD data to achieve this goal. However, collecting the annotated OOD data for a given domain is an expensive process. To mitigate this issue, previous works have proposed generative adversarial networks (GAN) based models to generate OOD data for a given domain automatically. However, these proposed models do not work directly with the text. They work with the text’s latent space instead, enforcing these models to include components responsible for encoding text into latent space and decoding it back, such as auto-encoder. These components increase the model complexity, making it difficult to train. We propose OodGAN, a sequential generative adversarial network (SeqGAN) based model for OOD data generation. Our proposed model works directly on the text and hence eliminates the need to include an auto-encoder. OOD data generated using OodGAN model outperforms state-of-the-art in OOD detection metrics for ROSTD (67% relative improvement in FPR 0.95) and OSQ datasets (28% relative improvement in FPR 0.95)</abstract>
      <url hash="3a5163ba">2021.naacl-industry.30</url>
      <doi>10.18653/v1/2021.naacl-industry.30</doi>
      <bibkey>marek-etal-2021-oodgan</bibkey>
      <video href="2021.naacl-industry.30.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/rostd">ROSTD</pwcdataset>
    </paper>
    <paper id="31">
      <title>Coherent and Concise Radiology Report Generation via Context Specific Image Representations and Orthogonal Sentence States</title>
      <author><first>Litton</first><last>J Kurisinkel</last></author>
      <author><first>Ai Ti</first><last>Aw</last></author>
      <author><first>Nancy F</first><last>Chen</last></author>
      <pages>246–254</pages>
      <abstract>Neural models for text generation are often designed in an end-to-end fashion, typically with zero control over intermediate computations, limiting their practical usability in downstream applications. In this work, we incorporate explicit means into neural models to ensure topical continuity, informativeness and content diversity of generated radiology reports. For the purpose we propose a method to compute image representations specific to each sentential context and eliminate redundant content by exploiting diverse sentence states. We conduct experiments to generate radiology reports from medical images of chest x-rays using MIMIC-CXR. Our model outperforms baselines by up to 18% and 29% respective in the evaluation for informativeness and content ordering respectively, relative on objective metrics and 16% on human evaluation.</abstract>
      <url hash="7d232e68">2021.naacl-industry.31</url>
      <doi>10.18653/v1/2021.naacl-industry.31</doi>
      <bibkey>j-kurisinkel-etal-2021-coherent</bibkey>
      <video href="2021.naacl-industry.31.mp4"/>
    </paper>
    <paper id="32">
      <title>An Empirical Study of Generating Texts for Search Engine Advertising</title>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Peinan</first><last>Zhang</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>255–262</pages>
      <abstract>Although there are many studies on neural language generation (NLG), few trials are put into the real world, especially in the advertising domain. Generating ads with NLG models can help copywriters in their creation. However, few studies have adequately evaluated the effect of generated ads with actual serving included because it requires a large amount of training data and a particular environment. In this paper, we demonstrate a practical use case of generating ad-text with an NLG model. Specially, we show how to improve the ads’ impact, deploy models to a product, and evaluate the generated ads.</abstract>
      <url hash="b89acc6f">2021.naacl-industry.32</url>
      <doi>10.18653/v1/2021.naacl-industry.32</doi>
      <bibkey>kamigaito-etal-2021-empirical</bibkey>
      <video href="2021.naacl-industry.32.mp4"/>
    </paper>
    <paper id="33">
      <title>Ad Headline Generation using Self-Critical Masked Language Model</title>
      <author><first>Yashal Shakti</first><last>Kanungo</last></author>
      <author><first>Sumit</first><last>Negi</last></author>
      <author><first>Aruna</first><last>Rajan</last></author>
      <pages>263–271</pages>
      <abstract>For any E-commerce website it is a nontrivial problem to build enduring advertisements that attract shoppers. It is hard to pass the creative quality bar of the website, especially at a large scale. We thus propose a programmatic solution to generate product advertising headlines using retail content. We propose a state of the art application of Reinforcement Learning (RL) Policy gradient methods on Transformer (Vaswani et al., 2017) based Masked Language Models (Devlin et al., 2019). Our method creates the advertising headline by jointly conditioning on multiple products that a seller wishes to advertise. We demonstrate that our method outperforms existing Transformer and LSTM + RL methods in overlap metrics and quality audits. We also show that our model generated headlines outperform human submitted headlines in terms of both grammar and creative quality as determined by audits.</abstract>
      <url hash="6cc38f5b">2021.naacl-industry.33</url>
      <doi>10.18653/v1/2021.naacl-industry.33</doi>
      <bibkey>kanungo-etal-2021-ad</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>LATEX</fixed-case>-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes</title>
      <author><first>Kartik</first><last>Mehta</last></author>
      <author><first>Ioana</first><last>Oprea</last></author>
      <author><first>Nikhil</first><last>Rasiwasia</last></author>
      <pages>272–279</pages>
      <abstract>In this paper, we present LATEX-Numeric - a high-precision fully-automated scalable framework for extracting E-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of active learning. We rely on distant supervision for training data generation, removing dependency on manual labels. One issue with distant supervision is that it leads to incomplete training annotation due to missing attribute values while matching. We propose a multi-task learning architecture to deal with missing labels in the training data, leading to F1 improvement of 9.2% for numeric attributes over state-of-the-art single-task architecture. While multi-task architecture benefits both numeric and non-numeric attributes, we present automated techniques to further improve the numeric attributes extraction models. Numeric attributes require a list of units (or aliases) for better matching with distant supervision. We propose an automated algorithm for alias creation using unstructured text and attribute values, leading to a 20.2% F1 improvement. Extensive experiments on real world datasets for 20 numeric attributes across 5 product categories and 3 English marketplaces show that LATEX-numeric achieves a high F1-score, without any manual intervention, making it suitable for practical applications. Finally we show that the improvements are language-agnostic and LATEX-Numeric achieves 13.9% F1 improvement for 3 non-English languages.</abstract>
      <url hash="7c0cc9e4">2021.naacl-industry.34</url>
      <doi>10.18653/v1/2021.naacl-industry.34</doi>
      <bibkey>mehta-etal-2021-latex</bibkey>
      <video href="2021.naacl-industry.34.mp4"/>
    </paper>
    <paper id="35">
      <title>Training Language Models under Resource Constraints for Adversarial Advertisement Detection</title>
      <author><first>Eshwar</first><last>Shamanna Girishekar</last></author>
      <author><first>Shiv</first><last>Surya</last></author>
      <author><first>Nishant</first><last>Nikhil</last></author>
      <author><first>Dyut Kumar</first><last>Sil</last></author>
      <author><first>Sumit</first><last>Negi</last></author>
      <author><first>Aruna</first><last>Rajan</last></author>
      <pages>280–287</pages>
      <abstract>Advertising on e-commerce and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multi-lingual training can be applied effectively to fine-tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline.</abstract>
      <url hash="85c1f1e5">2021.naacl-industry.35</url>
      <doi>10.18653/v1/2021.naacl-industry.35</doi>
      <bibkey>shamanna-girishekar-etal-2021-training</bibkey>
    </paper>
    <paper id="36">
      <title>Combining Weakly Supervised <fixed-case>ML</fixed-case> Techniques for Low-Resource <fixed-case>NLU</fixed-case></title>
      <author><first>Victor</first><last>Soto</last></author>
      <author><first>Konstantine</first><last>Arkoudas</last></author>
      <pages>288–295</pages>
      <abstract>Recent advances in transfer learning have improved the performance of virtual assistants considerably. Nevertheless, creating sophisticated voice-enabled applications for new domains remains a challenge, and meager training data is often a key bottleneck. Accordingly, unsupervised learning and SSL (semi-supervised learning) techniques continue to be of vital importance. While a number of such methods have been explored previously in isolation, in this paper we investigate the synergistic use of a number of weakly supervised techniques with a view to improving NLU (Natural Language Understanding) accuracy in low-resource settings. We explore three different approaches incorporating anonymized, unlabeled and automatically transcribed user utterances into the training process, two focused on data augmentation via SSL and another one focused on unsupervised and transfer learning. We show promising results, obtaining gains that range from 4.73% to 7.65% relative improvements on semantic error rate for each individual approach. Moreover, the combination of all three methods together yields a relative improvement of 11.77% over our current baseline model. Our methods are applicable to any new domain with minimal training data, and can be deployed over time into a cycle of continual learning.</abstract>
      <url hash="304e9ad3">2021.naacl-industry.36</url>
      <doi>10.18653/v1/2021.naacl-industry.36</doi>
      <bibkey>soto-arkoudas-2021-combining</bibkey>
      <video href="2021.naacl-industry.36.mp4"/>
    </paper>
    <paper id="37">
      <title>Label-Guided Learning for Item Categorization in e-Commerce</title>
      <author><first>Lei</first><last>Chen</last></author>
      <author><first>Hirokazu</first><last>Miyake</last></author>
      <pages>296–303</pages>
      <abstract>Item categorization is an important application of text classification in e-commerce due to its impact on the online shopping experience of users. One class of text classification techniques that has gained attention recently is using the semantic information of the labels to guide the classification task. We have conducted a systematic investigation of the potential benefits of these methods on a real data set from a major e-commerce company in Japan. Furthermore, using a hyperbolic space to embed product labels that are organized in a hierarchical structure led to better performance compared to using a conventional Euclidean space embedding. These findings demonstrate how label-guided learning can improve item categorization systems in the e-commerce domain.</abstract>
      <url hash="70249ff6">2021.naacl-industry.37</url>
      <doi>10.18653/v1/2021.naacl-industry.37</doi>
      <bibkey>chen-miyake-2021-label</bibkey>
      <video href="2021.naacl-industry.37.mp4"/>
    </paper>
    <paper id="38">
      <title>Benchmarking Commercial Intent Detection Services with Practice-Driven Evaluations</title>
      <author><first>Haode</first><last>Qi</last></author>
      <author><first>Lin</first><last>Pan</last></author>
      <author><first>Atin</first><last>Sood</last></author>
      <author><first>Abhishek</first><last>Shah</last></author>
      <author><first>Ladislav</first><last>Kunc</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Saloni</first><last>Potdar</last></author>
      <pages>304–310</pages>
      <abstract>Intent detection is a key component of modern goal-oriented dialog systems that accomplish a user task by predicting the intent of users’ text input. There are three primary challenges in designing robust and accurate intent detection models. First, typical intent detection models require a large amount of labeled data to achieve high accuracy. Unfortunately, in practical scenarios it is more common to find small, unbalanced, and noisy datasets. Secondly, even with large training data, the intent detection models can see a different distribution of test data when being deployed in the real world, leading to poor accuracy. Finally, a practical intent detection model must be computationally efficient in both training and single query inference so that it can be used continuously and re-trained frequently. We benchmark intent detection methods on a variety of datasets. Our results show that Watson Assistant’s intent detection model outperforms other commercial solutions and is comparable to large pretrained language models while requiring only a fraction of computational resources and training data. Watson Assistant demonstrates a higher degree of robustness when the training and test distributions differ.</abstract>
      <url hash="b748118c">2021.naacl-industry.38</url>
      <doi>10.18653/v1/2021.naacl-industry.38</doi>
      <bibkey>qi-etal-2021-benchmarking</bibkey>
      <video href="2021.naacl-industry.38.mp4"/>
      <pwccode url="https://github.com/haodeqi/BenchmarkingIntentDetection" additional="false">haodeqi/BenchmarkingIntentDetection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hint3">HINT3</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hwu64">HWU64</pwcdataset>
    </paper>
    <paper id="39">
      <title>Industry Scale Semi-Supervised Learning for Natural Language Understanding</title>
      <author><first>Luoxin</first><last>Chen</last></author>
      <author><first>Francisco</first><last>Garcia</last></author>
      <author><first>Varun</first><last>Kumar</last></author>
      <author><first>He</first><last>Xie</last></author>
      <author><first>Jianhua</first><last>Lu</last></author>
      <pages>311–318</pages>
      <abstract>This paper presents a production Semi-Supervised Learning (SSL) pipeline based on the student-teacher framework, which leverages millions of unlabeled examples to improve Natural Language Understanding (NLU) tasks. We investigate two questions related to the use of unlabeled data in production SSL context: 1) how to select samples from a huge unlabeled data pool that are beneficial for SSL training, and 2) how does the selected data affect the performance of different state-of-the-art SSL techniques. We compare four widely used SSL techniques, Pseudo-label (PL), Knowledge Distillation (KD), Virtual Adversarial Training (VAT) and Cross-View Training (CVT) in conjunction with two data selection methods including committee-based selection and submodular optimization based selection. We further examine the benefits and drawbacks of these techniques when applied to intent classification (IC) and named entity recognition (NER) tasks, and provide guidelines specifying when each of these methods might be beneficial to improve large scale NLU systems.</abstract>
      <url hash="ce571662">2021.naacl-industry.39</url>
      <doi>10.18653/v1/2021.naacl-industry.39</doi>
      <bibkey>chen-etal-2021-industry</bibkey>
      <video href="2021.naacl-industry.39.mp4"/>
    </paper>
  </volume>
  <event id="naacl-2021">
    <colocated>
      <volume-id>2021.alvr-1</volume-id>
      <volume-id>2021.americasnlp-1</volume-id>
      <volume-id>2021.autosimtrans-1</volume-id>
      <volume-id>2021.bionlp-1</volume-id>
      <volume-id>2021.calcs-1</volume-id>
      <volume-id>2021.clpsych-1</volume-id>
      <volume-id>2021.cmcl-1</volume-id>
      <volume-id>2021.dash-1</volume-id>
      <volume-id>2021.deelio-1</volume-id>
      <volume-id>2021.maiworkshop-1</volume-id>
      <volume-id>2021.nlp4if-1</volume-id>
      <volume-id>2021.nlpmc-1</volume-id>
      <volume-id>2021.nuse-1</volume-id>
      <volume-id>2021.privatenlp-1</volume-id>
      <volume-id>2021.sdp-1</volume-id>
      <volume-id>2021.sigtyp-1</volume-id>
      <volume-id>2021.smm4h-1</volume-id>
      <volume-id>2021.socialnlp-1</volume-id>
      <volume-id>2021.teachingnlp-1</volume-id>
      <volume-id>2021.textgraphs-1</volume-id>
      <volume-id>2021.trustnlp-1</volume-id>
      <volume-id>2021.vigil-1</volume-id>
    </colocated>
  </event>
</collection>
