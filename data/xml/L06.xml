<?xml version='1.0' encoding='UTF-8'?>
<collection id="L06">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the Fifth International Conference on Language Resources and Evaluation (<fixed-case>LREC</fixed-case>’06)</booktitle>
      <editor><first>Nicoletta</first><last>Calzolari</last></editor>
      <editor><first>Khalid</first><last>Choukri</last></editor>
      <editor><first>Aldo</first><last>Gangemi</last></editor>
      <editor><first>Bente</first><last>Maegaard</last></editor>
      <editor><first>Joseph</first><last>Mariani</last></editor>
      <editor><first>Jan</first><last>Odijk</last></editor>
      <editor><first>Daniel</first><last>Tapias</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Genoa, Italy</address>
      <month>May</month>
      <year>2006</year>
    </meta>
    <frontmatter/>
    <paper id="1">
      <author><first>Nina</first><last>Grønnum</last></author>
      <title><fixed-case>D</fixed-case>an<fixed-case>PASS</fixed-case> - A <fixed-case>D</fixed-case>anish Phonetically Annotated Spontaneous Speech Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/4_pdf.pdf</url>
      <abstract>A corpus is described consisting of non-scripted monologues and dialogues, recorded by 22 speakers, comprising a total of about 70.000 words, corresponding to well over 10 hours of speech. The monologues were recorded as one-way communication with blind partner where the speaker performed three different tasks: (S)he described a network consisting of various geometrical shapes in various colours. (S)he guided the listener through four different routes in a virtual city map.(S)he instructed the listener how to build a house from its individual parts. The dialogues are replicas of the HCRC map tasks (http://www.hcrc.ed.ac.uk/maptask/). Annotation is performed in Praat. The sound files are segmented into prosodic phrases, words, and syllables, always to the nearest zero-crossing in the waveform. It is supplied, in seven separate interval tiers, with an orthographical transcription, detailed part-of-speech tags, simplified part-of-speech tags, a phonological transcription, a broad phonetic transcription, the pitch relation between each stressed and post-tonic syllable, the phrasal intonation, and an empty tier for comments.</abstract>
    </paper>
    <paper id="2">
      <author><first>Andi</first><last>Wu</last></author>
      <author><first>Kirk</first><last>Lowery</last></author>
      <title>A <fixed-case>H</fixed-case>ebrew Tree Bank Based on Cantillation Marks</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/6_pdf.pdf</url>
      <abstract>In the Masoretic text of the Hebrew Bible (HB), the cantillation marks function like a punctuation system that shows the division and subdivision of each verse, forming a tree structure which is similar to the prosodic tree in modern linguistics. However, in the Masoretic text, the structure is hidden in a complicated set of diacritic symbols and the rich information is accessible only to a few trained scholars. In order to make the structural information available to the general public and to automatic processing by the computer, we built a tree bank where the hierarchical structure of each HB verse is explicitly represented in XML format. We coded the punctuation system in a context-tree grammar which was then used by a CYK parser to automatically generate trees for the whole HB. The results show that (1) the CFG correctly encoded the annotation rules and (2) the annotation done by the Masoretes is highly consistent.</abstract>
    </paper>
    <paper id="3">
      <author><first>Stéphane</first><last>Chaudiron</last></author>
      <author><first>Joseph</first><last>Mariani</last></author>
      <title>Techno-langue: The <fixed-case>F</fixed-case>rench National Initiative for Human Language Technologies (<fixed-case>HLT</fixed-case>)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/8_pdf.pdf</url>
      <abstract>Techno-langue is the French National Program on HLT supported by the French Ministries in charge of Research, Industry and Culture. It addresses four action lines: creating basic language and software resources, organizing evaluation campaigns, participating in the standardization process and creating a Web Portal for disseminating information and surveys to a large audience. This paper presents the main results of the program and an ongoing initiative for launching a transnational program at the European level on a similar basis.</abstract>
    </paper>
    <paper id="4">
      <author><first>Manny</first><last>Rayner</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Beth Ann</first><last>Hockey</last></author>
      <author><first>Nikos</first><last>Chatzichrisafis</last></author>
      <title><fixed-case>REGULUS</fixed-case>: A Generic Multilingual Open Source Platform for Grammar-Based Speech Applications</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/9_pdf.pdf</url>
      <abstract>We present an overview of Regulus, an Open Source platform that supports corpus-based derivation of efficient domain-specific speech recognisers from general linguistically motivated unification grammars. We list available Open Source resources, which include compilers, resource grammars for various languages, documentation and a development environment. The greater part of the paper presents a series of experiments carried out using a medium-vocabulary medical speech translation application and a corpus of 801 recorded domain utterances, designed to investigate the impact on speech understanding performance of vocabulary size, grammatical coverage, presence or absence of various linguistic features, degree of generality of thegrammar and use or otherwise of probabilistic weighting in the CFGlanguage model. In terms of task accuracy, the most significant factors were the use of probabilistic weighting, the degree of generality of the grammar and the inclusion of features which model sortal restrictions.</abstract>
    </paper>
    <paper id="5">
      <author><first>Anders</first><last>Berglund</last></author>
      <author><first>Richard</first><last>Johansson</last></author>
      <author><first>Pierre</first><last>Nugues</last></author>
      <title>Extraction of Temporal Information from Texts in <fixed-case>S</fixed-case>wedish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/10_pdf.pdf</url>
      <abstract>This paper describes the implementation and evaluation of a generic component to extract temporal information from texts in Swedish. It proceeds in two steps. The first step extracts time expressions and events, and generates a feature vector for each element it identifies. Using the vectors, the second step determines the temporal relations, possibly none, between the extracted events and orders them in time. We used a machine learning approach to find the relations between events. To run the learning algorithm, we collected a corpus of road accident reports from newspapers websites that we manually annotated. It enabled us to train decision trees and to evaluate the performance of the algorithm.</abstract>
    </paper>
    <paper id="6">
      <author><first>Jaroslava</first><last>Hlaváčová</last></author>
      <title>New Approach to Frequency Dictionaries - <fixed-case>C</fixed-case>zech Example</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/11_pdf.pdf</url>
      <abstract>On the example of the recent edition of the Frequency Dictionary of Czech wedescribe and explain some new general principles that should be followed forgetting better results for practical uses of frequency dictionaries. It ismainly adopting average reduced frequency instead of absolute frequency forordering items. The formula for calculation of the average reduced frequencyis presented in the contribution together with a brief explanation, including examples clarifying the difference between the measures. Then, the Frequency Dictionary of Czech and its parts are described.</abstract>
    </paper>
    <paper id="7">
      <author><first>Flora Ramírez</first><last>Bustamante</last></author>
      <author><first>Alfredo</first><last>Arnaiz</last></author>
      <author><first>Mar</first><last>Ginés</last></author>
      <title>A Spell Checker for a World Language: The New <fixed-case>M</fixed-case>icrosoft’s <fixed-case>S</fixed-case>panish Spell Checker</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/12_pdf.pdf</url>
      <abstract>This paper reports work carried out to develop a speller for Spanish at Microsoft Corporation, discusses the technique for isolated-word error correction used by the speller, provides general descriptions of the error data collection and error typology, and surveys a variety of linguistic considerations relevant when dealing with a world language spread over several countries and exposed to different language influences. We show that even though it has been claimed that the state of the art for practical applications based on isolated word error correction does not offer always a sensible set of ranked candidates for the misspelling, the introduction of a finer-grained categorization of errors and the use of their relative frequency has had a positive impact in the speller application developed for Spanish (the corresponding evaluation data is presented).</abstract>
    </paper>
    <paper id="8">
      <author><first>Elliott</first><last>Macklovitch</last></author>
      <title><fixed-case>T</fixed-case>rans<fixed-case>T</fixed-case>ype2 : The Last Word</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/14_pdf.pdf</url>
      <abstract>This paper presents the results of the usability evaluations that were conducted within TransType2, an international R&amp;D project the goal of which was to develop a novel approach to interactive machine translation. We briefly sketch the TransType system and then describe the methodology that we elaborated for the five rounds of user trials that were held on the premises of two translation agencies over the last eighteen months of the project. We provide the productivity results posted by the six translators who tested the system and we also discuss some of the non-quantitative factors which influenced the users reaction to TransType.</abstract>
    </paper>
    <paper id="9">
      <author><first>Ibon</first><last>Saratxaga</last></author>
      <author><first>Eva</first><last>Navas</last></author>
      <author><first>Inmaculada</first><last>Hernáez</last></author>
      <author><first>Iker</first><last>Aholab</last></author>
      <title>Designing and Recording an Emotional Speech Database for Corpus Based Synthesis in <fixed-case>B</fixed-case>asque</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/19_pdf.pdf</url>
      <abstract>This paper describes an emotional speech database recorded for standard Basque. The database has been designed with the twofold purpose of being used for corpus based synthesis, and also of allowing the study of prosodic models for the emotions. The database is thus large, to get good corpus based synthesis quality and contains the same texts recorded in the six basic emotions plus the neutral style. The recordings were carried out by two professional dubbing actors, a man and a woman. The paper explains the whole creation process, beginning with the design stage, following with the corpus creation and the recording phases, and finishing with some learned lessons and hints.</abstract>
    </paper>
    <paper id="10">
      <author><first>Dimitrios</first><last>Kokkinakis</last></author>
      <title>Collection, Encoding and Linguistic Processing of a <fixed-case>S</fixed-case>wedish Medical Corpus - The <fixed-case>MEDLEX</fixed-case> Experience</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/23_pdf.pdf</url>
      <abstract>Corpora annotated with structural and linguistic characteristics play a major role in nearly every area of language processing. During recent years a number of corpora and large data sets became known and available to research even in specialized fields such as medicine, but still however, targeted predominantly for the English language. This paper provides a description of the collection, encoding and linguistic processing of an ever growing Swedish medical corpus, the MEDLEX Corpus. MEDLEX consists of a variety of text-documents related to various medical text genres. The MEDLEX Corpus has been structurally annotated using the Corpus Encoding Standard for XML (XCES), lemmatized and automatically annotated with part-of-speech and semantic information (extended named entities and the Medical Subject Headings, MeSH, terminology). The results from the processing stages (part-of-speech, entities and terminology) have been merged into a single representation format and syntactically analysed using a cascaded finite state parser. Finally, the parsers results are converted into a tree structure that follows the TIGER-XML coding scheme, resulting a suitable for further exploration and fairly large Treebank of Swedish medical texts.</abstract>
    </paper>
    <paper id="11">
      <author><last>Noguchi</last><first>Masaki</first></author>
      <author><last>Ichikawa</last><first>Hiroshi</first></author>
      <author><last>Hashimoto</last><first>Taiichi</first></author>
      <author><last>Tokunaga</last><first>Takenobu</first></author>
      <title>A new approach to syntactic annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/29_pdf.pdf</url>
      <abstract>Many systems have been developed for creating syntactically annotated corpora. However, they mainly focus on interface usability and hardly pay attention toknowledge sharing among annotators in the task. In order to incorporate the functionality of knowledge sharing, we emphasized the importance of normalizingthe annotation process. As a first step toward knowledge sharing, this paper proposes a method of system initiative annotation in which the system suggests annotators the order of ambiguities to solve. To be more concrete, the system forces annotators to solve ambiguity of constituent structure in a top-down and depth-first manner, and then to solve ambiguity of grammatical category in a bottom-up and breadth-first manner. We implemented the system on top of eBonsai, our annotation tool, and conducted experiments to compare eBonsai and the proposed system in terms of annotation accuracy and efficiency. We found that at least for novice annotators, the proposed system is more efficient while keeping annotation accuracy comparable with eBonsai.</abstract>
    </paper>
    <paper id="12">
      <author><first>Tien-Ping</first><last>Tan</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <title>A <fixed-case>F</fixed-case>rench Non-Native Corpus for Automatic Speech Recognition</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/33_pdf.pdf</url>
      <abstract>Automatic speech recognition (ASR) technology has achieved a level of maturity, where it is already practical to be used by novice users. However, most non-native speakers are still not comfortable with services including ASR systems, because of the accuracy on non-native speakers. This paper describes our approach in constructing a non-native corpus particularly in French for testing and adapting non-native speaker for automatic speech recognition. Finally, we also propose in this paper a method for detecting pronunciation variants and possible pronunciation mistakes by non-native speakers.</abstract>
    </paper>
    <paper id="13">
      <author><first>Andrea</first><last>Sansò</last></author>
      <title>Documenting variation across <fixed-case>E</fixed-case>urope and the Mediterranean: the <fixed-case>P</fixed-case>avia Typological Database</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/35_pdf.pdf</url>
      <abstract>This paper describes the Pavia Typological Database (PTD), a follow-up to the MED-TYP database (Sansò 2004). The PTD is an ever-growing repository of primary linguistic data (words, clauses, sentences) documenting a number of morphosyntactic phenomena in the languages of Europe (and including in some cases languages from the Mediterranean area). Its prospective users are typologists wanting to access primary, typologically uninterpreted (but glossed) data, but also anyone interested in linguistic variation on a continental scale. The paper discusses the background and motivation for the creation of the PTD, its present coverage, the techniques used to annotate the primary data, and the general architecture of the database.</abstract>
    </paper>
    <paper id="14">
      <author id="mohammad-bahrani"><first>M.</first><last>Bahrani</last></author>
      <author id="hossein-sameti"><first>H.</first><last>Sameti</last></author>
      <author id="nazila-hafezi"><first>N.</first><last>Hafezi</last></author>
      <author id="hamed-movasagh"><first>H.</first><last>Movasagh</last></author>
      <title>Building and Incorporating Language Models for <fixed-case>P</fixed-case>ersian Continuous Speech Recognition Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/36_pdf.pdf</url>
      <abstract>In this paper building statistical language models for Persian language using a corpus and incorporating them in Persian continuous speech recognition (CSR) system are described. We used Persian Text Corpus for building the language models. First we preprocessed the texts of corpus by correcting the different orthography of words. Also, the number of POS tags was decreased by clustering POS tags manually. Then we extracted word based monogram and POS-based bigram and trigram language models from the corpus. We also present the procedure of incorporating language models in a Persian CSR system. By using the language models 27.4% reduction in word error rate was achieved in the best case.</abstract>
    </paper>
    <paper id="15">
      <author><first>Shisanu</first><last>Tongchim</last></author>
      <author><first>Prapass</first><last>Srichaivattana</last></author>
      <author><first>Virach</first><last>Sornlertlamvanich</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>Blind Evaluation for <fixed-case>T</fixed-case>hai Search Engines</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/37_pdf.pdf</url>
      <abstract>This paper compares the effectiveness of two different Thai search engines by using a blind evaluation. The probabilistic-based dictionary-less search engine is evaluated against the traditional word-based indexing method. The web documents from 12 Thai newspaper web sites consisting of 83,453 documents are used as the test collection. The relevance judgment is conducted on the first five returned results from each system. The evaluation process is completely blind. That is, the retrieved documents from both systems are shown to the judges without any information about thesearch techniques. Statistical testing shows that the dictionary-less approach is better than the word-based indexingapproach in terms of the number of found documents and the number of relevance documents.</abstract>
    </paper>
    <paper id="16">
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <author><first>Masayuki</first><last>Asahara</last></author>
      <author><first>Kiyota</first><last>Hashimoto</last></author>
      <author><first>Yukio</first><last>Tono</last></author>
      <author><first>Akira</first><last>Ohtani</last></author>
      <author><first>Toshio</first><last>Morita</last></author>
      <title>An Annotated Corpus Management Tool: <fixed-case>C</fixed-case>ha<fixed-case>K</fixed-case>i</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/39_pdf.pdf</url>
      <abstract>Large scale annotated corpora are very important not only inlinguistic research but also in practical natural language processingtasks since a number of practical tools such as Part-of-speech (POS) taggers and syntactic parsers are now corpus-based or machine learning-based systems which require some amount of accurately annotated corpora. This article presents an annotated corpus management tool that provides various functions that include flexible search, statistic calculation, and error correction for linguistically annotated corpora. The target of annotation covers POS tags, base phrase chunks and syntactic dependency structures. This tool aims at helping development of consistent construction of lexicon and annotated corpora to be used by researchers both in linguists and language processing communities.</abstract>
    </paper>
    <paper id="17">
      <author><first>Christophe</first><last>Jouis</last></author>
      <title>Hierarchical Relationships “is-a”: Distinguishing Belonging, Inclusion and Part/of Relationships.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/43_pdf.pdf</url>
      <abstract>In thesauri, conceptual structures or semantic networks, relationships are too often vague. For instance, in terminology, the relationships between concepts are often reduced to the distinction established by standard (ISO 704, 1987) and (ISO 1087, 1990) between hierarchical relationships (genus-species relationships and part/whole relationships) and non-hierarchical relationships (time, space, causal relationships, etc.). The semantics of relationships are vague because the principal users of these relationships are industrial actors (translators of technical handbooks, terminologists, data-processing specialists, etc.). Nevertheless, the consistency of the models built must always be guaranteed... One possible approach to this problem consists in organizing the relationships in a typology based on logical properties. For instance, we typically use only the general relation Is-a. It is too vague. We assume that general relation Is-a is characterized by asymmetry. This asymmetry is specified in: (1) the belonging of one individualizable entity to a distributive class, (2) Inclusion among distributive classes and (3) relation part of (or composition).</abstract>
    </paper>
    <paper id="18">
      <author><first>Ivan</first><last>Berlocher</last></author>
      <author><first>Hyun-gue</first><last>Huh</last></author>
      <author><first>Eric</first><last>Laporte</last></author>
      <author><first>Jee-sun</first><last>Nam</last></author>
      <title>Morphological annotation of <fixed-case>K</fixed-case>orean with Directly Maintainable Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/44_pdf.pdf</url>
      <abstract>This article describes an exclusively resource-based method of morphological annotation of written Korean text. Korean is an agglutinative language. Our annotator is designed to process text before the operation of a syntactic parser. In its present state, it annotates one-stem words only. The output is a graph of morphemes annotated with accurate linguistic information. The granularity of the tagset is 3 to 5 times higher than usual tagsets. A comparison with a reference annotated corpus showed that it achieves 89% recall without any corpus training. The language resources used by the system are lexicons of stems, transducers of suffixes and transducers of generation of allomorphs. All can be easily updated, which allows users to control the evolution of the performances of the system. It has been claimed that morphological annotation of Korean text could only be performed by a morphological analysis module accessing a lexicon of morphemes. We show that it can also be performed directly with a lexicon of words and without applying morphological rules at annotation time, which speeds up annotation to 1,210 words. The lexicon of words is obtained from the maintainable language resources through a fully automated compilation process.</abstract>
    </paper>
    <paper id="19">
      <author><first>Patrick</first><last>Saint-Dizier</last></author>
      <title><fixed-case>P</fixed-case>rep<fixed-case>N</fixed-case>et: a Multilingual Lexical Description of Prepositions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/45_pdf.pdf</url>
      <abstract>In this paper, we present the results of a preliminary investigation that aims at constructing a repository of preposition syntactic and semantic behaviors. A preliminary frame-based format for representing their prototypical behavior is then proposed together with related inferential patterns that describe functional or paradigmatic relations between preposition senses.</abstract>
    </paper>
    <paper id="20">
      <author><first>Marie-Claude</first><last>L’Homme</last></author>
      <author><first>Hee Sook</first><last>Bae</last></author>
      <title>A Methodology for Developing Multilingual Resources for Terminology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/48_pdf.pdf</url>
      <abstract>This paper presents a project that aims at building lexical resources for terminology. By lexical resources, we mean dictionaries that provide detailed lexico-semantic information on terms, i.e. lexical units the sense of which can be related to a special subject field. In terminology, there is a lack of such resources. The specific dictionaries we are currently developing describe basic French and Korean terms that belong to the fields of computer science and the Internet (e.g. computer, configure, user-friendly, Web, browse, spam). This paper presents the structure of the French and Korean articles: each component is examined and illustrated with examples. We then describe the corpus-based methodology and the different computer applications used for developing the articles. Our methodology comprises five steps: design of the corpora, selection of terms; sense distinction; definition of actantial structures and listing of semantic relations. Details on the current state of each database are also given.</abstract>
    </paper>
    <paper id="21">
      <author><first>Stephan</first><last>Raidt</last></author>
      <author><first>Gérard</first><last>Bailly</last></author>
      <author><first>Frederic</first><last>Elisei</last></author>
      <title>Does a Virtual Talking Face Generate Proper Multimodal Cues to Draw User’s Attention to Points of Interest?</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/49_pdf.pdf</url>
      <abstract>We present a series of experiments investigating face-to-face interaction between an Embodied Conversational Agent (ECA) and a human interlocutor. The ECA is embodied by a video realistic talking head with independent head and eye movements. For a beneficial application in face-to-face interaction, the ECA should be able to derive meaning from communicational gestures of a human interlocutor, and likewise to reproduce such gestures. Conveying its capability to interpret human behaviour, the system encourages the interlocutor to show appropriate natural activity. Therefore it is important that the ECA knows how to display what would correspond to mental states in humans. This allows to interpret the machine processes of the system in terms of human expressiveness and to assign them a corresponding meaning. Thus the system may maintain an interaction based on human patterns. During a first experiment we investigated the ability of our talking head to direct user attention with facial deictic cues (Raidt, Bailly et al. 2005). Users interact with the ECA during a simple card game offering different levels of help and guidance through facial deictic cues. We analyzed the users performance and their perception of the quality of assistance given by the ECA. The experiment showed that users profit from its presence and its facial deictic cues. In the continuative series of experiments presented here, we investigated the effect of an enhancement of the multimodality of the deictic gestures by adding a spoken instruction.</abstract>
    </paper>
    <paper id="22">
      <author><first>May Lai-Yin</first><last>Wong</last></author>
      <title>Skeleton Parsing in <fixed-case>C</fixed-case>hinese: Annotation Scheme and Guidelines</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/50_pdf.pdf</url>
      <abstract>This paper presents my manual skeleton parsing on a sample text of approximately 100,000 word tokens (or about 2,500 sentences) taken from the PFR Chinese Corpus with a clearly defined parsing scheme of 17 constituent labels. The manually-parsed sample skeleton treebank is one of the very few extant Chinese treebanks. While Chinese part-of-speech tagging and word segmentation have been the subject of concerted research for many years, the syntactic annotation of Chinese corpora is a comparatively new field. The difficulties that I encountered in the production of this treebank demonstrate some of the peculiarities of Chinese syntax. A noteworthy syntactic property is that some serial verb constructions tend to be used as if they were compound verbs. The two transitive verbs in series, unlike common transitive verbs, do not take an object separately within the construction; rather, the serial construction as a whole is able to take the same direct object and the perfective aspect marker le. The skeleton-parsed sample treebank is evaluated against Eyes &amp; Leech (1993)s criteria and proves to be accurate, uniform and linguistically valid.</abstract>
    </paper>
    <paper id="23">
      <author><first>Constantin</first><last>Orăsan</last></author>
      <author><first>Laura</first><last>Hasler</last></author>
      <title>Computer-aided summarisation – what the user really wants</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/52_pdf.pdf</url>
      <abstract>Computer-aided summarisation is a technology developed at the University of Wolverhampton as a complement to automatic summarisation, to produce high quality summaries with less effort. To achieve this, a user-friendly environment which incorporates several well-known summarisation methods has been developed. This paper presents the main features of the computer-aided summarisation environment and explains the changes introduced to it as a result of user feedback.</abstract>
    </paper>
    <paper id="24">
      <author><first>Nianwen</first><last>Xue</last></author>
      <title>Annotating the Predicate-Argument Structure of <fixed-case>C</fixed-case>hinese Nominalizations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/54_pdf.pdf</url>
      <abstract>This paper describes the Chinese NomBank Project, the goal of which is to annotate the predicate-argument structure of nominalized predicates in Chinese. The Chinese Nombank extends the general framework of the English and Chinese Proposition Banks to the annotation of nominalized predicates and adds a layer of semantic annotation to the Chinese Treebank. We first outline the scope of the work by discussing the markability of the nominalized predicates and their arguments. We then attempt to provide a categorization of the distribution of the arguments of nominalized predicates. We also discuss the relevance of the event/result distinction to the annotation of nominalized predicates and the phenomenon of incorporation. Finally we discuss some cross-linguistic differences between English and Chinese.</abstract>
    </paper>
    <paper id="25">
      <author><first>Kyo</first><last>Kageura</last></author>
      <author><first>Genichiro</first><last>Kikui</last></author>
      <title>A Self-Referring Quantitative Evaluation of the <fixed-case>ATR</fixed-case> Basic Travel Expression Corpus (<fixed-case>BTEC</fixed-case>)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/55_pdf.pdf</url>
      <abstract>In this paper we evaluate the Basic Travel Expression Corpus (BTEC), developed by ATR (Advanced Telecommunication Research Laboratory), Japan. BTEC was specifically developed as a wide-coverage, consistent corpus containing basic Japanese travel expressions with English counterparts, for the purpose of providing basic data for the development of high quality speech translation systems. To evaluate the corpus, we introduce a quantitative method for evaluating the sufficiency of qualitatively well-defined corpora, on the basis of LNRE methods that can estimate the potential growth patterns of various sparse data by fitting various skewed distributions such as the Zipfian group of distributions, lognormal distribution, and inverse Gauss-Poisson distribution to them. The analyses show the coverage of lexical items of BTEC vis-a-vis the possible targets implicitly defined by the corpus itself, and thus provides basic insights into strategies for enhancing BTEC in future.</abstract>
    </paper>
    <paper id="26">
      <author><first>Hiromi itoh</first><last>Ozaku</last></author>
      <author><first>Akinori</first><last>Abe</last></author>
      <author><first>Kaoru</first><last>Sagara</last></author>
      <author><first>Noriaki</first><last>Kuwahara</last></author>
      <author><first>Kiyoshi</first><last>Kogure</last></author>
      <title>Features of Terms in Actual Nursing Activities</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/58_pdf.pdf</url>
      <abstract>In this paper, we analyze nurses' dialogue and conversation data sets after manual transcriptions and show their features. Recently, medical risk management has been recognized as very important for both hospitals and their patients. To carry out medical risk management, it is important to model nursing activities as well as to collect many accident and incident examples. Therefore, we are now researching strategies of modeling nursing activities in order to understand them (E-nightingale Project). To model nursing activities, it is necessary to collect data of nurses' activities in actual situations and to accurately understand these activities and situations. We developed a method to determine any type of nursing activity from voice data. However we found that our method could not determine several activities because it misunderstood special nursing terms. To improve the accuracy of this method, we focus on analyzing nurses' dialogue and conversation data and on collecting special nursing terms. We have already collected 800 hours of nurses' dialogue and conversation data sets in hospitals to find the tendencies and features of how nurses use special terms such as abbreviations and jargon as well as new terms. Consequently, in this paper we categorize nursing terms according to their usage and effectiveness. In addition, based on the results, we show a rough strategy for building nursing dictionaries.</abstract>
    </paper>
    <paper id="27">
      <author><first>Diana</first><last>Santos</last></author>
      <author><first>Nuno</first><last>Seco</last></author>
      <author><first>Nuno</first><last>Cardoso</last></author>
      <author><first>Rui</first><last>Vilela</last></author>
      <title><fixed-case>HAREM</fixed-case>: An Advanced <fixed-case>NER</fixed-case> Evaluation Contest for <fixed-case>P</fixed-case>ortuguese</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/59_pdf.pdf</url>
      <abstract>In this paper we provide an overview of the first evaluation contest for named entity recognition in Portuguese, HAREM, which features several original traits and provided the first state of the art for the field in Portuguese, as well as a public-domain evaluation architecture.</abstract>
    </paper>
    <paper id="28">
      <author><first>Rafael</first><last>Banchs</last></author>
      <author><first>Antonio</first><last>Bonafonte</last></author>
      <author><first>Javier</first><last>Pérez</last></author>
      <title>Acceptance Testing of a Spoken Language Translation System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/60_pdf.pdf</url>
      <abstract>This paper describes an acceptance test procedure for evaluating a spoken language translation system between Catalan and Spanish. The procedure consists of two independent tests. The first test was an utterance-oriented evaluation for determining how the use of speech benefits communication. This test allowed for comparing relative performance of the different system components, explicitly: source text to target text, source text to target speech, source speech to target text, and source speech to target speech. The second test was a task-oriented experiment for evaluating if users could achieve some predefined goals for a given task with the state of the technology. Eight subjects familiar with the technology and four subjects not familiar with the technology participated in the tests. From the results we can conclude that state of technology is getting closer to provide effective speech-to-speech translation systems but there is still lot of work to be done in this area. No significant differences in performance between users that are familiar with the technology and users that are not familiar with the technology were evidenced. This constitutes, as far as we know, the first evaluation of a Spoken Translation System that considers performance at both, the utterance level and the task level.</abstract>
    </paper>
    <paper id="29">
      <author><first>Valentin</first><last>Tablan</last></author>
      <author><first>Wim</first><last>Peters</last></author>
      <author><first>Diana</first><last>Maynard</last></author>
      <author><first>Hamish</first><last>Cunningham</last></author>
      <title>Creating Tools for Morphological Analysis of <fixed-case>S</fixed-case>umerian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/64_pdf.pdf</url>
      <abstract>Sumerian is a long-extinct language documented throughout the ancient MiddleEast, arguably the first language for which we have written evidence, and is a language isolate (i.e. no related languages have so far been identified). The Electronic Text Corpus of Sumerian Literature (ETCSL), based at theUniversity of Oxford, aims to make accessible on the web over 350 literary workscomposed during the late third and early second millennia BCE. The transliterations and translations can be searched, browsed and read online using the tools of the website. In this paper we describe the creation of linguistic analysis and corpus search tools for Sumerian, as part of the development of the ETCSL. This is designed to enable Sumerian scholars, students and interested laymen to analyse the texts online and electronically, and to further knowledge about the language.</abstract>
    </paper>
    <paper id="30">
      <author><first>Ingunn</first><last>Amdal</last></author>
      <author><first>Torbjørn</first><last>Svendsen</last></author>
      <title><fixed-case>F</fixed-case>on<fixed-case>D</fixed-case>at1: A Speech Synthesis Corpus for <fixed-case>N</fixed-case>orwegian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/66_pdf.pdf</url>
      <abstract>This paper describes the Norwegian speech database FonDat1 designedfor development and assessment of Norwegian unit selection speechsynthesis. The quality of unit selection speech synthesis systems depends highly on the database used. The database should contain sufficient phonemicand prosodic coverage. High quality unit selection synthesis alsorequires that the database is annotated with accurate information about identity and position of the units.Traditionally this involves much manual work, either by hand labelingthe entire database or by correcting automatic annotations. We are working on methods for a complete automation of the annotationprocess. To validate these methods a realistic unit selectionsynthesis database is needed.In addition to serve as a testbed for annotation tools and synthesisexperiments, the process of producing the database using automaticmethods is in itself an important result.FonDat1 contains studio recordings of approximately 2000 sentencesread by two professional speakers, one male and one female. 10% ofthe database is manually annotated.</abstract>
    </paper>
    <paper id="31">
      <author><first>Alon</first><last>Itai</last></author>
      <author><first>Shuly</first><last>Wintner</last></author>
      <author><first>Shlomo</first><last>Yona</last></author>
      <title>A Computational Lexicon of Contemporary <fixed-case>H</fixed-case>ebrew</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/67_pdf.pdf</url>
      <abstract>Computational lexicons are among the most important resources for natural language processing (NLP). Their importance is even greater in languages with rich morphology, where the lexicon is expected to provide morphological analyzers with enough information to enable themto correctly process intricately inflected forms. We describe the Haifa Lexicon of Contemporary Hebrew, the broadest-coverage publicly available lexicon of Modern Hebrew, currently consisting of over 20,000 entries.While other lexical resources of Modern Hebrew have been developed in the past, this is the first publicly available large-scale lexicon of the language. In addition to supporting morphological processors (analyzers and generators), which was our primary objective, thelexicon is used as a research tool in Hebrew lexicography and lexical semantics. It is open for browsing on the web and several search tools and interfaces were developed which facilitate on-line access to its information. The lexicon is currently used for a variety of NLP applications.</abstract>
    </paper>
    <paper id="32">
      <author><first>Ruska</first><last>Ivanovska-Naskova</last></author>
      <title>Development of the First <fixed-case>LR</fixed-case>s for <fixed-case>M</fixed-case>acedonian: Current Projects</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/68_pdf.pdf</url>
      <abstract>This paper presents in brief several ongoing projects whose aim is to develop the first LRs for Macedonian, in particular the raw corpus compiled by Prof. George Mitrevski at the Auburn University, the preparation for the compilation of a reference corpus for the Macedonian written language at the MASA (Macedonian Academy of Sciences and Arts), the first small annotated corpus of the Macedonian translation of the Orwells 1984, the electronic dictionary of simple words created by Aleksandar Petrovski for the Macedonian module in the frame of the corpus processing system Intex/Nooj and the Morphological dictionary developed by the LTRC (Language Technology Research Center). Further we discuss the importance of the development of the basic LRs for Macedonian as a means of preservation and a prerequisite for the creation of the first commercial language products for this Slavic language.</abstract>
    </paper>
    <paper id="33">
      <author><first>Reinhard</first><last>Rapp</last></author>
      <author><first>Carlos Martin</first><last>Vide</last></author>
      <title>Example-Based Machine Translation Using a Dictionary of Word Pairs</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/74_pdf.pdf</url>
      <abstract>Machine translation systems, whether rule-based, example-based, or statistical, all rely on dictionaries that are in essence mappings between individual words of the source and the target language. Criteria for the disambiguation of ambiguous words and for differences in word order between the two languages are not accounted for in the lexicon. Instead, these important issues are dealt with in the translation engines. Because the engines tend to be compact and (even with data-oriented approaches) do not fully reflect the complexity of the problem, this approach generally does not account for the more fine grained facets of word behavior. This leads to wrong generalizations and, as a consequence, translation quality tends to be poor. In this paper we suggest to approach this problem by using a new type of lexicon that is not based on individual words but on pairs of words. For each pair of consecutive words in the source language the lexicon lists the possible translations in the target language together with information on order and distance of the target words. The process of machine translation is then seen as a combinatorial problem: For all word pairs in a source sentence all possible translations are retrieved from the lexicon and then those translations are discarded that lead to contradictions when constructing the target sentence. This process implicitly leads to word sense disambiguation and to language specific reordering of words.</abstract>
    </paper>
    <paper id="34">
      <author><first>Widad Mustafa</first><last>El Hadi</last></author>
      <author><first>Ismail</first><last>Timimi</last></author>
      <author><first>Marianne</first><last>Dabbadie</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <author><first>Yun-Chuang</first><last>Chiao</last></author>
      <title>Terminological Resources Acquisition Tools: Toward a User-oriented Evaluation Model</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/75_pdf.pdf</url>
      <abstract>This paper describes the CESART project which deals with the evaluation of terminological resources acquisition tools. The objective of the project is to propose and validate an evaluation protocol allowing one to objectively evaluate and compare different systems for terminology application such as terminological resource creation and semantic relation extraction. The project also aims to create quality-controlled resources such as domain-specific corpora, automatic scoring tool, etc.</abstract>
    </paper>
    <paper id="35">
      <author><first>Núria</first><last>Bel</last></author>
      <author><first>Sergio</first><last>Espeja</last></author>
      <author><first>Montserrat</first><last>Marimon</last></author>
      <title>New tools for the encoding of lexical data extracted from corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/76_pdf.pdf</url>
      <abstract>This paper describes the methodology and tools that are the basis of our platform AAILE.4 AAILE has been built for supplying those working in the construction of lexicons for syntactic parsing with more efficient ways of visualizing and analyzing data extracted from corpus. The platform offers support using techniques such as similarity measures, clustering and pattern classification.</abstract>
    </paper>
    <paper id="36">
      <author><first>Daniela</first><last>Braga</last></author>
      <author><first>Luís</first><last>Coelho</last></author>
      <author><first>João P.</first><last>Teixeira</last></author>
      <author><first>Diamantino</first><last>Freitas</last></author>
      <title><fixed-case>P</fixed-case>rogmatica: A Prosodic Database for <fixed-case>E</fixed-case>uropean <fixed-case>P</fixed-case>ortuguese</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/77_pdf.pdf</url>
      <abstract>In this work, a spontaneous speech corpus of broadcasted television material in European Portuguese (EP) is presented. We decided to name it ProGmatica as it is meant to combine prosody information under a pragmatic framework. Our purpose is to analyse, describe and predict the prosodic patterns that are involved in speech acts and discourse events. It is also our goal to relate both prosody and pragmatics to emotion, style and attitude. In future developments, we intend, by this way, to provide EP TTS systems with pragmatic and emotional dimensions. From the whole recorded material we selected, extracted and saved prototypical speech acts with the help of speech analysis tools. We have a multi-speaker corpus, where linguistic, paralinguistic and extra linguistic information are labelled and related to each other. The paper is organized as follows. In section one, a brief state-of-the-art for the available EP corpora containing prosodic information is presented. In section two, we explain the pragmatic criteria used to structure this database. Then, we describe how the speech signal was labelled and which information layers were considered. In section three, we propose a prosodic prediction model to be applied to each speech act in future. In section four, some of the main problems we went through are discussed and future work is presented.</abstract>
    </paper>
    <paper id="37">
      <author><first>Jesús</first><last>Giménez</last></author>
      <author><first>Enrique</first><last>Amigó</last></author>
      <title><fixed-case>I</fixed-case>qmt: A Framework for Automatic Machine Translation Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/79_pdf.pdf</url>
      <abstract>We present the IQMT Framework for Machine Translation Evaluation Inside QARLA. IQMT offers a common workbench in which existing evaluation metrics can be utilized and combined. It provides i) a measure to evaluate the quality of any set of similarity metrics (KING), ii) a measure to evaluate the quality of a translation using a set of similarity metrics (QUEEN), and iii) a measure to evaluate the reliability of a test set (JACK). The first release of the IQMT package is freely available for public use. Current version includes a set of 26 metrics from 7 different well-known metric families, and allows the user to supply its own metrics. For future releases, we are working on the design of new metrics that are able to capture linguistic aspects of translation beyond lexical ones.</abstract>
    </paper>
    <paper id="38">
      <author><first>Tommaso</first><last>Caselli</last></author>
      <author><first>Irina</first><last>Prodanof</last></author>
      <title>Annotating Bridging Anaphors in <fixed-case>I</fixed-case>talian: in Search of Reliability</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/80_pdf.pdf</url>
      <abstract>The aim of this work is the presentation and preliminary evaluation of an XML annotation scheme for marking bridging anaphors of the form definite article + N in Italian. The scheme is based on a corpus-study. The data we collected from the evaluation experiment seem to support the reliability of the scheme, although some problems still remain open.</abstract>
    </paper>
    <paper id="39">
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Christian</first><last>Gollan</last></author>
      <author><first>Asuncion</first><last>Moreno</last></author>
      <author><first>Djamel</first><last>Mostefa</last></author>
      <title><fixed-case>TC</fixed-case>-<fixed-case>STAR</fixed-case>: New language resources for <fixed-case>ASR</fixed-case> and <fixed-case>SLT</fixed-case> purposes</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/81_pdf.pdf</url>
      <abstract>In TC-STAR a variety of Language Resources (LR) is being produced. In this contribution we address the resources that have been created for Automatic Speech Recrognition and Spoken Language Translation. As yet, these are 14 LR in total: two training SLR for ASR (English and Spanish), three development LR and three evaluation LR for ASR (English, Spanish, Mandarin), and three development LR and three evaluation LR for SLT (English-Spanish, Spanish-English, Mandarin-English). In this paper we describe the properties, validation, and availability of these resources.</abstract>
    </paper>
    <paper id="40">
      <author><first>Yunqing</first><last>Xia</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <author><first>Wenjie</first><last>Li</last></author>
      <title>Constructing A <fixed-case>C</fixed-case>hinese Chat Language Corpus with A Two-Stage Incremental Annotation Approach</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/86_pdf.pdf</url>
      <abstract>Chat language refers to the special human language widely used in the community of digital network chat. As chat language holds anomalous characteristics in forming words, phrases, and non-alphabetical characters, conventional natural language processing tools are ineffective to handle chat language text. Previous research shows that knowledge based methods perform less effectively in proc-essing unseen chat terms. This motivates us to construct a chat language corpus so that corpus-based techniques of chat language text processing can be developed and evaluated. However, creating the corpus merely by hand is difficult. One, this work is manpower consuming. Second, annotation inconsistency is serious. To minimize manpower and annotation inconsistency, a two-stage incre-mental annotation approach is proposed in this paper in constructing a chat language corpus. Experiments conducted in this paper show that the performance of corpus annotation can be improved greatly with this approach.</abstract>
    </paper>
    <paper id="41">
      <author><first>Baden</first><last>Hughes</last></author>
      <title>Searching for Language Resources on the Web: User Behaviour in the Open Language Archives Community</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/87_pdf.pdf</url>
      <abstract>While much effort is expended in the curation of language resources, such investment is largely irrelevant if users cannot locate resourcesof interest. The Open Language Archives Community (OLAC) was established to define standards for the description of language resources and providecore infrastructure for a virtual digital library, thus addressing the resource discovery issue. In this paper we consider naturalistic user search behaviour in the Open Language Archives Community. Specifically, we have collected the query logs from the OLAC Search Engine over a 2 year period, collecting in excess of 1.2 million queries, in over 450K user search sessions. Subsequently we have mined these to discover user search patterns of various types, all pertaining to the discovery of language resources.A number of interesting observations can be made based on this analysis, in this paper we report on a range of properties and behaviours based on empirical evidence.</abstract>
    </paper>
    <paper id="42">
      <author><first>Yasunori</first><last>Ohishi</last></author>
      <author><first>Katunobu</first><last>Itou</last></author>
      <author><first>Kazuya</first><last>Takeda</last></author>
      <author><first>Atsushi</first><last>Fujii</last></author>
      <title>Statistical Analysis for Thesaurus Construction using an Encyclopedic Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/88_pdf.pdf</url>
      <abstract>This paper proposes a discrimination method for hierarchical relationsbetween word pairs. The method is a statistical one using an encyclopedic corpus' extracted and organized from Web pages.In the proposed method, we use the statistical naturethat hyponyms' descriptionstend to include hypernyms whereas hypernyms' descriptions do notinclude all of the hyponyms.Experimental results show that the method detected 61.7% of therelations in an actual thesaurus.</abstract>
    </paper>
    <paper id="43">
      <author><first>Catherine</first><last>Havasi</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Marc</first><last>Verhagen</last></author>
      <title><fixed-case>BULB</fixed-case>: A Unified Lexical Browser</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/89_pdf.pdf</url>
      <abstract>Natural language processing researchers currently have access to a wealth of information about words and word senses. This presents problems as well as resources, as it is often difficult to search through and coordinate lexical information across various data sources. We have approached this problem by creating a shared environment for various lexical resources. This browser, BULB (Brandeis Unified Lexical Browser) and its accompanying front-end provides the NLP researcher with a coordinated display from many of the available lexical resources, focusing, in particular, on a newly developed lexical database, the Brandeis Semantic Ontology (BSO). BULB is a module-based browser focusing on the interaction and display of modules from existing NLP tools. We discuss the BSO, PropBank, FrameNet, WordNet, and CQP, as well as other modules which will extend the system. We then outline future extensions to this work and present a release schedule for BULB.</abstract>
    </paper>
    <paper id="44">
      <author><first>Ulrich</first><last>Schäfer</last></author>
      <author><first>Daniel</first><last>Beck</last></author>
      <title>Automatic Testing and Evaluation of Multilingual Language Technology Resources and Components</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/91_pdf.pdf</url>
      <abstract>We describe SProUTomat, a tool for daily building, testing and evaluating a complex general-purpose multilingual natural language text processor including its linguistic resources (lingware). Software and lingware are developed, maintained and extended in a distributed manner by multiple authors and projects, i.e., the source code stored in a version control system is modified frequently. The modular design of different, dedicated lingware modules like tokenizers, morphology, gazetteers, type hierarchy, rule formalism on the one hand increases flexibility and re-usability, but on the other hand may lead to fragility with respect to changes. Therefore, frequent testing as known from software engineering is necessary also for lingware to warrant a high level of quality and overall stability of the system. We describe the build, testing and evaluation methods for LT software and lingware we have developed on the basis of the open source, platform-independent Apache Ant tool and the configurable evaluation tool JTaCo.</abstract>
    </paper>
    <paper id="45">
      <author><first>Elena</first><last>Grishina</last></author>
      <title>Spoken <fixed-case>R</fixed-case>ussian in the <fixed-case>R</fixed-case>ussian National Corpus (<fixed-case>RNC</fixed-case>)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/92_pdf.pdf</url>
      <abstract>The RNC now it is a 120 million-word collection of Russian text, thus, it is the most representative and authoritative corpus of the Russian language. It is available in the Internet at www.ruscorpora.ru. The RNC contains texts of all genres and types, which covers Russian from 19 up to 21 centuries. The practice of national corpora constructing has revealed that it's indispensable to include in the RNC the sub-corpora of spoken language. Therefore, the constructors of the RNC have an intention to include in it about 10 million words of Spoken Russian. Oral speech in the Corpus is represented in the standard Russian orthography. Although this decision made impossible any phonetic exploration of the Spoken Russian Corpus, but studying Spoken Russian from any other linguistic point of view is completely available. In addition to traditional annotations (metatextual and morphological), in Spoken Sub-corpus there is sociological annotation. Unlike the standard oral speech, which is spontaneous and isn't intended to be reproduced, Multimedia Spoken Russian (MSR) is otherwise in great deal premeditated and evidently meant to be reproduced. MSR is also to be included in the RNC: first of all we plan to make the very interesting and provocative part of the RNC from the textual ingredient of about 300 Russian films.</abstract>
    </paper>
    <paper id="46">
      <author><first>Paul</first><last>Buitelaar</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <author><first>Stefania</first><last>Racioppa</last></author>
      <author><first>Melanie</first><last>Siegel</last></author>
      <title>Ontology-based Information Extraction with <fixed-case>SOBA</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/93_pdf.pdf</url>
      <abstract>In this paper we describe SOBA, a sub-component of the SmartWeb multi-modal dialog system. SOBA is a component for ontologybased information extraction from soccer web pages for automatic population of a knowledge base that can be used for domainspecific question answering. SOBA realizes a tight connection between the ontology, knowledge base and the information extraction component. The originality of SOBA is in the fact that it extracts information from heterogeneous sources such as tabular structures, text and image captions in a semantically integrated way. In particular, it stores extracted information in a knowledge base, and in turn uses the knowledge base to interpret and link newly extracted information with respect to already existing entities.</abstract>
    </paper>
    <paper id="47">
      <author><first>Dominique</first><last>Dutoit</last></author>
      <title><fixed-case>A</fixed-case>lexandria: A Powerful Multilingual Resource for Web</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/94_pdf.pdf</url>
      <abstract>This paper is dealing with a new web interface to display linguistic data on the web. This new web interface is a general proposal for the web. Its present name is Alexandria. Alexandria is an amazing tool that can be downloaded free of charge, under certain conditions. Although the initial idea was hatched six or seven years ago, its technical realization has only been feasible for the past two years. If you want to read the HTML page, for instance http://www.memodata.com, double click on any word at random and you'll see a window open with a definition of the word followed by a list of synonyms and expressions using the word. If not, your browser is not in French. Then, you have to use the menu to modify the target language and choice the French between 22 languages.</abstract>
    </paper>
    <paper id="48">
      <author><first>Jan Frederik</first><last>Maas</last></author>
      <author><first>Britta</first><last>Wrede</last></author>
      <title><fixed-case>BITT</fixed-case>: A Corpus for Topic Tracking Evaluation on Multimodal Human-Robot-Interaction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/96_pdf.pdf</url>
      <abstract>Our research is concerned with the development of robotic systems which can support people in household environments, such as taking care of elderly people. A central goal of our research consists in creating robot systems which are able to learn and communicate about a given environment without the need of a specially trained user. For the communication with such users it is necessary that the robot is able to communicate multimodally, which especially includes the ability to communicate in natural language. Our research is concerned with the development of robotic systems which can support people in household environments, such as taking care of elderly people. A central goal of our research consists in creating robot systems which are able to learn and communicate about a given environment without the need of a specially trained user. For the communication with such users it is necessary that the robot is able to communicate multimodally, which especially includes the ability to communicate in natural language. We believe that the ability to communicate naturally in multimodal communication must be supported by the ability to access contextual information, with topical knowledge being an important aspect of this knowledge. Therefore, we currently develop a topic tracking system for situated human-robot communication on our robot systems. This paper describes the BITT (Bielefeld Topic Tracking) corpus which we built in order to develop and evaluate our system. The corpus consists of human-robot communication sequences about a home-like environment, delivering access to the information sources a multimodal topic tracking system requires.</abstract>
    </paper>
    <paper id="49">
      <author><first>Hercules</first><last>Dalianis</last></author>
      <author><first>Bart</first><last>Jongejan</last></author>
      <title>Hand-crafted versus Machine-learned Inflectional Rules: The Euroling-<fixed-case>S</fixed-case>ite<fixed-case>S</fixed-case>eeker Stemmer and <fixed-case>CST</fixed-case>’s Lemmatiser</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/97_pdf.pdf</url>
      <abstract>The Euroling stemmer is developed for a commercial web site and intranet search engine called SiteSeeker. SiteSeeker is basically used in the Swedish domain but to some extent also for the English domain. CST's lemmatiser comes from the Center for Language Technology, University of Copenhagen and was originally developed as a research prototype to create lemmatisation rules from training data. In this paper we compare the performance of the stemmer that uses handcrafted rules for Swedish, Danish and Norwegian as well one stemmer for Greek with CST's lemmatiser that uses training data to extract lemmatisation rules for Swedish, Danish, Norwegian and Greek. The performances of the two approaches are about the same with around 10 percent errors. The handcrafted rule based stemmer techniques are easy to get started with if the programmer has the proper linguistic knowledge. The machine trained sets of lemmatisation rules are very easy to produce without having linguistic knowledge given that one has correct training data.</abstract>
    </paper>
    <paper id="50">
      <author><first>Christian</first><last>Rohrer</last></author>
      <author><first>Martin</first><last>Forst</last></author>
      <title>Improving coverage and parsing quality of a large-scale <fixed-case>LFG</fixed-case> for <fixed-case>G</fixed-case>erman</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/99_pdf.pdf</url>
      <abstract>We describe experiments in parsing the German TIGER Treebank. In parsing the complete treebank, 86.44% of the sentences receive full parses; 13.56% receive fragment parses. We discuss the methods used to enhance coverage and parsing quality and we present an evaluation on a gold standard, to our knowledge the first one for a deep grammar of German. Considering the selection performed by our current version of a stochastic disambiguation component, we achieve an f-score of 84.2%, the upper and lower bounds being 87.4% and 82.3% respectively.</abstract>
    </paper>
    <paper id="51">
      <author><first>Bettina</first><last>Schrader</last></author>
      <title>Non-probabilistic alignment of rare <fixed-case>G</fixed-case>erman and <fixed-case>E</fixed-case>nglish nominal expressions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/100_pdf.pdf</url>
      <abstract>We present an alignment strategy that specifically deals with the correct alignment of rare German nominal compounds to their English multiword translations. It recognizes compounds and multiwords based on their character lengths and on their most frequent POS-patterns, and aligns them based on their length ratios. Our approach is designed on the basis of a data analysis on roughly 500 German hapax legomena, and as it does not use any frequency or co-occurrence information, it is well-suited to align rare compounds, but also achieves good results for more frequent expressions. Experiment results show that the strategy is able to correctly identify correct translations for 70% of the compound hapaxes in our data set. Additionally, we checked on 700 randomly chosen entries in the dictionary that was automatically generated by our alignment tool. Results of this experiment also indicate that our strategy works for non-hapaxes as well, including finding multiple correct translations for the same head compound.</abstract>
    </paper>
    <paper id="52">
      <author><first>Paula</first><last>Chesley</last></author>
      <author><first>Susanne</first><last>Salmon-Alt</last></author>
      <title>Automatic extraction of subcategorization frames for <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/101_pdf.pdf</url>
      <abstract>This paper describes the automatic extraction of French subcategorization frames from corpora. The subcategorization frames have been acquired via VISL, a dependency-based parser (Bick 2003), whose verb lexicon is currently incomplete with respect to subcategorization frames. Therefore, we have implemented binomial hypothesis testing as a post-parsing filtering step. On a test set of 104 frequent verbs we achieve lower bounds on type precision at 86.8% and on token recall at 54.3%. These results show that, contra (Korhonen et al. 2000), binomial hypothesis testing can be robust for determining subcategorization frames given corpus data. Additionally, we estimate that our extracted subcategorization frames account for 85.4% of all frames in French corpora. We conclude that using a language resource, such as the VISL parser, with a currently unevaluated (and potentially high) error rate can yield robust results in conjunction with probabilistic filtering of the resource output.</abstract>
    </paper>
    <paper id="53">
      <author><first>Yuki</first><last>Irie</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <author><first>Nobuo</first><last>Kawaguchi</last></author>
      <author><first>Yukiko</first><last>Yamaguchi</last></author>
      <author><first>Yasuyoshi</first><last>Inagaki</last></author>
      <title>Layered Speech-Act Annotation for Spoken Dialogue Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/103_pdf.pdf</url>
      <abstract>This paper describes the design of speech act tags for spoken dialogue corpora and its evaluation. Compared with the tags used for conventional corpus annotation, the proposed speech intention tag is specialized enough to determine system operations. However, detailed information description increases tag types. This causes an ambiguous tag selection. Therefore, we have designed an organization of tags, with focusing attention on layered tagging and context-dependent tagging. Over 35,000 utterance units in the CIAIR corpus have been tagged by hand. To evaluate the reliability of the intention tag, a tagging experiment was conducted. The reliability of tagging is evaluated by comparing the tagging among some annotators using kappa value. As a result, we confirmed that reliable data could be built. This corpus with speech intention tag could be widely used from basic research to applications of spoken dialogue. In particular, this would play an important role from the viewpoint of practical use of spoken dialogue corpora.</abstract>
    </paper>
    <paper id="54">
      <author><first>Khurshid</first><last>Ahmad</last></author>
      <author><first>Craig</first><last>Bennett</last></author>
      <author><first>Tim</first><last>Oliver</last></author>
      <title>Visual Surveillance and Video Annotation and Description</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/104_pdf.pdf</url>
      <abstract>The effectiveness of CCTV surveillance networks is in part determined by their ability to perceive possible threats. Our traditional means for determining a level of threat has been to manually observe a situation through the network and take action as appropriate. The increasing scale of such surveillance networks has however made such an approach untenable, leading us look for a means by which processes may be automated. Here we investigate the language used by security experts in an attempt to look for patterns in the way in which they describe events as observed through a CCTV camera. It is suggested that natural language based descriptions of events may provide the basis for an index which may prove an important component for future automated surveillance systems.</abstract>
    </paper>
    <paper id="55">
      <author><first>Mathieu</first><last>Mangeot</last></author>
      <author><first>Antoine</first><last>Chalvin</last></author>
      <title>Dictionary Building with the Jibiki Platform: the <fixed-case>GDEF</fixed-case> case</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/105_pdf.pdf</url>
      <abstract>This paper presents the use of the âJibikiâ generic dictionary online development platform in the case of the GDEF Estonian-French bilingual dictionary building project. This platform has been developed mainly by Mathieu Mangeot and Gilles SÃÂ©rasset based on their research work in the domain. The platform is generic and thus can be used in (almost) any kind of dictionary development project from simple monolingual lexicons to complex multilingual pivot dictionaries as well as terminological resources. The platform is available online, thus it allows entry writers to work and collaborate from any part of the world. It consists in two main modules and data management tools. There is one module for elaborating complex queries on the data and one module for editing entries online. The editing modules generate automatically an interface from the XML structure of the entry.</abstract>
    </paper>
    <paper id="56">
      <author><first>Tomohiro</first><last>Ohno</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <author><first>Hideki</first><last>Kashioka</last></author>
      <author><first>Naoto</first><last>Kato</last></author>
      <author><first>Yasuyoshi</first><last>Inagaki</last></author>
      <title>A Syntactically Annotated Corpus of <fixed-case>J</fixed-case>apanese Spoken Monologue</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/106_pdf.pdf</url>
      <abstract>Recently, monologue data such as lecture and commentary by professionals have been considered as valuable intellectual resources, and have been gathering attention. On the other hand, in order to use these monologue data effectively and efficiently, it is necessary for the monologue data not only just to be accumulated but also to be structured. This paper describes the construction of a Japanese spoken monologue corpus in which dependency structure is given to each utterance. Spontaneous monologue includes a lot of very long sentences composed of two or more clauses. In these sentences, there may exist the subject or the adverb common to multi-clauses, and it may be considered that the subject or adverb depend on multi-predicates. In order to give the dependency information in a real fashion, our research allows that a bunsetsu depends on multiple bunsetsus.</abstract>
    </paper>
    <paper id="57">
      <author><first>Jerneja Žganec</first><last>Gros</last></author>
      <author><first>Varja</first><last>Cvetko-Orešnik</last></author>
      <author><first>Primož</first><last>Jakopin</last></author>
      <author><first>Aleš</first><last>Mihelič</last></author>
      <title><fixed-case>SI</fixed-case>-<fixed-case>PRON</fixed-case>: A Pronunciation Lexicon for <fixed-case>S</fixed-case>lovenian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/111_pdf.pdf</url>
      <abstract>We present the efforts involved in designing SI-PRON, a comprehensive machine-readable pronunciation lexicon for Slovenian. It has been built from two sources and contains all the lemmas from the Dictionary of Standard Slovenian (SSKJ), the most frequent inflected word forms found in contemporary Slovenian texts, and a first pass of inflected word forms derived from SSKJ lemmas. The lexicon file contains the orthography, corresponding pronunciations, lemmas and morphosyntactic descriptors of lexical entries in a format based on requirements defined by the W3C Voice Browser Activity. The current version of the SI-PRON pronunciation lexicon contains over 1.4 million lexical entries. The word list determination procedure, the generation and validation of phonetic transcriptions, and the lexicon format are described in the paper. Along with Onomastica, SI-PRON presents a valuable language resource for linguistic studies and research of speech technologies for Slovenian. The lexicon is already being used by the AlpSynth Slovenian text-to-speech synthesis system and for generating audio samples of the SSKJ word list.</abstract>
    </paper>
    <paper id="58">
      <author><first>Silvie</first><last>Cinková</last></author>
      <title>From <fixed-case>P</fixed-case>rop<fixed-case>B</fixed-case>ank to <fixed-case>E</fixed-case>ng<fixed-case>V</fixed-case>al<fixed-case>L</fixed-case>ex: Adapting the <fixed-case>P</fixed-case>rop<fixed-case>B</fixed-case>ank-Lexicon to the Valency Theory of the Functional Generative Description</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/112_pdf.pdf</url>
      <abstract>EngValLex is the name of an FGD-compliant valency lexicon of English verbs, built from the PropBank-Lexicon and following the structure of Vallex, the FGD-based lexicon of Czech verbs. EngValLex is interlinked with the PropBank-Lexicon, thus preserving the original links between the PropBank-Lexicon and the PropBank-Corpus. Therefore it is also supposed to be part of corpus annotation. This paper describes the automatic conversion of the PropBank-Lexicon into Pre-EngValLex, as well as the progress of its subsequent manual refinement (EngValLex). At the start, the Propbank-arguments were automatically re-labeled with functors (semantic labels of FGD) and the PropBank-rolesets were split into the respective example sentences, which became FGD-valency frames of Pre-EngValLex. Human annotators check and correct the labels and make the preliminary valency frames FGD-compliant. The most essential theoretical difference between the original and EngValLex is the syntactic alternations used by the PropBank-Lexicon, not yet employed within the Czech framework. The alternation-based approach substantially affects the conception of the frame, making in very different from the one applied within the FGD-framework. Preserving the valuable alternation information required special linguistic rules for keeping, altering and re-merging the automatically generated preliminary valency frames.</abstract>
    </paper>
    <paper id="59">
      <author><first>Tonio</first><last>Wandmacher</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <title>Training Language Models without Appropriate Language Resources: Experiments with an <fixed-case>AAC</fixed-case> System for Disabled People</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/113_pdf.pdf</url>
      <abstract>Statistical Language Models (LM) are highly dependent on their training resources. This makes it not only difficult to interpret evaluation results, it also has a deteriorating effect on the use of an LM-based application. This question has already been studied by others. Considering a specific domain (text prediction in a communication aid for handicapped people) we want to address the problem from a different point of view: the influence of the language register. Considering corpora from five different registers, we want to discuss three methods to adapt a language model to its actual language resource ultimately reducing the effect of training dependency: (a) A simple cache model augmenting the probability of the n last inserted words; (b) a user dictionary, keeping every unseen word; and (c) a combined LM interpolating a base model with a dynamically updated user model. Our evaluation is based on the results obtained from a text prediction system working on a trigram LM.</abstract>
    </paper>
    <paper id="60">
      <author><first>Brian</first><last>Roark</last></author>
      <author><first>Mary</first><last>Harper</last></author>
      <author><first>Eugene</first><last>Charniak</last></author>
      <author><first>Bonnie</first><last>Dorr</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <author><first>Jeremy</first><last>Kahn</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <author><first>John</first><last>Hale</last></author>
      <author><first>Anna</first><last>Krasnyanskaya</last></author>
      <author><first>Matthew</first><last>Lease</last></author>
      <author><first>Izhak</first><last>Shafran</last></author>
      <author><first>Matthew</first><last>Snover</last></author>
      <author><first>Robin</first><last>Stewart</last></author>
      <author><first>Lisa</first><last>Yung</last></author>
      <title><fixed-case>SP</fixed-case>arseval: Evaluation Metrics for Parsing Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/116_pdf.pdf</url>
      <abstract>While both spoken and written language processing stand to benefit from parsing, the standard Parseval metrics (Black et al., 1991) and their canonical implementation (Sekine and Collins, 1997) are only useful for text. The Parseval metrics are undefined when the words input to the parser do not match the words in the gold standard parse tree exactly, and word errors are unavoidable with automatic speech recognition (ASR) systems. To fill this gap, we have developed a publicly available tool for scoring parses that implements a variety of metrics which can handle mismatches in words and segmentations, including: alignment-based bracket evaluation, alignment-based dependency evaluation, and a dependency evaluation that does not require alignment. We describe the different metrics, how to use the tool, and the outcome of an extensive set of experiments on the sensitivity.</abstract>
    </paper>
    <paper id="61">
      <author><first>Ming-Shun</first><last>Lin</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <title>Constructing a Named Entity Ontology from Web Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/118_pdf.pdf</url>
      <abstract>This paper proposes a named entity (NE) ontology generation engine, called XNE-Tree engine, which produces relational named entities by given a seed. The engine incrementally extracts high co-occurring named entities with the seed by using a common search engine. In each iterative step, the seed will be replaced by its siblings or descendants, which form new seeds. In this way, XNE-Tree engine will build a tree structure with the original seed as a root incrementally. Two seeds, Chinese transliteration names of Nicole Kidman (a famous actress) and Ernest Hemingway (a famous writer), are experimented to evaluate the performance of the XNE-Tree.¡@¡@For test the applicability of the ontology, we employ it to a phoneme-character conversion system, which convert input phoneme syllable sequences to text strings. Total 100 Chinese transliteration names, including 50 person names and 50 location names are used as test data. We derive an ontology composed of 7,642 named entities. The results of phoneme-character conversion show that both the recall rate and the MRR are improved from 0.79 and 0.50 to 0.84 to 0.55, respectively.</abstract>
    </paper>
    <paper id="62">
      <author><first>Flora Ramírez</first><last>Bustamante</last></author>
      <author><first>Enrique López</first><last>Díaz</last></author>
      <title>Spelling Error Patterns in <fixed-case>S</fixed-case>panish for Word Processing Applications</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/119_pdf.pdf</url>
      <abstract>This paper reports findings from the elaboration of a typology of spelling errors for Spanish. It also discusses previous generalizations about spelling error patterns found in other studies and offers new insights on them. The typology is based on the analysis of around 76K misspellings found in real-life texts produced by humans. The main goal of the elaboration of the typology was to help in the im-plementation of a spell checker that detects context-independent misspellings in general unrestricted texts with the most common con-fusion pairs (i.e. error/correction pairs) to improve the set of ranked correction candidates for misspellings. We found that spelling er-rors are language dependent and are closely related to the orthographic rules of each language. The statistical data we provide on spell-ing error patterns in Spanish and their comparison with other data in other related works are the novel contribution of this paper. In this line, this paper shows that some of the general statements found in the literature about spelling error patterns apply mainly to English and cannot be extrapolated to other languages.</abstract>
    </paper>
    <paper id="63">
      <author><first>Anders</first><last>Nøklestad</last></author>
      <author><first>Øystein</first><last>Reigem</last></author>
      <author><first>Christer</first><last>Johansson</last></author>
      <title>Developing a re-usable web-demonstrator for automatic anaphora resolution with support for manual editing of coreference chains</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/124_pdf.pdf</url>
      <abstract>Automatic markup and editing of anaphora and coreference is performed within one system. The processing is trained using memory based learning, and representations derive from various lexical resources. The current model reaches an expected combined precision and recall of F=62. The further improvement of the coreference detection is work in progress. Editing of coreference is separated into a module working on an xml-file. The editing mechanism can thus be reused in other projects. The editor is designed to store a copy on the server of all files that are edited over the internet using our demonstrator. This might help us to expand our database of texts annotated for anaphora and coreference. Further research includes creating high coverage lexical resources, and modules for other languages. The current system is trained on Norwegian bokm°al, but we hope to extend this to other languages with available tools (e.g. POS-taggers).</abstract>
    </paper>
    <paper id="64">
      <author><first>Hitomi</first><last>Tohyama</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <title>Collection of Simultaneous Interpreting Patterns by Using Bilingual Spoken Monologue Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/125_pdf.pdf</url>
      <abstract>The manual quantitative analysis of CIAIR simultaneous interpretation corpus and the collection of interpreting patterns This paper provides an investigation of simultaneous interpreting patterns using a bilingual spoken monologue corpus. 4,578 pairs of English-Japanese aligned utterances in CIAIR simultaneous interpretation database were used. This investigation is the largest scale as the observation of simultaneous interpreting speech. The simultaneous interpreters are required to generate the target speech simultaneously with the source speech. Therefore, they have various kinds of strategies to raise simultaneity. In this investigation, the simultaneous interpreting patterns with high frequency and high flexibility were extracted from the corpus. As a result, we collected 203 cases out of aligned utterances in which simultaneous interpretersf strategies for raising simultaneity were observed. These 203 cases could be categorized into 12 types of interpreting pattern. It was clarified that 4.5 percent of the English-Japanese monologue data were fitted in those interpreting patterns. These interpreting patterns can be expected to be used as interpreting rules of simultaneous machine interpretation.</abstract>
    </paper>
    <paper id="65">
      <author><first>Shuichi</first><last>Itahashi</last></author>
      <author><first>Chiu-yu</first><last>Tseng</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <title>Oriental <fixed-case>COCOSDA</fixed-case>: Past, Present and Future</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/127_pdf.pdf</url>
      <abstract>The purpose of Oriental COCOSDA is to exchange ideas, to share information and to discuss regional matters on creation, utilization, dissemination of spoken language corpora of oriental languages and also on the assessment methods of speech recognition/synthesis systems as well as to promote speech research on oriental languages. A series of International Workshop on East Asian Language Resources and Evaluation (EALREW) or Oriental COCOSDA Workshop has been held annually since the preparatory meeting held in 1997. After that, we have had a series of workshops every year in Japan, Taiwan, China, Korea, Thailand, Singapore, India and Indonesia. The Oriental COCOSDA is managed by a convener, three advisory members, and 21 representatives from ten regions in Oriental countries. We need much more Pan-Asia collaboration with research organizations and consortia, though there are some domestic activities in Oriental countries. We note that speech research has become popular gradually in Oriental countries including Malaysia, Vietnam, Xinjang Uygur Autonomous Region of China, etc. We plan to hold future Oriental COCOSDA meetings in these places in order to promote speech research there.</abstract>
    </paper>
    <paper id="66">
      <author><first>Angelika</first><last>Storrer</last></author>
      <author><first>Sandra</first><last>Wellinghoff</last></author>
      <title>Automated detection and annotation of term definitions in <fixed-case>G</fixed-case>erman text corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/128_pdf.pdf</url>
      <abstract>We describe an approach to automatically detect and annotate definitions for technical terms in German text corpora. This approach focuses on verbs that typically appear in definitions (= definitor verbs). We specify search patterns based on the valency frames of these definitor verbs and use them (1) to detect and delimit text segments containing definitions and (2) to annotate their main functional components: the definiendum (the term that is defined) and the definiens (meaning postulates for this term). On the basis of these annotations we aim at automatically extracting WordNet-style semantic relations that hold between the head nouns of the definiendum and the head nouns of the definiens. In this paper, we will describe our annotation scheme for definitions and report on two studies: (1) a pilot study that evaluates our definition extraction approach using a German corpus with manually annotated definitions as a gold standard. (2) A feasibility study that evaluates the possibility to extract hypernym, hyponym and holonym relations from these annotated definitions.</abstract>
    </paper>
    <paper id="67">
      <author id="mustafa-yaseen"><first>M.</first><last>Yaseen</last></author>
      <author id="mohamed-attia"><first>M.</first><last>Attia</last></author>
      <author id="bente-maegaard"><first>B.</first><last>Maegaard</last></author>
      <author id="khalid-choukri"><first>K.</first><last>Choukri</last></author>
      <author id="niklas-paulsson"><first>N.</first><last>Paulsson</last></author>
      <author id="salah-haamid"><first>S.</first><last>Haamid</last></author>
      <author id="steven-krauwer"><first>S.</first><last>Krauwer</last></author>
      <author id="chomicha-bendahman"><first>C.</first><last>Bendahman</last></author>
      <author id="hanne-fersoe"><first>H.</first><last>Fersøe</last></author>
      <author id="mohsen-rashwan"><first>M.</first><last>Rashwan</last></author>
      <author id="bassam-haddad"><first>B.</first><last>Haddad</last></author>
      <author id="chafic-mukbel"><first>C.</first><last>Mukbel</last></author>
      <author id="abdelhak-mouradi"><first>A.</first><last>Mouradi</last></author>
      <author id="adil-al-kufaishi"><first>A.</first><last>Al-Kufaishi</last></author>
      <author id="mostafa-shahin"><first>M.</first><last>Shahin</last></author>
      <author id="noureddine-chenfour"><first>N.</first><last>Chenfour</last></author>
      <author id="ahmed-ragheb"><first>A.</first><last>Ragheb</last></author>
      <title>Building Annotated Written and Spoken <fixed-case>A</fixed-case>rabic <fixed-case>LR</fixed-case>s in <fixed-case>NEMLAR</fixed-case> Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/131_pdf.pdf</url>
      <abstract>The NEMLAR project: Network for Euro-Mediterranean LAnguage Resource and human language technology development and support (www.nemlar.org) was a project supported by the EC with partners from Europe and Arabic countries, whose objective is to build a network of specialized partners to promote and support the development of Arabic Language Resources (LRs) in the Mediterranean region. The project focused on identifying the state of the art of LRs in the region, assessing priority requirements through consultations with language industry and communication players, and establishing a protocol for developing and identifying a Basic Language Resource Kit (BLARK) for Arabic, and to assess first priority requirements. The BLARK is defined as the minimal set of language resources that is necessary to do any pre-competitive research and education, in addition to the development of crucial components for any future NLP industry. Following the identification of high priority resources the NEMLAR partners agreed to focus on, and produce three main resources, which are 1) Annotated Arabic written corpus of about 500 K words, 2) Arabic speech corpus for TTS applications of 2x5 hours, and 3) Arabic broadcast news speech corpus of 40 hours Modern Standard Arabic. For each of the resources underlying linguistic models and assumptions of the corpus, technical specifications, methodologies for the collection and building of the resources, validation and verification mechanisms were put and applied for the three LRs.</abstract>
    </paper>
    <paper id="68">
      <author><first>Sašo</first><last>Džeroski</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <author><first>Nina</first><last>Ledinek</last></author>
      <author><first>Petr</first><last>Pajas</last></author>
      <author><first>Zdenek</first><last>Žabokrtsky</last></author>
      <author><first>Andreja</first><last>Žele</last></author>
      <title>Towards a <fixed-case>S</fixed-case>lovene Dependency Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/133_pdf.pdf</url>
      <abstract>The paper presents the initial release of the Slovene Dependency Treebank, currently containing 2000 sentences or 30.000 words. Ourapproach to annotation is based on the Prague Dependency Treebank, which serves as an excellent model due to the similarity of the languages, the existence of a detailed annotation guide and an annotation editor. The initial treebank contains a portion of theMULTEXT-East parallel word-level annotated corpus, namely the firstpart of the Slovene translation of Orwell's 1984. This corpus was first parsed automatically, to arrive at the initial analytic level dependency trees. These were then hand corrected using the tree editorTrEd; simultaneously, the Czech annotation manual was modified forSlovene. The current version is available in XML/TEI, as well asderived formats, and has been used in a comparative evaluation using the MALT parser, and as one of the languages present in the CoNLL-Xshared task on dependency parsing. The paper also discusses further work, in the first instance the composition of the corpus to be annotated next.</abstract>
    </paper>
    <paper id="69">
      <author><first>Canasai</first><last>Kruengkrai</last></author>
      <author><first>Virach</first><last>Sornlertlamvanich</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>A Conditional Random Field Framework for <fixed-case>T</fixed-case>hai Morphological Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/137_pdf.pdf</url>
      <abstract>This paper presents a framework for Thai morphological analysis based on the theoretical background of conditional random fields. We formulate morphological analysis of an unsegmented language as the sequential supervised learning problem. Given a sequence of characters, all possibilities of word/tag segmentation are generated, and then the optimal path is selected with some criterion. We examine two different techniques, including the Viterbi score and the confidence estimation. Preliminary results are given to show the feasibility of our proposed framework.</abstract>
    </paper>
    <paper id="70">
      <author><first>Yoshihide</first><last>Kato</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <author><first>Yasuyoshi</first><last>Inagaki</last></author>
      <title>A Corpus Search System Utilizing Lexical Dependency Structure</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/138_pdf.pdf</url>
      <abstract>This paper presents a corpus search system utilizing lexical dependency structure. The user's query consists of lexical dependency structure. The user's query consists of a sequence of keywords. For a given query, the system automatically generates the dependency structure patterns which consist of keywords in the query, and returns the sentences whose dependency structures match the generated patterns. The dependency structure patterns are generated by using two operations: combining and interpolation, which utilize dependency structures in the searched corpus. The operations enable the system to generate only the dependency structure patterns that occur in the corpus. The system achieves simple and intuitive corpus search and it is enough linguistically sophisticated to utilize structural information.</abstract>
    </paper>
    <paper id="71">
      <author><first>Peter</first><last>Berck</last></author>
      <author><first>Albert</first><last>Russel</last></author>
      <title><fixed-case>ANNEX</fixed-case> - a web-based Framework for Exploiting Annotated Media Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/139_pdf.pdf</url>
      <abstract>Manual annotation of various media streams, time series data and also text sequences is still a very time consuming work that has to be carried out in many areas of linguistics and beyond. Based on many theoretical discussions and practical experiences professional tools have been deployed such as ELAN that support the researcher in his/her work. Most of these annotation tools operate on local computers. However, since more and more language resources are stored in web-accessible archives, researchers want to take profit from the new possibilities. ANNEX was developed to fill this gap, since it allows web-based analysis of complex annotated media streams, i.e., the users dont have to download resources and dont have to download and install programs. By simply using a normal web-browser they can start their linguistic work. Yet, due to the architecture of the Internet, ANNEX does not offer the options to create annotations, but this feature will come. However, users have to be aware of the fact that media streaming does not offer that high accuracy as on local computers.</abstract>
    </paper>
    <paper id="72">
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <title>The <fixed-case>E</fixed-case>nglish-<fixed-case>S</fixed-case>lovene <fixed-case>ACQUIS</fixed-case> corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/140_pdf.pdf</url>
      <abstract>The paper presents the SVEZ-IJS corpus, a large parallel annotated English-Slovene corpus containing translated legal texts of the European Union, the ACQUIS Communautaire. The corpus contains approx. 2 x 5 million words and was compiled from the translation memory obtained from the Translation Unit of the Slovene Government Office for European Affairs. The corpus is encoded in XML, accordingto the Text Encoding Initiative Guidelines TEI P4, where each translation memory unit contains useful metadata and the two aligned segments (sentences). Both the Slovene and English text islinguistically annotated at the word-level, by context disambiguatedlemmas and morphosyntactic descriptions, which follow the MULTEXTguidelines. The complete corpus is freely available for research, either via an on-line concordancer, or for downloading from the corpushome page at http://nl.ijs.si/svez/.</abstract>
    </paper>
    <paper id="73">
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Andreas</first><last>Claus</last></author>
      <author><first>Freddy</first><last>Offenga</last></author>
      <author><first>Romuald</first><last>Skiba</last></author>
      <author><first>Paul</first><last>Trilsbeek</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <title><fixed-case>LAMUS</fixed-case>: the Language Archive Management and Upload System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/141_pdf.pdf</url>
      <abstract>Language Archiving, Resource Management LAMUS is a web-based service that allows researchers to deposit their language resources into a language resources archive. It was developed at the MPI for Psycholinguistics for stricter control of the archive coherence and consistency and allowing wider use of the archiving facilities without increasing the workload for archive and corpus managers. LAMUS is based on the use of IMDI metadata standard for language resources and offers metadata search and browsing over the archive.</abstract>
    </paper>
    <paper id="74">
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Freddy</first><last>Offenga</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <author><first>Peter</first><last>van der Kamp</last></author>
      <author><first>David</first><last>Nathan</last></author>
      <author><first>Sven</first><last>Strömqvist</last></author>
      <title>Technologies for a Federation of Language Resource Archives</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/142_pdf.pdf</url>
      <abstract>The DAM-LR project aims at virtually integrating various European language resource archives that allow users to navigate and operate in a single unified domain of language resources. This type of integration introduces Grid technology to the humanities disciplines and forms a federation of archives. It is the basis for establishing a research infrastructure for language resources which will finally enable eHumanities. Currently, the complete architecture is designed based on a few well-known components and some components are already tested. Based on the technological insights gathered and due to discussions within the international DELAMAN network the ethical and organizational basis for such a federation is defined.</abstract>
    </paper>
    <paper id="75">
      <author><first>Marc</first><last>Kemps-Snijders</last></author>
      <author><first>Julien</first><last>Ducret</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <title>An <fixed-case>API</fixed-case> for accessing the Data Category Registry</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/146_pdf.pdf</url>
      <abstract>Central Ontologies are increasingly important to manage interoperability between different types of language resources. This was the reason for ISO to set up a new committee ISO TC37/SC4 taking care of language resource management issues. Central to the work of this committee is the definition of a framework for a central registry of data categories that are important in the domain of language resources. This paper describes an application programming interface that was designed to request services from this data category registry. The DCR is operational and the described API has already been tested from a lexicon application.</abstract>
    </paper>
    <paper id="76">
      <author><first>Peter</first><last>Wittenburg</last></author>
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Wolfgang</first><last>Klein</last></author>
      <author><first>Stephen</first><last>Levinson</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <title>Foundations of Modern Language Resource Archives</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/147_pdf.pdf</url>
      <abstract>A number of serious reasons will convince an increasing amount of researchers to store their relevant material in centers which we will call "language resource archives". They combine the duty of taking care of long-term preservation as well as the task to give access to their material to different user groups. Access here is meant in the sense that an active interaction with the data will be made possible to support the integration of new data, new versions or commentaries of all sorts. Modern Language Resource Archives will have to adhere to a number of basic principles to fulfill all requirements and they will have to be involved in federations to create joint language resource domains making it even simpler for the researchers to access the data. This paper makes an attempt to formulate the essential pillars language resource archives have to adhere to.</abstract>
    </paper>
    <paper id="77">
      <author><first>Freddy</first><last>Offenga</last></author>
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <author><first>Julien</first><last>Ducret</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <title>Metadata Profile in the <fixed-case>ISO</fixed-case> Data Category Registry</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/148_pdf.pdf</url>
      <abstract>Metadata descriptions of language resources become an increasing necessity since the shear amount of language resources is increasing rapidly and especially since we are now creating infrastuctures to access these resources via the web through integrated domains of language resource archives. Yet, the metadata frameworks offered for the domain of language resources (IMDI and OLAC), although mature, are not as widely accepted as necessary. The lack of confidence in the stability and persistence of the concepts and formats introduced by these metadata sets seems to be one argument for people to not invest the time needed for metadata creation. The introduction of these concepts into an ISO standardization process may convince contributors to make use of the terminology. The availability of the ISO Data Category Registry that includes a metadata profile will also offer the opportunity for researchers to construct their own metadata set tailored to the needs of the project at hand, but nevertheless supporting interoperability.</abstract>
    </paper>
    <paper id="78">
      <author><first>Marc</first><last>Kemps-Snijders</last></author>
      <author><first>Mark-Jan</first><last>Nederhof</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <title><fixed-case>LEXUS</fixed-case>, a web-based tool for manipulating lexical resources lexicon</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/149_pdf.pdf</url>
      <abstract>LEXUS provides a flexible framework for the maintaining lexical structure and content. It is the first implementation of the Lexical Markup Framework model currently being developed at ISO TC37/SC4. Amongst its capabilities are the possibility to create lexicon structures, manipulate content and use of typed relations. Integration of well established Data Category Registries is supported to further promote interoperability by allowing access to well established linguistic concepts. Advanced linguistic functionality is offered to assist users in cross lexica operations such as search and comparison and merging of lexica. To enable use within various user groups the look and feel of each lexicon may be customized. In the near future more functionality will be added including integration with other tools accessing lexical content.</abstract>
    </paper>
    <paper id="79">
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <author><first>Darja</first><last>Fišer</last></author>
      <title>Building <fixed-case>S</fixed-case>lovene <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/150_pdf.pdf</url>
      <abstract>A WordNet is a lexical database in which nouns, verbs, adjectives and adverbs are organized in a conceptual hierarchy, linking semantically and lexically related concepts. Such semantic lexicons have become oneof the most valuable resources for a wide range of NLP research and applications, such as semantic tagging, automatic word-sense disambiguation, information retrieval and document summarisation. Following the WordNet design for the English languagedeveloped at Princeton, WordNets for a number of other languages havebeen developed in the past decade, taking the idea into the domain ofmultilingual processing. This paper reports on the prototype SloveneWordNet which currently contains about 5,000 top-level concepts. Theresource has been automatically translated from the Serbian WordNet, with the help of a bilingual dictionary, synset literals ranked according to the frequency of corpus occurrence, and results manually corrected. The paper presents the results obtained, discusses some problems encountered along the way and points out some possibilitiesof automated acquisition and refinement of synsets in the future.</abstract>
    </paper>
    <paper id="80">
      <author><first>Luciana</first><last>Bordoni</last></author>
      <author><first>Tiziana</first><last>Mazzoli</last></author>
      <title>Towards an Ontology for Art and Colours</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/151_pdf.pdf</url>
      <abstract>To meet a variety of needs in information modeling, software development and integration as well as knowledge management and reuse, various groups within industry, academia, and government have been developing and deploying sharable and reusable models known as ontologies. Ontologies play an important role in knowledge representation. In this paper, we address the problem of capturing knowledge needed for indexing and retrieving art resources. We describe a case study in which we attempt to construct an ontology for a subset of art. The aim of the present ontology is to build an extensible repository of knowledge and information about artists, their works and materials used in artistic creations. Influenced by the recent interest in colours and colouring materials, mainly shared by French researchers and linguists, an ontology prototype has been developed using Protégé. It allows to organize and catalog information about artists, art works, colouring materials and related colours.</abstract>
    </paper>
    <paper id="81">
      <author><first>Richard</first><last>Johansson</last></author>
      <author><first>Pierre</first><last>Nugues</last></author>
      <title>Construction of a <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et Labeler for <fixed-case>S</fixed-case>wedish Text</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/152_pdf.pdf</url>
      <abstract>We describe the implementation of a FrameNet-based semantic role labeling system for Swedish text. To train the system, we used a semantically annotated corpus that was produced by projection across parallel corpora. As part of the system, we developed two frame element bracketing algorithms that are suitable when no robust constituent parsers are available. Apart from being the first such system for Swedish, this is, as far as we are aware, the first semantic role labeling system for a language for which no role-semantic annotated corpora are available. The estimated accuracy of classification of pre-segmented frame elements is 0.75, and the precision and recall measures for the complete task are 0.67 and 0.47, respectively.</abstract>
    </paper>
    <paper id="82">
      <author><first>Peter</first><last>Wittenburg</last></author>
      <author><first>Hennie</first><last>Brugman</last></author>
      <author><first>Albert</first><last>Russel</last></author>
      <author><first>Alex</first><last>Klassmann</last></author>
      <author><first>Han</first><last>Sloetjes</last></author>
      <title><fixed-case>ELAN</fixed-case>: a Professional Framework for Multimodality Research</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/153_pdf.pdf</url>
      <abstract>Utilization of computer tools in linguistic research has gained importance with the maturation of media frameworks for the handling of digital audio and video. The increased use of these tools in gesture, sign language and multimodal interaction studies has led to stronger requirements on the flexibility, the efficiency and in particular the time accuracy of annotation tools. This paper describes the efforts made to make ELAN a tool that meets these requirements, with special attention to the developments in the area of time accuracy. In subsequent sections an overview will be given of other enhancements in the latest versions of ELAN that makes it a useful tool in multimodality research.</abstract>
    </paper>
    <paper id="83">
      <author><first>Peter</first><last>Berck</last></author>
      <author><first>Hans-Jörg</first><last>Bibiko</last></author>
      <author><first>Marc</first><last>Kemps-Snijders</last></author>
      <author><first>Albert</first><last>Russel</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <title>Ontology-based Language Archive Utilization</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/154_pdf.pdf</url>
      <abstract>At the MPI for Psycholinguistics a large archive with language resources has been created with contributions from many different individual researchers and research projects. All of these resources, in particular annotated media streams and multimedia lexica, are accessible via the web and can be utilized with the help of web-based utilization frameworks. Therefore, the archive lends itself to motivate users to operate across the boundaries of single corpora and to support cross-language work. This, however, can only be done when the problems of interoperability, in particular at the level of linguistic encoding, can be solved in an efficient way. Two Max-Planck-Institutes are cooperating to build a framework that allows users to easily create their own practical ontologies and if wanted to relate their concepts to central ontologies.</abstract>
    </paper>
    <paper id="84">
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Johan</first><last>Hall</last></author>
      <author><first>Jens</first><last>Nilsson</last></author>
      <title><fixed-case>M</fixed-case>alt<fixed-case>P</fixed-case>arser: A Data-Driven Parser-Generator for Dependency Parsing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/162_pdf.pdf</url>
      <abstract>We introduce MaltParser, a data-driven parser generator for dependency parsing. Given a treebank in dependency format, MaltParser can be used to induce a parser for the language of the treebank. MaltParser supports several parsing algorithms and learning algorithms, and allows user-defined feature models, consisting of arbitrary combinations of lexical features, part-of-speech features and dependency features. MaltParser is freely available for research and educational purposes and has been evaluated empirically on Swedish, English, Czech, Danish and Bulgarian.</abstract>
    </paper>
    <paper id="85">
      <author><first>Andrej</first><last>Žgank</last></author>
      <author><first>Darinka</first><last>Verdonik</last></author>
      <author><first>Aleksandra Zögling</first><last>Markuš</last></author>
      <author><first>Zdravko</first><last>Kačič</last></author>
      <title><fixed-case>SINOD</fixed-case> - <fixed-case>S</fixed-case>lovenian non-native speech database</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/163_pdf.pdf</url>
      <abstract>This paper presents the SINOD database, which is the first Slovenian non-native speech database. It will be used to improve the performance of large vocabulary continuous speech recogniser for non-native speakers. The main quality impact is expected for acoustic models and recognisers vocabulary. The SINOD database is designed as supplement to the Slovenian BNSI Broadcast News database. The same BN recommendations were used for both databases. Two interviews with non-native Slovenian speakers were incorporated in the set. Both non-native speakers were female, whereas the journalist was Slovenian native male speaker. The transcription approach applied in the production phase is presented. Different statistics and analyses of database are given in the paper.</abstract>
    </paper>
    <paper id="86">
      <author><first>Mark</first><last>van Assem</last></author>
      <author><first>Aldo</first><last>Gangemi</last></author>
      <author><first>Guus</first><last>Schreiber</last></author>
      <title>Conversion of <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et to a standard <fixed-case>RDF</fixed-case>/<fixed-case>OWL</fixed-case> representation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/165_pdf.pdf</url>
      <abstract>This paper presents an overview of the work in progress at the W3C to produce a conversion of WordNet to the RDF/OWL representation language in use in the Semantic Web community. Such a standard representation is useful to provide application developers a high-quality resource and to promote interoperability. Important requirements in this conversion process are that it should be complete and should stay close to WordNet's conceptual model. The paper explains the steps taken to produce the conversion and details design decisions such as the composition of the class hierarchy and properties, the addition of suitable OWL semantics and the chosen format of the URIs. Additional topics include a strategy to incorporate OWL and RDFS semantics in one schema such that both RDF(S) infrastructure and OWL infrastructure can interpret the information correctly, problems encountered in understanding the Prolog source files and the description of the two versions that are provided (Basic and Full) to accommodate different usages of WordNet.</abstract>
    </paper>
    <paper id="87">
      <author><first>Antal</first><last>van den Bosch</last></author>
      <author><first>Ineke</first><last>Schuurman</last></author>
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <title>Transferring <fixed-case>P</fixed-case>o<fixed-case>S</fixed-case>-tagging and lemmatization tools from spoken to written <fixed-case>D</fixed-case>utch corpus development</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/167_pdf.pdf</url>
      <abstract>We describe a case study in the reuse and transfer of tools in language resource development, from a corpus of spoken Dutch to a corpus of written Dutch. Once tools for a particular language have been developed, it is logical, but not trivial to reuse them for other types or registers of the language than the tools were originally designed for. This paper reviews the decisions and adaptations necessary to make this particular transfer from spoken to written language, focusing on a part-of-speech tagger and a lemmatizer. While the lemmatizer can be transferred fairly straightforwardly, the tagger needs to be adaptated considerably. We show how it can be adapted without starting from scratch. We describe how the part-of-speech tagset was adapted and how the tagger was retrained to deal with written-text phenomena it had not been trained on earlier.</abstract>
    </paper>
    <paper id="88">
      <author><first>Mark</first><last>Przybocki</last></author>
      <author><first>Gregory</first><last>Sanders</last></author>
      <author><first>Audrey</first><last>Le</last></author>
      <title>Edit Distance: A Metric for Machine Translation Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/168_pdf.pdf</url>
      <abstract>NIST has coordinated machine translation (MT) evaluations for several years using an automatic and repeatable evaluation measure. Under the Global Autonomous Language Exploitation (GALE) program, NIST is tasked with implementing an edit-distance-based evaluation of MT. Here edit distance is defined to be the number of modifications a human editor is required to make to a system translation such that the resulting edited translation contains the complete meaning in easily understandable English, as a single high-quality human reference translation. In preparation for this change in evaluation paradigm, NIST conducted two proof-of-concept exercises specifically designed to probe the data space, to answer questions related to editor agreement, and to establish protocols for the formal GALE evaluations. We report here our experimental design, the data used, and our findings for these exercises.</abstract>
    </paper>
    <paper id="89">
      <author><first>Niels Ole</first><last>Bernsen</last></author>
      <author><first>Laila</first><last>Dybkjær</last></author>
      <author><first>Svend</first><last>Kiilerich</last></author>
      <title><fixed-case>H</fixed-case>. <fixed-case>C</fixed-case>. Andersen Conversation Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/169_pdf.pdf</url>
      <abstract>This paper describes the design, collection and current status of the Hans Christian Andersen (HCA) conversation corpus. The corpus consists of five separate corpora and represents transcription and annotation of some 57 hours of English spoken and deictic gesture user-system interaction recorded mainly with children 2002-2005. The corpora were collected as part of the development and evaluation process of two consecutive research prototypes. The set-up used to collect each corpus is described as well as our use of each corpus in system development. We describe the annotation of each corpus and briefly present various uses we have made of the corpora so far. The HCA corpus was made publicly available at http://www.niceproject.com/data/ in March 2006.</abstract>
    </paper>
    <paper id="90">
      <author><first>Magnus</first><last>Sahlgren</last></author>
      <title>Towards pertinent evaluation methodologies for word-space models</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/170_pdf.pdf</url>
      <abstract>This paper discusses evaluation methodologies for a particular kind of meaning models known as word-space models, which use distributional information to assemble geometric representations of meaning similarities. Word-space models have received considerable attention in recent years, and have begun to see employment outside the walls of computational linguistics laboratories. However, the evaluation methodologies of such models remain infantile, and lack efforts at standardization. Very few studies have critically assessed the methodologies used to evaluate word spaces. This paper attempts to fill some of this void. It is the central goal of this paper to answer the question how can we determine whether a given word space is a good word space?</abstract>
    </paper>
    <paper id="91">
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <author><first>Paula</first><last>Estrella</last></author>
      <author><first>Margaret</first><last>King</last></author>
      <author><first>Nancy</first><last>Underwood</last></author>
      <title>A Model for Context-Based Evaluation of Language Processing Systems and its Application to Machine Translation Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/171_pdf.pdf</url>
      <abstract>In this paper, we propose a formal framework that takes into account the influence of the intended context of use of an NLP system on the procedure and the metrics used to evaluate the system. We introduce in particular the notion of a context-dependent quality model and explain how it can be adapted to a given context of use. More specifically, we define vector-space representations of contexts of use and of quality models, which are connected by a generic contextual quality model (GCQM). For each domain, experts in evaluation are needed to build a GCQM based on analytic knowledge and on previous evaluations, using the mechanism proposed here. The main inspiration source for this work is the FEMTI framework for the evaluation of machine translation, which implements partly the present model, and which is described briefly along with insights from other domains.</abstract>
    </paper>
    <paper id="92">
      <author><first>Martin</first><last>Forst</last></author>
      <author><first>Ronald M.</first><last>Kaplan</last></author>
      <title>The importance of precise tokenizing for deep grammars</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/172_pdf.pdf</url>
      <abstract>We present a non-deterministic finite-state transducer that acts as a tokenizer and normalizer for free text that is input to a broad-coverage LFG of German. We compare the basic tokenizer used in an earlier version of the grammar and the more sophisticated tokenizer that we now use. The revised tokenizer increases the coverage of the grammar in terms of full parses from 68.3% to 73.4% on sentences 8,001 through 10,000 of the TiGer Corpus.</abstract>
    </paper>
    <paper id="93">
      <author><first>Annalisa</first><last>Sandrelli</last></author>
      <author><first>Claudio</first><last>Bendazzoli</last></author>
      <title>Tagging a Corpus of Interpreted Speeches: the <fixed-case>E</fixed-case>uropean Parliament Interpreting Corpus (<fixed-case>EPIC</fixed-case>)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/174_pdf.pdf</url>
      <abstract>The performance of three different taggers (Treetagger, Freeling and GRAMPAL) is evaluated on three different languages, i.e. English, Italian and Spanish. The materials are transcripts from the European Parliament Interpreting Corpus (EPIC), a corpus of original (source) and simultaneously interpreted (target) speeches. Owing to the oral nature of our materials and to the specific characteristics of spoken language produced in simultaneous interpreting, the chosen taggers have to deal with non-standard word order, disfluencies and other features not to be found in written language. Parts of the tagged sub-corpora were automatically extracted in order to assess the success rate achieved in tagging and lemmatisation. Errors and problems are discussed for each tagger, and conclusions are drawn regarding future developments.</abstract>
    </paper>
    <paper id="94">
      <author><first>Hisami</first><last>Suzuki</last></author>
      <author><first>Gary</first><last>Kacmarcik</last></author>
      <title><fixed-case>R</fixed-case>ef<fixed-case>R</fixed-case>ef: A Tool for Viewing and Exploring Coreference Space</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/178_pdf.pdf</url>
      <abstract>We present RefRef, a tool for viewing and exploring coreference space, which is publicly available for research purposes. Unlike similar tools currently available whose main goal is to assist the annotation process of coreference links, RefRef is dedicated for viewing and exploring coreference-annotated data, whether manually tagged or automatically resolved. RefRef is also highly customizable, as the tool is being made available with the source code. In this paper we describe the main functionalities of RefRef as well as some possibilities for customization to meet the specific needs of the users of such coreference-annotated text.</abstract>
    </paper>
    <paper id="95">
      <author><first>Bernt</first><last>Andrassy</last></author>
      <author><first>Harald</first><last>Hoege</last></author>
      <title>Human and machine recognition as a function of <fixed-case>SNR</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/179_pdf.pdf</url>
      <abstract>In-car automatic speech recognition (ASR) is usually evaluated behaviour for different levels of noise. Yet this is interesting for car manufacturers in order to predict system performances for different speeds and different car models and thus allow to design speech based applications in a better way. It therefore makes sense to split the single WER into SNR dependent WERs, where SNR stands for the signal to noise ratio, which is an appropriate measure for the noise level. In this paper a SNR measure based on the concept of the Articulation Index is developed, which allows the direct comparison with human recognition performance.</abstract>
    </paper>
    <paper id="96">
      <author id="ruli-manurung"><first>R.</first><last>Manurung</last></author>
      <author id="dave-omara"><first>D.</first><last>O’Mara</last></author>
      <author id="helen-pain"><first>H.</first><last>Pain</last></author>
      <author id="graeme-ritchie"><first>G.</first><last>Ritchie</last></author>
      <author id="annalu-waller"><first>A.</first><last>Waller</last></author>
      <title>Building a Lexical Database for an Interactive Joke-Generator</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/180_pdf.pdf</url>
      <abstract>As part of a project to construct an interactive program which will encourage children to play with language by building jokes, we have developed a large lexical database, closely based on WordNet. As well as the standard WordNet information about part of speech, synonymy, hyponymy, etc, we have added phonetic representations and symbolic links allowing attachment of pictures. All information is represented in a relational database, allowing powerful searches using SQL via a Java API. The lexicon has a facility to label subsets of the lexicon with symbolic names, and we are working to incorporate some educationally relevant word lists as sublexicons. This should also allow us to improve the familiarity ratings which the lexicon assigns to words.</abstract>
    </paper>
    <paper id="97">
      <author><first>Bruno</first><last>Cartoni</last></author>
      <title>Dealing with unknown words by simple decomposition: feasibility studies with <fixed-case>I</fixed-case>talian prefixes.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/182_pdf.pdf</url>
      <abstract>In this article, we present an experiment that aims to evaluate the feasibility of a superficial morphological analysis, to analyse unknown constructed neologisms. For any morphosyntactic analyser, lexical incompleteness is a real problem. This lack of information is partly due to lexical creativity, and more especially to the productivity of some morphological processes. We present here a set of word formation rules based on constructional morphology principles that can be used to improve the performance of an Italian morphosyntactic analyser. These rules use only simple computing techniques in order to ensure efficiency because any improvements in coverage must not slow down the entire system. In the second part of this paper, we describe a method for constraining the rules, and an evaluation of these constraints in terms of performance. Great improvements are achieved in reducing the number of incorrect analyses of unknown neologisms (noise), although this is at the cost of some increase in silence (correct analyses which are no longer produced). This classic trade-off between noise and silence, however, can hardly be avoided and we believe that this experiment successfully demonstrates the feasibility of superficial analysis in improving performance and points the way to other avenues of research.</abstract>
    </paper>
    <paper id="98">
      <author><first>Serge</first><last>Sharoff</last></author>
      <title>A Uniform Interface to Large-Scale Linguistic Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/184_pdf.pdf</url>
      <abstract>In the paper we address two practical problems concerning the use of corpora in translation studies. The first stems from the limited resources available for targeted languages and genres within languages, whereas translation researchers and students need: sufficiently large modern corpora, either reflecting general language or specific to a problem domain. The second problem concerns the lackof a uniform interface for accessing the resources, even when the yexist. We deal with the first problem by developing a framework for semi-automatic acquisition of large corpora from the Internet for the languages relevant for our research and training needs. We outline the methodology used and discuss the composition of Internet-derived corpora. We deal with the second problem by developing a uniform interface to our corpora. In addition to standard options for choosingcorpora and sorting concordance lines, the interface can compute the list of collocations and filter the results according touser-specified patterns in order to detect language-specific syntacticstructures.</abstract>
    </paper>
    <paper id="99">
      <author><first>Carlo</first><last>Strapparava</last></author>
      <author><first>Alessandro</first><last>Valitutti</last></author>
      <author><first>Oliviero</first><last>Stock</last></author>
      <title>The Affective Weight of Lexicon</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/186_pdf.pdf</url>
      <abstract>This paper presents resources and functionalities for the recognition and selection of affective evaluative terms. An affective hierarchy as an extension of the WordNet-Affect lexical database was developed in the first place. The second phase was the development of a semantic similarity function, acquired automatically in an unsupervised way from a large corpus of texts, which allows us to put into relation concepts and emotional categories. The integration of the two components is a key element for several applications.</abstract>
    </paper>
    <paper id="100">
      <author><first>Agnes</first><last>Lisowska</last></author>
      <author><first>Nancy L.</first><last>Underwood</last></author>
      <title><fixed-case>ROTE</fixed-case>: A Tool to Support Users in Defining the Relative Importance of Quality Characteristics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/187_pdf.pdf</url>
      <abstract>This paper describes the Relative Ordering Tool for Evaluation (ROTE) which is designed to support the process of building a parameterised quality model for evaluation. It is a very simple tool which enables users to specify the relative importance of quality characteristics (and associated metrics) to reflect the users' particular requirements. The tool allows users to order any number of quality characteristics by comparing them in a pair-wise fashion. The tool was developed in the context of a collaborative project developing a text mining system. A full scale evaluation of the text mining system was designed and executed for three different users and the ROTE tool was successfully applied by those users during that process. The tool will be made available for general use by the evaluation community.</abstract>
    </paper>
    <paper id="101">
      <author><first>Serge</first><last>Sharoff</last></author>
      <author><first>Bogdan</first><last>Babych</last></author>
      <author><first>Anthony</first><last>Hartley</last></author>
      <title>Using collocations from comparable corpora to find translation equivalents</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/190_pdf.pdf</url>
      <abstract>In this paper we present a tool for finding appropriate translation equivalents for words from the general lexicon using comparable corpora. For a phrase in the source language the tool suggests arange of possible expressions used in similar contexts in target language corpora. In the paper we discuss the method and present results of human evaluation of the performance of the tool.</abstract>
    </paper>
    <paper id="102">
      <author><first>Folkert</first><last>de Vriend</last></author>
      <author><first>Lou</first><last>Boves</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <author><first>Roeland</first><last>van Hout</last></author>
      <author><first>Joep</first><last>Kruijsen</last></author>
      <author><first>Jos</first><last>Swanenberg</last></author>
      <title>A Unified Structure for <fixed-case>D</fixed-case>utch Dialect Dictionary Data</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/192_pdf.pdf</url>
      <abstract>The traditional dialect vocabulary of the Netherlands and Flanders is recorded and researched in several Dutch and Belgian research institutes and universities. Most of these distributed dictionary creation and research projects collaborate in the Permanent Overlegorgaan Regionale Woordenboeken (ReWo). In the project digital databases and digital tools for WBD and WLD (D-square) the dialect data published by two of these dictionary projects (Woordenboek van de Brabantse Dialecten and Woordenboek van de Limburgse Dialecten) is being digitised. One of the additional goals of the D-square project is the development of an infrastructure for electronic access to all dialect dictionaries collaborating in the ReWo. In this paper we will firstly reconsider the nature of the core data types - form, sense and location - present in the different dialect dictionaries and the ways these data types are further classified. Next we will focus on the problems encountered when trying to unify this dictionary data and their classifications and suggest solutions. Finally we will look at several implementation issues regarding a specific encoding for the dictionaries.</abstract>
    </paper>
    <paper id="103">
      <author id="elaine-ui-dhonnchadha"><first>E.</first><last>Uí Dhonnchadha</last></author>
      <author id="josef-van-genabith"><first>J.</first><last>Van Genabith</last></author>
      <title>A Part-of-speech tagger for <fixed-case>I</fixed-case>rish using Finite-State Morphology and Constraint Grammar Disambiguation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/193_pdf.pdf</url>
      <abstract>This paper describes the methodology used to develop a part-of-speech tagger for Irish, which is used to annotate a corpus of 30 million words of text with part-of-speech tags and lemmas. The tagger is evaluated using a manually disambiguated test corpus and it currently achieves 95% accuracy on unrestricted text. To our knowledge, this is the first part-of-speech tagger for Irish.</abstract>
    </paper>
    <paper id="104">
      <author><first>Véronique</first><last>Moriceau</last></author>
      <title>Language Challenges for Data Fusion in Question-Answering</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/194_pdf.pdf</url>
      <abstract>Search engines on the web and most existing question-answering systems provide the user with a set of hyperlinks and/or web page extracts containing answer(s) to a question. These answers are often incoherent to a certain degree (equivalent, contradictory, etc.). It is then quite difficult for the user to know which answer is the correct one. In this paper, we present an approach which aims at providing synthetic numerical answers in a question-answering system. These answers are generated in natural language and, in a cooperative perspective, the aim is to explain to the user the variation of numerical values when several values, apparently incoherent, are extracted from the web as possible answers to a question. We present in particular how lexical resources are essential to answer extraction from the web, to the characterization of the variation mode associated with the type of information and to answer generation in natural language.</abstract>
    </paper>
    <paper id="105">
      <author><first>Luís</first><last>Sarmento</last></author>
      <title><fixed-case>BACO</fixed-case> - A large database of text and co-occurrences</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/195_pdf.pdf</url>
      <abstract>In this paper we introduce a public resource named BACO (Base de Co-Ocorrências), a very large textual database built from the WPT03 collection, a publicly available crawl of the whole Portuguese web in 2003. BACO uses a generic relational database engine to store 1.5 million web documents in raw text (more than 6GB of plain text), corresponding to 35 million sentences, consisting of more than 1000 million words. BACO comprises four lexicon tables, including a standard single token lexicon, and three n-gram tables (2-grams, 3-grams and 4-grams) with several hundred million entries, and a table containing 780 million co-occurrence pairs. We describe the design choices and explain the preparation tasks involved in loading the data in the relational database. We present several statistics regarding storage requirements and we demonstrate how this resource is currently used.</abstract>
    </paper>
    <paper id="106">
      <author><first>Ulrich</first><last>Schäfer</last></author>
      <title><fixed-case>O</fixed-case>nto<fixed-case>NER</fixed-case>d<fixed-case>IE</fixed-case> – Mapping and Linking Ontologies to Named Entity Recognition and Information Extraction Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/196_pdf.pdf</url>
      <abstract>Semantic Web and NLP We describe an implemented offline procedure that maps OWL/RDF-encoded ontologies with large, dynamically maintained instance data to named entity recognition (NER) and information extraction (IE) engine resources, preserving hierarchical concept information and links back to the ontology concepts and instances. The main motivations are (i) improving NER/IE precision and recall in closed domains, (ii) exploiting linguistic knowledge (context, inflection, anaphora) for identifying ontology instances in texts more robustly, (iii) giving full access to ontology instances and concepts in natural language processing results, e.g. for subsequent ontology queries, navigation or inference, (iv) avoiding duplication of work in development and maintenance of similar resources in independent places, namely lingware and ontologies. We show an application in hybrid deep-shallow natural language processing that is e.g. used for question analysis in closed domains. Further applications could be automatic hyperlinking or other innovative semantic-web related applications.</abstract>
    </paper>
    <paper id="107">
      <author><first>Jonathan G.</first><last>Fiscus</last></author>
      <author><first>Jerome</first><last>Ajot</last></author>
      <author><first>Nicolas</first><last>Radde</last></author>
      <author><first>Christophe</first><last>Laprun</last></author>
      <title>Multiple Dimension <fixed-case>L</fixed-case>evenshtein Edit Distance Calculations for Evaluating Automatic Speech Recognition Systems During Simultaneous Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/197_pdf.pdf</url>
      <abstract>Since 1987, the National Institute of Standards and Technology has been providing evaluation infrastructure for the Automatic Speech Recognition (ASR), and more recently referred to as the Speech-To-Text (STT), research community. From the first efforts in the Resource Management domain to the present research, the NIST SCoring ToolKit (SCTK) has formed the tool set for system developers to make continued progress in many domains; Wall Street Journal, Conversational Telephone Speech (CTS), Broadcast News (BN), and Meetings (MTG) to name a few. For these domains, the community agreed to declared sections of simultaneous speech as not scoreable. While this had minor impact on most of these domains, the highly interactive nature of Meeting speech rendered a very large fraction of the test material not scoreable. This paper documents a multi-dimensional extension of the Dynamic Programming solution to Levenshtein Edit Distance calculations capable of evaluating STT systems during periods of overlapping, simultaneous speech.</abstract>
    </paper>
    <paper id="108">
      <author id="jordi-atserias"><first>J.</first><last>Atserias</last></author>
      <author id="bernardino-casas"><first>B.</first><last>Casas</last></author>
      <author id="elisabet-comelles"><first>E.</first><last>Comelles</last></author>
      <author id="meritxell-gonzalez"><first>M.</first><last>González</last></author>
      <author id="lluis-padro"><first>L.</first><last>Padró</last></author>
      <author id="muntsa-padro"><first>M.</first><last>Padró</last></author>
      <title><fixed-case>F</fixed-case>ree<fixed-case>L</fixed-case>ing 1.3: Syntactic and semantic services in an open-source <fixed-case>NLP</fixed-case> library</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/198_pdf.pdf</url>
      <abstract>This paper describes version 1.3 of the FreeLing suite of NLP tools. FreeLing was first released in February 2004 providing morphological analysis and PoS tagging for Catalan, Spanish, and English. From then on, the package has been improved and enlarged to cover more languages (i.e. Italian and Galician) and offer more services: Named entity recognition and classification, chunking, dependency parsing, and WordNet based semantic annotation. FreeLing is not conceived as end-user oriented tool, but as library on top of which powerful NLP applications can be developed. Nevertheless, sample interface programs are provided, which can be straightforwardly used as fast, flexible, and efficient corpus processing tools. A remarkable feature of FreeLing is that it is distributed under a free-software LGPL license, thus enabling any developer to adapt the package to his needs in order to get the most suitable behaviour for the application being developed.</abstract>
    </paper>
    <paper id="109">
      <author><first>Jiří</first><last>Semecký</last></author>
      <title>On Automatic Assignment of Verb Valency Frames in <fixed-case>C</fixed-case>zech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/199_pdf.pdf</url>
      <abstract>Many recent NLP applications, including machine translation and information retrieval, could benefit from semantic analysis of language data on the sentence level. This paper presents a method for automatic disambiguation of verb valency frames on Czech data. For each verb occurrence, we extracted features describing its local context. We experimented with diverse types of features, including morphological, syntax-based, idiomatic, animacy and WordNet-based features. The main contribution of the paper lies in determining which ones are most useful for the disambiguation task. The considered features were classified using decision trees, rule-based learning and a Naïve Bayes classifier. We evaluated the methods using 10-fold cross-validation on VALEVAL, a manually annotated corpus of frame annotations containing 7,778 sentences. Syntax-based features have shown to be the most effective. When we used the full set of features, we achieved an accuracy of 80.55% against the baseline 67.87% obtained by assigning the most frequent frame.</abstract>
    </paper>
    <paper id="110">
      <author><first>Ben</first><last>Medlock</last></author>
      <title>An Introduction to <fixed-case>NLP</fixed-case>-based Textual Anonymisation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/200_pdf.pdf</url>
      <abstract>We introduce the problem of automatic textual anonymisation and present a new publicly-available, pseudonymised benchmark corpus of personal email text for the task, dubbed ITAC (Informal Text Anonymisation Corpus). We discuss the method by which the corpus was constructed, and consider some important issues related to the evaluation of textual anonymisation systems. We also present some initial baseline results on the new corpus using a state of the art HMM-based tagger. We introduce the problem of automatic textual anonymisation and present a new publicly-available, pseudonymised benchmark corpus of personal email text for the task, dubbed ITAC (Informal Text Anonymisation Corpus). We discuss the method by which the corpus was constructed, and consider some important issues related to the evaluation of textual anonymisation systems. We also present some initial baseline results on the new corpus using a state of the art HMM-based tagger.</abstract>
    </paper>
    <paper id="111">
      <author><first>Hennie</first><last>Brugman</last></author>
      <author><first>Véronique</first><last>Malaisé</last></author>
      <author><first>Luit</first><last>Gazendam</last></author>
      <title>A Web Based General Thesaurus Browser to Support Indexing of Television and Radio Programs</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/201_pdf.pdf</url>
      <abstract>Documentation and retrieval processes at the Netherlands Institute for Sound and Vision are organized around a common thesaurus. To help improve the quality of these processes the thesaurus was transformed into a RDF/OWL ontology and extended on basis of implicit information and external resources. A thesaurus browser web application was designed, implemented and tested on future users.</abstract>
    </paper>
    <paper id="112">
      <author><first>Judit</first><last>Feliu</last></author>
      <author><first>Jorge</first><last>Vivaldi</last></author>
      <author><first>M. Teresa</first><last>Cabré</last></author>
      <title><fixed-case>SKELETON</fixed-case>: Specialised knowledge retrieval on the basis of terms and conceptual relations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/203_pdf.pdf</url>
      <abstract>The main goal of this paper is to present a first approach to an automatic detection of conceptual relations between two terms in specialised written text. Previous experiments on the basis of the manual analysis lead the authors to implement an automatic query strategy combining the term candidates proposed by an extractor together with a list of verbal syntactic patterns used for the relations refinement. Next step on the research will be the integration of the results into the term extractor in order to attain more restrictive pieces of information directly reused for the ontology building task.</abstract>
    </paper>
    <paper id="113">
      <author><first>Irene</first><last>Cramer</last></author>
      <author><first>Jochen L.</first><last>Leidner</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <title>Building an Evaluation Corpus for <fixed-case>G</fixed-case>erman Question Answering by Harvesting <fixed-case>W</fixed-case>ikipedia</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/206_pdf.pdf</url>
      <abstract>The growing interest in open-domain question answering is limited by the lack of evaluation and training resources. To overcome this resource bottleneck for German, we propose a novel methodology to acquire new question-answer pairs for system evaluation that relies on volunteer collaboration over the Internet. Utilizing Wikipedia, a popular free online encyclopedia available in several languages, we show that the data acquisition problem can be cast as a Web experiment. We present a Web-based annotation tool and carry out a distributed data collection experiment. The data gathered from the mostly anonymous contributors is compared to a similar dataset produced in-house by domain experts on the one hand, and the German questions from the from the CLEF QA 2004 effort on the other hand. Our analysis of the datasets suggests that using our novel method a medium-scale evaluation resource can be built at very small cost in a short period of time. The technique and software developed here is readily applicable to other languages where free online encyclopedias are available, and our resulting corpus is likewise freely available.</abstract>
    </paper>
    <paper id="114">
      <author><first>Gerhard</first><last>Fliedner</last></author>
      <title>Towards Natural Interactive Question Answering</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/210_pdf.pdf</url>
      <abstract>Interactive question answering systems should allow users to lead a coherent information seeking dialogue. Compared with systems that only locally evaluate a question, interactive systems facilitate the information seeking process and provide a more natural feel. We show that by extending a QA system to handle several types of anaphora and ellipsis, the naturalness of the interaction can be considerably improved. We describe an implementation in our prototype QA system for German and give a walk-through example of the enhanced interaction capabilities.</abstract>
    </paper>
    <paper id="115">
      <author><first>Benjamin</first><last>Waldron</last></author>
      <author><first>Ann</first><last>Copestake</last></author>
      <author><first>Ulrich</first><last>Schäfer</last></author>
      <author><first>Bernd</first><last>Kiefer</last></author>
      <title>Preprocessing and Tokenisation Standards in <fixed-case>DELPH</fixed-case>-<fixed-case>IN</fixed-case> Tools</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/214_pdf.pdf</url>
      <abstract>We discuss preprocessing and tokenisation standards within DELPH-IN, a large scale open-source collaboration providing multiple independent multilingual shallow and deep processors. We discuss (i) a component-specific XML interface format which has been used for some time to interface preprocessor results to the PET parser, and (ii) our implementation of a more generic XML interface format influenced heavily by the (ISO working draft) Morphosyntactic Annotation Framework (MAF). Our generic format encapsulates the information which may be passed from the preprocessing stage to a parser: it uses standoff-annotation, a lattice for the representation of structural ambiguity, intra-annotation dependencies and allows for highly structured annotation content. This work builds on the existing Heart of Gold middleware system, and previous work on Robust Minimal Recursion Semantics (RMRS) as part of an inter-component interface. We give examples of usage with a number of the DELPH-IN processing components and deep grammars.</abstract>
    </paper>
    <paper id="116">
      <author><first>Juri</first><last>Apresjan</last></author>
      <author><first>Igor</first><last>Boguslavsky</last></author>
      <author><first>Boris</first><last>Iomdin</last></author>
      <author><first>Leonid</first><last>Iomdin</last></author>
      <author><first>Andrei</first><last>Sannikov</last></author>
      <author><first>Victor</first><last>Sizov</last></author>
      <title>A Syntactically and Semantically Tagged Corpus of <fixed-case>R</fixed-case>ussian: State of the Art and Prospects</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/216_pdf.pdf</url>
      <abstract>We describe a project aimed at creating a deeply annotated corpus of Russian texts. The annotation consists of comprehensive morphological marking, syntactic tagging in the form of a complete dependency tree, and semantic tagging within a restricted semantic dictionary. Syntactic tagging is using about 80 dependency relations. The syntactically annotated corpus counts more than 28,000 sentences and makes an autonomous part of the Russian National Corpus (www.ruscorpora.ru). Semantic tagging is based on an inventory of semantic features (descriptors) and a dictionary comprising about 3,000 entries, with a set of tags assigned to each lexeme and its argument slots. The set of descriptors assigned to words has been designed in such a way as to construct a linguistically relevant classification for the whole Russian vocabulary. This classification serves for discovering laws according to which the elements of various lexical and semantic classes interact in the texts. The inventory of semantic descriptors consists of two parts, object descriptors (about 90 items in total) and predicate descriptors (about a hundred). A set of semantic roles is thoroughly elaborated and contains about 50 roles.</abstract>
    </paper>
    <paper id="117">
      <author><first>Martin</first><last>Hassel</last></author>
      <author><first>Jonas</first><last>Sjöbergh</last></author>
      <title>Towards Holistic Summarization – Selecting Summaries, Not Sentences</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/218_pdf.pdf</url>
      <abstract>In this paper we present a novel method for automatic text summarization through text extraction, using computational semantics. The new idea is to view all the extracted text as a whole and compute a score for the total impact of the summary, instead of ranking for instance individual sentences. A greedy search strategy is used to search through the space of possible summaries and select the summary with the highest score of those found. The aim has been to construct a summarizer that can be quickly assembled, with the use of only a very few basic language tools, for languages that lack large amounts of structured or annotated data or advanced tools for linguistic processing. The proposed method is largely language independent, though we only evaluate it on English in this paper, using ROUGE-scores on texts from among others the DUC 2004 task 2. On this task our method performs better than several of the systems evaluated there, but worse than the best systems.</abstract>
    </paper>
    <paper id="118">
      <author><first>Stefan</first><last>Schaden</last></author>
      <author><first>Ute</first><last>Jekosch</last></author>
      <title>“Casselberveetovallarga” and other Unpronounceable Places: The <fixed-case>C</fixed-case>ross<fixed-case>T</fixed-case>owns Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/219_pdf.pdf</url>
      <abstract>This paper presents a corpus of non-native speech that contains pronunciation variants of European city names from fivecountries spoken by speakers of four native languages. It was originally designed as a research tool for the study ofpronunciation errors by non-native speakers in the pronunciation of foreign city names. The corpus has now been released. Followinga brief sketch of the research context in which this data collection was established, the first part of this paper describes the contents and technical specifications of the corpus (design, speakers, language material, recording conditions).Compared to corpora of native speech, non-native speech compilations raise a number of additional difficulties that requirespecific attention and methodology. Therefore, the second part of the paper aims to point out some of these general issuesfrom the perspective of the experience gained in our research. Strategies to deal with these difficulties will be exploredalong with their specific benefits and shortfalls, concluding that non-native speech corpora require a number of specificdesign guidelines which are often difficult to put into practice.</abstract>
    </paper>
    <paper id="119">
      <author><first>Dimitrios</first><last>Kokkinakis</last></author>
      <author><first>Dana</first><last>Dannélls</last></author>
      <title>Recognizing Acronyms and their Definitions in <fixed-case>S</fixed-case>wedish Medical Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/220_pdf.pdf</url>
      <abstract>This paper addresses the task of recognizing acronym-definition pairs in Swedish (medical) texts as well as the compilation of a freely available sample of such manually annotated pairs. A material suitable not only for supervised learning experiments, but also as a testbed for the evaluation of the quality of future acronym-definition recognition systems. There are a number of approaches to the identification described in the literature, particularly within the biomedical domain, but none of those addresses the variation and complexity exhibited in a language other than English. This is realized by the fact that we can have a mixture of two languages in the same document and/or sentence, i.e. Swedish and English; that Swedish is a compound language that significantly deteriorates the performance of previous approaches (without adaptations) and, most importantly, the fact that there is a large variation of possible acronym-definition permutations realized in the analysed corpora, a variation that is usually ignored in previous studies.</abstract>
    </paper>
    <paper id="120">
      <author><first>Lun-Wei</first><last>Ku</last></author>
      <author><first>Yu-Ting</first><last>Liang</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <title>Tagging Heterogeneous Evaluation Corpora for Opinionated Tasks</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/222_pdf.pdf</url>
      <abstract>Opinion retrieval aims to tell if a document is positive, neutral or negative on a given topic. Opinion extraction further identifies the supportive and the non-supportive evidence of a document. To evaluate the performance of technologies for opinionated tasks, a suitable corpus is necessary. This paper defines the annotations for opinionated materials. Heterogeneous experimental materials are annotated, and the agreements among annotators are analyzed. How human can monitor opinions of the whole is also examined. The corpus can be employed to opinion extraction, opinion summarization, opinion tracking and opinionated question answering.</abstract>
    </paper>
    <paper id="121">
      <author><first>Joakim</first><last>Nivre</last></author>
      <author><first>Jens</first><last>Nilsson</last></author>
      <author><first>Johan</first><last>Hall</last></author>
      <title><fixed-case>T</fixed-case>albanken05: A <fixed-case>S</fixed-case>wedish Treebank with Phrase Structure and Dependency Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/223_pdf.pdf</url>
      <abstract>We introduce Talbanken05, a Swedish treebank based on a syntactically annotated corpus from the 1970s, Talbanken76, converted to modern formats. The treebank is available in three different formats, besides the original one: two versions of phrase structure annotation and one dependency-based annotation, all of which are encoded in XML. In this paper, we describe the conversion process and exemplify the available formats. The treebank is freely available for research and educational purposes.</abstract>
    </paper>
    <paper id="122">
      <author><first>Oana</first><last>Postolache</last></author>
      <author><first>Dan</first><last>Cristea</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <title>Transferring Coreference Chains through Word Alignment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/224_pdf.pdf</url>
      <abstract>This paper investigates the problem of automatically annotating resources with NP coreference information using a parallel corpus, English-Romanian, in order to transfer, through word alignment, coreference chains from the English part to the Romanian part of the corpus. The results show that we can detect Romanian referential expressions and coreference chains with over 80% F-measure, thus using our method as a preprocessing step followed by manual correction as part of an annotation effort for creating a large Romanian corpus with coreference information is worthwhile.</abstract>
    </paper>
    <paper id="123">
      <author id="giovanni-tummarello"><first>G.</first><last>Tummarello</last></author>
      <author id="christian-morbidoni"><first>C.</first><last>Morbidoni</last></author>
      <author id="fabio-kepler"><first>F.</first><last>Kepler</last></author>
      <author id="francesco-piazza"><first>F.</first><last>Piazza</last></author>
      <author id="paolo-puliti"><first>P.</first><last>Puliti</last></author>
      <title>A novel Textual Encoding paradigm based on Semantic Web tools and semantics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/225_pdf.pdf</url>
      <abstract>In this paper we perform a preliminary evaluation on how Semantic Web technologies such as RDF and OWL can be used to perform textual encoding. Among the potential advantages, we notice how RDF, given its conceptual graph structure, appears naturally suited to deal with overlapping hierarchies of annotations, something notoriously problematic using classic XML based markup. To conclude, we show how complex querying can be performed using slight modifications of already existing Semantic Web query tools.</abstract>
    </paper>
    <paper id="124">
      <author><first>Anna</first><last>Bogacka</last></author>
      <author><first>Katarzyna</first><last>Dziubalska-Kołaczyk</last></author>
      <author><first>Grzegorz</first><last>Krynicki</last></author>
      <author><first>Dawid</first><last>Pietrala</last></author>
      <author><first>Mikołaj</first><last>Wypych</last></author>
      <title>General and Task-Specific Corpus Resources for <fixed-case>P</fixed-case>olish Adult Learners of <fixed-case>E</fixed-case>nglish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/227_pdf.pdf</url>
      <abstract>This paper offers a comparison of two resources for Polish adult learners of English. The first has been designed for Polish-English Literacy Tutor (PELT), a multimodal system for foreign language learning, as training input to speech recognition system for highly accented, strongly variable second language speech. The second corpus is a task-specific resource designed in the PELT framework to investigate the vowel space of English produced by Poles. Presented are linguistically and technologically challenging aspects of the two ventures and their complementary character.</abstract>
    </paper>
    <paper id="125">
      <author><first>Olena</first><last>Medelyan</last></author>
      <author><first>Stefan</first><last>Schulz</last></author>
      <author><first>Jan</first><last>Paetzold</last></author>
      <author><first>Michael</first><last>Poprat</last></author>
      <author><first>Kornél</first><last>Markó</last></author>
      <title>Language Specific and Topic Focused Web Crawling</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/228_pdf.pdf</url>
      <abstract>We describe an experiment on collecting large language and topic specific corpora automatically by using a focused Web crawler. Our crawler combines efficient crawling techniques with a common text classification tool. Given a sample corpus of medical documents, we automatically extract query phrases and then acquire seed URLs with a standard search engine. Starting from these seed URLs, the crawler builds a new large collection consisting only of documents that satisfy both the language and the topic model. The manual analysis of acquired English and German medicine corpora reveals the high accuracy of the crawler. However, there are significant differences between both languages.</abstract>
    </paper>
    <paper id="126">
      <author><first>Martin</first><last>Reynaert</last></author>
      <title>Corpus-Induced Corpus Clean-up</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/229_pdf.pdf</url>
      <abstract>We explore the feasibility of using only unsupervised means to identify non-words, i.e. typos, in a frequency list derived from a large corpus of Dutch and to distinguish between these non-words and real-words in the language. We call the system we built and evaluate in this paper ciccl, which stands for Corpus-Induced Corpus Clean-up. The algorithm on which ciccl is primarily based is the anagram-key hashing algorithm introduced by (Reynaert, 2004). The core correction mechanism is a simple and effective method which translates the actual characters which make up a word into a large natural number in such a way that all the anagrams, i.e. all the words composed of precisely the same subset of characters, are allocated the same natural number. In effect, this constitutes a novel approximate string matching algorithm for indexed text search. This is because by simple addition, subtraction or a combination of both, all variants within reach of the range of numerical values defined in the alphabet are retrieved by iterating over the alphabet. ciccl's input consists primarily of corpus derived frequency lists, from which it derives valuable morphological information by performing frequency counts over the substrings of the words, which are then used to perform decompounding, as well as for distinguishing between most likely correctly spelled words and typos.</abstract>
    </paper>
    <paper id="127">
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <author><first>Maria</first><last>Georgescul</last></author>
      <title><fixed-case>TQB</fixed-case>: Accessing Multimodal Data Using a Transcript-based Query and Browsing Interface</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/233_pdf.pdf</url>
      <abstract>This article describes an interface for searching and browsing multimodal recordings of group meetings. We provide first an overall perspective of meeting processing and retrieval applications, and distinguish between the media/modalities that are recorded and the ones that are used for browsing. We then proceed to describe the data and the annotations that are stored in a meeting database. Two scenarios of use for the transcript-based query and browsing interface (TQB) are then outlined: search and browse vs. overview and browse. The main functionalities of TQB, namely the database backend and the multimedia rendering solutions are described. An outline of evaluation perspectives is finally provided, with a description of the user interaction features that will be monitored.</abstract>
    </paper>
    <paper id="128">
      <author><first>Feng</first><last>Pan</last></author>
      <author><first>Rutu</first><last>Mulkar</last></author>
      <author><first>Jerry R.</first><last>Hobbs</last></author>
      <title>An Annotated Corpus of Typical Durations of Events</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/234_pdf.pdf</url>
      <abstract>In this paper, we present our work on generating an annotated corpus for extracting information about the typical durations of events from texts. We include the annotation guidelines, the event classes we categorized, the way we use normal distributions to model vague and implicit temporal information, and how we evaluate inter-annotator agreement. The experimental results show that our guidelines are effective in improving the inter-annotator agreement.</abstract>
    </paper>
    <paper id="129">
      <author><first>Agam</first><last>Patel</last></author>
      <author><first>Dragomir R.</first><last>Radev</last></author>
      <title>Lexical similarity can distinguish between automatic and manual translations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/235_pdf.pdf</url>
      <abstract>We consider the problem of identifying automatic translations from manual translations of the same sentence. Using two different similarity metrics (BLEU and Levenshtein edit distance), we found out that automatic translations are closer to each other than they are to manual translations. We also use phylogenetic trees to provide a visual representation of the distances between pairs of individual sentences in a set of translations. The differences in lexical distance are statistically significant, both for Chinese to English and for Arabic to English translations.</abstract>
    </paper>
    <paper id="130">
      <author><first>Nick</first><last>Campbell</last></author>
      <author><first>Toshiyuki</first><last>Sadanobu</last></author>
      <author><first>Masataka</first><last>Imura</last></author>
      <author><first>Naoto</first><last>Iwahashi</last></author>
      <author><first>Suzuki</first><last>Noriko</last></author>
      <author><first>Damien</first><last>Douxchamps</last></author>
      <title>Multimedia Database of Meetings and Informal Interactions for Tracking Participant Involvement and Discourse Flow</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/236_pdf.pdf</url>
      <abstract>At ATR, we are collecting and analysing meetings data using a table-top sensor device consisting of a small 360-degree camera surrounded by an array of high-quality directional microphones. This equipment provides a stream of information about the audio and visual events of the meeting which is then processed to form a representation of the verbal and non-verbal interpersonal activity, or discourse flow, during the meeting. This paper describes the resulting corpus of speech and video data which is being collected for the abovere search. It currently includes data from 12 monthly sessions, comprising 71 video and 33 audio modules. Collection is continuingmonthly and is scheduled to include another ten sessions.</abstract>
    </paper>
    <paper id="131">
      <author><first>Fu Lee</first><last>Wang</last></author>
      <author><first>Xiaotie</first><last>Deng</last></author>
      <author><first>Feng</first><last>Zou</last></author>
      <title>Towards Unified <fixed-case>C</fixed-case>hinese Segmentation Algorithm</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/237_pdf.pdf</url>
      <abstract>As Chinese is an ideographic character-based language, the words in the texts are not delimited by spaces. Indexing of Chinese documents is impossible without a proper segmentation algorithm. Many Chinese segmentation algorithms have been proposed in the past. Traditional segmentation algorithms cannot operate without a large dictionary or a large corpus of training data. Nowadays, the Web has become the largest corpus that is ideal for Chinese segmentation. Although the search engines do not segment texts into proper words, they maintain huge databases of documents and frequencies of character sequences in the documents. Their databases are important potential resources for segmentation. In this paper, we propose a segmentation algorithm by mining web data with the help from search engines. It is the first unified segmentation algorithm for Chinese language from different geographical areas. Experiments have been conducted on the datasets of a recent Chinese segmentation competition. The results show that our algorithm outperforms the traditional algorithms in terms of precision and recall. Moreover, our algorithm can effectively deal with the problem of segmentation ambiguity, new word (unknown word) detection, and stop words.</abstract>
    </paper>
    <paper id="132">
      <author><first>Doaa</first><last>Samy</last></author>
      <author><first>Antonio Moreno</first><last>Sandoval</last></author>
      <author><first>José M.</first><last>Guirao</last></author>
      <author><first>Enrique</first><last>Alfonseca</last></author>
      <title>Building a Parallel Multilingual Corpus (<fixed-case>A</fixed-case>rabic-<fixed-case>S</fixed-case>panish-<fixed-case>E</fixed-case>nglish)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/238_pdf.pdf</url>
      <abstract>This paper presents the results (1st phase) of the on-going research in the Computational Linguistics Laboratory at Autónoma University of Madrid (LLI-UAM) aiming at the development of a multi-lingual parallel corpus (Arabic-Spanish-English) aligned on the sentence level and tagged on the POS level. A multilingual parallel corpus which brings together Arabic, Spanish and English is a new resource for the NLP community that completes the present panorama of parallel corpora. In the first part of this study, we introduce the novelty of our approach and the challenges encountered to create such a corpus. This introductory part highlights the main features of the corpus and the criteria applied during the selection process. The second part focuses on two main stages: basic processing (tokenization and segmentation) and alignment. Methodology of alignment is explained in detail and results obtained in the three different linguistic pairs are compared. POS tagging and tools used in this stage are discussed in the third part. The final output is available in two versions: the non-aligned version and the aligned one. The latter adopts the TMX (Translation Memory Exchange) standard format. At the end, the section dedicated to the future work points out the key stages concerned with extending the corpus and the studies that can benefit, directly or indirectly, from such a resource.</abstract>
    </paper>
    <paper id="133">
      <author><first>Donna K.</first><last>Byron</last></author>
      <author><first>Eric</first><last>Fosler-Lussier</last></author>
      <title>The <fixed-case>OSU</fixed-case> Quake 2004 corpus of two-party situated problem-solving dialogs</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/241_pdf.pdf</url>
      <abstract>This report describes the Ohio State University Quake 2004 corpus of English spontaneous task-oriented two-person situated dialog. The corpus was collected using a first-person display of an interior space (rooms, corridors, stairs) in which the partners collaborate on a treasure hunt task. The corpus contains exciting new features such as deictic and exophoric reference, language that is calibrated against the spatial arrangement of objects in the world, and partial-observability of the task world imposed by the perceptual limitations inherent in the physical arrangement of the world. The corpus differs from prior dialog collections which intentionally restricted the interacting subjects from sharing any perceptual context, and which allowed one subject (the direction-giver or system) to have total knowledge of the state of the task world. The corpus consists of audio/video recordings of each person's experience in the virtual world and orthographic transcriptions. The virtual world can also be used by other researchers who want to conduct additional studies using this stimulus.</abstract>
    </paper>
    <paper id="134">
      <author><first>Md. Aminul</first><last>Islam</last></author>
      <author><first>Diana</first><last>Inkpen</last></author>
      <title>Second Order Co-occurrence <fixed-case>PMI</fixed-case> for Determining the Semantic Similarity of Words</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/242_pdf.pdf</url>
      <abstract>This paper presents a new corpus-based method for calculating the semantic similarity of two target words. Our method, called Second Order Co-occurrencePMI (SOC-PMI), uses Pointwise Mutual Information to sort lists of important neighbor words of the two target words. Then we consider the words which are common in both lists and aggregate their PMI values (from the opposite list) to calculate the relative semantic similarity. Our method was empirically evaluated using Miller and Charlers (1991) 30 noun pair subset, Ruben-stein and Goodenoughs (1965) 65 noun pairs, 80 synonym test questions from the Test of English as a Foreign Language (TOEFL), and 50 synonym test questions from a collection of English as a Second Language (ESL) tests. Evaluation results show that our method outperforms several competing corpus-based methods.</abstract>
    </paper>
    <paper id="135">
      <author><first>Yoji</first><last>Kiyota</last></author>
      <author><first>Hiroshi</first><last>Nakagawa</last></author>
      <title>A Domain Ontology Production Tool Kit Based on Automatically Constructed Case Frames</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/243_pdf.pdf</url>
      <abstract>This paper proposes a tool kit to produce a domain ontology for text mining, based on case frames automatically constructed from a raw corpus of a specific domain. Since case frames are strongly related to implicit facts hidden in large domain-specific corpora, we can say that case frames are a promising device for text mining. The aim of the tool kit is to enable automatic analysis of event reports, from which implicit factors of the events are to be extracted. The tool kit enables us to produce a domain ontology by iterating associative retrieval of case frames and manual refinement. In this study, the tool kit is applied to the Japan Airlines pilot report collection, and a domain ontology of contributing factors in the civil aviation domain is experimentally produced. A lot of interesting examples are found in the ontology. In addition, a brief examination of the production process shows the efficiency of the tool kit.</abstract>
    </paper>
    <paper id="136">
      <author><first>Ronnie</first><last>Taib</last></author>
      <author><first>Natalie</first><last>Ruiz</last></author>
      <title>Tangible Objects for the Acquisition of Multimodal Interaction Patterns</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/244_pdf.pdf</url>
      <abstract>Multimodal user interfaces offer more intuitive interaction for end-users, however, usually only through predefined input schemes. This paper describes a user experiment for multimodal interaction pattern identification, using head gesture and speech inputs for a 3D graph manipulation. We show that a direct mapping between head gestures and the 3D object predominates, however even for such a simple task inputs vary greatly between users, and do not exhibit any clustering pattern. Also, in spite of the high degree of expressiveness of linguistic modalities, speech commands in particular tend to use a limited vocabulary. We observed a common set of verb and adverb compounds in a majority of users. In conclusion, we recommend that multimodal user interfaces be individually customisable or adaptive to users interaction preferences.</abstract>
    </paper>
    <paper id="137">
      <author><first>Václav</first><last>Novák</last></author>
      <author><first>Jan</first><last>Hajič</last></author>
      <title>Perspectives of Turning <fixed-case>P</fixed-case>rague Dependency Treebank into a Knowledge Base</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/245_pdf.pdf</url>
      <abstract>Recently, the Prague Dependency Treebank 2.0 (PDT 2.0) has emerged as the largest text corpora annotated on the level of tectogrammatical representation (linguistic meaning) described in Sgall et al. (2004) and containing about 0.8 milion words (see Hajic (2004)). We hope that this level of annotation is so close to the meaning of the utterances contained in the corpora that it should enable us to automatically transform texts contained in the corpora to the form of knowledge base, usable for information extraction, question answering, summarization, etc. We can use Multilayered Extended Semantic Networks (MultiNet) described in Helbig (2006) as the target formalism. In this paper we discuss the suitability of such approach and some of the main issues that will arise in the process. In section 1, we introduce formalisms underlying PDT 2.0 and MultiNet, in section 2. We describe the role MultiNet can play in the system of Functional Generative Description (FGD), section 3 discusses issues of automatic conversion to MultiNet and section 4 gives some conclusions.</abstract>
    </paper>
    <paper id="138">
      <author><first>Jonas</first><last>Granfeldt</last></author>
      <author><first>Pierre</first><last>Nugues</last></author>
      <author><first>Malin</first><last>Ågren</last></author>
      <author><first>Jonas</first><last>Thulin</last></author>
      <author><first>Emil</first><last>Persson</last></author>
      <author><first>Suzanne</first><last>Schlyter</last></author>
      <title><fixed-case>CEFLE</fixed-case> and Direkt Profil: a New Computer Learner Corpus in <fixed-case>F</fixed-case>rench <fixed-case>L</fixed-case>2 and a System for Grammatical Profiling</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/246_pdf.pdf</url>
      <abstract>The importance of computer learner corpora for research in both second language acquisition and foreign language teaching is rapidly increasing. Computer learner corpora can provide us with data to describe the learners interlanguage system at different points of its development and they can be used to create pedagogical tools. In this paper, we first present a new computer learner corpus in French. We then describe an analyzer called Direkt Profil, that we have developed using this corpus. The system carries out a sentence analysis based on developmental sequences, i.e. local morphosyntactic phenomena linked to a development in the acquisition of French as a foreign language. We present a brief introduction to developmental sequences and some examples in French. In the final section, we introduce and evaluate a method to optimize the definition and detection of learner profiles using machine-learning techniques.</abstract>
    </paper>
    <paper id="139">
      <author><first>Qian</first><last>Yang</last></author>
      <author><first>Jean-Pierre</first><last>Martens</last></author>
      <author><first>Nanneke</first><last>Konings</last></author>
      <author><first>Henk</first><last>van den Heuvel</last></author>
      <title>Development of a phoneme-to-phoneme (p2p) converter to improve the grapheme-to-phoneme (g2p) conversion of names</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/248_pdf.pdf</url>
      <abstract>It is acknowledged that a good phonemic transcription of proper names is imperative for the success of many modern speech-based services such as directory assistance, car navigation, etc. It is also known that state-of-the-art general-purpose grapheme-to-phoneme (g2p) converters perform rather poorly on many name categories. This paper proposes to use a g2p-p2p tandem comprising a state-of-the-art general-purpose g2p converter that produces an initial transcription and a name category specific phoneme-to-phoneme (p2p) converter that aims at correcting the mistakes made by the g2p converter. The main body of the paper describes a novel methodology for the automatic construction of the p2p converter. The methodology is implemented in a software toolbox that will be made publicly available in a form that will permit the user to design a p2p converter for an arbitrary name category. To give a proof of concept, the toolbox was used for the development of three p2p converters for first names, surnames and geographical names respectively. The obtained systems are small (few rules) and effective: significant improvements (up to 50% relative) of the grapheme-to-phoneme conversion are obtained. These encouraging results call for a further development and improvement of the approach.</abstract>
    </paper>
    <paper id="140">
      <author><first>Jaeyoung</first><last>Jung</last></author>
      <author><first>Maki</first><last>Miyake</last></author>
      <author><first>Hiroyuki</first><last>Akam</last></author>
      <title>Recurrent <fixed-case>M</fixed-case>arkov Cluster (<fixed-case>RMCL</fixed-case>) Algorithm for the Refinement of the Semantic Network</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/249_pdf.pdf</url>
      <abstract>The purpose of this work is to propose a new methodology to ameliorate the Markov Cluster (MCL) Algorithm that is well known as an efficient way of graph clustering (Van Dongen, 2000). The MCL when applied to a graph of word associations has the effect of producing concept areas in which words are grouped into the similar topics or similar meanings as paradigms. However, since a word is determined to belong to only one cluster that represents a concept, Markov clusters cannot show the polysemy or semantic indetermination among the properties of natural language. Our Recurrent MCL (RMCL) allows us to create a virtual adjacency relationship among the Markov hard clusters and produce a downsized and intrinsically informative semantic network of word association data. We applied one of the RMCL algorithms (Stepping-stone type) to a Japanese associative concept dictionary and obtained a satisfactory level of performance in refining the semantic network generated from MCL.</abstract>
    </paper>
    <paper id="141">
      <author><first>Catia</first><last>Cucchiarini</last></author>
      <author><first>Hugo</first><last>Van hamme</last></author>
      <author><first>Olga</first><last>van Herwijnen</last></author>
      <author><first>Felix</first><last>Smits</last></author>
      <title><fixed-case>JASMIN</fixed-case>-<fixed-case>CGN</fixed-case>: Extension of the Spoken <fixed-case>D</fixed-case>utch Corpus with Speech of Elderly People, Children and Non-natives in the Human-Machine Interaction Modality</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/254_pdf.pdf</url>
      <abstract>Large speech corpora (LSC) constitute an indispensable resource for conducting research in speech processing and for developing real-life speech applications. In 2004 the Spoken Dutch Corpus (CGN) became available, a corpus of standard Dutch as spoken by adult natives in the Netherlands and Flanders. Owing to budget constraints, CGN does not include speech of children, non-natives, elderly people and recordings of speech produced in human-machine interactions. Since such recordings would be extremely useful for conducting research and for developing HLT applications for these specific groups of speakers of Dutch, a new project, JASMIN-CGN, was started which aims at extending CGN in different ways: by collecting a corpus of contemporary Dutch as spoken by children of different age groups, non-natives with different mother tongues and elderly people in the Netherlands and Flanders and, in addition, by collecting speech material in a communication setting that was not envisaged in CGN: human-machine interaction. We expect that the knowledge gathered from these data can be generalized to developing appropriate systems also for other speaker groups (i.e. adult natives). One third of the data will be collected in Flanders and two thirds in the Netherlands.</abstract>
    </paper>
    <paper id="142">
      <author><first>Cédrick</first><last>Fairon</last></author>
      <author><first>Sébastien</first><last>Paumier</last></author>
      <title>A framework for real-time dictionary updating</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/255_pdf.pdf</url>
      <abstract>We present a framework that combines a web-based text acquisition tool, a term extractor and a two-level workflow management system tailored for facilitating dictionary updates. Our aim is to show that, thanks to such a methodology, it is possible to monitor data sources and rapidly review and code new dictionary entries. Once approved, these new entries can feed in real-time client dictionary-based applications that need to be continuously kept up to date.</abstract>
    </paper>
    <paper id="143">
      <author><first>Vicente</first><last>Alabau</last></author>
      <author><first>Carlos D.</first><last>Martínez</last></author>
      <title>Bilingual speech corpus in two phonetically similar languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/256_pdf.pdf</url>
      <abstract>As Speech Recognition Systems improve, they become suitable for facingnew problems. Multilingual speech recognition is one such problems.In the present work, the case of the Comunitat Valenciana multilingual environment is studied.The official languages in the Comunitat Valenciana (Spanish and Valencian) share most of their acoustic units, and their vocabularies and syntax are quite similar.They have influenced each other for many years.A small corpus on an Information System task was developed for experimentationpurposes.This choice will make it possible to develop a working prototype in the future,and it is simple enough to build semi-automatic language models.The design of the acoustic corpus is discussed, showing that all combinations of accents have been studied (native, non-native speakers, male, female, etc.).</abstract>
    </paper>
    <paper id="144">
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <author><first>Ineke</first><last>Schuurman</last></author>
      <author><first>Michael</first><last>Carl</last></author>
      <author><first>Stella</first><last>Markantonatou</last></author>
      <author><first>Toni</first><last>Badia</last></author>
      <title><fixed-case>METIS</fixed-case>-<fixed-case>II</fixed-case>: Machine Translation for Low Resource Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/258_pdf.pdf</url>
      <abstract>In this paper we describe a machine translation prototype in which we use only minimal resources for both the source and the target language. A shallow source language analysis, combined with a translation dictionary and a mapping system of source language phenomena into the target language and a target language corpus for generation are all the resources needed in the described system. Several approaches are presented.</abstract>
    </paper>
    <paper id="145">
      <author><first>Elisabeth</first><last>D’Halleweyn</last></author>
      <author><first>Jan</first><last>Odijk</last></author>
      <author><first>Lisanne</first><last>Teunissen</last></author>
      <author><first>Catia</first><last>Cucchiarini</last></author>
      <title>The <fixed-case>D</fixed-case>utch-<fixed-case>F</fixed-case>lemish <fixed-case>HLT</fixed-case> Programme <fixed-case>STEVIN</fixed-case>: Essential Speech and Language Technology Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/259_pdf.pdf</url>
      <abstract>In 2004 a consortium of ministries and organizations in the Netherlands and Flanders launched the comprehensive Dutch-Flemish HLT programme STEVIN (a Dutch acronym for Essential Speech and Language Technology Resources). To guarantee its Dutch-Flemish character, this large-scale programme is carried out under the auspices of the intergovernmental Dutch Language Union (NTU). The aim of STEVIN is to contribute to the further progress of HLT for the Dutch language, by raising awareness of HLT results, stimulating the demand of HLT products, promoting strategic research in HLT, and developing HLT resources that are essential and are known to be missing. Furthermore, a structure was set up for the management, maintenance and distribution of HLT resources. The STEVIN programme, which will run from 2004 to 2009, resulted from HLT activities in the Dutch language area, which were reported on at previous LREC conferences (2000, 2002, 2004). In this paper we will explain how different activities are combined in one comprehensive programme. We will show how cooperation can successfully be realized between different parties (language and speech technology, Flanders and the Netherlands, academia, industry and policy institutions) so as to achieve one common goal: progress in HLT.</abstract>
    </paper>
    <paper id="146">
      <author><first>Emiko</first><last>Suzuki</last></author>
      <author><first>Tomomi</first><last>Suzuki</last></author>
      <author><first>Kyoko</first><last>Kakihana</last></author>
      <title>On the Web Trilingual Sign Language Dictionary to Learn the foreign Sign Language without Learning a Target Spoken Language</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/262_pdf.pdf</url>
      <abstract>This paper describes a trilingual sign language dictionary (Japanese Sign Language and American Sign Language, and Korean Sign Language) which helps those who learn each sign language directly from their mother sign language. Our discussion covers two main points. The first describes the necessity of a trilingual dictionary. Since there is no universal sign language or real international sign language deaf people should learn at least four languages: they want to talk to people whose mother tongue is different from their owns, the mother sign language, the mother spoken language as the first intermediate language, the target spoken language as the second intermediate language, and the sign language in which they want to communicate. Those two spoken languages become language barriers for deaf people and our trilingual dictionary will remove the barrier. The second describes the use of computer. As the use of computers becomes widespread, it is increasingly convenient to study through computer software or Internet facilities. Our WWW dictionary system provides deaf people with an easy means of access using their mother-sign language, which means they don't have to overcome the barrier of learning a foreign spoken language. It also provides a way for people who are going to learn three sign languages to look up new vocabulary. We are further planning to examine how our dictionary system could be used to educate and assist deaf people.</abstract>
    </paper>
    <paper id="147">
      <author><first>Daniela</first><last>Goecke</last></author>
      <author><first>Andreas</first><last>Witt</last></author>
      <title>Exploiting logical document structure for anaphora resolution</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/266_pdf.pdf</url>
      <abstract>The aim of the paper is twofold. Firstly, an approach is presented how to select the correct antecedent for an anaphoric element according to the kind of text segments in which both of them occur. Basically, information on logical text structure (e.g. chapters, sections, paragraphs) is used in order to select the antecedent life span of a linguistic expression, i.e. some linguistic expressions are more likely to be chosen as an antecedent throughout the whole text than others. In addition, an appropriate search scope for an anaphora expressed by an expression can be defined according to the document structuring elements that include the linguistic expression. Corpus investigations give rise to the supposition that logical text structure influences the search scope of candidates for antecedents. Second, a solution is presented how to integrate the resources used for anaphora resolution. In this approach, multi-layered XML annotation is used in order to make a set of resources accessible for the anaphora resolution system.</abstract>
    </paper>
    <paper id="148">
      <author><first>Cédrick</first><last>Fairon</last></author>
      <author><first>Sébastien</first><last>Paumier</last></author>
      <title>A translated corpus of 30,000 <fixed-case>F</fixed-case>rench <fixed-case>SMS</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/270_pdf.pdf</url>
      <abstract>The development of communication technologies has contributed to the appearance of new forms in the written language that scientists have to study according to their peculiarities (typing or viewing constraints, synchronicity, etc). In the particular case of SMS (Short Message Service), studies are complicated by a lack of data, mainly due to technical constraints and privacy considerations. In this paper, we present a corpus of 30,000 French SMS collected through a project in Belgium named Faites don de vos SMS à la science (Give your SMS to Science). This corpus is unique in its quality, its size and the fact that the SMS have been manually translated into standard French. We will first describe the collection process and discuss the writers' profiles. Then we will explain in detail how the translation was carried out.</abstract>
    </paper>
    <paper id="149">
      <author><first>Feng</first><last>Zou</last></author>
      <author><first>Fu Lee</first><last>Wang</last></author>
      <author><first>Xiaotie</first><last>Deng</last></author>
      <author><first>Song</first><last>Han</last></author>
      <title>Evaluation of Stop Word Lists in <fixed-case>C</fixed-case>hinese Language</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/273_pdf.pdf</url>
      <abstract>In modern information retrieval systems, effective indexing can be achieved by removal of stop words. Till now many stop word lists have been developed for English language. However, no standard stop word list has been constructed for Chinese language yet. With the fast development of information retrieval in Chinese language, exploring the evaluation of Chinese stop word lists becomes critical. In this paper, to save the time and release the burden of manual comparison, we propose a novel stop word list evaluation method with a mutual information-based Chinese segmentation methodology. Experiments have been conducted on training texts taken from a recent international Chinese segmentation competition. Results show that effective stop word lists can improve the accuracy of Chinese segmentation significantly.</abstract>
    </paper>
    <paper id="150">
      <author><first>Anna</first><last>Sinopalnikova</last></author>
      <author><first>Pavel</first><last>Smrž</last></author>
      <title>Intelligent Dictionary Interfaces: Usability Evaluation of Access-Supporting Enhancements</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/275_pdf.pdf</url>
      <abstract>The present paper describes psycholinguistic experiments aimed at exploring the way people behave while accessing electronic dictionaries. In our work we focused on the access by meaning that, in comparison with the access by form, is currently less studied and very seldom implemented in modern dictionary interfaces. Thus, the goal of our experiments was to explore dictionary users requirements and to study what services an intelligent dictionary interface should be able to supply to help solving access by meaning problems. We tested several access-supporting enhancements of electronic dictionaries based on various language resources (corpora, wordnets, word association norms and explanatory dictionaries). Experiments were carried out with native speakers of three European languages  English, Czech and Russian. Results for monolingual and bilingual cases are presented.</abstract>
    </paper>
    <paper id="151">
      <author><first>Hannes</first><last>Mögele</last></author>
      <author><first>Moritz</first><last>Kaiser</last></author>
      <author><first>Florian</first><last>Schiel</last></author>
      <title><fixed-case>S</fixed-case>mart<fixed-case>W</fixed-case>eb <fixed-case>UMTS</fixed-case> Speech Data Collection: The <fixed-case>S</fixed-case>mart<fixed-case>W</fixed-case>eb Handheld Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/277_pdf.pdf</url>
      <abstract>In this paper we outline the German speech data collection for the SmartWeb project, which is fundedby the German Ministry of Science and Education. We focus on the SmartWeb Handheld Corpus (SHC), which has been collected by the Bavarian Archive for Speech Signals (BAS) at the Phonetic Institute (IPSK) of Munich University. Signals of SHC are being recorded in real-life environments(indoor and outdoor) with real background noise as well as real transmission line errors.We developed a new elicitation method and recording technique, calledsituational prompting, which facilitates collecting realistic dialogue speech data in a cost efficient way.We can show that almost realistic speech queries to a dialogue system issued over a mobile PDA or smart phonecan be collected very efficiently using an automatic speech server.We describe the technical and linguistic features of the resulting speech corpus, which will bepublicly available at BAS or ELDA.</abstract>
    </paper>
    <paper id="152">
      <author><first>Markéta</first><last>Lopatková</last></author>
      <author><first>Zdeněk</first><last>Žabokrtský</last></author>
      <author><first>Karolina</first><last>Skwarska</last></author>
      <title>Valency Lexicon of <fixed-case>C</fixed-case>zech Verbs: Alternation-Based Model</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/278_pdf.pdf</url>
      <abstract>The main objective of this paper is to introduce an alternation-based model of valency lexicon of Czech verbs VALLEX. Alternations describe regular changes in valency structure of verbs -- they are seen as transformations taking one lexical unit and return a modified lexical unit as a result. We characterize and exemplify syntactically-based and semantically-based' alternations and their effects on verb argument structure. The alternation-based model allows to distinguish a minimal form of lexicon, which provides compact characterization of valency structure of Czech verbs, and an expanded form of lexicon useful for some applications.</abstract>
    </paper>
    <paper id="153">
      <author><first>Darinka</first><last>Verdonik</last></author>
      <author><first>Matej</first><last>Rojc</last></author>
      <title>Are you ready for a call? - Spontaneous conversations in tourism for speech-to-speech translation systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/279_pdf.pdf</url>
      <abstract>The paper represents the Turdis database of spontaneous conversations in tourist domain in Slovenian language. Database was built for use in developing speech-to-speech translation components, however it can be used also for developing dialog systems or used for linguistic researches. The idea was to record a database of telephone conversations in tourism where the naturalness of conversations is affected as little as possible while we still obtain a permission for recording from all the speakers. When recording in studio environment there can be many problems. It is especially difficult to imitate a tourist agent if a speaker does not have such experiences and therefore lacks the background knowledge that a tourist agent has. Therefore the Turdis database was recorded with professional tourist agents. The agreement with local tourist companies enabled that we recorded a tourist agent while he was at his working place in his working time answering the telephone. Callers were contacted individually and asked to use the Turdis system and make a call to selected tourist company. Technically the recording was done using PC ISDN card. Database was orthographically transcribed with Transcriber tool. At the present it includes cca 43 000 words.</abstract>
    </paper>
    <paper id="154">
      <author><first>Moritz</first><last>Kaiser</last></author>
      <author><first>Hannes</first><last>Mögele</last></author>
      <author><first>Florian</first><last>Schiel</last></author>
      <title>Bikers Accessing the Web: The <fixed-case>S</fixed-case>mart<fixed-case>W</fixed-case>eb Motorbike Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/280_pdf.pdf</url>
      <abstract>Three advanced German speech corpora have been collected during theGerman SmartWeb project. One of them, the SmartWeb MotorbikeCorpus (SMC) is described in this paper.As with all SmartWeb speech corpora SMC is designed for a dialogue system dealing with open domains.The corpus is recorded under the special circumstances of a motorbike ride and contains utterances of the driver related to information retrieval from various sources and different topics. Audio tracks show characteristic noise from the engine and surrounding traffic as well as drop outs caused by the transmission over Bluetooth and the UMTS mobile network. We discuss the problems of the technical setup and the fully automatic evocation of natural-spoken queries by means of dialogue-like sequences.</abstract>
    </paper>
    <paper id="155">
      <author><first>Asanee</first><last>Kawtrakul</last></author>
      <author><first>Chaveevan</first><last>Pechsiri</last></author>
      <author><first>Trakul</first><last>Permpool</last></author>
      <author><first>Dussadee</first><last>Thamvijit</last></author>
      <author><first>Phukao</first><last>Sornprasert</last></author>
      <author><first>Chaiyakorn</first><last>Yingsaeree</last></author>
      <author><first>Mukda</first><last>Suktarachan</last></author>
      <title>Ontology Driven K-Portal Construction and K-Service Provision</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/281_pdf.pdf</url>
      <abstract>Knowledge has been crucial for the countrys development and business intelligence, where valuable knowledge is distributed over several websites with heterogeneous formats. Moreover, finding the needed information is a complex task since there has been lack of semantic relation and organization. Even if it has been found, an overload may occur because there is no content digestion. This paper focuses on ontology-driven knowledge extraction with natural language processing techniques and a framework of usercentric design for accessing the required information based on their demands. These demands can be expressed in the form of Knowwhat, Know-why, Know-where, Know-when, Know-how, and Know-who for a question answering system.</abstract>
    </paper>
    <paper id="156">
      <author><first>Corina</first><last>Forăscu</last></author>
      <author><first>Ionuț Cristian</first><last>Pistol</last></author>
      <author><first>Dan</first><last>Cristea</last></author>
      <title>Temporality in relation with discourse structure</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/282_pdf.pdf</url>
      <abstract>Temporal relations between events and times are often difficult to discover, time-consuming and expensive. In this paper a corpus study is performed to derive a strong relation between discourse structure, as revealed by Veins theory, and the temporal links between entities, as addressed in the TimeML annotation standard. The data interpretation helps us gain insight on how Veins theory can improve the manual and even (semi-) automatic detection of temporal relations.</abstract>
    </paper>
    <paper id="157">
      <author><first>Eva</first><last>Hajičová</last></author>
      <author><first>Petr</first><last>Sgall</last></author>
      <title>Corpus Annotation as a Test of a Linguistic Theory</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/283_pdf.pdf</url>
      <abstract>In the present contribution we claim that corpus annotation serves, among other things, as an invaluable test for linguistic theories standing behind the annotation schemes, and as such represents an irreplaceable resource of linguistic information for the build-up of grammars. To support this claim we present four linguistic phenomena for the study and relevant description of which in grammar a deep layer of corpus annotation as introduced in the Prague Dependency Treebank has brought important observations, namely the information structure of the sentence, condition of projectivity and word order, types of dependency relations and textual coreference.</abstract>
    </paper>
    <paper id="158">
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Magdelena</first><last>Prokopová</last></author>
      <title><fixed-case>C</fixed-case>zech-<fixed-case>E</fixed-case>nglish Word Alignment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/285_pdf.pdf</url>
      <abstract>We describe an experiment with Czech-English word alignment. Half a thousand sentences were manually annotated by two annotators in parallel and the most frequent reasons for disagreement are described. We evaluate the accuracy of GIZA++ alignment toolkit on the data and identify that lemmatization of the Czech part can reduce alignment error to a half. Furthermore we document that about 38% of tokens difficult for GIZA++ were difficult for humans already.</abstract>
    </paper>
    <paper id="159">
      <author><first>Emiliano</first><last>Guevara</last></author>
      <author><first>Sergio</first><last>Scalise</last></author>
      <author><first>Antonietta</first><last>Bisetto</last></author>
      <author><first>Chiara</first><last>Melloni</last></author>
      <title><fixed-case>MORBO</fixed-case>/<fixed-case>COMP</fixed-case>: a multilingual database of compound words</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/286_pdf.pdf</url>
      <abstract>The aim of this paper is to present the MORBO/COMP project, which has reached its final stage in development and will soon be published on-line. MORBO/COMP is large database of compound types in over 20 languages. The data for these languages have been collected and analysed by a group of morphologists from various European countries.</abstract>
    </paper>
    <paper id="160">
      <author><first>Daniel</first><last>Sonntag</last></author>
      <author><first>Massimo</first><last>Romanelli</last></author>
      <title>A Multimodal Result Ontology for Integrated Semantic Web Dialogue Applications</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/288_pdf.pdf</url>
      <abstract>General purpose ontologies and domain ontologies make up the infrastructure of the Semantic Web, which allow for accurate data representations with relations, and data inferences. In our approach to multimodal dialogue systems providing question answering functionality (SMARTWEB), the ontological infrastructure is essential. We aim at an integrated approach in which all knowledge-aware system modules are based on interoperating ontologiesin a common data model. The discourse ontology is meant to provide the necessary dialogue- and HCI concepts. We present the ontological syntactic structure of multimodal question answering results as partof this discourse ontology which extends the W3C EMMA annotation framework and uses MPEG-7 annotations. In addition, we describe anextension to ontological result structures where automatic and context-based sorting mechanisms can be naturally incorporated.</abstract>
    </paper>
    <paper id="161">
      <author><first>Nicole</first><last>Grégoire</last></author>
      <title>Elaborating the parameterized Equivalence Class Method for <fixed-case>D</fixed-case>utch</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/292_pdf.pdf</url>
      <abstract>This paper discusses the parameterized Equivalence Class Method for Dutch, an approach developed to incorporate standard lexical representations for Dutch idioms into representations required by any specific NLP system with as minimal manual work as possible. The purpose of the paper is to give an overview of parameters applicable to Dutch, which are determined by examining a large set of data and two Dutch NLP systems. The effects of the introduced parameters are evaluated and the results presented.</abstract>
    </paper>
    <paper id="162">
      <author><first>Anders</first><last>Green</last></author>
      <author><first>Helge</first><last>Hüttenrauch</last></author>
      <author><first>Elin Anna</first><last>Topp</last></author>
      <author><first>Kerstin</first><last>Severinson</last></author>
      <title>Developing a <fixed-case>C</fixed-case>ontextualized<fixed-case>M</fixed-case>ultimodal Corpus for Human-Robot Interaction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/293_pdf.pdf</url>
      <abstract>This paper describes the development process of a contextualized corpus for research on Human-Robot Communication. The data have been collected in two Wizard-of-Oz user studies performedwith 22 and 5 users respectively in a scenario that is called the HomeTour. In this scenario the users show the environment (a single room, or a whole floor) to the robot using a combination of speech and gestures. The corpus has been transcribed and annotated with respect to gestures and conversational acts, thus forming a core annotation. We have also annotated or linked other types of data, e.g., laser range finder readings, positioning analysis, questionnaire data and task descriptions that form the annotated context of the scenario. By providing a rich set of different annotated data, thecorpus is thus an important resource both for research on natural language speech interfaces for robots and for research on human-robot communication in general.</abstract>
    </paper>
    <paper id="163">
      <author><first>Wei-Yun</first><last>Ma</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <title>Uniform and Effective Tagging of a Heterogeneous Giga-word Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/294_pdf.pdf</url>
      <abstract>Tagging as the most crucial annotation of language resources can still be challenging when the corpus size is big and when the corpus data is not homogeneous. The Chinese Gigaword Corpus is confounded by both challenges. The corpus containsroughly 1.12 billion Chinese characters from two heterogeneous sources: respective news in Taiwan and in Mainland China. In other words, in addition to its size, the data also contains two variants of Chinese that are known to exhibit substantial linguistic differences. We utilize Chinese Sketch Engine as the corpus query tool, by which grammar behaviours of the two heterogeneous resources could be captured and displayed in a unified web interface. In this paper, we report our answer to the two challenges to effectively tag this large-scale corpus. The evaluation result shows our mechanism of tagging maintains high annotation quality.</abstract>
    </paper>
    <paper id="164">
      <author><first>Ana-Maria</first><last>Barbu</last></author>
      <author><first>Emil</first><last>Ionescu</last></author>
      <author><first>Verginica Barbu</first><last>Mititelu</last></author>
      <title><fixed-case>R</fixed-case>omanian Valence Dictionary in <fixed-case>XML</fixed-case> Format</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/295_pdf.pdf</url>
      <abstract>Valence dictionaries are dictionaries in which logical predicates (most of the times verbs) are inventoried alongside with the semantic and syntactic information regarding the role of the arguments with which they combine, as well as the syntactic restrictions these arguments have to obey. In this article we present the incipient stage of the project Syntactic and semantic database in XML format: an HPSG representation of verb valences in Romanian. Its aim is the development of a valence dictionary in XML format for a set of 3000 Romanian verbs. Valences are specified for each sense of each verb, alongside with an illustrative example, possible argument alternations and a set of multiword expressions in which the respective verb occurs with the respective sense. The grammatical formalism we make use of is Head-driven Phrase Structure Grammar, which offers one of the most comprehensive frames of encoding various types of linguistic information for lexical items. XML is the most appropriate mark-up language for describing information structured in HPSG framework. The project can be further on extended so that to cover all Romanian verbs (around 7000) and also other predicates (nouns, adjectives, prepositions).</abstract>
    </paper>
    <paper id="165">
      <author><first>Niels Ole</first><last>Bernsen</last></author>
      <author><first>Thomas K.</first><last>Hansen</last></author>
      <author><first>Svend</first><last>Kiilerich</last></author>
      <author><first>Torben Kruchov</first><last>Madsen</last></author>
      <title>Field Evaluation of a Single-Word Pronunciation Training System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/296_pdf.pdf</url>
      <abstract>Many learning tasks require substantial skills training. Ideally, the student might benefit the most from having a human expert  a teacher or trainer  at hand throughout, but human expertise remains a scarce resource. The second-best solution could be to do skills training with a computer-based self-training system. This vision of the computer as tutor currently motivates increasing efforts world-wide, in all manner of fields, including that of computer-assisted language learning, or CALL. But, as pointed out by Hincks [2003], along with the growth of the CALL area comes a growing need for empirical evidence that CALL systems have a beneficial effect. This point is reiterated by Chapelle [2002] who defines the goal for Computer Assisted Second Language Research as the gathering of evidence for the effect of CALL and instructional design. This paper presents results of a field test of our pronunciation training system which enables immigrants and others to self-train their pronunciation skills of single Danish words.</abstract>
    </paper>
    <paper id="166">
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Wenjie</first><last>Li</last></author>
      <author><first>Qin</first><last>Lu</last></author>
      <title>Mining Implicit Entities in Queries</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/297_pdf.pdf</url>
      <abstract>Entities are pivotal in describing events and objects, and also very important in Document Summarization. In general only explicit entities which can be extracted by a Named Entity Recognizer are used in real applications. However, implicit entities hidden behind the phrases or words, e.g. entity referred by the phrase cross border, are proved to be helpful in Document Summarization. In our experiment, we extract the implicit entities from the web resources.</abstract>
    </paper>
    <paper id="167">
      <author><first>Kiyotaka</first><last>Uchimoto</last></author>
      <author><first>Ryoji</first><last>Hamabe</last></author>
      <author><first>Takehiko</first><last>Maruyama</last></author>
      <author><first>Katsuya</first><last>Takanashi</last></author>
      <author><first>Tatsuya</first><last>Kawahara</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>Dependency-structure Annotation to Corpus of Spontaneous <fixed-case>J</fixed-case>apanese</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/298_pdf.pdf</url>
      <abstract>In Japanese, syntactic structure of a sentence is generally represented by the relationship between phrasal units, or bunsetsus inJapanese, based on a dependency grammar. In the same way, thesyntactic structure of a sentence in a large, spontaneous, Japanese-speech corpus, the Corpus of Spontaneous Japanese (CSJ), isrepresented by dependency relationships between bunsetsus. This paper describes the criteria and definitions of dependency relationships between bunsetsus in the CSJ. The dependency structure of the CSJ is investigated, and the difference in the dependency structures ofwritten text and spontaneous speech is discussed in terms of thedependency accuracies obtained by using a corpus-based model. It is shown that the accuracy of automatic dependency-structure analysis canbe improved if characteristic phenomena of spontaneous speech such as self-corrections, basic utterance units in spontaneous speech, and bunsetsus that have no modifiee are detected and used for dependency-structure analysis.</abstract>
    </paper>
    <paper id="168">
      <author id="nerea-areta"><first>N.</first><last>Areta</last></author>
      <author id="antton-gurrutxaga"><first>A.</first><last>Gurrutxaga</last></author>
      <author id="igor-leturia"><first>I.</first><last>Leturia</last></author>
      <author id="ziortza-polin"><first>Z.</first><last>Polin</last></author>
      <author id="rafa-saiz"><first>R.</first><last>Saiz</last></author>
      <author id="inaki-alegria"><first>I.</first><last>Alegria</last></author>
      <author id="xabier-artola"><first>X.</first><last>Artola</last></author>
      <author id="arantza-diaz-de-ilarraza"><first>A.</first><last>Diaz de Ilarraza</last></author>
      <author id="nerea-ezeiza"><first>N.</first><last>Ezeiza</last></author>
      <author id="aitor-sologaistoa"><first>A.</first><last>Sologaistoa</last></author>
      <author id="aitor-soroa"><first>A.</first><last>Soroa</last></author>
      <author id="andoni-valverde"><first>A.</first><last>Valverde</last></author>
      <title>Structure, Annotation and Tools in the <fixed-case>B</fixed-case>asque <fixed-case>ZT</fixed-case> Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/299_pdf.pdf</url>
      <abstract>The ZT corpus (Basque Corpus of Science and Technology) is a tagged collection of specialized texts in Basque, which wants to be a main resource in research and development about written technical Basque: terminology, syntax and style. It will be the first written corpus in Basque which will be distributed by ELDA (at the end of 2006) and it wants to be a methodological and functional reference for new projects in the future (i.e. a national corpus for Basque). We also present the technology and the tools to build this Corpus. These tools, Corpusgile and Eulia, provide a flexible and extensible infrastructure for creating, visualizing and managing corpora and for consulting, visualizing and modifying annotations generated by linguistic tools.</abstract>
    </paper>
    <paper id="169">
      <author><first>Joseph</first><last>Polifroni</last></author>
      <author><first>Marilyn</first><last>Walker</last></author>
      <title>Learning Database Content for Spoken Dialogue System Design</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/301_pdf.pdf</url>
      <abstract>Spoken dialogue systems are common interfaces to backend data in information retrieval domains. As more data is made available on the Web and IE technology matures, dialogue systems, whether they be speech- or text-based, will be more in demand to provide user-friendly access to this data. However, dialogue systems must become both easier to configure, as well as more informative than the traditional form-based systems that are currently available. We present techniques in this paper to address the issue of automating both content selection for use in summary responses and in system initiative queries.</abstract>
    </paper>
    <paper id="170">
      <author><first>Jorge</first><last>Civera</last></author>
      <author><first>Alfons</first><last>Juan</last></author>
      <title>Bilingual Machine-Aided Indexing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/304_pdf.pdf</url>
      <abstract>The proliferation of multilingual documentation in our Information Society has become a common phenomenon. This documentation is usually categorised by hand, entailing a time-consuming and arduous burden. This is particularly true in the case of keyword assignment, in which a list of keywords (descriptors) from a controlled vocabulary (thesaurus) is assigned to a document. A possible solution to alleviate this problem comes from the hand of the so-called Machine-Aided Indexing (MAI) systems. These systems work in cooperation with professional indexer by providing a initial list of descriptors from which those most appropiated will be selected. This way of proceeding increases the productivity and eases the task of indexers. In this paper, we propose a statistical text classification framework for bilingual documentation, from which we derive two novel bilingual classifiers based on the naive combination of monolingual classifiers. We report preliminary results on the multilingual corpus Acquis Communautaire (AC) that demonstrates the suitability of the proposed classifiers as the backend of a fully-working MAI system.</abstract>
    </paper>
    <paper id="171">
      <author><last>Panunzi</last><first>Alessandro</first></author>
      <author><last>Fabbri</last><first>Marco</first></author>
      <author><last>Moneglia</last><first>Massimo</first></author>
      <title>Integrating Methods and <fixed-case>LR</fixed-case>s for Automatic Keyword Extraction from Open Domain Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/305_pdf.pdf</url>
      <abstract>The paper presents a tool for keyword extraction from multilingual resources developed within the AXMEDIS project. In this tool lexical collocations (Sinclair, 1991) internal to documents are used to enhance the performance obtained through standard statistical procedure. A first set of mono-term keywords is extracted through the TF.IDF algorithm (Salton, 1989). The internal analysis of the document generates a second set of multi-term keywords based on the first set, rather than on multi-term frequency comparison with a general resource (Witten et al. 1999). Collocations in which a mono-term keyword occurs as the head are considered as multi-term keywords, and are assumed to increase the identification of the content. The evaluation compares the results of the TF.IDF procedure and the ones obtained with the enhanced procedure in terms of precision. Each set of keywords received a value from the point of view of a possible user, regarding: (a) overall efficiency of the whole set of keywords for the identification of the content; (b) adequacy of each extracted keyword. Results show that multi-term keywords increase the content identification with a 100% relative factor and that the adequacy is enhanced in 33% of cases.</abstract>
    </paper>
    <paper id="172">
      <author><first>Luís Fernando</first><last>Costa</last></author>
      <author><first>Luís</first><last>Sarmento</last></author>
      <title>Component Evaluation in a Question Answering System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/306_pdf.pdf</url>
      <abstract>Automatic question answering (QA) is a complex task, which lies in the cross-road of Natural Language Processing, Information Retrieval and Human Computer Interaction. A typical QA system has four modules  question processing, document retrieval, answer extraction and answer presentation. In each of these modules, a multitude of tools can be used. Therefore, the performance evaluation of each of these components is of great importance in order to check their impact in the global performance, and to conclude whether these components are necessary, need to be improved or substituted.This paper describes some experiments performed in order to evaluate several components of the question answering system Esfinge.We describe the experimental set up and present the results of error analysis based on runtime logs of Esfinge. We present the results of component analysis, which provides good insights about the importance of the individual components and pre-processing modules at various levels, namely stemming, named-entity recognition, PoS Filtering and filtering of undesired answers. We also present the results of substituting the document source in which Esfinge tries to find possible answers and compare the results obtained using web sources such as Google, Yahoo and BACO, a large database of web documents in Portuguese.</abstract>
    </paper>
    <paper id="173">
      <author><first>Stefan</first><last>Breuer</last></author>
      <author><first>Sven</first><last>Bergmann</last></author>
      <author><first>Ralf</first><last>Dragon</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <title>Set-up of a Unit-Selection Synthesis with a Prominent Voice</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/307_pdf.pdf</url>
      <abstract>In this paper, we describe the set-up process and an initial evaluation of a unit-selection speech synthesizer. The synthesizer is specific in that it is intended to speak with a prominent voice. As a consequence, only very limited resources were available for setting up the unit database. These resources have been extracted from an audio book, segmented with the help of an HMM-based wrapper, and then used with the non-uniform unit-selection approach implemented in the Bonn Open Synthesis System (BOSS). In order to adapt the database to the BOSS implementation, the label files were amended by phrase boundaries, converted to XML, amended by prosodic and spectral information, and then further converted to a MySQL relational database structure. The BOSS system selects units on the basis of this information, adding individual unit costs to the concatenation costs given by MFCC and F0 distances. The paper discusses the problems which occurred during the database set-up, the invested effort, as well as the quality level which can be reached by this approach.</abstract>
    </paper>
    <paper id="174">
      <author><first>Nasredine</first><last>Semmar</last></author>
      <author><first>Meriama</first><last>Laib</last></author>
      <author><first>Christian</first><last>Fluhr</last></author>
      <title>A Deep Linguistic Analysis for Cross-language Information Retrieval</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/308_pdf.pdf</url>
      <abstract>Cross-language information retrieval consists in providing a query in one language and searching documents in one or different languages. These documents are ordered by the probability of being relevant to the user's request. The highest ranked document is considered to be the most likely relevant document. The LIC2M cross-language information retrieval system is a weighted Boolean search engine based on a deep linguistic analysis of the query and the documents. This system is composed of a linguistic analyzer, a statistic analyzer, a reformulator, a comparator and a search engine. The linguistic analysis processes both documents to be indexed and queries to extract concepts representing their content. This analysis includes a morphological analysis, a part-of-speech tagging and a syntactic analysis. In this paper, we present the deep linguistic analysis used in the LIC2M cross-lingual search engine and we will particularly focus on the impact of the syntactic analysis on the retrieval effectiveness.</abstract>
    </paper>
    <paper id="175">
      <author><first>Diana</first><last>Santos</last></author>
      <author><first>Susana</first><last>Inácio</last></author>
      <title>Annotating <fixed-case>COMPARA</fixed-case>, a Grammar-aware Parallel Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/309_pdf.pdf</url>
      <abstract>In this paper we describe the annotation of COMPARA, currently the largest post-edited parallel corpora which include Portuguese. We describe the motivation, the results so far, and the way the corpus is being annotated. We also provide the first grounded results about syntactical ambiguity in Portuguese. Finally, we discuss some interesting problems in this connection.</abstract>
    </paper>
    <paper id="176">
      <author><first>Lina</first><last>Henriksen</last></author>
      <author><first>Claus</first><last>Povlsen</last></author>
      <author><first>Andrejs</first><last>Vasiljevs</last></author>
      <title><fixed-case>E</fixed-case>uro<fixed-case>T</fixed-case>erm<fixed-case>B</fixed-case>ank - a Terminology Resource based on Best Practice</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/310_pdf.pdf</url>
      <abstract>The new EU member countries face the problems of terminology resource fragmentation and lack of coordination in terminology development in general. The EuroTermBank project aims at contributing to improve the terminology infrastructure of the new EU countries and the project will result in a centralized online terminology bank - interlinked to other terminology banks and resources - for languages of the new EU member countries. The main focus of this paper is on a description of how to identify best practice within terminology work seen from a broad perspective. Surveys of real life terminology work have been conducted and these surveys have resulted in identification of scenario specific best practice descriptions of terminology work. Furthermore, this paper will present an outline of the specific criteria that have been used for selection of existing term resources to be included in the EuroTermBank database.</abstract>
    </paper>
    <paper id="177">
      <author><first>Florbela</first><last>Barreto</last></author>
      <author><first>António</first><last>Branco</last></author>
      <author><first>Eduardo</first><last>Ferreira</last></author>
      <author><first>Amália</first><last>Mendes</last></author>
      <author><first>Maria Fernanda Bacelar do</first><last>Nascimento</last></author>
      <author><first>Filipe</first><last>Nunes</last></author>
      <author><first>João Ricardo</first><last>Silva</last></author>
      <title>Open Resources and Tools for the Shallow Processing of <fixed-case>P</fixed-case>ortuguese: The <fixed-case>T</fixed-case>ag<fixed-case>S</fixed-case>hare Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/311_pdf.pdf</url>
      <abstract>This paper presents the TagShare project and the linguistic resources and tools for the shallow processing of Portuguese developed in its scope. These resources include a 1 million token corpus that has been accurately hand annotated with a variety of linguistic information, as well as several state of the art shallow processing tools capable of automatically producing that type of annotation. At present, the linguistic annotations in the corpus are sentence and paragraph boundaries, token boundaries, morphosyntactic POS categories, values of inflection features, lemmas and namedentities. Hence, the set of tools comprise a sentence chunker, a tokenizer, a POS tagger, nominal and verbal analyzers and lemmatizers, a verbal conjugator, a nominal inflector, and a namedentity recognizer, some of which underline several online services.</abstract>
    </paper>
    <paper id="178">
      <author><first>Luís</first><last>Sarmento</last></author>
      <author><first>Belinda</first><last>Maia</last></author>
      <author><first>Diana</first><last>Santos</last></author>
      <author><first>Ana</first><last>Pinto</last></author>
      <author><first>Luís</first><last>Cabral</last></author>
      <title>Corpógrafo V3 - From Terminological Aid to Semi-automatic Knowledge Engineering</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/312_pdf.pdf</url>
      <abstract>In this paper we will present Corpógrafo, a mature web-based environment for working with corpora, for terminology extraction, and for ontology development. We will explain Corpógrafos workflow and describe the most important information extraction methods used, namely its term extraction, and definition / semantic relations identification procedures. We will describe current Corpógrafo users and present a brief overview of the XML format currently used to export terminology databases. Finally, we present future improvements for this tool.</abstract>
    </paper>
    <paper id="179">
      <author><first>Liviu</first><last>Dinu</last></author>
      <author><first>Anca</first><last>Dinu</last></author>
      <title>On the data base of <fixed-case>R</fixed-case>omanian syllables and some of its quantitative and cryptographic aspects</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/313_pdf.pdf</url>
      <abstract>In this paper we argue for the need to construct a data base of Romanian syllables. We explain the reasons for our choice of the DOOM corpus which we have used. We describe the way syllabification was performed and explain how we have constructed the data base. The main quantitative aspects which we have extracted from our research are presented. We also computed the entropy of the syllables and the entropy of the syllables w.r.t. the consonant-vowel structure. The results are compared with results of similar researches realized for different languages.</abstract>
    </paper>
    <paper id="180">
      <author><first>Alessandro Bahgat</first><last>Shehata</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <title>A Dependency-based Algorithm for Grammar Conversion</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/314_pdf.pdf</url>
      <abstract>In this paper we present a model to transfer a grammatical formalism in another. The model is applicable only on restrictive conditions. However, it is fairly useful for many purposes: parsing evaluation, researching methods for truly combining different parsing outputs to reach better parsing performances, and building larger syntactically annotated corpora for data-driven approaches. The model has been tested over a case study: the translation of the Turin Tree Bank Grammar to the Shallow Grammar of the CHAOS Italian parser.</abstract>
    </paper>
    <paper id="181">
      <author><first>Khurshid</first><last>Ahmad</last></author>
      <author><first>Maria Teresa</first><last>Musacchio</last></author>
      <author><first>Giuseppe</first><last>Palumbo</last></author>
      <title>Ontological and Terminological Commitments and the Discourse of Specialist Communities</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/315_pdf.pdf</url>
      <abstract>The paper presents a corpus-based study aimed at an analysis of ontological and terminological commitments in the discourse of specialist communities. The analyzed corpus contains the lectures delivered by the Nobel Prize winners in Physics and Economics. The analysis focuses on (a) the collocational use of automatically identified domain-specific terms and (b) a description of meta-discourse in the lectures. Candidate terms are extracted based on the z-score of frequency and weirdness. Compounds comprising these candidate terms are then identified using the ontology representation system Protégé. This method is then replicated to complete analysis by including an investigation of metadiscourse markers signalling how writers project themselves into their work.</abstract>
    </paper>
    <paper id="182">
      <author><first>Alessandro</first><last>Oltramari</last></author>
      <title><fixed-case>L</fixed-case>exi<fixed-case>P</fixed-case>ass methodology: a conceptual path from frames to senses and back</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/318_pdf.pdf</url>
      <abstract>In this paper we claim that an integration of FrameNet and WordNet will improve interoperability, user-friendliness and usability of both lexical resources. If the former provides a sophisticated representational structure compared to a narrow lexical coverage, the latter - on the other side - supplies a dense network of word senses and semantic relations although not supporting advanced accessibility (i.e., via frames). According to the integration perspective we present in the paper, we introduce LexiPass methodology, which combines Burckardts tool WordNet Detour of FrameNet with basic statistical analysis, enabling frame-guided search and extraction of domain synsets from WordNet.</abstract>
    </paper>
    <paper id="183">
      <author><first>Chloé</first><last>Clavel</last></author>
      <author><first>Ioana</first><last>Vasilescu</last></author>
      <author><first>Laurence</first><last>Devillers</last></author>
      <author><first>Thibaut</first><last>Ehrette</last></author>
      <author><first>Gaël</first><last>Richard</last></author>
      <title>Fear-type emotions of the <fixed-case>SAFE</fixed-case> Corpus: annotation issues</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/319_pdf.pdf</url>
      <abstract>The present research focuses on annotation issues in the context of the acoustic detection of fear-type emotions for surveillance applications. The emotional speech material used for this study comes from the previously collected SAFE Database (Situation Analysis in a Fictional and Emotional Database) which consists of audio-visual sequences extracted from movie fictions. A generic annotation scheme was developed to annotate the various emotional manifestations contained in the corpus. The annotation was carried out by two labellers and the two annotations strategies are confronted. It emerges that the borderline between emotion and neutral vary according to the labeller. An acoustic validation by a third labeller allows at analysing the two strategies. Two human strategies are then observed: a first one, context-oriented which mixes audio and contextual (video) information in emotion categorization; and a second one, based mainly on audio information. The k-means clustering confirms the role of audio cues in human annotation strategies. It particularly helps in evaluating those strategies from the point of view of a detection system based on audio cues.</abstract>
    </paper>
    <paper id="184">
      <author><first>Arne</first><last>Mauser</last></author>
      <author><first>Evgeny</first><last>Matusov</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <title>Training a Statistical Machine Translation System without <fixed-case>GIZA</fixed-case>++</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/320_pdf.pdf</url>
      <abstract>The IBM Models (Brown et al., 1993) enjoy great popularity in the machine translation community because they offer high quality word alignments and a free implementation is available with the GIZA++ Toolkit (Och and Ney, 2003). Several methods have been developed to overcome the asymmetry of the alignment generated by the IBM Models. A remaining disadvantage, however, is the high model complexity. This paper describes a word alignment training procedure for statistical machine translation that uses a simple and clear statistical model, different from the IBM models. The main idea of the algorithm is to generate a symmetric and monotonic alignment between the target sentence and a permutation graph representing different reorderings of the words in the source sentence. The quality of the generated alignment is shown to be comparable to the standard GIZA++ training in an SMT setup.</abstract>
    </paper>
    <paper id="185">
      <author><first>Evie</first><last>Coussé</last></author>
      <author><first>Steven</first><last>Gillis</last></author>
      <title>Regional Bias in the Broad Phonetic Transcriptions of the Spoken <fixed-case>D</fixed-case>utch Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/323_pdf.pdf</url>
      <abstract>In this paper, we assess an aspect of the quality of the broad phonetic transcriptions in the Spoken Dutch Corpus (CGN). The corpus contains speech from native speakers of Dutch originating from The Netherlands and the Dutch speaking part of Belgium. The phonetic transcriptions were made by transcribers from both regions. In previous research, we have identified regional differences in the transcribers' behaviour. In this paper, we explore the precise sources of the regional bias in the CGN transcriptions and we evaluate its impact on the phonetic transcriptions. More specifically, (1) the regional bias in the canonical transcriptions that served as the basis for the verification task of the transcribers is critically analysed, and (2) we verify in an experiment the regional bias introduced by the transcribers themselves. The possible effects of this inherent regional bias in the CGN transcriptions on subsequent linguistic analyses are briefly discussed.</abstract>
    </paper>
    <paper id="187">
      <author><first>Atsushi</first><last>Fujii</last></author>
      <author><first>Makoto</first><last>Iwayama</last></author>
      <author><first>Noriko</first><last>Kando</last></author>
      <title>Test Collections for Patent Retrieval and Patent Classification in the Fifth <fixed-case>NTCIR</fixed-case> Workshop</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/324_pdf.pdf</url>
      <abstract>This paper describes the test collections produced for the Patent Retrieval Task in the Fifth NTCIR Workshop. We performed the invalidity search task, in which each participant group searches a patent collection for the patents that can invalidate the demand in an existing claim. For this purpose, we performed both document and passage retrieval tasks. We also performed the automatic patent classification task using the F-term classification system. The test collections will be available to the public for research purposes.</abstract>
    </paper>
    <paper id="188">
      <author><first>Željko</first><last>Agić</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <title>Evaluating Morphosyntactic Tagging of <fixed-case>C</fixed-case>roatian Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/326_pdf.pdf</url>
      <abstract>This paper describes results of the first successful effort in applying a stochastic strategy  or, namely, a second order Markov model paradigm implemented by the TnT trigram tagger  to morphosyntactic tagging of Croatian texts. Beside the tagger, for purposes of both training and testing, we had at our disposal only a 100 Kw Croatia Weekly newspaper subcorpus, manually tagged using approximately 1000 different MULTEXT-East v3 morphosyntactic tags. The test basically consisted of randomly assigning a variable size portion of the corpus for the taggers training procedure and also another fixed-size portion, sized at 10% of the corpus, for the tagging procedure itself; this method allowed us not only to provide preliminary results regarding tagger accuracy on Croatian texts, but also to inspect the behavior of the stochastic tagging paradigm in general. The results were then taken from the test case providing 90% of the corpus for training purposes and varied from around 86% in the worst case scenario up to a peak of around 95% correctly assigned full MSD tags. Results on PoS only expectedly reached the human error level, with TnT correctly tagging above 98% of test sets on average. Most MSD errors occurred on types with the highest number of candidate tags per word form  nouns, pronouns and adjectives  while errors on PoS, although following the same pattern, were almost insignificant. Detailed insight on tagging, F-measure for all PoS categories is provided in the course of the paper along with other facts of interest.</abstract>
    </paper>
    <paper id="189">
      <author><first>Martin</first><last>Rajman</last></author>
      <author><first>Marita</first><last>Ailomaa</last></author>
      <author><first>Agnes</first><last>Lisowska</last></author>
      <author><first>Miroslav</first><last>Melichar</last></author>
      <author><first>Susan</first><last>Armstrong</last></author>
      <title>Extending the <fixed-case>W</fixed-case>izard of <fixed-case>O</fixed-case>z Methodologie for Multimodal Language-enabled Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/328_pdf.pdf</url>
      <abstract>In this paper we present a proposal for extending the standard Wizard of Oz experimental methodology to language-enabled multimodal systems. We first discuss how Wizard of Oz experiments involving multimodal systems differ from those involving voice-only systems. We then go on to discuss the Extended Wizard of Oz methodology and the Wizard of Oz testing environment and protocol that we have developed. We then describe an example of applying this methodology to Archivus, a multimodal system for multimedia meeting retrieval and browsing. We focus in particular on the tools that the wizards would need to successfully and efficiently perform their tasks in a multimodal context. We conclude with some general comments about which questions need to be addressed when developing and using the Wizard of Oz methodology for testing multimodal systems.</abstract>
    </paper>
    <paper id="190">
      <author><first>Gertjan</first><last>van Noord</last></author>
      <author><first>Ineke</first><last>Schuurman</last></author>
      <author><first>Vincent</first><last>Vandeghinste</last></author>
      <title>Syntactic Annotation of Large Corpora in <fixed-case>STEVIN</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/329_pdf.pdf</url>
      <abstract>The construction of a 500-million-word reference corpus of written Dutch has been identified as one of the priorities in the Dutch/Flemish STEVIN programme. For part of this corpus, manually corrected syntactic annotations will be provided. The paper presents the background of the syntactic annotation efforts, the Alpino parser which is used as an important tool for constructing the syntactic annotations, as well as a number of other annotation tools and guidelines. For the full STEVIN corpus, automatically derived syntactic annotations will be provided in a later phase of the programme. A number of arguments is provided suggesting that such a resource can be very useful for applications in information extraction, ontology building, lexical acquisition, machine translation and corpus linguistics.</abstract>
    </paper>
    <paper id="191">
      <author><first>Davide</first><last>Buscaldi</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <title>Mining Knowledge from<fixed-case>W</fixed-case>ikipedia for the Question Answering task</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/332_pdf.pdf</url>
      <abstract>Although significant advances have been made recently in the Question Answering technology, more steps have to be undertaken in order to obtain better results. Moreover, the best systems at the CLEF and TREC evaluation exercises are very complex systems based on custom-built, expensive ontologies whose aim is to provide the systems with encyclopedic knowledge. In this paper we investigated the use of Wikipedia, the open domain encyclopedia, for the Question Answering task. Previous works considered Wikipedia as a resource where to look for the answers to the questions. We focused on some different aspects of the problem, such as the validation of the answers as returned by our Question Answering System and on the use of Wikipedia categories in order to determine a set of patterns that should fit with the expected answer. Validation consists in, given a possible answer, saying wether it is the right one or not. The possibility to exploit the categories ofWikipedia was not considered until now. We performed our experiments using the Spanish version of Wikipedia, with the set of questions of the last CLEF Spanish monolingual exercise. Results show that Wikipedia is a potentially useful resource for the Question Answering task.</abstract>
    </paper>
    <paper id="192">
      <author><first>Sabine</first><last>Schulte im Walde</last></author>
      <title>Human Verb Associations as the Basis for Gold Standard Verb Classes: Validation against <fixed-case>G</fixed-case>erma<fixed-case>N</fixed-case>et and <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/333_pdf.pdf</url>
      <abstract>We describe a gold standard for semantic verb classes which is based on human associations to verbs. The associations were collected in a web experiment and then applied as verb features in a hierarchical cluster analysis. We claim that the resulting classes represent a theory-independent gold standard classification which covers a variety of semantic verb relations, and whose features can be used to guide the feature selection in automatic processes. To evaluate our claims, the association-based classification is validated against two standard approaches to semantic verb classes, GermaNet and FrameNet.</abstract>
    </paper>
    <paper id="193">
      <author><first>Peter W.</first><last>Wagacha</last></author>
      <author><first>Guy</first><last>De Pauw</last></author>
      <author><first>Pauline W.</first><last>Githinji</last></author>
      <title>A Grapheme-Based Approach for Accent Restoration in <fixed-case>G</fixed-case>ikuyu</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/336_pdf.pdf</url>
      <abstract>The orthography of Gikuyu includes a number of accented characters to represent the entire vowel system. These characters are however not readily available on standard computer keyboards and are usually represented as the nearest available character. This can render reading and understanding written texts more difficult. This paper describes a system that is able to automatically place these accents in Gikuyu text on the basis of local graphemic context. This approach avoids the need for an extensive digital lexicon, typically not available for resource-scarce languages. Using an extended trigram based-approach, the experiments show that this method can achieve a very high accuracy even with a limited amount of digitally available textual data. The experiments on Gikuyu are contrasted with experiments on French, German and Dutch.</abstract>
    </paper>
    <paper id="194">
      <author><first>Juan José Rodríguez</first><last>Soler</last></author>
      <author><first>Pedro Concejero</first><last>Cerezo</last></author>
      <author><first>Daniel Tapias</first><last>Merino</last></author>
      <author><first>José</first><last>Sánchez</last></author>
      <title><fixed-case>MEDUSA</fixed-case>: User-Centred Design and usability evaluation of Automatic Speech Recognition telephone services in Telefónica Móviles España</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/338_pdf.pdf</url>
      <abstract>One of the greatest challenges in the design of speech recognition based interfaces is about the navigation through the different service hierarchies and structures. On the one hand, the interactions based on human machine dialogues force a high level of hierarchical structuring of services, and on the other hand, it is necessary to wait for the last phases of the user interface development to obtain a global vision of the dialogue problems by means of user trials. To tackle these problems, Telefónica Móviles España has carried out several projects with the final aim to define a corporate methodology based on rapid prototyping of the user interfaces, so that designers could integrate the process of design of voice interfaces with emulations of the navigation through the flow charts. This was also the starting point for a specific software product (MEDUSA) which addresses the needs of rapid prototyping of these user interfaces from the earliest stages of the design and analysis phases.</abstract>
    </paper>
    <paper id="195">
      <author><first>Aljoscha</first><last>Burchardt</last></author>
      <author><first>Katrin</first><last>Erk</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <author><first>Andrea</first><last>Kowalski</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <author><first>Manfred</first><last>Pinkal</last></author>
      <title>The <fixed-case>SALSA</fixed-case> Corpus: a <fixed-case>G</fixed-case>erman Corpus Resource for Lexical Semantics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/339_pdf.pdf</url>
      <abstract>This paper describes the SALSA corpus, a large German corpus manually annotated with manual role-semantic annotation, based on the syntactically annotated TIGER newspaper corpus. The first release, comprising about 20,000 annotated predicate instances (about half the TIGER corpus), is scheduled for mid-2006. In this paper we discuss the annotation framework (frame semantics) and its cross-lingual applicability, problems arising from exhaustive annotation, strategies for quality control, and possible applications.</abstract>
    </paper>
    <paper id="196">
      <author><first>Ralf</first><last>Steinberger</last></author>
      <author><first>Bruno</first><last>Pouliquen</last></author>
      <author><first>Anna</first><last>Widiger</last></author>
      <author><first>Camelia</first><last>Ignat</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <author><first>Dan</first><last>Tufiş</last></author>
      <author><first>Dániel</first><last>Varga</last></author>
      <title>The <fixed-case>JRC</fixed-case>-<fixed-case>A</fixed-case>cquis: A Multilingual Aligned Parallel Corpus with 20+ Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/340_pdf.pdf</url>
      <abstract>We present a new, unique and freely available parallel corpus containing European Union (EU) documents of mostly legal nature. It is available in all 20 official EU languages, with additional documents being available in the languages of the EU candidate countries. The corpus consists of almost 8,000 documents per language, with an average size of nearly 9 million words per language. Pair-wise paragraph alignment information produced by two different aligners (Vanilla and HunAlign) is available for all 190+ language pair combinations. Most texts have been manually classified according to the EUROVOC subject domains so that the collection can also be used to train and test multi-label classification algorithms and keyword-assignment software. The corpus is encoded in XML, according to the Text Encoding Initiative Guidelines. Due to the large number of parallel texts in many languages, the JRC-Acquis is particularly suitable to carry out all types of cross-language research, as well as to test and benchmark text analysis software across different languages (for instance for alignment, sentence splitting and term extraction).</abstract>
    </paper>
    <paper id="197">
      <author><first>Aljoscha</first><last>Burchardt</last></author>
      <author><first>Katrin</first><last>Erk</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <author><first>Andrea</first><last>Kowalski</last></author>
      <author><first>Sebastian</first><last>Pado</last></author>
      <title><fixed-case>SALTO</fixed-case> - A Versatile Multi-Level Annotation Tool</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/341_pdf.pdf</url>
      <abstract>In this paper, we describe the SALTO tool. It was originally developed for the annotation of semantic roles in the frame semantics paradigm, but can be used for graphical annotation of treebanks with general relational information in a simple drag-and-drop fashion. The tool additionally supports corpus management and quality control.</abstract>
    </paper>
    <paper id="198">
      <author><first>Véronique</first><last>Hoste</last></author>
      <author><first>Guy</first><last>De Pauw</last></author>
      <title><fixed-case>KNACK</fixed-case>-2002: a Richly Annotated Corpus of <fixed-case>D</fixed-case>utch Written Text</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/342_pdf.pdf</url>
      <abstract>In this paper, we introduce the annotated KNACK-2002 corpus of Dutch written text. The corpus features five different annotation layers, ranging from the annotation of morphological boundaries at the word level, over the annotation of part-of-speech tags and phrase chunks at the syntactic level to the annotation of named entities at the semantic level and coreferential relations at the discourse level. We believe the corpus is unique in the Dutch language area because of its richness of annotation layers, providing researchers with a useful gold standard data set for different NLP tasks in the domains of morphology, (morpho)syntax, semantics and discourse.</abstract>
    </paper>
    <paper id="199">
      <author id="philipp-cimiano"><first>P.</first><last>Cimiano</last></author>
      <author id="matthias-hartung"><first>M.</first><last>Hartung</last></author>
      <author id="esther-ratsch"><first>E.</first><last>Ratsch</last></author>
      <title>Finding the Appropriate Generalization Level for Binary Ontological Relations Extracted from the <fixed-case>G</fixed-case>enia Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/343_pdf.pdf</url>
      <abstract>Recent work has aimed at discovering ontological relations from text corpora. Most approaches are based on the assumption that verbs typically indicate semantic relations between concepts. However, the problem of finding the appropriate generalization level for the verb's arguments with respect to a given taxonomy has not received much attention in the ontology learning community. In this paper, we address the issue of determining the appropriate level of abstraction for binary relations extracted from a corpus with respect to a given concept hierarchy. For this purpose, we reuse techniques from the subcategorization and selectional restrictions acquisition communities. The contribution of our work lies in the systematic analysis of three different measures. We conduct our experiments on the Genia corpus and the Genia ontology and evaluate the different measures by comparing the results of our approach with a gold standard provided by one of the authors, a biologist.</abstract>
    </paper>
    <paper id="200">
      <author><first>Tomoyuki</first><last>Kato</last></author>
      <author><first>Tomiki</first><last>Toda</last></author>
      <author><first>Hiroshi</first><last>Saruwatari</last></author>
      <author><first>Kiyohiro</first><last>Shikano</last></author>
      <title>Transcription Cost Reduction for Constructing Acoustic Models Using Acoustic Likelihood Selection Criteria</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/344_pdf.pdf</url>
      <abstract>This paper describes a novel method for reducing the transcription effort in the construction of task-adapted acoustic models for a practical automatic speech recognition (ASR) system. We have to prepare actual data samples collected in the practical system and transcribe them for training the task-adapted acoustic models. However, transcribing utterances is a time-consuming and laborious process. In the proposed method, we firstly adapt initial models to acoustic environment of the system using a small number of collected data samples with transcriptions. And then, we automatically select informative training data samples to be transcribed from a large-sized speech corpus based on acoustic likelihoods of the models. We perform several experimental evaluations in the framework of Takemarukun, a practical speech-oriented guidance system. Experimental results show that 1) utterance sets with low likelihoods cause better task-adapted models compared with those with high likelihoods although the set with the lowest likelihoods causes the performance degradation because of including outliers, and 2) MLLR adaptation is effective for training the task-adapted models when the amount of the transcribed data is small and EM training outperforms MLLR if we transcribe more than around 10,000 utterances.</abstract>
    </paper>
    <paper id="201">
      <author><first>Margot</first><last>Mieskes</last></author>
      <author><first>Michael</first><last>Strube</last></author>
      <title>Part-of-Speech Tagging of Transcribed Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/345_pdf.pdf</url>
      <abstract>We used four Part-of-Speech taggers, which are available for research purposes and were originally trained on text to tag a corpus of transcribed multiparty spoken dialogues. The assigned tags were then manually corrected. The correction was first used to evaluate the four taggers, then to retrain them. Despite limited resources in time, money and annotators we reached results comparable to those reported for the taggers on text. Based on our experience we present guidelines to produce reliably POS tagged corpora of new domains.</abstract>
    </paper>
    <paper id="202">
      <author><first>Branimir</first><last>Boguraev</last></author>
      <author><first>Rie Kubota</first><last>Ando</last></author>
      <title>Analysis of <fixed-case>T</fixed-case>ime<fixed-case>B</fixed-case>ank as a Resource for <fixed-case>T</fixed-case>ime<fixed-case>ML</fixed-case> Parsing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/346_pdf.pdf</url>
      <abstract>In our work, we present an analysis of the TimeBank corpus---the only available reference sample of TimeML-compliant annotation---from the point of view of its utility as a training resource for developing automated TimeML annotators. We are encouraged by experimental results indicative of the potential of TimeBank; at the same time, closer inspection of causes for some systematic errors shows off certain deficiencies in the corpus, primarily to do with small size and inconsistent annotation. Our analysis suggests that even a reference resource, developed outside of a rigorous process of training corpus design and creation, can be extremely valuable for training and development purposes. The analysis also highlights areas of correction and improvement for evolving the current reference corpus into a community infrastructure resource.</abstract>
    </paper>
    <paper id="203">
      <author><first>Daisuke</first><last>Kawahara</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <title>Case Frame Compilation from the Web using High-Performance Computing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/347_pdf.pdf</url>
      <abstract>Case frames are important knowledge for a variety of NLP systems, especially when wide-coverage case frames are available. To acquire such large-scale case frames, it is necessary to automatically compile them from an enormous amount of corpus. In this paper, we consider the web as a corpus. We first build a huge text corpus from the web, and then construct case frames from the corpus. It is infeasible to do these processes by one CPU, and thus we employ a high-performance computing environment composed of 350 CPUs. The acquired corpus consists of 470M sentences, and the case frames compiled from them have 90,000 verb entries. The case frames contain most examples of usual use, and are ready to be applied to lots of NLP analyses and applications.</abstract>
    </paper>
    <paper id="204">
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <author><first>Christelle</first><last>Ayache</last></author>
      <title>Data, Annotations and Measures in <fixed-case>EASY</fixed-case> the Evaluation Campaign for Parsers of <fixed-case>F</fixed-case>rench.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/348_pdf.pdf</url>
      <abstract>This paper presents the protocol of EASY the evaluation campaign for syntactic parsers of French in the EVALDA project of the TECHNOLANGUE program. We describe the participants, the corpus and its genre partitioning, the annotation scheme, which allows for the annotation of both constituents and relations, the evaluation methodology and, as an illustration, the results obtained by one participant on half of the corpus.</abstract>
    </paper>
    <paper id="205">
      <author><first>Toshiyuki</first><last>Kanamaru</last></author>
      <author><first>Masaki</first><last>Murata</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>Creation of a <fixed-case>J</fixed-case>apanese Adverb Dictionary that Includes Information on the Speaker’s Communicative Intention Using Machine Learning</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/349_pdf.pdf</url>
      <abstract>Japanese adverbs are classified as either declarative or normal; the former declare the communicative intention of the speaker, while the latter convey a manner of action, a quantity, or a degree by which the adverb modifies the verb or adjective that it accompanies. We have automatically classified adverbs as either declarative or not declarative using a machine-learning method such as the maximum entropy method. We defined adverbs having positive or negative connotations as the positive data. We classified adverbs in the EDR dictionary and IPADIC used by Chasen using this result and built an adverb dictionary that contains descriptions of the communicative intentions of the speaker.</abstract>
    </paper>
    <paper id="206">
      <author><first>Gregory</first><last>Grefenstette</last></author>
      <author><first>Fathi</first><last>Debili</last></author>
      <author><first>Christian</first><last>Fluhr</last></author>
      <author><first>Svitlana</first><last>Zinger</last></author>
      <title>Exploiting text for extracting image processing resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/350_pdf.pdf</url>
      <abstract>Much everyday knowledge about physical aspects of objects does not exist as computer data, though such computer-based knowledge will be needed to communicate with next generation voice-commanded personal robots as well in other applications involving visual scene recognition. The largest attempt at manually creating common-sense knowledge, the CYC project, has not yet produced the information needed for these tasks. A new direction is needed, based on an automated approach to knowledge extraction. In this article we present our project to mine web text to find properties of objects that are not currently stored in computer readable form.</abstract>
    </paper>
    <paper id="207">
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <title>Clustering acronyms in biomedical text for disambiguation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/351_pdf.pdf</url>
      <abstract>Given the increasing number of neologisms in biomedicine (names of genes, diseases, molecules, etc.), the rate of acronyms used in literature also increases. Existing acronym dictionaries cannot keep up with the rate of new creations. Thus, discovering and disambiguating acronyms and their expanded forms are essential aspects of text mining and terminology management. We present a method for clustering long forms identified by an acronym recognition method. Applying the acronym recognition method to MEDLINE abstracts, we obtained a list of short/long forms. The recognized short/long forms were classified by abiologist to construct an evaluation set for clustering sets of similar long forms. We observed five types of term variation in the evaluation set and defined four similarity measures to gathers the similar longforms (i.e., orthographic, morphological, syntactic, lexico semantic variants, nested abbreviations). The complete-link clustering with the four similarity measures achieved 87.5% precision and 84.9% recall on the evaluation set.</abstract>
    </paper>
    <paper id="208">
      <author><first>Goran</first><last>Nenadic</last></author>
      <author><first>Naoki</first><last>Okazaki</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <title>Towards a terminological resource for biomedical text mining</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/352_pdf.pdf</url>
      <abstract>One of the main challenges in biomedical text mining is the identification of terminology, which is a key factor for accessing and integrating the information stored in literature. Manual creation of biomedical terminologies cannot keep pace with the data that becomes available. Still, many of them have been used in attempts to recognise terms in literature, but their suitability for text mining has been questioned as substantial re-engineering is needed to tailor the resources for automatic processing. Several approaches have been suggested to automatically integrate and map between resources, but the problems of extensive variability of lexical representations and ambiguity have been revealed. In this paper we present a methodology to automatically maintain a biomedical terminological database, which contains automatically extracted terms, their mutual relationships, features and possible annotations that can be useful in text processing. In addition to TermDB, a database used for terminology management and storage, we present the following modules that are used to populate the database: TerMine (recognition, extraction and normalisation of terms from literature), AcroTerMine (extraction and clustering of acronyms and their long forms), AnnoTerm (annotation and classification of terms), and ClusTerm (extraction of term associations and clustering of terms).</abstract>
    </paper>
    <paper id="209">
      <author><first>Hans</first><last>Hjelm</last></author>
      <title>Extraction of Cross Language Term Correspondences</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/356_pdf.pdf</url>
      <abstract>This paper describes a method for extracting translations of terms across languages, using parallel corpora. The extracted term correspondences are such that they are useful when performing query expansion for cross language information retrieval, or for bilingual lexicon extraction. The method makes use of the mutual information measure and allows for mapping between single word- to multi-word terms and vice versa. The method is scalable (accommodates addition or removal of data) and produces high quality results, while keeping the computational costs low enough for allowing on-the-fly translations in e.g., cross language information retrieval systems. The work was carried out in collaboration with Intrafind Software AG (Munich, Germany).</abstract>
    </paper>
    <paper id="210">
      <author><first>David</first><last>Guthrie</last></author>
      <author><first>Ben</first><last>Allison</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Louise</first><last>Guthrie</last></author>
      <author><first>Yorick</first><last>Wilks</last></author>
      <title>A Closer Look at Skip-gram Modelling</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/357_pdf.pdf</url>
      <abstract>Data sparsity is a large problem in natural language processing that refers to the fact that language is a system of rare events, so varied and complex, that even using an extremely large corpus, we can never accurately model all possible strings of words. This paper examines the use of skip-grams (a technique where by n-grams are still stored to model language, but they allow for tokens to be skipped) to overcome the data sparsity problem. We analyze this by computing all possible skip-grams in a training corpus and measure how many adjacent (standard) n-grams these cover in test documents. We examine skip-gram modelling using one to four skips with various amount of training data and test against similar documents as well as documents generated from a machine translation system. In this paper we also determine the amount of extra training data required to achieve skip-gram coverage using standard adjacent tri-grams.</abstract>
    </paper>
    <paper id="211">
      <author id="boris-v-dobrov"><last>Dobrov</last><first>B.</first></author>
      <author id="natalia-loukachevitch"><last>Loukachevitch</last><first>N.</first></author>
      <title>Development of Linguistic Ontology on Natural Sciences and Technology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/359_pdf.pdf</url>
      <abstract>The paper describes the main principles of development and current state of Linguistic Ontology on Natural Sciences and Technology intended for information-retrieval tasks. In the development of the ontology we combined three different methodologies: development of information-retrieval thesauri, development of wordnets, formal ontology research. Combination of these methodologies allows us to develop large ontologies for broad domains.</abstract>
    </paper>
    <paper id="212">
      <author><first>Matthew W.</first><last>Bilotti</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <title>Evaluation for Scenario Question Answering Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/360_pdf.pdf</url>
      <abstract>Scenario Question Answering is a relatively new direction in Question Answering (QA) research that presents a number of challenges for evaluation. In this paper, we propose a comprehensive evaluation strategy for Scenario QA, including amethodology for building reusable test collections for Scenario QA and metrics for evaluating system performance over such test collections. Using this methodology, we have built a test collection, which we have made available for public download as a service to the research community. It is our hope that widespread availability of quality evaluation materials fuels research in new approaches to the Scenario QA task.</abstract>
    </paper>
    <paper id="213">
      <author><first>Dirk</first><last>Bühler</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <title>Stochastic Spoken Natural Language Parsing in the Framework of the <fixed-case>F</fixed-case>rench <fixed-case>MEDIA</fixed-case> Evaluation Campaign</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/363_pdf.pdf</url>
      <abstract>A stochastic parsing component has been applied on a French spoken language dialogue corpus, recorded in the framework of the MEDIA evaluation campaign. Realized as an ergodic HMM using Viterbide coding, the parser outputs the most likely semantic representation given a transcribed utterance as input. The semantic sequences used for training and testing have been derived from the semantic representations of the MEDIA corpus. The HMM parameters have been estimated given the word sequences along with their semantic representation. The performance score of the stochastic parser has been automatically determined using the mediaval tool applied to a held out reference corpus. Evaluation results will be presented in the paper.</abstract>
    </paper>
    <paper id="214">
      <author><first>Stephan</first><last>Oepen</last></author>
      <author><first>Jan Tore</first><last>Lønning</last></author>
      <title>Discriminant-Based <fixed-case>MRS</fixed-case> Banking</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/364_pdf.pdf</url>
      <abstract>We present an approach to discriminant-based MRS banking, i.e. the construction of an annotated corpus where each input item is paired with a logical-form semantics. Semantic annotations are produced by parsing with a broad-coverage precision grammar, followed by manual disambiguation. The selection of the preferred analysis for each item (and hence its semantic form) builds on a notion of semantic discriminants, essentially localized dependencies extracted from a full-fledged, underspecified semantic representation.</abstract>
    </paper>
    <paper id="215">
      <author><first>György</first><last>Szarvas</last></author>
      <author><first>Richárd</first><last>Farkas</last></author>
      <author><first>László</first><last>Felföldi</last></author>
      <author><first>András</first><last>Kocsor</last></author>
      <author><first>János</first><last>Csirik</last></author>
      <title>A highly accurate Named Entity corpus for <fixed-case>H</fixed-case>ungarian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/365_pdf.pdf</url>
      <abstract>A highly accurate Named Entity (NE) corpus for Hungarian that is publicly available for research purposes is introduced in the paper, along with its main properties. The results of experiments that apply various Machine Learning models and classifier combination schemes are also presented to serve as a benchmark for further research based on the corpus. The data is a segment of the Szeged Corpus (Csendes et al., 2004), consisting of short business news articles collected from MTI (Hungarian News Agency, www.mti.hu). The annotation procedure was carried out paying special attention to annotation accuracy. The corpus went through a parallel annotation phase done by two annotators, resulting in a tagging with inter-annotator agreement rate of 99.89%. Controversial taggings were collected and discussed by the two annotators and a linguist with several years of experience in corpus annotation. These examples were tagged following the decision they made together, and finally all entities that had suspicious or dubious annotations were collected and checked for consistency. We consider the result of this correcting process virtually be free of errors. Our best performing Named Entity Recognizer (NER) model attained an accuracy of 92.86% F measure on the corpus.</abstract>
    </paper>
    <paper id="216">
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Mihaela</first><last>Vela</last></author>
      <title>Generic <fixed-case>NLP</fixed-case> Tools for Supporting Shallow Ontology Building</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/366_pdf.pdf</url>
      <abstract>In this paper we present on-going investigations on how complex syntactic annotation, combined with linguistic semantics, can possibly help in supporting the semi-automatic building of (shallow) ontologies from text by proposing an automated extraction of (possibly underspecified) semantic relations from linguistically annotated text.</abstract>
    </paper>
    <paper id="217">
      <author><first>Julia</first><last>Ritz</last></author>
      <author><first>Ulrich</first><last>Heid</last></author>
      <title>Extraction tools for collocations and their morphosyntactic specificities</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/367_pdf.pdf</url>
      <abstract>We describe tools for the extraction of collocations not only in the form of word combinations, but also of data about the morphosyntactic properties of collocation candidates. Such data are needed for a detailed lexical description of collocations, and to support both their recognition in text and the generation of collocationally acceptable text. We describe the tool architecture, report on a case study based on noun+verb collocations, and we give a first rough evaluation of the data quality produced.</abstract>
    </paper>
    <paper id="218">
      <author><first>Luke</first><last>Nezda</last></author>
      <author><first>Andrew</first><last>Hickl</last></author>
      <author><first>John</first><last>Lehmann</last></author>
      <author><first>Sarmad</first><last>Fayyaz</last></author>
      <title>What in the world is a Shahab?: Wide Coverage Named Entity Recognition for <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/368_pdf.pdf</url>
      <abstract>This paper describes the development of CiceroArabic, the first wide coverage named entity recognition (NER) system for Modern Standard Arabic. Capable of classifying 18 different named entity classes with over 85% F, CiceroArabic utilizes a new 800,000-word annotated Arabic newswire corpus in order to achieve high performance without the need for hand-crafted rules or morphological information. In addition to describing results from our system, we show that accurate named entity annotation for a large number of semantic classes is feasible, even for very large corpora, and we discuss new techniques designed to boost agreement and consistency among annotators over a long-term annotation effort.</abstract>
    </paper>
    <paper id="219">
      <author id="massimo-poesio"><first>M.</first><last>Poesio</last></author>
      <author id="mijail-kabadjov"><first>M. A.</first><last>Kabadjov</last></author>
      <author><first>P.</first><last>Goux</last></author>
      <author id="udo-kruschwitz"><first>U.</first><last>Kruschwitz</last></author>
      <author id="elizabeth-bishop"><first>E.</first><last>Bishop</last></author>
      <author id="louise-corti"><first>L.</first><last>Corti</last></author>
      <title>An Anaphora Resolution-Based Anonymization Module</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/372_pdf.pdf</url>
      <abstract>Growing privacy and security concerns mean there is an increasing need for data to be anonymized before being publically released. We present a module for anonymizing references implemented as part of the SQUAD tools for specifying and testing non-proprietary means of storing and marking-up data using universal (XML) standards and technologies. The tool is implemented on top of the GUITAR anaphoric resolver.</abstract>
    </paper>
    <paper id="220">
      <author><first>Manfred</first><last>Sailer</last></author>
      <author><first>Beata</first><last>Trawiński</last></author>
      <title>The Collection of Distributionally Idiosyncratic Items: A Multilingual Resource for Linguistic Research</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/375_pdf.pdf</url>
      <abstract>We present two collections of lexical items with idiosyncratic distribution. The collections document the behavior of German and English bound words (BW, such as English headway), i.e., words which can only occur in one expression (make headway). BWs are a problem for both general and idiomatic dictionaries since it is unclear whether they have an independent lexical status and to what extent the expressions in which they occur are typical idiomatic expressions. We propose a system which allows us to document the information about BWs from dictionaries and linguistic literature, together with corpus data and example queries for major text corpora. We present our data structure and point to other phraseologically oriented collections. We will also show differences between the German and the English collection.</abstract>
    </paper>
    <paper id="221">
      <author><first>Ulrich</first><last>Heid</last></author>
      <author><first>Elsabé</first><last>Taljard</last></author>
      <author><first>Danie J.</first><last>Prinsloo</last></author>
      <title>Grammar-based tools for the creation of tagging resources for an unresourced language: the case of <fixed-case>N</fixed-case>orthern <fixed-case>S</fixed-case>otho</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/376_pdf.pdf</url>
      <abstract>We describe an architecture for the parallel construction of a tagger lexicon and an annotated reference corpus for the part-of-speech tagging of Nothern Sotho, a Bantu language of South Africa, for which no tagged resources have been available so far. Our tools make use of grammatical properties (morphological and syntactic) of the language. We use symbolic pretagging, followed by stochastic tagging, an architecture which proves useful not only for the bootstrapping of tagging resources, but also for the tagging of any new text. We discuss the tagset design, the tool architecture and the current state of our ongoing effort.</abstract>
    </paper>
    <paper id="222">
      <author><first>Maria Clara Paixão</first><last>de Sousa</last></author>
      <author><first>Thorsten</first><last>Trippel</last></author>
      <title>Building a historical corpus for Classical <fixed-case>P</fixed-case>ortuguese: some technological aspects</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/378_pdf.pdf</url>
      <abstract>This paper describes the restructuring process of a large corpus of historical documents and the system architecture that is used for accessing it. The initial challenge of this process was to get the most out of existing material, normalizing the legacy markup and harvesting the inherent information using widely available standards. This resulted in a conceptual and technical restructuring of the formerly existing corpus. The development of the standardized markup and techniques allowed the inclusion of important new materials, such as original 16th and 17th century prints and manuscripts; and enlarged the potential user groups. On the technological side, we were grounded on the premise that open standards are the best way of making sure that the resources will be accessible even after years in an archive. This is a welcomed result in view of the additional consequence of the remodeled corpus concept: it serves as a repository for important historical documents, some of which had been preserved for 500 years in paper format. This very rich material can from now on be handled freely for linguistic research goals.</abstract>
    </paper>
    <paper id="223">
      <author><first>Maria Teresa</first><last>Pazienza</last></author>
      <author><first>Marco</first><last>Pennacchiotti</last></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last></author>
      <title>Mixing <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et, <fixed-case>V</fixed-case>erb<fixed-case>N</fixed-case>et and <fixed-case>P</fixed-case>rop<fixed-case>B</fixed-case>ank for studying verb relations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/379_pdf.pdf</url>
      <abstract>In this paper we present a novel resource for studying the semantics of verb relations. The resource is created by mixing sense relational knowledge enclosed in WordNet, frame knowledge enclosed in VerbNet and corpus knowledge enclosed in PropBank. As a result, a set of about 1000 frame pairs is made available. A frame pair represents a pair of verbs in a peculiar semantic relation accompanied with specific information, such as: the syntactic-semantic frames of the two verbs, the mapping among their thematic roles and a set of textual examples extracted from the PennTreeBank. We specifically focus on four relations: Troponymy, Causation, Entailment and Antonymy. The different steps required for the mapping are described in detail and statistics on resource mutual coverage are reported. We also propose a practical use of the resource for the task of Textual Entailment acquisition and for Question Answering. A first attempt for automate the mapping among verb arguments is also presented: early experiments show that simple techniques can achieve good results, up to 85% F-Measure.</abstract>
    </paper>
    <paper id="224">
      <author><first>Leo</first><last>Wanner</last></author>
      <author><first>Margarita Alonso</first><last>Ramos</last></author>
      <title>Local Document Relevance Clustering in <fixed-case>IR</fixed-case> Using Collocation Information</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/381_pdf.pdf</url>
      <abstract>A series of different automatic query expansion techniques has been suggested in Information Retrieval. To estimate how suitable a document term is as an expansion term, the most popular of them use a measure of the frequency of the co-occurrence of this term with one or several query terms. The benefit of the use of the linguistic relations that hold between query terms is often questioned. If a linguistic phenomenon is taken into account, it is the phrase structure or lexical compound. We propose a technique that is based on the restricted lexical cooccurrence (collocation) of query terms. We use the knowledge on collocations formed by query terms for two tasks: (i) document relevance clustering done in the first stage of local query expansion and (ii) choice of suitable expansion terms from the relevant document cluster. In this paper, we describe the first task, providing evidence from first preliminary experiments on Spanish material that local relevance clustering benefits largely from knowledge on collocations.</abstract>
    </paper>
    <paper id="225">
      <author><first>Andrea</first><last>Esuli</last></author>
      <author><first>Fabrizio</first><last>Sebastiani</last></author>
      <title><fixed-case>SENTIWORDNET</fixed-case>: A Publicly Available Lexical Resource for Opinion Mining</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/384_pdf.pdf</url>
      <abstract>Opinion mining (OM) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. OM has a rich set of applications, ranging from tracking users opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the PNpolarity of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been instead much scarcer. In this work we describe SENTIWORDNET, a lexical resource in which each WORDNET synset sis associated to three numerical scores Obj(s), Pos(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop SENTIWORDNET is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classi.cation. The three scores are derived by combining the results produced by a committee of eight ternary classi.ers, all characterized by similar accuracy levels but different classification behaviour. SENTIWORDNET is freely available for research purposes, and is endowed with a Web-based graphical user interface.</abstract>
    </paper>
    <paper id="226">
      <author><first>Alexandre</first><last>Denis</last></author>
      <author><first>Matthieu</first><last>Quignard</last></author>
      <author><first>Guillaume</first><last>Pitel</last></author>
      <title>A Deep-Parsing Approach to Natural Language Understanding in Dialogue System: Results of a Corpus-Based Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/386_pdf.pdf</url>
      <abstract>This paper presents an approach to dialogue understanding based on a deep parsing and rule-based semantic analysis. Its performance in the semantic evaluation performed in the framework of the EVALDA/MEDIA campaign is encouraging. The MEDIA project aims to evaluate natural language understanding systems for French on a hotel reservation task (Devillers et al., 2004). For the evaluation, five participating teams had to produce an annotated version of the input utterances in compliance with a commonly agreed format (the MEDIA formalism). An approach based on symbolic processing was not straightforward given the conditions of the evaluation but we achieved a score close to that of statistical systems, without needing an annotated corpus. Despite the architecture has been designed for this campaign, exclusively dedicated to spoken dialogue understanding, we believe that our approach based on a LTAG parser and two ontologies can be used in real dialogue systems, providing quite robust speech understanding and facilities for interfacing with a dialogue manager and the application itself.</abstract>
    </paper>
    <paper id="227">
      <author><first>Felix</first><last>Sasaki</last></author>
      <title>Work within the <fixed-case>W</fixed-case>3<fixed-case>C</fixed-case> Internationalization Activity and its Benefit for the Creation and Manipulation of Language Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/388_pdf.pdf</url>
      <abstract>This paper introduces ongoing and current work within Internationalization (i18n) Activity, in the World Wide Web Consortium (W3C). The focus is on aspects of the W3C i18n Activity which are of benefit for the creation and manipulation of multilingual language resources. In particular, the paper deals with ongoing work concerning encoding, visualization and processing of characters; current work on language and locale identification; and current work on internationalization of markup. The main usage scenario is the design of multilingual corpora. This includes issues of corpus creation and manipulation.</abstract>
    </paper>
    <paper id="228">
      <author><first>Margaret</first><last>King</last></author>
      <author><first>Nancy</first><last>Underwood</last></author>
      <title>Evaluating Symbiotic Systems: the challenge</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/389_pdf.pdf</url>
      <abstract>This paper looks at a class of systems which pose severe problems in evaluation design for current conventional approaches to evaluation. After describing the two conventional evaluation paradigms: the functionality paradigm as typified by evaluation campaigns and the ISO inspired user-centred paradigm typified by the work of the EAGLES and ISLE projects, it goes on to outline the problems posed by the evaluation of systems which are designed to work in critical interaction with a human expert user and to work over vast amounts of data. These systems pose problems for both paradigms although for different reasons. The primary aim of this paper is to provoke discussion and the search for solutions. We have no proven solutions at present. However, we describe a programme of exploratory research on which we have already embarked, which involves ground clearing work which we expect to result in a deep understanding of the systems and users, a pre-requisite for developing a general framework for evaluation in this field.</abstract>
    </paper>
    <paper id="229">
      <author><first>Aimilios</first><last>Chalamandaris</last></author>
      <author><first>Athanassios</first><last>Protopapas</last></author>
      <author><first>Pirros</first><last>Tsiakoulis</last></author>
      <author><first>Spyros</first><last>Raptis</last></author>
      <title>All <fixed-case>G</fixed-case>reek to me! An automatic Greeklish to <fixed-case>G</fixed-case>reek transliteration system</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/390_pdf.pdf</url>
      <abstract>This paper presents research on Greeklish, that is, a transliteration of Greek using the Latin alphabet, which is used frequently in Greek e-mail communication. Greeklish is not standardized and there are a number of competing conventions co-existing in communication, based on personal preferences regarding similarities between Greek and Latin letters in shape, sound, or keyboard position. Our research has led to the development of All Greek to me! the first automatic transliteration system that can cope with any type of Greeklish. In this paper we first present previous research on Greeklish, describing other approaches that have attempted to deal with the same problems. We then provide a brief description of our approach, illustrating the functional flowchart of our system and the main ideas that underlie it. We present measures of system performance, based on about a years worth of usage as a public web service, and preliminary research, based on the same corpus, on the use of Greeklish and the trends in preferred Latin-Greek letter mapping. We evaluate the consistency of different transliteration patterns among users as well as the within-user consistency based on coherent principles. Finally we outline planned future research to further understand the use of Greeklish and improve All Greek to me! to function reliably embedded in integrated communication platforms bridging e-mail to mobile telephony and ubiquitous connectivity.</abstract>
    </paper>
    <paper id="230">
      <author><first>Thurid</first><last>Vogt</last></author>
      <author><first>Elisabeth</first><last>André</last></author>
      <title>Improving Automatic Emotion Recognition from Speech via Gender Differentiaion</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/392_pdf.pdf</url>
      <abstract>Feature extraction is still a disputed issue for the recognition of emotions from speech. Differences in features for male and female speakers are a well-known problem and it is established that gender-dependent emotion recognizers perform better than gender-independent ones. We propose a way to improve the discriminative quality of gender-dependent features: The emotion recognition system is preceded by an automatic gender detection that decides upon which of two gender-dependent emotion classifiers is used to classify an utterance. This framework was tested on two different databases, one with emotional speech produced by actors and one with spontaneous emotional speech from a Wizard-of-Oz setting. Gender detection achieved an accuracy of about 90 % and the combined gender and emotion recognition system improved the overall recognition rate of a gender-independent emotion recognition system by 2-4 %.</abstract>
    </paper>
    <paper id="231">
      <author><first>Khurshid</first><last>Ahmad</last></author>
      <author><first>Lee</first><last>Gillam</last></author>
      <author><first>David</first><last>Cheng</last></author>
      <title>Sentiments on a Grid: Analysis of Streaming News and Views</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/394_pdf.pdf</url>
      <abstract>In this paper we report on constructing a finite state automaton comprising automatically extracted terminology and significant collocation patterns from a training corpus of specialist news (Reuters Financial News). The automaton can be used to unambiguously identify sentiment-bearing words that might be able to make or break people, companies, perhaps even governments. The paper presents the emerging face of corpus linguistics where a corpus is used to bootstrap both the terminology and the significant meaning bearing patterns from the corpus. Much of the current content analysis software systems require a human coder to eyeball terms and sentiment words. Such an approach might yield very good quality results on small text collections but when confronted with a 40-50 million word corpus such an approach does not scale, and a large-scale computer-based approach is required. We report on the use of Grid computing technologies and techniques to cope with this analysis.</abstract>
    </paper>
    <paper id="232">
      <author><first>Briony</first><last>Williams</last></author>
      <author><first>Rhys James</first><last>Jones</last></author>
      <author><first>Ivan</first><last>Uemlianin</last></author>
      <title>Tools and resources for speech synthesis arising from a <fixed-case>W</fixed-case>elsh <fixed-case>TTS</fixed-case> project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/395_pdf.pdf</url>
      <abstract>The WISPR project ("Welsh and Irish Speech Processing Resources") has been building text-to-speech synthesis systems for Welsh and for Irish, as well as building links between the developers and potential users of the software. The Welsh half of the project has encountered various challenges, in the areas of the tokenisation of input text, the formatting of letter-to-sound rules, and the implementation of the "greedy algorithm" for text selection. The solutions to these challenges have resulted in various tools which may be of use to other developers using Festival for TTS for other languages. These resources are made freely available.</abstract>
    </paper>
    <paper id="233">
      <author><first>Thierry</first><last>Declerck</last></author>
      <author><first>Asunción Gómez</first><last>Pérez</last></author>
      <author><first>Ovidiu</first><last>Vela</last></author>
      <author><first>Zeno</first><last>Gantner</last></author>
      <author><first>David</first><last>Manzano-Macho</last></author>
      <title>Multilingual Lexical Semantic Resources for Ontology Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/397_pdf.pdf</url>
      <abstract>We describe the integration of some multilingual language resources in ontological descriptions, with the purpose of providing ontologies, which are normally using concept labels in just one (natural) language, with multilingual facility in their design and use in the context of Semantic Web applications, supporting both the semantic annotation of textual documents with multilingual ontology labels and ontology extraction from multilingual text sources.</abstract>
    </paper>
    <paper id="235">
      <author><first>Beatrice</first><last>Alex</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <author><first>Claire</first><last>Grover</last></author>
      <title>The Impact of Annotation on the Performance of Protein Tagging in Biomedical Text</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/398_pdf.pdf</url>
      <abstract>In this paper we discuss five different corpora annotated forprotein names. We present several within- and cross-dataset proteintagging experiments showing that different annotation schemes severelyaffect the portability of statistical protein taggers. By means of adetailed error analysis we identify crucial annotation issues thatfuture annotation projects should take into careful consideration.</abstract>
    </paper>
    <paper id="236">
      <author><first>Ben</first><last>Wellner</last></author>
      <author><first>Marc</first><last>Vilain</last></author>
      <title>Leveraging Machine Readable Dictionaries in Discriminative Sequence Models</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/404_pdf.pdf</url>
      <abstract>Many natural language processing tasks make use of a lexicon  typically the words collected from some annotated training data along with their associated properties. We demonstrate here the utility of corpora-independent lexicons derived from machine readable dictionaries. Lexical information is encoded in the form of features in a Conditional Random Field tagger providing improved performance in cases where: i) limited training data is made available ii) the data is case-less and iii) the test data genre or domain is different than that of the training data. We show substantial error reductions, especially on unknown words, for the tasks of part-of-speech tagging and shallow parsing, achieving up to 20% error reduction on Penn TreeBank part-of-speech tagging and up to a 15.7% error reduction for shallow parsing using the CoNLL 2000 data. Our results here point towards a simple, but effective methodology for increasing the adaptability of text processing systems by training models with annotated data in one genre augmented with general lexical information or lexical information pertinent to the target genre (or domain).</abstract>
    </paper>
    <paper id="237">
      <author><first>Saša</first><last>Hasan</last></author>
      <author><first>Anas El</first><last>Isbihani</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <title>Creating a Large-Scale <fixed-case>A</fixed-case>rabic to <fixed-case>F</fixed-case>rench Statistical <fixed-case>M</fixed-case>achine<fixed-case>T</fixed-case>ranslation System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/405_pdf.pdf</url>
      <abstract>In this work, the creation of a large-scale Arabic to French statistical machine translation system is presented. We introduce all necessary steps from corpus aquisition, preprocessing the data to training and optimizing the system and eventual evaluation. Since no corpora existed previously, we collected large amounts of data from the web. Arabic word segmentation was crucial to reduce the overall number of unknown words. We describe the phrase-based SMT system used for training and generation of the translation hypotheses. Results on the second CESTA evaluation campaign are reported. The setting was inthe medical domain. The prototype reaches a favorable BLEU score of40.8%.</abstract>
    </paper>
    <paper id="238">
      <author><last>Chen</last><first>Yirong</first></author>
      <author><last>Lu</last><first>Qin</first></author>
      <author><last>Li</last><first>Wenjie</first></author>
      <author><last>Sui</last><first>Zhifang</first></author>
      <author><last>Ji</last><first>Luning</first></author>
      <title>A Study on Terminology Extraction Based on Classified Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/407_pdf.pdf</url>
      <abstract>Algorithms for automatic term extraction in a specific domain should consider at least two issues, namely Unithood and Termhood (Kageura, 1996). Unithood refers to the degree of a string to occur as a word or a phrase. Termhood (Chen Yirong, 2005) refers to the degree of a word or a phrase to occur as a domain specific concept. Unlike unithood, study on termhood is not yet widely reported. In classified corpora, the class information provides the cue to the nature of data and can be used in termhood calculation. Three algorithms are provided and evaluated to investigate termhood based on classified corpora. The three algorithms are based on lexicon set computing, term frequency and document frequency, and the strength of the relation between a term and its document class respectively. Our objective is to investigate the effects of these different termhood measurement features. After evaluation, we can find which features are more effective and also, how we can improve these different features to achieve the best performance. Preliminary results show that the first measure can effectively filter out independent terms or terms of general use.</abstract>
    </paper>
    <paper id="239">
      <author><first>Anna</first><last>Estellés</last></author>
      <author><first>Amparo</first><last>Alcina</last></author>
      <author><first>Victoria</first><last>Soler</last></author>
      <title>Retrieving Terminological Data from the <fixed-case>T</fixed-case>xt<fixed-case>C</fixed-case>eram Tagged Domain Corpus: A First Step towards a Terminological Ontology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/408_pdf.pdf</url>
      <abstract>In this paper we will focus on corpora as a resource for researching language processing for terminological purposes. Based on the TEI guide, we present the templates used to tag our TxtCeram corpus and its features when working with WordSmith, a text analysis tool. We present an experiment for studying the frequency of hyperonyms in the introduction section of texts, while testing WordSmiths suitability to work with our tagged corpus.</abstract>
    </paper>
    <paper id="240">
      <author><first>Violetta</first><last>Cavalli-Sforza</last></author>
      <author><first>Abdelhadi</first><last>Soudi</last></author>
      <title><fixed-case>IMORPHĒ</fixed-case>: An Inheritance and Equivalence Based Morphology Description Compiler</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/409_pdf.pdf</url>
      <abstract>IMORPHĒ is a significantly extended version of MORPHE, a morphology description compiler. MORPHEs morphology description language is based on two constructs: 1) a morphological form hierarchy, whose nodes relate and differentiate surface forms in terms of the common and distinguishing inflectional features of lexical items; and 2) transformational rules, attached to leaf nodes of the hierarchy, which generate the surface form of an item from the base form stored in the lexicon. While MORPHEs approach to morphology description is intuitively appealing and was successfully used for generating the morphology of several European languages, its application to Modern Standard Arabic yielded morphological descriptions that were highly complex and redundant. Previous modifications and enhancements attempted to capture more elegantly and concisely different aspects of the complex morphology of Arabic, finding theoretical grounding in Lexeme-Based Morphology. Those extensions are being incorporated in a more flexible and less ad hoc fashion in IMORPHE, which retains the unique features of our previous work but embeds them in an inheritance-based framework in order to achieve even more concise and modular morphology descriptions and greater runtime efficiency, and lays the groundwork for IMORPHE to become an analyzer as well as a generator.</abstract>
    </paper>
    <paper id="241">
      <author><first>Laurianne</first><last>Sitbon</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <title>Tools and methods for objective or contextual evaluation of topic segmentation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/410_pdf.pdf</url>
      <abstract>In this paper we discuss the way of evaluating topic segmentation, from mathematical measures on variously constructed reference corpus to contextual evaluation depending on different topic segmentation usages. We present an overview of the different ways of building reference corpora and of mathematically evaluating segmentation methods, and then we focus on three tasks which may involve a topic segmentation: text extraction, information retrieval and document presentation. We have developed two graphical interfaces, one for an intrinsic comparison, and the other one dedicated to an evaluation in an information retrieval context. These tools will be very soon distributed under GPL licences on the Technolangue project web page.</abstract>
    </paper>
    <paper id="242">
      <author id="laurence-devillers"><first>L.</first><last>Devillers</last></author>
      <author id="roddy-cowie"><first>R.</first><last>Cowie</last></author>
      <author id="jean-claude-martin"><first>J-C.</first><last>Martin</last></author>
      <author id="ellen-douglas-cowie"><first>E.</first><last>Douglas-Cowie</last></author>
      <author id="sarkis-abrilian"><first>S.</first><last>Abrilian</last></author>
      <author id="margaret-mcrorie"><first>M.</first><last>McRorie</last></author>
      <title>Real life emotions in <fixed-case>F</fixed-case>rench and <fixed-case>E</fixed-case>nglish <fixed-case>TV</fixed-case> video clips: an integrated annotation protocol combining continuous and discrete approaches</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/411_pdf.pdf</url>
      <abstract>A major barrier to the development of accurate and realistic models of human emotions is the absence of multi-cultural / multilingual databases of real-life behaviours and of a federative and reliable annotation protocol. QUB and LIMSI teams are working towards the definition of an integrated coding scheme combining their complementary approaches. This multilevel integrated scheme combines the dimensions that appear to be useful for the study of real-life emotions: verbal labels, abstract dimensions and contextual (appraisal based) annotations. This paper describes this integrated coding scheme, a protocol that was set-up for annotating French and English video clips of emotional interviews and the results (e.g. inter-coder agreement measures and subjective evaluation of the scheme).</abstract>
    </paper>
    <paper id="243">
      <author><first>Maja</first><last>Popović</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <title><fixed-case>POS</fixed-case>-based Word Reorderings for Statistical Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/412_pdf.pdf</url>
      <abstract>Translation In this work we investigate new possibilities for improving the quality of statistical machine translation (SMT) by applying word reorderings of the source language sentences based on Part-of-Speech tags. Results are presented on the European Parliament corpus containing about 700k sentences and 15M running words. In order to investigate sparse training data scenarios, we also report results obtained on about 1\% of the original corpus. The source languages are Spanish and English and target languages are Spanish, English and German. We propose two types of reorderings depending on the language pair and the translation direction: local reorderings of nouns and adjectives for translation from and into Spanish and long-range reorderings of verbs for translation into German. For our best translation system, we achieve up to 2\% relative reduction of WER and up to 7\% relative increase of BLEU score. Improvements can be seen both on the reordered sentences as well as on the rest of the test corpus. Local reorderings are especially important for the translation systems trained on the small corpus whereas long-range reorderings are more effective for the larger corpus.</abstract>
    </paper>
    <paper id="244">
      <author><first>David</first><last>Vilar</last></author>
      <author><first>Jia</first><last>Xu</last></author>
      <author><first>Luis Fernando</first><last>D’Haro</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <title>Error Analysis of Statistical Machine Translation Output</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/413_pdf.pdf</url>
      <abstract>Evaluation of automatic translation output is a difficult task. Several performance measures like Word Error Rate, Position Independent Word Error Rate and the BLEU and NIST scores are widely use and provide a useful tool for comparing different systems and to evaluate improvements within a system. However the interpretation of all of these measures is not at all clear, and the identification of the most prominent source of errors in a given system using these measures alone is not possible. Therefore some analysis of the generated translations is needed in order to identify the main problems and to focus the research efforts. This area is however mostly unexplored and few works have dealt with it until now. In this paper we will present a framework for classification of the errors of a machine translation system and we will carry out an error analysis of the system used by the RWTH in the first TC-STAR evaluation.</abstract>
    </paper>
    <paper id="245">
      <author><first>Irene</first><last>Castellón</last></author>
      <author><first>Ana</first><last>Fernández-Montraveta</last></author>
      <author><first>Gloria</first><last>Vázquez</last></author>
      <author><first>Laura</first><last>Alonso Alemany</last></author>
      <author><first>Joan Antoni</first><last>Capilla</last></author>
      <title>The Sensem Corpus: a Corpus Annotated at the Syntactic and Semantic Level</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/414_pdf.pdf</url>
      <abstract>The primary aim of the project SENSEM (Sentence Semantics, BFF2003-06456) is the construction of a Lexical Data Base illustrating the syntactic and semantic behavior of each of the senses of the 250 most frequent verbs of Spanish. With this objective in mind, we are currently building an annotated corpus consisting of sentences extracted from the electronic version of the newspaper El Periódico de Catalunya, totalling approximately 1 million words, with 100 examples of each verb. By the time of the conference, we will be about to complete the annotation of 25,000 sentences, which means roughly a corpus of 800,000 words. Approximately 400,000 of them will have been revised. We expect to make the corpus publicly available by the end of 2006.</abstract>
    </paper>
    <paper id="246">
      <author><first>Javier</first><last>Pérez</last></author>
      <author><first>Antonio</first><last>Bonafonte</last></author>
      <title><fixed-case>GAIA</fixed-case>: Common Framework for the Development of Speech Translation Technologies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/415_pdf.pdf</url>
      <abstract>We present here an open-source software platform for the integration of speech translation components. This tool is useful to integrate into a common framework different automatic speech recognition, spoken language translation and text-to-speech synthesis solutions, as demonstrated in the evaluation of the European LC-STAR project, and during the development of the national ALIADO project. Gaia operates with great flexibility, and it has been used to obtain the text and speech corpora needed when performing speech translation. The platform follows a modular distributed approach, with a specifically designed extensible network protocol handling the communication with the different modules. A well defined and publicly available API facilitates the integration of existing solutions into the architecture. Completely functional audio and text interfaces together with remote monitoring tools are provided.</abstract>
    </paper>
    <paper id="247">
      <author><first>Attila</first><last>Novák</last></author>
      <title>Morphological Tools for Six Small Uralic Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/416_pdf.pdf</url>
      <abstract>This article presents a set of morphological tools for six small endangered minority languages belonging to the Uralic language family, Udmurt, Komi, Eastern Mari, Northern Mansi, Tundra Nenets and Nganasan. Following an introduction to the languages, the two sets of tools used in the project (MorphoLogic's Humor tools and the Xerox Finite State Tool) are described and compared. The article is concluded by a comparison of the six computational morphologies.</abstract>
    </paper>
    <paper id="248">
      <author><first>Javier</first><last>Pérez</last></author>
      <author><first>Antonio</first><last>Bonafonte</last></author>
      <author><first>Horst-Udo</first><last>Hain</last></author>
      <author><first>Eric</first><last>Keller</last></author>
      <author><first>Stefan</first><last>Breuer</last></author>
      <author><first>Jilei</first><last>Tian</last></author>
      <title><fixed-case>ECESS</fixed-case> Inter-Module Interface Specification for Speech Synthesis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/417_pdf.pdf</url>
      <abstract>The newly founded European Centre of Excellence for Speech Synthesis (ECESS) is an initiative to promote the development of the European research area (ERA) in the field of Language Technology. ECESS focuses on the great challenge of high-quality speech synthesis which is of crucial importance for future spoken-language technologies. The main goals of ECESS are to achieve the critical mass needed to promote progress in TTS technology substantially, to integrate basic research know-how related to speech synthesis and to attract public and private funding. To this end, a common system architecture based on exchangeable modules supplied by the ECESS members is to be established. The XML-based interface that connects these modules is the topic of this paper.</abstract>
    </paper>
    <paper id="249">
      <author><first>Petr</first><last>Němec</last></author>
      <title>Tree Searching/Rewriting Formalism</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/418_pdf.pdf</url>
      <abstract>We present a formalism capable of searching and optionally replacing forests of subtrees within labelled trees. In particular, the formalism is developed to process linguistic treebanks. When used as a substitution tool, the interpreter processes rewrite rules consisting of left and right side. The left side specifies a forest of subtrees to be searched for within a tree by imposing a set of constraints encoded as a query formula. The right side contains the respective substitutions for these subtrees. In the search mode only the left side is present. The formalism is fully implemented. The performance of the implemented tool allows to process even large linguistic corpora in acceptable time. The main contribution of the presented work consists of the expressiveness of the query formula, in the elegant and intuitive way the rules are written (and their easy reversibility), and in the performance of the implemented tool.</abstract>
    </paper>
    <paper id="250">
      <author><first>Maite</first><last>Taboada</last></author>
      <author><first>Caroline</first><last>Anthony</last></author>
      <author><first>Kimberly</first><last>Voll</last></author>
      <title>Methods for Creating Semantic Orientation Dictionaries</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/420_pdf.pdf</url>
      <abstract>We describe and compare different methods for creating a dictionary of words with their corresponding semantic orientation (SO). We tested how well different dictionaries helped determine the SO of entire texts. To extract SO for each individual word, we used a common method based on pointwise mutual information. Mutual information between a set of seed words and the target words was calculated using two different methods: a NEAR search on the search engine Altavista (since discontinued); an AND search on Google. These two dictionaries were tested against a manually annotated dictionary of positive and negative words. The results show that all three methods are quite close, and none of them performs particularly well. We discuss possible further avenues for research, and also point out some potential problems in calculating pointwise mutual information using Google.</abstract>
    </paper>
    <paper id="251">
      <author><first>Masaki</first><last>Itagaki</last></author>
      <author><first>Anthony</first><last>Aue</last></author>
      <author><first>Takako</first><last>Aikawa</last></author>
      <title>Detecting Inter-domain Semantic Shift using Syntactic Similarity</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/423_pdf.pdf</url>
      <abstract>This poster is a preliminary report of our experiments for detecting semantically shifted terms between different domains for the purposes of new concept extraction. A given term in one domain may represent a different concept in another domain. In our approach, we quantify the degree of similarity of words between different domains by measuring the degree of overlap in their domain-specific semantic spaces. The domain-specific semantic spaces are defined by extracting families of syntactically similar words, i.e. words that occur in the same syntactic context. Our method does not rely on any external resources other than a syntactic parser. Yet it has the potential to extract semantically shifted terms between two different domains automatically while paying close attention to contextual information. The organization of the poster is as follows: Section 1 provides our motivation. Section 2 provides an overview of our NLP technology and explains how we extract syntactically similar words. Section 3 describes the design of our experiments and our method. Section 4 provides our observations and preliminary results. Section 5 presents some work to be done in the future and concluding remarks.</abstract>
    </paper>
    <paper id="252">
      <author><first>Hynek</first><last>Bořil</last></author>
      <author><first>Tomáš</first><last>Bořil</last></author>
      <author><first>Petr</first><last>Pollák</last></author>
      <title>Methodology of <fixed-case>L</fixed-case>ombard Speech Database Acquisition: Experiences with <fixed-case>CLSD</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/427_pdf.pdf</url>
      <abstract>In this paper, process of the Czech Lombard Speech Database (CLSD'05) acquisition is presented. Feature analyses have proven a strong appearance of Lombard effect in the database. In the small vocabulary recognition task, significant performance degradation was observed for the Lombard speech recorded in the database. Aim of this paper is to describe the hardware platform, scenarios and recording tool used for the acquisition of CLSD'05. During the database recording and processing, several difficulties were encountered. The most important question was how to adjust the level of speech feedback for the speaker. A method for minimization of the speech attenuation introduced to the speaker by headphones is proposed in this paper. Finally, contents and corpus of the database are presented to outline it's suitability for analysis and modeling of Lombard effect. The whole CLSD'05 database with a detailed documentation is now released for public use.</abstract>
    </paper>
    <paper id="253">
      <author><first>Harry</first><last>Bunt</last></author>
      <title>Dimensions in Dialogue Act Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/428_pdf.pdf</url>
      <abstract>This paper is concerned with the fundamentals of multidimensional dialogue act annotation, i.e. with what it means to annotate dialogues with information about the communicative acts that are performed with the utterances, taking various 'dimensions' into account. Two ideas seem to be prevalent in the literature concerning the notion of dimension: (1) dimensions correspond to different types of information; and (2) a dimension is formed by a set of mutually exclusive tags. In DAMSL, for instance, the terms dimension and layer are used sometimes in the sense of (1) and sometimes in that of (2). We argue that being mutually exclusive is not a good criterion for a set of dialogue act types to constitute a dimension, even though the description of an object in a multidimensional space should never assign more than one value per dimension. We define a dimension of dialogue act annotation as an aspect of participating in a dialogue that can be addressed independently by means of dialogue acts. We show that DAMSL dimensions such as Info-request, Statement, and Answer do not qualify as proper dimensions, and that the communicative functions in these categories do not fall in any specific dimension, but should be considered as general-purpose in the sense that they can be used in any dimension. We argue that using the notion of dimension that we propose, a multidimensional taxonomy of dialogue acts emerges that optimally supports multidimensional dialogue act annotation.</abstract>
    </paper>
    <paper id="254">
      <author><first>Olivier</first><last>Baude</last></author>
      <author><first>Michel</first><last>Jacobson</last></author>
      <author><first>Atanas</first><last>Tchobanov</last></author>
      <author><first>Richard</first><last>Walter</last></author>
      <title>Interoperability of audio corpora : the case of the <fixed-case>F</fixed-case>rench corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/430_pdf.pdf</url>
      <abstract>We present here the choices which were made within the framework of three oral corpora projects: Socio-linguistics studies on Orleans (ESLO), Phonology of the Contemporary French (PFC), the Archivage corpus of the LACITO lab. This comparative presentation of three corpora of audio linguistic resources comes from a analysis about the options the project have to operate to describe them for discovery purposes and to compare the contents. The aim is to illustrate the interest to think the interoperability and the methodology of codings and the metadata. Through this step, we want to simplify the technical creation of audio corpora and thus the constitution of linguistic resources, usable by enlarged academic and industrial communities.</abstract>
    </paper>
    <paper id="255">
      <author id="raffaella-bernardi"><first>R.</first><last>Bernardi</last></author>
      <author id="diego-calvanese"><first>D.</first><last>Calvanese</last></author>
      <author id="luca-dini"><first>L.</first><last>Dini</last></author>
      <author id="vittorio-di-tomaso"><first>V.</first><last>Di Tomaso</last></author>
      <author id="elisabeth-frasnelli"><first>E.</first><last>Frasnelli</last></author>
      <author id="ulrike-kugler"><first>U.</first><last>Kugler</last></author>
      <author id="barbara-plank"><first>B.</first><last>Plank</last></author>
      <title>Multilingual Search in Libraries. The case-study of the Free <fixed-case>U</fixed-case>niversity of <fixed-case>B</fixed-case>ozen-<fixed-case>B</fixed-case>olzano</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/433_pdf.pdf</url>
      <abstract>This paper presents an on-going project aiming at enhancing the OPAC (Online Public Access Catalog) search system of the Library of the Free University of Bozen-Bolzano with multilingual access. The Multilingual search system (MUSIL), we have developed, integrates advanced linguistic technologies in a user friendly interface and bridges the gap between the world of free text search and the world of conceptual librarian search. In this paper we present the architecture of the system, its interface and preliminary evaluations of the precision of the search results.</abstract>
    </paper>
    <paper id="256">
      <author><first>Irene</first><last>Langkilde-Geary</last></author>
      <author><first>Justin</first><last>Betteridge</last></author>
      <title>A Factored Functional Dependency Transformation of the <fixed-case>E</fixed-case>nglish <fixed-case>P</fixed-case>enn <fixed-case>T</fixed-case>reebank for Probabilistic Surface Generation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/435_pdf.pdf</url>
      <abstract>This paper describes a featurized functional dependency corpus automatically derived from the Penn Treebank. Each word in the corpus is associated with over three dozen features describing the functional syntactic structure of a sentence as well as some shallow morphology. The corpus was created for use in probabilistic surface generation, but could also be useful as a resource for the study of English and the development of other NLP applications.</abstract>
    </paper>
    <paper id="257">
      <author><first>Jim</first><last>Talley</last></author>
      <title>Bootstrapping New Language <fixed-case>ASR</fixed-case> Capabilities: Achieving Best Letter-to-Sound Performance under Resource Constraints</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/436_pdf.pdf</url>
      <abstract>One of the most critical components in the process of building automatic speech recognition (ASR) capabilities for a new language is the lexicon, or pronouncing dictionary. For practical reasons, it is desirable to manually create only the minimal lexicon using available native-speaker phonetic expertise and, then, use the resulting seed lexicon for machine learning based induction of a high-quality letter-to-sound (L2S) model for generation of pronunciations for the remaining words of the language. This paper examines the viability of this scenario, specifically investigating three possible strategies for selection of lexemes (words) for manual transcription  choosing the most frequent lexemes of the language, choosing lexemes randomly, and selection of lexemes via an information theoretic diversity measure. The relative effectiveness of these three strategies is evaluated as a function of the number of lexemes to be transcribed to create a bootstrapping lexicon. Generally, the newly developed orthographic diversity based selection strategy outperforms the others for this scenario where a limited number of lexemes can be transcribed. The experiments also provide generally useful insight into expected L2S accuracy sacrifice as a function of decreasing training set size.</abstract>
    </paper>
    <paper id="258">
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <author><first>Liang</first><last>Zhou</last></author>
      <author><first>Junichi</first><last>Fukumoto</last></author>
      <title>Automated Summarization Evaluation with Basic Elements.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/438_pdf.pdf</url>
      <abstract>As part of evaluating a summary automati-cally, it is usual to determine how much of the contents of one or more human-produced ideal summaries it contains. Past automated methods such as ROUGE compare using fixed word ngrams, which are not ideal for a variety of reasons. In this paper we describe a framework in which summary evaluation measures can be instantiated and compared, and we implement a specific evaluation method using very small units of content, called Basic Elements that address some of the shortcomings of ngrams. This method is tested on DUC 2003, 2004, and 2005 systems and produces very good correlations with human judgments.</abstract>
    </paper>
    <paper id="259">
      <author><first>Hiroyuki</first><last>Kaji</last></author>
      <author><first>Mariko</first><last>Watanabe</last></author>
      <title>Automatic Construction of <fixed-case>J</fixed-case>apanese <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/439_pdf.pdf</url>
      <abstract>Although WordNets have been developed for a number of languages, no attempts to construct a Japanese WordNet have been known to exist. Taking this into account, we launched a project to automatically translate the Princeton WordNet into Japanese by a method of unsupervised word-sense disambiguation using bilingual comparable corpora. The method we propose aligns English word associations with those in Japanese and iteratively calculates a correlation matrix of Japanese translations of an English word versus its associated words. It then determines the Japanese translation for the English word in a synset by calculating scores for translation candidates according to the correlation matrix and the associated words appearing in the gloss appended to the synset. This method is not robust because a gloss only contains a few associated words. To overcome this difficulty, we extended the method so that it retrieves texts by using the gloss as a query and uses the retrieved texts as well as the gloss to calculate scores for translation candidates. A preliminary experiment using Wall Street Journal and Nihon Keizai Shimbun corpora demonstrated that the proposed method is promising for constructing a Japanese WordNet.</abstract>
    </paper>
    <paper id="260">
      <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <author><first>Bill</first><last>MacCartney</last></author>
      <author><first>Christopher D.</first><last>Manning</last></author>
      <title>Generating Typed Dependency Parses from Phrase Structure Parses</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/440_pdf.pdf</url>
      <abstract>This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.</abstract>
    </paper>
    <paper id="261">
      <author id="sonia-frota"><first>S.</first><last>Frota</last></author>
      <author id="marina-vigario"><first>M.</first><last>Vigário</last></author>
      <author id="fernando-martins"><first>F.</first><last>Martins</last></author>
      <title><fixed-case>F</fixed-case>re<fixed-case>P</fixed-case>: An electronic tool for extracting frequency information of phonological units from <fixed-case>P</fixed-case>ortuguese written text</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/441_pdf.pdf</url>
      <abstract>The importance of frequency for phonological phenomena has long been noticed in the literature. However, frequency information available for phonological units in Portuguese is scarce, non-replicable, corpus dependent, and hard to obtain due to the non-existence of a free tool for public use. This paper describes FreP, a new electronic tool that provides frequency counts of phonological units at the word-level and below from Portuguese written text: namely, major classes of segments, syllables and syllable types, phonological clitics, clitic types and size, prosodic words and their shape, word stress location, and syllable type by position within the word and/or status relative to word stress. Useful applications of FreP in general linguistics, phonology, language acquisition and development, speech evaluation and therapy are also described. Forthcoming extensions of the tool include the ability to extract frequency information for different varieties of Portuguese, Brazilian Portuguese in particular, and the ability to provide a SAMPA output from the written text, together with the frequency of segmental features, like manner, place of articulation and laryngeal features. Updated information on FreP can be found at http://www.fl.ul.pt/LaboratorioFonetica/FreP.</abstract>
    </paper>
    <paper id="262">
      <author><first>Ulrik</first><last>Petersen</last></author>
      <title>Querying Both Parallel And Treebank Corpora: Evaluation Of A Corpus Query System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/442_pdf.pdf</url>
      <abstract>The last decade has seen a large increase in the number of available corpus query systems. Some of these are optimized for a particular kind of linguistic annotation (e.g., time-aligned, treebank, word-oriented, etc.). In this paper, we report on our own corpus query system, called Emdros. Emdros is very generic, and can be applied to almost any kind of linguistic annotation using almost any linguistic theory. We describe Emdros and its query language, showing some of the benfits that linguists can derive from using Emdros for their corpora. We then describe the underlying database model of Emdros, and show how two corpora can be imported into the system. One of the two is a parallel corpus of Hungarian and English (the Hunglish corpus), while the other is a treebank of German (the TIGER Corpus). In order to evaluate the performance of Emdros, we then run some performance tests. It is shown that Emdros has extremely good performance on small corpora (less than 1 million words), and that it scales well to corpora of many millions of words.</abstract>
    </paper>
    <paper id="263">
      <author><first>Liang</first><last>Zhou</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <title>Summarizing Answers for Complicated Questions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/443_pdf.pdf</url>
      <abstract>Recent work in several computational linguistics (CL) applications (especially question answering) has shown the value of semantics (in fact, many people argue that the current performance ceiling experienced by so many CL applications derives from their inability to perform any kind of semantic processing). But the absence of a large semantic information repository that provides representations for sentences prevents the training of statistical CL engines and thus hampers the development of such semantics-enabled applications. This talk refers to recent work in several projects that seek to annotate large volumes of text with shallower or deeper representations of some semantic phenomena. It describes one of the essential problemscreating, managing, and annotating (at large scale) the meanings of words, and outlines the Omega ontology, being built at ISI, that acts as term repository. The talk illustrates how one can proceed from words via senses to concepts, and how the annotation process can help verify good concept decisions and expose bad ones. Much of this work is performed in the context of the OntoNotes project, joint with BBN, the Universities of Colorado and Pennsylvania, and ISI, that is working to build a corpus of about 1M words (English, Chinese, and Arabic), annotated for shallow semantics, over the next few years.</abstract>
    </paper>
    <paper id="265">
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Jochen</first><last>Friedrich</last></author>
      <author><first>Giulio</first><last>Maltese</last></author>
      <author><first>Michele</first><last>Mammini</last></author>
      <author><first>Jan</first><last>Odijk</last></author>
      <author><first>Marisa</first><last>Ulivieri</last></author>
      <title>Unified Lexicon and Unified Morphosyntactic Specifications for Written and Spoken <fixed-case>I</fixed-case>talian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/445_pdf.pdf</url>
      <abstract>The goal of this paper is (1) to illustrate a specific procedure for merging different monolingual lexicons, focussing on techniques for detecting and mapping equivalent lexical entries, and (2) to sketch a production model that enables one to obtain lexical resources via unification of existing data. We describe the creation of a Unified Lexicon (UL) from a common sample of the Italian PAROLE-SIMPLE-CLIPS phonological lexicon and of the Italian LCSTAR pronunciation lexicon. We expand previous experiments carried out at ILC-CNR: based on a detailed mechanism for mapping grammatical classifications of candidate UL entries, a consensual set of Unified Morphosyntactic Specifications (UMS) shared by lexica for the written and spoken areas is proposed. The impact of the UL on cross-validation issues is analysed: by looking into conflicts, mismatches and diverging classifications can be detected in both resources. The work presented is in line with the activities promoted by ELRA towards the development of methods for packaging new language resources by combining independently created resources, and was carried out as part of the ELRA Production Committee activities. ELRA aims to exploit the UL experience to carry out such merging activities for resources available on the ELRA catalogue in order to fulfill the users' needs.</abstract>
    </paper>
    <paper id="266">
      <author><first>Ronny</first><last>Melz</last></author>
      <author><first>Pum-Mo</first><last>Ryu</last></author>
      <author><first>Key-Sun</first><last>Choi</last></author>
      <title>Compiling large language resources using lexical similarity metrics for domain taxonomy learning</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/446_pdf.pdf</url>
      <abstract>In this contribution we present a new methodology to compile large language resources for domain-specific taxonomy learning. We describe the necessary stages to deal with the rich morphology of an agglutinative language, i.e. Korean, and point out a second order machine learning algorithm to unveil term similarity from a given raw text corpus. The language resource compilation described is part of a fully automatic top-down approach to construct taxonomies, without involving the human efforts which are usually required.</abstract>
    </paper>
    <paper id="267">
      <author><first>Felix</first><last>Pîrvan</last></author>
      <author><first>Dan</first><last>Tufiş</last></author>
      <title>Tagset Mapping and Statistical Training Data Cleaning-up</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/448_pdf.pdf</url>
      <abstract>The paper describes a general method (as well as its implementation and evaluation) for deriving mapping systems for different tagsets available in existing training corpora (gold standards) for a specific language. For each pair of corpora (tagged with different tagsets), one such mapping system is derived. This mapping system is then used to improve the tagging of each of the two corpora with the tagset of the other (this process will be called cross-tagging). By reapplying the algorithm to the newly obtained corpora, the accuracy of the underlying training corpora can also be improved. Furthermore, comparing the results with the gold standards makes it possible to assess the distributional adequacy of various tagsets used in processing the language in case. Unlike other methods, such as those reported in (Brants, 1995) or (Tufis &amp; Dragomirescu, 2004), which assume a subsumption relation between the considered tagsets, and as such they aim at minimizing the tagsets by eliminating the feature-value redundancy, this method is applicable for completely unrelated tagsets. Although the experiments were focused on morpho-syntactic (POS) tagging, the method is applicable to other types of tagging as well.</abstract>
    </paper>
    <paper id="268">
      <author><first>Dan</first><last>Tufiş</last></author>
      <author><first>Elena</first><last>Irimia</last></author>
      <title><fixed-case>R</fixed-case>o<fixed-case>C</fixed-case>o-News: A Hand Validated Journalistic Corpus of <fixed-case>R</fixed-case>omanian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/451_pdf.pdf</url>
      <abstract>The paper briefly describes the RoCo project and, in details, one of its first outcomes, the RoCo-News corpus. RoCo-News is a middle-sized journalistic corpus of Romanian, abundant in proper names, numerals and named entities. The initially raw text was previously segmented with MtSeg segmenter, then POS annotated with TNT tagger. RoCo-News was further lemmatized and validated. Because of limited human resources, time constraints and the dimension of the corpus, hand validation of each individual token was out of question. The validation stage required a coherent methodology for automatically identifying as many POS annotation and lemmatization errors as possible. The hand validation process was focused on these automatically spotted possible errors. This methodology relied on three main techniques for automatic detection of potential errors: 1. when lemmatizing the corpus, we extracted all the triples that were not found in the word-form lexicon; 2. we checked the correctness of POS annotation for closed class lexical categories, technique described by (Dickinson &amp; Meurers, 2003); 3. we exploited the hypothesis (Tufiº, 1999) according to which an accurately tagged text, re-tagged with the language model learnt from it (biased evaluation) should have more than 98% tokens identically tagged.</abstract>
    </paper>
    <paper id="269">
      <author><first>Eckhard</first><last>Bick</last></author>
      <title>Turning a Dependency Treebank into a <fixed-case>PSG</fixed-case>-style Constituent Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/452_pdf.pdf</url>
      <abstract>In this paper, we present and evaluate a new method to convert Constraint Grammar (CG) parses of running text into Constituent Treebanks. The conversion is two-step - first a grammar-based method is used to bridge the gap between raw CG annotation and full dependency structure, then phrase structure bracketing and non-terminal nodes are introduced by clustering sister dependents, effectively building one syntactic treebank on top of another. The method is compared with another approach (Bick 2003-2), where constituent structures are arrived at by employing a function-tag based Phrase Structure Grammar (PSG). Results are evaluated on a small reference corpus for both raw and revised CG input, with bracketing F-Scores of 87.5% for raw text and 97.1% for revised CG input, and a raw text edge label accuracy of 95.9% for forms and 86% for functions, or 99.7% and 99.4%, respectively, for revised CG. By applying the tools to the CG-only part of the Danish Arboretum treebank we were able to increase the size of the treebank by 86%, from 197.400 to 367.500 words.</abstract>
    </paper>
    <paper id="270">
      <author><first>Dan</first><last>Ştefănescu</last></author>
      <author><first>Dan</first><last>Tufiş</last></author>
      <title>Aligning Multilingual Thesauri</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/453_pdf.pdf</url>
      <abstract>The aligning and merging of ontologies with overlapping information are actual one of the most active domain of investigation in the Semantic Web community. Multilingual lexical ontologies thesauri are fundamental knowledge sources for most NLP projects addressing multilinguality. The alignment of multilingual lexical knowledge sources has various applications ranging from knowledge acquisition to semantic validation of interlingual equivalence of presumably the same meaning express in different languages. In this paper, we present a general method for aligning ontologies, which was used to align a conceptual thesaurus, lexicalized in 20 languages with a partial version of it lexicalized in Romanian. The objective of our work was to align the existing terms in the Romanian Eurovoc to the terms in the English Eurovoc and to automatically update the Romanian Eurovoc. The general formulation of the ontology alignment problem was set up along the lines established by Heterogeneity group of the KnowledgeWeb consortium, but the actual case study was motivated by the needs of a specific NLP project.</abstract>
    </paper>
    <paper id="271">
      <author><first>Radu</first><last>Ion</last></author>
      <author><first>Alexandru</first><last>Ceauşu</last></author>
      <author><first>Dan</first><last>Tufiş</last></author>
      <title>Dependency-Based Phrase Alignment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/454_pdf.pdf</url>
      <abstract>Phrase alignment is the task that requires the constituent phrases of two halves of a bitext to be aligned. In order to align phrases, one must discover them first and this article presents a method of aligning phrases that are discovered automatically. Here, the notion of a 'phrase' will be understood as being given by a subtree of a dependency-like structure of a sentence called linkage. To discover phrases, we will make use of two distinct, language independent methods: the IBM-1 model (Brown et al., 1993) adapted to detect linkages and Constrained Lexical Attraction Models (Ion &amp; Barbu Mititelu, 2006). The methods will be combined and the resulted model will be used to annotate the bitext. The accuracy of phrase alignment will be evaluated by obtaining word alignments from link alignments and then by checking the F-measure of the latter word aligner.</abstract>
    </paper>
    <paper id="272">
      <author><first>Alexandru</first><last>Ceauşu</last></author>
      <author><first>Dan</first><last>Ştefănescu</last></author>
      <author><first>Dan</first><last>Tufiş</last></author>
      <title><fixed-case>A</fixed-case>cquis <fixed-case>C</fixed-case>ommunautaire Sentence Alignment using Support Vector Machines</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/456_pdf.pdf</url>
      <abstract>Sentence alignment is a task that requires not only accuracy, as possible errors can affect further processing, but also requires small computation resources and to be language pair independent. Although many implementations do not use translation equivalents because they are dependent on the language pair, this feature is a requirement for the accuracy increase. The paper presents a hybrid sentence aligner that has two alignment iterations. The first iteration is based mostly on sentences length, and the second is based on a translation equivalents table estimated from the results of the first iteration. The aligner uses a Support Vector Machine classifier to discriminate between positive and negative examples of sentence pairs.</abstract>
    </paper>
    <paper id="273">
      <author><first>Claire</first><last>Grover</last></author>
      <author><first>Richard</first><last>Tobin</last></author>
      <title>Rule-Based Chunking and Reusability</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/457_pdf.pdf</url>
      <abstract>In this paper we discuss a rule-based approach to chunking implemented using the LT-XML2 and LT-TTT2 tools. We describe the tools and the pipeline and grammars that have been developed for the task of chunking. We show that our rule-based approach is easy to adapt to different chunking styles and that the mark-up of further linguistic information such as nominal and verbal heads can be added to the rules at little extra cost. We evaluate our chunker against the CoNLL 2000 data and discuss discrepancies between our output and the CoNLL mark-up as well as discrepancies within the CoNLL data itself. We contrast our results with the higher scores obtained using machine learning and argue that the portability and flexibility of our approach still make it a more practical solution.</abstract>
    </paper>
    <paper id="274">
      <author><first>Baden</first><last>Hughes</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <author><first>Jeremy</first><last>Nicholson</last></author>
      <author><first>Andrew</first><last>MacKinlay</last></author>
      <title>Reconsidering Language Identification for Written Language Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/459_pdf.pdf</url>
      <abstract>The task of identifying the language in which a given document (ranging from a sentence to thousands of pages) is written has been relatively well studied over several decades. Automated approachesto written language identification are used widely throughout research and industrial contexts, over both oral and written source materials. Despite this widespread acceptance, a review of previous research in written language identification reveals a number of questions which remain openand ripe for further investigation.</abstract>
    </paper>
    <paper id="275">
      <author><first>Yasuko</first><last>Senda</last></author>
      <author><first>Yasusi</first><last>Sinohara</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <title>Automatic Terminology Intelligibility Estimation for Readership-oriented Technical Writing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/461_pdf.pdf</url>
      <abstract>This paper describes automatic terminology intelligibility estimation for readership-oriented technical writing. We assume that the term frequency weighted by the types of documents can be an indicator of the term intelligibility for a certain readership. From this standpoint, we analyzed the relationship between the following: average intelligibility levels of 46 technical terms that were rated by about 120 laymen; numbers of documents that an Internet search</abstract>
    </paper>
    <paper id="276">
      <author><first>Mats</first><last>Lundälv</last></author>
      <author><first>Katarina</first><last>Mühlenbock</last></author>
      <author><first>Bengt</first><last>Farre</last></author>
      <author><first>Annika</first><last>Brännström</last></author>
      <title><fixed-case>SYMBERED</fixed-case> - a Symbol-Concept Editing Tool</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/463_pdf.pdf</url>
      <abstract>The aim of the Nordic SYMBERED project - funded by NUH (the Nordic Development Centre for Rehabilitation Technology) - is to develop a user friendly editing tool that makes use of concept coding to produce web pages with flexible graphical symbol support targeted towards people with Augmentative and Alternative Communication (AAC) needs. Documents produced with the editing tool will be in XML/XHTML format, well suited for publishing on the Internet. These documents will then contain natural language text, such as Swedish or English. Some, or all, of the words in the text will be marked with a concept code defining its meaning. The coded words/concepts may then easily be represented by alternative kinds of graphical symbols and by additional text representations in alternative languages. Thus, within one web document created by the author with the SYMBERED tool, one symbol language can easily be swapped for another. This means that a Bliss and a PCS symbol user can each have his/her preferred kind of symbol support. The SYMBERED editing tool will initially support a limited vocabulary in four to five Nordic languages plus English, and three to four symbol systems, with built-in extensibility to cover more languages and symbol systems.</abstract>
    </paper>
    <paper id="277">
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Valia</first><last>Kordoni</last></author>
      <title>Automated Deep Lexical Acquisition for Robust Open Texts Processing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/464_pdf.pdf</url>
      <abstract>In this paper, we report on methods to detect and repair lexical errors for deep grammars. The lack of coverage has for long been the major problem for deep processing. The existence of various errors in the hand-crafted large grammars prevents their usage in real applications. The manual detection and repair of errors requires asignificant amount of human effort. An experiment with the British National Corpus shows about 70% of the sentences contain unknownword(s) for the English Resource Grammar. With the help of error mining methods, many lexical errors are discovered, which cause a large part of the parsing failures. Moreover, with a lexical type predictor based on a maximum entropy model, new lexical entries are automatically generated. The contribution of various features for the model is evaluated. With the disambiguated full parsing results, the precision of the predictor is enhanced significantly.</abstract>
    </paper>
    <paper id="278">
      <author id="jean-claude-martin"><first>J.-C.</first><last>Martin</last></author>
      <author id="george-caridakis"><first>G.</first><last>Caridakis</last></author>
      <author id="laurence-devillers"><first>L.</first><last>Devillers</last></author>
      <author id="kostas-karpouzis"><first>K.</first><last>Karpouzis</last></author>
      <author id="sarkis-abrilian"><first>S.</first><last>Abrilian</last></author>
      <title>Manual Annotation and Automatic Image Processing of Multimodal Emotional Behaviours: Validating the Annotation of <fixed-case>TV</fixed-case> Interviews</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/465_pdf.pdf</url>
      <abstract>There has been a lot of psychological researches on emotion and nonverbal communication. Yet, these studies were based mostly on acted basic emotions. This paper explores how manual annotation and image processing can cooperate towards the representation of spontaneous emotional behaviour in low resolution videos from TV. We describe a corpus of TV interviews and the manual annotations that have been defined. We explain the image processing algorithms that have been designed for the automatic estimation of movement quantity. Finally, we explore how image processing can be used for the validation of manual annotations.</abstract>
    </paper>
    <paper id="279">
      <author><first>Cvetana</first><last>Krstev</last></author>
      <author><first>Ranka</first><last>Stanković</last></author>
      <author><first>Duško</first><last>Vitas</last></author>
      <author><first>Ivan</first><last>Obradović</last></author>
      <title><fixed-case>WS</fixed-case>4<fixed-case>LR</fixed-case>: A Workstation for Lexical Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/467_pdf.pdf</url>
      <abstract>In this paper we describe WS4LR, the workstation for lexical resources, a software tool developed within the Human Language Technology Group at the Faculty of Mathematics, University of Belgrade. The tool is aimed at manipulating heterogeneous lexical resources, and the need for such a tool came from the large volume of resources the Group has developed in the course of many years and within different projects. The tool handles morphological dictionaries, wordnets, aligned texts and transducers equally and has already proved very useful for various tasks. Although it has so far been used mainly for Serbian, WS4LR is not language dependent and can be successfully used for resources in other languages provided that they follow the described formats and methodologies. The tool operates on the .NET platform and runs on a personal computer under Windows 2000/XP/2003 operating system with at least 256MB of internal memory.</abstract>
    </paper>
    <paper id="280">
      <author><first>Karin</first><last>Kipper</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <author><first>Neville</first><last>Ryant</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <title>Extending <fixed-case>V</fixed-case>erb<fixed-case>N</fixed-case>et with Novel Verb Classes</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/468_pdf.pdf</url>
      <abstract>Lexical classifications have proved useful in supporting various natural language processing (NLP) tasks. The largest verb classification for English is Levin's (1993) work which defined groupings of verbs based on syntactic properties. VerbNet - the largest computational verb lexicon currently available for English - provides detailed syntactic-semantic descriptions of Levin classes. While the classes included are extensive enough for some NLP use, they are not comprehensive. Korhonen and Briscoe (2004) have proposed a significant extension of Levin's classification which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. This paper describes the integration of these classes into VerbNet. The result is the most extensive Levin-style classification for English verbs which can be highly useful for practical applications.</abstract>
    </paper>
    <paper id="281">
      <author><first>James</first><last>Pustejovsky</last></author>
      <author><first>Catherine</first><last>Havasi</last></author>
      <author><first>Jessica</first><last>Littman</last></author>
      <author><first>Anna</first><last>Rumshisky</last></author>
      <author><first>Marc</first><last>Verhagen</last></author>
      <title>Towards a Generative Lexical Resource: The <fixed-case>B</fixed-case>randeis Semantic Ontology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/469_pdf.pdf</url>
      <abstract>In this paper we describe the structure and development of the Brandeis Semantic Ontology (BSO), a large generative lexicon ontology and lexical database. The BSO has been designed to allow for more widespread access to Generative Lexicon-based lexical resources and help researchers in a variety of computational tasks. The specification of the type system used in the BSO largely follows that proposed by the SIMPLE specification (Busa et al., 2001), which was adopted by the EU-sponsored SIMPLE project (Lenci et al., 2000).</abstract>
    </paper>
    <paper id="282">
      <author><first>Hans</first><last>Dybkjær</last></author>
      <author><first>Laila</first><last>Dybkjær</last></author>
      <title>Act-Topic Patterns for Automatically Checking Dialogue Models</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/471_pdf.pdf</url>
      <abstract>When dialogue models are evaluated today, this is normally done by using some evaluation method to collect data, often involving users interacting with the system model, and then subsequently analysing the collected data. We present a tool called DialogDesigner that enables automatic evaluation performed directly on the dialogue model and that does not require any data collection first. DialogDesigner is a tool in support of rapid design and evaluation of dialogue models. The first version was developed in 2005 and enabled developers to create an electronic dialogue model, get various graphical views of the model, run a Wizard-of-Oz (WOZ) simulation session, and extract different presentations in HTML. The second version includes extensions in terms of support for automatic dialogue model evaluation. Various aspects of dialogue model well-formedness can be automatically checked. Some of the automatic analyses simply perform checks based on the state and transition structure of the dialogue model while the core part are based on act-topic annotation of prompts and transitions in the dialogue model and specification of act-topic patterns. This paper focuses on the version 2 extensions.</abstract>
    </paper>
    <paper id="283">
      <author><first>David M.</first><last>Rojas</last></author>
      <author><first>Takako</first><last>Aikawa</last></author>
      <title>Predicting <fixed-case>MT</fixed-case> Quality as a Function of the Source Language</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/472_pdf.pdf</url>
      <abstract>This paper describes one phase of a large-scale machine translation (MT) quality assurance project. We explore a novel approach to discriminating MT-unsuitable source sentences by predicting the expected quality of the output. The resources required include a set of source/MT sentence pairs, human judgments on the output, a source parser, and an MT system. We extract a number of syntactic, semantic, and lexical features from the source sentences only and train a classifier that we call the Syntactic, Semantic, and Lexical Model (SSLM) (cf. Gamon et al., 2005; Liu &amp; Gildea, 2005; Rajman &amp; Hartley, 2001). Despite the simplicity of the approach, SSLM scores correlate with human judgments and can help determine whether sentences are suitable or unsuitable for translation by our MT system. SSLM also provides information about which source features impact MT quality, connecting this work with the field of controlled language (CL) (cf. Reuther, 2003; Nyberg &amp; Mitamura, 1996). With a focus on the input side of MT, SSLM differs greatly from evaluation approaches such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee &amp; Lavie, 2005) in that these other systems compare MT output with reference sentences for evaluation and do not provide feedback regarding potentially problematic source material. Our method bridges the research areas of CL and MT evaluation by addressing the importance of providing MT-suitable English input to enhance output quality.</abstract>
    </paper>
    <paper id="284">
      <author><first>Paweł</first><last>Mazur</last></author>
      <author><first>Robert</first><last>Dale</last></author>
      <title>Named Entity Extraction with Conjunction Disambiguation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/473_pdf.pdf</url>
      <abstract>The recognition of named entities is now a well-developed area, with a range of symbolic and machine learning techniques that deliver high accuracy extraction and categorisation of a variety of entity types. However, there are still some named entity phenomena that present problems for existing techniques; in particular, relatively little work has explored the disambiguation of conjunctions appearing in candidate named entity strings. We demonstrate that there are in fact four distinct uses of conjunctions in the context of named entities; we present some experiments using machine-learned classifiers to disambiguate the different uses of the conjunction, with 85% of test examples being correctly classified.</abstract>
    </paper>
    <paper id="285">
      <author><first>Michel</first><last>Boekestein</last></author>
      <author><first>Griet</first><last>Depoorter</last></author>
      <author><first>Remco</first><last>van Veenendaal</last></author>
      <title>Functioning of the Centre for <fixed-case>D</fixed-case>utch Language and Speech Technology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/478_pdf.pdf</url>
      <abstract>The TST Centre manages a broad collection of Dutch digital language resources. It is an initiative of the Dutch Language Union (Nederlandse Taalunie), and is meant to reinforce research in the area of language and speech technology. It does this by stimulating the reuse of these language resources. The TST Centre keeps these resources up to date, facilitates their availability, and offers services such as providing information, documentation, online access, offering catalogues, custom-made data, etc. Also, the TST Centre strives for a uniformised, if not standardised, treatment of language resources of the same nature. A well-thought, structured administration system is needed to manage the various language resources, their updates, derived products, IPR, user administration, etc. We will discuss the organisation, tasks and services of the TST Centre, and the language resources it maintains. Also, we will look into practical data management solutions, IPR issues, and our activities in standardisation and linking language resources.</abstract>
    </paper>
    <paper id="286">
      <author><first>Bente</first><last>Maegaard</last></author>
      <author><first>Lene</first><last>Offersgaard</last></author>
      <author><first>Lina</first><last>Henriksen</last></author>
      <author><first>Hanne</first><last>Jansen</last></author>
      <author><first>Xavier</first><last>Lepetit</last></author>
      <author><first>Costanza</first><last>Navarretta</last></author>
      <author><first>Claus</first><last>Povlsen</last></author>
      <title>The <fixed-case>MULINCO</fixed-case> corpus and corpus platform</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/480_pdf.pdf</url>
      <abstract>The MULINCO project (MUltiLINgual Corpus of the University of Copenhagen) started early 2005. The purpose of this cross-disciplinary project is to create a corpus platform for education and research in monolingual and translation studies. The project covers two main types of corpus texts: literary and non-literary. The platform is being developed using available tools as far as possible, and integrating them in a very open architecture. In this paper we describe the current status and future developments of both the text and tool side of the corpus platform, and we show some examples of student exercises taking advantage of tagged and aligned texts.</abstract>
    </paper>
    <paper id="287">
      <author><first>Claudia</first><last>Soria</last></author>
      <author><first>Maurizio</first><last>Tesconi</last></author>
      <author><first>Francesca</first><last>Bertagna</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>Andrea</first><last>Marchetti</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <title>Moving to dynamic computational lexicons with <fixed-case>L</fixed-case>e<fixed-case>XF</fixed-case>low</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/481_pdf.pdf</url>
      <abstract>In this paper we present LeXFlow, a web application framework where lexicons already expressed in standardised format semi-automatically interact by reciprocally enriching themselves. LeXFlow is intended for, on the one hand, paving the way to the development of dynamic multi-source lexicons; and on the other, for fostering the adoption of standards. Borrowing from techniques used in the domain of document workflows, we model the activity of lexicon management as a particular case of workflow instance, where lexical entries move across agents and become dynamically updated. To this end, we have designed a lexical flow (LF) corresponding to the scenario where an entry of a lexicon A becomes enriched via basically two steps. First, by virtue of being mapped onto a corresponding entry belonging to a lexicon B, the entry(LA) inherits the semantic relations available in lexicon B. Second, by resorting to an automatic application that acquires information about semantic relations from corpora, the relations acquired are integrated into the entry and proposed to the human encoder. As a result of the lexical flow, in addition, for each starting lexical entry(LA) mapped onto a corresponding entry(LB) the flow produces a new entry representing the merging of the original two.</abstract>
    </paper>
    <paper id="288">
      <author><first>Caroline</first><last>Sporleder</last></author>
      <author><first>Marieke</first><last>van Erp</last></author>
      <author><first>Tijn</first><last>Porcelijn</last></author>
      <author><first>Antal</first><last>van den Bosch</last></author>
      <author><first>Pim</first><last>Arntzen</last></author>
      <title>Identifying Named Entities in Text Databases from the Natural History Domain</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/482_pdf.pdf</url>
      <abstract>In this paper, we investigate whether it is possible to bootstrap a named entity tagger for textual databases by exploiting the database structure to automatically generate domain and database-specific gazetteer lists. We compare three tagging strategies: (i) using the extracted gazetteers in a look-up tagger, (ii) using the gazetteers to automatically extract training data to train a database-specific tagger, and (iii) using a generic named entity tagger. Our results suggest that automatically built gazetteers in combination with a look-up tagger lead to a relatively good performance and that generic taggers do not perform particularly well on this type of data.</abstract>
    </paper>
    <paper id="289">
      <author><first>Harold</first><last>Somers</last></author>
      <author><first>Gareth</first><last>Evans</last></author>
      <author><first>Zeinab</first><last>Mohamed</last></author>
      <title>Developing Speech Synthesis for Under-Resourced Languages by “Faking it”: An Experiment with <fixed-case>S</fixed-case>omali</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/483_pdf.pdf</url>
      <abstract>Speech synthesis or text-to-speech (TTS) systems are currently available for a number of the world's major languages, but for thousands of other, unsupported, languages no such technology is available. While awaiting the development of such technology, we propose using an existing TTS system for a major language (the base language, BL) to "fake" TTS for an unsupported language (the target language, TL). This paper describes the factors which determine the choice of a suitable BL for a given TL, and describe an experiment with a fake Somali TTS system evaluated in the real-life situation of a doctorpatient dialogue. 28 Somali participants were asked to judge the comprehensibility of 25 short Somali sentences recorded with a German TTS system. Results suggest that "faking it" provides reasonable stop-gap TTS for unsupported languages.</abstract>
    </paper>
    <paper id="290">
      <author><first>Dragoş</first><last>Ciobanu</last></author>
      <author><first>Tony</first><last>Hartley</last></author>
      <author><first>Serge</first><last>Sharoff</last></author>
      <title>Using Richly Annotated Trilingual Language Resources for Acquiring Reading Skills in a Foreign Language</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/484_pdf.pdf</url>
      <abstract>In an age when demand for innovative and motivating language teaching methodologies is at a very high level, TREAT - the Trilingual REAding Tutor - combines the most advanced natural language processing (NLP) techniques with the latest second and third language acquisition (SLA/TLA) research in an intuitive and user-friendly environment that has been proven to help adult learners (native speakers of L1) acquire reading skills in an unknown L3 which is related to (cognate with) an L2 they know to some extent. This corpus-based methodology relies on existing linguistic resources, as well as materials that are easy to assemble, and can be adapted to support other pairs of L2-L3 related languages, as well. A small evaluation study conducted at the Leeds University Centre for Translation Studies indicates that, when using TREAT, learners feel more motivated to study an unknown L3, acquire significant linguistic knowledge of both the L3 and L2 rapidly, and increase their performance when translating from L3 into L1.</abstract>
    </paper>
    <paper id="291">
      <author id="gianmaria-ajani"><first>G.</first><last>Ajani</last></author>
      <author id="guido-boella"><first>G.</first><last>Boella</last></author>
      <author id="leonardo-lesmo"><first>L.</first><last>Lesmo</last></author>
      <author id="marco-martin"><first>M.</first><last>Martin</last></author>
      <author id="alessandro-mazzei"><first>A</first><last>Mazzei</last></author>
      <author id="piercarlo-rossi"><first>P.</first><last>Rossi</last></author>
      <title>A Development Tool For Multilingual Ontology-based Conceptual</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/485_pdf.pdf</url>
      <abstract>This paper introduces a number theoretical and practical issues related to the Syllabus. Syllabusis a multi-lingua ontology based tool, designed to improve the applications of the European Directives in the various European countries.</abstract>
    </paper>
    <paper id="292">
      <author><first>Bente</first><last>Maegaard</last></author>
      <author><first>Jens-Erik</first><last>Fenstad</last></author>
      <author><first>Lars</first><last>Ahrenberg</last></author>
      <author><first>Knut</first><last>Kvale</last></author>
      <author><first>Katarina</first><last>Mühlenbock</last></author>
      <author><first>Bernt-Erik</first><last>Heid</last></author>
      <title><fixed-case>KUNSTI</fixed-case> - Knowledge Generation for <fixed-case>N</fixed-case>orwegian Language Technology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/487_pdf.pdf</url>
      <abstract>KUNSTI is the Norwegian national language technology programme, running 2001-2006 inclusive. The goal of the programme is to boost Norwegian language technology research. In this paper we describe the background, the objectives, the methodology applied in the management of the programme, the projects selected, and our first conclusions. We also describe national programmes form Sweden, France and Germany and compare objectives and methods.</abstract>
    </paper>
    <paper id="293">
      <author><first>Péter</first><last>Halácsy</last></author>
      <author><first>András</first><last>Kornai</last></author>
      <author><first>Csaba</first><last>Oravecz</last></author>
      <author><first>Viktor</first><last>Trón</last></author>
      <author><first>Dániel</first><last>Varga</last></author>
      <title>Using a morphological analyzer in high precision <fixed-case>POS</fixed-case> tagging of <fixed-case>H</fixed-case>ungarian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/488_pdf.pdf</url>
      <abstract>The paper presents an evaluation of maxent POS disambiguation systems that incorporate an open source morphological analyzer to constrain the probabilistic models. The experiments show that the best proposed architecture, which is the first application of the maximum entropy framework in a Hungarian NLP task, outperforms comparable state of the art tagging methods and is able to handle out of vocabulary items robustly, allowing for efficient analysis of large (web-based) corpora.</abstract>
    </paper>
    <paper id="294">
      <author><first>Dominic</first><last>Widdows</last></author>
      <author><first>Adil</first><last>Toumouh</last></author>
      <author><first>Beate</first><last>Dorow</last></author>
      <author><first>Ahmed</first><last>Lehireche</last></author>
      <title>Ongoing Developments in Automatically Adapting Lexical Resources to the Biomedical Domain</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/489_pdf.pdf</url>
      <abstract>This paper describes a range of experiments using empirical methods to adapt theWordNet noun ontology for specific use in the biomedical domain. Our basic technique is to extract relationships between terms using the Ohsumed corpus, a large collection of abstracts from PubMed, and to compare the relationships extracted with those that would be expected for medical terms, given the structure of the WordNet ontology. The linguistic methods involve the use of a variety of lexicosyntactic patterns that enable us to extract pairs of coordinate noun terms, and also related groups of adjectives and nouns, using Markov clustering. This enables us in many cases to analyse ambiguous words and select the correct meaning for the biomedical domain. While results are often encouraging, the paper also highlights evident problems and drawbacks with the method, and outlines suggestions for future work.</abstract>
    </paper>
    <paper id="295">
      <author><first>Renata</first><last>Savy</last></author>
      <author><first>Francesco</first><last>Cutugno</last></author>
      <author><first>Claudia</first><last>Crocco</last></author>
      <title>Multilevel corpus analysis: generating and querying an <fixed-case>AG</fixed-case>set of spoken <fixed-case>I</fixed-case>talian (<fixed-case>S</fixed-case>p<fixed-case>I</fixed-case>t-<fixed-case>MD</fixed-case>b).</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/492_pdf.pdf</url>
      <abstract>In this paper we present an application of AGTK to a corpus of spoken Italian annotated at many different linguistic levels. The work consists of two parts: a) the presentation of AG-SpIt, a toolkit devoted to corpus data management that we developed according to AGTK proposals; b) the presentation of corpus structure together with some examples and results of cross-level linguistic analyses obtained querying the database (SpIt-MDb). As this work is still an ongoing investigation, results must be considered preliminary, as a demo illustrating the potentiality of the tool and the advantages it introduces to validate linguistic theories and annotation systems. Currently, SpIt-MDb is a linguistic resource under development; it represents one of the first attempts to create an Italian corpus labelled at various linguistic levels (from acoustic/sub-phonetic, to textual/pragmatic ones) which can be queried in the interrelations among levels.</abstract>
    </paper>
    <paper id="296">
      <author><first>Baden</first><last>Hughes</last></author>
      <author><first>Dafydd</first><last>Gibbon</last></author>
      <author><first>Thorsten</first><last>Trippel</last></author>
      <title>Feature-based Encoding and Querying Language Resources with Character Semantics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/493_pdf.pdf</url>
      <abstract>In this paper we discuss the explicit representation of character features pertaining to written language resources, which we argue are critically necessary in the long term of archiving language data. Much focus on the creation of language resources and their associated preservation is at the level of the corpus itself; however it is generally accepted that long term interpretation of these language resources requires more than a best practice data format. In particular, where language resources are created in linguistic fieldwork, and especially for minority languages, the need for preservation not only of the resource itself, but of additional metadata which allows for the resource to be accurately interpreted in the future is becoming a topic of research in itself. In this paper we extend earlier work on semantically based character decomposition to include representation of character properties in a variety of models, and a mechanism for exploiting these properties through queries.</abstract>
    </paper>
    <paper id="297">
      <author><first>Rajen</first><last>Subba</last></author>
      <author><first>Barbara</first><last>Di Eugenio</last></author>
      <author><first>Elena</first><last>Terenzi</last></author>
      <title>Building lexical resources for <fixed-case>P</fixed-case>rinc<fixed-case>P</fixed-case>ar, a large coverage parser that generates principled semantic representations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/495_pdf.pdf</url>
      <abstract>Parsing, one of the more successful areas of Natural Language Processing has mostly been concerned with syntactic structure. Though uncovering the syntactic structure of sentences is very important, in many applications a meaningrepresentation for the input must be derived as well. We report on PrincPar, a parser that builds full meaning representations. It integrates LCFLEX, a robust parser, with alexicon and ontology derived from two lexical resources, VerbNet and CoreLex that represent the semantics of verbs and nouns respectively. We show that these two different lexical resources that focus on verbs and nouns can be successfully integrated. We report parsing results on a corpus of instructional text and assess the coverage of those lexical resources. Our evaluation metric is the number of verb frames that are assigned a correct semantics: 72.2% verb frames are assigned a perfect semantics, and another 10.9% are assigned a partially correctsemantics. Our ultimate goal is to develop a (semi)automatic method to derive domain knowledge from instructional text, in the form of linguistically motivated action schemes.</abstract>
    </paper>
    <paper id="298">
      <author><first>Kiyotaka</first><last>Uchimoto</last></author>
      <author><first>Naoko</first><last>Hayashida</last></author>
      <author><first>Toru</first><last>Ishida</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>Automatic Detection and Semi-Automatic Revision of Non-Machine-Translatable Parts of a Sentence</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/496_pdf.pdf</url>
      <abstract>We developed a method for automatically distinguishing the machine-translatable and non-machine-translatable parts of a given sentence for a particular machine translation (MT) system. They can be distinguished by calculating the similarity between a source-language sentence and its back translation for each part of the sentence. The parts with low similarities are highly likely to be non-machine-translatable parts. We showed that the parts of a sentence that are automatically distinguished as non-machine-translatable provide useful information for paraphrasing or revising the sentence in the source language to improve the quality of the translation by the MT system. We also developed a method of providing knowledge useful to effectively paraphrasing or revising the detected non-machine-translatable parts. Two types of knowledge were extracted from the EDR dictionary: one for transforming a lexical entry into an expression used in the definition and the other for conducting the reverse paraphrasing, which transforms an expression found in a definition into the lexical entry. We found that the information provided by the methods helped improve the machine translatability of the originally input sentences.</abstract>
    </paper>
    <paper id="299">
      <author><first>Isa</first><last>Maks</last></author>
      <author><first>Bob</first><last>Boelhouwer</last></author>
      <title>Exploring opportunities for Comparability and Enrichment by Linking lexical databases</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/497_pdf.pdf</url>
      <abstract>Results are presented of an ongoing project of the Dutch TST-centre for language and speech technology aiming at linking of various lexical databases. The project involves four Dutch monolingual lexicons: WlNT05, e-Lex, RBN and RBBN. These databases differ in organisational structure and content. To enable linkage between these lexicons, we developed a common feature value set and a common organisational structure. Both are based upon existing standards for the creation and reusability of lexicons: the Lexical Markup Framework and the EAGLES standard. Examples of the content and structure of each of the lexical databases are presented in their original form. Also, the structure and content is shown when mapped onto the common framework and feature value set. Thus, the commonalities and the complementarity of the lexical databases are more readily apparent. Besides, this elaboration of the databases opens up the opportunity for mutual enrichment.</abstract>
    </paper>
    <paper id="300">
      <author><first>Horacio</first><last>Saggion</last></author>
      <title>Multilingual Multidocument Summarization Tools and Evaluation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/498_pdf.pdf</url>
      <abstract>We describe a number of experiments carried out to address the problem of creating summaries from multiple sources in multiple languages. A centroid-based sentence extraction system has been developed which decides the content of the summary using texts in different languages and uses sentences from English sources alone to create the final output. We describe the evaluation of the system in the recent Multilingual Summarization Evaluation MSE 2005 using the pyramids and ROUGE methods.</abstract>
    </paper>
    <paper id="301">
      <author><first>Olivier</first><last>Ferret</last></author>
      <title>Building a network of topical relations from a corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/500_pdf.pdf</url>
      <abstract>Lexical networks such as WordNet are known to have a lack of topical relations although these relations are very useful for tasks such as text summarization or information extraction. In this article, we present a method for automatically building from a large corpus a lexical network whose relations are preferably topical ones. As it does not rely on resources such as dictionaries, this method is based on self-bootstrapping: a network of lexical cooccurrences is first built from a corpus and then, is filtered by using the words of the corpus that are selected by the initial network. We report an evaluation about topic segmentation showing that the results got with the filtered network are the same as the results got with the initial network although the first one is significantly smaller than the second one.</abstract>
    </paper>
    <paper id="302">
      <author id="paolo-bouquet"><first>P.</first><last>Bouquet</last></author>
      <author id="luciano-serafini"><first>L.</first><last>Serafini</last></author>
      <author id="stefano-zanobini"><first>S.</first><last>Zanobini</last></author>
      <title>The role of lexical resources in matching classification schemas</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/501_pdf.pdf</url>
      <abstract>In this paper, we describe the role and the use of WORDNET as an external lexical resource in a methodology for matching hierarchical classification schemas. The main difference between our methodology and others which were presented is that we pay a lot of effort in eliciting the meaning of the structures we match, and we do this by using extensively lexical knowledge about the words occurring in labels. The result of this elicitation process is encoded in a formal language, called WDL (WORDNET Description Logic), which is our proposal for injecting lexical semantics into more standard knowledge representation languages.</abstract>
    </paper>
    <paper id="303">
      <author><first>Manolis</first><last>Maragoudakis</last></author>
      <author><first>Katia</first><last>Kermanidis</last></author>
      <author><first>Aristogiannis</first><last>Garbis</last></author>
      <author><first>Nikos</first><last>Fakotakis</last></author>
      <title>Dealing with Imbalanced Data using <fixed-case>B</fixed-case>ayesian Techniques</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/503_pdf.pdf</url>
      <abstract>For the present work, we deal with the significant problem of high imbalance in data in binary or multi-class classification problems. We study two different linguistic applications. The former determines whether a syntactic construction (environment) co-occurs with a verb in a natural text corpus consists a subcategorization frame of the verb or not. The latter is called Name Entity Recognition (NER) and it concerns determining whether a noun belongs to a specific Name Entity class. Regarding the subcategorization domain, each environment is encoded as a vector of heterogeneous attributes, where a very high imbalance between positive and negative examples is observed (an imbalance ratio of approximately 1:80). In the NER application, the imbalance between a name entity class and the negative class is even greater (1:120). In order to confront the plethora of negative instances, we suggest a search tactic during training phase that employs Tomek links for reducing unnecessary negative examples from the training set. Regarding the classification mechanism, we argue that Bayesian networks are well suited and we propose a novel network structure which efficiently handles heterogeneous attributes without discretization and is more classification-oriented. Comparing the experimental results with those of other known machine learning algorithms, our methodology performs significantly better in detecting examples of the rare class.</abstract>
    </paper>
    <paper id="304">
      <author><first>José-Miguel</first><last>Benedí</last></author>
      <author><first>Eduardo</first><last>Lleida</last></author>
      <author><first>Amparo</first><last>Varona</last></author>
      <author><first>María-José</first><last>Castro</last></author>
      <author><first>Isabel</first><last>Galiano</last></author>
      <author><first>Raquel</first><last>Justo</last></author>
      <author><first>Iñigo</first><last>López de Letona</last></author>
      <author><first>Antonio</first><last>Miguel</last></author>
      <title>Design and acquisition of a telephone spontaneous speech dialogue corpus in <fixed-case>S</fixed-case>panish: <fixed-case>DIHANA</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/504_pdf.pdf</url>
      <abstract>In the framework of the DIHANA project, we present the acquisitionprocess of a spontaneous speech dialogue corpus in Spanish. Theselected application consists of information retrieval by telephone for nationwide trains. A total of 900 dialogues from 225 users were acquired using the Wizard of Oz technique. In this work, we present the design and planning of the dialogue scenes and the wizard strategy used for the acquisition of the corpus. Then, we also present the acquisition tools and a description of the acquisition process.</abstract>
    </paper>
    <paper id="305">
      <author><first>Rainer</first><last>Osswald</last></author>
      <author><first>Hermann</first><last>Helbig</last></author>
      <author><first>Sven</first><last>Hartrumpf</last></author>
      <title>The Representation of <fixed-case>G</fixed-case>erman Prepositional Verbs in a Semantically Based Computer Lexicon</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/505_pdf.pdf</url>
      <abstract>We describe the treatment of verbs with prepositional complements inHaGenLex, a semantically based computer lexicon for German.Prepositional verbs such as bestehen auf (insist on) subcategorize for a prepositional phrase where the preposition usually has no independent meaning of its own. The lexical semantic information inHaGenLex is specified by means of MultiNet, a full-fledged knowledge representation formalism, which proves to be particularly useful for representing the semantics of verbs with prepositional complements.We indicate how the semantic representation in HaGenLex can be used to define semantic classes of prepositional verbs and briefly discuss the relation of these classes to Levin's verb classes. Moreover, wepresent first results on the automatic identification of prepositionalverbs by corpus-based methods.</abstract>
    </paper>
    <paper id="306">
      <author><first>Yun-Chuang</first><last>Chiao</last></author>
      <author><first>Olivier</first><last>Kraif</last></author>
      <author><first>Dominique</first><last>Laurent</last></author>
      <author><first>Thi Minh Huyen</first><last>Nguyen</last></author>
      <author><first>Nasredine</first><last>Semmar</last></author>
      <author><first>François</first><last>Stuck</last></author>
      <author><first>Jean</first><last>Véronis</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <title>Evaluation of multilingual text alignment systems: the <fixed-case>ARCADE</fixed-case> <fixed-case>II</fixed-case> project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/506_pdf.pdf</url>
      <abstract>This paper describes the ARCADE II project, concerned with the evaluation of parallel text alignment systems. The ARCADE II project aims at exploring the techniques of multilingual text alignment through a fine evaluation of the existing techniques and the development of new alignment methods. The evaluation campaign consists of two tracks devoted to the evaluation of alignment at sentence and word level respectively. It differs from ARCADE I in the multilingual aspect and the investigation of lexical alignment.</abstract>
    </paper>
    <paper id="307">
      <author><first>Francesca</first><last>Bertagna</last></author>
      <title>Representation and Inference for Open-Domain <fixed-case>QA</fixed-case>: Strength and Limits of two <fixed-case>I</fixed-case>talian Semantic Lexicons</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/508_pdf.pdf</url>
      <abstract>The paper reports on the results of the exploitation of two Italian lexicons (ItalWordNet and SIMPLE-CLIPS) in an Open-Domain Question Answering application for Italian. The intent is to analyse the behavior of the lexicons in application in order to understand what are their limits and points of strength. The final aim of the paper is contributing to the debate about usefulness of computational lexicons in NLP, by providing evidence from the point of view of a particular application.</abstract>
    </paper>
    <paper id="308">
      <author><first>Abdelrahim</first><last>Abdelsapor</last></author>
      <author><first>Noha</first><last>Adly</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <author><first>Ossama</first><last>Emam</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <author><first>Magdi</first><last>Nagi</last></author>
      <title>Building a Heterogeneous Information Retrieval Collection of Printed <fixed-case>A</fixed-case>rabic Documents</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/509_pdf.pdf</url>
      <abstract>This paper describes the development of an Arabic document image collection containing 34,651 documents from 1,378 different books and 25 topics with their relevance judgments. The books from which the collection is obtained are a part of a larger collection 75,000 books being scanned for archival and retrieval at the bibliotheca Alexandrina (BA). The documents in the collection vary widely in topics, fonts, and degradation levels. Initial baseline experiments were performed to examine the effectiveness of different index terms, with and without blind relevance feedback, on Arabic OCR degraded text.</abstract>
    </paper>
    <paper id="309">
      <author><first>Saturnino</first><last>Luz</last></author>
      <author><first>Matt-Mouley</first><last>Bouamrane</last></author>
      <author><first>Masood</first><last>Masoodian</last></author>
      <title>Gathering a corpus of multimodal computer-mediated meetings</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/510_pdf.pdf</url>
      <abstract>In this paper we describe the gathering of a corpus of synchronised speech and text interaction over the network. The data collection scenarios characterise audio meetings with a significant textual component. Unlike existing meeting corpora, the corpus described in this paper emphasises temporal relationships between speech and text media streams. This is achieved through detailed logging and timestamping of text editing operations, actions on shared user interface widgets and gesturing, as well as generation of speech activity profiles. A set of tools has been developed specifically for these purposes which can be used as a data collection platform for the development of meeting browsers. The data gathered to date consists of nearly 30 hours of recorded audio and time stamped editing operations and gestures.</abstract>
    </paper>
    <paper id="310">
      <author><first>Dimou</first><last>Athanassia Lida</last></author>
      <author><first>Chalamandaris</first><last>Aimilios</last></author>
      <title>Language identification from suprasegmental cues: Speech synthesis of <fixed-case>G</fixed-case>reek utterances from different dialectal variations.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/511_pdf.pdf</url>
      <abstract>In this paper we present the continuation of our research on the ability of native Greek adults to identify their mother tongue from synthesized stimuli which contain only prosodic - melodic and rhythmic - information. In the first section we present the ideas that underlie our theory, together with a brief review of our preliminary results. In the second section the detailed description of our experimental approach is given, as well as the results and their statistical analysis. In the final two sections we provide the conclusions derived from our experiments and the future work we are planning to carry out.</abstract>
    </paper>
    <paper id="311">
      <author><first>Roger</first><last>Levy</last></author>
      <author><first>Galen</first><last>Andrew</last></author>
      <title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/513_pdf.pdf</url>
      <abstract>With syntactically annotated corpora becoming increasingly available for a variety of languages and grammatical frameworks, tree query tools have proven invaluable to linguists and computer scientists for both data exploration and corpus-based research. We provide a combined engine for tree query (Tregex) and manipulation (Tsurgeon) that can operate on arbitrary tree data structures with no need for preprocessing. Tregex remedies several expressive and implementational limitations of existing query tools, while Tsurgeon is to our knowledge the most expressive tree manipulation utility available.</abstract>
    </paper>
    <paper id="312">
      <author id="laurent-gillard"><first>L.</first><last>Gillard</last></author>
      <author id="patrice-bellot"><first>P.</first><last>Bellot</last></author>
      <author id="marc-el-beze"><first>M.</first><last>El-Bèze</last></author>
      <title>Question Answering Evaluation Survey</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/515_pdf.pdf</url>
      <abstract>Evaluating Question Answering (QA) Systems is a very complex task: state-of-the-art systems involve processing whose influences and contributions on the final result are not clear and need to be studied. We present some key points on different aspects of the QA Systems (QAS) evaluation: mainly, as performed during large-scale campaigns, but also with clues on the evaluation of QAS typical software components; the last part of this paper, is devoted to a brief presentation of the French QA campaign EQueR and presents two issues: inter-annotator agreement during campaign and the reuse of reference patterns.</abstract>
    </paper>
    <paper id="313">
      <author id="bernardo-magnini"><first>B.</first><last>Magnini</last></author>
      <author id="emanuele-pianta"><first>E.</first><last>Pianta</last></author>
      <author id="christian-girardi"><first>C.</first><last>Girardi</last></author>
      <author id="matteo-negri"><first>M.</first><last>Negri</last></author>
      <author id="lorenza-romano"><first>L.</first><last>Romano</last></author>
      <author id="manuela-speranza"><first>M.</first><last>Speranza</last></author>
      <author id="valentina-bartalesi-lenzi"><first>V.</first><last>Bartalesi Lenzi</last></author>
      <author id="rachele-sprugnoli"><first>R.</first><last>Sprugnoli</last></author>
      <title><fixed-case>I</fixed-case>-<fixed-case>CAB</fixed-case>: the <fixed-case>I</fixed-case>talian Content Annotation Bank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/518_pdf.pdf</url>
      <abstract>In this paper we present work in progress for the creation of the Italian Content Annotation Bank (I-CAB), a corpus of Italian news annotated with semantic information at different levels. The first level is represented by temporal expressions, the second level is represented by different types of entities (i.e. person, organizations, locations and geo-political entities), and the third level is represented by relations between entities (e.g. the affiliation relation connecting a person to an organization). So far I-CAB has been manually annotated with temporal expressions, person entities and organization entities. As we intend I-CAB to become a benchmark for various automatic Information Extraction tasks, we followed a policy of reusing already available markup languages. In particular, we adopted the annotation schemes developed for the ACE Entity Detection and Time Expressions Recognition and Normalization tasks. As the ACE guidelines have originally been developed for English, part of the effort consisted in adapting them to the specific morpho-syntactic features of Italian. Finally, we have extended them to include a wider range of entities, such as conjunctions.</abstract>
    </paper>
    <paper id="314">
      <author><first>Bente</first><last>Maegaard</last></author>
      <author><first>Steven</first><last>Krauwer</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <author><first>Lise Damsgaard</first><last>Jørgensen</last></author>
      <title>The <fixed-case>BLARK</fixed-case> concept and <fixed-case>BLARK</fixed-case> for <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/521_pdf.pdf</url>
      <abstract>The EU project NEMLAR (Network for Euro-Mediterranean LAnguage Resources) on Arabic language resources carried out two surveys on the availability of Arabic LRs in the region, and on industrial requirements. The project also worked out a BLARK (Basic Language Resource Kit) for Arabic. In this paper we describe the further development of the BLARK concept made during the work on a BLARK for Arabic, as well as the results for Arabic.</abstract>
    </paper>
    <paper id="315">
      <author><first>Gabriella</first><last>Pardelli</last></author>
      <author><first>Manuela</first><last>Sassi</last></author>
      <author><first>Sara</first><last>Goggi</last></author>
      <author><first>Paola</first><last>Orsolini</last></author>
      <title>Natural Language Processing: A Terminological and Statistical Approach</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/524_pdf.pdf</url>
      <abstract>The aim of this article is to provide a statistical representation of significant terms used in the field of Natural Language Processing from the 1960s till nowadays, in order to draft a survey on the most significant research trends in that period. By retrieving these keywords it should be possible to highlight the ebb and flow of some thematic topics. The NLP terminological sample derives from a database created for this purpose using the DBT software (Textual Data Base, ILC patent).</abstract>
    </paper>
    <paper id="316">
      <author><first>Suzan</first><last>Verberne</last></author>
      <author><first>Lou</first><last>Boves</last></author>
      <author><first>Nelleke</first><last>Oostdijk</last></author>
      <author><first>Peter-Arno</first><last>Coppen</last></author>
      <title>Data for question answering: The case of why</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/525_pdf.pdf</url>
      <abstract>For research and development of an approach for automatically answering why-questions (why-QA) a data collection was created. The data set was obtained by way of elicitation and comprises a total of 395 why-questions. For each question, the data set includes the source document and one or two user-formulated answers. In addition, for a subset of the questions, user-formulated paraphrases are available. All question-answer pairs have been annotated with information on topic and semantic answer type. The resulting data set is of importance not only for our research, but we expect it to contribute to and stimulate other research in the field of why-QA.</abstract>
    </paper>
    <paper id="317">
      <author><first>Kiril</first><last>Simov</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <title>Shallow Semantic Annotation of <fixed-case>B</fixed-case>ulgarian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/527_pdf.pdf</url>
      <abstract>The paper discusses shallow semantic annotation of Bulgarian treebank. Our goal is to construct the next layer of linguistic interpretation over the morphological and syntactic layers that have already been encoded in the treebank. The annotation is called shallow because it encodes only the senses for the non-functional words and the relations between the semantic indices connected to them. We do not encode quantifiers and scope information. An ontology is employed as a stock of the concepts and relations that form the word senses. Our lexicon is based on the Generative Lexicon (GL) model (Pustejovsky 1995) as it was implemented in the SIMPLE project (Lenci et. al. 2000). GL defines the way in which the words are connected to the concepts and the relations in the ontology. Also it provides mechanisms for literal sense changes like type-coercion, metonymy, and similar. Some of these phenomena are presented in the annotation.</abstract>
    </paper>
    <paper id="318">
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Walt</first><last>Andrews</last></author>
      <author><first>Joseph P.</first><last>Campbell</last></author>
      <author><first>George</first><last>Doddington</last></author>
      <author><first>Jack</first><last>Godfrey</last></author>
      <author><first>Shudong</first><last>Huang</last></author>
      <author><first>Mark</first><last>Liberman</last></author>
      <author><first>Alvin</first><last>Martin</last></author>
      <author><first>Hirotaka</first><last>Nakasone</last></author>
      <author><first>Mark</first><last>Przybocki</last></author>
      <author><first>Kevin</first><last>Walker</last></author>
      <title>The Mixer and Transcript Reading Corpora: Resources for Multilingual, Crosschannel Speaker Recognition Research</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/530_pdf.pdf</url>
      <abstract>This paper describes the planning and creation of the Mixer and Transcript Reading corpora, their properties and yields, and reports on the lessons learned during their development.</abstract>
    </paper>
    <paper id="319">
      <author id="sarkis-abrilian"><first>S.</first><last>Abrilian</last></author>
      <author id="laurence-devillers"><first>L.</first><last>Devillers</last></author>
      <author id="jean-claude-martin"><first>J-C.</first><last>Martin</last></author>
      <title>Annotation of Emotions in Real-Life Video Interviews: Variability between Coders</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/531_pdf.pdf</url>
      <abstract>Research on emotional real-life data has to tackle the problem of their annotation. The annotation of emotional corpora raises the issue of how different coders perceive the same multimodal emotional behaviour. The long-term goal of this paper is to produce a guideline for the selection of annotators. The LIMSI team is working towards the definition of a coding scheme integrating emotion, context and multimodal annotations. We present the current defined coding scheme for emotion annotation, and the use of soft vectors for representing a mixture of emotions. This paper describes a perceptive test of emotion annotations and the results obtained with 40 different coders on a subset of complex real-life emotional segments selected from the EmoTV Corpus collected at LIMSI. The results of this first study validate previous annotations of emotion mixtures and highlight the difference of annotation between male and female coders.</abstract>
    </paper>
    <paper id="320">
      <author><first>Ya-Min</first><last>Chou</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <title>Hantology-A Linguistic Resource for <fixed-case>C</fixed-case>hinese Language Processing and Studying</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/532_pdf.pdf</url>
      <abstract>Hantology, a character-based Chinese language resource is created to provide an infrastructure for language processing and research on the writing system. Unlike alphabetic or syllabic writing systems, the ideographic writing system of Chinese poses both a challenge and an opportunity. The challenge is that a totally different resources structure must be created to represent and process speakers conventionalization of the language. The rare opportunity is that the structure itself is enriched with conceptual classification and can be utilized for ontology building. We describe the contents and possible applications of Hantology in this paper. The applications of Hantology include: (1) an account for the diachronic development of Chinese lexica (2) character-based language processing, (3) a study of conceptual structure differences in Chinese and English, and (4) comparisons of different ideographic writing systems.</abstract>
    </paper>
    <paper id="321">
      <author><first>Maria</first><last>Gavrilidou</last></author>
      <author><first>Penny</first><last>Labropoulou</last></author>
      <author><first>Stelios</first><last>Piperidis</last></author>
      <author><first>Voula</first><last>Giouli</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Claudia</first><last>Soria</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <title>Language Resources Production Models: the Case of the <fixed-case>INTERA</fixed-case> Multilingual Corpus and Terminology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/534_pdf.pdf</url>
      <abstract>This paper reports on the multilingual Language Resources (MLRs), i.e. parallel corpora and terminological lexicons for less widely digitally available languages, that have been developed in the INTERA project and the methodology adopted for their production. Special emphasis is given to the reality factors that have influenced the MLRs development approach and their final constitution. Building on the experience gained in the project, a production model has been elaborated, suggesting ways and techniques that can be exploited in order to improve LRs production taking into account realistic issues.</abstract>
    </paper>
    <paper id="322">
      <author><first>Kyoko</first><last>Kanzaki</last></author>
      <author><first>Qing</first><last>Ma</last></author>
      <author><first>Eiko</first><last>Yamamoto</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>Semantic Analysis of Abstract Nouns to Compile a Thesaurus of Adjectives</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/535_pdf.pdf</url>
      <abstract>Aiming to compile a thesaurus of adjectives, we discuss how to extract abstract nouns categorizing adjectives, clarify the semantic and syntactic functions of these abstract nouns, and manually evaluate the capability to extract the instance-category relations. We focused on some Japanese syntactic structures and utilized possibility of omission of abstract noun to decide whether or not a semantic relation between an adjective and an abstract noun is an instance-category relation. For 63% of the adjectives (57 groups/90 groups) in our experiments, our extracted categories were found to be most suitable. For 22 % of the adjectives (20/90), the categories in the EDR lexicon were found to be most suitable. For 14% of the adjectives (13/90), neither our extracted categories nor those in EDR were found to be suitable, or examinees own categories were considered to be more suitable. From our experimental results, we found that the correspondence between a group of adjectives and their category name was more suitable in our method than in the EDR lexicon.</abstract>
    </paper>
    <paper id="323">
      <author><first>Katrin</first><last>Erk</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <title>Shalmaneser - A Toolchain For Shallow Semantic Parsing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/537_pdf.pdf</url>
      <abstract>This paper presents Shalmaneser, a software package for shallow semantic parsing, the automatic assignment of semantic classes and roles to free text. Shalmaneser is a toolchain of independent modules communicating through a common XML format. System output can be inspected graphically. Shalmaneser can be used either as a black box to obtain semantic parses for new datasets (classifiers for English and German frame-semantic analysis are included), or as a research platform that can be extended to new parsers, languages, or classification paradigms.</abstract>
    </paper>
    <paper id="324">
      <author><first>Valentin</first><last>Tablan</last></author>
      <author><first>Tamara</first><last>Polajnar</last></author>
      <author><first>Hamish</first><last>Cunningham</last></author>
      <author><first>Kalina</first><last>Bontcheva</last></author>
      <title>User-friendly ontology authoring using a controlled language</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/538_pdf.pdf</url>
      <abstract>In recent years, following the rapid development in the Semantic Web and Knowledge Management research, ontologies have become more in demand in Natural Language Processing. An increasing number of systems use ontologies either internally, for modelling the domain of the application, or as data structures that hold the output resulting from the work of the system, in the form of knowledge bases. While there are many ontology editing tools aimed at expert users, there are very few which are accessible to users wishing to create simple structures without delving into the intricacies of knowledge representation languages. The approach described in this paper allows users to create and edit ontologies simply by using a restricted version of the English language. The controlled language described within is based on an open vocabulary and a restricted set of grammatical constructs. Sentences written in this language unambiguously map into a number of knowledge representation formats including OWL and RDF-S to allow round-trip ontology management.</abstract>
    </paper>
    <paper id="325">
      <author><first>Laura</first><last>Hasler</last></author>
      <author><first>Constantin</first><last>Orasan</last></author>
      <author><first>Karin</first><last>Naumann</last></author>
      <title><fixed-case>NP</fixed-case>s for Events: Experiments in Coreference Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/539_pdf.pdf</url>
      <abstract>This paper describes a pilot project which developed a methodology for NP and event coreference annotation consisting of detailed annotation schemes and guidelines. In order to develop this, a small sample annotated corpus in the domain of terrorism/security was built. The methodology developed can be used as a basis for large-scale annotation to produce much-needed resources. In contrast to related projects, ours focused almost exclusively on the development of annotation guidelines and schemes, to ensure that future annotations based on this methodology capture the phenomena both reliably and in detail. The project also involved extensive discussions in order to redraft the guidelines, as well as major extensions to PALinkA, our existing annotation tool, to accommodate event as well as NP coreference annotation.</abstract>
    </paper>
    <paper id="326">
      <author><first>Amália</first><last>Mendes</last></author>
      <author><first>Sandra</first><last>Antunes</last></author>
      <author><first>Maria Fernanda Bacelar do</first><last>Nascimento</last></author>
      <author><first>João Miguel</first><last>Casteleiro</last></author>
      <author><first>Luísa</first><last>Pereira</last></author>
      <author><first>Tiago</first><last>Sá</last></author>
      <title><fixed-case>COMBINA</fixed-case>-<fixed-case>PT</fixed-case>: A Large Corpus-extracted and Hand-checked Lexical Database of <fixed-case>P</fixed-case>ortuguese Multiword Expressions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/540_pdf.pdf</url>
      <abstract>This paper presents the COMBINA-PT project, a study of corpus-extracted Portuguese Multiword (MW) expressions. The objective of this on-going project is to compile a large lexical database of multiword (MW) units of the Portuguese language, automatically extracted from a balanced 50 million word corpus, and manually validated with the help of lexical association measures. MW expressions considered in the database include named entities and lexical associations with different degrees of cohesion, ranging from frozen groups, which undergo little or no variation, to lexical collocations composed of words that tend to occur together and that constitute syntactic dependencies, although with a low degree of fixedness. This new resource has a two-fold objective: (i) to be an important research tool which supports the development of MW expressions typologies and their lexicographic treatment; (ii) to be of major help in developing and evaluating language processing tools able of dealing with MW expressions.</abstract>
    </paper>
    <paper id="327">
      <author><first>David</first><last>Graff</last></author>
      <author><first>Tim</first><last>Buckwalter</last></author>
      <author><first>Mohamed</first><last>Maamouri</last></author>
      <author><first>Hubert</first><last>Jin</last></author>
      <title>Lexicon Development for Varieties of Spoken Colloquial <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/541_pdf.pdf</url>
      <abstract>In Arabic speech communities, there is a diglossic gap between written/formal Modern Standard Arabic (MSA) and spoken/casual colloquial dialectal Arabic (DA): the common spoken language has no standard representation in written form, while the language observed in texts has limited occurrence in speech. Hence the task of developing language resources to describe and model DA speech involves extra work to establish conventions for orthography and grammatical analysis. We describe work being done at the LDC to develop lexicons for DA, comprising pronunciation, morphology and part-of-speech labeling for word forms in recorded speech. Components of the approach are: (a) a two-layer transcription, providing a consonant-skeleton form and a pronunciation form; (b) manual annotation of morphology, part-of-speech and English gloss, followed by development of automatic word parsers modeled on the Buckwalter Morphological Analyzer for MSA; (c) customized user interfaces and supporting tools for all stages of annotation; and (d) a relational database for storing, emending and publishing the transcription corpus as well as the lexicon.</abstract>
    </paper>
    <paper id="328">
      <author><first>Alexandre</first><last>Patry</last></author>
      <author><first>Fabrizio</first><last>Gotti</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <title><fixed-case>MOOD</fixed-case>: A Modular Object-Oriented Decoder for Statistical Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/542_pdf.pdf</url>
      <abstract>We present an Open Source framework called MOOD developed in order tofacilitate the development of a Statistical Machine Translation Decoder.MOOD has been modularized using an object-oriented approach which makes itespecially suitable for the fast development of state-of-the-art decoders. Asa proof of concept, a clone of the pharaoh decoder has been implemented andevaluated. This clone named ramses is part of the current distribution of MOOD.</abstract>
    </paper>
    <paper id="329">
      <author><first>Mohamed</first><last>Maamouri</last></author>
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Tim</first><last>Buckwalter</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <author><first>Dalila</first><last>Tabessi</last></author>
      <title>Developing and Using a Pilot Dialectal <fixed-case>A</fixed-case>rabic Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/543_pdf.pdf</url>
      <abstract>In this paper, we describe the methodological procedures and issues that emerged from the development of a pilot Levantine Arabic Treebank (LATB) at the Linguistic Data Consortium (LDC) and its use at the Johns Hopkins University (JHU) Center for Language and Speech Processing workshop on Parsing Arabic Dialects (PAD). This pilot, consisting of morphological and syntactic annotation of approximately 26,000 words of Levantine Arabic conversational telephone speech, was developed under severe time constraints; hence the LDC team drew on their experience in treebanking Modern Standard Arabic (MSA) text. The resulting Levantine dialect treebanked corpus was used by the PAD team to develop and evaluate parsers for Levantine dialect texts. The parsers were trained on MSA resources and adapted using dialect-MSA lexical resources (some developed especially for this task) and existing linguistic knowledge about syntactic differences between MSA and dialect. The use of the LATB for development and evaluation of syntactic parsers allowed the PAD team to provide feedbasck to the LDC treebank developers. In this paper, we describe the creation of resources for this corpus, as well as transformations on the corpus to eliminate speech effects and lessen the gap between our pre-existing MSA resources and the new dialectal corpus</abstract>
    </paper>
    <paper id="330">
      <author><first>Beáta Bandmann</first><last>Megyesi</last></author>
      <author><first>Anna Sågvall</first><last>Hein</last></author>
      <author><first>Éva Csató</first><last>Johanson</last></author>
      <title>Building a <fixed-case>S</fixed-case>wedish-<fixed-case>T</fixed-case>urkish Parallel Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/544_pdf.pdf</url>
      <abstract>We present a SwedishTurkish Parallel Corpus aimed to be used in linguistic research, teaching, and applications in natural language processing, primarily machine translation. The corpus being under development is built by using a Basic LAnguage Resource Kit (BLARK) for the two languages which is then used in the automatic alignment phase to improve alignment accuracy. The corpus is balanced with respect to source and target language and is automatically processed using the Uplug toolkit.</abstract>
    </paper>
    <paper id="331">
      <author><first>Horacio</first><last>Saggion</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <title>Language Resources for Background Gathering</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/545_pdf.pdf</url>
      <abstract>We describe the Cubreporter information access system which allows access to news archives through the use of natural language technology. The system includes advanced text search, question answering, summarization, and entity profiling capabilities. It has been designed taking into account the characteristics of the background gathering task.</abstract>
    </paper>
    <paper id="332">
      <author><first>Julie</first><last>Medero</last></author>
      <author><first>Kazuaki</first><last>Maeda</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Christopher</first><last>Walker</last></author>
      <title>An Efficient Approach to Gold-Standard Annotation: Decision Points for Complex Tasks</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/550_pdf.pdf</url>
      <abstract>Inter-annotator consistency is a concern for any corpus building effort relying on human annotation. Adjudication is as effective way to locate and correct discrepancies of various kinds. It can also be both difficult and time-consuming. This paper introduces Linguistic Data Consortium (LDC)s model for decision point-based annotation and adjudication, and describes the annotation tools developed to enable this approach for the Automatic Content Extraction (ACE) Program. Using a customized user interface incorporating decision points, we improved adjudication efficiency over 2004 annotation rates, despite increased annotation task complexity. We examine the factors that lead to more efficient, less demanding adjudication. We further discuss how a decision point model might be applied to annotation tools designed for a wide range of annotation tasks. Finally, we consider issues of annotation tool customization versus development time in the context of a decision point model.</abstract>
    </paper>
    <paper id="333">
      <author><first>Stefan</first><last>Klatt</last></author>
      <title>A Corpus-based Approach to the Interpretation of Unknown Words with an Application to <fixed-case>G</fixed-case>erman</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/552_pdf.pdf</url>
      <abstract>Usually a high portion of the different word forms in a corpusreceive no reading by the lexical and/or morphological analysis.These unknown words constitute a huge problem for NLP analysis tasks likePOS-tagging or syntactic parsing. We present a parameterizable (in principle language-independent) corpus-basedapproach for the interpretation of unknown words that only needs a tokenizedcorpus and can be used in both offline and online applications. In combination with a few linguistic (language-dependent) rules unknown verbs, adjectives, nouns, multiword units etc. are identified.Depending on the recognized word class(es), more detailed morphosyntactic and semantic information is additionally identified in opposite to the majority ofother unknown word guessing methods,which only uses a very narrow decision window to assign an unknown wordits correct reading respective Part-of-Speech tag in a given text. We tested our approach by experiments with German data and received very promising results.</abstract>
    </paper>
    <paper id="334">
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Sandra</first><last>Petel</last></author>
      <title>The Ritel Corpus - An annotated Human-Machine open-domain question answering spoken dialog corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/553_pdf.pdf</url>
      <abstract>In this paper we present a real (as opposed to Wizard-of-Oz) Human-Computer QA-oriented spoken dialog corpus collected with our Ritel platform. This corpus has been orthographically transcribed and annotated in terms of Specific Entities and Topics. Twelve main topics have been chosen. They are refined into 22 sub-topics. The Specific Entities are from five categories and cover Named Entities, linguistic entities, topic-defining entities, general entities and extended entities. The corpus contains 582 dialogs for 6 hours of user speech.</abstract>
    </paper>
    <paper id="335">
      <author><first>Anna</first><last>Feldman</last></author>
      <author><first>Jirka</first><last>Hana</last></author>
      <author><first>Chris</first><last>Brew</last></author>
      <title>A Cross-language Approach to Rapid Creation of New Morpho-syntactically Annotated Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/554_pdf.pdf</url>
      <abstract>We take a novel approach to rapid, low-cost development of morpho-syntactically annotated resources without using parallel corpora or bilingual lexicons. The overall research question is how to exploit language resources and properties to facilitate and automate the creation of morphologically annotated corpora for new languages. This portability issue is especially relevant to minority languages, for which such resources are likely to remain unavailable in the foreseeable future. We compare the performance of our system on languages that belong to different language families (Romance vs. Slavic), as well as different language pairs within the same language family (Portuguese via Spanish vs. Catalan via Spanish). We show that across language families, the most difficult category is the category of nominals (the noun homonymy is challenging for morphological analysis and the order variation of adjectives within a sentence makes it challenging to create a realiable model), whereas different language families present different challenges with respect to their morpho-syntactic descriptions: for the Slavic languages, case is the most challenging category; for the Romance languages, gender is more challenging than case. In addition, we present an alternative evaluation metric for our system, where we measure how much human labor will be needed to convert the result of our tagging to a high precision annotated resource.</abstract>
    </paper>
    <paper id="336">
      <author><first>Ionas</first><last>Michailidis</last></author>
      <author><first>Konstantinos</first><last>Diamantaras</last></author>
      <author><first>Spiros</first><last>Vasileiadis</last></author>
      <author><first>Yannick</first><last>Frère</last></author>
      <title><fixed-case>G</fixed-case>reek Named Entity Recognition using Support Vector Machines, Maximum Entropy and Onetime</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/557_pdf.pdf</url>
      <abstract>We describe our work on Greek Named Entity Recognition using comparatively three different machine learning techniques: (i) Support Vector Machines (SVM), (ii) Maximum Entropy and (iii) Onetime, a shortcut method based on previous work of one of the authors. The majority of our systems features use linguistic knowledge provided by: morphology, punctuation, position of the lexical units within a sentence and within a text, electronic dictionaries, and the outputs of external tools (a tokenizer, a sentence splitter, and a Hellenic version of Brills Part of Speech Tagger). After testing we observed that the application of a few simple Post Testing Classification Correction (PTCC) rules created after the observation of output errors, improved the results of the SVM and the Maximum Entropy systems output. We achieved very good results with the three methods. Our best configurations (Support Vector Machines with a second degree polynomial kernel and Maximum Entropy) achieved both after the application of PTCC rules an overall F-measure of 91.06.</abstract>
    </paper>
    <paper id="337">
      <author><first>Anna</first><last>Korhonen</last></author>
      <author><first>Yuval</first><last>Krymolowski</last></author>
      <author><first>Ted</first><last>Briscoe</last></author>
      <title>A Large Subcategorization Lexicon for Natural Language Processing Applications</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/558_pdf.pdf</url>
      <abstract>We introduce a large computational subcategorizationlexicon which includes subcategorization frame (SCF) and frequencyinformation for 6,397 English verbs. This extensive lexicon was acquiredautomatically from five corpora and the Web using the current version of the comprehensive subcategorization acquisition system of Briscoe and Carroll (1997). The lexicon is provided freely for research use, along with a script which can be used to filter and build sub-lexicons suited for different natural languageprocessing (NLP) purposes. Documentation is also provided whichexplains each sub-lexicon option and evaluates its accuracy.</abstract>
    </paper>
    <paper id="338">
      <author><first>Nancy</first><last>Ide</last></author>
      <author><first>Keith</first><last>Suderman</last></author>
      <title>Integrating Linguistic Resources: The <fixed-case>A</fixed-case>merican National Corpus Model</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/560_pdf.pdf</url>
      <abstract>This paper describes the architecture of the American National Corpus and the design decisions we have made in order to make the corpus easy to use with a variety of existing tools with varying functionality, and to allow for layering multiple annotations over the data. The overall goal of the ANC project is to provide an open linguistic infrastructure for American English, consisting of as many self-generated or contributed annotations of the data as possible together with derived. The availability of a wide variety of annotations for the same data and in a common format should significantly simplify the processing required to extract annotations from different sources and enable use of the ANC and its annotations with off-the-shelf software.</abstract>
    </paper>
    <paper id="339">
      <author><first>Nancy</first><last>Ide</last></author>
      <author><first>Laurent</first><last>Romary</last></author>
      <title>Representing Linguistic Corpora and Their Annotations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/562_pdf.pdf</url>
      <abstract>A Linguistic Annotation Framework (LAF) is being developed within the International Standards Organization Technical Committee 37 Sub-committee on Language Resource Management (ISO TC37 SC4). LAF is intended to provide a standardized means to represent linguistic data and its annotations that is defined broadly enough to accommodate all types of linguistic annotations, and at the same time provide means to represent precise and potentially complex linguistic information. The general principles informing the design of LAF have been previously reported (Ide and Romary, 2003; Ide and Romary, 2004a). This paper describes some of the more technical aspects of the LAF design that have been addressed in the process of finalizing the specifications for the standard.</abstract>
    </paper>
    <paper id="340">
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <author><first>Lei</first><last>Chen</last></author>
      <author><first>Mary</first><last>Harper</last></author>
      <title>An Open Source Prosodic Feature Extraction Tool</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/565_pdf.pdf</url>
      <abstract>There has been an increasing interest in utilizing a wide variety of knowledge sources in order to perform automatic tagging of speech events, such as sentence boundaries and dialogue acts. In addition to the word spoken, the prosodic content of the speech has been proved quite valuable in a variety of spoken language processing tasks such as sentence segmentation and tagging, disfluency detection, dialog act segmentation and tagging, and speaker recognition. In this paper, we report on an open source prosodic feature extraction tool based on Praat, with a description of the prosodic features and the implementation details, as well as a discussion of its extension capability. We also evaluate our tool on a sentence boundary detection task and report the system performance on the NIST RT04 CTS data.</abstract>
    </paper>
    <paper id="341">
      <author><first>Alina</first><last>Andreevskaia</last></author>
      <author><first>Sabine</first><last>Bergler</last></author>
      <title>Semantic Tag Extraction from <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et Glosses</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/566_pdf.pdf</url>
      <abstract>We propose a method that uses information from WordNet glosses to assign semantic tags to individual word meanings, rather than to entire words. The produced lists of annotated words will be used in sentiment annotation of texts and phrases and in other NLP tasks. The method was implemented in the Semantic Tag Extraction Program (STEP) and evaluated on the category of sentiment (positive, negative or neutral) using two human-annotated lists. The lists were first compared to each other and then used to assess the accuracy of the proposed system. We argue that significant disagreement on sentiment tags between the two human-annotated lists reflects a naturally occurring ambiguity of words located on the periphery of the category of sentiment. The category of sentiment, thus, is believed to be structured as a fuzzy set. Finally, we evaluate the generalizability of STEP to other semantic categories on the example of the category of words denoting increase/decrease in magnitude, intensity or quality of some state or process. The implications of this study for both semantic tagging system development and for performance evaluation practices are discussed.</abstract>
    </paper>
    <paper id="342">
      <author><first>Kow</first><last>Kuroda</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>Getting Deeper Semantics than <fixed-case>B</fixed-case>erkeley <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et with <fixed-case>MSFA</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/567_pdf.pdf</url>
      <abstract>This paper illustrates relevant details of an on-going semantic-role annotation work based on a framework called MULTILAYERED/DIMENSIONAL SEMANTIC FRAME ANALYSIS (MSFA for short) (Kuroda and Isahara, 2005b), which is inspired by, if not derived from, Frame Semantics/Berkeley FrameNet approach to semantic annotation (Lowe et al., 1997; Johnson and Fillmore, 2000).</abstract>
    </paper>
    <paper id="343">
      <author><first>Enrique</first><last>Alfonseca</last></author>
      <author><first>Antonio</first><last>Moreno-Sandoval</last></author>
      <author><first>José María</first><last>Guirao</last></author>
      <author><first>María</first><last>Ruiz-Casado</last></author>
      <title>The wraetlic <fixed-case>NLP</fixed-case> suite</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/569_pdf.pdf</url>
      <abstract>In this paper, we describe the second release of a suite of language analysers, developed over the last five years, called wraetlic, which includes tools for several partial parsing tasks, both for English and Spanish. It has been successfully used in fields such as Information Extraction, thesaurus acquisition, Text Summarisation and Computer Assisted Assessment.</abstract>
    </paper>
    <paper id="344">
      <author><first>Tomoko</first><last>Ohta</last></author>
      <author><first>Yuka</first><last>Tateisi</last></author>
      <author><first>Jin-Dong</first><last>Kim</last></author>
      <author><first>Akane</first><last>Yakushiji</last></author>
      <author><first>Jun-ichi</first><last>Tsujii</last></author>
      <title>Linguistic and Biological Annotations of Biological Interaction Events</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/570_pdf.pdf</url>
      <abstract>This paper discusses an augmentation of a corpus ofresearch abstracts in biomedical domain (the GENIA corpus) with two kinds of annotations: tree annotation and event annotation. The tree annotation identifies the linguistic structure that encodes the relations among entities. The event annotation reveals the semantic structure of the biological interaction events encoded in the text. With these annotations we aim to provide a link between the clue and the target of biological event information extraction.</abstract>
    </paper>
    <paper id="345">
      <author><first>Kari</first><last>Tenfjord</last></author>
      <author><first>Paul</first><last>Meurer</last></author>
      <author><first>Knut</first><last>Hofland</last></author>
      <title>The <fixed-case>ASK</fixed-case> Corpus - a Language Learner Corpus of <fixed-case>N</fixed-case>orwegian as a Second Language</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/573_pdf.pdf</url>
      <abstract>In our paper we present the design and interface of ASK, a language learner corpus of Norwegian as a second language which contains essays collected from language tests on two different proficiency levels as well as personal data from the test takers. In addition, the corpus also contains texts and relevant personal data from native Norwegians as control data. The texts as well as the personal data are marked up in XML according to the TEI Guidelines. In order to be able to classify errors in the texts, we have introduced new attributes to the TEI corr and sic tags. For each error tag, a correct form is also in the text annotation. Finally, we employ an automatic tagger developed for standard Norwegian, the Oslo-Bergen Tagger, together with a facility for manual tag correction. As corpus query system, we are using the Corpus Workbench developed at the University of Stuttgart together with a web search interface developed at Aksis, University of Bergen. The system allows for searching for combinations of words, error types, grammatical annotation and personal data.</abstract>
    </paper>
    <paper id="346">
      <author><first>Ivana</first><last>Kruijff-Korbayová</last></author>
      <author><first>Klára</first><last>Chvátalová</last></author>
      <author><first>Oana</first><last>Postolache</last></author>
      <title>Annotation Guidelines for <fixed-case>C</fixed-case>zech-<fixed-case>E</fixed-case>nglish Word Alignment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/575_pdf.pdf</url>
      <abstract>We report on our experience with manual alignment of Czech and English parallel corpus text. We applied existing guidelines for English and French and augmented them to cover systematically occurring cases in our corpus. We describe the main extensions covered in our guidelines and provide examples. We evaluated both intra- and inter-annotator agreement and obtained very good results of Kappa well above 0.9 and agreement of 95% and 93%, respectively.</abstract>
    </paper>
    <paper id="347">
      <author><first>Jérémie</first><last>Segouat</last></author>
      <author><first>Annelies</first><last>Braffort</last></author>
      <author><first>Emilie</first><last>Martin</last></author>
      <title>Sign Language corpus analysis: Synchronisation of linguistic annotation and numerical data</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/576_pdf.pdf</url>
      <abstract>This paper presents a study on synchronization of linguistic annotation and numerical data on a video corpus of French Sign Language. We detail the methodology and sketches out the potential observations that can be provided by such a kind of mixed annotation. The corpus is composed of three views: close-up, frontal and top. Some image processing has been performed on each video in order to provide global information on the movement of the signers. That consists of the size and position of a bounding box surrounding the signer. Linguists have studied this corpus and have provided annotations on iconic structures, such as "personal transfers" (role shifts). We used an annotation software, ANVIL, to synchronize linguistic annotation and numerical data. This new approach of annotation seems promising for automatic detection of linguistic phenomena, such as classification of the signs according to their size in the signing space, and detection of some iconic structures. Our first results must be consolidated and extended on the whole corpus. The next step will consist of designing automatic processes in order to assist SL annotation.</abstract>
    </paper>
    <paper id="348">
      <author><first>Gil</first><last>Francopoulo</last></author>
      <author><first>Monte</first><last>George</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Nuria</first><last>Bel</last></author>
      <author><first>Mandy</first><last>Pet</last></author>
      <author><first>Claudia</first><last>Soria</last></author>
      <title>Lexical Markup Framework (<fixed-case>LMF</fixed-case>)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/577_pdf.pdf</url>
      <abstract>Optimizing the production, maintenance and extension of lexical resources is one the crucial aspects impacting Natural Language Processing (NLP). A second aspect involves optimizing the process leading to their integration in applications. With this respect, we believe that the production of a consensual specification on lexicons can be a useful aid for the various NLP actors. Within ISO, the purpose of LMF is to define a standard for lexicons. LMF is a model that provides a common standardized framework for the construction of NLP lexicons. The goals of LMF are to provide a common model for the creation and use of lexical resources, to manage the exchange of data between and among these resources, and to enable the merging of large number of individual electronic resources to form extensive global electronic resources. In this paper, we describe the work in progress within the sub-group ISO-TC37/SC4/WG4. Various experts from a lot of countries have been consulted in order to take into account best practices in a lot of languages for (we hope) all kinds of NLP lexicons.</abstract>
    </paper>
    <paper id="349">
      <author><first>Bruno</first><last>Pouliquen</last></author>
      <author><first>Marco</first><last>Kimler</last></author>
      <author><first>Ralf</first><last>Steinberger</last></author>
      <author><first>Camelia</first><last>Ignat</last></author>
      <author><first>Tamara</first><last>Oellinger</last></author>
      <author><first>Ken</first><last>Blackler</last></author>
      <author><first>Flavio</first><last>Fluart</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Anna</first><last>Widiger</last></author>
      <author><first>Ann-Charlotte</first><last>Forslund</last></author>
      <author><first>Clive</first><last>Best</last></author>
      <title>Geocoding Multilingual Texts: Recognition, Disambiguation and Visualisation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/578_pdf.pdf</url>
      <abstract>We are presenting a method to recognise geographical references in free text. Our tool must work on various languages with a minimum of language-dependent resources, except a gazetteer. The main difficulty is to disambiguate these place names by distinguishing places from persons and by selecting the most likely place out of a list of homographic place names world-wide. The system uses a number of language-independent clues and heuristics to disambiguate place name homographs. The final aim is to index texts with the countries and cities they mention and to automatically visualise this information on geographical maps using various tools.</abstract>
    </paper>
    <paper id="350">
      <author><first>Bolette Sandford</first><last>Pedersen</last></author>
      <title>Query Expansion on Compounds</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/580_pdf.pdf</url>
      <abstract>Compounds constitute a specific issue in search, in particular in languages where they are written in one word, as is the case for Danish and the other Scandinavian languages. For such languages, expansion of the query compound into separate lemmas is a way of finding the often frequent alternative synonymous phrases in which the content of a compound can also be expressed. However, it is crucial to note that the number of irrelevant hits is generally very high when using this expansion strategy. The aim of this paper is to examine how we can obtain better search results on split compounds, partly by looking at the internal structure of the original compound, partly by analyzing the context in which the split compound occurs. We perform an NP analysis and introduce a new, linguistically based threshold for retrieved hits. The results obtained by using this strategy demonstrate that compound splitting combined with a shallow linguistic analysis focusing on the recognition of NPs can improve search by bringing down the number of irrelevant hits.</abstract>
    </paper>
    <paper id="351">
      <author><first>Thatsanee</first><last>Charoenporn</last></author>
      <author><first>Canasai</first><last>Kruengkrai</last></author>
      <author><first>Thanaruk</first><last>Theeramunkong</last></author>
      <author><first>Virach</first><last>Sornlertlamvanich</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>Word Knowledge Acquisition for Computational Lexicon Construction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/581_pdf.pdf</url>
      <abstract>The growing of multilingual information processing technology has created the need of linguistic resources, especially lexical database. Many attempts were put to alter the traditional dictionary to computational dictionary, or widely named as computational lexicon. TCLs Computational Lexicon (TCLLEX) is a recent development of a large-scale Thai Lexicon, which aims to serve as a fundamental linguistic resource for natural language processing research. We design either terminology or ontology for structuring the lexicon based on the idea of computability and reusability.</abstract>
    </paper>
    <paper id="352">
      <author><first>Wim</first><last>Peters</last></author>
      <author><first>Maria Teresa</first><last>Sagri</last></author>
      <author><first>Daniela</first><last>Tiscornia</last></author>
      <author><first>Sara</first><last>Castagnoli</last></author>
      <title>The <fixed-case>LOIS</fixed-case> Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/582_pdf.pdf</url>
      <abstract>The LOIS (Lexical Ontologies for legal Information Sharing) project The legal knowledge base resulting from the LOIS (Lexical Ontologies for legal Information Sharing) (Lexical Ontologies for legal Information Sharing) project consists of legal WordNets in six languages (Italian, Dutch, Portuguese, German, Czech, English). Its architecture is based on the EuroWordNet (EWN) framework (Vossen et al, 1997). Using the EWN framework assures compatibility of the LOIS WordNets with EWN, allowing them to function as an extension of EWN for the legal domain. For each legal system, the document-derived legal concepts are integrated into a taxonomy, which links into existing formal ontologies. These give the legal wordnets a first formal backbone, which can, in future, be further extended. The database consists of 33,000 synsets, and is aimed to be used in information retrieval, where it provides mono- and multi-lingual access to European legal databases for legal experts as well as for laymen. The LOIS knowledge base also provides a flexible, modular architecture that allows integration of multiple classification schemes, and enables the comparison of legal systems by exploring translation, equivalence and structure across the different legal wordnets.</abstract>
    </paper>
    <paper id="353">
      <author><first>Joris</first><last>Vaneyghen</last></author>
      <author><first>Guy</first><last>De Pauw</last></author>
      <author><first>Dirk</first><last>Van Compernolle</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <title>A mixed word / morphological approach for extending <fixed-case>CELEX</fixed-case> for high coverage on contemporary large corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/583_pdf.pdf</url>
      <abstract>This paper describes an alternative approach to morphological language modeling, which incorporates constraints on the morphological production of new words.This is done by applying the constraints as a preprocessing step in which only one morphological production rule can be applied to an extended lexicon of knownmorphemes, lemmas and word forms. This approach is used to extend the CELEX Dutch morphological database, so that a higher coverage can be reached on a largecorpus of Dutch newspaper articles. We present experimental results on the coverage of this extended database and use the extension to further evaluate our morphologicalsystem, as well as the impact of the constraints on the coverage of out-of-vocabulary words.</abstract>
    </paper>
    <paper id="354">
      <author><first>Adriana</first><last>Roventini</last></author>
      <title>Linking Verbal Entries of Different Lexical Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/584_pdf.pdf</url>
      <abstract>In the field of Computational Linguistics, many lexical resources have been developed which aim at encoding complex lexical semantic information according to different linguistic models (WordNet, Frame Semantics, Generative Lexicon, etc.). However, these resources are often not easily accessible nor available in their entirety. Yet, from the point of view of the continuous growth of technology (Semantic Web), their visibility, availability and integration are becoming of utmost importance. ItalWordNet and PAROLE/SIMPLE/CLIPS are two resources which, tackling lexical semantics from different perspectives and being at least partially complementary, can profit from linking each other. In this paper we address the issue of the linking of these resources focusing on the most problematic part of the lexicon: the second order entities. In particular, after a brief description of the two resources, their different approaches to the verb semantics are described; an accurate comparison of a set of verbal entries belonging to Speech Act semantic class is carried out aiming at evaluate the possibilities and the advantages of a semiautomatic link.</abstract>
    </paper>
    <paper id="355">
      <author id="olivier-hamon"><first>O.</first><last>Hamon</last></author>
      <author id="andrei-popescu-belis"><first>A.</first><last>Popescu-Belis</last></author>
      <author id="khalid-choukri"><first>K.</first><last>Choukri</last></author>
      <author id="marianne-dabbadie"><first>M.</first><last>Dabbadie</last></author>
      <author id="anthony-hartley"><first>A.</first><last>Hartley</last></author>
      <author id="widad-mustafa-el-hadi"><first>W.</first><last>Mustafa El Hadi</last></author>
      <author id="martin-rajman"><first>M.</first><last>Rajman</last></author>
      <author id="ismail-timimi"><first>I.</first><last>Timimi</last></author>
      <title><fixed-case>CESTA</fixed-case>: First Conclusions of the Technolangue <fixed-case>MT</fixed-case> Evaluation Campaign</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/586_pdf.pdf</url>
      <abstract>This article outlines the evaluation protocol and provides the main results of the French Evaluation Campaign for Machine Translation Systems, CESTA. Following the initial objectives and evaluation plans, the evaluation metrics are briefly described: along with fluency and adequacy assessed by human judges, a number of recently proposed automated metrics are used. Two evaluation campaigns were organized, the first one in the general domain, and the second one in the medical domain. Up to six systems translating from English into French, and two systems translating from Arabic into French, took part in the campaign. The numerical results illustrate the differences between classes of systems, and provide interesting indications about the reliability of the automated metrics for French as a target language, both by comparison to human judges and using correlations between metrics. The corpora that were produced, as well as the information about the reliability of metrics, constitute reusable resources for MT evaluation.</abstract>
    </paper>
    <paper id="356">
      <author><first>André Le</first><last>Meur</last></author>
      <author><first>Marie-Jeanne</first><last>Derouin</last></author>
      <title>Lemma-oriented dictionaries, concept-oriented terminology and translation memories</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/589_pdf.pdf</url>
      <abstract>Market surveys have pointed out translators demand for integrated specialist dictionaries in translation memory tools which they could use in addition to their own compiled dictionaries or stored translated parts of text. For this purpose the German specialist dictionary publisher, Langenscheidt Fachverlag in Munich has developed a method and tools together with experts from the University Rennes 2 in France and well known Translation Memory Providers. The conversion-tools of dictionary entries (lemma-oriented) in terminological entries (concept-oriented) are based on lexicographical and terminological ISO standards: ISO 1951 for dictionaries and ISO 16642 for terminology. The method relies on the analysis of polysemic structures into a set of data categories that can be recombined into monosemic entries compatible with most of the terminology management engines on the market. The whole process is based on the TermBridge semantic repository (http://www.genetrix.org ) for terminology and machine readable dictionaries and on a XML model LexTerm which is a subset of Geneter (ISO 16642 Annex C). It illustrates the interest for linguistic applications to define data elements in semantic repositories so that they are reusable in various contexts. This operation is fully integrated in the editorial XML workflow and applies to a series of specialist dictionaries which are now available.</abstract>
    </paper>
    <paper id="357">
      <author><first>Martí</first><last>Umbert</last></author>
      <author><first>Asunción</first><last>Moreno</last></author>
      <author><first>Pablo</first><last>Agüero</last></author>
      <author><first>Antonio</first><last>Bonafonte</last></author>
      <title><fixed-case>S</fixed-case>panish Synthesis Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/590_pdf.pdf</url>
      <abstract>This paper deals with the design of a synthesis database for a high quality corpus-based Speech Synthesis system in Spanish. The database has been designed for speech synthesis, speech conversion and expressive speech. The design follows the specifications of TC-STAR project and has been applied to collect equivalent English and Mandarin synthesis databases. The sentences of the corpus have been selected mainly from transcribed speech and novels. The selection criterion is a phonetic and prosodic coverage. The corpus was completed with sentences specifically designed to cover frequent phrases and words. Two baseline speakers and four bilingual speakers were recorded. Recordings consist of 10 hours of speech for each baseline speaker and one hour of speech for each voice conversion bilingual speaker. The database is labelled and segmented. Pitch marks and phonetic segmentation was done automatically and up to 50% manually supervised. The database will be available at ELRA.</abstract>
    </paper>
    <paper id="358">
      <author><first>Pavel</first><last>Ircing</last></author>
      <author><first>Jan</first><last>Hoidekr</last></author>
      <author><first>Josef</first><last>Psutka</last></author>
      <title>Exploiting Linguistic Knowledge in Language Modeling of <fixed-case>C</fixed-case>zech Spontaneous Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/591_pdf.pdf</url>
      <abstract>In our paper, we present a method for incorporating available linguistic information into a statistical language model that is used in ASR system for transcribing spontaneous speech. We employ the class-based language model paradigm and use the morphological tags as the basis for world-to-class mapping. Since the number of different tags is at least by one order of magnitude lower than the number of words even in the tasks with moderately-sized vocabularies, the tag-based model can be rather robustly estimated using even the relatively small text corpora. Unfortunately, this robustness goes hand in hand with restricted predictive ability of the class-based model. Hence we apply the two-pass recognition strategy, where the first pass is performed with the standard word-based n-gram and the resulting lattices are rescored in the second pass using the aforementioned class-based model. Using this decoding scenario, we have managed to moderately improve the word error rate in the performed ASR experiments.</abstract>
    </paper>
    <paper id="359">
      <author><first>Milena</first><last>Slavcheva</last></author>
      <title>Semantic Descriptors: The Case of Reflexive Verbs</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/593_pdf.pdf</url>
      <abstract>This paper presents a semantic classification of reflexive verbs in Bulgarian, augmenting the morphosyntactic classes of verbs in the large Bulgarian Lexical Data Base - a language resource utilized in a number of Language Engineering (LE) applications. Thesemantic descriptors conform to the Unified Eventity Representation (UER), developed by Andrea Schalley. The UER is a graphical formalism, introducing the object-oriented system design to linguistic semantics. Reflexive/non-reflexive verb pairs are analyzed where the non-reflexive member of the opposition, a two-place predicate, is considered the initial linguistic entity from which the reflexive correlate is derived. The reflexive verbs are distributed into initial syntactic-semantic classes which serve as the basis for defining the relevant semantic descriptors in the form of EVENTITY FRAME diagrams. The factors that influence the categorization of the reflexives are the lexical paradigmaticapproach to the data, the choice of only one reading for each verb, top level generalization of the semantic descriptors. The language models described in this paper provide the possibility for building linguistic components utilizable in knowledge-driven systems.</abstract>
    </paper>
    <paper id="360">
      <author><first>Günter</first><last>Neumann</last></author>
      <author><first>Berthold</first><last>Crysmann</last></author>
      <title>Exploring <fixed-case>HPSG</fixed-case>-based Treebanks for Probabilistic Parsing <fixed-case>HPSG</fixed-case> grammar extraction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/595_pdf.pdf</url>
      <abstract>We describe a method for the automatic extraction of a Stochastic Lexicalized Tree Insertion Grammar from a linguistically rich HPSG Treebank. The extraction method is strongly guided by HPSG-based head and argument decomposition rules. The tree anchors correspond to lexical labels encoding fine-grained information. The approach has been tested with a German corpus achieving a labeled recall of 77.33% and labeled precision of 78.27%, which is competitive to recent results reported for German parsing using the Negra Treebank.</abstract>
    </paper>
    <paper id="361">
      <author><first>Rita</first><last>Marinelli</last></author>
      <author><first>Remo</first><last>Bindi</last></author>
      <title>Proper Names and Linguistic Dynamics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/596_pdf.pdf</url>
      <abstract>Pragmatics is the study of how people exchange meanings through the use of language. In this paper we describe our experience with regard to texts belonging to a large contemporary corpus of written language, in order to verify the uses, changes and flexibility of the meaning of Proper Names (PN). As a matter of fact, while building the lexical semantic database ItalWordNet (IWN), a considerable set of PN (up to now, about 4,000) has been inserted and studied. We give prominence to the polysemy of PN and their shifting or moving from one class to another as an example of the extensibility of language and the possibility of change considering meaning as a dynamic process. Many examples of the sense shifting phenomenon can be evidenced by textual corpora. By comparing the percentages regarding the texts belonging to two different periods of time, an increasing use of the PN with sense extension has been verified. This evidence could confirm the tendency to consider the derived or extended senses as more salient and prevailing on the base senses, confirming a gradual fixation of meaning during the time. The object of our study (in progress) is to observe the uses of sense extensions also examining in detail freshly coined examples and taking into account their relationship with meta representational capacity and human creativity and the ways in which linguistic dynamics can activate the meaning potential of the words.</abstract>
    </paper>
    <paper id="362">
      <author><first>Sonja E.</first><last>Bosch</last></author>
      <author><first>Laurette</first><last>Pretorius</last></author>
      <author><first>Jackie</first><last>Jones</last></author>
      <title>Towards machine-readable lexicons for <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>frican <fixed-case>B</fixed-case>antu languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/597_pdf.pdf</url>
      <abstract>Lexical information for South African Bantu languages is not readily available in the form of machine-readable lexicons. At present the availability of lexical information is restricted to a variety of paper dictionaries. These dictionaries display considerable diversity in the organisation and representation of data. In order to proceed towards the development of reusable and suitably standardised machine-readable lexicons for these languages, a data model for lexical entries becomes a prerequisite. In this study the general purpose model as developed by Bell &amp; Bird (2000) is used as a point of departure. Firstly, the extent to which the Bell &amp; Bird (2000) data model may be applied to and modified for the above-mentioned languages is investigated. Initial investigations indicate that modification of this data model is necessary to make provision for the specific requirements of lexical entries in these languages. Secondly, a data model in the form of an XML DTD for the languages in question, based on our findings regarding (Bell &amp; Bird, 2000) and (Weber, 2002) is presented. Included in this model are additional particular requirements for complete and appropriate representation of linguistic information as identified in the study of available paper dictionnaries.</abstract>
    </paper>
    <paper id="363">
      <author id="gilles-lechenadec"><last>Lechenadec</last><first>G.</first></author>
      <author id="valerie-maffiolo"><last>Maffiolo</last><first>V.</first></author>
      <author id="noel-chateau"><last>Chateau</last><first>N.</first></author>
      <author id="jean-marc-colletta"><last>Colletta</last><first>J.M.</first></author>
      <title>Creation of a corpus of multimodal spontaneous expressions of emotions in Human-Machine Interaction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/599_pdf.pdf</url>
      <abstract>This paper presents an experience in laboratory dealing with the constitution of a corpus of multimodal spontaneous expressions of emotions. The originality of this corpus resides in its characteristics (interactions between a virtual actor and humans learning a theater text), in its content (multimodal spontaneous expressions of emotions) and in its two sources of characterization (by the participant and by one of his/her close relation). The corpus collection is part of a study on the fusion of multimodal information (verbal, facial, gestural, postural, and physiological) to improve the detection and characterization of expressions of emotions in human-machine interaction (HMI).</abstract>
    </paper>
    <paper id="364">
      <author><first>Yoshihiko</first><last>Hayashi</last></author>
      <author><first>Toru</first><last>Ishida</last></author>
      <title>A Dictionary Model for Unifying Machine Readable Dictionaries and Computational Concept Lexicons</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/600_pdf.pdf</url>
      <abstract>The Language Grid, recently proposed by one of the authors, is a language infrastructure available on the Internet. It aims to resolve the problems of accessibility and usability inherent in the currently available language services. The infrastructure will accommodate an operational environment in which a user and/or a software agent can develop a language service that is tailored to specific requirements derived from the various situations of intercultural communication. In order to effectively operate the infrastructure, each atomic language service has to be discovered by the planner of a composite service and incorporated into the composite service scenario. Meta-description of an atomic service is crucial to accomplish the planning process. This paper focuses on dictionary access services and proposes an abstract dictionary model that is vital for the accurate meta-description of such a service. In principle, the proposed model is based on the organization compatible with Princeton WordNet. Computational lexicons, including the EDR dictionary, as well as a range of human monolingual/bilingual dictionaries are uniformly organized into a WordNet-like lexical concept system. A modeling example with a few dictionary instances demonstrates the fundamental validity of the model.</abstract>
    </paper>
    <paper id="365">
      <author><first>Rita</first><last>Marinelli</last></author>
      <author><first>Adriana</first><last>Roventini</last></author>
      <author><first>Giovanni</first><last>Spadoni</last></author>
      <title>Using Core Ontology for Domain Lexicon Structuring</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/602_pdf.pdf</url>
      <abstract>The users demand has determined the need to manage the growing new technical maritime terminology which includes very different domains such as the juridical or commercial ones. A terminological database was built by exploiting the computational tools of ItalWordNet (IWN) and its lexical-semantic model (EuroWordNet).This paper concerns the development of database structure and data coding, relevance of the concepts of term and domain, information potential of the terms, complexity of this domain and detailed ontology structuring recently undertaken and still in progress. Our domain structure is described defining a core set of terms representing the two main sub-domains specified in technical-nautical and maritime transport terminology. These terms are sufficiently general to be the root nodes of the core ontology we are developing. They are mostly domain-dependent, but the link with the Top Ontology of IWN remains, endorsing either general and foundation information, or detailed description directly connected with the specific domain. Through the semantic relations linking the synsets, every term inherits the top ontology definitions and becomes itself an integral part of the structure. While codifying a term in the maritime database, the reference is at the same time allowed to the Base Concepts of the terminological ontology embedding the term in the semantic network, showing that upper and core ontologies make it possible for the framework to integrate different views on the same domain in a meaningful way.</abstract>
    </paper>
    <paper id="366">
      <author><first>Witold</first><last>Abramowicz</last></author>
      <author><first>Agata</first><last>Filipowska</last></author>
      <author><first>Jakub</first><last>Piskorski</last></author>
      <author><first>Krzysztof</first><last>Węcel</last></author>
      <author><first>Karol</first><last>Wieloch</last></author>
      <title>Linguistic Suite for <fixed-case>P</fixed-case>olish Cadastral System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/603_pdf.pdf</url>
      <abstract>This paper reports on an endeavour of creating basic linguistic resources for geo-referencing of Polish free-text documents. We have defined a fine-grained named entity hierarchy, produced an exhaustive gazetteer, and developed named-entity grammars for Polish. Additionally, an annotated corpus for the cadastral domain was prepared for evaluation purposes. Our baseline approach to geo-referencing is based on application of aforementioned resources and a lightweight co-referencing technique which utilizes string-similarity metric of Jaro-Winkler. We carried out a detailed evaluation of detecting locations, organizations and persons, which revealed that best results are obtained via application of a combined grammar for all types. The application of lightweight co-referencing for organizations and persons improves recall but deteriorates precision, and no gain is observed for locations. The paper is accompanied by a demo, a geo-referencing application capable of: (a) finding documents and text fragments based on named entities and (b) populating the spatial ontology from texts.</abstract>
    </paper>
    <paper id="367">
      <author><first>Hiromichi</first><last>Kawanami</last></author>
      <author><first>Takahiro</first><last>Kitamura</last></author>
      <author><first>Kiyohiro</first><last>Shikano</last></author>
      <title>Long-term Analysis of Prosodic Features of Spoken Guidance System User Speech</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/604_pdf.pdf</url>
      <abstract>As a practical information guidance system, we have been developing a speech-oriented system named "Takemaru-kun". The system has been operated on a public space since Nov. 2002. The system answers to user's question about the hall facilities, sightseeing, transportation, weather information around the city, etc. All triggered inputs to the system have been recorded since the operation started. And all system inputs during 22 months are manually transcribed and labelled for speakers gender and age category. In this paper, we conduct a long-term prosody analysis of user speech to find a clue to obtain users attitude from a users speech. In this preliminary analysis, it is observed that F0 decreases regardless of age and gender category when the stability of the dialogue system is not established.</abstract>
    </paper>
    <paper id="368">
      <author><first>Vít</first><last>Nováček</last></author>
      <author><first>Pavel</first><last>Smrž</last></author>
      <author><first>Jan</first><last>Pomikálek</last></author>
      <title>Text Mining for Semantic Relations as a Support Base of a Scientific Portal Generator</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/606_pdf.pdf</url>
      <abstract>Current Semantic Web implementation efforts pose a number of challenges. One of the big ones among them is development and evolution of specific resources --- the ontologies --- as a base for representation of the meaning of the web. This paper deals with the automatic acquisition of semantic relations from the text of scientific publications (journal articles, conference papers, project descriptions, etc.). We also describe the process of building of corresponding ontological resources and their application for semi--automatic generation of scientific portals. Extracted relations and ontologies are crucial for the structuring of the information at the portal pages, automatic classification of the presented documents as well as for personalisation at the presentation level. Besides a general description of the portal generating system, we give also a detailed overview of extraction of semantic relations in the form of a domain--specific ontology. The overview consists of presentation of an architecture of the ontology extraction system, description of methods used for mining of semantic relations and analysis of selected results and examples.</abstract>
    </paper>
    <paper id="369">
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <author><first>Andrea</first><last>Bolognesi</last></author>
      <author><first>Corrado</first><last>Seidenari</last></author>
      <author><first>Fabio</first><last>Tamburini</last></author>
      <title><fixed-case>POS</fixed-case> tagset design for <fixed-case>I</fixed-case>talian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/608_pdf.pdf</url>
      <abstract>We aim to automatically induce a PoS tagset for Italian by analysing the distributional behaviour of Italian words. To this end, we propose an algorithm that (a) extracts information from loosely labelled dependency structures that encode only basic and broadly accepted syntactic relations, namely Head/Dependent and the distinction of dependents into Argument vs. Adjunct, and (b) derives a possible set of word classes. The paper reports on some preliminary experiments carried out using the induced tagset in conjunction with state-of-the-art PoS taggers. The method proposed to design a proper tagset exploits little, if any, language-specific knowledge: hence it is in principle applicable to any language.</abstract>
    </paper>
    <paper id="370">
      <author><first>Kentaro</first><last>Inui</last></author>
      <author><first>Toru</first><last>Hirano</last></author>
      <author><first>Ryu</first><last>Iida</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <title>Augmenting a Semantic Verb Lexicon with a Large Scale Collection of Example Sentences</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/610_pdf.pdf</url>
      <abstract>One of the crucial issues in semantic parsing is how to reduce costs of collecting a sufficiently large amount of labeled data. This paper presents a new approach to cost-saving annotation of example sentences with predicate-argument structure information, taking Japanese as a target language. In this scheme, a large collection of unlabeled examples are first clustered and selectively sampled, and for each sampled cluster, only one representative example is given a label by a human annotator. The advantages of this approach are empirically supported by the results of our preliminary experiments, where we use an existing similarity function and naive sampling strategy.</abstract>
    </paper>
    <paper id="371">
      <author id="corinna-onelli"><first>C.</first><last>Onelli</last></author>
      <author id="domenico-proietti"><first>D.</first><last>Proietti</last></author>
      <author id="corrado-seidenari"><first>C.</first><last>Seidenari</last></author>
      <author id="fabio-tamburini"><first>F.</first><last>Tamburini</last></author>
      <title>The <fixed-case>D</fixed-case>ia<fixed-case>CORIS</fixed-case> project: a diachronic corpus of written <fixed-case>I</fixed-case>talian</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/611_pdf.pdf</url>
      <abstract>The DiaCORIS project aims at the construction of a diachronic corpus comprising written Italian texts produced between 1861 and 1945, extending the structure and the research possibilities of the synchronic 100-million word corpus CORIS/CODIS. A preliminary in depth study has been performed in order to design a representative and well balanced sample of the Italian language over a time period that contains all the main events of contemporary Italian history from the National Unification to the end of the Second World War. The paper describes in detail such design processes as the definition of the main subcorpora and their proportions, the type of documents inserted in each part of the corpus, the document annotation schema and the technological infrastructure designed to manage the corpus access as well as the web interface to corpus data.</abstract>
    </paper>
    <paper id="372">
      <author><first>Eneko</first><last>Agirre</last></author>
      <author><first>Izaskun</first><last>Aldezabal</last></author>
      <author><first>Jone</first><last>Etxeberria</last></author>
      <author><first>Eli</first><last>Pociello</last></author>
      <title>A Preliminary Study for Building the <fixed-case>B</fixed-case>asque <fixed-case>P</fixed-case>rop<fixed-case>B</fixed-case>ank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/612_pdf.pdf</url>
      <abstract>This paper presents a methodology for adding a layer of semantic annotation to a syntactically annotated corpus of Basque (EPEC), in terms of semantic roles. The proposal we make here is the combination of three resources: the model used in the PropBank project (Palmer et al., 2005), an in-house database with syntactic/semantic subcategorization frames for Basque verbs (Aldezabal, 2004) and the Basque dependency treebank (Aduriz et al., 2003). In order to validate the methodology and to confirm whether the PropBank model is suitable for Basque and our treebank design, we have built lexical entries and labelled all argument and adjuncts occurring in our treebank for 3 Basque verbs. The result of this study has been very positive, and has produced a methodology adapted to the characteristics of the language and the Basque dependency treebank. Another goal of this study was to study whether semi-automatic tagging was possible. The idea is to present the human taggers a pre-tagged version of the corpus. We have seen that many arguments could be automatically tagged with high precision, given only the verbal entries for the verbs and a handful of examples.</abstract>
    </paper>
    <paper id="373">
      <author><first>Eiko</first><last>Yamamoto</last></author>
      <author><first>Kyoko</first><last>Kanzaki</last></author>
      <author><first>Hitoshi</first><last>Isahara</last></author>
      <title>Detection of inconsistencies in concept classifications in a large dictionary — Toward an improvement of the <fixed-case>EDR</fixed-case> electronic dictionary —</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/613_pdf.pdf</url>
      <abstract>The EDR electronic dictionary is a machine-tractable dictionary developed for advanced computer-based processing of natural lan-guage. This dictionary comprises eleven sub-dictionaries, including a concept dictionary, word dictionaries, bilingual dictionaries, co-occurrence dictionaries, and a technical terminology dictionary. In this study, we focus on the concept dictionary and aim to revise the arrangement of concepts for improving the EDR electronic dictionary. We believe that unsuitable concepts in a class differ from other concepts in the same class from an abstract perspective. From this notion, we first try to automatically extract those concepts unsuited to the class. We then try semi-automatically to amend the concept explications used to explain the meanings to human users and rearrange them in suitable classes. In the experiment, we try to revise those concepts that are the lower-concepts of the concept human in the concept hierarchy and that are directly arranged under concepts with concept explications such as person as defined by  and person viewed from . We analyze the result and evaluate our approach.</abstract>
    </paper>
    <paper id="374">
      <author><first>Eneko</first><last>Agirre</last></author>
      <author><first>Izaskun</first><last>Aldezabal</last></author>
      <author><first>Jone</first><last>Etxeberria</last></author>
      <author><first>Eli</first><last>Izagirre</last></author>
      <author><first>Karmele</first><last>Mendizabal</last></author>
      <author><first>Eli</first><last>Pociello</last></author>
      <author><first>Mikel</first><last>Quintian</last></author>
      <title>A methodology for the joint development of the <fixed-case>B</fixed-case>asque <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et and Semcor</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/614_pdf.pdf</url>
      <abstract>This paper describes the methodology adopted to jointly develop the Basque WordNet and a hand annotated corpora (the Basque Semcor). This joint development allows for better motivated sense distinctions, and a tighter coupling between both resources. The methodology involves edition, tagging and refereeing tasks. We are currently half way through the nominal part of the 300.000 word corpus (roughly equivalent to a 500.000 word corpus for English). We present a detailed description of the task, including the main criteria for difficult cases in the edition of the senses and the tagging of the corpus, with special mention to multiword entries. Finally we give a detailed picture of the current figures, as well as an analysis of the agreement rates.</abstract>
    </paper>
    <paper id="375">
      <author><first>Jan</first><last>Hoidekr</last></author>
      <author id="josef-psutka"><first>J.V.</first><last>Psutka</last></author>
      <author><first>Aleš</first><last>Pražák</last></author>
      <author><first>Josef</first><last>Psutka</last></author>
      <title>Benefit of a Class-based Language Model for Real-time Closed-captioning of <fixed-case>TV</fixed-case> Ice-hockey Commentaries</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/615_pdf.pdf</url>
      <abstract>This article describes the real-time speech recognition system for closed-captioning of TV ice-hockey commentaries. Automatic transcription of TV commentary accompanying an ice-hockey match is usually a hard task due to the spontaneous speech of a commentator put often into a very loud background noise created by the public, music, siren, drums, whistle, etc. Data for building this system was collected from 41 matches that were played during World Championships in years 2000, 2001, and 2002 and were transmitted by the Czech TV channels. The real-time closed-captioning system is based on the class-based language model designed after careful analysis of training data and OOV words in new (till now unseen) commentaries with the goal to decrease an OOV (Out-Of-Vocabulary) rate and increase recognition accuracy.</abstract>
    </paper>
    <paper id="376">
      <author><first>Stefanie</first><last>Anstein</last></author>
      <author><first>Gerhard</first><last>Kremer</last></author>
      <author><first>Uwe</first><last>Reyle</last></author>
      <title>Identifying and Classifying Terms in the Life Sciences: The Case of Chemical Terminology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/617_pdf.pdf</url>
      <abstract>Facing the huge amount of textual and terminological data in the life sciences, we present a theoretical basis for the linguistic analysis of chemical terms. Starting with organic compound names, we conduct a morpho-semantic deconstruction into morphemes and yield a semantic representation of the terms' functional and structural properties. These semantic representations imply both the molecular structure of the named molecules and their class membership. A crucial feature of this analysis, which distinguishes it from all similar existing systems, is its ability to deal with terms that do not fully specify a structure as well as terms for generic classes of chemical compounds. Such `underspecified' terms occur very frequently in scientific literature. Our approach will serve for the support of manual database curation and as a basis for text processing applications.</abstract>
    </paper>
    <paper id="377">
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <title>Conceptual Vector Learning - Comparing Bootstrapping from a Thesaurus or Induction by Emergence</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/618_pdf.pdf</url>
      <abstract>In the framework of the Word Sense Disambiguation (WSD) and lexical transfer in Machine Translation (MT), the representation of word meanings is one critical issue. The conceptual vector model aims at representing thematic activations for chunks of text, lexical entries, up to whole documents. Roughly speaking, vectors are supposed to encode ideas associated to words or expressions. In this paper, we first expose the conceptual vectors model and the notions of semantic distance and contextualization between terms. Then, we present in details the text analysis process coupled with conceptual vectors, which is used in text classification, thematic analysis and vector learning. The question we focus on is whether a thesaurus is really needed and desirable for bootstrapping the learning. We conducted two experiments with and without a thesaurus and are exposing here some comparative results. Our contribution is that dimension distribution is done more regularly by an emergent procedure. In other words, the resources are more efficiently exploited with an emergent procedure than with a thesaurus terms (concepts) as listed in a thesaurus somehow relate to their importance in the language but nor to their frequency in usage neither to their power of discrimination or representativeness.</abstract>
    </paper>
    <paper id="378">
      <author><first>Ernesto William</first><last>De Luca</last></author>
      <author><first>Andreas</first><last>Nürnberger</last></author>
      <title>Rebuilding Lexical Resources for Information Retrieval using Sense Folder Detection and Merging Methods</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/620_pdf.pdf</url>
      <abstract>In this paper we discuss the problem of sense disambiguation using lexical resources like ontologies or thesauri with a focus on the application of sense detection and merging methods in information retrieval systems. For an information retrieval task it is important to detect the meaning of a query word for retrieving the related relevant documents. In order to recognize the meaning of a search word, lexical resources, like WordNet, can be used for word sense disambiguation. But, analyzing the WordNet structure, we see that this ontology is fraught with different problems. The too fine grained distinction between word senses, for example, is unfavorable for a usage in information retrieval. We describe related problems and present four implemented online methods to merge SynSets based on relations like hypernyms and hyponyms, and further context information like glosses and domain. Afterwards we show a first evaluation of our approach, compare the different merging methods and discuss briefly future work.</abstract>
    </paper>
    <paper id="379">
      <author><first>Pavel</first><last>Smrž</last></author>
      <title>Automatic Acquisition of Semantics-Extraction Patterns</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/621_pdf.pdf</url>
      <abstract>This paper examines the use of parallel and comparable corpora for automatic acquisition of semantics-extraction patterns. It presents a new method of the pattern extraction which takes advantage of parallel texts to "port" text mining solutions from a source language to a target language. It is shown thatthe technique can help in situations when the extraction procedure is to beapplied in a language (languages) with a limited set of available resources,e.g. domain-specific thesauri. The primary motivation of our work lies in a particular multilingual e-learning system. For testing purposes, other applications of the given approach were implemented. They include pattern extraction from general texts (tested on wordnet relations), acquisition of domain-specific patterns from large parallel corpus of legal EU documents, and mining of subjectivity expressions for multilingual opinion extraction system.</abstract>
    </paper>
    <paper id="380">
      <author><first>Lea</first><last>Cyrus</last></author>
      <title>Building a resource for studying translation shifts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/622_pdf.pdf</url>
      <abstract>This paper describes an interdisciplinary approach which brings together the fields of corpus linguistics and translation studies. It presents ongoing work on the creation of a corpus resource in which translation shifts are explicitly annotated. Translation shifts denote departures from formal correspondence between source and target text, i.e. deviations that have occurred during the translation process. A resource in which such shifts are annotated in a systematic way will make it possible to study those phenomena that need to be addressed if machine translation output is to resemble human translation. The resource described in this paper contains English source texts (parliamentary proceedings) and their German translations. The shift annotation is based on predicate-argument structures and proceeds in two steps: first, predicates and their arguments are annotated monolingually in a straightforward manner. Then, the corresponding English and German predicates and arguments are aligned with each other. Whenever a shift - mainly grammatical or semantic - has occurred, the alignment is tagged accordingly.</abstract>
    </paper>
    <paper id="381">
      <author><first>Christoph</first><last>Draxler</last></author>
      <author><first>Klaus</first><last>Jänsch</last></author>
      <title>Speech Recordings in Public Schools in <fixed-case>G</fixed-case>ermany - the Perfect Show Case for Web-based Recordings and Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/623_pdf.pdf</url>
      <abstract>In the Ph@ttSessionz project, geographically distributed high-bandwidth recordings of adolescent speakers are performed in public schools all over Germany. To achieve a consistent technical signal quality, a standard configuration of recording equipment is sent to the participating schools. The recordings are made using the SpeechRecorder software for prompted speech recordings via the WWW. During a recording session, prompts are downloaded from a server, and the speech data is uploaded to the server in a background process. This paper focuses on the technical aspects of the distributed Ph@ttSessionz speech recordings and their annotation.</abstract>
    </paper>
    <paper id="382">
      <author><first>Benjamin K.</first><last>Tsou</last></author>
      <author><first>Tom B.Y.</first><last>Lai</last></author>
      <author id="king-kui-sin"><first>K.K.</first><last>Sin</last></author>
      <author><first>Lawrence Y.L.</first><last>Cheung</last></author>
      <title>Court Stenography-To-Text (“<fixed-case>STT</fixed-case>”) in <fixed-case>H</fixed-case>ong <fixed-case>K</fixed-case>ong: A Jurilinguistic Engineering Effort</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/624_pdf.pdf</url>
      <abstract>Implementation of legal bilingualism in Hong Kong after 1997 has necessitated the production of voluminous and extensive court proceedings and judgments in both Chinese and English. For the former, Cantonese, a dialect of Chinese, is the home language of more than 90% of the population in Hong Kong and so used in the courts. To record speech in Cantonese verbatim, a Chinese Computer-Aided Transcription system has been developed. The transcription system converts stenographic codes into Chinese text, i.e. from phonetic to orthographic representation of the language. The main challenge lies in the resolution of the sever ambiguity resulting from homocode problems in the conversion process. Cantonese Chinese is typified by problematic homonymy, which presents serious challenges. The N-gram statistical model is employed to estimate the most probable character string of the input transcription codes. Domain-specific corpora have been compiled to support the statistical computation. To improve accuracy, scalable techniques such as domain-specific transcription and special encoding are used. Put together, these techniques deliver 96% transcription accuracy.</abstract>
    </paper>
    <paper id="383">
      <author><first>Dorothee</first><last>Beermann</last></author>
      <author><first>Lars</first><last>Hellan</last></author>
      <title>Word Sense Disambiguation and Semantic Disambiguation for Construction Types in Deep Processing Grammars</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/625_pdf.pdf</url>
      <abstract>The paper presents advances in the use of semantic features and interlingua relations for word sense disambiguation (WSD) as part of unification-based deep processing grammars. Formally we present an extension of Minimal Recursion Semantics, introducing sortal specifications as well as linterlingua semantic relations as a means of semantic decomposition.</abstract>
    </paper>
    <paper id="384">
      <author><first>Dennis</first><last>Reidsma</last></author>
      <author><first>Dirk</first><last>Heylen</last></author>
      <author><first>Roeland</first><last>Ordelman</last></author>
      <title>Annotating Emotions in Meetings</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/626_pdf.pdf</url>
      <abstract>We present the results of two trials testing procedures for the annotation of emotion and mental state of the AMI corpus. The first procedure is an adaptation of the FeelTrace method, focusing on a continuous labelling of emotion dimensions. The second method is centered around more discrete labeling of segments using categorical labels. The results reported are promising for this hard task.</abstract>
    </paper>
    <paper id="385">
      <author id="helene-bonneau-maynard"><first>H.</first><last>Bonneau-Maynard</last></author>
      <author id="christelle-ayache"><first>C.</first><last>Ayache</last></author>
      <author id="frederic-bechet"><first>F.</first><last>Bechet</last></author>
      <author id="alexandre-denis"><first>A.</first><last>Denis</last></author>
      <author id="anne-kuhn"><first>A.</first><last>Kuhn</last></author>
      <author id="fabrice-lefevre"><first>F.</first><last>Lefevre</last></author>
      <author id="djamel-mostefa"><first>D.</first><last>Mostefa</last></author>
      <author id="matthieu-quignard"><first>M.</first><last>Quignard</last></author>
      <author id="sophie-rosset"><first>S.</first><last>Rosset</last></author>
      <author id="christophe-servan"><first>C.</first><last>Servan</last></author>
      <author id="jeanne-villaneau"><first>J.</first><last>Villaneau</last></author>
      <title>Results of the <fixed-case>F</fixed-case>rench Evalda-Media evaluation campaign for literal understanding</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/627_pdf.pdf</url>
      <abstract>The aim of the Media-Evalda project is to evaluate the understanding capabilities of dialog systems. This paper presents the Media protocol for speech understanding evaluation and describes the results of the June 2005 literal evaluation campaign. Five systems, both symbolic or corpus-based, participated to the evaluation which is based on a common semantic representation. Different scorings have been performed on the system results. The understanding error rate, for the Full scoring is, depending on the systems, from 29% to 41.3%. A diagnosis analysis of these results is proposed.</abstract>
    </paper>
    <paper id="386">
      <author><first>Jonas</first><last>Kuhn</last></author>
      <author><first>Michael</first><last>Jellinghaus</last></author>
      <title>Multilingual parallel treebanking: a lean and flexible approach</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/629_pdf.pdf</url>
      <abstract>We propose a bootstrapping approach to creating a phrase-level alignment over a sentence-aligned parallel corpus, reporting concrete treebank annotation work performed on a sample of sentence tuples from the Europarl corpus, currently for English, French, German, and Spanish. The manually annotated seed data will be used as the basis for automatically labelling the rest of the corpus. Some preliminary experiments addressing the bootstrapping aspects are presented.The representation format for syntactic correspondence across parallel text that we propose as the starting point for a process of successive refinement emphasizes correspondences of major constituents that realize semantic arguments or modifiers; language-particular details of morphosyntactic realization are intentionally left largely unlabelled. We believe this format is a good basis for training NLPtools for multilingual application contexts in which consistency across languages is more central than fine-grained details in specific languages (in particular, syntax-based statistical Machine Translation).</abstract>
    </paper>
    <paper id="387">
      <author><first>Julie</first><last>Mauclair</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <author><first>Simon</first><last>Petit-Renaud</last></author>
      <author><first>Paul</first><last>Deléglise</last></author>
      <title>Automatic Detection of Well Recognized Words in Automatic Speech Transcriptions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/630_pdf.pdf</url>
      <abstract>This work adresses the use of confidence measures for extracting well recognized words with very low error rate from automatically transcribed segments in a unsupervised way. We present and compare several confidence measures and propose a method to merge them into a new one. We study its capabilities on extracting correct recognized word-segments compared to the amount of rejected words. We apply this fusion measure to select audio segments composed of words with a high confidence score. These segments come from an automatic transcription of french broadcast news given by our speech recognition system based on the CMU Sphinx3.3 decoder. Injecting new data resulting from unsupervised treatments of raw audio recordings in the training corpus of acoustic models gives statistically significant improvement (95% confident interval) in terms of word error rate. Experiments have been carried out on the corpus used during ESTER, the french evaluation campaign.</abstract>
    </paper>
    <paper id="388">
      <author><first>Federico</first><last>Calzolari</last></author>
      <author><first>Eva</first><last>Sassolini</last></author>
      <author><first>Manuela</first><last>Sassi</last></author>
      <author><first>Sebastiana</first><last>Cucurullo</last></author>
      <author><first>Eugenio</first><last>Picchi</last></author>
      <author><first>Francesca</first><last>Bertagna</last></author>
      <author><first>Alessandro</first><last>Enea</last></author>
      <author><first>Monica</first><last>Monachini</last></author>
      <author><first>Claudia</first><last>Soria</last></author>
      <author><first>Nicoletta</first><last>Calzolari</last></author>
      <title>Next Generation Language Resources using Grid</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/631_pdf.pdf</url>
      <abstract>This paper presents a case study concerning the challenges and requirements posed by next generation language resources, realized as an overall model of open, distributed and collaborative language infrastructure. If a sort of new paradigm for language resource sharing is required, we think that the emerging and still evolving technology connected to Grid computing is a very interesting and suitable one for a concrete realization of this vision. Given the current limitations of Grid computing, it is very important to test the new environment on basic language analysis tools, in order to get the feeling of what are the potentialities and possible limitations connected to its use in NLP. For this reason, we have done some experiments on a module of the Linguistic Miner, i.e. the extraction of linguistic patterns from restricted domain corpora. The Grid environment has produced the expected results (reduction of the processing time, huge storage capacity, data redundancy) without any additional cost for the final user.</abstract>
    </paper>
    <paper id="389">
      <author><first>Sanni</first><last>Nimb</last></author>
      <title><fixed-case>LEXADV</fixed-case> - a multilingual semantic Lexicon for Adverbs</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/632_pdf.pdf</url>
      <abstract>Topic: lexical ressources, international project Abstract: The LEXADV-project is a Scandinavian research project (2004-2006, financed by Nordplus Sprog) with the aim of extending three Scandinavian semantic lexicons building on the SIMPLE lexicon model (Lenci et al., 2000) with the word class of adverbs. In the lexicons of approx. 400 Danish, Norwegian and Swedish adverbs the different senses are described with a semantic type and a set of semantic features. A classification covering the many meanings that adverbs can have has been established and integrated in the original SIMPLE ontology. Similarly new features have been added to the model in order to describe the adverb senses. The working method of the project builds on the fact that the vocabularies of Danish, Norwegian and Swedish are closely related. An encoding tool has been developed with the special purpose of permitting easy transfer of semantic types and features between entries in the three languages. The Danish adverb senses have been described first, based on the definition in a modern, comprehensive Danish dictionary. Afterwards the lemmas have been translated and the semantic data have been copied into the Swedish as well as into the Norwegian equivalent entry. Finally these copies have been evaluated and when necessary adjusted by native speakers.</abstract>
    </paper>
    <paper id="390">
      <author><first>Voula</first><last>Giouli</last></author>
      <author><first>Alexis</first><last>Konstandinidis</last></author>
      <author><first>Elina</first><last>Desypri</last></author>
      <author><first>Harris</first><last>Papageorgiou</last></author>
      <title>Multi-domain Multi-lingual Named Entity Recognition: Revisiting &amp; Grounding the resources issue</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/633_pdf.pdf</url>
      <abstract>The paper reports on the development methodology of a system aimed at multi-domain multi-lingual recognition and classification of names in texts, the focus being on the linguistic resources used for training and testing purposes. The corpus presented here has been collected and annotated in the framework of different projects the critical issue being the development of a final resource that is homogenous, re-usable and adaptable to different domains and languages with a view to robust multi-domain and multi-lingual NERC.</abstract>
    </paper>
    <paper id="391">
      <author><first>Rebecca</first><last>Passonneau</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Owen</first><last>Rambow</last></author>
      <title>Inter-annotator Agreement on a Multilingual Semantic Annotation Task</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/634_pdf.pdf</url>
      <abstract>Six sites participated in the Interlingual Annotation of Multilingual Text Corpora (IAMTC) project (Dorr et al., 2004; Farwell et al., 2004; Mitamura et al., 2004). Parsed versions of English translations of news articles in Arabic, French, Hindi, Japanese, Korean and Spanish were annotated by up to ten annotators. Their task was to match open-class lexical items (nouns, verbs, adjectives, adverbs) to one or more concepts taken from the Omega ontology (Philpot et al., 2003), and to identify theta roles for verb arguments. The annotated corpus is intended to be a resource for meaning-based approaches to machine translation. Here we discuss inter-annotator agreement for the corpus. The annotation task is characterized by annotators freedom to select multiple concepts or roles per lexical item. As a result, the annotation categories are sets, the number of which is bounded only by the number of distinct annotator-lexical item pairs. We use a reliability metric designed to handle partial agreement between sets. The best results pertain to the part of the ontology derived from WordNet. We examine change over the course of the project, differences among annotators, and differences across parts of speech. Our results suggest a strong learning effect early in the project.</abstract>
    </paper>
    <paper id="392">
      <author><first>Rebecca</first><last>Passonneau</last></author>
      <title>Measuring Agreement on Set-valued Items (<fixed-case>MASI</fixed-case>) for Semantic and Pragmatic Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/636_pdf.pdf</url>
      <abstract>Annotation projects dealing with complex semantic or pragmatic phenomena face the dilemma of creating annotation schemes that oversimplify the phenomena, or that capture distinctions conventional reliability metrics cannot measure adequately. The solution to the dilemma is to develop metrics that quantify the decisions that annotators are asked to make. This paper discusses MASI, distance metric for comparing sets, and illustrates its use in quantifying the reliability of a specific dataset. Annotations of Summary Content Units (SCUs) generate models referred to as pyramids which can be used to evaluate unseen human summaries or machine summaries. The paper presents reliability results for five pairs of pyramids created for document sets from the 2003 Document Understanding Conference (DUC). The annotators worked independently of each other. Differences between application of MASI to pyramid annotation and its previous application to co-reference annotation are discussed. In addition, it is argued that a paradigmatic reliability study should relate measures of inter-annotator agreement to independent assessments, such as significance tests of the annotated variables with respect to other phenomena. In effect, what counts as sufficiently reliable intera-annotator agreement depends on the use the annotated data will be put to.</abstract>
    </paper>
    <paper id="393">
      <author><first>Tomoyosi</first><last>Akiba</last></author>
      <title>Exploiting Dynamic Passage Retrieval for Spoken Question Recognition and Context Processing towards Speech-driven Information Access Dialogue</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/637_pdf.pdf</url>
      <abstract>Speech interfaces and dialogue processing abilities have promise for improving the utility of open-domain question answering (QA).We propose a novel method of resolving disambiguation problems arisen in those speech and dialogue enhanced QA tasks. The proposed method exploits passage retrieval, which is one of main components common in many QA systems. The basic idea of the method is that the similarity with some passage in the target documents can be used to select the appropriate question from the candidates. In this paper, we applied the method to solve two subtasks of QA, which are (1) N-best rescoring of LVCSR outputs, which selects a most appropriate candidate as a question sentence, in speech-driven QA (SDQA) task and (2) context processing, which compose a complete question sentence from a submitted incomplete one by using the elements appeared in the dialogue context, in information access dialogue (IAD) task. For both tasks, a dynamic passage retrieval is introduced to further improve the performance. The experimental results showed that the proposed method is quite effective in order to improve the performance of QA in both two tasks.</abstract>
    </paper>
    <paper id="394">
      <author><first>Marc</first><last>Verhagen</last></author>
      <author><first>Robert</first><last>Knippen</last></author>
      <author><first>Inderjeet</first><last>Mani</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <title>Annotation of Temporal Relations with Tango</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/638_pdf.pdf</url>
      <abstract>Temporal annotation is a complex task characterized by low markup speed and low inter-annotator agreements scores. Tango is a graphical annotation tool for temporal relations. It is developed for the TimeML annotation language and allows annotators to build a graph that resembles a timeline. Temporal relations are added by selecting events and drawing labeled arrows between them. Tango is integrated with a temporal closure component and includes features like SmartLink, user prompting and automatic linking of time expressions. Tango has been used to create two corpora with temporal annotation, TimeBank and the AQUAINT Opinion corpus.</abstract>
    </paper>
    <paper id="395">
      <author><first>Patrizia</first><last>Paggio</last></author>
      <title>Annotating Information Structure in a Corpus of Spoken <fixed-case>D</fixed-case>anish</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/639_pdf.pdf</url>
      <abstract>This paper presents the work done to annotate a corpus of spoken Danish with information structure tags, and describes a preliminary study in which the corpus has been used to investigate the relation between focus and intra-clausal pauses. The study indicates that the pauses that do fall within the focus domain tend to precede property-expressing words by which the object in focus is distinguished from other similar ones.</abstract>
    </paper>
    <paper id="396">
      <author><first>Uwe</first><last>Quasthoff</last></author>
      <author><first>Matthias</first><last>Richter</last></author>
      <author><first>Christian</first><last>Biemann</last></author>
      <title>Corpus Portal for Search in Monolingual Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/641_pdf.pdf</url>
      <abstract>A simple and flexible schema for storing and presenting monolingual language resources is proposed. In this format, data for 18 different languages is already available in various sizes. The data is provided free of charge for online use and download. The main target is to ease the application of algorithms for monolingual and interlingual studies.</abstract>
    </paper>
    <paper id="397">
      <author><first>Tristan</first><last>Vanrullen</last></author>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Jean-Marie</first><last>Balfourier</last></author>
      <title>Constraint-Based Parsing as an Efficient Solution: Results from the Parsing Evaluation Campaign <fixed-case>EAS</fixed-case>y</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/642_pdf.pdf</url>
      <abstract>This paper describes the unfolding of the EASy evaluation campaign for french parsers as well as the techniques employed for the participation of laboratory LPL to this campaign. Three symbolic parsers based on a same resource and a same formalism (Property Grammars) are described and evaluated. The first results of this evaluation are analyzed and lead to the conclusion that symbolic parsing in a constraint-based formalism is efficient and robust.</abstract>
    </paper>
    <paper id="398">
      <author><first>Andreas</first><last>Eisele</last></author>
      <title>Parallel Corpora and Phrase-Based Statistical Machine Translation for New Language Pairs via Multiple Intermediaries</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/643_pdf.pdf</url>
      <abstract>We present a large parallel corpus of texts published by the United Nations Organization, which we exploit for the creation ofphrase-based statistical machine translation (SMT) systems for new language pairs. We present a setup where phrase tables for these language pairs are used for translation between languages for which parallel corpora of sufficient size are so far not available. We give some preliminary results for this novel application of SMT and discuss further refinements.</abstract>
    </paper>
    <paper id="399">
      <author><first>Owen</first><last>Rambow</last></author>
      <author><first>Bonnie</first><last>Dorr</last></author>
      <author><first>David</first><last>Farwell</last></author>
      <author><first>Rebecca</first><last>Green</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Stephen</first><last>Helmreich</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Lori</first><last>Levin</last></author>
      <author><first>Keith J.</first><last>Miller</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <author><first>Florence</first><last>Reeder</last></author>
      <author><first>Advaith</first><last>Siddharthan</last></author>
      <title>Parallel Syntactic Annotation of Multiple Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/645_pdf.pdf</url>
      <abstract>This paper describes an effort to investigate the incrementally deepening development of an interlingua notation, validated by human annotation of texts in English plus six languages. We begin with deep syntactic annotation, and in this paper present a series of annotation manuals for six different languages at the deep-syntactic level of representation. Many syntactic differences between languages are removed in the proposed syntactic annotation, making them useful resources for multilingual NLP projects with semantic components.</abstract>
    </paper>
    <paper id="400">
      <author id="sylvain-galliano"><first>S.</first><last>Galliano</last></author>
      <author id="edouard-geoffrois"><first>E.</first><last>Geoffrois</last></author>
      <author id="guillaume-gravier"><first>G.</first><last>Gravier</last></author>
      <author id="jean-francois-bonastre"><first>J.-F.</first><last>Bonastre</last></author>
      <author id="djamel-mostefa"><first>D.</first><last>Mostefa</last></author>
      <author id="khalid-choukri"><first>K.</first><last>Choukri</last></author>
      <title>Corpus description of the <fixed-case>ESTER</fixed-case> Evaluation Campaign for the Rich Transcription of <fixed-case>F</fixed-case>rench Broadcast News</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/646_pdf.pdf</url>
      <abstract>This paper presents the audio corpus developed in the framework of the ESTER evaluation campaign of French broadcast news transcription systems. This corpus includes 100 hours of manually annotated recordings and 1,677 hours of non transcribed data. The manual annotations include the detailed verbatim orthographic transcription, the speaker turns and identities, information about acoustic conditions, and name entities. Additional resources generated by automatic speech processing systems, such as phonetic alignments and word graphs, are also described.</abstract>
    </paper>
    <paper id="401">
      <author><first>Juan José Rodríguez</first><last>Soler</last></author>
      <author><first>Pedro Concejero</first><last>Cerezo</last></author>
      <author><first>Carlos Lázaro</first><last>Ávila</last></author>
      <author><first>Daniel Tapias</first><last>Merino</last></author>
      <title>Usability evaluation of 3<fixed-case>G</fixed-case> multimodal services in Telefónica Móviles España</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/647_pdf.pdf</url>
      <abstract>Third generation (3G) services boost mobile multimodal interaction offering users richer communication alternatives for accessing different applications and information services. These 3G services provide more interaction alternatives as well as active learning possibilities than previous technologies but, at the same time, these facts increase the complexity of user interfaces. Therefore, usability in multimodal interfaces has become a key factor in the service design process. In this paper we present the work done to evaluate the usability of automatic video services based on avatars with real potential users of a video-voice mail service. We describe the methodology, the tests carried out and the results and conclusions of the study. This study addresses UMTS/3G problems like the interface model, the voice-image synchronization and the user attention and memory. All the user tests have been carried out using a mobile device to take into account the constraints imposed by the screen size and the presentation and interaction limitations of a current mobile phone.</abstract>
    </paper>
    <paper id="402">
      <author><first>Márton</first><last>Miháltz</last></author>
      <author><first>Gábor</first><last>Pohl</last></author>
      <title>Exploiting Parallel Corpora for Supervised Word-Sense Disambiguation in <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>ungarian Machine Translation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/649_pdf.pdf</url>
      <abstract>In this paper we present an experiment to automatically generate annotated training corpora for a supervised word sense disambiguation module operating in an English-Hungarian and a Hungarian-English machine translation system. Training examples for the WSD module of the MT system are produced by annotating ambiguous lexical items in the source language (words having several possible translations) with their proper target language translations. Since manually annotating training examples is very costly, we are experimenting with a method to produce examples automatically from parallel corpora. Our algorithm relies on monolingual and bilingual lexicons and dictionaries in addition to statistical methods in order to annotate examples extracted from a large English-Hungarian parallel corpus accurately aligned at sentence level. In the paper, we present an experiment with the English noun state, where we categorized the different occurrences in the Hunglish parallel corpus. For this noun, most of the examples were covered by multiword lexical items originating from our lexical sources.</abstract>
    </paper>
    <paper id="403">
      <author><first>Porfírio</first><last>Filipe</last></author>
      <author><first>Nuno</first><last>Mamede</last></author>
      <title>A Framework to Integrate Ubiquitous Knowledge Modeling</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/650_pdf.pdf</url>
      <abstract>This paper describes our contribution to let end users configure mixed-initiative spoken dialogue systems to suit their personalized goals. The main problem that we want to address is the reconfiguration of spoken language dialogue systems to deal with generic plug and play artifacts. Such reconfiguration can be seen as a portability problem and is a critical research issue. In order to solve this problem we describe a hybrid approach to design ubiquitous domain models that allows the dialogue system to perform recognition of available tasks on the fly. Our approach considers two kinds of domain knowledge: the global knowledge and the local knowledge. The global knowledge, that is modeled using a top-down approach, is associated at design time with the dialogue system itself. The local knowledge, that is modeled using a bottom-up approach, is defined with each one of the artifacts. When an artifact is activated or deactivated, a bilateral process, supported by a broker, updates the domain knowledge considering the artifact local knowledge. We assume that everyday artifacts are augmented with computational capabilities and semantic descriptions supported by their own knowledge model. A case study focusing a microwave oven is depicted.</abstract>
    </paper>
    <paper id="404">
      <author><first>Felice</first><last>Dell’Orletta</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <author><first>Simonetta</first><last>Montemagni</last></author>
      <author><first>Vito</first><last>Pirrelli</last></author>
      <title>Searching treebanks for functional constraints: cross-lingual experiments in grammatical relation assignment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/651_pdf.pdf</url>
      <abstract>The paper reports on a detailed quantitative analysis of distributional language data of both Italian and Czech, highlighting the relative contribution of a number of distributed grammatical factors to sentence-based identification of subjects and direct objects. The work is based on a Maximum Entropy model of stochastic resolution of grammatical conflicting constraints, and is demonstrably capable of putting explanatory theoretical accounts to the challenging test of an extensive, usage-based empirical verification.</abstract>
    </paper>
    <paper id="405">
      <author><first>Thierry</first><last>Declerck</last></author>
      <title><fixed-case>S</fixed-case>yn<fixed-case>AF</fixed-case>: Towards a Standard for Syntactic Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/652_pdf.pdf</url>
      <abstract>In the paper we present the actual state of development of an international standard for syntactic annotation, called SynAF. This standard is being prepared by the Technical Committee ISO/TC 37 (Terminology and Other Language Resources), Subcommittee SC 4 (Language Resource Management), in collaboration with the European eContent Project LIRICS (Linguistic Infrastructure for Interoperable Resources and Systems).</abstract>
    </paper>
    <paper id="406">
      <author><first>Christelle</first><last>Ayache</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <title><fixed-case>EQ</fixed-case>ue<fixed-case>R</fixed-case>: the <fixed-case>F</fixed-case>rench Evaluation campaign of Question-Answering Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/653_pdf.pdf</url>
      <abstract>This paper describes the EQueR-EVALDA Evaluation Campaign, the French evaluation campaign of Question-Answering (QA) systems. The EQueR Evaluation Campaign included two tasks of automatic answer retrieval: the first one was a QA task over a heterogeneous collection of texts - mainly newspaper articles, and the second one a specialised one in the Medical field over a corpus of medical texts. In total, seven groups participated in the General task and five groups participated in the Medical task. For the General task, the best system obtained 81.46% of correct answers during the evalaution of the passages, while it obtained 67.24% during the evaluation of the short answers. We describe herein the specifications, the corpora, the evaluation, the phase of judgment of results, the scoring phase and the results for the two different types of evaluation.</abstract>
    </paper>
    <paper id="407">
      <author><first>Maria Fernanda Bacelar do</first><last>Nascimento</last></author>
      <author><first>José Bettencourt</first><last>Gonçalves</last></author>
      <author><first>Luísa</first><last>Pereira</last></author>
      <author><first>Antónia</first><last>Estrela</last></author>
      <author><first>Afonso</first><last>Pereira</last></author>
      <author><first>Rui</first><last>Santos</last></author>
      <author><first>Sancho M.</first><last>Oliveira</last></author>
      <title>The <fixed-case>A</fixed-case>frican Varieties of <fixed-case>P</fixed-case>ortuguese: Compiling Comparable Corpora and Analyzing Data-Derived Lexicon</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/654_pdf.pdf</url>
      <abstract>Linguistic Resources for the Study of the Portuguese African Varieties is an ongoing project that aims at the constitution, treatment, analysis and availability of a corpus of the African varieties of Portuguese, with 3 million words of written and spoken texts, constituted by five comparable subcorpora, corresponding to the varieties of Angola, Cape Verde, Guinea-Bissau, Mozambique and Sao Tome and Principe. This material will allow intra and intercorpora comparative studies, which will make visible variations that result from discursive and pragmatic differences of each corpus and aspects of linguistic unity or diversity that characterise the spoken Portuguese of this referred five African countries. The five corpora are comparable in size (600,000 words each), in chronology (the last 30 years) and in types and genres (24,000 spoken words and c. 580,000 written words, the last belonging to newspapers, literature and varia). The corpus is automatically annotated and after the extraction of alphabetical lists of lexical forms, these data will be automatically lemmatised. Five separated lists of vocabulary for each variety will be established. A tool for word extraction and preferential calculus according to predefined indexes in order to achieve lexicon comparison of the African Portuguese Varieties is being developed. Concordances extraction will be also performed.</abstract>
    </paper>
    <paper id="408">
      <author><first>Benjamin K.</first><last>Tsou</last></author>
      <author><first>Oi Yee</first><last>Kwong</last></author>
      <title>Toward a Pan-<fixed-case>C</fixed-case>hinese Thesaurus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/656_pdf.pdf</url>
      <abstract>In this paper, we propose a corpus-based approach to the construction of a Pan-Chinese lexical resource, starting out with the aim to enrich existing Chinese thesauri in the Pan-Chinese context. The resulting thesaurus is thus expected to contain not only the core senses and usages of Chinese lexical items but also usages specific to individual Chinese speech communities. We introduce the ideas behind the construction of the resource, outline the steps to be taken, and discuss some preliminary analyses. The work is backed up by a unique and large Chinese synchronous corpus containing textual data from various Chinese speech communities including Hong Kong, Beijing, Taipei and Singapore.</abstract>
    </paper>
    <paper id="409">
      <author><first>Nelleke</first><last>Oostdijk</last></author>
      <author><first>Lou</first><last>Boves</last></author>
      <title>User requirements analysis for the design of a reference corpus of written <fixed-case>D</fixed-case>utch</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/657_pdf.pdf</url>
      <abstract>The Dutch Language Corpus Initiative (D-Coi) project aims to specify the design of a 500-million-word reference corpus of written Dutch, and to put the tools and procedures in place that are needed to actually construct such a corpus. One of the tasks in the project is to conduct a user requirements study that should provide the basis for the eventual design of the 500-million-word reference corpus. The present paper outlines the user requirements analysis and reports the results so far.</abstract>
    </paper>
    <paper id="410">
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <author><first>Laura</first><last>Monceaux</last></author>
      <title><fixed-case>FRASQUES</fixed-case>: A Question Answering system in the <fixed-case>EQ</fixed-case>ue<fixed-case>R</fixed-case> evaluation campaign</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/658_pdf.pdf</url>
      <abstract>Question-answering (QA) systems aim at providing either a small passage or just the answer to a question in natural language. We have developed several QA systems that work on both English and French. This way, we are able to provide answers to questions given in both languages by searching documents in both languages also. In this article, we present our French monolingual system FRASQUES which participated in the EQueR evaluation campaign of QA systems for French in 2004. First, the QA architecture common to our systems is shown. Then, for every step of the QA process, we consider which steps are language-independent, and for those that are language-dependent, the tools or processes that need to be adapted to switch for one language to another. Finally, our results at EQueR are given and commented; an error analysis is conducted, and the kind of knowledge needed to answer a question is studied.</abstract>
    </paper>
    <paper id="411">
      <author><first>Gábor</first><last>Hodász</last></author>
      <title>Evaluation Methods of a Linguistically Enriched Translation Memory System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/661_pdf.pdf</url>
      <abstract>The paper gives an overview of the evaluation methods of memory-based translation systems: Translation Memories (TM) and Example Based Machine Translation (EBMT) systems. After a short comparison with the well-discussed methods of evaluation of Machine Translation (MT) Systems we give a brief overview of current methodology on memory-based applications. We propose a new aspect, which takes the content of memory into account: a measure to describe the correspondence between the memory and the current segment to translate. We also offer a brief survey of a linguistically enriched translation memory on which these new methods will be tested.</abstract>
    </paper>
    <paper id="412">
      <author><first>Alberto</first><last>Simões</last></author>
      <author><first>José João</first><last>Almeida</last></author>
      <title><fixed-case>T</fixed-case>2<fixed-case>O</fixed-case> - Recycling Thesauri into a Multilingual Ontology</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/664_pdf.pdf</url>
      <abstract>In this article we present T2O - a workbench to assist the process of translating heterogeneous resources into ontologies, to enrich and add multilingual information, to help programming with them, and to support ontology publishing. T2O is an ontology algebra.</abstract>
    </paper>
    <paper id="413">
      <author><first>Saba</first><last>Amsalu</last></author>
      <title>Data-driven <fixed-case>A</fixed-case>mharic-<fixed-case>E</fixed-case>nglish Bilingual Lexicon Acquisition</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/666_pdf.pdf</url>
      <abstract>This paper describes a simple approach of statistical language modelling for bilingual lexicon acquisition from Amharic-English parallel corpora. The goal is to induce a seed translation lexicon from sentence-aligned corpora. The seed translation lexicon contains matches of Amharic lexemes to weekly inflected English words. Purely statistical measures of term distribution are used as the basis for finding correlations between terms. An authentic scoring scheme is codified based on distributional properties of words. For low frequency terms a two step procedure of: first a rough alignment; and then an automatic filtering to sift the output and improve the precision is made. Given the disparity of the languages and the small size of corpora used the results demonstrate the viability of the approach.</abstract>
    </paper>
    <paper id="414">
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <title><fixed-case>ISA</fixed-case> &amp; <fixed-case>ICA</fixed-case> - Two Web Interfaces for Interactive Alignment of Bitexts alignment of parallel texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/667_pdf.pdf</url>
      <abstract>ISA and ICA are two web interfaces for interactive alignment of parallel texts. ISA provides an interface for automatic and manual sentence alignment. It includes cognate filters and uses structural markup to improve automatic alignment and provides intuitive tools for editing them. Alignment results can be saved to disk or sent via e-mail. ICA provides an interface to the clue aligner from the Uplug toolbox. It allows one to set various parameters and visualizes alignment results in a two-dimensional matrix. Word alignments can be edited and saved to disk.</abstract>
    </paper>
    <paper id="415">
      <author><first>Petra-Maria</first><last>Strauß</last></author>
      <author><first>Holger</first><last>Hoffman</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <author><first>Heiko</first><last>Neumann</last></author>
      <author><first>Günther</first><last>Palm</last></author>
      <author><first>Stefan</first><last>Scherer</last></author>
      <author><first>Friedhelm</first><last>Schwenker</last></author>
      <author><first>Harald</first><last>Traue</last></author>
      <author><first>Welf</first><last>Walter</last></author>
      <author><first>Ulrich</first><last>Weidenbacher</last></author>
      <title><fixed-case>W</fixed-case>izard-of-<fixed-case>O</fixed-case>z Data Collection for Perception and Interaction in Multi-User Environments</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/669_pdf.pdf</url>
      <abstract>In this paper we present the setup of an extensive Wizard-of-Oz environment used for the data collection and the development of a dialogue system. The envisioned Perception and Interaction Assistant will act as an independent dialogue partner. Passively observing the dialogue between the two human users with respect to a limited domain, the system should take the initiative and get meaningfully involved in the communication process when required by the conversational situation. The data collection described here involves audio and video data. We aim at building a rich multi-media data corpus to be used as a basis for our research which includes, inter alia, speech and gaze direction recognition, dialogue modelling and proactivity of the system. We further aspire to obtain data with emotional content to perfom research on emotion recognition, psychopysiological and usability analysis.</abstract>
    </paper>
    <paper id="416">
      <author><first>Nancy L.</first><last>Underwood</last></author>
      <author><first>Agnes</first><last>Lisowska</last></author>
      <title>The Evolution of an Evaluation Framework for a Text Mining System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/670_pdf.pdf</url>
      <abstract>The Parmenides project developed a text mining application applied in three different domains exemplified by case studies for the three user partners in the project. During the lifetime of the project (and in parallel with the development of the system itself) an evaluation framework was developed by the authors in conjunction with the users, and was eventually applied to the system. The object of the exercise was two-fold: firstly to develop and perform a complete user-centered evaluation of the system to assess how well it answered the users' requirements and, secondly, to develop a general framework which could be applied in the context of other users' requirements and (with some modification) to similar systems. In this paper we describe not only the framework but the process of building and parameterising the quality model for each case study and, perhaps most interestingly, the way in which the quality model and users' requirements and expectations evolved over time.</abstract>
    </paper>
    <paper id="417">
      <author><first>Eline</first><last>Westerhout</last></author>
      <author><first>Paola</first><last>Monachesi</last></author>
      <title>A pilot study for a Corpus of <fixed-case>D</fixed-case>utch Aphasic Speech (<fixed-case>C</fixed-case>o<fixed-case>DAS</fixed-case>)</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/672_pdf.pdf</url>
      <abstract>In this paper, a pilot study for the development of a corpus of Dutch Aphasic Speech (CoDAS) is presented. Given the lack of resources of this kind not only for Dutch but also for other languages, CoDAS will be able to set standards and will contribute to the future research in this area. Given the special character of the speech contained in CoDAS, we cannot simply carry over the design and annotation protocols of existing corpora, such as the Corpus Gesproken Nederlands or CHILDES. However, they have been assumed as starting point. We have investigated whether and how the procedures and protocols for the annotation (part-of-speech tagging) and transcription (orthographic and phonetic) used for the CGN should be adapted in order to annotate and transcribe aphasic speech properly. Besides, we have established the basic requirements with respect to text types, metadata, and annotation levels that CoDAS should fulfill.</abstract>
    </paper>
    <paper id="418">
      <author><first>Jan</first><last>Bungeroth</last></author>
      <author><first>Daniel</first><last>Stein</last></author>
      <author><first>Philippe</first><last>Dreuw</last></author>
      <author><first>Morteza</first><last>Zahedi</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <title>A <fixed-case>G</fixed-case>erman <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Corpus of the Domain Weather Report</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/673_pdf.pdf</url>
      <abstract>All systems for automatic sign language translation and recognition, in particular statistical systems, rely on adequately sized corpora. For this purpose, we created the Phoenix corpus that is based on German television weather reports translated into German Sign Language. It comes with a rich annotation of the video data, a bilingual text-based sentence corpus and a monolingual German corpus. All systems for automatic sign language translation and recognition, in particular statistical systems, rely on adequately sized corpora. For this purpose, we created the Phoenix corpus that is based on German television weather reports translated into German Sign Language. It comes with a rich annotation of the video data, a bilingual text-based sentence corpus and a monolingual German corpus.</abstract>
    </paper>
    <paper id="419">
      <author><first>Roberto</first><last>Bartolini</last></author>
      <author><first>Caterina</first><last>Caracciolo</last></author>
      <author><first>Emiliano</first><last>Giovanetti</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <author><first>Simone</first><last>Marchi</last></author>
      <author><first>Vito</first><last>Pirrelli</last></author>
      <author><first>Chiara</first><last>Renso</last></author>
      <author><first>Laura</first><last>Spinsanti</last></author>
      <title>Creation and Use of Lexicons and Ontologies for <fixed-case>NL</fixed-case> Interfaces to Databases</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/674_pdf.pdf</url>
      <abstract>In this paper we present an original approach to natural language query interpretation which has been implemented withinthe FuLL (Fuzzy Logic and Language) Italian project of BC S.r.l. In particular, we discuss here the creation of linguisticand ontological resources, together with the exploitation of existing ones, for natural language-driven database access andretrieval. Both the database and the queries we experiment with are Italian, but the methodology we broach naturally extends to other languages.</abstract>
    </paper>
    <paper id="420">
      <author><first>Andrea</first><last>Mulloni</last></author>
      <author><first>Viktor</first><last>Pekar</last></author>
      <title>Automatic Detection of Orthographics Cues for Cognate Recognition</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/676_pdf.pdf</url>
      <abstract>Present-day machine translation technologies crucially depend on the size and quality of lexical resources. Much of recent research in the area has been concerned with methods to build bilingual dictionaries automatically. In this paper we propose a methodology for the automatic detection of cognates between two languages based solely on the orthography of words. From a set of known cognates, the method induces rules capturing regularities of orthographic mutations that a word undergoes when migrating from one language into the other. The rules are then applied as a preprocessing step before measuring the orthographic similarity between putative cognates. As a result, the method allows to achieve an improvement in the F-measure of 11,86% in comparison with detecting cognates based only on the edit distance between them.</abstract>
    </paper>
    <paper id="421">
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Su’ad</first><last>Awab</last></author>
      <title>Open Source Corpus Analysis Tools for <fixed-case>M</fixed-case>alay</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/677_pdf.pdf</url>
      <abstract>Tokenisers, lemmatisers and POS taggers are vital to the linguistic and digital furtherment of any language. In this paper, we present an open source toolkit for Malay incorporating a word and sentence tokeniser, a lemmatiser and a partial POS tagger, based on heavy reuse of pre-existing language resources. We outline the software architecture of each component, and present an evaluation of each over a 26K word sample of Malay text.</abstract>
    </paper>
    <paper id="422">
      <author><first>Fidelia</first><last>Ibekwe-Sanjuan</last></author>
      <title>A task-oriented framework for evaluating theme detection systems: A discussion paper</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/678_pdf.pdf</url>
      <abstract>This paper discusses the inherent difficulties in evaluating systems for theme detection. Such systems are based essentially on unsupervised clustering aiming to discover the underlying structure in a corpus of texts. As the structures are precisely unknown beforehand, it is difficult to devise a satisfactory evaluation protocol. Several problems are posed by cluster evaluation: determining the optimal number of clusters, cluster content evaluation, topology of the discovered structure. Each of these problems has been studied separately but some of the proposed metrics portray significant flaws. Moreover, no benchmark has been commonly agreed upon. Finally, it is necessary to distinguish between task-oriented and activity-oriented evaluation as the two frameworks imply different evaluation protocols. Possible solutions to the activity-oriented evaluation can be sought from the data and text mining communities.</abstract>
    </paper>
    <paper id="423">
      <author id="asuncion-moreno"><first>A.</first><last>Moreno</last></author>
      <author><first>Albert</first><last>Febrer</last></author>
      <author><first>Lluis</first><last>Márquez</last></author>
      <title>Generation of Language Resources for the Development of Speech Technologies in <fixed-case>C</fixed-case>atalan</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/679_pdf.pdf</url>
      <abstract>This paper describes a joint initiative of the Catalan and Spanish Government to produce Language Resources for the Catalan language. A similar methodology to the Basic Language Resource Kit (BLARK) concept was applied to determine the priorities on the production of the Language Resources. The paper shows the LR and tools currently available for the Catalan Language both for Language and Speech technologies. The production of large databases for Automatic Speech Recognition purposes already started. All the resources generated in the project follow EU standards, will be validated by an external centre and will be free and public available through ELRA.</abstract>
    </paper>
    <paper id="424">
      <author><first>Georgiana</first><last>Puşcaşu</last></author>
      <author><first>Ruslan</first><last>Mitkov</last></author>
      <title>If “it” were “then”, then when was “it”? Establishing the anaphoric role of “then”</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/680_pdf.pdf</url>
      <abstract>The adverb "then" is among the most frequent Englishtemporal adverbs, being also capable of filling a variety of semantic roles. The identification of anaphoric usages of "then"is important for temporal expression resolution, while thetemporal relationship usage is important for event ordering. Given that previous work has not tackled the identification and temporal resolution of anaphoric "then", this paper presents a machine learning approach for setting apart anaphoric usages and a rule-based normaliser that resolves it with respect to an antecedent. The performance of the two modules is evaluated. The present paper also describes the construction of an annotated corpus and the subsequent derivation of training data required by the machine learning module.</abstract>
    </paper>
    <paper id="425">
      <author><first>Viktor</first><last>Trón</last></author>
      <author><first>Péter</first><last>Halácsy</last></author>
      <author><first>Péter</first><last>Rebrus</last></author>
      <author><first>András</first><last>Rung</last></author>
      <author><first>Péter</first><last>Vajda</last></author>
      <author><first>Eszter</first><last>Simon</last></author>
      <title><fixed-case>M</fixed-case>orphdb.hu: <fixed-case>H</fixed-case>ungarian lexical database and morphological grammar</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/683_pdf.pdf</url>
      <abstract>This paper describes morphdb.hu, a Hungarian lexical database and morphological grammar. Morphdb.hu is the outcome of a several-year collaborative effort and represents the resource with the widest coverage and broadest range of applicability presently available for Hungarian. The grammar resource is the formalization of well-founded theoretical decisions handling inflection and productive derivation. The lexical database was created by merging three independent lexical databases, and the resulting resource was further extended.</abstract>
    </paper>
    <paper id="426">
      <author id="phuong-le-hong"><last>Le</last><first>H. Phuong</first></author>
      <author id="thi-minh-huyen-nguyen"><last>Nguyen</last><first>T. M. Huyen</first></author>
      <author><last>Romary</last><first>Laurent</first></author>
      <author><last>Roussanaly</last><first>Azim</first></author>
      <title>A <fixed-case>L</fixed-case>exicalized <fixed-case>T</fixed-case>ree-<fixed-case>A</fixed-case>djoining <fixed-case>G</fixed-case>rammar for <fixed-case>V</fixed-case>ietnamese</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/685_pdf.pdf</url>
      <abstract>In this paper, we present the first sizable grammar built for Vietnamese using LTAG, developed over the past two years, named vnLTAG. This grammar aims at modelling written language and is general enough to be both application- and domain-independent. It can be used for the morpho-syntactic tagging and syntactic parsing of Vietnamese texts, as well as text generation. We then present a robust parsing scheme using vnLTAG and a parser for the grammar. We finish with an evaluation using a test suite.</abstract>
    </paper>
    <paper id="427">
      <author><first>Silvie</first><last>Cinková</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <author><first>Petr</first><last>Podveský</last></author>
      <author><first>Pavel</first><last>Schlesinger</last></author>
      <title>Semi-automatic Building of <fixed-case>S</fixed-case>wedish Collocation Lexicon</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/687_pdf.pdf</url>
      <abstract>This work focuses on semi-automatic extraction of verb-noun collocations from a corpus, performed to provide lexical evidence for the manual lexicographical processing of Support Verb Constructions (SVCs) in the Swedish-Czech Combinatorial Valency Lexicon of Predicate Nouns. Efficiency of pure manual extractionprocedure is significantly improved by utilization of automatic statistical methods based lexical association measures.</abstract>
    </paper>
    <paper id="428">
      <author><first>Dominika</first><last>Oliver</last></author>
      <author><first>Krzysztof</first><last>Szklanny</last></author>
      <title>Creation and analysis of a <fixed-case>P</fixed-case>olish speech database for use in unit selection synthesis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/688_pdf.pdf</url>
      <abstract>The main aim of this study is to describe the process of creating a speech database to be used in corpus based text-to-speech synthesis. To help achieve natural sounding speech synthesis, the database construction was aimed at rich phonetic and prosodic coverage based on variable length units (phoneme, diphone, triphone) from different phonetic and prosodic contexts. Following previous work on determining the optimal coverage (Szklanny and Oliver, 2005), text selection was based on the existing text corpus containing parliamentary statements. Corpus balancing was followed by recording of the material. Automatic segmentation was performed, followed by both an automatic and manual check of the data to determine speaker specific phenomena and correct the labelling. Additionally, prosodic annotation involving assignment of the intonation contours was performed in order to assess the accent realisation and determine the prosodic coverage of the database. The prototype speech synthesiser was built to determine the validity of the above steps and test the resulting voice quality.</abstract>
    </paper>
    <paper id="429">
      <author><first>Jorge</first><last>Kinoshita</last></author>
      <author><first>Laís do Nascimento</first><last>Salvador</last></author>
      <author><first>Carlos Eduardo Dantas</first><last>de Menezes</last></author>
      <title><fixed-case>C</fixed-case>o<fixed-case>G</fixed-case>r<fixed-case>OO</fixed-case>: a <fixed-case>B</fixed-case>razilian-<fixed-case>P</fixed-case>ortuguese Grammar Checker based on the <fixed-case>CETENFOLHA</fixed-case> Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/689_pdf.pdf</url>
      <abstract>This paper describes an ongoing Portuguese Language grammar checker project, called CoGrOO1-Corretor Gramatical para OpenOffice (Grammar Checker for OpenOffice), based on CETENFOLHA, a Brazilian Portuguese morphosyntactic annotated Corpus. Two of its features are highlighted: - hybrid architecture, mixing rules and statistics; - free software project. This project aims at checking grammatical errors such as nominal and verbal agreement, crase (the coalescence of preposition a (to) + definitive singular determiner a yielding à), nominal and verbal government and other common errors in Brazilian Portuguese Language. We also present some empirical results based on the implemented techniques.</abstract>
    </paper>
    <paper id="430">
      <author><first>Stefan</first><last>Schaden</last></author>
      <title>Evaluation of Automatically Generated Transcriptions of Non-Native Pronunciations using a Phonetic Distance Measure</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/691_pdf.pdf</url>
      <abstract>The paper reports on the evaluation of a rule-based technique to model prototypical non-native pronunciation variants on the symbolic transcription level. This technique was developed to explore the possibility of an automatic generation of adapted pronunciation lexicons for different non-native speaker groups. The rule sets, which are currently available for nine language directions, are based on non-native speech data compiled specifically for this purpose. Since manual phonetic annotations are available for the speech data, the evaluation was performed on the transcription level by measuring the phonetic distance of the automatically generated pronunciations variants and actual pronunciations of non-native speakers. One of the central questions to be addressed by the evaluation is whether the rules have any predictive value: It has to be determined if and to what degree the rules are capable of generating realistic pronunciation variants for previously unseen speakers. Secondly, the rules should not only represent the pronunciations of individual speakers adequately; instead, they should be representative of speaker groups (cross-speaker representation). The paper outlines the evaluation methodology and presents results for selected language directions.</abstract>
    </paper>
    <paper id="431">
      <author><first>Isabella</first><last>Chiari</last></author>
      <title>Slips and errors in spoken data transcription</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/692_pdf.pdf</url>
      <abstract>The present work illustrates the main results of an experiment on errors and repairs in spoken language transcription, with significant relevance for the evaluation of validity, reliability and correctness of transcriptions of speech belonging to several different typologies, set for the annotation of spoken corpora. In particular, we dealt with errors and repair strategies that appear on the first drafts of the transcription process that are not easily detectable with automatic post-editing procedures. 20 participants were asked to give an accurate transcription of 22 short utterances, repeated from one to four times, belonging two non-spontaneous (10) and spontaneous conversation (10). Error analysis suggests a general preference for meaning preservation even after the alteration of the original form, and for the preference for certain error patterns and repair strategies.</abstract>
    </paper>
    <paper id="432">
      <author><first>Motoko</first><last>Ueyama</last></author>
      <title>Evaluation of Web-based Corpora: Effects of Seed Selection and Time Interval</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/697_pdf.pdf</url>
      <abstract>Recently, there have been efforts to construct written corpora by using the WWW. A promising approach to build Web corpora is to run automated queries to search engines and download pages found in this way. This makes it possible to build corpora rapidly and economically, but we cannot control what are contained in resulting corpora. Under these circumstances, it is important to verify the general nature of Web corpora. This study, in particular, investigated effects of two essential factors on three Japanese corpora that we built: seed terms used for queries; and time interval between different corpus construction sessions, which measures the stability of query results over time. We evaluated the corpora qualitatively, in terms of domains, genres and typical lexical items. Results show these two patterns: 1) both seed selection and time interval affect the distribution of text and lexicon; 2) the effect of seed selection is much stronger. The prominent effect of seed selection suggests that a good understanding of the cause-and-effect relation between seeds and retrieved documents is an important step to gain some control over the characteristics of Web corpora, in particular, for the construction of general corpora meant to represent a language as a whole.</abstract>
    </paper>
    <paper id="433">
      <author><first>José</first><last>Iria</last></author>
      <author><first>Christopher</first><last>Brewster</last></author>
      <author><first>Fabio</first><last>Ciravegna</last></author>
      <author><first>Yorick</first><last>Wilks</last></author>
      <title>An Incremental Tri-Partite Approach To Ontology Learning</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/700_pdf.pdf</url>
      <abstract>In this paper we present a new approach to ontology learning. Its basis lies in a dynamic and iterative view of knowledge acquisition for ontologies. The Abraxas approach is founded on three resources, a set of texts, a set of learning patterns and a set of ontological triples, each of which must remain in equilibrium. As events occur which disturb this equilibrium various actions are triggered to re- establish a balance between the resources. Such events include acquisition of a further text from external resources such as the Web or the addition of ontological triples to the ontology. We develop the concept of a knowledge gap between the coverage of an ontology and the corpus of texts as a measure triggering actions. We present an overview of the algorithm and its functionalities.</abstract>
    </paper>
    <paper id="434">
      <author><first>Thomas</first><last>Pellegrini</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <title>Experimental detection of vowel pronunciation variants in <fixed-case>A</fixed-case>mharic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/701_pdf.pdf</url>
      <abstract>The pronunciation lexicon is a fundamental element in an automatic speech transcription system. It associates each lexical entry (usually a grapheme), with one or more phonemic or phone-like forms, the pronunciation variants. Thorough knowledge of the target language is a priori necessary to establish the pronunciation baseforms and variants. The reliance on human expertise can pose difficulties in developing a system for a language where such knowledge may not be readily available. In this article a speech recognizer is used to help select pronunciation variants in Amharic, the official language of Ethiopia, focusing on alternate choices for vowels. This study is carried out using an audio corpus composed of 37 hours of speech from radio broadcasts which were orthographically transcribed by native speakers. Since the corpus is relatively small for estimating pronunciation variants, a first set of studies were carried out at a syllabic level. Word lexica were then constructed based on the observed syllable occurences. Automatic alignments were compared for lexica containing different vowel variants, with both context-independent and context-dependent acoustic models sets. The variant2+ measure proposed in (Adda-Decker and Lamel, 1999) is used to assess the potential need for pronunciation variants.</abstract>
    </paper>
    <paper id="435">
      <author><first>Roberta</first><last>Catizone</last></author>
      <author><first>Angelo</first><last>Dalli</last></author>
      <author><first>Yorick</first><last>Wilks</last></author>
      <title>Evaluating Automatically Generated Timelines from the Web</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf</url>
      <abstract>As web searches increase, there is a need to represent the search results in the most comprehensible way possible. In particular, we focus on search results from queries about people and places. The standard method for presentation of search results is an ordered list determined by the Web search engine. Although this is satisfactory in some cases, when searching for people and places, presenting the information indexed by time may be more desirable. We are developing a system called Cronopath, which generates a timeline of web search engine results by determining the time frame of each document in the collection and linking elements in the timeline to the relevant articles. In this paper, we propose evaluation guidelines for judging the quality of automatically generated timelines based on a set of common features.</abstract>
    </paper>
    <paper id="436">
      <author><first>Ivana</first><last>Kruijff-Korbayová</last></author>
      <author><first>Tilman</first><last>Becker</last></author>
      <author><first>Nate</first><last>Blaylock</last></author>
      <author><first>Ciprian</first><last>Gerstenberger</last></author>
      <author><first>Michael</first><last>Kaißer</last></author>
      <author><first>Peter</first><last>Poller</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <author><first>Jan</first><last>Schehl</last></author>
      <title>The <fixed-case>SAMMIE</fixed-case> Corpus of Multimodal Dialogues with an <fixed-case>MP</fixed-case>3 Player</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/704_pdf.pdf</url>
      <abstract>We describe a corpus of multimodal dialogues with an MP3player collected in Wizard-of-Oz experiments and annotated with a richfeature set at several layers. We are using the Nite XML Toolkit (NXT) to represent and further process the data. We designed an NXTdata model, converted experiment log file data and manualtranscriptions into NXT, and are building tools for additionalannotation using NXT libraries. The annotated corpus will be used to (i) investigate various aspects of multimodal presentation andinteraction strategies both within and across annotation layers; (ii) design an initial policy for reinforcement learning of multimodalclarification requests.</abstract>
    </paper>
    <paper id="437">
      <author><first>Rebecca</first><last>Passonneau</last></author>
      <author><first>Roberta</first><last>Blitz</last></author>
      <author><first>David</first><last>Elson</last></author>
      <author><first>Angela</first><last>Giral</last></author>
      <author><first>Judith</first><last>Klavans</last></author>
      <title><fixed-case>CL</fixed-case>i<fixed-case>MB</fixed-case> <fixed-case>T</fixed-case>ool<fixed-case>K</fixed-case>it: A Case Study of Iterative Evaluation in a Multidisciplinary Project</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/705_pdf.pdf</url>
      <abstract>Digital image collections in libraries and other curatorial institutions grow too rapidly to create new descriptive metadata for subject matter search or browsing. CLiMB (Computational Linguistics for Metadata Building) was a project designed to address this dilemma that involved computer scientists, linguists, librarians, and art librarians. The CLiMB project followed an iterative evaluation model: each next phase of the project emerged from the results of an evaluation. After assembling a suite of text processing tools to be used in extracting metada, we conducted a formative evaluation with thirteen participants, using a survey in which we varied the order and type of four conditions under which respondents would propose or select image search terms. Results of the formative evaluation led us to conclude that a CLiMB ToolKit would work best if its main function was to propose terms for users to review. After implementing a prototype ToolKit using a browser interface, we conducted an evaluation with ten experts. Users found the ToolKit very habitable, remained consistently satisfied throughout a lengthy evaluation, and selected a large number of terms per image.</abstract>
    </paper>
    <paper id="438">
      <author><first>Anna</first><last>Rumshisky</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <title>Inducing Sense-Discriminating Context Patterns from Sense-Tagged Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/706_pdf.pdf</url>
      <abstract>Traditionally, context features used in word sense disambiguation are based on collocation statistics and use only minimal syntactic and semantic information. Corpus Pattern Analysis is a technique for producing knowledge-rich context features that capture sense distinctions. It involves (1) identifying sense-carrying context patterns and using the derived context features to discriminate between the unseen instances. Both stages require manual seeding. In this paper, we show how to automate inducing sense-discriminating context features from a sense-tagged corpus.</abstract>
    </paper>
    <paper id="439">
      <author><first>Milen</first><last>Kouylekov</last></author>
      <author><first>Bernardo</first><last>Magnini</last></author>
      <title>Building a Large-Scale Repository of Textual Entailment Rules</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/707_pdf.pdf</url>
      <abstract>Entailment rules are rules where the left hand side (LHS) specifies some knowledge which entails the knowledge expressed n the RHS of the rule, with some degree of confidence. Simple entailment rules can be combined in complex entailment chains, which n turn are at the basis of entailment-based reasoning, which has been recently proposed as a pervasive and application independent approach to Natural Language Understanding. We present the first elease of a large-scale repository of entailment rules at the lexical level, which have been derived from a number of available resources, including WordNet and a word similarity database. Experiments on the PASCAL-RTE dataset show that this resource plays a crucial role in recognizing textual entailment.</abstract>
    </paper>
    <paper id="440">
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <author><first>Roberto</first><last>Basili</last></author>
      <title>A Tree Kernel approach to Question and Answer Classification in Question Answering Systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/708_pdf.pdf</url>
      <abstract>A critical step in Question Answering design is the definition of the models for question focus identification and answer extraction. In case of factoid questions, we can use a question classifier (trained according to a target taxonomy) and a named entity recognizer. Unfortunately, this latter cannot be applied to generate answers related to non-factoid questions. In this paper, we tackle such problem by designing classifiers of non-factoid answers. As the feature design for this learning task is very complex, we take advantage of tree kernels to generate large feature set from the syntactic parse trees of passages relevant to the target question. Such kernels encode syntactic and lexical information in Support Vector Machines which can decide if a sentence focuses on a target taxonomy subject. The experiments with SVMs on the TREC 10 dataset show that our approach is an interesting future research.</abstract>
    </paper>
    <paper id="441">
      <author><first>Philippe Boula</first><last>de Mareüil</last></author>
      <author><first>Christophe</first><last>d’Alessandro</last></author>
      <author><first>Alexander</first><last>Raake</last></author>
      <author><first>Gérard</first><last>Bailly</last></author>
      <author><first>Marie-Neige</first><last>Garcia</last></author>
      <author><first>Michel</first><last>Morel</last></author>
      <title>A joint intelligibility evaluation of <fixed-case>F</fixed-case>rench text-to-speech synthesis systems: the <fixed-case>E</fixed-case>va<fixed-case>S</fixed-case>y <fixed-case>SUS</fixed-case>/<fixed-case>ACR</fixed-case> campaign</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/709_pdf.pdf</url>
      <abstract>The EVALDA/EvaSy project is dedicated to the evaluation of text-to-speech synthesis systems for the French language. It is subdivided into four components: evaluation of the grapheme-to-phoneme conversion module (Boula de Mareüil et al., 2005), evaluation of prosody (Garcia et al., 2006), evaluation of intelligibility, and global evaluation of the quality of the synthesised speech. This paper reports on the key results of the intelligibility and global evaluation of the synthesised speech. It focuses on intelligibility, assessed on the basis of semantically unpredictable sentences, but a comparison with absolute category rating in terms of e.g. pleasantness and naturalness is also provided. Three diphone systems and three selection systems have been evaluated. It turns out that the most intelligible system (diphone-based) is far from being the one which obtains the best mean opinion score.</abstract>
    </paper>
    <paper id="442">
      <author><first>Winston N</first><last>Anderson</last></author>
      <author><first>Petronella M</first><last>Kotzé</last></author>
      <title>Finite state tokenisation of an orthographical disjunctive agglutinative language: The verbal segment of <fixed-case>N</fixed-case>orthern <fixed-case>S</fixed-case>otho</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/710_pdf.pdf</url>
      <abstract>Tokenisation is an important first pre-processing step required to adequately test finite-state morphological analysers. In agglutinative languages each morpheme is concatinatively added on to form a complete morphological structure. Disjunctive agglutinative languages like Northern Sotho write these morphemes, for certain morphological categories only, as separate words separated by spaces or line breaks. These breaks are, by their nature, different from breaks that separate words that are written conjunctively. A tokeniser is required to isolate categories, like a verb, from raw text before they can be correctly morphologically analysed. The authors have successfully produced a finite state tokeniser for Northern Sotho, where verb segments are written disjunctively but nominal segments conjunctively. The authors show that since reduplication in the Northern Sotho language does not affect the pre-processing tokeniser, the disjunctive standard verbal segment as a construct in Northern Sotho is deterministic, finite-state and a regular Type 0 language in the Chomsky hierarchy and that the copulative verbal segment, due to its semi-disjunctivism, is ambiguously non-deterministic.</abstract>
    </paper>
    <paper id="443">
      <author><first>Jean-François</first><last>Couturier</last></author>
      <author><first>Sylvain</first><last>Neuvel</last></author>
      <author><first>Patrick</first><last>Drouin</last></author>
      <title>Applying Lexical Constraints on Morpho-Syntactic Patterns for the Identification of Conceptual-Relational Content in Specialized Texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/711_pdf.pdf</url>
      <abstract>In this paper, we describe a formal constraint mechanism, which we label Conceptual Constraint Variables (CCVs), introduced to restrict surface patterns during automated text analysis with the objective of increasing precision in the representation of informational contents. We briefly present, and exemplify, the various types of CCVs applicable to the English texts of our corpora, and show how these constraints allow us to resolve some of the problems inherent to surface pattern recognition, more specifically, those related to the resolution of conceptual or syntactic ambiguities introduced by the most frequent English prepositions.</abstract>
    </paper>
    <paper id="444">
      <author><first>Katerina</first><last>Pastra</last></author>
      <title>Beyond Multimedia Integration: corpora and annotations for cross-media decision mechanisms</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/712_pdf.pdf</url>
      <abstract>In this paper, we look into the notion of cross-media decision mechanisms, focussing on ones that work within multimedia documents for a variety of applications, such as the generation of intelligent multimedia presentations and multimedia indexing. In order for these mechanisms to go beyond the identification of semantic equivalence relations between media, which is what integration does, appropriate corpora and annotations are needed. Drawing from our experience in the REVEAL THIS project, we indicate the characteristics that such corpora should have, and suggest a number of annotations that would allow for training/designing such mechanisms. We conclude with a view on the suitability of two related markup languages (MPEG-7 and EMMA) for accommodating the suggested annotations.</abstract>
    </paper>
    <paper id="445">
      <author><first>Yoshihiko</first><last>Nitta</last></author>
      <author><first>Masashi</first><last>Saraki</last></author>
      <author><first>Satoru</first><last>Ikehara</last></author>
      <title>Building Carefully Tagged Bilingual Corpora to Cope with Linguistic Idiosyncrasy</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/715_pdf.pdf</url>
      <abstract>We illustrate the effectiveness of medium-sized carefully tagged bilingual core corpus, that is, semantic typology patterns in our term together with some examples to give concrete evidence of its usefulness. The most important characteristic of these semantic typology patterns is the bridging mechanism between two languages which is based on sequences syntactic codes and semantic codes. This characteristic gives both wide coverage and flexible applicability of core bilingual core corpus though its volume size is not so large. A further work is to be done for grasping some intuitive feeling of pertinent coarseness and fineness of patterns. Here coarseness feeling is concerning the generalization in phrase-level and clause-level semantic patterns and fineness is concerning word-level semantic patterns. Based on this feeling we will complete the core tagged bilingual corpora while enhancing the necessary support functions and utilities.</abstract>
    </paper>
    <paper id="446">
      <author><first>Roser</first><last>Saurí</last></author>
      <author><first>Marc</first><last>Verhagen</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <title><fixed-case>S</fixed-case>link<fixed-case>ET</fixed-case>: A Partial Modal Parser for Events</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/716_pdf.pdf</url>
      <abstract>We present SlinkET, a parser for identifying contexts of event modality in text developed within the TARSQI (Temporal Awareness and Reasoning Systems for Question Interpretation) research framework. SlinkET is grounded on TimeML, a specification language for capturing temporal and event related information in discourse, which provides an adequate foundation to handle event modality. SlinkET builds on top of a robust event recognizer, and provides each relevant event with a value that specifies the degree of certainty about its factuality; e.g., whether it has happened or holds (factive or counter-factive), whether it is being reported or witnessed by somebody else (evidential), or if it is introduced as a possibility (modal). It is based on well-established technology in the field (namely, finite-state techniques), and informed with corpus-induced knowledge that relies on basic information, such as morphological features, POS, and chunking. SlinkET is under continuing development and it currently achieves a performance ratio of 70% F1-measure.</abstract>
    </paper>
    <paper id="447">
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Mark</first><last>Liberman</last></author>
      <title>More Data and Tools for More Languages and Research Areas: A Progress Report on <fixed-case>LDC</fixed-case> Activities</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/717_pdf.pdf</url>
      <abstract>This presentation reports on recent progress the Linguistic Data Consortium has made in addressing the needs of multiple research communities by collecting, annotating and distributing, simplifying access and developing standards and tools. Specifically, it describes new trends in publication, a sample of recent projects and significant improvements to LDC Online that improve access to LDC data especially for those with limited computing support.</abstract>
    </paper>
    <paper id="448">
      <author id="antonio-bonafonte"><first>A.</first><last>Bonafonte</last></author>
      <author id="harald-hoge"><first>H.</first><last>Höge</last></author>
      <author id="imre-kiss"><first>I.</first><last>Kiss</last></author>
      <author id="asuncion-moreno"><first>A.</first><last>Moreno</last></author>
      <author id="ute-ziegenhain"><first>U.</first><last>Ziegenhain</last></author>
      <author id="henk-van-den-heuvel"><first>H.</first><last>van den Heuvel</last></author>
      <author id="horst-udo-hain"><first>H.-U.</first><last>Hain</last></author>
      <author id="xia-wang"><first>X. S.</first><last>Wang</last></author>
      <author id="marie-neige-garcia"><first>M. N.</first><last>Garcia</last></author>
      <title><fixed-case>TC</fixed-case>-<fixed-case>STAR</fixed-case>:Specifications of Language Resources and Evaluation for Speech Synthesis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/719_pdf.pdf</url>
      <abstract>In the framework of the EU funded project TC-STAR (Technology and Corpora for Speech to Speech Translation),research on TTS aims on providing a synthesized voice sounding like the source speaker speaking the target language. To progress in this direction, research is focused on naturalness, intelligibility, expressivity and voice conversion both, in the TC-STAR framework. For this purpose, specifications on large, high quality TTS databases have been developed and the data have been recorded for UK English, Spanish and Mandarin. The development of speech technology in TC-STAR is evaluation driven. Assessment of speech synthesis is needed to determine how well a system or technique performs in comparison to previous versions as well as other approaches (systems &amp; methods). Apart from testing the whole system, all components of the system will be evaluated separately. This approach grants better assesment of each component as well as identification of the best techniques in the different speech synthesisprocesses.This paper describes the specifications of Language Resources for speech synthesis and the specifications for evaluation of speech synthesis activities.</abstract>
    </paper>
    <paper id="449">
      <author><first>Miriam</first><last>Voghera</last></author>
      <author><first>Francesco</first><last>Cutugno</last></author>
      <title>An observatory on Spoken <fixed-case>I</fixed-case>talian linguistic resources and descriptive standards.</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/720_pdf.pdf</url>
      <abstract>We present the national project Parlare italiano: osservatorio degli usi linguistici, funded by the Italian Ministry of Education, Scientific Research and University (PRIN 2004). Ten research groups participate to the project from various Italian universities. The project has four fundamental objectives: 1) to plan a national website that collects the most recent theoretical and applied results on spoken language; 2) to create an observatory of the linguistic usages of the Italian spoken language; 3) to delineate and implement standard and formalized methods and procedures for the study of spoken language; 4) to develop a training program for young researchers. The website will be accessible starting from November 2006.</abstract>
    </paper>
    <paper id="450">
      <author><first>Kamel</first><last>Smaïli</last></author>
      <author><first>Caroline</first><last>Lavecchia</last></author>
      <author><first>Jean-Paul</first><last>Haton</last></author>
      <title>Linguistic features modeling based on Partial New Cache</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/721_pdf.pdf</url>
      <abstract>The agreement in gender and number is a critical problem in statistical language modeling. One of the main problems in the speech recognition of French language is the presence of misrecognized words due to the bad agreement (in gender and number) between words. Statistical language models do not treat this phenomenon directly. This paper focuses on how to handle the issue of agreements. We introduce an original model called Features-Cache (FC) to estimate the gender and the number of the word to predict. It is a dynamic variable-length Features-Cache for which the size is determined in accordance to syntagm delimitors. This model does not need any syntactic parsing, it is used as any other statistical language model. Several models have been carried out and the best one achieves an improvement of more than 8 points in terms of perplexity.</abstract>
    </paper>
    <paper id="451">
      <author><first>Stefan</first><last>Schulz</last></author>
      <author><first>Kornél</first><last>Markó</last></author>
      <author><first>Philipp</first><last>Daumke</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <author><first>Susanne</first><last>Hanser</last></author>
      <author><first>Percy</first><last>Nohama</last></author>
      <author><first>Roosewelt Leite</first><last>de Andrade</last></author>
      <author><first>Edson</first><last>Pacheco</last></author>
      <author><first>Martin</first><last>Romacker</last></author>
      <title>Semantic Atomicity and Multilinguality in the Medical Domain: Design Considerations for the <fixed-case>M</fixed-case>orpho<fixed-case>S</fixed-case>aurus Subword Lexicon</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/724_pdf.pdf</url>
      <abstract>We present the lexico-semantic foundations underlying a multilingual lexicon the entries of which are constituted by so-called subwords. These subwords reflect semantic atomicity constraints in the medical domain which diverge from canonical lexicological understanding in NLP. We focus here on criteria to identify and delimit reasonable subword units, to group them into functionally adequate synonymy classes and relate them by two types of lexical relations. The lexicon we implemented on the basis of these considerations forms the lexical backbone for MorphoSaurus, a cross-language document retrieval engine for the medical domain.</abstract>
    </paper>
    <paper id="452">
      <author><first>Marko</first><last>Salmenkivi</last></author>
      <title>Finding representative sets of dialect words for geographical regions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/725_pdf.pdf</url>
      <abstract>We investigate a corpus of geographical distributions of 17,126 Finnish dialect words. Our goal is to automatically find sets of words characteristic to geographical regions. Though our approach is related to the problem of dividing the investigation area into linguistically (and geographically) relatively coherent dialect regions, we do not aim at constructing more or less questionable dialect regions. Instead, we let the boundaries of the regions overlap to get insight to the degree of lexical change between adjacent areas. More concretely, we study the applicability of data clustering approaches to find sets of words with tight spatial distributions, and to cluster the extracted distributions according to their distribution areas. The extracted words belonging to the same cluster can then be utilized as a means to characterize the lexicon of the region. We also automatically pick up words with occurrences appearing in two or more areas that are geographically far from each other. These words may give valuable insight to, e.g., the study of cultural history and history of settlement.</abstract>
    </paper>
    <paper id="453">
      <author><first>Olga</first><last>Uryupina</last></author>
      <title>Coreference Resolution with and without Linguistic Knowledge</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/726_pdf.pdf</url>
      <abstract>State-of-the-art statistical approaches to the Coreference Resolution task rely on sophisticated modeling, but very few (10-20) simple features. In this paper we propose to extend the standard feature set substantially, incorporating more linguistic knowledge. To investigate the usability of linguistically motivated features, we evaluate our system for a variety of machine learners on the standard dataset (MUC-7) with the traditional learning set-up.</abstract>
    </paper>
    <paper id="454">
      <author><first>Keith J.</first><last>Miller</last></author>
      <author><first>Michelle</first><last>Vanni</last></author>
      <title>Formal v. Informal: Register-Differentiated <fixed-case>A</fixed-case>rabic <fixed-case>MT</fixed-case> Evaluation in the <fixed-case>PLATO</fixed-case> Paradigm</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/727_pdf.pdf</url>
      <abstract>Tasks performed on machine translation (MT) output are associated with input text types such as genre and topic. Predictive Linguistic Assessments of Translation Output, or PLATO, MT Evaluation (MTE) explores a predictive relationship between linguistic metrics and the information processing tasks reliably performable on output. PLATO assigns a linguistic signature, which cuts across the task-based and automated metric paradigms. Here we report on PLATO assessments of clarity, coherence, morphology, syntax, lexical robustness, name-rendering, and terminology in a comparison of Arabic MT engines in which register differentiates the input. With a team of 10 assessors employing eight linguistic tests, we analyzed the results of five systems processing of 10 input texts from two distinct linguistic registers: a total we analyzed 800 data sets. The analysis pointed to specific areas, such as general lexical robustness, where system performance was comparable on both types of input. Divergent performance, however, was observed on clarity and name-rendering assessments. These results suggest that, while systems may be considered reliable regardless of input register for the lexicon-dependent triage task, register may have an affect on the suitability of MT systems output for relevance judgment and information extraction tasks, which rely on clearness and proper named-entity rendering. Further, we show that the evaluation metrics incorporated in PLATO differentiate between MT systems performance on a text type for which they are presumably optimized and one on which they are not.</abstract>
    </paper>
    <paper id="455">
      <author id="olivier-hamon"><first>O.</first><last>Hamon</last></author>
      <author id="martin-rajman"><first>M.</first><last>Rajman</last></author>
      <title><fixed-case>X</fixed-case>-Score: Automatic Evaluation of Machine Translation Grammaticality</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/729_pdf.pdf</url>
      <abstract>In this paper we report an experiment of an automated metric used to analyse the grammaticality of machine translation output. The approach (Rajman, Hartley, 2001) is based on the distribution of the linguistic information within a translated text, which is supposed similar between a learning corpus and the translation. This method is quite inexpensive, since it does not need any reference translation. First we describe the experimental method and the different tests we used. Then we show the promising results we obtained on the CESTA data, and how they correlate well with human judgments.</abstract>
    </paper>
    <paper id="456">
      <author><first>Roberto</first><last>Navigli</last></author>
      <title>Reducing the Granularity of a Computational Lexicon via an Automatic Mapping to a Coarse-Grained Sense Inventory</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/733_pdf.pdf</url>
      <abstract>WordNet is the reference sense inventory of most of the current Word Sense Disambiguation systems. Unfortunately, it encodes too fine-grained distinctions, making it difficult even for humans to solve the ambiguity of words in context. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense groups, namely the Oxford Dictionary of English. We assess the quality of the mapping and discuss the potential of the method.</abstract>
    </paper>
    <paper id="457">
      <author><first>Dafydd</first><last>Gibbon</last></author>
      <author><first>Flaviane Romani</first><last>Fernandes</last></author>
      <author><first>Thorsten</first><last>Trippel</last></author>
      <title>A <fixed-case>BLARK</fixed-case> extension for temporal annotation mining</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/735_pdf.pdf</url>
      <abstract>The Basic Language Resource Kit (BLARK) proposed by Krauwer is designed for the creation of initial textual resources. There are a number of toolkits for the development of spoken language resources and systems, but tools for second level resources, that is, resources which are the result of processing primary level speech resources such as speech recordings. Typically, processing of this kind in phonetics is done manually, with the aid of spreadsheets multi-purpose statistics software. We propose a Basic Language and Speech Kit (BLAST) as an extension to BLARK and suggest a strategy for integrating the kit into the Natural Language Toolkit (NLTK). The prototype kit is evaluated in an application to examining temporal properties of spoken Brazilian Portuguese.</abstract>
    </paper>
    <paper id="458">
      <author><first>Michael</first><last>Schiehlen</last></author>
      <author><first>Kristina</first><last>Spranger</last></author>
      <title>The Mass-Count Distinction: Acquisition and Disambiguation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/736_pdf.pdf</url>
      <abstract>At least in the realm of fast parsing, the masscount distinction has led the life of a wallflower. We argue in this paper that this should not be so. In particular, we argue, both theoretical linguistics and computational linguistics can gain by a corpus-based investigation of this distinction: Computational linguists get more accurate parses; the knowledge extracted from these parses becomes more reliable; theoretical linguists are presented with new data in a field that has been intensely discussed and yet remains in a state that is not satisfactory from a practical point of view.</abstract>
    </paper>
    <paper id="459">
      <author><first>Andrew W.</first><last>Cole</last></author>
      <title>Corpus Development and Publication</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/738_pdf.pdf</url>
      <abstract>This paper will discuss issues relevant to corpus development and publication at the LDC and will illustrate those issues by examining the history of three LDC corpora. This paper will also briefly examine alternative corpus creation and distribution methods and their challenges. The intent of this paper is to increase the available linguistic resources by describing the regulatory and technical environment and thus improving the understanding and interaction between corpus providers and distributors.</abstract>
    </paper>
    <paper id="460">
      <author><first>Dafydd</first><last>Gibbon</last></author>
      <author><first>Shu-Chuan</first><last>Tseng</last></author>
      <title>Discourse functions of duration in <fixed-case>M</fixed-case>andarin: resource design and implementation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/739_pdf.pdf</url>
      <abstract>A dedicated resource, consisting of annotated speech tools, and workflow design, was developed for the detailed investigation of discourse phenomena in Taiwan Mandarin. The discourse phenomena have functions which are associated with positions in utterances, and temporal properties, and include discourse markers (NAGE, NA, e.g. hesitation, utterance initiation), discourse particles (A, e.g. utterance finality, utterance continuity, focus, etc.), and fillers (UHN, hesitation). The distribution of particles in relation to their position in utterances and the temporal properties of particles are investigated. The results of the investigation diverge considerably from claims in existing grammars of Mandarin with respect to utterance position, and show in general greater length than for regular syllables. These properties suggest the possibility of developing an automatic discourse item tagger.</abstract>
    </paper>
    <paper id="461">
      <author><first>Leonardo</first><last>Lesmo</last></author>
      <author><first>Livio</first><last>Robaldo</last></author>
      <title>From Natural Language to Databases via Ontologies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/740_pdf.pdf</url>
      <abstract>This paper describes an approach to Natural Language access to databases based on ontologies. Their role is to make the central part of the translation process independent both of the specific language and of the particular database schema. The input sentence is parsed and the parse tree is semantically annotated via references to the ontology describing the application. This first step is, of course, language dependent: the parsing process depends on the syntax of the language and the annotation depends on the meaning of words, expressed as links between words and concepts in the ontology. Then, the annotated tree is used to produce an ontological query, i.e. a query expressed in terms of paths on the ontology. This second step is entirely language- and DB-independent. Finally, the ontological query is translated into a standard SQL query, on the basis of a concept-to-DB mapping, specifying how each concept and relation is mapped onto the database.</abstract>
    </paper>
    <paper id="462">
      <author id="adeline-nazarenko"><first>A.</first><last>Nazarenko</last></author>
      <author id="erick-alphonse"><first>E.</first><last>Alphonse</last></author>
      <author id="julien-deriviere"><first>J.</first><last>Derivière</last></author>
      <author id="thierry-hamon"><first>T.</first><last>Hamon</last></author>
      <author id="guillaume-vauvert"><first>G.</first><last>Vauvert</last></author>
      <author id="davy-weissenbacher"><first>D.</first><last>Weissenbacher</last></author>
      <title>The <fixed-case>ALVIS</fixed-case> Format for Linguistically Annotated Documents</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/742_pdf.pdf</url>
      <abstract>The paper describes the ALVIS annotation format and discusses the problems that we encountered for the indexing of large collections of documents for topic specific search engines. This paper is exemplified on the biological domain and on MedLine abstracts, as developing a specialized search engine for biologist is one of the ALVIS case studies. The ALVIS principle for linguistic annotations is based on existing works and standard propositions. We made the choice of stand-off annotations rather than inserted mark-up, and annotations are encoded as XML elements which form the linguistic subsection of the document record.</abstract>
    </paper>
    <paper id="463">
      <author><first>Alexander</first><last>Raake</last></author>
      <author><first>Brian FG</first><last>Katz</last></author>
      <title><fixed-case>US</fixed-case>-based Method for Speech Reception Threshold Measurement in <fixed-case>F</fixed-case>rench</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/744_pdf.pdf</url>
      <abstract>We propose a new method for measuring the threshold of 50% sentence intelligibility in noisy or multi-source speech communication situations (Speech Reception Threshold, SRT). Our SRT-test complements those available e.g. for English, German, Dutch, Swedish and Finnish by a French test method. The approach we take is based on semantically unpredictable sentences (SUS), which can principally be created for various languages. This way, the proposed method enables better cross-language comparisons of intelligibility tests. As a starting point for the French language, a set of 288 sentences (24 lists of 12 sentences each) was created. Each of the 24 lists is optimized for homogeneity in terms of phoneme-distribution as compared to average French, and for word occurrence frequency of the employed monosyllabic keywords as derived from French language databases. Based on the optimized text material, a speech target sentence database has been recorded with a trained speaker. A test calibration was carried out to yield uniform measurement results over the set of target sentences. First intelligibility measurements show good reliability of the method.</abstract>
    </paper>
    <paper id="464">
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Andrew</first><last>Cole</last></author>
      <author><first>Denise</first><last>Dipersio</last></author>
      <author><first>Mark</first><last>Liberman</last></author>
      <author><first>Xiaoyi</first><last>Ma</last></author>
      <author><first>Mohamed</first><last>Maamouri</last></author>
      <author><first>Kazuaki</first><last>Maeda</last></author>
      <title>Integrated Linguistic Resources for Language Exploitation Technologies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/745_pdf.pdf</url>
      <abstract>Linguistic Data Consortium has recently embarked on an effort to create integrated linguistic resources and related infrastructure for language exploitation technologies within the DARPA GALE (Global Autonomous Language Exploitation) Program. GALE targets an end-to-end system consisting of three major engines: Transcription, Translation and Distillation. Multilingual speech or text from a variety of genres is taken as input and English text is given as output, with information of interest presented in an integrated and consolidated fashion to the end user. GALE's goals require a quantum leap in the performance of human language technology, while also demanding solutions that are more intelligent, more robust, more adaptable, more efficient and more integrated. LDC has responded to this challenge with a comprehensive approach to linguistic resource development designed to support GALE's research and evaluation needs and to provide lasting resources for the larger Human Language Technology community.</abstract>
    </paper>
    <paper id="465">
      <author><first>Xiaoyi</first><last>Ma</last></author>
      <title><fixed-case>C</fixed-case>hampollion: A Robust Parallel Text Sentence Aligner</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/746_pdf.pdf</url>
      <abstract>This paper describes Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text. Champollion increases the robustness of the alignment by assigning greater weights to less frequent translated words. Experiments on a manually aligned Chinese  English parallel corpus show that Champollion achieves high precision and recall on noisy data. Champollion can be easily ported to new language pairs. Its freely available to the public.</abstract>
    </paper>
    <paper id="466">
      <author><first>Fabian</first><last>Behrens</last></author>
      <author><first>Jan-Torsten</first><last>Milde</last></author>
      <title>The Eclipse Annotator: an extensible system for multimodal corpus creation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/748_pdf.pdf</url>
      <abstract>The Eclipse-Annotator is an extensible tool for the creation of multimodal language resources. It is based on the TASX-Annotator, which has been refactored in order to fit into the plugin based architecture of the new application.</abstract>
    </paper>
    <paper id="467">
      <author><first>Harris</first><last>Papageorgiou</last></author>
      <author><first>Elina</first><last>Desipri</last></author>
      <author><first>Maria</first><last>Koutsombogera</last></author>
      <author><first>Kanella</first><last>Pouli</last></author>
      <author><first>Prokopis</first><last>Prokopidis</last></author>
      <title>Adding multi-layer semantics to the <fixed-case>G</fixed-case>reek Dependency Treebank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/749_pdf.pdf</url>
      <abstract>In this paper we give an overview of the approach adopted to add a layer of semantic information to the Greek Dependency Treebank [GDT]. Our ultimate goal is to come up with a large corpus, reliably annotated with rich semantic structures. To this end, a corpus has been compiled encompassing various data sources and domains. This collection has been preprocessed, annotated and validated on the basis of dependency representation. Taking into account multi-layered annotation schemes designed to provide deeper representations of structure and meaning, we describe the methodology followed as regards the semantic layer, we report on the annotation process and the problems faced and we conclude with comments on future work and exploitation of the resulting resource.</abstract>
    </paper>
    <paper id="468">
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Vincenzo</first><last>Lombardo</last></author>
      <title>Comparing linguistic information in treebank annotations</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/750_pdf.pdf</url>
      <abstract>The paper investigates the issue of portability of methods and results over treebanks in different languages and annotation formats. In particular, it addresses the problem of converting an Italian treebank, the Turin University Treebank (TUT), developed in dependency format, into the Penn Treebank format, in order to possibly exploit the tools and methods already developed and compare the adequacy of information encoding in the two formats. We describe the procedures for converting the two annotation formats and we present an experiment that evaluates some linguistic knowledge extracted from the two formats, namely sub-categorization frames.</abstract>
    </paper>
    <paper id="469">
      <author><first>Nella</first><last>Cucurullo</last></author>
      <author><first>Simonetta</first><last>Montemagni</last></author>
      <author><first>Matilde</first><last>Paoli</last></author>
      <author><first>Eugenio</first><last>Picchi</last></author>
      <author><first>Eva</first><last>Sassolini</last></author>
      <title>Dialectal resources on-line: the <fixed-case>ALT</fixed-case>-Web experience</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/752_pdf.pdf</url>
      <abstract>The paper presents an on-line dialectal resource, ALT-Web, which gives access to the linguistic data of the Atlante Lessicale Toscano, a specially designed linguistic atlas in which lexical data have both a diatopic and diastratic characterisation. The paper focuses on: the dialectal data representation model; the access modalities to the ALT dialectal corpus; ontology-based search.</abstract>
    </paper>
    <paper id="470">
      <author><first>Xiaoyi</first><last>Ma</last></author>
      <author><first>Christopher</first><last>Cieri</last></author>
      <title>Corpus Support for Machine Translation at <fixed-case>LDC</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/754_pdf.pdf</url>
      <abstract>This paper describes LDC's efforts in collecting, creating and processing different types of linguistic data, including lexicons, parallel text, multiple translation corpora, and human assessment of translation quality, to support the research and development in Machine Translation. Through a combination of different procedures and core technologies, the LDC was able to create very large, high quality, and cost-efficient corpora, which have contributed significantly to recent advances in Machine Translation. Multiple translation corpora and human assessment together facilitate, validate and improve automatic evaluation metrics, which are vital to the development of MT systems. The Bilingual Internet Text Search (BITS) and Champollion sentence aligner enable the finding and processing of large quantities of parallel text. All specifications and tools used by LDC and described in the paper are or will be available to the general public.</abstract>
    </paper>
    <paper id="471">
      <author><first>Ann</first><last>Bies</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <author><first>Haejoong</first><last>Lee</last></author>
      <author><first>Kazuaki</first><last>Maeda</last></author>
      <author><first>Seth</first><last>Kulick</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Mary</first><last>Harper</last></author>
      <author><first>Matthew</first><last>Lease</last></author>
      <title>Linguistic Resources for Speech Parsing</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/755_pdf.pdf</url>
      <abstract>We report on the success of a two-pass approach to annotating metadata, speech effects and syntactic structure in English conversational speech: separately annotating transcribed speech for structural metadata, or structural events, (fillers, speech repairs ( or edit dysfluencies) and SUs, or syntactic/semantic units) and for syntactic structure (treebanking constituent structure and shallow argument structure). The two annotations were then combined into a single representation. Certain alignment issues between the two types of annotation led to the discovery and correction of annotation errors in each, resulting in a more accurate and useful resource. The development of this corpus was motivated by the need to have both metadata and syntactic structure annotated in order to support synergistic work on speech parsing and structural event detection. Automatic detection of these speech phenomena would simultaneously improve parsing accuracy and provide a mechanism for cleaning up transcriptions for downstream text processing. Similarly, constraints imposed by text processing systems such as parsers can be used to help improve identification of disfluencies and sentence boundaries. This paper reports on our efforts to develop a linguistic resource providing both spoken metadata and syntactic structure information, and describes the resulting corpus of English conversational speech.</abstract>
    </paper>
    <paper id="472">
      <author><first>Tomasz</first><last>Obrębski</last></author>
      <author><first>Michał</first><last>Stolarski</last></author>
      <title><fixed-case>UAM</fixed-case> Text Tools - a flexible <fixed-case>NLP</fixed-case> architecture</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/756_pdf.pdf</url>
      <abstract>The paper presents a new language processing toolkit developed at Adam Mickiewicz University. Its functionality includes currently tokenization, sentence splitting, dictionary-based morphological analysis, heuristic morphological analysis of unknown words, spelling correction, pattern search, and generation of concordances. It is organized as a collection of command-line programs, each performing one operation. The components may be connected in various ways to provide various text processing services. Also new user-deoned components may be easily incorporated into the system. The toolkit is destined for processing raw (not annotated) text corpora. The system was originally intended for Polish, but its adaptation to other languages is possible.</abstract>
    </paper>
    <paper id="473">
      <author><first>Markus</first><last>Geilfuss</last></author>
      <author><first>Jan-Torsten</first><last>Milde</last></author>
      <title><fixed-case>SAM</fixed-case> - an annotation editor for parallel texts</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/757_pdf.pdf</url>
      <abstract>Annotated parallel texts are an important resource for quantitative and qualitative linguistic research. Creating parallel corpora enables the generation of (bilingual) lexica, provides a basis for the extraction of data used for translation memories, makes is possible to describe the differences between text versions simply allows scientists to create texts in cooperation. We describe the design and implementation of an interactive editor allowing the user to annotate parallel texts: SAM, the Script Annotation Manager.</abstract>
    </paper>
    <paper id="474">
      <author><first>Hans</first><last>Uszkoreit</last></author>
      <author><first>Feiyu</first><last>Xu</last></author>
      <author><first>Jörg</first><last>Steffen</last></author>
      <author><first>Ilhan</first><last>Aslan</last></author>
      <title>The pragmatic combination of different crosslingual resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/758_pdf.pdf</url>
      <abstract>We will describe new cross-lingual strategies for the development multilingual information services on mobile devices. The novelty of our approach is the intelligent modeling of cross-lingual application domains and the combination of textual translation with speech generation. The final system helps users to speak foreign languages and communicate with the local people in relevant situations, such as restaurant, taxi and emergencies. The advantage of our information services is that they are robust enough for the use in real-world situations. They are developed for the Beijing Olympic Games 2008, where most foreigners will have to rely on translation assistance. Their deployment is foreseen as part of the planned ubiquitous mobile information system of the Olympic Games.</abstract>
    </paper>
    <paper id="475">
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Clinton</first><last>Mah</last></author>
      <author><first>Sabiha</first><last>Imran</last></author>
      <author><first>Randy</first><last>Calistri-Yeh</last></author>
      <author><first>Páraic</first><last>Sheridan</last></author>
      <title>Design, Construction and Validation of an <fixed-case>A</fixed-case>rabic-<fixed-case>E</fixed-case>nglish Conceptual Interlingua for Cross-lingual Information Retrieval</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/759_pdf.pdf</url>
      <abstract>This paper describes the issues involved in extending a trans-lingual lexicon, the TextWise Conceptual Interlingua (CI), with Arabic terms. The Conceptual Interlingua is based on the Princeton English WordNet (Fellbaum, 1998). It is a central component in the cross-lingual information retrieval (CLIR) system CINDOR (Conceptual INterlingua for DOcument Retrieval). Arabic has a rich morphological system combining templatic and affixational paradigms for both inflectional and derivational morphology. This rich morphology poses a major challenge to the design and building of the Arabic CI and also its validation. This is because the available resources for Arabic, whether manually constructed bilingual lexicons or lexicons automatically derived from bilingual parallel corpora, exist at different levels of morphological representation. We describe here the issues and decisions made in the design and construction of the Arabic-English CI using different types of manual and automatic resources. We also present the results of an extensive validation of the Arabic CI and briefly discuss the evaluation of its use for CLIR on the TREC Arabic Benchmark collection.</abstract>
    </paper>
    <paper id="476">
      <author><first>Grażyna</first><last>Vetulani</last></author>
      <author><first>Zygmunt</first><last>Vetulani</last></author>
      <author><first>Tomasz</first><last>Obrębski</last></author>
      <title>Syntactic Lexicon of <fixed-case>P</fixed-case>olish Predicative Nouns</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/760_pdf.pdf</url>
      <abstract>In the paper we report realization of SyntLex project aiming at construction of a full lexicon grammar for Polish. The lexicon-grammar based paradigm in computer linguistics is derived from the predicate logic and attributes a central role to the predicative constructions. An important class of syntactic constructions in many languages (French, English, Polish and other Slavonic languages in particular) are those based on verbo-nominal collocations, with the verb playing a support role with respect to the noun considered as carrying the predicative information. In this paper we refer to the former research by one of the authors aiming at full description of verbo-nominal predicative constructions for Polish in the form of an electronic resource for LI applications. We describe procedures to complete and corpus-validate the resource obtained so far.</abstract>
    </paper>
    <paper id="477">
      <author><first>Antonietta</first><last>Alonge</last></author>
      <title>The <fixed-case>I</fixed-case>talian Metaphor Database</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/762_pdf.pdf</url>
      <abstract>This paper describes the main features of the Italian Metaphor Database, buing built at the University of Perugia (Italy). The database is being developed as a resource to be used both as a knowledge base on conceptual metaphors in Italian and their lexical expressions, and to enrich general lexical resources. The reason to develop such a database is that most NLP systems have to deal with metaphorical expressions sooner or later but, as previous research has shown, existing lexical resources for Italian do not contain complete and consistent data on metaphors, empirically derived but theoretically motivated. Thus, by referring to the Cognitive Theory of metaphor, conceptual metaphors instantiated in Italian are being represented in the resource, together with data on the way they are expressed in the language (i.e., through lexical units or multiword expressions), examples of them found within a corpus, and data on metaphorical linguistic expressions encoded/missing within ItalWordNet.</abstract>
    </paper>
    <paper id="478">
      <author><first>José</first><last>Iria</last></author>
      <author><first>Fabio</first><last>Ciravegna</last></author>
      <title>A Methodology and Tool for Representing Language Resources for Information Extraction</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/765_pdf.pdf</url>
      <abstract>In recent years there has been a growing interest in clarifying the process of Information Extraction (IE) from documents, particularly when coupled with Machine Learning. We believe that a fundamental step forward in clarifying the IE process would be to be able to perform comparative evaluations on the use of different representations. However, this is difficult because most of the time the way information is represented is too tightly coupled with the algorithm at an implementation level, making it impossible to vary representation while keeping the algorithm constant. A further motivation behind our work is to reduce the complexity of designing, developing and testing IE systems. The major contribution of this work is in defining a methodology and providing a software infrastructure for representing language resources independently of the algorithm, mainly for Information Extraction but with application in other fields - we are currently evaluating its use for ontology learning and document classification.</abstract>
    </paper>
    <paper id="479">
      <author><first>Harry</first><last>Halpin</last></author>
      <title>Automatic Evaluation and Composition of <fixed-case>NLP</fixed-case> Pipelines with Web Services</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/767_pdf.pdf</url>
      <abstract>We describe the innovative use of describing an existing natural language pipeline using the Semantic Web, and focus on how the performance and results of the components may be described. Earlier work has shown how NLP Web Services can be automatically composed via Semantic Web Service composition, and once the results of NLP components can be stored directly, they can also be used to direct the composition, leading to advances in the sharing and evaluation of NLP resources.</abstract>
    </paper>
    <paper id="480">
      <author><first>Harry</first><last>Bunt</last></author>
      <author><first>Amanda</first><last>Schiffrin</last></author>
      <title>Methodological Aspects of Semantic Annotation</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/769_pdf.pdf</url>
      <abstract>This paper constitutes a preliminary report on the work carried out on semantic content annotation in the LIRICS project, in close collaboration with the activities of ISO TC 37/SC 4/TDG 31. This consists primarily of: (1) identifying commonalities in alternative approaches to the annotation and representation of various types of semantic information; and (2) developing methodological principles and concepts for identifying and characterising representational concepts for semantic content. The LIRICS project does not aim to develop a standard format for the annotation and representation of semantic content, but at providing well-defined descriptive concepts. In particular, the aim is to build an on-line registry of definitions of such concepts, called data categories, in accordance with ISO standard 12620. These semantic data categories are abstract concepts, whose use is not restricted to any particular format or representation language. We advocate the use of the metamodel as a tool to extract the most important of these abstract overarching concepts, with examples from dialogue act, temporal, reference and semantic role annotation.</abstract>
    </paper>
    <paper id="481">
      <author><first>Whitney</first><last>Gegg-Harrison</last></author>
      <author><first>Donna K.</first><last>Byron</last></author>
      <title><fixed-case>PYCOT</fixed-case>: An <fixed-case>O</fixed-case>ptimality <fixed-case>T</fixed-case>heory-based Pronoun Resolution Toolkit</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/770_pdf.pdf</url>
      <abstract>In this paper, we present PYCOT, a pronoun resolution toolkit. This toolkit is written in the Python programming language and is intended to be an addition to the open-source NLTK collection of natural language processing tools. We discuss the design of the module as well as studies of its performance on pronoun resolution in English and in Korean.</abstract>
    </paper>
    <paper id="482">
      <author><first>Emma</first><last>Barker</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>François</first><last>Mairesse</last></author>
      <author><first>Robert</first><last>Gaizauskas</last></author>
      <author><first>Marilyn</first><last>Walker</last></author>
      <author><first>Jonathan</first><last>Foster</last></author>
      <title>Simulating Cub Reporter Dialogues: The collection of naturalistic human-human dialogues for information access to text archives</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/772_pdf.pdf</url>
      <abstract>This paper describes a dialogue data collection experiment and resulting corpus for dialogues between a senior mobile journalist and a junior cub reporter back at the office. The purpose of the dialogue is for the mobile journalist to collect background information in preparation for an interview or on-the-site coverage of a breaking story. The cub reporter has access to text archives that contain such background information. A unique aspect of these dialogues is that they capture information-seeking behavior for an open-ended task against a large unstructured data source. Initial analyses of the corpus show that the experimental design leads to real-time, mixedinitiative, highly interactive dialogues with many interesting properties.</abstract>
    </paper>
    <paper id="483">
      <author><first>Jeongwoo</first><last>Ko</last></author>
      <author><first>Laurie</first><last>Hiyakumoto</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <title>Exploiting Multiple Semantic Resources for Answer Selection</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/774_pdf.pdf</url>
      <abstract>This paper describes the utility of semantic resources such as the Web, WordNet and gazetteers in the answer selection process for a question-answering system. In contrast with previous work using individual semantic resources to support answer selection, our work combines multiple resources to boost the confidence scores assigned to correct answers and evaluates different combination strategies based on unweighted sums, weighted linear combinations, and logistic regression. We apply our approach to select answers from candidates produced by three different extraction techniques of varying quality, focusing on TREC questions whose answers represent locations or proper-names. Our experimental results demonstrate that the combination of semantic resources is more effective than individual resources for all three extraction techniques, improving answer selection accuracy by as much as 32.35% for location questions and 72% for proper-name questions. Of the combination strategies tested, logistic regression models produced the best results for both location and proper-name questions.</abstract>
    </paper>
    <paper id="484">
      <author><first>Kazuaki</first><last>Maeda</last></author>
      <author><first>Christopher</first><last>Cieri</last></author>
      <author><first>Kevin</first><last>Walker</last></author>
      <title>Low-cost Customized Speech Corpus Creation for Speech Technology Applications</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/776_pdf.pdf</url>
      <abstract>Speech technology applications, such as speech recognition, speech synthesis, and speech dialog systems, often require corpora based on highly customized specifications. Existing corpora available to the community, such as TIMIT and other corpora distributed by LDC and ELDA, do not always meet the requirements of such applications. In such cases, the developers need to create their own corpora. The creation of a highly customized speech corpus, however, could be a very expensive and time-consuming task, especially for small organizations. It requires multidisciplinary expertise in linguistics, management and engineering as it involves subtasks such as the corpus design, human subject recruitment, recording, quality assurance, and in some cases, segmentation, transcription and annotation. This paper describes LDC's recent involvement in the creation of a low-cost yet highly-customized speech corpus for a commercial organization under a novel data creation and licensing model, which benefits both the particular data requester and the general linguistic data user community.</abstract>
    </paper>
    <paper id="485">
      <author><first>John</first><last>Niekrasz</last></author>
      <author><first>Alexander</first><last>Gruenstein</last></author>
      <title><fixed-case>NOMOS</fixed-case>: A Semantic Web Software Framework for Annotation of Multimodal Corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/777_pdf.pdf</url>
      <abstract>We present NOMOS, an open-source software framework for annotation, processing, and analysis of multimodal corpora. NOMOS is designed for use by annotators, corpus developers, and corpus consumers, emphasizing configurability for a variety of specific annotation tasks. Its features include synchronized multi-channel audio and video playback, compatibility with several corpora, platform independence, and mixed display of capabilities and a well-defined method for layering datasets. Second, we describe how the system is used. For corpus development and annotation we present a typical use scenario involving the creation of a schema and specialization of the user interface. For processing and analysis we describe the GUI- and Java-based methods available, including a GUI for query construction and execution, and an automatically generated schema-conforming Java API for processing of annotations. Additionally, we present some specific annotation and research tasks for which NOMOS has been specialized and used, annotation and research tasks for which NOMOS has been specialized and used, including topic segmentation and decision-point annotation of meetings.</abstract>
    </paper>
    <paper id="486">
      <author><first>Christoph</first><last>Benzmüller</last></author>
      <author><first>Helmut</first><last>Horacek</last></author>
      <author><first>Henri</first><last>Lesourd</last></author>
      <author><first>Ivana</first><last>Kruijff-Korbayova</last></author>
      <author><first>Marvin</first><last>Schiller</last></author>
      <author><first>Magdalena</first><last>Wolska</last></author>
      <title>A corpus of tutorial dialogs on theorem proving; the influence of the presentation of the study-material</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/778_pdf.pdf</url>
      <abstract>We present a new corpus of tutorial dialogs on mathematical theorem proving that was collected in a Wizard-of-Oz setup. Our study is a follow up on a previous experiment conducted in a similar simulated environment. A major difference between the current and the previous experimental setup was that in this study we varied the presentation of the study-material with which the subjects were provided. One sub-group of the subjects was presented with a highly formalized presentation consisting mainly of formulas, while the other with a presentation mainly in natural language. Our goal was to obtain more data on the kind of mixed-language that is characteristic of informal mathematical discourse. We hypothesized that the language style of the subjects' interaction with the simulated system will reflect the style of presentation of the study-material. In the paper we briefly present the experimental setup, the corpus, and a preliminary quantitative result of the corpus analysis.</abstract>
    </paper>
    <paper id="487">
      <author><first>Jamal</first><last>Laoudi</last></author>
      <author><first>Calandra R.</first><last>Tate</last></author>
      <author><first>Clare R.</first><last>Voss</last></author>
      <title>Task-based <fixed-case>MT</fixed-case> Evaluation: From Who/When/Where Extraction to Event Understanding</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/779_pdf.pdf</url>
      <abstract>Task-based machine translation (MT) evaluation asks, how well do people perform text-handling tasks given MT output? This method of evaluation yields an extrinsic assessment of an MT engine, in terms of users task performance on MT output. While this method is time-consuming, its key advantage is that MT users and stakeholders understand how to interpret the assessment results. Prior experiments showed that subjects can extract individual who-, when-, and where-type elements of information from MT output passages that were not especially fluent. This paper presents the results of a pilot study to assess a slightly more complex task: when given such wh-items already identified in an MT output passage, how well can subjects properly select from and place these items into wh-typed slots to complete a sentence-template about the passages event? The results of the pilot with nearly sixty subjects, while only preliminary, indicate that this task was extremely challenging: given six test templates to complete, half of the subjects had no completely correct templates and 42% had exactly one completely correct template. The provisional interpretation of this pilot study is that event-based template completion defines a task ceiling, against which to evaluate future improvements on MT engines.</abstract>
    </paper>
    <paper id="488">
      <author><first>Kazuaki</first><last>Maeda</last></author>
      <author><first>Haejoong</first><last>Lee</last></author>
      <author><first>Julie</first><last>Medero</last></author>
      <author><first>Stephanie</first><last>Strassel</last></author>
      <title>A New Phase in Annotation Tool Development at the <fixed-case>L</fixed-case>inguistic <fixed-case>D</fixed-case>ata <fixed-case>C</fixed-case>onsortium: The Evolution of the Annotation Graph Toolkit</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/780_pdf.pdf</url>
      <abstract>The Linguistic Data Consortium (LDC) has created various annotated linguistic data for a variety of common task evaluation programs and projects to create shared linguistic resources. The majority of these annotated linguistic data were created with highly customized annotation tools developed at LDC. The Annotation Graph Toolkit (AGTK) has been used as a primary infrastructure for annotation tool development at LDC in recent years. Thanks to the direct feedback from annotation task designers and annotators in-house, annotation tool development at LDC has entered a new, more mature and productive phase. This paper describes recent additions to LDC's annotation tools that are newly developed or significantly improved since our last report at the Fourth International Conference on Language Resource and Evaluation Conference in 2004. These tools are either directly based on AGTK or share a common philosophy with other AGTK tools.</abstract>
    </paper>
    <paper id="489">
      <author><first>Hideki</first><last>Shima</last></author>
      <author><first>Mengqiu</first><last>Wang</last></author>
      <author><first>Frank</first><last>Lin</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <title>Modular Approach to Error Analysis and Evaluation for Multilingual Question Answering</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/782_pdf.pdf</url>
      <abstract>Multilingual Question Answering systems are generally very complex, integrating several sub-modules to achieve their result. Global metrics (such as average precision and recall) are insufficient when evaluating the performance of individual sub-modules and their influence on each other. In this paper, we present a modular approach to error analysis and evaluation; we use manually-constructed, gold-standard input for each module to obtain an upper-bound for the (local) performance of that module. This approach enables us to identify existing problem areas quickly, and to target improvements accordingly.</abstract>
    </paper>
    <paper id="490">
      <author><first>Jeongwoo</first><last>Ko</last></author>
      <author><first>Fumihiko</first><last>Murase</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <author><first>Masahiko</first><last>Tateishi</last></author>
      <author><first>Ichiro</first><last>Akahori</last></author>
      <title>Analyzing the Effects of Spoken Dialog Systems on Driving Behavior</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/783_pdf.pdf</url>
      <abstract>This paper presents an evaluation of a spoken dialog system for automotive environments. Our overall goal was to measure the impact of user-system interaction on the users driving performance, and to determine whether adding context-awareness to the dialog system might reduce the degree of user distraction during driving. To address this issue, we incorporated context-awareness into a spoken dialog system, and implemented three system features using user context, network context and dialog context. A series of experiments were conducted under three different configurations: driving without a dialog system, driving while using a context-aware dialog system, and driving while using a context-unaware dialog system. We measured the differences between the three configurations by comparing the average car speed, the frequency of speed changes and the angle between the cars direction and the centerline on the road. These results indicate that context-awareness could reduce the degree of user distraction when using a dialog system during driving.</abstract>
    </paper>
    <paper id="491">
      <author><first>Magesh</first><last>Balasubramanya</last></author>
      <author><first>Michael</first><last>Higgins</last></author>
      <author><first>Peter</first><last>Lucas</last></author>
      <author><first>Jeff</first><last>Senn</last></author>
      <author><first>Dominic</first><last>Widdows</last></author>
      <title>Collaborative Annotation that Lasts Forever: Using Peer-to-Peer Technology for Disseminating Corpora and Language Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/787_pdf.pdf</url>
      <abstract>This paper describes a peer-to-peer architecture for representing and disseminating linguistic corpora, linguistic annotation, and resources such as lexical databases and gazetteers. The architecture is based upon a Universal Database technology in which all information is represented in globally identified, extensible bundles of attribute-value pairs. These objects are replicated at will between peers in the network, and the business rules that implement replication involve checking digital signatures and proper attribution of data, to avoid information being tampered with or abuse of copyright. Universal identifiers enable comprehensive standoff annotation and commentary. A carefully constructed publication mechanism is described that enables different users to subscribe to material provided by trusted publishers on recognized topics or themes. Access to content and related annotation is provided by distributed indexes, represented using the same underlying data objects as the rest of the database.</abstract>
    </paper>
    <paper id="492">
      <author><first>Vasile</first><last>Rus</last></author>
      <author><first>Art</first><last>Graesser</last></author>
      <title>The Look and Feel of a Confident Entailer</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/788_pdf.pdf</url>
      <abstract>The paper presents a software system that embodies a lexico-syntactic approach to the task of Textual Entailment. Although the approach is based on a minimal set of resources it is highly confident. The architecture of the system is open and can be easily expanded with more and deeper processing modules. Results on a standard data set are presented.</abstract>
    </paper>
    <paper id="493">
      <author><first>Gregory</first><last>Marton</last></author>
      <author><first>Boris</first><last>Katz</last></author>
      <title>Using Semantic Overlap Scoring in Answering <fixed-case>TREC</fixed-case> Relationship Questions</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/790_pdf.pdf</url>
      <abstract>A first step in answering complex questions, such as those in the Relationship' task of the Text REtrieval Conference's Question Answering track (TREC/QA), is finding passages likely to contain pieces of the answer---passage retrieval. We introduce semantic overlap scoring, a new passage retrieval algorithm that facilitates credit assignment for inexact matches between query and candidate answer. Our official submission ranked best among fully automatic systems, at 23% F-measure, while the best system, with manual input, reached 28%. We use our Nuggeteer tool to robustly evaluate each component of our Relationship system post hoc. Ablation studies show that semantic overlap scoring achieves significant performance improvements over a standard passage retrieval baseline.</abstract>
    </paper>
    <paper id="494">
      <author><first>Finley</first><last>Lacatusu</last></author>
      <author><first>Andrew</first><last>Hickl</last></author>
      <author><first>Sanda</first><last>Harabagiu</last></author>
      <title>Impact of Question Decomposition on the Quality of Answer Summaries</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/792_pdf.pdf</url>
      <abstract>Generating answers to complex questions in the form of multi-document summaries requires access to question decomposition methods. In this paper we present three methods for decomposing complex questions and we evaluate their impact on the responsiveness of the answers they enable.</abstract>
    </paper>
    <paper id="495">
      <author><first>Sanda</first><last>Harabagiu</last></author>
      <author><first>Cosmin Adrian</first><last>Bejan</last></author>
      <title>An Answer Bank for Temporal Inference</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/794_pdf.pdf</url>
      <abstract>Answering questions that ask about temporal information involves several forms of inference. In order to develop question answering capabilities that benefit from temporal inference, we believe that a large corpus of questions and answers that are discovered based on temporal information should be available. This paper describes our methodology for creating AnswerTime-Bank, a large corpus of questions and answers on which Question Answering systems can operate using complex temporal inference.</abstract>
    </paper>
    <paper id="496">
      <author><first>Paul C.</first><last>Morărescu</last></author>
      <title>Principles for annotating and reasoning with spatial information</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/795_pdf.pdf</url>
      <abstract>In this paper we present the first phase of the ongoing SpaceBank project that attempts to create a linguistic resource for annotating and reasoning with spatial information from text. SpaceBank is the spatial counterpart of TimeBank, an electronic resource for temporal semantics and reasoning. The paper focuses on building an ontology of lexicalized spatial concepts. The textual occurrences of the concepts in this ontology will be annotated using the SpaceML language, briefly described here. SpaceBank is designed to be integrated with TimeBank, for a spatio-temporal model of the textual information.</abstract>
    </paper>
    <paper id="497">
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Qin</first><last>Lu</last></author>
      <author><first>Wenjie</first><last>Li</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <title>Interaction between Lexical Base and Ontology with Formal Concept Analysis</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/796_pdf.pdf</url>
      <abstract>An ontology describes conceptual knowledge in a specific domain. A lexical base collects a repository of words and gives independent definition of concepts. In this paper, we propose to use FCA as a tool to help constructing an ontology through an existing lexical base. We mainly address two issues. The first issue is how to select attributes to visualize the relations between lexical terms. The second issue is how to revise lexical definitions through analysing the relations in the ontology. Thus the focus is on the effect of interaction between a lexical base and an ontology for the purpose of good ontology construction. Finally, experiments have been conducted to verify our ideas.</abstract>
    </paper>
    <paper id="498">
      <author><first>Rachada</first><last>Kongkachandra</last></author>
      <author><first>Kosin</first><last>Chamnongthai</last></author>
      <title>Semantic-Based Keyword Recovery Function for Keyword Extraction System</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/798_pdf.pdf</url>
      <abstract>The goal of implementing a keyword extraction system is to increase as near as 100% of precision and recall. These values are affected by the amount of extracted keywords. There are two groups of errors happened i.e. false-rejected and false-accepted keywords. To improve the performance of the system, false-rejected keywords should be recovered and the false-accepted keywords should be reduced. In this paper, we enhance the conventional keyword extraction systems by attaching the keyword recovery function. This function recovers the previously false-rejected keywords by comparing their semantic information with the contents of each relevant document. The function is automated in three processes i.e. Domain Identification, Knowledge Base Generation and Keyword Determination. Domain identification process identifies domain of interest by searching domains from domain knowledge base by using extracted keywords.The most general domains are selected and then used subsequently. To recover the false-rejected keywords, we match them with keywords in the identified domain within the domain knowledge base rely on their semantics by keyword determination process. To semantically recover keywords, definitions of false-reject keywords and domain knowledge base are previously represented in term of conceptual graph by knowledge base generator process. To evaluate the performance of the proposed function, EXTRACTOR, KEA and our keyword-database-mapping based keyword extractor are compared. The experiments were performed in two modes i.e. training and recovering. In training mode, we use four glossaries from the Internet and 60 articles from the summary sections of IEICE transaction. While in the recovering mode, 200 texts from three resources i.e. summary section of 15 chapters in a computer textbook and articles from IEICE and ACM transactions are used. The experimental results revealed that our proposed function improves the precision and recall rates of the conventional keyword extraction systems approximately 3-5% of precision and 6-10% of recall, respectively.</abstract>
    </paper>
    <paper id="499">
      <author><first>Ruifeng</first><last>Xu</last></author>
      <author><first>Qin</first><last>Lu</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <title>The Design and Construction of A <fixed-case>C</fixed-case>hinese Collocation Bank</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/799_pdf.pdf</url>
      <abstract>This paper presents an annotated Chinese collocation bank developed at the Hong Kong Polytechnic University. The definition of collocation with good linguistic consistency and good computational operability is first discussed and the properties of collocations are then presented. Secondly, based on the combination of different properties, collocations are classified into four types. Thirdly, the annotation guideline is presented. Fourthly, the implementation issues for collocation bank construction are addressed including the annotation with categorization, dependency and contextual information. Currently, the collocation bank is completed for 3,643 headwords in a 5-million-word corpus.</abstract>
    </paper>
    <paper id="500">
      <author><first>Nilda</first><last>Ruimy</last></author>
      <title>Merging two Ontology-based Lexical Resources</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/800_pdf.pdf</url>
      <abstract>ItalWordNet (IWN) and PAROLE/SIMPLE/CLIPS (PSC), the two largest electronic, general-purpose lexical resources of Italian language present many compatible aspects although they are based on two different lexical models having their own underlying principles and peculiarities. Such compatibility prompted us to study the feasibility of semi-automatically linking and eventually merging the two lexicons. To this purpose, the mapping of the ontologies on which basis both lexicons are structured was performed and the sets of semantic relations enabling to relate lexical units were compared. An overview of this preliminary phase is provided in this paper. The linking methodology and related problematic issues are described. Beyond the advantage for the end user to dispose of a more exhaustive and in-depth lexical information combining the potentialities and most outstanding features offered by the two lexical models, resulting benefits and enhancements for the two resources are illustrated that definitely legitimize the soundness of this linking and merging initiative.</abstract>
    </paper>
    <paper id="501">
      <author><last>Nimaan</last><first>Abdillahi</first></author>
      <author><last>Nocera</last><first>Pascal</first></author>
      <author><last>Bonastre</last><first>Jean-François</first></author>
      <title>Towards automatic transcription of <fixed-case>S</fixed-case>omali language</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/801_pdf.pdf</url>
      <abstract>Most African countries follow an oral tradition system to transmit their cultural, scientific and historic heritage through generations. This ancestral knowledge accumulated during centuries is today threatened of disappearing. This paper presents the first steps in the building of an automatic speech to text transcription for African oral patrimony, particularly the Djibouti cultural heritage. This work is dedicated to process Somali language, which represents half of the targeted Djiboutian audio archives. The main problem is the lack of annotated audio and textual resources for this language. We describe the principal characteristics of audio (10 hours) and textual (3M words) training corpora collected. Using the large vocabulary speech recognizer engine, Speeral, developed at the Laboratoire Informatique dAvignon (LIA) (computer science laboratory of Avignon), we obtain about 20.9% word error rate (WER). This is an encouraging result, considering the small size of our corpora. This first recognizer of Somali language will serve as a reference and will be used to transcribe some Djibouti cultural archives. We will also discuss future ways of research like sub-words indexing of audio archives, related to the specificities of the Somali language.</abstract>
    </paper>
    <paper id="502">
      <author><first>Susanne</first><last>Burger</last></author>
      <author><first>Zachary A.</first><last>Sloane</last></author>
      <author><first>Jie</first><last>Yang</last></author>
      <title>Competitive Evaluation of Commercially Available Speech Recognizers in Multiple Languages</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/802_pdf.pdf</url>
      <abstract>Recent improvements in speech recognition technology have resulted in products that can now demonstrate commercial value in a variety of applications. Many vendors are marketing products which combine ASR applications including continuous dictation, command-and-control interfaces, and transcription of recorded speech at an accuracy of 98%. In this study, we measured the accuracy of certain commercially available desktop speech recognition engines in multiple languages. Using word error rate as a benchmark, this work compares recognition accuracy across eight languages and the products of three manufacturers. Results show that two systems performed almost the same while a third system recognized at lower accuracy, although none of the systems reached the claimed accuracy. Read speech was recognized better than spontaneous speech. The systems for US-English, Japanese and Spanish showed higher accuracy than the systems for UK-English, German, French and Chinese.</abstract>
    </paper>
    <paper id="503">
      <author><first>Kornel</first><last>Laskowski</last></author>
      <author><first>Susanne</first><last>Burger</last></author>
      <title>Annotation and Analysis of Emotionally Relevant Behavior in the <fixed-case>ISL</fixed-case> Meeting Corpus</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/803_pdf.pdf</url>
      <abstract>We present an annotation scheme for emotionally relevant behavior at the speaker contribution level in multiparty conversation. The scheme was applied to a large, publicly available meeting corpus by three annotators, and subsequently labeled with emotional valence. We report inter-labeler agreement statistics for the two schemes, and explore the correlation between speaker valence and behavior, as well as that between speaker valence and the previous speaker's behavior. Our analyses show that the co-occurrence of certain behaviors and valence classes significantly deviates from what is to be expected by chance; in isolated cases, behaviors are predictive of valence.</abstract>
    </paper>
    <paper id="504">
      <author><first>Sabri</first><last>Elkateb</last></author>
      <author><first>William</first><last>Black</last></author>
      <author><first>Horacio</first><last>Rodríguez</last></author>
      <author><first>Musa</first><last>Alkhalifa</last></author>
      <author><first>Piek</first><last>Vossen</last></author>
      <author><first>Adam</first><last>Pease</last></author>
      <author><first>Christiane</first><last>Fellbaum</last></author>
      <title>Building a <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et for <fixed-case>A</fixed-case>rabic</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/805_pdf.pdf</url>
      <abstract>This paper introduces a recently initiated project that focuses on building a lexical resource for Modern Standard Arabic based on the widely used Princeton WordNet for English (Fellbaum, 1998). Our aim is to develop a linguistic resource with a deep formal semantic foundation in order to capture the richness of Arabic as described in Elkateb (2005). Arabic WordNet is being constructed following methods developed for EuroWordNet (Vossen, 1998). In addition to the standard wordnet representation of senses, word meanings are also being defined with a machine understandable semantics in first order logic. The basis for this semantics is the Suggested Upper Merged Ontology and its associated domain ontologies (Niles and Pease, 2001). We will greatly extend the ontology and its set of mappings to provide formal terms and definitions for each synset. Tools to be developed as part of this effort include a lexicographer's interface modeled on that used for EuroWordNet, with added facilities for Arabic script, following Black and Elkateb's earlier work (2004).</abstract>
    </paper>
    <paper id="505">
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Pierre</first><last>Boullier</last></author>
      <title>Deep non-probabilistic parsing of large corpora</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/806_pdf.pdf</url>
      <abstract>This paper reports a large-scale non-probabilistic parsing experiment with a deep LFG parser. We briefly introduce the parser we used, named SXLFG, and the resources that were used together with it. Then we report quantitative results about the parsing of a multi-million word journalistic corpus. We show that we can parse more than 6 million words in less than 12 hours, only 6.7% of all sentences reaching the 1s timeout. This shows that deep large-coverage non-probabilistic parsers can be efficient enough to parse very large corpora in a reasonable amount of time.</abstract>
    </paper>
    <paper id="506">
      <author><first>Magnar</first><last>Brekke</last></author>
      <author><first>Kai</first><last>Innselset</last></author>
      <author><first>Marita</first><last>Kristiansen</last></author>
      <author><first>Kari</first><last>Øvsthus</last></author>
      <title>Automatic Term Extraction from Knowledge Bank of Economics</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/807_pdf.pdf</url>
      <abstract>KB-N is a web-accessible searchable Knowledge Bank comprising A) a parallel corpus of quality assured and calibrated English and Norwegian text drawn from economic-administrative knowledge domains, and B) a domain-focused database representing that knowledge universe in terms of defined concepts and their respective bilingual terminological entries. A central mechanism in connecting A and B is an algorithm for the automatic extraction of term candidates from aligned translation pairs on the basis of linguistic, lexical and statistical filtering (first ever for Norwegian). The system is designed and programmed by Paul Meurer at Aksis (UiB). An important pilot application of the term base is subdomain and collocations based word-sense disambiguation for LOGON, a system for Norwegian-to-English MT currently being developed.</abstract>
    </paper>
    <paper id="507">
      <author><first>Alex</first><last>Klassmann</last></author>
      <author><first>Freddy</first><last>Offenga</last></author>
      <author><first>Daan</first><last>Broeder</last></author>
      <author><first>Romuald</first><last>Skiba</last></author>
      <author><first>Peter</first><last>Wittenburg</last></author>
      <title>Comparison of Resource Discovery Methods</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/808_pdf.pdf</url>
      <abstract>It is an ongoing debate whether categorical systems created by some experts are an appropriate way to help users finding useful resources in the internet. However for the much more restricted domain of language documentation such a category system might still prove reasonable if not indispensable. This article gives an overview over the particular IMDI category set and presents a rough evaluation of its practical use at the Max-Planck-Institute Nijmegen.</abstract>
    </paper>
    <paper id="508">
      <author><first>Peter</first><last>Lucas</last></author>
      <author><first>Magesh</first><last>Balasubramanya</last></author>
      <author><first>Dominic</first><last>Widdows</last></author>
      <author><first>Michael</first><last>Higgins</last></author>
      <title>The Information Commons Gazetteer</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/809_pdf.pdf</url>
      <abstract>Advances in location aware computing and the convergence of geographic and textual information systems will require a comprehensive, extensible, information rich framework called the Information Commons Gazetteer that can be freely disseminated to small devices in a modular fashion. This paper describes the infrastructure and datasets used to create such a resource. The Gazetteer makes use of MAYA Design's Universal Database Architecture; a peer-to-peer system based upon bundles of attribute-value pairs with universally unique identity, and sophisticated indexing and data fusion tools. The Gazetteer primarily constitutes publicly available geographic information from various agencies that is organized into a well-defined scalable hierarchy of worldwide administrative divisions and populated places. The data from various sources are imported into the commons incrementally and are fused with existing data in an iterative process allowing for rich information to evolve over time. Such a flexible and distributed public resource of the geographic places and place names allows for both researchers and practitioners to realize location aware computing in an efficient and useful way in the near future by eliminating redundant time consuming fusion of disparate sources.</abstract>
    </paper>
    <paper id="509">
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Lionel</first><last>Clément</last></author>
      <author><first>Éric</first><last>Villemonte de La Clergerie</last></author>
      <author><first>Pierre</first><last>Boullier</last></author>
      <title>The Lefff 2 syntactic lexicon for <fixed-case>F</fixed-case>rench: architecture, acquisition, use</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/810_pdf.pdf</url>
      <abstract>In this paper, we introduce a new lexical resource for French which is freely available as the second version of the Lefff (Lexique des formes fléchies du français - Lexicon of French inflected forms). It is a wide-coverage morphosyntactic and syntactic lexicon, whose architecture relies on properties inheritance, which makes it more compact and more easily maintainable and allows to describe lexical entries independantly from the formalisms it is used for. For these two reasons, we define it as a meta-lexicon. We describe its architecture, several automatic or semi-automatic approaches we use to acquire, correct and/or enrich such a lexicon, as well as the way it is used both with an LFG parser and with a TAG parser based on a meta-grammar, so as to build two large-coverage parsers for French. The web site of the Lefff is http://www.lefff.net/.</abstract>
    </paper>
    <paper id="510">
      <author><first>Nilda</first><last>Ruimy</last></author>
      <title>Structuring a Domain Vocabulary in a General Knowledge Environment</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/811_pdf.pdf</url>
      <abstract>The study which is reported here aims at investigating the extent to which the conceptual and representational tools provided by a lexical model designed for the semantic representation of general language may suit the requirements of knowledge modelling in a domain-specific perspective. A general linguistic ontology and a set of semantic links, which allow classifying, describing and interconnecting word senses, play a central role in structuring and representing such knowledge. The health and medicine vocabulary has been taken as a case study for this investigation.</abstract>
    </paper>
    <paper id="511">
      <author><first>Alexander</first><last>Geyken</last></author>
      <author><first>Norbert</first><last>Schrader</last></author>
      <title><fixed-case>L</fixed-case>exiko<fixed-case>N</fixed-case>et - a lexical database based on type and role hierarchies</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/812_pdf.pdf</url>
      <abstract>In this paper LexikoNet, a large lexical ontology of German nouns is presented. Unlike GermaNet and the Princeton WordNet, LexikoNet has distinguished type and role hypernyms right from the outset and organizes those lexemes in a parallel, independent hierarchy. In addition to roles and types, LexikoNet uses meronymic and holonymic relations as well as the instance relation. LexikoNet is based on a conceptual hierarchy of currently 1,470 classes to which approximately 90,000 word senses taken from a large German monolingual dictionary, the W\"orterbuch der deutschen Gegenwartssprache (WDG), are attached. The conceptual classes provide a useful degree of abstraction for the lexicographic description of selectional restrictions, thus making LexikoNet a useful filtering tool for corpus based lexicographic analysis. LexikoNet is currently used in-house as a filter for lexicographic extraction tasks in the DWDS project. Furthermore, it is used as a classification tool of the words of the week provided for the newspaper Die ZEIT on www.zeit.de</abstract>
    </paper>
    <paper id="512">
      <author><first>Djamel</first><last>Mostefa</last></author>
      <author><first>Olivier</first><last>Hamon</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <title>Evaluation of Automatic Speech Recognition and Speech Language Translation within <fixed-case>TC</fixed-case>-<fixed-case>STAR</fixed-case>:Results from the first evaluation campaign</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/813_pdf.pdf</url>
      <abstract>This paper reports on the evaluation activities conducted in the first year of the TC-STAR project. The TC-STAR project, financed by the European Commission within the Sixth Framework Program, is envisaged as a long-term effort to advance research in the core technologies of Speech-to-Speech Translation (SST). SST technology is a combination of Automatic Speech Recognition (ASR), Spoken Language Translation (SLT) and Text To Speech (TTS).</abstract>
    </paper>
    <paper id="513">
      <author><first>Djamel</first><last>Mostefa</last></author>
      <author><first>Marie-Neige</first><last>Garcia</last></author>
      <author><first>Khalid</first><last>Choukri</last></author>
      <title>Evaluation of multimodal components within <fixed-case>CHIL</fixed-case>: The evaluation packages and results</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/814_pdf.pdf</url>
      <abstract>This article describes the first CHIL evaluation campaign in which 12 technologies were evaluated. The major outcomes of the first evaluation campaign are the so-called Evaluation Packages. An evaluation package is the full documentation (definition and description of the evaluation methodologies, protocols and metrics) alongside the data sets and software scoring tools, which an organisation needs in order to perform the evaluation of one or more systems for a given technology. These evaluation packages will be made available to the community through ELDA General Catalogue.</abstract>
    </paper>
    <paper id="514">
      <author><first>Carol</first><last>Peters</last></author>
      <title>The Impact of Evaluation on Multilingual Information Retrieval System Development</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/815_pdf.pdf</url>
      <abstract>The Cross-Language Evaluation Forum (CLEF) promotes research into the development of truly multilingual systems capable of retrieving relevant information from collections in many languages and in mixed media. The paper discusses some of the main results achieved in the first six years of activity.</abstract>
    </paper>
    <paper id="515">
      <author><first>Bernardo</first><last>Magnini</last></author>
      <author><first>Danilo</first><last>Giampiccolo</last></author>
      <author><first>Lili</first><last>Aunimo</last></author>
      <author><first>Christelle</first><last>Ayache</last></author>
      <author><first>Petya</first><last>Osenova</last></author>
      <author><first>Anselmo</first><last>Peñas</last></author>
      <author><first>Maarten</first><last>de Rijke</last></author>
      <author><first>Bogdan</first><last>Sacaleanu</last></author>
      <author><first>Diana</first><last>Santos</last></author>
      <author><first>Richard</first><last>Sutcliffe</last></author>
      <title>The Multilingual Question Answering Track at <fixed-case>CLEF</fixed-case></title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/816_pdf.pdf</url>
      <abstract>This paper presents an overview of the Multilingual Question Answering evaluation campaigns which have been organized at CLEF (Cross Language Evaluation Forum) since 2003. Over the years, the competition has registered a steady increment in the number of participants and languages involved. In fact, from the original eight groups which participated in 2003 QA track, the number of competitors in 2005 rose to twenty-four. Also, the performances of the systems have steadily improved, and the average of the best performances in the 2005 saw an increase of 10% with respect to the previous year.</abstract>
    </paper>
    <paper id="516">
      <author><first>Marie-Neige</first><last>Garcia</last></author>
      <author><first>Christophe</first><last>d’Alessandro</last></author>
      <author><first>Gérard</first><last>Bailly</last></author>
      <author><first>Philippe</first><last>Boula de Mareüil</last></author>
      <author><first>Michel</first><last>Morel</last></author>
      <title>A joint prosody evaluation of <fixed-case>F</fixed-case>rench text-to-speech synthesis systems</title>
      <url>http://www.lrec-conf.org/proceedings/lrec2006/pdf/817_pdf.pdf</url>
      <abstract>This paper reports on prosodic evaluation in the framework of the EVALDA/EvaSy project for text-to-speech (TTS) evaluation for the French language. Prosody is evaluated using a prosodic transplantation paradigm. Intonation contours generated by the synthesis systems are transplanted on a common segmental content. Both diphone based synthesis and natural speech are used. Five TTS systems are tested along with natural voice. The test is a paired preference test (with 19 subjects), using 7 sentences. The results indicate that natural speech obtains consistently the first rank (with an average preference rate of 80%), followed by a selection based system (72%) and a diphone based system (58%). However, rather large variations in judgements are observed among subjects and sentences, and in some cases synthetic speech is preferred to natural speech. These results show the remarkable improvement achieved by the best selection based synthesis systems in terms of prosody. In this way; a new paradigm for evaluation of the prosodic component of TTS systems has been successfully demonstrated.</abstract>
    </paper>
  </volume>
</collection>
