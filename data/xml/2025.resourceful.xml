<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.resourceful">
  <volume id="1" ingest-date="2025-03-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Resources and Representations for Under-Resourced Languages and Domains (RESOURCEFUL-2025)</booktitle>
      <editor><first>Špela Arhar</first><last>Holdt</last></editor>
      <editor><first>Nikolai</first><last>Ilinykh</last></editor>
      <editor><first>Barbara</first><last>Scalvini</last></editor>
      <editor><first>Micaella</first><last>Bruton</last></editor>
      <editor><first>Iben Nyholm</first><last>Debess</last></editor>
      <editor><first>Crina Madalina</first><last>Tudor</last></editor>
      <publisher>University of Tartu Library, Estonia</publisher>
      <address>Tallinn, Estonia</address>
      <month>March</month>
      <year>2025</year>
      <url hash="d941b753">2025.resourceful-1</url>
      <venue>resourceful</venue>
      <venue>ws</venue>
      <isbn>978-9908-53-121-2</isbn>
    </meta>
    <frontmatter>
      <url hash="57034b31">2025.resourceful-1.0</url>
      <bibkey>resourceful-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies Treebank for <fixed-case>U</fixed-case>zbek</title>
      <author><first>Arofat</first><last>Akhundjanova</last></author>
      <author><first>Luigi</first><last>Talamo</last></author>
      <pages>1–6</pages>
      <abstract>We present the first Universal Dependencies treebank for Uzbek, a low-resource language from the Turkic family. The treebank contains 500 sentences (5850 tokens) sourced from the news and fiction genres and it is annotated for lemmas, part-of-speech (POS) tags, morphological features, and dependency relations. We describe our methodology for building the treebank, which consists of a mix of manual and automatic annotation and discuss some constructions of the Uzbek language that pose challenges to the UD framework.</abstract>
      <url hash="58d70977">2025.resourceful-1.1</url>
      <bibkey>akhundjanova-talamo-2025-universal</bibkey>
    </paper>
    <paper id="2">
      <title>Fine-Tuning Cross-Lingual <fixed-case>LLM</fixed-case>s for <fixed-case>POS</fixed-case> Tagging in Code-Switched Contexts</title>
      <author><first>Shayaan</first><last>Absar</last></author>
      <pages>7–12</pages>
      <abstract>Code-switching (CS) involves speakers switching between two (or potentially more) languages during conversation and is a common phenomenon in bilingual communities. The majority of NLP research has been devoted to mono-lingual language modelling. Consequentially, most models perform poorly on code-switched data. This paper investigates the effectiveness of Cross-Lingual Large Language Models on the task of POS (Part-of-Speech) tagging in code-switched contexts, once they have undergone a fine-tuning process. The models are trained on code-switched combinations of Indian languages and English. This paper also seeks to investigate whether fine-tuned models are able to generalise and POS tag code-switched combinations that were not a part of the fine-tuning dataset. Additionally, this paper presents a new metric, the S-index (Switching-Index), for measuring the level of code-switching within an utterance.</abstract>
      <url hash="22bd9e15">2025.resourceful-1.2</url>
      <bibkey>absar-2025-fine</bibkey>
    </paper>
    <paper id="4">
      <title>Second language <fixed-case>K</fixed-case>orean <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency treebank v1.2: Focus on Data Augmentation and Annotation Scheme Refinement</title>
      <author><first>Hakyung</first><last>Sung</last></author>
      <author><first>Gyu-Ho</first><last>Shin</last></author>
      <pages>13–19</pages>
      <abstract>We expand the second language (L2) Korean Universal Dependencies (UD) treebank with 5,454 manually annotated sentences. The annotation guidelines are also revised to better align with the UD framework. Using this enhanced treebank, we fine-tune three Korean language models—Stanza, spaCy, and Trankit—and evaluate their performance on in-domain and out-of-domain L2-Korean datasets. The results show that fine-tuning significantly improves their performance across various metrics, thus highlighting the importance of using well-tailored L2 datasets for fine-tuning first-language-based, general-purpose language models for the morphosyntactic analysis of L2 data.</abstract>
      <url hash="1f934d1f">2025.resourceful-1.4</url>
      <bibkey>sung-shin-2025-second</bibkey>
    </paper>
    <paper id="6">
      <title>Recommendations for Overcoming Linguistic Barriers in Healthcare: Challenges and Innovations in <fixed-case>NLP</fixed-case> for <fixed-case>H</fixed-case>aitian <fixed-case>C</fixed-case>reole</title>
      <author><first>Ludovic</first><last>Mompelat</last></author>
      <pages>20–31</pages>
      <abstract>Haitian Creole, spoken by millions in Haiti and its diaspora, remains underrepresented in Natural Language Processing (NLP) research, limiting the availability of effective translation tools. In Miami, a significant Haitian Creole-speaking population faces healthcare disparities exacerbated by language barriers. Existing translation systems fail to address key challenges such as linguistic variation within the Creole language, frequent code-switching, and the lack of standardized medical terminology. This work proposes a structured methodology for the development of an AI-assisted translation and interpretation tool tailored for patient-provider communication in a medical setting. To achieve this, we propose a hybrid NLP approach that integrates fine-tuned Large Language Models (LLMs) with traditional machine translation methods. This combination ensures accurate, context-sensitive translation that adapts to both formal medical discourse and conversational registers while maintaining linguistic consistency. Additionally, we discuss data collection strategies, annotation challenges, and evaluation metrics necessary for building an ethically designed, scalable NLP system. By addressing these issues, this research provides a foundation for improving healthcare accessibility and linguistic equity for Haitian Creole speakers.</abstract>
      <url hash="c448b405">2025.resourceful-1.6</url>
      <bibkey>mompelat-2025-recommendations</bibkey>
    </paper>
    <paper id="7">
      <title>Beyond a Means to an End: A Case Study in Building Phonotactic Corpora for <fixed-case>C</fixed-case>entral <fixed-case>A</fixed-case>ustralian Languages</title>
      <author><first>Saliha</first><last>Muradoglu</last></author>
      <author><first>James</first><last>Gray</last></author>
      <author><first>Jane Helen</first><last>Simpson</last></author>
      <author><first>Michael</first><last>Proctor</last></author>
      <author><first>Mark</first><last>Harvey</last></author>
      <pages>32–37</pages>
      <abstract>Linguistic datasets are essential across fields: computational linguists use them for NLP development, theoretical linguists for statistical arguments supporting hypotheses about language, and documentary linguists for preserving examples and aiding grammatical descriptions. Transforming raw data (e.g., recordings or dictionaries) into structured forms (e.g., tables) requires non-trivial decisions within processing pipelines.This paper highlights the importance of these processes in understanding linguistic systems. Our contributions include: (1) an interactive dashboard for four central Australian languages with custom filters, and (2) demonstrating how data processing decisions influence measured outcomes.</abstract>
      <url hash="d60264a6">2025.resourceful-1.7</url>
      <bibkey>muradoglu-etal-2025-beyond</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>OCR</fixed-case> Error Post-Correction with <fixed-case>LLM</fixed-case>s in Historical Documents: No Free Lunches</title>
      <author><first>Jenna</first><last>Kanerva</last></author>
      <author><first>Cassandra</first><last>Ledins</last></author>
      <author><first>Siiri</first><last>Käpyaho</last></author>
      <author><first>Filip</first><last>Ginter</last></author>
      <pages>38–47</pages>
      <abstract>Optical Character Recognition (OCR) systems often introduce errors when transcribing historical documents, leaving room for post-correction to improve text quality. This study evaluates the use of open-weight LLMs for OCR error correction in historical English and Finnish datasets. We explore various strategies, including parameter optimization, quantization, segment length effects, and text continuation methods. Our results demonstrate that while modern LLMs show promise in reducing character error rates (CER) in English, a practically useful performance for Finnish was not reached. Our findings highlight the potential and limitations of LLMs in scaling OCR post-correction for large historical corpora.</abstract>
      <url hash="5decde74">2025.resourceful-1.8</url>
      <bibkey>kanerva-etal-2025-ocr</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>F</fixed-case>o<fixed-case>QA</fixed-case>: A <fixed-case>F</fixed-case>aroese Question-Answering Dataset</title>
      <author><first>Annika</first><last>Simonsen</last></author>
      <author><first>Dan Saattrup</first><last>Nielsen</last></author>
      <author><first>Hafsteinn</first><last>Einarsson</last></author>
      <pages>48–57</pages>
      <abstract>We present FoQA, a Faroese extractive question-answering (QA) dataset with 2,000 samples, created using a semi-automated approach combining Large Language Models (LLMs) and human validation. The dataset was generated from Faroese Wikipedia articles using GPT-4-turbo for initial QA generation, followed by question rephrasing to increase complexity and native speaker validation to ensure quality. We provide baseline performance metrics for FoQA across multiple models, including LLMs and BERT, demonstrating its effectiveness in evaluating Faroese QA performance. The dataset is released in three versions: a validated set of 2,000 samples, a complete set of all 10,001 generated samples, and a set of 2,395 rejected samples for error analysis.</abstract>
      <url hash="78e72a35">2025.resourceful-1.11</url>
      <bibkey>simonsen-etal-2025-foqa</bibkey>
    </paper>
    <paper id="12">
      <title>Automatic Validation of the Non-Validated <fixed-case>S</fixed-case>panish Speech Data of Common Voice 17.0</title>
      <author><first>Carlos Daniel</first><last>Hernández Mena</last></author>
      <author><first>Barbara</first><last>Scalvini</last></author>
      <author><first>Dávid í</first><last>Lág</last></author>
      <pages>58–63</pages>
      <abstract>Mozilla Common Voice is a crowdsourced project that aims to create a public, multilingual dataset of voice recordings for training speech recognition models. In Common Voice, anyone can contribute by donating or validating recordings in various languages. However, despite the availability of many recordings in certain languages, a significant percentage remains unvalidated by users. This is the case for Spanish, where in version 17.0 of Common Voice, 75% of the 2,220 hours of recordings are unvalidated. In this work, we used the Whisper recognizer to automatically validate approximately 784 hours of recordings which are more than the 562 hours validated by users. To verify the accuracy of the validation, we developed a speech recognition model based on a version of NVIDIA-NeMo’s Parakeet, which does not have an official Spanish version. Our final model achieved a WER of less than 4% on the test and validation splits of Common Voice 17.0. Both the model and the speech corpus are publicly available on Hugging Face.</abstract>
      <url hash="20096f93">2025.resourceful-1.12</url>
      <bibkey>hernandez-mena-etal-2025-automatic</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>W</fixed-case>iki<fixed-case>QA</fixed-case>-<fixed-case>IS</fixed-case>: Assisted Benchmark Generation and Automated Evaluation of <fixed-case>I</fixed-case>celandic Cultural Knowledge in <fixed-case>LLM</fixed-case>s</title>
      <author><first/><last>Þórunn Arnardóttir</last></author>
      <author><first/><last>Elías Bjartur Einarsson</last></author>
      <author><first/><last>Garðar Ingvarsson Juto</last></author>
      <author><first/><last>Þorvaldur Páll Helgason</last></author>
      <author><first/><last>Hafsteinn Einarsson</last></author>
      <pages>64–73</pages>
      <abstract>This paper presents WikiQA-IS, a novel question-answering dataset focusing on Icelandic culture and history, along with an automated pipeline for dataset generation and evaluation. Leveraging GPT-4 to create questions and answers based on Icelandic Wikipedia articles and news sources, we produced a high-quality corpus of 2,000 question-answer pairs. We introduce an automatic evaluation method using GPT-4o as a judge, which shows strong agreement with human evaluations. Our benchmark reveals varying performances across different language models, with closed-source models generally outperforming open-weights alternatives. This work contributes a resource for evaluating language models’ knowledge of Icelandic culture and offers a replicable framework for creating similar datasets in other cultural contexts.</abstract>
      <url hash="8bd974c1">2025.resourceful-1.13</url>
      <bibkey>thorunn-arnardottir-etal-2025-wikiqa</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>DUDU</fixed-case>: A Treebank for <fixed-case>O</fixed-case>ttoman <fixed-case>T</fixed-case>urkish in <fixed-case>UD</fixed-case> Style</title>
      <author><first>Enes</first><last>Yılandiloğlu</last></author>
      <author><first>Janine</first><last>Siewert</last></author>
      <pages>74–79</pages>
      <abstract>This paper introduces a recently released Ottoman Turkish (ota) treebank in Universal Dependencies (UD) style, DUDU. The DUDU Treebank consists of 1,064 automatically annotated and manually corrected sentences. The texts were manually collected from various academic or literary sources available on the Internet. Following preprocessing, the sentences were annotated using a MaCHAMP-based neural network model utilizing the large language model (LLM) architecture and manually corrected. The treebank became publicly available with the 2.14 release, and future steps involve expanding the treebank with more data and refining the annotation scheme. The treebank is the first and only treebank that utilizes the IJMES transliteration alphabet. The treebank not only gives insight on Ottoman Turkish lexically, morphologically, and syntactically, but also provides a small but robust test set for future computational models for Ottoman Turkish.</abstract>
      <url hash="a4571e6d">2025.resourceful-1.16</url>
      <bibkey>yilandiloglu-siewert-2025-dudu</bibkey>
    </paper>
    <paper id="19">
      <title>A Simple Audio and Text Collection-Annotation Tool Targeted to <fixed-case>B</fixed-case>razilian Indigenous Language Native Speakers</title>
      <author><first>Gustavo Padilha</first><last>Polleti</last></author>
      <author><first>Fabio</first><last>Cozman</last></author>
      <author><first>Fabricio</first><last>Gerardi</last></author>
      <pages>80–85</pages>
      <abstract>In this paper we present an audio and text annotation tool for native speakers, with a particular focus on Brazilian indigenous languages. Our tool simplifies the process of language resource annotation and employs gamefication techniques typically found in language learning games. Then we describe the annotation tool and present preliminary results for the Bororo language. We discuss the limitations of our tool, highlighting ethical and practical implementation concerns.</abstract>
      <url hash="2cdd897a">2025.resourceful-1.19</url>
      <bibkey>polleti-etal-2025-simple</bibkey>
    </paper>
    <paper id="22">
      <title>First Steps in Benchmarking <fixed-case>L</fixed-case>atvian in Large Language Models</title>
      <author><first>Inguna</first><last>Skadina</last></author>
      <author><first>Bruno</first><last>Bakanovs</last></author>
      <author><first>Roberts</first><last>Darģis</last></author>
      <pages>86–95</pages>
      <abstract>The performance of multilingual large language models (LLMs) in low-resource languages, such as Latvian, has been under-explored. In this paper, we investigate the capabilities of several open and commercial LLMs in the Latvian language understanding tasks. We evaluate these models across several well-known benchmarks, such as the Choice of Plausible Alternatives (COPA) and Measuring Massive Multitask Language Understanding (MMLU), which were adapted into Latvian using machine translation. Our results highlight significant variability in model performance, emphasizing the challenges of extending LLMs to low-resource languages. We also analyze the effect of post-editing on machine-translated datasets, observing notable improvements in model accuracy, particularly with BERT-based architectures. We also assess open-source LLMs using the Belebele dataset, showcasing competitive performance from open-weight models when compared to proprietary systems. This study reveals key insights into the limitations of current LLMs in low-resource settings and provides datasets for future benchmarking efforts.</abstract>
      <url hash="74af5bfb">2025.resourceful-1.22</url>
      <bibkey>skadina-etal-2025-first</bibkey>
    </paper>
    <paper id="23">
      <title>On the Usage of Semantics, Syntax, and Morphology for Noun Classification in <fixed-case>I</fixed-case>si<fixed-case>Z</fixed-case>ulu</title>
      <author><first>Imaan</first><last>Sayed</last></author>
      <author><first>Zola</first><last>Mahlaza</last></author>
      <author><first>Alexander</first><last>van der Leek</last></author>
      <author><first>Jonathan</first><last>Mopp</last></author>
      <author><first>C. Maria</first><last>Keet</last></author>
      <pages>96–105</pages>
      <abstract>There is limited work aimed at solving the core task of noun classification for Nguni languages. The task focuses on identifying the semantic categorisation of each noun and plays a crucial role in the ability to form semantically and morphologically valid sentences. The work by Byamugisha (2022) was the first to tackle the problem for a related, but non-Nguni, language. While there have been efforts to replicate it for a Nguni language, there has been no effort focused on comparing the technique used in the original work vs. contemporary neural methods or a number of traditional machine learning classification techniques that do not rely on human-guided knowledge to the same extent. We reproduce Byamugisha (2022)’s work with different configurations to account for differences in access to datasets and resources, compare the approach with a pre-trained transformer-based model, and traditional machine learning models that relyon less human-guided knowledge. The newly created data-driven models outperform the knowledge-infused models, with the best performing models achieving an F1 score of 0.97.</abstract>
      <url hash="8371729e">2025.resourceful-1.23</url>
      <bibkey>sayed-etal-2025-usage</bibkey>
    </paper>
    <paper id="24">
      <title>Annotating Attitude in <fixed-case>S</fixed-case>wedish Political Tweets</title>
      <author><first>Anna</first><last>Lindahl</last></author>
      <pages>106-110</pages>
      <abstract>There is a lack of Swedish datasets annotated for emotional and argumentative language. This work therefore presents an annotation procedure and a dataset of Swedish political tweets. The tweets are annotated for positive and negative attitude. Challenges with this type of annotation is identified and described. The evaluation shows that the annotators do not agree on where to annotate spans, but that they agree on labels. This is demonstrated with a new implementation of the agreement coefficient Krippendorff’s unitized alpha.</abstract>
      <url hash="6ee0ee47">2025.resourceful-1.24</url>
      <bibkey>lindahl-2025-annotating</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>V</fixed-case>erb<fixed-case>C</fixed-case>raft: Morphologically-Aware <fixed-case>A</fixed-case>rmenian Text Generation Using <fixed-case>LLM</fixed-case>s in Low-Resource Settings</title>
      <author><first>Hayastan</first><last>Avetisyan</last></author>
      <author><first>David</first><last>Broneske</last></author>
      <pages>111–119</pages>
      <abstract>Understanding and generating morphologically complex verb forms is a critical challenge in Natural Language Processing (NLP), particularly for low-resource languages like Armenian. Armenian’s verb morphology encodes multiple layers of grammatical information, such as tense, aspect, mood, voice, person, and number, requiring nuanced computational modeling. We introduce VerbCraft, a novel neural model that integrates explicit morphological classifiers into the mBART-50 architecture. VerbCraft achieves a BLEU score of 0.4899 on test data, compared to the baseline’s 0.9975, reflecting its focus on prioritizing morphological precision over fluency. With over 99% accuracy in aspect and voice predictions and robust performance on rare and irregular verb forms, VerbCraft addresses data scarcity through synthetic data generation with human-in-the-loop validation. Beyond Armenian, it offers a scalable framework for morphologically rich, low-resource languages, paving the way for linguistically informed NLP systems and advancing language preservation efforts.</abstract>
      <url hash="cf90f9ba">2025.resourceful-1.25</url>
      <bibkey>avetisyan-broneske-2025-verbcraft</bibkey>
    </paper>
    <paper id="26">
      <title>Post-<fixed-case>OCR</fixed-case> Correction of Historical <fixed-case>G</fixed-case>erman Periodicals using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Vera</first><last>Danilova</last></author>
      <author><first>Gijs</first><last>Aangenendt</last></author>
      <pages>120–129</pages>
      <abstract>Optical Character Recognition (OCR) is critical for accurate access to historical corpora, providing a foundation for processing pipelines and the reliable interpretation of historical texts. Despite advances, the quality of OCR in historical documents remains limited, often requiring post-OCR correction to address residual errors. Building on recent progress with instruction-tuned Llama 2 models applied to English historical newspapers, we examine the potential of German Llama 2 and Mistral models for post-OCR correction of German medical historical periodicals. We perform instruction tuning using two configurations of training data, augmenting our small annotated dataset with two German datasets from the same time period. The results demonstrate that German Mistral enhances the raw OCR output, achieving a lower average word error rate (WER). However, the average character error rate (CER) either decreases or remains unchanged across all models considered. We perform an analysis of performance within the error groups and provide an interpretation of the results.</abstract>
      <url hash="adaf1302">2025.resourceful-1.26</url>
      <bibkey>danilova-aangenendt-2025-post</bibkey>
    </paper>
    <paper id="27">
      <title>From Words to Action: A National Initiative to Overcome Data Scarcity for the <fixed-case>S</fixed-case>lovene <fixed-case>LLM</fixed-case></title>
      <author><first>Špela Arhar</first><last>Holdt</last></author>
      <author><first>Špela</first><last>Antloga</last></author>
      <author><first>Tina</first><last>Munda</last></author>
      <author><first>Eva</first><last>Pori</last></author>
      <author><first>Simon</first><last>Krek</last></author>
      <pages>130–136</pages>
      <abstract>Large Language Models (LLMs) have demonstrated significant potential in natural language processing, but they depend on vast, diverse datasets, creating challenges for languages with limited resources. The paper presents a national initiative that addresses these challenges for Slovene. We outline strategies for large-scale text collection, including the creation of an online platform to engage the broader public in contributing texts and a communication campaign promoting openly accessible and transparently developed LLMs.</abstract>
      <url hash="a70797eb">2025.resourceful-1.27</url>
      <bibkey>holdt-etal-2025-words</bibkey>
    </paper>
    <paper id="28">
      <title>Assessing the Similarity of Cross-Lingual <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Sentence Embeddings Using Low-Resource Spectral Clustering</title>
      <author><first>Nelson</first><last>Moll</last></author>
      <author><first>Tahseen</first><last>Rabbani</last></author>
      <pages>137–142</pages>
      <abstract>In this work, we study the cross-lingual distance of machine translations through alignment of seq2seq representations over small corpora. First, we use the M2M100 model to collect sentence-level representations of The Book of Revelation in several languages. We then perform unsupervised manifold alignment (spectral clustering) between these collections of embeddings. As verses between translations are not necessarily aligned, our procedure falls under the challenging, but more realistic non-correspondence regime. The cost function associated with each alignment is used to rank the relative (machine) similarity of one language to another. We then perform correspondent alignment over another cluster of languages, this time using FLORES+ parallel NLLB model embeddings. Our experiments demonstrate that the representations of closely-related languages group closely, and are cheap to align (requiring <tex-math>&lt;</tex-math>1000 sentences) via our strategy.</abstract>
      <url hash="1daea75d">2025.resourceful-1.28</url>
      <bibkey>moll-rabbani-2025-assessing</bibkey>
    </paper>
    <paper id="29">
      <title>Voices of <fixed-case>L</fixed-case>uxembourg: Tackling Dialect Diversity in a Low-Resource Setting</title>
      <author><first>Nina</first><last>Hosseini-Kivanani</last></author>
      <author><first>Christoph</first><last>Schommer</last></author>
      <author><first>Peter</first><last>Gilles</last></author>
      <pages>143–152</pages>
      <abstract>Dialect classification is essential for preserving linguistic diversity, particularly in low-resource languages such as Luxembourgish. This study introduces one of the first systematic approaches to classifying Luxembourgish dialects, addressing phonetic, prosodic, and lexical variations across four major regions. We benchmarked multiple models, including state-of-the-art pre-trained speech models like Wav2Vec2, XLSR-Wav2Vec2, and Whisper, alongside traditional approaches such as Random Forest and CNN-LSTM. To overcome data limitations, we applied targeted data augmentation strategies and analyzed their impact on model performance. Our findings highlight the superior performance of CNN-Spectrogram and CNN-LSTM models while identifying the strengths and limitations of data augmentation. This work establishes foundational benchmarks and provides actionable insights for advancing dialectal NLP in Luxembourgish and other low-resource languages.</abstract>
      <url hash="3556ea5c">2025.resourceful-1.29</url>
      <bibkey>hosseini-kivanani-etal-2025-voices</bibkey>
    </paper>
    <paper id="31">
      <title>The Application of Corpus-Based Language Distance Measurement to the Diatopic Variation Study (on the Material of the Old Novgorodian Birchbark Letters)</title>
      <author><first>Ilia</first><last>Afanasev</last></author>
      <author><first>Olga</first><last>Lyashevskaya</last></author>
      <pages>153–164</pages>
      <abstract>The paper presents a computer-assisted exploration of a set of texts, where qualitative analysis complements the linguistically-aware vector-based language distance measurements, interpreting them through close reading and thus proving or disproving their conclusions. It proposes using a method designed for small raw corpora to explore the individual, chronological, and gender-based differences within an extinct single territorial lect, known only by a scarce collection of documents. The material under consideration is the Novgorodian birchbark letters, a set of rather small manuscripts (not a single one is more than 1000 tokens) that are witnesses of the Old Novgorodian lect, spoken on the territories of modern Novgorod and Staraya Russa at the first half of the first millennium CE. The study shows the existence of chronological variation, a mild degree of individual variation, and almost absent gender-based differences. Possible prospects of the study include its application to the newly discovered birchbark letters and using an outgroup for more precise measurements.</abstract>
      <url hash="4ece1fdc">2025.resourceful-1.31</url>
      <bibkey>afanasev-lyashevskaya-2025-application</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>Ï</fixed-case> Need More Context and an <fixed-case>E</fixed-case>nglish Translation: Analysing How <fixed-case>LLM</fixed-case>s Identify Personal Information in <fixed-case>K</fixed-case>omi, <fixed-case>P</fixed-case>olish, and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Maria Irena</first><last>Szawerna</last></author>
      <pages>165–178</pages>
      <abstract>Automatic identification of personal information (PI) is particularly difficult for languages with limited linguistic resources. Recently, large language models (LLMs) have been applied to various tasks involving low-resourced languages, but their capability to process PI in such contexts remains under-explored. In this paper we provide a qualitative analysis of the outputs from three LLMs prompted to identify PI in texts written in Komi (Permyak and Zyrian), Polish, and English. Our analysis highlights challenges in using pre-trained LLMs for PI identification in both low- and medium-resourced languages. It also motivates the need to develop LLMs that understand the differences in how PI is expressed across languages with varying levels of availability of linguistic resources.</abstract>
      <url hash="115da3fd">2025.resourceful-1.32</url>
      <bibkey>ilinykh-szawerna-2025-need</bibkey>
    </paper>
    <paper id="33">
      <title>Multi-label <fixed-case>S</fixed-case>candinavian Language Identification (<fixed-case>SLIDE</fixed-case>)</title>
      <author><first>Mariia</first><last>Fedorova</last></author>
      <author><first>Jonas Sebulon</first><last>Frydenberg</last></author>
      <author><first>Victoria</first><last>Handford</last></author>
      <author><first>Victoria Ovedie Chruickshank</first><last>Langø</last></author>
      <author><first>Solveig Helene</first><last>Willoch</last></author>
      <author><first>Marthe Løken</first><last>Midtgaard</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Petter</first><last>Mæhlum</last></author>
      <author><first>David</first><last>Samuel</last></author>
      <pages>179–189</pages>
      <abstract>Identifying closely related languages at sentence level is difficult, in particular because it is often impossible to assign a sentence to a single language. In this paper, we focus on multi-label sentence-level Scandinavian language identification (LID) for Danish, Norwegian Bokmål, Norwegian Nynorsk, and Swedish. We present the Scandinavian Language Identification and Evaluation, SLIDE, a manually curated multi-label evaluation dataset and a suite of LID models with varying speed–accuracy tradeoffs. We demonstrate that the ability to identify multiple languages simultaneously is necessary for any accurate LID method, and present a novel approach to training such multi-label LID models.</abstract>
      <url hash="615eb2e5">2025.resourceful-1.33</url>
      <bibkey>fedorova-etal-2025-multi</bibkey>
    </paper>
    <paper id="34">
      <title>Federated Meta-Learning for Low-Resource Translation of <fixed-case>K</fixed-case>irundi</title>
      <author><first>Kyle Rui</first><last>Sang</last></author>
      <author><first>Tahseen</first><last>Rabbani</last></author>
      <author><first>Tianyi</first><last>Zhou</last></author>
      <pages>190–194</pages>
      <abstract>In this work, we reframe multilingual neural machine translation (NMT) as a federated meta-learning problem and introduce a translation dataset for the low-resource Kirundi language. We aggregate machine translation models () locally trained on varying (but related) source languages to produce a global meta-model that encodes abstract representations of key semantic structures relevant to the parent languages. We then use the Reptile algorithm and Optuna fine-tuning to fit the global model onto a target language. The target language may live outside the subset of parent languages (such as closely-related dialects or sibling languages), which is particularly useful for languages with limitedly available sentence pairs. We first develop a novel dataset of Kirundi-English sentence pairs curated from Biblical translation. We then demonstrate that a federated learning approach can produce a tiny 4.8M Kirundi translation model and a stronger NLLB-600M model which performs well on both our Biblical corpus and the FLORES-200 Kirundi corpus.</abstract>
      <url hash="33c0d08d">2025.resourceful-1.34</url>
      <bibkey>sang-etal-2025-federated</bibkey>
    </paper>
  </volume>
</collection>
