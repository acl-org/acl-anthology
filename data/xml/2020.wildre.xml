<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.wildre">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the WILDRE5– 5th Workshop on Indian Language Data: Resources and Evaluation</booktitle>
      <editor><first>Girish Nath</first><last>Jha</last></editor>
      <editor><first>Kalika</first><last>Bali</last></editor>
      <editor><first>Sobha</first><last>L.</last></editor>
      <editor><first>S. S.</first><last>Agrawal</last></editor>
      <editor><first>Atul Kr.</first><last>Ojha</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-67-2</isbn>
      <venue>wildre</venue>
    </meta>
    <frontmatter>
      <url hash="ce03e7a8">2020.wildre-1.0</url>
      <bibkey>wildre-2020-wildre5</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Part-of-Speech Annotation Challenges in <fixed-case>M</fixed-case>arathi</title>
      <author><first>Gajanan</first><last>Rane</last></author>
      <author><first>Nilesh</first><last>Joshi</last></author>
      <author><first>Geetanjali</first><last>Rane</last></author>
      <author><first>Hanumant</first><last>Redkar</last></author>
      <author><first>Malhar</first><last>Kulkarni</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>1–6</pages>
      <abstract>Part of Speech (POS) annotation is a significant challenge in natural language processing. The paper discusses issues and challenges faced in the process of POS annotation of the Marathi data from four domains viz., tourism, health, entertainment and agriculture. During POS annotation, a lot of issues were encountered. Some of the major ones are discussed in detail in this paper. Also, the two approaches viz., the lexical (L approach) and the functional (F approach) of POS tagging have been discussed and presented with examples. Further, some ambiguous cases in POS annotation are presented in the paper.</abstract>
      <url hash="7ced6113">2020.wildre-1.1</url>
      <language>eng</language>
      <bibkey>rane-etal-2020-part</bibkey>
    </paper>
    <paper id="2">
      <title>A Dataset for Troll Classification of <fixed-case>T</fixed-case>amil<fixed-case>M</fixed-case>emes</title>
      <author><first>Shardul</first><last>Suryawanshi</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <author><first>Pranav</first><last>Verma</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <author><first>John Philip</first><last>McCrae</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <pages>7–13</pages>
      <abstract>Social media are interactive platforms that facilitate the creation or sharing of information, ideas or other forms of expression among people. This exchange is not free from offensive, trolling or malicious contents targeting users or communities. One way of trolling is by making memes, which in most cases combines an image with a concept or catchphrase. The challenge of dealing with memes is that they are region-specific and their meaning is often obscured in humour or sarcasm. To facilitate the computational modelling of trolling in the memes for Indian languages, we created a meme dataset for Tamil (TamilMemes). We annotated and released the dataset containing suspected trolls and not-troll memes. In this paper, we use the a image classification to address the difficulties involved in the classification of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score.</abstract>
      <url hash="58a68762">2020.wildre-1.2</url>
      <language>eng</language>
      <bibkey>suryawanshi-etal-2020-dataset</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tamil-memes">Tamil Memes</pwcdataset>
    </paper>
    <paper id="3">
      <title><fixed-case>O</fixed-case>di<fixed-case>E</fixed-case>n<fixed-case>C</fixed-case>orp 2.0: <fixed-case>O</fixed-case>dia-<fixed-case>E</fixed-case>nglish Parallel Corpus for Machine Translation</title>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Satya Ranjan</first><last>Dash</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <author><first>Priyanka</first><last>Pattnaik</last></author>
      <author><first>Debasish Kumar</first><last>Mallick</last></author>
      <pages>14–19</pages>
      <abstract>The preparation of parallel corpora is a challenging task, particularly for languages that suffer from under-representation in the digital world. In a multi-lingual country like India, the need for such parallel corpora is stringent for several low-resource languages. In this work, we provide an extended English-Odia parallel corpus, OdiEnCorp 2.0, aiming particularly at Neural Machine Translation (NMT) systems which will help translate English↔Odia. OdiEnCorp 2.0 includes existing English-Odia corpora and we extended the collection by several other methods of data acquisition: parallel data scraping from many websites, including Odia Wikipedia, but also optical character recognition (OCR) to extract parallel data from scanned images. Our OCR-based data extraction approach for building a parallel corpus is suitable for other low resource languages that lack in online content. The resulting OdiEnCorp 2.0 contains 98,302 sentences and 1.69 million English and 1.47 million Odia tokens. To the best of our knowledge, OdiEnCorp 2.0 is the largest Odia-English parallel corpus covering different domains and available freely for non-commercial and research purposes.</abstract>
      <url hash="cf9ba874">2020.wildre-1.3</url>
      <language>eng</language>
      <bibkey>parida-etal-2020-odiencorp</bibkey>
    </paper>
    <paper id="4">
      <title>Handling Noun-Noun Coreference in <fixed-case>T</fixed-case>amil</title>
      <author><first>Vijay</first><last>Sundar Ram</last></author>
      <author><first>Sobha</first><last>Lalitha Devi</last></author>
      <pages>20–24</pages>
      <abstract>Natural language understanding by automatic tools is the vital requirement for document processing tools. To achieve it, automatic system has to understand the coherence in the text. Co-reference chains bring coherence to the text. The commonly occurring reference markers which bring cohesiveness are Pronominal, Reflexives, Reciprocals, Distributives, One-anaphors, Noun–noun reference. Here in this paper, we deal with noun-noun reference in Tamil. We present the methodology to resolve these noun-noun anaphors and also present the challenges in handling the noun-noun anaphoric relations in Tamil.</abstract>
      <url hash="91362146">2020.wildre-1.4</url>
      <language>eng</language>
      <bibkey>sundar-ram-lalitha-devi-2020-handling</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>M</fixed-case>alayalam Speech Corpus: Design and Development for <fixed-case>D</fixed-case>ravidian Language</title>
      <author><first>Lekshmi</first><last>K R</last></author>
      <author><first>Jithesh</first><last>V S</last></author>
      <author><first>Elizabeth</first><last>Sherly</last></author>
      <pages>25–28</pages>
      <abstract>To overpass the disparity between theory and applications in language-related technology in the text as well as speech and several other areas, a well-designed and well-developed corpus is essential. Several problems and issues encountered while developing a corpus, especially for low resource languages. The Malayalam Speech Corpus (MSC) is one of the first open speech corpora for Automatic Speech Recognition (ASR) research to the best of our knowledge. It consists of 250 hours of Agricultural speech data. We are providing a transcription file, lexicon and annotated speech along with the audio segment. It is available in future for public use upon request at “www.iiitmk.ac.in/vrclc/utilities/ml_speechcorpus”. This paper details the development and collection process in the domain of agricultural speech corpora in the Malayalam Language.</abstract>
      <url hash="4b123c1a">2020.wildre-1.5</url>
      <language>eng</language>
      <bibkey>k-r-etal-2020-malayalam</bibkey>
    </paper>
    <paper id="6">
      <title>Multilingual Neural Machine Translation involving <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Pulkit</first><last>Madaan</last></author>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <pages>29–32</pages>
      <abstract>Neural Machine Translations (NMT) models are capable of translating a single bilingual pair and require a new model for each new language pair. Multilingual Neural Machine Translation models are capable of translating multiple language pairs, even pairs which it hasn’t seen before in training. Availability of parallel sentences is a known problem in machine translation. Multilingual NMT model leverages information from all the languages to improve itself and performs better. We propose a data augmentation technique that further improves this model profoundly. The technique helps achieve a jump of more than 15 points in BLEU score from the multilingual NMT model. A BLEU score of 36.2 was achieved for Sindhi–English translation, which is higher than any score on the leaderboard of the LoResMT SharedTask at MT Summit 2019, which provided the data for the experiments.</abstract>
      <url hash="b25be9e5">2020.wildre-1.6</url>
      <language>eng</language>
      <bibkey>madaan-sadat-2020-multilingual</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Treebanks for Low-Resource <fixed-case>I</fixed-case>ndian Languages: The Case of <fixed-case>B</fixed-case>hojpuri</title>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <author><first>Daniel</first><last>Zeman</last></author>
      <pages>33–38</pages>
      <abstract>This paper presents the first dependency treebank for Bhojpuri, a resource-poor language that belongs to the Indo-Aryan language family. The objective behind the Bhojpuri Treebank (BHTB) project is to create a substantial, syntactically annotated treebank which not only acts as a valuable resource in building language technological tools, also helps in cross-lingual learning and typological research. Currently, the treebank consists of 4,881 annotated tokens in accordance with the annotation scheme of Universal Dependencies (UD). A Bhojpuri tagger and parser were created using machine learning approach. The accuracy of the model is 57.49% UAS, 45.50% LAS, 79.69% UPOS accuracy and 77.64% XPOS accuracy. The paper describes the details of the project including a discussion on linguistic analysis and annotation process of the Bhojpuri UD treebank.</abstract>
      <url hash="45a64efe">2020.wildre-1.7</url>
      <language>eng</language>
      <bibkey>ojha-zeman-2020-universal</bibkey>
    </paper>
    <paper id="8">
      <title>A Fully Expanded Dependency Treebank for <fixed-case>T</fixed-case>elugu</title>
      <author><first>Sneha</first><last>Nallani</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>39–44</pages>
      <abstract>Treebanks are an essential resource for syntactic parsing. The available Paninian dependency treebank(s) for Telugu is annotated only with inter-chunk dependency relations and not all words of a sentence are part of the parse tree. In this paper, we automatically annotate the intra-chunk dependencies in the treebank using a Shift-Reduce parser based on Context Free Grammar rules for Telugu chunks. We also propose a few additional intra-chunk dependency relations for Telugu apart from the ones used in Hindi treebank. Annotating intra-chunk dependencies finally provides a complete parse tree for every sentence in the treebank. Having a fully expanded treebank is crucial for developing end to end parsers which produce complete trees. We present a fully expanded dependency treebank for Telugu consisting of 3220 sentences. In this paper, we also convert the treebank annotated with Anncorra part-of-speech tagset to the latest BIS tagset. The BIS tagset is a hierarchical tagset adopted as a unified part-of-speech standard across all Indian Languages. The final treebank is made publicly available.</abstract>
      <url hash="d538693e">2020.wildre-1.8</url>
      <language>eng</language>
      <bibkey>nallani-etal-2020-fully</bibkey>
    </paper>
    <paper id="9">
      <title>Determination of Idiomatic Sentences in Paragraphs Using Statement Classification and Generalization of Grammar Rules</title>
      <author><first>Naziya</first><last>Shaikh</last></author>
      <pages>45–50</pages>
      <abstract>The translation systems are often not able to determine the presence of an idiom in a given paragraph. Due to this many systems tend to return the word-for-word translation of such statements leading to loss in the flavor of the idioms in the paragraph. This paper suggests a novel approach to efficiently determine probability of any statement in a given English paragraph to be an idiom. This approach combines the rule-based generalization of idioms in English language and classification of statements based on the context to determine the idioms in the sentence. The context based classification method can be used further for determination of idioms in regional Indian languages such as Marathi, Konkani and Hindi as the difference in the semantic context of the proverb as compared to the context in a paragraph is also evident in these other languages.</abstract>
      <url hash="fa36bd95">2020.wildre-1.9</url>
      <language>eng</language>
      <bibkey>shaikh-2020-determination</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>P</fixed-case>olish Lexicon-Grammar Development Methodology as an Example for Application to other Languages</title>
      <author><first>Zygmunt</first><last>Vetulani</last></author>
      <author><first>Grażyna</first><last>Vetulani</last></author>
      <pages>51–59</pages>
      <abstract>In the paper we present our methodology with the intention to propose it as a reference for creating lexicon-grammars. We share our long-term experience gained during research projects (past and on-going) concerning the description of Polish using this approach. The above-mentioned methodology, linking semantics and syntax, has revealed useful for various IT applications. Among other, we address this paper to researchers working on “less” or “middle-resourced” Indo-European languages as a proposal of a long term academic cooperation in the field. We believe that the confrontation of our lexicon-grammar methodology with other languages – Indo-European, but also Non-Indo-European languages of India, Ugro-Finish or Turkic languages in Eurasia – will allow for better understanding of the level of versatility of our approach and, last but not least, will create opportunities to intensify comparative studies. The reason of presenting some our works on language resources within the Wildre workshop is the intention not only to take up the challenge thrown down in the CFP of this workshop which is: “To provide opportunity for researchers from India to collaborate with researchers from other parts of the world”, but also to generalize this challenge to other languages.</abstract>
      <url hash="cc748602">2020.wildre-1.10</url>
      <language>eng</language>
      <bibkey>vetulani-vetulani-2020-polish</bibkey>
    </paper>
    <paper id="11">
      <title>Abstractive Text Summarization for <fixed-case>S</fixed-case>anskrit Prose: A Study of Methods and Approaches</title>
      <author><first>Shagun</first><last>Sinha</last></author>
      <author><first>Girish</first><last>Jha</last></author>
      <pages>60–65</pages>
      <abstract>The authors present a work-in-progress in the field of Abstractive Text Summarization (ATS) for Sanskrit Prose – a first attempt at ATS for Sanskrit (SATS). We will evaluate recent approaches and methods used for ATS and argue for the ones to be adopted for Sanskrit prose considering the unique properties of the language. There are three goals of SATS - to make manuscript summaries, to enrich the semantic processing of Sanskrit, and to improve the information retrieval systems in the language. While Extractive Text Summarization (ETS) is an important method, the summaries it generates are not always coherent. For qualitative coherent summaries, ATS is considered a better option by scholars. This paper reviews various ATS/ETS approaches for Sanskrit and other Indian Languages done till date. In the preliminary overview, authors conclude that of the two available approaches - structure-based and semantic-based - the latter would be viable owing to the rich morphology of Sanskrit. Moreover, a graph-based method may also be suitable. The second suggested method is the supervised-learning method. The authors also suggest attempting cross-lingual summarization as an extension to this work in future.</abstract>
      <url hash="656fc1d3">2020.wildre-1.11</url>
      <language>eng</language>
      <bibkey>sinha-jha-2020-abstractive</bibkey>
    </paper>
    <paper id="12">
      <title>A Deeper Study on Features for Named Entity Recognition</title>
      <author><first>Malarkodi</first><last>C S</last></author>
      <author><first>Sobha</first><last>Lalitha Devi</last></author>
      <pages>66–72</pages>
      <abstract>This paper deals with the various features used for the identification of named entities. The performance of the machine learning system heavily depends on the feature selection criteria. The intention to trace the essential features required for the development of named entity system across languages motivated us to conduct this study. The linguistic analysis was done to find out the part of speech patterns surrounding the context of named entities and from the observation linguistic oriented features are identified for both Indian and European languages. The Indian languages belongs to Dravidian language family such as Tamil, Telugu, Malayalam, Indo-Aryan language family such as Hindi, Punjabi, Bengali and Marathi, European languages such as English, Spanish, Dutch, German and Hungarian are used in this work. The machine learning technique CRFs was used for the system development. The experiments were conducted using the linguistic features and the results obtained for each languages are comparable with state-of-art systems.</abstract>
      <url hash="601c2e27">2020.wildre-1.12</url>
      <language>eng</language>
      <bibkey>c-s-lalitha-devi-2020-deeper</bibkey>
    </paper>
  </volume>
</collection>
