<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.bppf">
  <volume id="1" ingest-date="2021-07-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future</booktitle>
      <editor><first>Kenneth</first><last>Church</last></editor>
      <editor><first>Mark</first><last>Liberman</last></editor>
      <editor><first>Valia</first><last>Kordoni</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>Aug</month>
      <year>2021</year>
      <url hash="b25d3c83">2021.bppf-1</url>
      <venue>bppf</venue>
    </meta>
    <frontmatter>
      <url hash="16c5e21a">2021.bppf-1.0</url>
      <bibkey>bppf-2021-benchmarking</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Benchmarking: Past, Present and Future</title>
      <author><first>Kenneth</first><last>Church</last></author>
      <author><first>Mark</first><last>Liberman</last></author>
      <author><first>Valia</first><last>Kordoni</last></author>
      <pages>1–7</pages>
      <abstract>Where have we been, and where are we going? It is easier to talk about the past than the future. These days, benchmarks evolve more bottom up (such as papers with code). There used to be more top-down leadership from government (and industry, in the case of systems, with benchmarks such as SPEC). Going forward, there may be more top-down leadership from organizations like MLPerf and/or influencers like David Ferrucci, who was responsible for IBM’s success with Jeopardy, and has recently written a paper suggesting how the community should think about benchmarking for machine comprehension. Tasks such as reading comprehension become even more interesting as we move beyond English. Multilinguality introduces many challenges, and even more opportunities.</abstract>
      <url hash="cf8f869a">2021.bppf-1.1</url>
      <doi>10.18653/v1/2021.bppf-1.1</doi>
      <bibkey>church-etal-2021-benchmarking</bibkey>
      <pwccode url="https://github.com/kwchurch/benchmarking_past_present_future" additional="false">kwchurch/benchmarking_past_present_future</pwccode>
    </paper>
    <paper id="2">
      <title>Guideline Bias in <fixed-case>W</fixed-case>izard-of-<fixed-case>O</fixed-case>z Dialogues</title>
      <author><first>Victor Petrén Bach</first><last>Hansen</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>8–14</pages>
      <abstract>NLP models struggle with generalization due to sampling and annotator bias. This paper focuses on a different kind of bias that has received very little attention: guideline bias, i.e., the bias introduced by how our annotator guidelines are formulated. We examine two recently introduced dialogue datasets, CCPE-M and Taskmaster-1, both collected by trained assistants in a Wizard-of-Oz set-up. For CCPE-M, we show how a simple lexical bias for the word like in the guidelines biases the data collection. This bias, in effect, leads to poor performance on data without this bias: a preference elicitation architecture based on BERT suffers a 5.3% absolute drop in performance, when like is replaced with a synonymous phrase, and a 13.2% drop in performance when evaluated on out-of-sample data. For Taskmaster-1, we show how the order in which instructions are resented, biases the data collection.</abstract>
      <url hash="4aab095f">2021.bppf-1.2</url>
      <doi>10.18653/v1/2021.bppf-1.2</doi>
      <bibkey>hansen-sogaard-2021-guideline</bibkey>
      <pwccode url="https://github.com/vpetren/guideline_bias" additional="false">vpetren/guideline_bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ccpe-m">CCPE-M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/taskmaster-1">Taskmaster-1</pwcdataset>
    </paper>
    <paper id="3">
      <title>We Need to Consider Disagreement in Evaluation</title>
      <author><first>Valerio</first><last>Basile</last></author>
      <author><first>Michael</first><last>Fell</last></author>
      <author><first>Tommaso</first><last>Fornaciari</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Silviu</first><last>Paun</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Massimo</first><last>Poesio</last></author>
      <author><first>Alexandra</first><last>Uma</last></author>
      <pages>15–21</pages>
      <abstract>Evaluation is of paramount importance in data-driven research fields such as Natural Language Processing (NLP) and Computer Vision (CV). Current evaluation practice largely hinges on the existence of a single “ground truth” against which we can meaningfully compare the prediction of a model. However, this comparison is flawed for two reasons. 1) In many cases, more than one answer is correct. 2) Even where there is a single answer, disagreement among annotators is ubiquitous, making it difficult to decide on a gold standard. We argue that the current methods of adjudication, agreement, and evaluation need serious reconsideration. Some researchers now propose to minimize disagreement and to fix datasets. We argue that this is a gross oversimplification, and likely to conceal the underlying complexity. Instead, we suggest that we need to better capture the sources of disagreement to improve today’s evaluation practice. We discuss three sources of disagreement: from the annotator, the data, and the context, and show how this affects even seemingly objective tasks. Datasets with multiple annotations are becoming more common, as are methods to integrate disagreement into modeling. The logical next step is to extend this to evaluation.</abstract>
      <url hash="52c712dc">2021.bppf-1.3</url>
      <doi>10.18653/v1/2021.bppf-1.3</doi>
      <bibkey>basile-etal-2021-need</bibkey>
    </paper>
    <paper id="4">
      <title>How Might We Create Better Benchmarks for Speech Recognition?</title>
      <author><first>Alëna</first><last>Aksënova</last></author>
      <author><first>Daan</first><last>van Esch</last></author>
      <author><first>James</first><last>Flynn</last></author>
      <author><first>Pavel</first><last>Golik</last></author>
      <pages>22–34</pages>
      <abstract>The applications of automatic speech recognition (ASR) systems are proliferating, in part due to recent significant quality improvements. However, as recent work indicates, even state-of-the-art speech recognition systems – some which deliver impressive benchmark results, struggle to generalize across use cases. We review relevant work, and, hoping to inform future benchmark development, outline a taxonomy of speech recognition use cases, proposed for the next generation of ASR benchmarks. We also survey work on metrics, in addition to the de facto standard Word Error Rate (WER) metric, and we introduce a versatile framework designed to describe interactions between linguistic variation and ASR performance metrics.</abstract>
      <url hash="2eee939f">2021.bppf-1.4</url>
      <doi>10.18653/v1/2021.bppf-1.4</doi>
      <bibkey>aksenova-etal-2021-might</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/common-voice">Common Voice</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
  </volume>
</collection>
