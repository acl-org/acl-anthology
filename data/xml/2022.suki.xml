<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.suki">
  <volume id="1" ingest-date="2022-06-29">
    <meta>
      <booktitle>Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI)</booktitle>
      <editor><first>Wenhu</first><last>Chen</last></editor>
      <editor><first>Xinyun</first><last>Chen</last></editor>
      <editor><first>Zhiyu</first><last>Chen</last></editor>
      <editor><first>Ziyu</first><last>Yao</last></editor>
      <editor><first>Michihiro</first><last>Yasunaga</last></editor>
      <editor><first>Tao</first><last>Yu</last></editor>
      <editor><first>Rui</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, USA</address>
      <month>July</month>
      <year>2022</year>
      <url hash="dc2bdc76">2022.suki-1</url>
      <venue>suki</venue>
    </meta>
    <frontmatter>
      <url hash="25851b88">2022.suki-1.0</url>
      <bibkey>suki-2022-structured</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>F</fixed-case>ab<fixed-case>KG</fixed-case>: A Knowledge graph of Manufacturing Science domain utilizing structured and unconventional unstructured knowledge source</title>
      <author><first>Aman</first><last>Kumar</last></author>
      <author><first>Akshay</first><last>Bharadwaj</last></author>
      <author><first>Binil</first><last>Starly</last></author>
      <author><first>Collin</first><last>Lynch</last></author>
      <pages>1-8</pages>
      <abstract>As the demands for large-scale information processing have grown, knowledge graph-based approaches have gained prominence for representing general and domain knowledge. The development of such general representations is essential, particularly in domains such as manufacturing which intelligent processes and adaptive education can enhance. Despite the continuous accumulation of text in these domains, the lack of structured data has created information extraction and knowledge transfer barriers. In this paper, we report on work towards developing robust knowledge graphs based upon entity and relation data for both commercial and educational uses. To create the FabKG (Manufacturing knowledge graph), we have utilized textbook index words, research paper keywords, FabNER (manufacturing NER), to extract a sub knowledge base contained within Wikidata. Moreover, we propose a novel crowdsourcing method for KG creation by leveraging student notes, which contain invaluable information but are not captured as meaningful information, excluding their use in personal preparation for learning and written exams. We have created a knowledge graph containing 65000+ triples using all data sources. We have also shown the use case of domain-specific question answering and expression/formula-based question answering for educational purposes.</abstract>
      <url hash="836d6db8">2022.suki-1.1</url>
      <bibkey>kumar-etal-2022-fabkg</bibkey>
      <doi>10.18653/v1/2022.suki-1.1</doi>
    </paper>
    <paper id="2">
      <title>Modeling Compositionality with Dependency Graph for Dialogue Generation</title>
      <author><first>Xiaofeng</first><last>Chen</last></author>
      <author><first>Yirong</first><last>Chen</last></author>
      <author><first>Xiaofen</first><last>Xing</last></author>
      <author><first>Xiangmin</first><last>Xu</last></author>
      <author><first>Wenjing</first><last>Han</last></author>
      <author><first>Qianfeng</first><last>Tie</last></author>
      <pages>9-16</pages>
      <abstract>Because of the compositionality of natural language, syntactic structure which contains the information about the relationship between words is a key factor for semantic understanding. However, the widely adopted Transformer is hard to learn the syntactic structure effectively in dialogue generation tasks. To explicitly model the compositionaity of language in Transformer Block, we restrict the information flow between words by constructing directed dependency graph and propose Dependency Relation Attention (DRA). Experimental results demonstrate that DRA can further improve the performance of state-of-the-art models for dialogue generation.</abstract>
      <url hash="e3cb1817">2022.suki-1.2</url>
      <bibkey>chen-etal-2022-modeling-compositionality</bibkey>
      <doi>10.18653/v1/2022.suki-1.2</doi>
    </paper>
    <paper id="3">
      <title>Strategies to Improve Few-shot Learning for Intent Classification and Slot-Filling</title>
      <author><first>Samyadeep</first><last>Basu</last></author>
      <author><first>Amr</first><last>Sharaf</last></author>
      <author><first>Karine</first><last>Ip Kiun Chong</last></author>
      <author><first>Alex</first><last>Fischer</last></author>
      <author><first>Vishal</first><last>Rohra</last></author>
      <author><first>Michael</first><last>Amoake</last></author>
      <author><first>Hazem</first><last>El-Hammamy</last></author>
      <author><first>Ehi</first><last>Nosakhare</last></author>
      <author><first>Vijay</first><last>Ramani</last></author>
      <author><first>Benjamin</first><last>Han</last></author>
      <pages>17-25</pages>
      <abstract>Intent classification (IC) and slot filling (SF) are two fundamental tasks in modern Natural Language Understanding (NLU) systems. Collecting and annotating large amounts of data to train deep learning models for such systems are not scalable. This problem can be addressed by learning from few examples using fast supervised meta-learning techniques such as prototypical networks. In this work, we systematically investigate how contrastive learning and data augmentation methods can benefit these existing meta-learning pipelines for jointly modelled IC/SF tasks. Through extensive experiments across standard IC/SF benchmarks (SNIPS and ATIS), we show that our proposed approaches outperform standard meta-learning methods: contrastive losses as a regularizer in conjunction with prototypical networks consistently outperform the existing state-of-the-art for both IC and SF tasks, while data augmentation strategies primarily improve few-shot IC by a significant margin</abstract>
      <url hash="827c40f2">2022.suki-1.3</url>
      <bibkey>basu-etal-2022-strategies</bibkey>
      <doi>10.18653/v1/2022.suki-1.3</doi>
      <video href="2022.suki-1.3.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="4">
      <title>Learning Open Domain Multi-hop Search Using Reinforcement Learning</title>
      <author><first>Enrique</first><last>Noriega-Atala</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <author><first>Clayton</first><last>Morrison</last></author>
      <pages>26-35</pages>
      <abstract>We propose a method to teach an automated agent to learn how to search for multi-hop paths of relations between entities in an open domain. The method learns a policy for directing existing information retrieval and machine reading resources to focus on relevant regions of a corpus. The approach formulates the learning problem as a Markov decision process with a state representation that encodes the dynamics of the search process and a reward structure that minimizes the number of documents that must be processed while still finding multi-hop paths. We implement the method in an actor-critic reinforcement learning algorithm and evaluate it on a dataset of search problems derived from a subset of English Wikipedia. The algorithm finds a family of policies that succeeds in extracting the desired information while processing fewer documents compared to several baseline heuristic algorithms.</abstract>
      <url hash="b8775896">2022.suki-1.4</url>
      <bibkey>noriega-atala-etal-2022-learning</bibkey>
      <doi>10.18653/v1/2022.suki-1.4</doi>
      <video href="2022.suki-1.4.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="5">
      <title>Table Retrieval May Not Necessitate Table-specific Model Design</title>
      <author><first>Zhiruo</first><last>Wang</last></author>
      <author><first>Zhengbao</first><last>Jiang</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>36-46</pages>
      <abstract>Tables are an important form of structured data for both human and machine readers alike, providing answers to questions that cannot, or cannot easily, be found in texts. Recent work has designed special models and training paradigms for table-related tasks such as table-based question answering and table retrieval. Though effective, they add complexity in both modeling and data acquisition compared to generic text solutions and obscure which elements are truly beneficial. In this work, we focus on the task of table retrieval, and ask: “is table-specific model design necessary for table retrieval, or can a simpler text-based model be effectively used to achieve a similar result?’’ First, we perform an analysis on a table-based portion of the Natural Questions dataset (NQ-table), and find that structure plays a negligible role in more than 70% of the cases. Based on this, we experiment with a general Dense Passage Retriever (DPR) based on text and a specialized Dense Table Retriever (DTR) that uses table-specific model designs. We find that DPR performs well without any table-specific design and training, and even achieves superior results compared to DTR when fine-tuned on properly linearized tables. We then experiment with three modules to explicitly encode table structures, namely auxiliary row/column embeddings, hard attention masks, and soft relation-based attention biases. However, none of these yielded significant improvements, suggesting that table-specific model design may not be necessary for table retrieval.</abstract>
      <url hash="557212f3">2022.suki-1.5</url>
      <bibkey>wang-etal-2022-table</bibkey>
      <doi>10.18653/v1/2022.suki-1.5</doi>
      <video href="2022.suki-1.5.mp4"/>
      <pwccode url="https://github.com/zorazrw/nqt-retrieval" additional="false">zorazrw/nqt-retrieval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="6">
      <title>Transfer Learning and Masked Generation for Answer Verbalization</title>
      <author><first>Sebastien</first><last>Montella</last></author>
      <author><first>Lina</first><last>Rojas-Barahona</last></author>
      <author><first>Frederic</first><last>Bechet</last></author>
      <author><first>Johannes</first><last>Heinecke</last></author>
      <author><first>Alexis</first><last>Nasr</last></author>
      <pages>47-54</pages>
      <abstract>Structured Knowledge has recently emerged as an essential component to support fine-grained Question Answering (QA). In general, QA systems query a Knowledge Base (KB) to detect and extract the raw answers as final prediction. However, as lacking of context, language generation can offer a much informative and complete response. In this paper, we propose to combine the power of transfer learning and the advantage of entity placeholders to produce high-quality verbalization of extracted answers from a KB. We claim that such approach is especially well-suited for answer generation. Our experiments show 44.25%, 3.26% and 29.10% relative gain in BLEU over the state-of-the-art on the VQuAnDA, ParaQA and VANiLLa datasets, respectively. We additionally provide minor hallucinations corrections in VANiLLa standing for 5% of each of the training and testing set. We witness a median absolute gain of 0.81 SacreBLEU. This strengthens the importance of data quality when using automated evaluation.</abstract>
      <url hash="43c256c2">2022.suki-1.6</url>
      <bibkey>montella-etal-2022-transfer</bibkey>
      <doi>10.18653/v1/2022.suki-1.6</doi>
      <video href="2022.suki-1.6.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/paraqa">ParaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vanilla">VANiLLa</pwcdataset>
    </paper>
    <paper id="7">
      <title>Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering</title>
      <author><first>Lingbo</first><last>Mo</last></author>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Jie</first><last>Zhao</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <pages>55-66</pages>
      <abstract>Multi-hop question answering (QA) combines multiple pieces of evidence to search for the correct answer. Reasoning over a text corpus (TextQA) and/or a knowledge base (KBQA) has been extensively studied and led to distinct system architectures. However, knowledge transfer between such two QA systems has been under-explored. Research questions like what knowledge is transferred or whether the transferred knowledge can help answer over one source using another one, are yet to be answered. In this paper, therefore, we study the knowledge transfer of multi-hop reasoning between structured and unstructured sources. We first propose a unified QA framework named SimultQA to enable knowledge transfer and bridge the distinct supervisions from KB and text sources. Then, we conduct extensive analyses to explore how knowledge is transferred by leveraging the pre-training and fine-tuning paradigm. We focus on the low-resource fine-tuning to show that pre-training SimultQA on one source can substantially improve its performance on the other source. More fine-grained analyses on transfer behaviors reveal the types of transferred knowledge and transfer patterns. We conclude with insights into how to construct better QA datasets and systems to exploit knowledge transfer for future work.</abstract>
      <url hash="7fdd8ffc">2022.suki-1.7</url>
      <bibkey>mo-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.suki-1.7</doi>
      <video href="2022.suki-1.7.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
    </paper>
    <paper id="8">
      <title>Hierarchical Control of Situated Agents through Natural Language</title>
      <author><first>Shuyan</first><last>Zhou</last></author>
      <author><first>Pengcheng</first><last>Yin</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>67-84</pages>
      <abstract>When humans perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, most works on natural language (NL) command of situated agents have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a method for representing hierarchical procedural knowledge for agent command and control aimed at enabling easy application to various scenarios. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that our framework is more data-efficient, and that it allows for fast iterative development.</abstract>
      <url hash="901a9f1e">2022.suki-1.8</url>
      <bibkey>zhou-etal-2022-hierarchical-control</bibkey>
      <doi>10.18653/v1/2022.suki-1.8</doi>
      <video href="2022.suki-1.8.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/alfred">ALFRED</pwcdataset>
    </paper>
  </volume>
</collection>
