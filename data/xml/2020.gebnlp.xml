<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.gebnlp">
  <volume id="1" ingest-date="2020-12-10">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Gender Bias in Natural Language Processing</booktitle>
      <editor><first>Marta R.</first><last>Costa-jussà</last></editor>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Will</first><last>Radford</last></editor>
      <editor><first>Kellie</first><last>Webster</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
      <venue>gebnlp</venue>
    </meta>
    <frontmatter>
      <url hash="2ab94dbb">2020.gebnlp-1.0</url>
      <bibkey>gebnlp-2020-gender</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Unmasking Contextual Stereotypes: Measuring and Mitigating <fixed-case>BERT</fixed-case>’s Gender Bias</title>
      <author><first>Marion</first><last>Bartl</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <pages>1–16</pages>
      <abstract>Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models.</abstract>
      <url hash="a83b80c1">2020.gebnlp-1.1</url>
      <bibkey>bartl-etal-2020-unmasking</bibkey>
      <pwccode url="https://github.com/marionbartl/gender-bias-BERT" additional="false">marionbartl/gender-bias-BERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="2">
      <title>Interdependencies of Gender and Race in Contextualized Word Embeddings</title>
      <author><first>May</first><last>Jiang</last></author>
      <author><first>Christiane</first><last>Fellbaum</last></author>
      <pages>17–25</pages>
      <abstract>Recent years have seen a surge in research on the biases in word embeddings with respect to gender and, to a lesser extent, race. Few of these studies, however, have given attention to the critical intersection of race and gender. In this case study, we analyze the dimensions of gender and race in contextualized word embeddings of given names, taken from BERT, and investigate the nature and nuance of their interaction. We find that these demographic axes, though typically treated as physically and conceptually separate, are in fact interdependent and thus inadvisable to consider in isolation. Further, we show that demographic dimensions predicated on default settings in language, such as in pronouns, may risk rendering groups with multiple marginalized identities invisible. We conclude by discussing the importance and implications of intersectionality for future studies on bias and debiasing in NLP.</abstract>
      <url hash="cc5715e8">2020.gebnlp-1.2</url>
      <bibkey>jiang-fellbaum-2020-interdependencies</bibkey>
    </paper>
    <paper id="3">
      <title>Fine-tuning Neural Machine Translation on Gender-Balanced Datasets</title>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <author><first>Adrià</first><last>de Jorge</last></author>
      <pages>26–34</pages>
      <abstract>Misrepresentation of certain communities in datasets is causing big disruptions in artificial intelligence applications. In this paper, we propose using an automatically extracted gender-balanced dataset parallel corpus from Wikipedia. This balanced set is used to perform fine-tuning techniques from a bigger model trained on unbalanced datasets to mitigate gender biases in neural machine translation.</abstract>
      <url hash="9ce7ac05">2020.gebnlp-1.3</url>
      <bibkey>costa-jussa-de-jorge-2020-fine</bibkey>
    </paper>
    <paper id="4">
      <title>Neural Machine Translation Doesn’t Translate Gender Coreference Right Unless You Make It</title>
      <author><first>Danielle</first><last>Saunders</last></author>
      <author><first>Rosie</first><last>Sallis</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>35–43</pages>
      <abstract>Neural Machine Translation (NMT) has been shown to struggle with grammatical gender that is dependent on the gender of human referents, which can cause gender bias effects. Many existing approaches to this problem seek to control gender inflection in the target language by explicitly or implicitly adding a gender feature to the source sentence, usually at the sentence level. In this paper we propose schemes for incorporating explicit word-level gender inflection tags into NMT. We explore the potential of this gender-inflection controlled translation when the gender feature can be determined from a human reference, or when a test sentence can be automatically gender-tagged, assessing on English-to-Spanish and English-to-German translation. We find that simple existing approaches can over-generalize a gender-feature to multiple entities in a sentence, and suggest effective alternatives in the form of tagged coreference adaptation data. We also propose an extension to assess translations of gender-neutral entities from English given a corresponding linguistic convention, such as a non-binary inflection, in the target language.</abstract>
      <url hash="e8cc84e6">2020.gebnlp-1.4</url>
      <bibkey>saunders-etal-2020-neural</bibkey>
      <pwccode url="https://github.com/DCSaunders/tagged-gender-coref" additional="false">DCSaunders/tagged-gender-coref</pwccode>
    </paper>
    <paper id="5">
      <title>Can Existing Methods Debias Languages Other than <fixed-case>E</fixed-case>nglish? First Attempt to Analyze and Mitigate <fixed-case>J</fixed-case>apanese Word Embeddings</title>
      <author><first>Masashi</first><last>Takeshita</last></author>
      <author><first>Yuki</first><last>Katsumata</last></author>
      <author><first>Rafal</first><last>Rzepka</last></author>
      <author><first>Kenji</first><last>Araki</last></author>
      <pages>44–55</pages>
      <abstract>It is known that word embeddings exhibit biases inherited from the corpus, and those biases reflect social stereotypes. Recently, many studies have been conducted to analyze and mitigate biases in word embeddings. Unsupervised Bias Enumeration (UBE) (Swinger et al., 2019) is one of approach to analyze biases for English, and Hard Debias (Bolukbasi et al., 2016) is the common technique to mitigate gender bias. These methods focused on English, or, in smaller extent, on Indo-European languages. However, it is not clear whether these methods can be generalized to other languages. In this paper, we apply these analyzing and mitigating methods, UBE and Hard Debias, to Japanese word embeddings. Additionally, we examine whether these methods can be used for Japanese. We experimentally show that UBE and Hard Debias cannot be sufficiently adapted to Japanese embeddings.</abstract>
      <url hash="d864d2e5">2020.gebnlp-1.5</url>
      <revision id="1" href="2020.gebnlp-1.5v1" hash="f9b081d0"/>
      <revision id="2" href="2020.gebnlp-1.5v2" hash="d864d2e5" date="2021-03-26">Updates a reference</revision>
      <bibkey>takeshita-etal-2020-existing</bibkey>
    </paper>
    <paper id="6">
      <title>Evaluating Bias In <fixed-case>D</fixed-case>utch Word Embeddings</title>
      <author><first>Rodrigo Alejandro</first><last>Chávez Mulsa</last></author>
      <author><first>Gerasimos</first><last>Spanakis</last></author>
      <pages>56–71</pages>
      <abstract>Recent research in Natural Language Processing has revealed that word embeddings can encode social biases present in the training data which can affect minorities in real world applications. This paper explores the gender bias implicit in Dutch embeddings while investigating whether English language based approaches can also be used in Dutch. We implement the Word Embeddings Association Test (WEAT), Clustering and Sentence Embeddings Association Test (SEAT) methods to quantify the gender bias in Dutch word embeddings, then we proceed to reduce the bias with Hard-Debias and Sent-Debias mitigation methods and finally we evaluate the performance of the debiased embeddings in downstream tasks. The results suggest that, among others, gender bias is present in traditional and contextualized Dutch word embeddings. We highlight how techniques used to measure and reduce bias created for English can be used in Dutch embeddings by adequately translating the data and taking into account the unique characteristics of the language. Furthermore, we analyze the effect of the debiasing techniques on downstream tasks which show a negligible impact on traditional embeddings and a 2% decrease in performance in contextualized embeddings. Finally, we release the translated Dutch datasets to the public along with the traditional embeddings with mitigated bias.</abstract>
      <url hash="8ca5a9da">2020.gebnlp-1.6</url>
      <bibkey>chavez-mulsa-spanakis-2020-evaluating</bibkey>
      <pwccode url="https://github.com/Noixas/Official-Evaluating-Bias-In-Dutch" additional="false">Noixas/Official-Evaluating-Bias-In-Dutch</pwccode>
    </paper>
    <paper id="7">
      <title>Conversational Assistants and Gender Stereotypes: Public Perceptions and Desiderata for Voice Personas</title>
      <author><first>Amanda</first><last>Cercas Curry</last></author>
      <author><first>Judy</first><last>Robertson</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>72–78</pages>
      <abstract>Conversational voice assistants are rapidly developing from purely transactional systems to social companions with “personality”. UNESCO recently stated that the female and submissive personality of current digital assistants gives rise for concern as it reinforces gender stereotypes. In this work, we present results from a participatory design workshop, where we invite people to submit their preferences for a what their ideal persona might look like, both in drawings as well as in a multiple choice questionnaire. We find no clear consensus which suggests that one possible solution is to let people configure/personalise their assistants. We then outline a multi-disciplinary project of how we plan to address the complex question of gender and stereotyping in digital assistants.</abstract>
      <url hash="1ebcee76">2020.gebnlp-1.7</url>
      <bibkey>cercas-curry-etal-2020-conversational</bibkey>
    </paper>
    <paper id="8">
      <title>Semi-Supervised Topic Modeling for Gender Bias Discovery in <fixed-case>E</fixed-case>nglish and <fixed-case>S</fixed-case>wedish</title>
      <author><first>Hannah</first><last>Devinney</last></author>
      <author><first>Jenny</first><last>Björklund</last></author>
      <author><first>Henrik</first><last>Björklund</last></author>
      <pages>79–92</pages>
      <abstract>Gender bias has been identified in many models for Natural Language Processing, stemming from implicit biases in the text corpora used to train the models. Such corpora are too large to closely analyze for biased or stereotypical content. Thus, we argue for a combination of quantitative and qualitative methods, where the quantitative part produces a view of the data of a size suitable for qualitative analysis. We investigate the usefulness of semi-supervised topic modeling for the detection and analysis of gender bias in three corpora (mainstream news articles in English and Swedish, and LGBTQ+ web content in English). We compare differences in topic models for three gender categories (masculine, feminine, and nonbinary or neutral) in each corpus. We find that in all corpora, genders are treated differently and that these differences tend to correspond to hegemonic ideas of gender.</abstract>
      <url hash="0fb7f6ce">2020.gebnlp-1.8</url>
      <bibkey>devinney-etal-2020-semi</bibkey>
    </paper>
    <paper id="9">
      <title>Investigating Societal Biases in a Poetry Composition System</title>
      <author><first>Emily</first><last>Sheng</last></author>
      <author><first>David</first><last>Uthus</last></author>
      <pages>93–106</pages>
      <abstract>There is a growing collection of work analyzing and mitigating societal biases in language understanding, generation, and retrieval tasks, though examining biases in creative tasks remains underexplored. Creative language applications are meant for direct interaction with users, so it is important to quantify and mitigate societal biases in these applications. We introduce a novel study on a pipeline to mitigate societal biases when retrieving next verse suggestions in a poetry composition system. Our results suggest that data augmentation through sentiment style transfer has potential for mitigating societal biases.</abstract>
      <url hash="e9629c82">2020.gebnlp-1.9</url>
      <bibkey>sheng-uthus-2020-investigating</bibkey>
      <pwccode url="https://github.com/google-research-datasets/poem-sentiment" additional="false">google-research-datasets/poem-sentiment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gutenberg-poem-dataset">Gutenberg Poem Dataset</pwcdataset>
    </paper>
    <paper id="10">
      <title>Situated Data, Situated Systems: A Methodology to Engage with Power Relations in Natural Language Processing Research</title>
      <author><first>Lucy</first><last>Havens</last></author>
      <author><first>Melissa</first><last>Terras</last></author>
      <author><first>Benjamin</first><last>Bach</last></author>
      <author><first>Beatrice</first><last>Alex</last></author>
      <pages>107–124</pages>
      <abstract>We propose a bias-aware methodology to engage with power relations in natural language processing (NLP) research. NLP research rarely engages with bias in social contexts, limiting its ability to mitigate bias. While researchers have recommended actions, technical methods, and documentation practices, no methodology exists to integrate critical reflections on bias with technical NLP methods. In this paper, after an extensive and interdisciplinary literature review, we contribute a bias-aware methodology for NLP research. We also contribute a definition of biased text, a discussion of the implications of biased NLP systems, and a case study demonstrating how we are executing the bias-aware methodology in research on archival metadata descriptions.</abstract>
      <url hash="273c2634">2020.gebnlp-1.10</url>
      <bibkey>havens-etal-2020-situated</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
    </paper>
    <paper id="11">
      <title>Gender and sentiment, critics and authors: a dataset of <fixed-case>N</fixed-case>orwegian book reviews</title>
      <author><first>Samia</first><last>Touileb</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>125–138</pages>
      <abstract>Gender bias in models and datasets is widely studied in NLP. The focus has usually been on analysing how females and males express themselves, or how females and males are described. However, a less studied aspect is the combination of these two perspectives, how female and male describe the same or opposite gender. In this paper, we present a new gender annotated sentiment dataset of critics reviewing the works of female and male authors. We investigate if this newly annotated dataset contains differences in how the works of male and female authors are critiqued, in particular in terms of positive and negative sentiment. We also explore the differences in how this is done by male and female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively.</abstract>
      <url hash="bad90b76">2020.gebnlp-1.11</url>
      <bibkey>touileb-etal-2020-gender</bibkey>
      <pwccode url="https://github.com/ltgoslo/norec_gender" additional="false">ltgoslo/norec_gender</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/norec">NoReC</pwcdataset>
    </paper>
    <paper id="12">
      <title>Gender-Aware Reinflection using Linguistically Enhanced Neural Models</title>
      <author><first>Bashar</first><last>Alhafni</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <pages>139–150</pages>
      <abstract>In this paper, we present an approach for sentence-level gender reinflection using linguistically enhanced sequence-to-sequence models. Our system takes an Arabic sentence and a given target gender as input and generates a gender-reinflected sentence based on the target gender. We formulate the problem as a user-aware grammatical error correction task and build an encoder-decoder architecture to jointly model reinflection for both masculine and feminine grammatical genders. We also show that adding linguistic features to our model leads to better reinflection results. The results on a blind test set using our best system show improvements over previous work, with a 3.6% absolute increase in M2 F0.5.</abstract>
      <url hash="816d13db">2020.gebnlp-1.12</url>
      <bibkey>alhafni-etal-2020-gender</bibkey>
      <pwccode url="https://github.com/camel-lab/gender-reinflection" additional="false">camel-lab/gender-reinflection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
  </volume>
</collection>
