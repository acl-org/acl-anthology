<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.textgraphs">
  <volume id="1" ingest-date="2022-10-06">
    <meta>
      <booktitle>Proceedings of TextGraphs-16: Graph-based Methods for Natural Language Processing</booktitle>
      <editor><first>Dmitry</first><last>Ustalov</last></editor>
      <editor><first>Yanjun</first><last>Gao</last></editor>
      <editor><first>Alexander</first><last>Panchenko</last></editor>
      <editor><first>Marco</first><last>Valentino</last></editor>
      <editor><first>Mokanarangan</first><last>Thayaparan</last></editor>
      <editor><first>Thien Huu</first><last>Nguyen</last></editor>
      <editor><first>Gerald</first><last>Penn</last></editor>
      <editor><first>Arti</first><last>Ramesh</last></editor>
      <editor><first>Abhik</first><last>Jana</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Gyeongju, Republic of Korea</address>
      <month>October</month>
      <year>2022</year>
      <url hash="c2e2a382">2022.textgraphs-1</url>
      <venue>textgraphs</venue>
    </meta>
    <frontmatter>
      <url hash="4dd3b289">2022.textgraphs-1.0</url>
      <bibkey>textgraphs-2022-textgraphs</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Multilevel Hypernode Graphs for Effective and Efficient Entity Linking</title>
      <author><first>David</first><last>Montero</last></author>
      <author><first>Javier</first><last>Martínez</last></author>
      <author><first>Javier</first><last>Yebes</last></author>
      <pages>1–10</pages>
      <abstract>Information extraction on documents still remains a challenge, especially when dealing with unstructured documents with complex and variable layouts. Graph Neural Networks seem to be a promising approach to overcome these difficulties due to their flexible and sparse nature, but they have not been exploited yet. In this work, we present a multi-level graph-based model that performs entity building and linking on unstructured documents, purely based on GNNs, and extremely light (0.3 million parameters). We also propose a novel strategy for an optimal propagation of the information between the graph levels based on hypernodes. The conducted experiments on public and private datasets demonstrate that our model is suitable for solving the tasks, and that the proposed propagation strategy is optimal and outperforms other approaches.</abstract>
      <url hash="875d20c2">2022.textgraphs-1.1</url>
      <bibkey>montero-etal-2022-multilevel</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cord">CORD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/funsd">FUNSD</pwcdataset>
    </paper>
    <paper id="2">
      <title>Cross-Modal Contextualized Hidden State Projection Method for Expanding of Taxonomic Graphs</title>
      <author><first>Irina</first><last>Nikishina</last></author>
      <author><first>Alsu</first><last>Vakhitova</last></author>
      <author><first>Elena</first><last>Tutubalina</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>11–24</pages>
      <abstract>Taxonomy is a graph of terms organized hierarchically using is-a (hypernymy) relations. We suggest novel candidate-free task formulation for the taxonomy enrichment task. To solve the task, we leverage lexical knowledge from the pre-trained models to predict new words missing in the taxonomic resource. We propose a method that combines graph-, and text-based contextualized representations from transformer networks to predict new entries to the taxonomy. We have evaluated the method suggested for this task against text-only baselines based on BERT and fastText representations. The results demonstrate that incorporation of graph embedding is beneficial in the task of hyponym prediction using contextualized models. We hope the new challenging task will foster further research in automatic text graph construction methods.</abstract>
      <url hash="a99161d8">2022.textgraphs-1.2</url>
      <bibkey>nikishina-etal-2022-cross</bibkey>
    </paper>
    <paper id="3">
      <title>Sharing Parameter by Conjugation for Knowledge Graph Embeddings in Complex Space</title>
      <author><first>Xincan</first><last>Feng</last></author>
      <author><first>Zhi</first><last>Qu</last></author>
      <author><first>Yuchang</first><last>Cheng</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Nobuhiro</first><last>Yugami</last></author>
      <pages>25–34</pages>
      <abstract>A Knowledge Graph (KG) is the directed graphical representation of entities and relations in the real world. KG can be applied in diverse Natural Language Processing (NLP) tasks where knowledge is required. The need to scale up and complete KG automatically yields Knowledge Graph Embedding (KGE), a shallow machine learning model that is suffering from memory and training time consumption issues. To mitigate the computational load, we propose a parameter-sharing method, i.e., using conjugate parameters for complex numbers employed in KGE models. Our method improves memory efficiency by 2x in relation embedding while achieving comparable performance to the state-of-the-art non-conjugate models, with faster, or at least comparable, training time. We demonstrated the generalizability of our method on two best-performing KGE models <tex-math>5^{\bigstar}\mathrm{E}</tex-math> (CITATION) and <tex-math>\mathrm{ComplEx}</tex-math> (CITATION) on five benchmark datasets.</abstract>
      <url hash="04e17e68">2022.textgraphs-1.3</url>
      <bibkey>feng-etal-2022-sharing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="4">
      <title>A Clique-based Graphical Approach to Detect Interpretable Adjectival Senses in <fixed-case>H</fixed-case>ungarian</title>
      <author><first>Enikő</first><last>Héja</last></author>
      <author><first>Noémi</first><last>Ligeti-Nagy</last></author>
      <pages>35–43</pages>
      <abstract>The present paper introduces an ongoing research which aims to detect interpretable adjectival senses from monolingual corpora applying an unsupervised WSI approach. According to our expectations the findings of our investigation are going to contribute to the work of lexicographers, linguists and also facilitate the creation of benchmarks with semantic information for the NLP community. For doing so, we set up four criteria to distinguish between senses. We experiment with a graphical approach to model our criteria and then perform a detailed, linguistically motivated manual evaluation of the results.</abstract>
      <url hash="2b61e6b3">2022.textgraphs-1.4</url>
      <bibkey>heja-ligeti-nagy-2022-clique</bibkey>
      <pwccode url="https://github.com/nytud/huwic" additional="false">nytud/huwic</pwccode>
    </paper>
    <paper id="5">
      <title><fixed-case>GUSUM</fixed-case>: Graph-based Unsupervised Summarization Using Sentence Features Scoring and Sentence-<fixed-case>BERT</fixed-case></title>
      <author><first>Tuba</first><last>Gokhan</last></author>
      <author><first>Phillip</first><last>Smith</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>44–53</pages>
      <abstract>Unsupervised extractive document summarization aims to extract salient sentences from a document without requiring a labelled corpus. In existing graph-based methods, vertex and edge weights are usually created by calculating sentence similarities. In this paper, we develop a Graph-Based Unsupervised Summarization(GUSUM) method for extractive text summarization based on the principle of including the most important sentences while excluding sentences with similar meanings in the summary. We modify traditional graph ranking algorithms with recent sentence embedding models and sentence features and modify how sentence centrality is computed. We first define the sentence feature scores represented at the vertices, indicating the importance of each sentence in the document. After this stage, we use Sentence-BERT for obtaining sentence embeddings to better capture the sentence meaning. In this way, we define the edges of a graph where semantic similarities are represented. Next we create an undirected graph that includes sentence significance and similarities between sentences. In the last stage, we determine the most important sentences in the document with the ranking method we suggested on the graph created. Experiments on CNN/Daily Mail, New York Times, arXiv, and PubMed datasets show our approach achieves high performance on unsupervised graph-based summarization when evaluated both automatically and by humans.</abstract>
      <url hash="a92de595">2022.textgraphs-1.5</url>
      <bibkey>gokhan-etal-2022-gusum</bibkey>
      <pwccode url="https://github.com/tubagokhan/gusum" additional="false">tubagokhan/gusum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="6">
      <title>The Effectiveness of Masked Language Modeling and Adapters for Factual Knowledge Injection</title>
      <author><first>Sondre</first><last>Wold</last></author>
      <pages>54–59</pages>
      <abstract>This paper studies the problem of injecting factual knowledge into large pre-trained language models. We train adapter modules on parts of the ConceptNet knowledge graph using the masked language modeling objective and evaluate the success of the method by a series of probing experiments on the LAMA probe. Mean P@K curves for different configurations indicate that the technique is effective, increasing the performance on sub-sets of the LAMA probe for large values of k by adding as little as 2.1% additional parameters to the original models.</abstract>
      <url hash="5fe1ed86">2022.textgraphs-1.6</url>
      <bibkey>wold-2022-effectiveness</bibkey>
      <pwccode url="https://github.com/sondrewold/adapters-mlm-injection" additional="false">sondrewold/adapters-mlm-injection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="7">
      <title>Text-Aware Graph Embeddings for Donation Behavior Prediction</title>
      <author><first>MeiXing</first><last>Dong</last></author>
      <author><first>Xueming</first><last>Xu</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>60–69</pages>
      <abstract>Predicting user behavior is essential for a large number of applications including recommender and dialog systems, and more broadly in domains such as healthcare, education, and economics. In this paper, we show that we can effectively predict donation behavior by using text-aware graph models, building upon graphs that connect user behaviors and their interests. Using a university donation dataset, we show that the graph representation significantly improves over learning from textual representations. Moreover, we show how incorporating implicit information inferred from text associated with the graph entities brings additional improvements. Our results demonstrate the role played by text-aware graph representations in predicting donation behavior.</abstract>
      <url hash="6301d0bc">2022.textgraphs-1.7</url>
      <bibkey>dong-etal-2022-text</bibkey>
    </paper>
    <paper id="8">
      <title>Word Sense Disambiguation of <fixed-case>F</fixed-case>rench Lexicographical Examples Using Lexical Networks</title>
      <author><first>Aman</first><last>Sinha</last></author>
      <author><first>Sandrine</first><last>Ollinger</last></author>
      <author><first>Mathieu</first><last>Constant</last></author>
      <pages>70–76</pages>
      <abstract>This paper focuses on the task of word sense disambiguation (WSD) on lexicographic examples relying on the French Lexical Network (fr-LN). For this purpose, we exploit the lexical and relational properties of the network, that we integrated in a feedforward neural WSD model on top of pretrained French BERT embeddings. We provide a comparative study with various models and further show the impact of our approach regarding polysemic units.</abstract>
      <url hash="463bcec4">2022.textgraphs-1.8</url>
      <bibkey>sinha-etal-2022-word</bibkey>
      <pwccode url="https://github.com/atilf-umr7118/graphwsd" additional="false">atilf-umr7118/graphwsd</pwccode>
    </paper>
    <paper id="9">
      <title><fixed-case>R</fixed-case>u<fixed-case>DSI</fixed-case>: Graph-based Word Sense Induction Dataset for <fixed-case>R</fixed-case>ussian</title>
      <author><first>Anna</first><last>Aksenova</last></author>
      <author><first>Ekaterina</first><last>Gavrishina</last></author>
      <author><first>Elisei</first><last>Rykov</last></author>
      <author><first>Andrey</first><last>Kutuzov</last></author>
      <pages>77–88</pages>
      <abstract>We present RuDSI, a new benchmark for word sense induction (WSI) in Russian. The dataset was created using manual annotation and semi-automatic clustering of Word Usage Graphs (WUGs). RuDSI is completely data-driven (based on texts from Russian National Corpus), with no external word senses imposed on annotators. We present and analyze RuDSI, describe our annotation workflow, show how graph clustering parameters affect the dataset, report the performance that several baseline WSI methods obtain on RuDSI and discuss possibilities for improving these scores.</abstract>
      <url hash="68fdc30f">2022.textgraphs-1.9</url>
      <bibkey>aksenova-etal-2022-rudsi</bibkey>
      <pwccode url="https://github.com/kategavrishina/rudsi" additional="true">kategavrishina/rudsi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/russe">RUSSE</pwcdataset>
    </paper>
    <paper id="10">
      <title>Temporal Graph Analysis of Misinformation Spreaders in Social Media</title>
      <author><first>Joan</first><last>Plepi</last></author>
      <author><first>Flora</first><last>Sakketou</last></author>
      <author><first>Henri-Jacques</first><last>Geiss</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>89–104</pages>
      <abstract>Proactively identifying misinformation spreaders is an important step towards mitigating the impact of fake news on our society. Although the news domain is subject to rapid changes over time, the temporal dynamics of the spreaders’ language and network have not been explored yet. In this paper, we analyze the users’ time-evolving semantic similarities and social interactions and show that such patterns can, on their own, indicate misinformation spreading. Building on these observations, we propose a dynamic graph-based framework that leverages the dynamic nature of the users’ network for detecting fake news spreaders. We validate our design choice through qualitative analysis and demonstrate the contributions of our model’s components through a series of exploratory and ablative experiments on two datasets.</abstract>
      <url hash="3f27268d">2022.textgraphs-1.10</url>
      <bibkey>sakketou-etal-2022-temporal</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2022 Shared Task on Natural Language Premise Selection</title>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Deborah</first><last>Ferreira</last></author>
      <author><first>Mokanarangan</first><last>Thayaparan</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <author><first>Dmitry</first><last>Ustalov</last></author>
      <pages>105–113</pages>
      <abstract>The Shared Task on Natural Language Premise Selection (NLPS) asks participants to retrieve the set of premises that are most likely to be useful for proving a given mathematical statement from a supporting knowledge base. While previous editions of the TextGraphs shared tasks series targeted multi-hop inference for explanation regeneration in the context of science questions (Thayaparan et al., 2021; Jansen and Ustalov, 2020, 2019), NLPS aims to assess the ability of state-of-the-art approaches to operate on a mixture of natural and mathematical language and model complex multi-hop reasoning dependencies between statements. To this end, this edition of the shared task makes use of a large set of approximately 21k mathematical statements extracted from the PS-ProofWiki dataset (Ferreira and Freitas, 2020a). In this summary paper, we present the results of the 1st edition of the NLPS task, providing a description of the evaluation data, and the participating systems. Additionally, we perform a detailed analysis of the results, evaluating various aspects involved in mathematical language processing and multi-hop inference. The best-performing system achieved a MAP of 15.39, improving the performance of a TF-IDF baseline by approximately 3.0 MAP.</abstract>
      <url hash="648dac0f">2022.textgraphs-1.11</url>
      <bibkey>valentino-etal-2022-textgraphs</bibkey>
      <pwccode url="https://github.com/ai-systems/tg2022task_premise_retrieval" additional="false">ai-systems/tg2022task_premise_retrieval</pwccode>
    </paper>
    <paper id="12">
      <title><fixed-case>IJS</fixed-case> at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs-16 Natural Language Premise Selection Task: Will Contextual Information Improve Natural Language Premise Selection?</title>
      <author><first>Thi Hong Hanh</first><last>Tran</last></author>
      <author><first>Matej</first><last>Martinc</last></author>
      <author><first>Antoine</first><last>Doucet</last></author>
      <author><first>Senja</first><last>Pollak</last></author>
      <pages>114–118</pages>
      <abstract>Natural Language Premise Selection (NLPS) is a mathematical Natural Language Processing (NLP) task that retrieves a set of applicable relevant premises to support the end-user finding the proof for a particular statement. In this research, we evaluate the impact of Transformer-based contextual information and different fundamental similarity scores toward NLPS. The results demonstrate that the contextual representation is better at capturing meaningful information despite not being pretrained in the mathematical background compared to the statistical approach (e.g., the TF-IDF) with a boost of around 3.00% MAP@500.</abstract>
      <url hash="5c806b9a">2022.textgraphs-1.12</url>
      <bibkey>tran-etal-2022-ijs</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>SNLP</fixed-case> at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2022 Shared Task: Unsupervised Natural Language Premise Selection in Mathematical Texts Using Sentence-<fixed-case>MPN</fixed-case>et</title>
      <author><first>Paul</first><last>Trust</last></author>
      <author><first>Provia</first><last>Kadusabe</last></author>
      <author><first>Haseeb</first><last>Younis</last></author>
      <author><first>Rosane</first><last>Minghim</last></author>
      <author><first>Evangelos</first><last>Milios</last></author>
      <author><first>Ahmed</first><last>Zahran</last></author>
      <pages>119–123</pages>
      <abstract>This paper describes our system for the submission to the TextGraphs 2022 shared task at COLING 2022: Natural Language Premise Selection (NLPS) from mathematical texts. The task of NLPS is about selecting mathematical statements called premises in a knowledge base written in natural language and mathematical formulae that are most likely to be used to prove a particular mathematical proof. We formulated this task as an unsupervised semantic similarity task by first obtaining contextualized embeddings of both the premises and mathematical proofs using sentence transformers. We then obtained the cosine similarity between the embeddings of premises and proofs and then selected premises with the highest cosine scores as the most probable. Our system improves over the baseline system that uses bag of words models based on term frequency inverse document frequency in terms of mean average precision (MAP) by about 23.5% (0.1516 versus 0.1228).</abstract>
      <url hash="c3dd81d1">2022.textgraphs-1.13</url>
      <bibkey>trust-etal-2022-snlp</bibkey>
    </paper>
    <paper id="14">
      <title>Keyword-based Natural Language Premise Selection for an Automatic Mathematical Statement Proving</title>
      <author><first>Doratossadat</first><last>Dastgheib</last></author>
      <author><first>Ehsaneddin</first><last>Asgari</last></author>
      <pages>124–126</pages>
      <abstract>Extraction of supportive premises for a mathematical problem can contribute to profound success in improving automatic reasoning systems. One bottleneck in automated theorem proving is the lack of a proper semantic information retrieval system for mathematical texts. In this paper, we show the effect of keyword extraction in the natural language premise selection (NLPS) shared task proposed in TextGraph-16 that seeks to select the most relevant sentences supporting a given mathematical statement.</abstract>
      <url hash="010ef808">2022.textgraphs-1.14</url>
      <bibkey>dastgheib-asgari-2022-keyword</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs-16 Natural Language Premise Selection Task: Zero-Shot Premise Selection with Prompting Generative Language Models</title>
      <author><first>Liubov</first><last>Kovriguina</last></author>
      <author><first>Roman</first><last>Teucher</last></author>
      <author><first>Robert</first><last>Wardenga</last></author>
      <pages>127–132</pages>
      <abstract>Automated theorem proving can benefit a lot from methods employed in natural language processing, knowledge graphs and information retrieval: this non-trivial task combines formal languages understanding, reasoning, similarity search. We tackle this task by enhancing semantic similarity ranking with prompt engineering, which has become a new paradigm in natural language understanding. None of our approaches requires additional training. Despite encouraging results reported by prompt engineering approaches for a range of NLP tasks, for the premise selection task vanilla re-ranking by prompting GPT-3 doesn’t outperform semantic similarity ranking with SBERT, but merging of the both rankings shows better results.</abstract>
      <url hash="a73eaae0">2022.textgraphs-1.15</url>
      <bibkey>kovriguina-etal-2022-textgraphs</bibkey>
    </paper>
  </volume>
</collection>
