<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.iwsds">
  <volume id="1" ingest-date="2025-05-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 15th International Workshop on Spoken Dialogue Systems Technology</booktitle>
      <editor><first>Maria Ines</first><last>Torres</last></editor>
      <editor><first>Yuki</first><last>Matsuda</last></editor>
      <editor><first>Zoraida</first><last>Callejas</last></editor>
      <editor><first>Arantza</first><last>del Pozo</last></editor>
      <editor><first>Luis Fernando</first><last>D'Haro</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bilbao, Spain</address>
      <month>May</month>
      <year>2025</year>
      <url hash="736b83cd">2025.iwsds-1</url>
      <venue>iwsds</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="bee70a0e">2025.iwsds-1.0</url>
      <bibkey>iwsds-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Automatic Generation of Structured Domain Knowledge for Dialogue-based <fixed-case>XAI</fixed-case> Systems</title>
      <author><first>Carolin</first><last>Schindler</last></author>
      <author><first>Isabel</first><last>Feustel</last></author>
      <author><first>Niklas</first><last>Rach</last></author>
      <author><first>Wolfgang</first><last>Minker</last></author>
      <pages>1–11</pages>
      <abstract>Explanatory dialogue systems serve as intuitive interface between non-expert users and explainable AI (XAI) systems. The interaction with these kind of systems benefits especially from the integration of structured domain knowledge, e.g., by means of bipolar argumentation trees. So far, these domain-specific structures need to be created manually, therewith impairing the flexibility of the system with respect to the domain. We address this limitation by adapting an existing pipeline for topic-independent acquisition of argumentation trees in the field of persuasive, argumentative dialogue to the area of explanatory dialogue. This shift is achieved by a) introducing and investigating different formulations of auxiliary claims per feature of the explanation of the AI model, b) exploring the influence of pre-grouping of the arguments with respect to the feature they address, c) suggesting adaptions to the existing algorithm of the pipeline for obtaining a tree structure, and d) utilizing a new approach for determining the type of the relationship between the arguments. Through a step-wise expert evaluation for the domain titanic survival, we identify the best performing variant of our pipeline. With this variant we conduct a user study comparing the automatically generated argumentation trees against their manually created counterpart in the domains titanic survival and credit acquisition. This assessment of the suitability of the generated argumentation trees for a later integration into dialogue-based XAI systems as domain knowledge yields promising results.</abstract>
      <url hash="6958c309">2025.iwsds-1.1</url>
      <bibkey>schindler-etal-2025-automatic</bibkey>
    </paper>
    <paper id="2">
      <title>Exploring the Impact of Modalities on Building Common Ground Using the Collaborative Scene Reconstruction Task</title>
      <author><first>Yosuke</first><last>Ujigawa</last></author>
      <author><first>Asuka</first><last>Shiotani</last></author>
      <author><first>Masato</first><last>Takizawa</last></author>
      <author><first>Eisuke</first><last>Midorikawa</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Kazunori</first><last>Takashio</last></author>
      <pages>12–19</pages>
      <abstract>To deepen our understanding of verbal and non-verbal modalities in establishing common ground, this study introduces a novel “collaborative scene reconstruction task.” In this task, pairs of participants, each provided with distinct image sets derived from the same video, work together to reconstruct the sequence of the original video. The level of agreement between the participants on the image order—quantified using Kendall’s rank correlation coefficient—serves as a measure of common ground construction. This approach enables the analysis of how various modalities contribute to the constraction of common ground. A corpus comprising 40 dialogues from 20 participants was collected and analyzed. The findings suggest that specific gestures play a significant role in fostering common ground, offering valuable insights for the development of dialogue systems that leverage multimodal information to enhance user the counstraction of common ground.</abstract>
      <url hash="b77f0a54">2025.iwsds-1.2</url>
      <bibkey>ujigawa-etal-2025-exploring</bibkey>
    </paper>
    <paper id="3">
      <title>Design, Generation and Evaluation of a Synthetic Dialogue Dataset for Contextually Aware Chatbots in Art Museums</title>
      <author><first>Inass</first><last>Rachidi</last></author>
      <author><first>Anas</first><last>Ezzakri</last></author>
      <author><first>Jaime</first><last>Bellver-Soler</last></author>
      <author><first>Luis Fernando</first><last>D’Haro</last></author>
      <pages>20–28</pages>
      <abstract>This paper presents the design, synthetic generation, and automated evaluation of ArtGenEval-GPT++, an advanced dataset for training and fine-tuning conversational agents with artificial awareness capabilities targeting to the art domain. Building on the foundation of a previously released dataset (ArtGenEval-GPT), the new version introduces enhancements for greater personalization (e.g., gender, ethnicity, age, and knowledge) while addressing prior limitations, including low-quality dialogues and hallucinations. The dataset comprises approximately 12,500 dyadic, multi-turn dialogues generated using state-of-the-art large language models (LLMs). These dialogues span diverse museum scenarios, incorporating varied visitor profiles, emotional states, interruptions, and chatbot behaviors. Objective evaluations confirm the dataset’s quality and contextual coherence. Ethical considerations, including biases and hallucinations, are analyzed, with proposed directions for improving the dataset utility. This work contributes to the development of personalized, context-aware conversational agents capable of navigating complex, real-world environments, such as museums, to enhance visitor engagement and satisfaction.</abstract>
      <url hash="23300049">2025.iwsds-1.3</url>
      <bibkey>rachidi-etal-2025-design</bibkey>
    </paper>
    <paper id="4">
      <title>A Voice-Controlled Dialogue System for <fixed-case>NPC</fixed-case> Interaction using Large Language Models</title>
      <author><first>Milan</first><last>Wevelsiep</last></author>
      <author><first>Nicholas Thomas</first><last>Walker</last></author>
      <author><first>Nicolas</first><last>Wagner</last></author>
      <author><first>Stefan</first><last>Ultes</last></author>
      <pages>29–38</pages>
      <abstract>This paper explores the integration of voice-controlled dialogue systems in narrative-driven video games, addressing the limitations of existing approaches. We propose a hybrid interface that allows players to freely paraphrase predefined dialogue options, combining player expressiveness with narrative cohesion. The prototype was developed in Unity, and a large language model was used to map the transcribed voice input to existing dialogue options. The approach was evaluated in a user study (n=14) that compared the hybrid interface to traditional point-and-click methods. Results indicate the proposed interface enhances player’s degree of joy and perceived freedom while maintaining narrative consistency. The findings provide insights into the design of scalable and engaging voice-controlled systems for interactive storytelling. Future research should focus on reducing latency and refining language model accuracy to further improve user experience and immersion.</abstract>
      <url hash="fa78f8af">2025.iwsds-1.4</url>
      <bibkey>wevelsiep-etal-2025-voice</bibkey>
    </paper>
    <paper id="5">
      <title>A Dialogue System for Semi-Structured Interviews by <fixed-case>LLM</fixed-case>s and its Evaluation on Persona Information Collection</title>
      <author><first>Ryo</first><last>Hasegawa</last></author>
      <author><first>Yijie</first><last>Hua</last></author>
      <author><first>Takehito</first><last>Utsuro</last></author>
      <author><first>Ekai</first><last>Hashimoto</last></author>
      <author><first>Mikio</first><last>Nakano</last></author>
      <author><first>Shun</first><last>Shiramatsu</last></author>
      <pages>39–59</pages>
      <abstract>In this paper, we propose a dialogue control management framework using large language models for semi-structured interviews. Specifically, large language models are used to generate the interviewer’s utterances and to make conditional branching decisions based on the understanding of the interviewee’s responses. The framework enables flexible dialogue control in interview conversations by generating and updating slots and values according to interviewee answers. More importantly, we invented through LLMs’ prompt tuning the framework of accumulating the list of slots generated along the course of incrementing the number of interviewees through the semi-structured interviews. Evaluation results showed that the proposed approach of accumulating the list of generated slots throughout the semi-structured interviews outperform the baseline without accumulating generated slots in terms of the number of persona attributes and values collected through the semi-structured interview.</abstract>
      <url hash="a97ae3d5">2025.iwsds-1.5</url>
      <bibkey>hasegawa-etal-2025-dialogue</bibkey>
    </paper>
    <paper id="6">
      <title>Exploring Personality-Aware Interactions in Salesperson Dialogue Agents</title>
      <author><first>Sijia</first><last>Cheng</last></author>
      <author><first>Wen Yu</first><last>Chang</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>60–71</pages>
      <abstract>The integration of dialogue agents into the sales domain requires a deep understanding of how these systems interact with users possessing diverse personas. This study explores the influence of user personas, defined using the Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance of sales-oriented dialogue agents. Through large-scale testing and analysis, we assess the pre-trained agent’s effectiveness, adaptability, and personalization capabilities across a wide range of MBTI-defined user types. Our findings reveal significant patterns in interaction dynamics, task completion rates, and dialogue naturalness, underscoring the future potential for dialogue agents to refine their strategies to better align with varying personality traits. This work not only provides actionable insights for building more adaptive and user-centric conversational systems in the sales domain but also contributes broadly to the field by releasing persona-defined user simulators. These simulators, unconstrained by domain, offer valuable tools for future research and demonstrate the potential for scaling personalized dialogue systems across diverse applications.</abstract>
      <url hash="a3bac12d">2025.iwsds-1.6</url>
      <bibkey>cheng-etal-2025-exploring</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>R</fixed-case>e<fixed-case>S</fixed-case>p<fixed-case>A</fixed-case>ct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational <fixed-case>AI</fixed-case> Agents</title>
      <author><first>Vardhan</first><last>Dongre</last></author>
      <author><first>Xiaocheng</first><last>Yang</last></author>
      <author><first>Emre Can</first><last>Acikgoz</last></author>
      <author><first>Suvodip</first><last>Dey</last></author>
      <author><first>Gokhan</first><last>Tur</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>72–102</pages>
      <abstract>Large language model (LLM)-based agents have been increasingly used to interact with external environments (e.g., games, APIs, etc.) and solve tasks. However, current frameworks do not enable these agents to work with users and interact with them to align on the details of their tasks and reach user-defined goals; instead, in ambiguous situations, these agents may make decisions based on assumptions. This work introduces ReSpAct (Reason, Speak, and Act), a novel framework that synergistically combines the essential skills for building task-oriented “conversational” agents. ReSpAct addresses this need for agents, expanding on the ReAct approach. ReSpAct framework enables agents to interpret user instructions, reason about complex tasks, execute appropriate actions and engage in dynamic dialogue to seek guidance, clarify ambiguities, understand user preferences, resolve problems, and use the intermediate feedback and responses of users to update their plans. We evaluated ReSpAct with GPT-4 in environments supporting user interaction, such as task-oriented dialogue (MultiWOZ) and interactive decision-making (Alfworld, WebShop), ReSpAct is flexible enough to incorporate dynamic user feedback and addresses prevalent issues like error propagation and agents getting stuck in reasoning loops. This results in more interpretable, human-like task-solving trajectories than baselines relying solely on reasoning traces. In two interactive decision-making benchmarks, AlfWorld and WebShop, ReSpAct outperforms strong reasoning-only method ReAct by an absolute success rate of 6% and 4%, respectively. In the task-oriented dialogue benchmark MultiWOZ, ReSpAct improved Inform and Success scores by 5.5% and 3%, respectively.</abstract>
      <url hash="f7184b61">2025.iwsds-1.7</url>
      <bibkey>dongre-etal-2025-respact</bibkey>
    </paper>
    <paper id="8">
      <title>Examining Older Adults’ Motivation for Interacting with Health-Monitoring Conversational Systems Through Field Trials</title>
      <author><first>Mariko</first><last>Yoshida</last></author>
      <author><first>Ryo</first><last>Hori</last></author>
      <author><first>Yuki</first><last>Zenimoto</last></author>
      <author><first>Mayu</first><last>Urata</last></author>
      <author><first>Mamoru</first><last>Endo</last></author>
      <author><first>Takami</first><last>Yasuda</last></author>
      <author><first>Aiko</first><last>Inoue</last></author>
      <author><first>Takahiro</first><last>Hayashi</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>103–114</pages>
      <abstract>When assessing the health of older adults, oral interviews and written questionnaires are commonly used. However, these methods are time-consuming in terms of both execution and data aggregation. To address this issue, systems utilizing generative AI for health information collection through conversation have been developed and implemented. Despite these advancements, the motivation of older adults to consistently engage with such systems in their daily lives has not been thoroughly explored. In this study, we developed a smart-speaker extension that uses generative AI to monitor health status through casual conversations with older adult users. The system was tested in a two-week home trial with older adult participants. We conducted post-trial questionnaires and interviews, and we analyzed conversation log data. The results revealed that older adult users enjoy interacting with such systems and can integrate their use into their daily routines. Customized notifications through text messages encouraged system use, and the system’s ability to refer to previous conversations and address users by name was identified as a key factor motivating continued use.</abstract>
      <url hash="6964cb90">2025.iwsds-1.8</url>
      <bibkey>yoshida-etal-2025-examining</bibkey>
    </paper>
    <paper id="9">
      <title>Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems</title>
      <author><first>Shang-Chi</first><last>Tsai</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>115–128</pages>
      <abstract>With the advancement of large language models, many dialogue systems are now capable of providing reasonable and informative responses to patients’ medical conditions. However, when patients consult their doctor, they may experience negative emotions due to the severity and urgency of their situation. If the model can provide appropriate comfort and empathy based on the patient’s negative emotions while answering medical questions, it will likely offer a more reassuring experience during the medical consultation process. To address this issue, our paper explores the balance between knowledge sharing and emotional support in the healthcare dialogue process. We utilize a large language model to rewrite a real-world interactive medical dialogue dataset, generating patient queries with negative emotions and corresponding medical responses aimed at soothing the patient’s emotions while addressing their concerns. The modified data serves to refine the latest large language models with various fine-tuning methods, enabling them to accurately provide sentences with both emotional reassurance and constructive suggestions in response to patients’ questions. Compared to the original LLM model, our experimental results demonstrate that our methodology significantly enhances the model’s ability to generate emotional responses while maintaining its original capability to provide accurate knowledge-based answers.</abstract>
      <url hash="87429493">2025.iwsds-1.9</url>
      <bibkey>tsai-chen-2025-balancing</bibkey>
    </paper>
    <paper id="10">
      <title>Context or Retrieval? Evaluating <fixed-case>RAG</fixed-case> Methods for Art and Museum <fixed-case>QA</fixed-case> System</title>
      <author><first>Samuel</first><last>Ramos-Varela</last></author>
      <author><first>Jaime</first><last>Bellver-Soler</last></author>
      <author><first>Marcos</first><last>Estecha-Garitagoitia</last></author>
      <author><first>Luis Fernando</first><last>D’Haro</last></author>
      <pages>129–136</pages>
      <abstract>Recent studies suggest that increasing the context window of language models could outperform retrieval-augmented generation (RAG) methods in certain tasks. However, in domains such as art and museums, where information is inherently multimodal, combining images and detailed textual descriptions, this assumption needs closer examination. To explore this, we compare RAG techniques with direct large-context input approaches for answering questions about artworks. Using a dataset of painting images paired with textual information, we develop a synthetic database of question-answer (QA) pairs for evaluating these methods. The focus is on assessing the efficiency and accuracy of RAG in retrieving and using relevant information compared to passing the entire textual context to a language model. Additionally, we experiment with various strategies for segmenting and retrieving text to optimise the RAG pipeline. The results aim to clarify the trade-offs between these approaches and provide valuable insights for interactive systems designed for art and museum contexts.</abstract>
      <url hash="0c135120">2025.iwsds-1.10</url>
      <bibkey>ramos-varela-etal-2025-context</bibkey>
    </paper>
    <paper id="11">
      <title>Paralinguistic Attitude Recognition for Spoken Dialogue Systems</title>
      <author><first>Kouki</first><last>Miyazawa</last></author>
      <author><first>Zhi</first><last>Zhu</last></author>
      <author><first>Yoshinao</first><last>Sato</last></author>
      <pages>137–142</pages>
      <abstract>Although paralinguistic information is critical for human communication, most spoken dialogue systems ignore such information, hindering natural communication between humans and machines. This study addresses the recognition of paralinguistic attitudes in user speech. Specifically, we focus on four essential attitudes for generating an appropriate system response, namely agreement, disagreement, questions, and stalling. The proposed model can help a dialogue system better understand what the user is trying to convey. In our experiments, we trained and evaluated a model that classified paralinguistic attitudes on a reading-speech dataset without using linguistic information. The proposed model outperformed human perception. Furthermore, experimental results indicate that speech enhancement alleviates the degradation of model performance caused by background noise, whereas reverberation remains a challenge.</abstract>
      <url hash="c20008a1">2025.iwsds-1.11</url>
      <bibkey>miyazawa-etal-2025-paralinguistic</bibkey>
    </paper>
    <paper id="12">
      <title>Exploring <fixed-case>R</fixed-case>e<fixed-case>A</fixed-case>ct Prompting for Task-Oriented Dialogue: Insights and Shortcomings</title>
      <author><first>Michelle</first><last>Elizabeth</last></author>
      <author><first>Morgan</first><last>Veyret</last></author>
      <author><first>Miguel</first><last>Couceiro</last></author>
      <author><first>Ondrej</first><last>Dusek</last></author>
      <author><first>Lina M.</first><last>Rojas Barahona</last></author>
      <pages>143–153</pages>
      <abstract>Large language models (LLMs) gained immense popularity due to their impressive capabilities in unstructured conversations. Empowering LLMs with advanced prompting strategies such as reasoning and acting (ReAct) (Yao et al., 2022) has shown promise in solving complex tasks traditionally requiring reinforcement learning. In this work, we apply the ReAct strategy to guide LLMs performing task-oriented dialogue (TOD). We evaluate ReAct-based LLMs (ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely underperform state-of-the-art approaches on success rate in simulation, this difference becomes less pronounced in human evaluation. Moreover, compared to the baseline, humans report higher subjective satisfaction with ReAct-LLM despite its lower success rate, most likely thanks to its natural and confidently phrased responses.</abstract>
      <url hash="1614015d">2025.iwsds-1.12</url>
      <bibkey>elizabeth-etal-2025-exploring</bibkey>
    </paper>
    <paper id="13">
      <title>Design of a conversational agent to support people on suicide risk</title>
      <author><first>Mario</first><last>Manso Vázquez</last></author>
      <author><first>José Manuel</first><last>Ramírez Sánchez</last></author>
      <author><first>Carmen</first><last>García-Mateo</last></author>
      <author><first>Laura</first><last>Docío-Fernández</last></author>
      <author><first>Manuel José</first><last>Fernández-Iglesias</last></author>
      <author><first>Beatriz</first><last>Gómez-Gómez</last></author>
      <author><first>Beatriz</first><last>Pinal</last></author>
      <author><first>Antia</first><last>Brañas</last></author>
      <author><first>Alejandro</first><last>García-Caballero</last></author>
      <pages>154–159</pages>
      <abstract>In this paper, we present a core component of the VisIA project: a conversational agent designed to detect suicide risk factors during real-time chat interactions. By adhering to clinical guidelines and the state-of-the-art theories of suicide, the agent aims to provide a scalable and effective approach to identifying individuals at risk. Preliminary results demonstrate the feasibility and potential of conversational agents in enhancing suicide risk detection.</abstract>
      <url hash="d1d564de">2025.iwsds-1.13</url>
      <bibkey>manso-vazquez-etal-2025-design</bibkey>
    </paper>
    <paper id="14">
      <title>Optimizing <fixed-case>RAG</fixed-case>: Classifying Queries for Dynamic Processing</title>
      <author><first>Kabir</first><last>Olawore</last></author>
      <author><first>Michael</first><last>McTear</last></author>
      <author><first>Yaxin</first><last>Bi</last></author>
      <author><first>David</first><last>Griol</last></author>
      <pages>160–164</pages>
      <abstract>In Retrieval-Augmented Generation (RAG) systems efficient information retrieval is crucial for enhancing user experience and satisfaction, as response times and computational demands significantly impact performance. RAG can be unnecessarily resource-intensive for frequently asked questions (FAQs) and simple questions. In this paper we introduce an approach in which we categorize user questions into simple queries that do not require RAG processing. Evaluation results show that our proposal reduces latency and improves response efficiency compared to systems relying solely on RAG.</abstract>
      <url hash="e12774fd">2025.iwsds-1.14</url>
      <bibkey>olawore-etal-2025-optimizing</bibkey>
    </paper>
    <paper id="15">
      <title>Enhancing Proactive Dialogue Systems Through Self-Learning of Reasoning and Action-Planning</title>
      <author><first>Ryosuke</first><last>Ito</last></author>
      <author><first>Tetsuya</first><last>Takiguchi</last></author>
      <author><first>Yasuo</first><last>Ariki</last></author>
      <pages>165–171</pages>
      <abstract>A proactive dialogue system refers to a conversational system designed to guide the direction of a conversation in order to achieve pre-defined targets or fulfill specific goals. Recent studies have shown that Proactive Chain-of-Thought, which guides the system to explicitly think through intermediate reasoning and action-planning steps toward a conversational goal before generating a response, can significantly enhance the performance of proactive dialogue systems. However, these improvements primarily focus on prompt-based control, while the potential of fine-tuning Proactive-CoT remains largely unexplored. Furthermore, fine-tuning Proactive-CoT requires manual annotation of reasoning processes and action plans, which incurs significant time and cost. In this study, we propose a novel approach for automatically annotating reasoning processes and action plans through self-learning. This method enables fully automated annotation, significantly reducing the time and cost associated with manual annotation. Experimental results show that models trained using our proposed method outperform those trained with other fine-tuning approaches. These findings highlight the potential of self-learning approaches to advance the development of more robust and efficient proactive dialogue systems.</abstract>
      <url hash="b0102cef">2025.iwsds-1.15</url>
      <bibkey>ito-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>T</fixed-case>rust<fixed-case>B</fixed-case>oost: Balancing flexibility and compliance in conversational <fixed-case>AI</fixed-case> systems</title>
      <author><first>David</first><last>Griol</last></author>
      <author><first>Zoraida</first><last>Callejas</last></author>
      <author><first>Manuel</first><last>Gil-Martín</last></author>
      <author><first>Ksenia</first><last>Kharitonova</last></author>
      <author><first>Juan Manuel</first><last>Montero-Martínez</last></author>
      <author><first>David</first><last>Pérez Fernández</last></author>
      <author><first>Fernando</first><last>Fernández-Martínez</last></author>
      <pages>172–175</pages>
      <abstract>Conversational AI (ConvAI) systems are gaining growing importance as an alternative for more natural interaction with digital services. In this context, Large Language Models (LLMs) have opened new possibilities for less restricted interaction and richer natural language understanding. However, despite their advanced capabilities, LLMs can pose accuracy and reliability problems, as they sometimes generate factually incorrect or contextually inappropriate content that does not fulfill the regulations or business rules of a specific application domain. In addition, they still do not possess the capability to adjust to users’ needs and preferences, showing emotional awareness, while concurrently adhering to the regulations and limitations of their designated domain. In this paper we present the TrustBoost project, which addresses the challenge of improving trustworthiness of ConvAI from two dimensions: cognition (adaptability, flexibility, compliance, and performance) and affectivity (familiarity, emotional dimension, and perception). The duration of the project is from September 2024 to December 2027.</abstract>
      <url hash="7bc6a9d5">2025.iwsds-1.16</url>
      <bibkey>griol-etal-2025-trustboost</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>S</fixed-case>cript<fixed-case>B</fixed-case>oard: Designing modern spoken dialogue systems through visual programming</title>
      <author><first>Divesh</first><last>Lala</last></author>
      <author><first>Mikey</first><last>Elmers</last></author>
      <author><first>Koji</first><last>Inoue</last></author>
      <author><first>Zi Haur</first><last>Pang</last></author>
      <author><first>Keiko</first><last>Ochi</last></author>
      <author><first>Tatsuya</first><last>Kawahara</last></author>
      <pages>176–182</pages>
      <abstract>Implementation of spoken dialogue systems can be time-consuming, in particular for people who are not familiar with managing dialogue states and turn-taking in real-time. A GUI-based system where the user can quickly understand the dialogue flow allows rapid prototyping of experimental and real-world systems. In this demonstration we present ScriptBoard, a tool for creating dialogue scenarios which is independent of any specific robot platform. ScriptBoard has been designed with multi-party scenarios in mind and makes use of large language models to both generate dialogue and make decisions about the dialogue flow. This program promotes both flexibility and reproducibility in spoken dialogue research and provides everyone the opportunity to design and test their own dialogue scenarios.</abstract>
      <url hash="3c7888ba">2025.iwsds-1.17</url>
      <bibkey>lala-etal-2025-scriptboard</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>D</fixed-case>4<fixed-case>AC</fixed-case>: A Tool for Developing Multimodal Dialogue Systems without Coding</title>
      <author><first>Mikio</first><last>Nakano</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>183–189</pages>
      <abstract>To enable the broader application of dialogue system technology across various fields, it is beneficial to empower individuals with limited programming experience to build dialogue systems. Domain experts, where dialogue system technology is highly relevant, may not necessarily possess expertise in information technology. This paper presents D4AC, which works as a client for text-based dialogue servers. By combining D4AC with a no-code tool for developing text-based dialogue servers, it is possible to build multimodal dialogue systems without coding. These systems can adapt to the user’s age, gender, emotions, and engagement levels obtained from their facial images. D4AC can be installed, launched, and configured without technical knowledge. D4AC was used in student projects at a university, which suggested the effectiveness of D4AC.</abstract>
      <url hash="011a2a64">2025.iwsds-1.18</url>
      <bibkey>nakano-higashinaka-2025-d4ac</bibkey>
    </paper>
    <paper id="19">
      <title>A Multilingual Speech-Based Driver Assistant for <fixed-case>B</fixed-case>asque and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Antonio</first><last>Aparicio Akcharov</last></author>
      <author><first>Asier</first><last>López Zorrilla</last></author>
      <author><first>Juan Camilo</first><last>Vásquez Correa</last></author>
      <author><first>Oscar</first><last>Montserrat</last></author>
      <author><first>José Maria</first><last>Echevarría</last></author>
      <author><first>Begoña</first><last>Arrate</last></author>
      <author><first>Joxean</first><last>Zapirain</last></author>
      <author><first>Mikel</first><last>deVelasco Vázquez</last></author>
      <author><first>Santiago Andrés</first><last>Moreno-Acevedo</last></author>
      <author><first>Ander</first><last>González-Docasal</last></author>
      <author><first>Maria Ines</first><last>Torres</last></author>
      <author><first>Aitor</first><last>Álvarez</last></author>
      <pages>190–195</pages>
      <abstract>This demo paper presents a prototype of a multilingual, speech-based driver assistant, designed to support both English and Basque languages. The inclusion of Basque—a low-resource language with limited domain-specific training data—marks a significant contribution, as publicly available AI models, including Large Language Models, often underperform for such languages compared to high-resource languages like English. Despite these challenges, our system demonstrates robust performance, successfully understanding user queries and delivering rapid responses in a demanding environment: a car simulator. Notably, the system achieves comparable performance in both English and Basque, showcasing its effectiveness in addressing linguistic disparities in AI-driven applications. A demo of our prototype will be available in the workshop.</abstract>
      <url hash="a645241d">2025.iwsds-1.19</url>
      <bibkey>aparicio-akcharov-etal-2025-multilingual</bibkey>
    </paper>
    <paper id="20">
      <title>Intimebot – A Dialogue Agent for Timekeeping Support</title>
      <author><first>Shoaib</first><last>Khan</last></author>
      <author><first>Alex</first><last>Samani</last></author>
      <author><first>Rafael</first><last>Banchs</last></author>
      <pages>196–199</pages>
      <abstract>This demo paper presents intimebot, an AI-powered timekeeping solution designed to assist with timekeeping. Timekeeping is a fundamental but also overwhelming and complex task in many professional services practices. Our intimebot demo demonstrates how Artificial Intelligence can be utilized to implement a more efficient timekeeping process within a firm. Based on brief work descriptions provided by the timekeeper, intimebot is able to (1) predict the relevant combination of client, matter, and phase, (2) estimate the work effort hours, and (3) rewrite and normalize the provided work description into a compliant narrative. This can save a significant amount of time for busy professionals while ensuring terms of business compliance and best practices.</abstract>
      <url hash="2d1b23af">2025.iwsds-1.20</url>
      <bibkey>khan-etal-2025-intimebot</bibkey>
    </paper>
    <paper id="21">
      <title>A Chatbot for Providing Suicide Prevention Information in <fixed-case>S</fixed-case>panish</title>
      <author><first>Pablo</first><last>Ascorbe</last></author>
      <author><first>María S.</first><last>Campos</last></author>
      <author><first>César</first><last>Domínguez</last></author>
      <author><first>Jónathan</first><last>Heras</last></author>
      <author><first>Magdalena</first><last>Pérez</last></author>
      <author><first>Ana Rosa</first><last>Terroba-Reinares</last></author>
      <pages>200–204</pages>
      <abstract>Suicide has been identified by the World Health Organization as one of the most serious health problems that can affect people. Among the interventions that have been proposed to help people suffering from this problem and their relatives, the dissemination of accurate information is crucial. To achieve this goal, we have developed PrevenIA, a chatbot that provides reliable information on suicide prevention. The chatbot consists of a Retrieval Augmented Module for answering users’ queries based on a curated list of documents. In addition, it includes several models to avoid undesirable behaviours. The system has been validated by specialists and is currently being evaluated by different populations. Thanks to this project, reliable information on suicide will be disseminated in an easy and understandable form.</abstract>
      <url hash="0ba4c16b">2025.iwsds-1.21</url>
      <bibkey>ascorbe-etal-2025-chatbot</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>LAMIA</fixed-case>: An <fixed-case>LLM</fixed-case> Approach for Task-Oriented Dialogue Systems in Industry 5.0</title>
      <author><first>Cristina</first><last>Fernandez</last></author>
      <author><first>Izaskun</first><last>Fernandez</last></author>
      <author><first>Cristina</first><last>Aceta</last></author>
      <pages>205–214</pages>
      <abstract>Human-Machine Interaction (HMI) plays an important role in Industry 5.0, improving worker well-being by automating repetitive tasks and enhancing seamless collaboration between humans and intelligent systems. In this context, Task-Oriented Dialogue (TOD) systems are a commonly used approach to enable natural communication in these settings, traditionally developed using rule-based approaches. However, the revolution of Large Language Models (LLMs) is changing how dialogue systems are being developed without the necessity of relying on tedious and rigid handcrafted rules. Despite their popularity, their application in industrial contexts remains underexplored, necessitating a solution to challenges such as hallucinations, lack of domain-specific data, high training costs, and limited adaptability. In order to explore the contribution of LLMs in the industry field, this work presents LAMIA, a task-oriented dialogue system for industrial scenarios that leverages LLMs through prompt tuning. This system has been adapted and evaluated for a bin-picking use case, using GPT-3.5 Turbo, showing to be an intuitive method for new use cases in Industry 5.0.</abstract>
      <url hash="84335d3e">2025.iwsds-1.22</url>
      <bibkey>fernandez-etal-2025-lamia</bibkey>
    </paper>
    <paper id="23">
      <title>Conversational Tutoring in <fixed-case>VR</fixed-case> Training: The Role of Game Context and State Variables</title>
      <author><first>Maia</first><last>Aguirre</last></author>
      <author><first>Ariane</first><last>Méndez</last></author>
      <author><first>Aitor</first><last>García-Pablos</last></author>
      <author><first>Montse</first><last>Cuadros</last></author>
      <author><first>Arantza</first><last>del Pozo</last></author>
      <author><first>Oier</first><last>Lopez de Lacalle</last></author>
      <author><first>Ander</first><last>Salaberria</last></author>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <author><first>Pablo</first><last>Martínez</last></author>
      <author><first>Muhammad Zeshan</first><last>Afzal</last></author>
      <pages>215–224</pages>
      <abstract>Virtual Reality (VR) training provides safe, cost-effective engagement with lifelike scenarios but lacks intuitive communication between users and the virtual environment. This study investigates the use of Large Language Models (LLMs) as conversational tutors in VR health and safety training, examining the impact of game context and state variables on LLM-generated answers in zero- and few-shot settings. Results demonstrate that incorporating both game context and state information significantly improves answer accuracy, with human evaluations showing gains of up to 0.26 points in zero-shot and 0.18 points in few-shot settings on a 0-1 scale.</abstract>
      <url hash="e06b5670">2025.iwsds-1.23</url>
      <bibkey>aguirre-etal-2025-conversational</bibkey>
    </paper>
    <paper id="24">
      <title>A Methodology for Identifying Evaluation Items for Practical Dialogue Systems Based on Business-Dialogue System Alignment Models</title>
      <author><first>Mikio</first><last>Nakano</last></author>
      <author><first>Hironori</first><last>Takeuchi</last></author>
      <author><first>Kazunori</first><last>Komatani</last></author>
      <pages>225–237</pages>
      <abstract>This paper proposes a methodology for identifying evaluation items for practical dialogue systems. Traditionally, user satisfaction and user experiences have been the primary metrics for evaluating dialogue systems. However, there are various other evaluation items to consider when developing and operating practical dialogue systems, and such evaluation items are expected to lead to new research topics. So far, there has been no methodology for identifying these evaluation items. We propose identifying evaluation items based on business-dialogue system alignment models, which are applications of business-IT alignment models used in the development and operation of practical IT systems. We also present a generic model that facilitates the construction of a business-dialogue system alignment model for each dialogue system.</abstract>
      <url hash="f2cf94dc">2025.iwsds-1.24</url>
      <bibkey>nakano-etal-2025-methodology</bibkey>
    </paper>
    <paper id="25">
      <title>Speech-Controlled Smart Speaker for Accurate, Real-Time Health and Care Record Management</title>
      <author><first>Jonathan E.</first><last>Carrick</last></author>
      <author><first>Nina</first><last>Dethlefs</last></author>
      <author><first>Lisa</first><last>Greaves</last></author>
      <author><first>Venkata M. V.</first><last>Gunturi</last></author>
      <author><first>Rameez Raja</first><last>Kureshi</last></author>
      <author><first>Yongqiang</first><last>Cheng</last></author>
      <pages>238–244</pages>
      <abstract>To help alleviate the pressures felt by care workers, we have begun new research into improving the efficiency of care plan management by advancing recent developments in automatic speech recognition. Our novel approach adapts off-the-shelf tools in a purpose-built application for the speech domain, addressing challenges of accent adaption, real-time processing and speech hallucinations. We augment the speech-recognition scope of Open AI’s Whisper model through fine-tuning, reducing word error rates (WERs) from 16.8 to 1.0 on a range of British dialects. Addressing the speech-hallucination side effect of adapting to real-time recognition by enforcing a signal-to-noise ratio threshold and audio stream checks, we achieve a WER of 5.1, compared to 14.9 with Whisper’s original model. These ongoing research efforts tackle challenges that are necessary to build the speech-control basis for a custom smart speaker system that is both accurate and timely.</abstract>
      <url hash="0a197e4c">2025.iwsds-1.25</url>
      <bibkey>carrick-etal-2025-speech</bibkey>
    </paper>
    <paper id="26">
      <title>Analysis of Voice Activity Detection Errors in <fixed-case>API</fixed-case>-based Streaming <fixed-case>ASR</fixed-case> for Human-Robot Dialogue</title>
      <author><first>Kenta</first><last>Yamamoto</last></author>
      <author><first>Ryu</first><last>Takeda</last></author>
      <author><first>Kazunori</first><last>Komatani</last></author>
      <pages>245–253</pages>
      <abstract>In human-robot dialogue systems, streaming automatic speech recognition (ASR) services (e.g., Google ASR) are often utilized, with the microphone positioned close to the robot’s loudspeaker. Under these conditions, both the robot’s and the user’s utterances are captured, resulting in frequent failures to detect user speech. This study analyzes voice activity detection (VAD) errors by comparing results from such streaming ASR to those from standalone VAD models. Experiments conducted on three distinct dialogue datasets showed that streaming ASR tends to ignore user utterances immediately following system utterances. We discuss the underlying causes of these VAD errors and provide recommendations for improving VAD performance in human-robot dialogue.</abstract>
      <url hash="cf99a92f">2025.iwsds-1.26</url>
      <bibkey>yamamoto-etal-2025-analysis</bibkey>
    </paper>
    <paper id="27">
      <title>A Survey of Recent Advances on Turn-taking Modeling in Spoken Dialogue Systems</title>
      <author><first>Galo</first><last>Castillo-López</last></author>
      <author><first>Gael</first><last>de Chalendar</last></author>
      <author><first>Nasredine</first><last>Semmar</last></author>
      <pages>254–271</pages>
      <abstract>The rapid growth of dialogue systems adoption to serve humans in daily tasks has increased the realism expected from these systems. One trait of realism is the way speaking agents take their turns. We provide here a review of recent methods on turn-taking modeling and thoroughly describe the corpora used in these studies. We observe that 72% of the reviewed works in this survey do not compare their methods with previous efforts. We argue that one of the challenges in the field is the lack of well-established benchmarks to monitor progress. This work aims to provide the community with a better understanding of the current state of research around turn-taking modeling and future directions to build more realistic spoken conversational agents.</abstract>
      <url hash="aded0270">2025.iwsds-1.27</url>
      <bibkey>castillo-lopez-etal-2025-survey</bibkey>
    </paper>
    <paper id="28">
      <title>Integrating Respiration into Voice Activity Projection for Enhancing Turn-taking Performance</title>
      <author><first>Takao</first><last>Obi</last></author>
      <author><first>Kotaro</first><last>Funakoshi</last></author>
      <pages>272–276</pages>
      <abstract>Voice Activity Projection (VAP) models predict upcoming voice activities on a continuous timescale, enabling more nuanced turn-taking behaviors in spoken dialogue systems. Although previous studies have shown robust performance with audio-based VAP, the potential of incorporating additional physiological information, such as respiration, remains relatively unexplored. In this paper, we investigate whether respiratory information can enhance VAP performance in turn-taking. To this end, we collected Japanese dialogue data with synchronized audio and respiratory waveforms, and then we integrated the respiratory information into the VAP model. Our results showed that the VAP model combining audio and respiratory information had better performance than the audio-only model. This finding underscores the potential for improving the turn-taking performance of VAP by incorporating respiration.</abstract>
      <url hash="74d7bc62">2025.iwsds-1.28</url>
      <bibkey>obi-funakoshi-2025-integrating</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>DSLCMM</fixed-case>: A Multimodal Human-Machine Dialogue Corpus Built through Competitions</title>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Tetsuro</first><last>Takahashi</last></author>
      <author><first>Shinya</first><last>Iizuka</last></author>
      <author><first>Sota</first><last>Horiuchi</last></author>
      <author><first>Michimasa</first><last>Inaba</last></author>
      <author><first>Zhiyang</first><last>Qi</last></author>
      <author><first>Yuta</first><last>Sasaki</last></author>
      <author><first>Kotaro</first><last>Funakoshi</last></author>
      <author><first>Shoji</first><last>Moriya</last></author>
      <author><first>Shiki</first><last>Sato</last></author>
      <author><first>Takashi</first><last>Minato</last></author>
      <author><first>Kurima</first><last>Sakai</last></author>
      <author><first>Tomo</first><last>Funayama</last></author>
      <author><first>Masato</first><last>Komuro</last></author>
      <author><first>Hiroyuki</first><last>Nishikawa</last></author>
      <author><first>Ryosaku</first><last>Makino</last></author>
      <author><first>Hirofumi</first><last>Kikuchi</last></author>
      <author><first>Mayumi</first><last>Usami</last></author>
      <pages>277–283</pages>
      <abstract>A corpus of dialogues between multimodal systems and humans is indispensable for the development and improvement of such systems. However, there is a shortage of human-machine multimodal dialogue datasets, which hinders the widespread deployment of these systems in society. To address this issue, we construct a Japanese multimodal human-machine dialogue corpus, DSLCMM, by collecting and organizing data from the Dialogue System Live Competitions (DSLCs). This paper details the procedure for constructing the corpus and presents our analysis of the relationship between various dialogue features and evaluation scores provided by users.</abstract>
      <url hash="979e4a62">2025.iwsds-1.29</url>
      <bibkey>higashinaka-etal-2025-dslcmm</bibkey>
    </paper>
    <paper id="30">
      <title>Cutting Through Overload: Efficient Token Dropping for Speech Emotion Recognition in Multimodal Large Language Models</title>
      <author><first>Jaime</first><last>Bellver-Soler</last></author>
      <author><first>Mario</first><last>Rodriguez-Cantelar</last></author>
      <author><first>Ricardo</first><last>Córdoba</last></author>
      <author><first>Luis Fernando</first><last>D’Haro</last></author>
      <pages>284–289</pages>
      <abstract>Recent developments in Multimodal Large Language Models (MLLMs) have provided novel insights into Speech Emotion Recognition (SER). However, combining high-dimensional speech signals with textual tokens can lead to a rapid growth in input tokens, increasing computational costs and inference times. This “token overload” also risks shadowing essential textual cues, affecting the reasoning capabilities of the language model and diluting emotional information crucial to accurate SER. In this paper, we explore different token drop methods that mitigate excessive token counts while preserving both emotional nuances and the core linguistic capabilities of the model. Specifically, we compare various pooling approaches to produce a compact representation. Our preliminary findings suggest that these techniques can reduce computational costs without decreasing SER accuracy.</abstract>
      <url hash="67d503fe">2025.iwsds-1.30</url>
      <bibkey>bellver-soler-etal-2025-cutting</bibkey>
    </paper>
    <paper id="31">
      <title>Integrating Conversational Entities and Dialogue Histories with Knowledge Graphs and Generative <fixed-case>AI</fixed-case></title>
      <author><first>Graham</first><last>Wilcock</last></author>
      <author><first>Kristiina</first><last>Jokinen</last></author>
      <pages>290–298</pages>
      <abstract>Existing methods for storing dialogue history and for tracking mentioned entities in spoken dialogues usually handle these tasks separately. Recent advances in knowledge graphs and generative AI make it possible to integrate them in a framework with a uniform representation for dialogue management. This may help to build more natural and grounded dialogue models that can reduce misunderstanding and lead to more reliable dialogue-based interactions with AI agents. The paper describes ongoing work on this approach.</abstract>
      <url hash="ece16ab2">2025.iwsds-1.31</url>
      <bibkey>wilcock-jokinen-2025-integrating</bibkey>
    </paper>
    <paper id="32">
      <title>Enabling Trait-based Personality Simulation in Conversational <fixed-case>LLM</fixed-case> Agents: Case Study of Customer Assistance in <fixed-case>F</fixed-case>rench</title>
      <author><first>Ahmed</first><last>Njifenjou</last></author>
      <author><first>Virgile</first><last>Sucal</last></author>
      <author><first>Bassam</first><last>Jabaian</last></author>
      <author><first>Fabrice</first><last>Lefèvre</last></author>
      <pages>299–308</pages>
      <abstract>Among the numerous models developed to represent the multifaceted complexity of human personality, particularly in psychology, the Big Five (commonly referred to as ‘OCEAN’, an acronym of its five traits) stands out as a widely used framework. Although personalized chatbots have incorporated this model, existing approaches, such as focusing on individual traits or binary combinations, may not capture the full diversity of human personality. In this study, we propose a five-dimensional vector representation, where each axis corresponds to the degree of presence of an OCEAN trait on a continuous scale from 0 to 1. This representation is designed to enable greater versatility in modeling personality. Application to customer assistance scenarios in French demonstrates that, based on humans-bots as well as bots-bots conversations, assigned personality vectors are distinguishable by both humans and LLMs acting as judges. Both of their subjective evaluations also confirm the measurable impacts of the assigned personality on user experience, agent efficiency, and conversation quality.</abstract>
      <url hash="1dc79343">2025.iwsds-1.32</url>
      <bibkey>njifenjou-etal-2025-enabling</bibkey>
    </paper>
    <paper id="33">
      <title>Developing Classifiers for Affirmative and Negative User Responses with Limited Target Domain Data for Dialogue System Development Tools</title>
      <author><first>Yunosuke</first><last>Kubo</last></author>
      <author><first>Ryo</first><last>Yanagimoto</last></author>
      <author><first>Mikio</first><last>Nakano</last></author>
      <author><first>Kenta</first><last>Yamamoto</last></author>
      <author><first>Ryu</first><last>Takeda</last></author>
      <author><first>Kazunori</first><last>Komatani</last></author>
      <pages>309–317</pages>
      <abstract>We aim to develop a library for classifying affirmative and negative user responses, intended for integration into a dialogue system development toolkit. Such a library is expected to highly perform even with minimal annotated target domain data, addressing the practical challenge of preparing large datasets for each target domain. This short paper compares several approaches under conditions where little or no annotated data is available in the target domain. One approach involves fine-tuning a pre-trained BERT model, while the other utilizes a GPT API for zero-shot or few-shot learning. Since these approaches differ in execution speed, development effort, and execution costs, in addition to performance, the results serve as a basis for discussing an appropriate configuration suited to specific requirements. Additionally, we have released the training data and the fine-tuned BERT model for Japanese affirmative/negative classification.</abstract>
      <url hash="38632201">2025.iwsds-1.33</url>
      <bibkey>kubo-etal-2025-developing</bibkey>
    </paper>
    <paper id="34">
      <title>Why Do We Laugh? Annotation and Taxonomy Generation for Laughable Contexts in Spontaneous Text Conversation</title>
      <author><first>Koji</first><last>Inoue</last></author>
      <author><first>Mikey</first><last>Elmers</last></author>
      <author><first>Divesh</first><last>Lala</last></author>
      <author><first>Tatsuya</first><last>Kawahara</last></author>
      <pages>318–323</pages>
      <abstract>Laughter serves as a multifaceted communicative signal in human interaction, yet its identification within dialogue presents a significant challenge for conversational AI systems. This study addresses this challenge by annotating laughable contexts in Japanese spontaneous text conversation data and developing a taxonomy to classify the underlying reasons for such contexts. Initially, multiple annotators manually labeled laughable contexts using a binary decision (laughable or non-laughable). Subsequently, an LLM was used to generate explanations for the binary annotations of laughable contexts, which were then categorized into a taxonomy comprising ten categories, including “Empathy and Affinity” and “Humor and Surprise,” highlighting the diverse range of laughter-inducing scenarios. The study also evaluated GPT-4o’s performance in recognizing the majority labels of laughable contexts, achieving an F1 score of 43.14%. These findings contribute to the advancement of conversational AI by establishing a foundation for more nuanced recognition and generation of laughter, ultimately fostering more natural and engaging human-AI interactions.</abstract>
      <url hash="de36f0d2">2025.iwsds-1.34</url>
      <bibkey>inoue-etal-2025-laugh</bibkey>
    </paper>
    <paper id="35">
      <title>Adaptive Psychological Distance in <fixed-case>J</fixed-case>apanese Spoken Human-Agent Dialogue: A Politeness-Based Management Model</title>
      <author><first>Akira</first><last>Inaba</last></author>
      <author><first>Emmanuel</first><last>Ayedoun</last></author>
      <author><first>Masataka</first><last>Tokumaru</last></author>
      <pages>324–329</pages>
      <abstract>While existing spoken dialogue systems can adapt various aspects of interaction, systematic management of psychological distance through verbal politeness remains underexplored. Current approaches typically maintain fixed levels of formality and social distance, limiting naturalness in long-term human-agent interactions. We propose a novel dialogue management model that dynamically adjusts verbal politeness levels in Japanese based on user preferences. We evaluated the model using two pseudo-users with distinct distance preferences in daily conversations. Human observers (n=20) assessed the interactions, with 70% successfully distinguishing the intended social distance variations. The results demonstrate that systematic modulation of verbal politeness can create perceptibly different levels of psychological distance in spoken dialogue, with implications for culturally appropriate human-agent interaction in Japanese contexts.</abstract>
      <url hash="57ebbf74">2025.iwsds-1.35</url>
      <bibkey>inaba-etal-2025-adaptive</bibkey>
    </paper>
    <paper id="36">
      <title>An <fixed-case>LLM</fixed-case> Benchmark for Addressee Recognition in Multi-modal Multi-party Dialogue</title>
      <author><first>Koji</first><last>Inoue</last></author>
      <author><first>Divesh</first><last>Lala</last></author>
      <author><first>Mikey</first><last>Elmers</last></author>
      <author><first>Keiko</first><last>Ochi</last></author>
      <author><first>Tatsuya</first><last>Kawahara</last></author>
      <pages>330–334</pages>
      <abstract>Handling multi-party dialogues represents a significant step for advancing spoken dialogue systems, necessitating the development of tasks specific to multi-party interactions. To address this challenge, we are constructing a multi-modal multi-party dialogue corpus of triadic (three-participant) discussions. This paper focuses on the task of addressee recognition, identifying who is being addressed to take the next turn, a critical component unique to multi-party dialogue systems. A subset of the corpus was annotated with addressee information, revealing that explicit addressees are indicated in approximately 20% of conversational turns. To evaluate the task’s complexity, we benchmarked the performance of a large language model (GPT-4o) on addressee recognition. The results showed that GPT-4o achieved an accuracy only marginally above chance, underscoring the challenges of addressee recognition in multi-party dialogue. These findings highlight the need for further research to enhance the capabilities of large language models in understanding and navigating the intricacies of multi-party conversational dynamics.</abstract>
      <url hash="0ce87502">2025.iwsds-1.36</url>
      <bibkey>inoue-etal-2025-llm</bibkey>
    </paper>
    <paper id="37">
      <title>Will <fixed-case>AI</fixed-case> shape the way we speak? The emerging sociolinguistic influence of synthetic voices</title>
      <author><first>Eva</first><last>Szekely</last></author>
      <author><first>Jura</first><last>Miniota</last></author>
      <author><first>Míša (Michaela)</first><last>Hejná</last></author>
      <pages>335–340</pages>
      <abstract>The growing prevalence of conversational voice interfaces, powered by developments in both speech and language technologies, raises important questions about their influence on human communication. While written communication can signal identity through lexical and stylistic choices, voice-based interactions inherently amplify socioindexical elements – such as accent, intonation, and speech style – which more prominently convey social identity and group affiliation. There is evidence that even passive media such as television is likely to influence the audience’s linguistic patterns. Unlike passive media, conversational AI is interactive, creating a more immersive and reciprocal dynamic that holds a greater potential to impact how individuals speak in everyday interactions. Such heightened influence can be expected to arise from phenomena such as acoustic-prosodic entrainment and linguistic accommodation, which occur naturally during interaction and enable users to adapt their speech patterns in response to the system. While this phenomenon is still emerging, its potential societal impact could provide organisations, movements, and brands with a subtle yet powerful avenue for shaping and controlling public perception and social identity. We argue that the socioindexical influence of AI-generated speech warrants attention and should become a focus of interdisciplinary research, leveraging new and existing methodologies and technologies to better understand its implications.</abstract>
      <url hash="bb191305">2025.iwsds-1.37</url>
      <bibkey>szekely-etal-2025-will</bibkey>
    </paper>
  </volume>
</collection>
