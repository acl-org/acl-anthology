<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.mcg">
  <volume id="1" ingest-date="2025-01-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop on Multilingual Counterspeech Generation</booktitle>
      <editor><first>Helena</first><last>Bonaldi</last></editor>
      <editor><first>María Estrella</first><last>Vallecillo-Rodríguez</last></editor>
      <editor><first>Irune</first><last>Zubiaga</last></editor>
      <editor><first>Arturo</first><last>Montejo-Ráez</last></editor>
      <editor><first>Aitor</first><last>Soroa</last></editor>
      <editor><first>María Teresa</first><last>Martín-Valdivia</last></editor>
      <editor><first>Marco</first><last>Guerini</last></editor>
      <editor><first>Rodrigo</first><last>Agerri</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, UAE</address>
      <month>January</month>
      <year>2025</year>
      <url hash="3a7d727c">2025.mcg-1</url>
      <venue>mcg</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="913306bf">2025.mcg-1.0</url>
      <bibkey>mcg-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>PANDA</fixed-case> - Paired Anti-hate Narratives Dataset from <fixed-case>A</fixed-case>sia: Using an <fixed-case>LLM</fixed-case>-as-a-Judge to Create the First <fixed-case>C</fixed-case>hinese Counterspeech Dataset</title>
      <author><first>Michael</first><last>Bennie</last></author>
      <author><first>Demi</first><last>Zhang</last></author>
      <author><first>Bushi</first><last>Xiao</last></author>
      <author><first>Jing</first><last>Cao</last></author>
      <author><first>Chryseis Xinyi</first><last>Liu</last></author>
      <author><first>Jian</first><last>Meng</last></author>
      <author><first>Alayo</first><last>Tripp</last></author>
      <pages>1–12</pages>
      <abstract>Despite the global prevalence of Modern Standard Chinese language, counterspeech (CS) resources for Chinese remain virtually nonexistent. To address this gap in East Asian counterspeech research we introduce the a corpus of Modern Standard Mandarin counterspeech that focuses on combating hate speech in Mainland China. This paper proposes a novel approach of generating CS by using an LLM-as-a-Judge, simulated annealing, LLMs zero-shot CN generation and a round-robin algorithm. This is followed by manual verification for quality and contextual relevance. This paper details the methodology for creating effective counterspeech in Chinese and other non-Eurocentric languages, including unique cultural patterns of which groups are maligned and linguistic patterns in what kinds of discourse markers are programmatically marked as hate speech (HS). Analysis of the generated corpora, we provide strong evidence for the lack of open-source, properly labeled Chinese hate speech data and the limitations of using an LLM-as-Judge to score possible answers in Chinese. Moreover, the present corpus servers as the first East Asian language based CS corpus and provides an essential resource for future research on counterspeech generation and evaluation.</abstract>
      <url hash="6d30f082">2025.mcg-1.1</url>
      <bibkey>bennie-etal-2025-panda</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>RSSN</fixed-case> at Multilingual Counterspeech Generation: Leveraging Lightweight Transformers for Efficient and Context-Aware Counter-Narrative Generation</title>
      <author><first>Ravindran</first><last>V</last></author>
      <pages>13–18</pages>
      <abstract>This paper presents a system for counter-speech generation, developed for the COLING 2025 shared task. By leveraging lightweight transformer models, DistilBART and T5-small, we optimize computational efficiency while maintaining strong performance. The work includes an in-depth analysis of a multilingual dataset, addressing hate speech instances across diverse languages and target groups. Through systematic error analysis, we identify challenges such as lack of specificity and context misinterpretation in generated counter-narratives. Evaluation metrics like BLEU, ROUGE, and BERTScore demonstrate the effectiveness of our approaches, while comparative insights highlight complementary strengths in fluency, contextual integration, and creativity. Future directions focus on enhancing preprocessing, integrating external knowledge sources, and improving scalability.</abstract>
      <url hash="00ab0ef0">2025.mcg-1.2</url>
      <bibkey>v-2025-rssn</bibkey>
    </paper>
    <paper id="3">
      <title>Northeastern Uni at Multilingual Counterspeech Generation: Enhancing Counter Speech Generation with <fixed-case>LLM</fixed-case> Alignment through Direct Preference Optimization</title>
      <author><first>Sahil</first><last>Wadhwa</last></author>
      <author><first>Chengtian</first><last>Xu</last></author>
      <author><first>Haoming</first><last>Chen</last></author>
      <author><first>Aakash</first><last>Mahalingam</last></author>
      <author><first>Akankshya</first><last>Kar</last></author>
      <author><first>Divya</first><last>Chaudhary</last></author>
      <pages>19–28</pages>
      <abstract>The automatic generation of counter-speech (CS) is a critical strategy for addressing hate speech by providing constructive and informed responses. However, existing methods often fail to generate high-quality, impactful, and scalable CS, particularly across diverse lin- guistic contexts. In this paper, we propose a novel methodology to enhance CS generation by aligning Large Language Models (LLMs) using Supervised Fine-Tuning (SFT) and Di- rect Preference Optimization (DPO). Our ap- proach leverages DPO to align LLM outputs with human preferences, ensuring contextu- ally appropriate and linguistically adaptable responses. Additionally, we incorporate knowl- edge grounding to enhance the factual accuracy and relevance of generated CS. Experimental results demonstrate that DPO-aligned models significantly outperform SFT baselines on CS benchmarks while scaling effectively to mul- tiple languages. These findings highlight the potential of preference-based alignment tech- niques to advance CS generation across var- ied linguistic settings. The model supervision and alignment is done in English and the same model is used for reporting metrics across other languages like Basque, Italian, and Spanish.</abstract>
      <url hash="920d830f">2025.mcg-1.3</url>
      <bibkey>wadhwa-etal-2025-northeastern</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>NLP</fixed-case>@<fixed-case>IIMAS</fixed-case>-<fixed-case>CLTL</fixed-case> at Multilingual Counterspeech Generation: Combating Hate Speech Using Contextualized Knowledge Graph Representations and <fixed-case>LLM</fixed-case>s</title>
      <author><first>David Salvador</first><last>Márquez</last></author>
      <author><first>Helena Montserrat</first><last>Gómez Adorno</last></author>
      <author><first>Ilia</first><last>Markov</last></author>
      <author><first>Selene</first><last>Báez Santamaría</last></author>
      <pages>29–36</pages>
      <abstract>We present our approach for the shared task on Multilingual Counterspeech Generation (MCG) to counteract hate speech (HS) in Spanish, English, Basque, and Italian. To accomplish this, we followed two different strategies: 1) a graph-based generative model that encodes graph representations of knowledge related to hate speech, and 2) leveraging prompts for a large language model (LLM), specifically GPT-4o. We find that our graph-based approach tends to perform better in terms of traditional evaluation metrics (i.e., RougeL, BLEU, BERTScore), while the JudgeLM evaluation employed in the shared task favors the counter-narratives generated by the LLM-based approach, which was ranked second for English and third for Spanish on the leaderboard.</abstract>
      <url hash="18ca5188">2025.mcg-1.4</url>
      <bibkey>marquez-etal-2025-nlp</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>CODEOFCONDUCT</fixed-case> at Multilingual Counterspeech Generation: A Context-Aware Model for Robust Counterspeech Generation in Low-Resource Languages</title>
      <author><first>Michael</first><last>Bennie</last></author>
      <author><first>Bushi</first><last>Xiao</last></author>
      <author><first>Chryseis Xinyi</first><last>Liu</last></author>
      <author><first>Demi</first><last>Zhang</last></author>
      <author><first>Jian</first><last>Meng</last></author>
      <author><first>Alayo</first><last>Tripp</last></author>
      <pages>37–46</pages>
      <abstract>This paper introduces a context-aware model for robust counterspeech generation, which achieved significant success in the MCG-COLING-2025 shared task. Our approach particularly excelled in low-resource language settings. By leveraging a simulated annealing algorithm fine-tuned on multilingual datasets, the model generates factually accurate responses to hate speech. We demonstrate state-of-the-art performance across four languages (Basque, English, Italian, and Spanish), with our system ranking first for Basque, second for Italian, and third for both English and Spanish. Notably, our model swept all three top positions for Basque, highlighting its effectiveness in low-resource scenarios. Evaluation of the shared task employs both traditional metrics (BLEU, ROUGE, BERTScore, Novelty) and the LLM-based JudgeLM. We present a detailed analysis of our results, including error cases and potential improvements. This work contributes to the growing body of research on multilingual counterspeech generation, offering insights into developing robust models that can adapt to diverse linguistic and cultural contexts in the fight against online hate speech.</abstract>
      <url hash="0111f96c">2025.mcg-1.5</url>
      <attachment type="OptionalSupplementaryMaterial" hash="ff086434">2025.mcg-1.5.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>bennie-etal-2025-codeofconduct</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> at Multilingual Counterspeech Generation</title>
      <author><first>Xinglin</first><last>Lyu</last></author>
      <author><first>Haolin</first><last>Wang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <pages>47–55</pages>
      <abstract>Multilingual counterspeech generation (MCSG) contributes to generating counterspeech with respectful, non-offensive information that is specific and truthful for the given hate speech, especially those for languages other than English. Generally, the training data of MCSG in low-source language is rare and hard to curate. Even with the impressive large language models (LLMs), it is a struggle to generate an appreciative counterspeech under the multilingual scenario. In this paper, we design a pipeline with a generation-reranking mode to effectively generate counterspeech under the multilingual scenario via LLM. Considering the scarcity of training data, we first utilize the training-free strategy, i.e., in-context learning (ICL), to generate the candidate counterspeechs. Then, we propose to rerank those candidate counterspeech via the Elo rating algorithm and a fine-tuned reward model. Experimental results on four languages, including English (EN), Italian (IT), Basque (EU) and Spanish (ES), our system achieves a comparative or even better performance in four metrics compared to the winner in this shared task.</abstract>
      <url hash="7308fc38">2025.mcg-1.6</url>
      <bibkey>lyu-etal-2025-hw</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>MNLP</fixed-case>@Multilingual Counterspeech Generation: Evaluating Translation and Background Knowledge Filtering</title>
      <author><first>Emanuele</first><last>Moscato</last></author>
      <author><first>Arianna</first><last>Muti</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <pages>56–64</pages>
      <abstract>We describe our participation in the Multilingual Counterspeech Generation shared task, which aims to generate a counternarrative to counteract hate speech, given a hateful sentence and relevant background knowledge. Our team tested two different aspects: translating outputs from English vs generating outputs in the original languages and filtering pieces of the background knowledge provided vs including all the background knowledge. Our experiments show that filtering the background knowledge in the same prompt and leaving data in the original languages leads to more adherent counternarrative generations, except for Basque, where translating the output from English and filtering the background knowledge in a separate prompt yields better results. Our system ranked first in English, Italian, and Spanish and fourth in Basque.</abstract>
      <url hash="54167b81">2025.mcg-1.7</url>
      <bibkey>moscato-etal-2025-mnlp</bibkey>
    </paper>
    <paper id="8">
      <title>Hyderabadi Pearls at Multilingual Counterspeech Generation : <fixed-case>HALT</fixed-case> : Hate Speech Alleviation using Large Language Models and Transformers</title>
      <author><first>Md Shariq</first><last>Farhan</last></author>
      <pages>65–76</pages>
      <abstract>This paper explores the potential of using fine- tuned Large Language Models (LLMs) for generating counter-narratives (CNs) to combat hate speech (HS). We focus on English and Basque, leveraging the ML_MTCONAN_KN dataset, which provides hate speech and counter-narrative pairs in multiple languages. Our study compares the performance of Mis- tral, Llama, and a Llama-based LLM fine- tuned on a Basque language dataset for CN generation. The generated CNs are evalu- ated using JudgeLM (a LLM to evaluate other LLMs in open-ended scenarios) along with traditional metrics such as ROUGE-L, BLEU, BERTScore, and other traditional metrics. The results demonstrate that fine-tuned LLMs can produce high-quality contextually relevant CNs for low-resource languages that are comparable to human-generated responses, offering a sig- nificant contribution to combating online hate speech across diverse linguistic settings.</abstract>
      <url hash="fb4d2bcc">2025.mcg-1.8</url>
      <bibkey>farhan-2025-hyderabadi</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>T</fixed-case>ren<fixed-case>T</fixed-case>eam at Multilingual Counterspeech Generation: Multilingual Passage Re-Ranking Approaches for Knowledge-Driven Counterspeech Generation Against Hate</title>
      <author><first>Daniel</first><last>Russo</last></author>
      <pages>77–91</pages>
      <abstract>Hate speech (HS) in online spaces poses severe risks, including real-world violence and psychological harm to victims, necessitating effective countermeasures. Counterspeech (CS), which responds to hateful messages with opposing yet non-hostile narratives, offer a promising solution by mitigating HS while upholding free expression. However, the growing volume of HS demands automation, making Natural Language Processing a viable solution for the automatic generation of CS. Recent works have explored knowledge-driven approaches, leveraging external sources to improve the relevance and informativeness of responses. These methods typically involve multi-step pipelines combining retrieval and passage re-ranking modules. While effective, most studies have focused on English, with limited exploration of multilingual contexts. This paper addresses these gaps by proposing a multilingual, knowledge-driven approach to CS generation. We integrate state-of-the-art re-ranking mechanisms into the CS generation pipeline and evaluate them using the MT-CONAN-KN dataset, which includes hate speech, relevant knowledge sentences, and counterspeech in four languages: English, Italian, Spanish, and Basque. Our approach compares reranker-based systems employing multilingual cross-encoders and LLMs to a simpler end-to-end system where the language model directly handles both knowledge selection and CS generation. Results demonstrate that reranker-based systems outperformed end-to-end systems in syntactic and semantic similarity metrics, with LLM-based re-rankers delivering the strongest performance overall. This work is the result of our participation in the Shared Task on Multilingual Counterspeech Generation held at COLING 2025.</abstract>
      <url hash="35520254">2025.mcg-1.9</url>
      <bibkey>russo-2025-trenteam</bibkey>
    </paper>
    <paper id="10">
      <title>The First Workshop on Multilingual Counterspeech Generation at <fixed-case>COLING</fixed-case> 2025: Overview of the Shared Task</title>
      <author><first>Helena</first><last>Bonaldi</last></author>
      <author><first>María Estrella</first><last>Vallecillo-Rodríguez</last></author>
      <author><first>Irune</first><last>Zubiaga</last></author>
      <author><first>Arturo</first><last>Montejo-Raez</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <author><first>María-Teresa</first><last>Martín-Valdivia</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <pages>92–107</pages>
      <abstract>This paper presents an overview of the Shared Task organized in the First Workshop on Multilingual Counterspeech Generation at COLING 2025. While interest in automatic approaches to Counterspeech generation has been steadily growing, the large majority of the published experimental work has been carried out for English. This is due to the scarcity of both non-English manually curated training data and to the crushing predominance of English in the generative Large Language Models (LLMs) ecosystem. The task’s goal is to promote and encourage research on Counterspeech generation in a multilingual setting (Basque, English, Italian, and Spanish) potentially leveraging background knowledge provided in the proposed dataset. The task attracted 11 participants, 9 of whom presented a paper describing their systems. Together with the task, we introduce a new multilingual counterspeech dataset with 2384 triplets of hate speech, counterspeech, and related background knowledge covering 4 languages. The dataset is available at: https://huggingface.co/datasets/LanD-FBK/ML_MTCONAN_KN.</abstract>
      <url hash="ed8ada69">2025.mcg-1.10</url>
      <bibkey>bonaldi-etal-2025-first</bibkey>
    </paper>
  </volume>
</collection>
