<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.latechclfl">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 9th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature (LaTeCH-CLfL 2025)</booktitle>
      <editor><first>Anna</first><last>Kazantseva</last></editor>
      <editor><first>Stan</first><last>Szpakowicz</last></editor>
      <editor><first>Stefania</first><last>Degaetano-Ortlieb</last></editor>
      <editor><first>Yuri</first><last>Bizzoni</last></editor>
      <editor><first>Janis</first><last>Pagel</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>May</month>
      <year>2025</year>
      <url hash="4f4d5be1">2025.latechclfl-1</url>
      <venue>latechclfl</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-241-1</isbn>
    </meta>
    <frontmatter>
      <url hash="05969c86">2025.latechclfl-1.0</url>
      <bibkey>latechclfl-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Matching and Linking Entries in Historical <fixed-case>S</fixed-case>wedish Encyclopedias</title>
      <author><first>Simon</first><last>Börjesson</last><affiliation>Lund University</affiliation></author>
      <author><first>Erik</first><last>Ersmark</last><affiliation>Lund University</affiliation></author>
      <author><first>Pierre</first><last>Nugues</last><affiliation>Lund University</affiliation></author>
      <pages>1-10</pages>
      <abstract>The Nordisk familjebok is a Swedish encyclopedia from the 19th and 20th centuries. It was written by a team of experts and aimed to be an intellectual reference, stressing precision and accuracy. This encyclopedia had four main editions remarkable by their size, ranging from 20 to 38 volumes. As a consequence, the Nordisk familjebok had a considerable influence in universities, schools, the media, and society overall. As new editions were released, the selection of entries and their content evolved, reflecting intellectual changes in Sweden.In this paper, we used digitized versions from Project Runeberg. We first resegmented the raw text into entries and matched pairs of entries between the first and second editions using semantic sentence embeddings. We then extracted the geographical entries from both editions using a transformer-based classifier and linked them to Wikidata. This enabled us to identify geographic trends and possible shifts between the first and second editions, written between 1876–1899 and 1904–1926, respectively.Interpreting the results, we observe a small but significant shift in geographic focus away from Europe and towards North America, Africa, Asia, Australia, and northern Scandinavia from the first to the second edition, confirming the influence of the First World War and the rise of new powers. The code and data are available on GitHub at https://github.com/sibbo/nordisk-familjebok.</abstract>
      <url hash="16745c75">2025.latechclfl-1.1</url>
      <bibkey>borjesson-etal-2025-matching</bibkey>
    </paper>
    <paper id="2">
      <title>Preserving Comorian Linguistic Heritage: Bidirectional Transliteration Between the <fixed-case>L</fixed-case>atin Alphabet and the <fixed-case>K</fixed-case>amar-Eddine System</title>
      <author><first>Abdou Mohamed</first><last>Naira</last><affiliation>Institut National de Statistique et d’Economie Appliquée</affiliation></author>
      <author><first>Abdessalam</first><last>Bahafid</last><affiliation>Institut National de Statistique et d’Economie Appliquée</affiliation></author>
      <author><first>Zakarya</first><last>Erraji</last><affiliation>Institut National de Statistique et d’Economie Appliquée</affiliation></author>
      <author><first>Anass</first><last>Allak</last><affiliation>Institut National de Statistique et d’Economie Appliquée</affiliation></author>
      <author><first>Mohamed Soibira</first><last>Naoufal</last><affiliation>Sciences Po Grenoble – UGA (Université Grenoble Alpes)</affiliation></author>
      <author><first>Imade</first><last>Benelallam</last><affiliation>Institut National de Statistique et d’Economie Appliquée</affiliation></author>
      <pages>11-18</pages>
      <abstract>The Comoros Islands, rich in linguistic diversity, are home to dialects derived from Swahili and influenced by Arabic. Historically, the Kamar-Eddine system, based on the Arabic alphabet, was one of the first writing systems used for Comorian. However, it has gradually been replaced by the Latin alphabet, even though numerous archival texts are written in this system, and older speakers continue to use it, highlighting its cultural and historical significance. In this article, we present Shialifube, a bidirectional transliteration tool between Latin and Arabic scripts, designed in accordance with the rules of the Kamar-Eddine system. To evaluate its performance, we applied a round-trip transliteration technique, achieving a word error rate of 14.84% and a character error rate of 9.56%. These results demonstrate the reliability of our system for complex tasks. Furthermore, Shialifube was tested in a practical case related to speech recognition, showcasing its potential in Natural Language Processing. This project serves as a bridge between tradition and modernity, contributing to the preservation of Comorian linguistic heritage while paving the way for better integration of local dialects into advanced technologies.</abstract>
      <url hash="1c46b216">2025.latechclfl-1.2</url>
      <bibkey>naira-etal-2025-preserving</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>LLM</fixed-case>-based Adversarial Dataset Augmentation for Automatic Media Bias Detection</title>
      <author><first>Martin</first><last>Wessel</last><affiliation>TU München</affiliation></author>
      <pages>19-24</pages>
      <abstract>This study presents BiasAdapt, a novel data augmentation strategy designed to enhance the robustness of automatic media bias detection models. Leveraging the BABE dataset, BiasAdapt uses a generative language model to identify bias-indicative keywords and replace them with alternatives from opposing categories, thus creating adversarial examples that preserve the original bias labels. The contributions of this work are twofold: it proposes a scalable method for augmenting bias datasets with adversarial examples while preserving labels, and it publicly releases an augmented adversarial media bias dataset.Training on BiasAdapt reduces the reliance on spurious cues in four of the six evaluated media bias categories.</abstract>
      <url hash="da3a1c4f">2025.latechclfl-1.3</url>
      <bibkey>wessel-2025-llm</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>H</fixed-case>iero<fixed-case>LM</fixed-case>: <fixed-case>E</fixed-case>gyptian Hieroglyph Recovery with Next Word Prediction Language Model</title>
      <author><first>Xuheng</first><last>Cai</last><affiliation>Stanford University</affiliation></author>
      <author><first>Erica</first><last>Zhang</last><affiliation>Stanford University</affiliation></author>
      <pages>25-31</pages>
      <abstract>Egyptian hieroglyphs are found on numerous ancient Egyptian artifacts, but it is common that they are blurry or even missing due to erosion. Existing efforts to restore blurry hieroglyphs adopt computer vision techniques such as CNNs and model hieroglyph recovery as an image classification task, which suffers from two major limitations: (i) They cannot handle severely damaged or completely missing hieroglyphs. (ii) They make predictions based on a single hieroglyph without considering contextual and grammatical information. This paper proposes a novel approach to model hieroglyph recovery as a next word prediction task and use language models to address it. We compare the performance of different SOTA language models and choose LSTM as the architecture of our HieroLM due to the strong local affinity of semantics in Egyptian hieroglyph texts. Experiments show that HieroLM achieves over 44% accuracy and maintains notable performance on multi-shot predictions and scarce data, which makes it a pragmatic tool to assist scholars in inferring missing hieroglyphs. It can also complement CV-based models to significantly reduce perplexity in recognizing blurry hieroglyphs. Ourcode is available at https://github.com/Rick-Cai/HieroLM/.</abstract>
      <url hash="2be279e1">2025.latechclfl-1.4</url>
      <bibkey>cai-zhang-2025-hierolm</bibkey>
    </paper>
    <paper id="5">
      <title>Evaluating <fixed-case>LLM</fixed-case>-Prompting for Sequence Labeling Tasks in Computational Literary Studies</title>
      <author><first>Axel</first><last>Pichler</last><affiliation>SRC Text Studies</affiliation></author>
      <author><first>Janis</first><last>Pagel</last><affiliation>Department of Digital Humanities, University of Cologne</affiliation></author>
      <author><first>Nils</first><last>Reiter</last><affiliation>University of Cologne</affiliation></author>
      <pages>32-46</pages>
      <abstract>Prompt engineering holds the promise for the computational literary studies (CLS) to obtain high quality markup for literary research questions by simply prompting large language models with natural language strings. We test prompt engineering’s validity for two CLS sequence labeling tasks under the following aspects: (i) how generalizable are the results of identical prompts on different dataset splits?, (ii) how robust are performance results when re-formulating the prompts?, and (iii) how generalizable are certain fixed phrases added to the prompts that are generally considered to increase performance. We find that results are sensitive to data splits and prompt formulation, while the addition of fixed phrases does not change performance in most cases, depending on the chosen model.</abstract>
      <url hash="fd41d7ad">2025.latechclfl-1.5</url>
      <bibkey>pichler-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="6">
      <title>Generation of <fixed-case>R</fixed-case>ussian Poetry of Different Genres and Styles Using Neural Networks with Character-Level Tokenization</title>
      <author><first>Ilya</first><last>Koziev</last><affiliation>SalutDevices</affiliation></author>
      <author><first>Alena</first><last>Fenogenova</last><affiliation>SaluteDevices</affiliation></author>
      <pages>47-63</pages>
      <abstract>Automatic poetry generation is an immensely complex task, even for the most advanced Large Language Models (LLMs) that requires a profound understanding of intelligence, world and linguistic knowledge, and a touch of creativity.This paper investigates the use of LLMs in generating Russian syllabo-tonic poetry of various genres and styles. The study explores a character-level tokenization architectures and demonstrates how a language model can be pretrained and finetuned to generate poetry requiring knowledge of a language’s phonetics. Additionally, the paper assesses the quality of the generated poetry and the effectiveness of the approach in producing different genres and styles. The study’s main contribution is the introduction of two end-to-end architectures for syllabo-tonic Russian poetry: pretrained models, a comparative analysis of the approaches, and poetry evaluation metrics.</abstract>
      <url hash="fa7e68c9">2025.latechclfl-1.6</url>
      <bibkey>koziev-fenogenova-2025-generation</bibkey>
    </paper>
    <paper id="7">
      <title>Automating Violence Detection and Categorization from Ancient Texts</title>
      <author><first>Alhassan</first><last>Abdelhalim</last><affiliation>University of Hamburg</affiliation></author>
      <author><first>Michaela</first><last>Regneri</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>64-78</pages>
      <abstract>Violence descriptions in literature offer valuable insights for a wide range of research in the humanities. For historians, depictions of violence are of special interest for analyzing the societal dynamics surrounding large wars and individual conflicts of influential people. Harvesting data for violence research manually is laborious and time-consuming. This study is the first one to evaluate the effectiveness of large language models (LLMs) in identifying violence in ancient texts and categorizing it across multiple dimensions. Our experiments identify LLMs as a valuable tool to scale up the accurate analysis of historical texts and show the effect of fine-tuning and data augmentation, yielding an F1-score of up to 0.93 for violence detection and 0.86 for fine-grained violence categorization.</abstract>
      <url hash="b1dea123">2025.latechclfl-1.7</url>
      <bibkey>abdelhalim-regneri-2025-automating</bibkey>
    </paper>
    <paper id="8">
      <title>Rethinking Scene Segmentation. Advancing Automated Detection of Scene Changes in Literary Texts</title>
      <author><first>Svenja</first><last>Guhr</last><affiliation>Technical University of Darmstadt</affiliation></author>
      <author><first>Huijun</first><last>Mao</last><affiliation>Stanford University</affiliation></author>
      <author><first>Fengyi</first><last>Lin</last><affiliation>Stanford University</affiliation></author>
      <pages>79-86</pages>
      <abstract>Automated scene segmentation is an ongoing challenge in computational literary studies (CLS) to approach literary texts by analyzing comparable units. In this paper, we present our approach (work in progress) to text segmentation using a classifier that identifies the position of a scene change in English-language fiction. By manually annotating novels from a 20th-century US-English romance fiction corpus, we prepared training data for fine-tuning transformer models, yielding promising preliminary results for improving automated text segmentation in CLS.</abstract>
      <url hash="4de30326">2025.latechclfl-1.8</url>
      <bibkey>guhr-etal-2025-rethinking</bibkey>
    </paper>
    <paper id="9">
      <title>Sentence-Alignment in Semi-parallel Datasets</title>
      <author><first>Steffen</first><last>Frenzel</last><affiliation>University of Potsdam</affiliation></author>
      <author><first>Manfred</first><last>Stede</last><affiliation>University of Potsdam</affiliation></author>
      <pages>87-96</pages>
      <abstract>In this paper, we are testing sentence alignment on complex, semi-parallel corpora, i.e., different versions of the same text that have been altered to some extent. We evaluate two hypotheses: To make alignment algorithms more efficient, we test the hypothesis that matching pairs can be found in the immediate vicinity of the source sentence and that it is sufficient to search for paraphrases in a ‘context window’. To improve the alignment quality on complex, semi-parallel texts, we test the implementation of a segmentation into Elementary Discourse Units (EDUs) in order to make more precise alignments at this level. Since EDUs are the smallest possible unit for communicating a full proposition, we assume that aligning at this level can improve the overall quality. Both hypotheses are tested and validated with several embedding models on varying degrees of parallel German datasets. The advantages and disadvantages of the different approaches are presented, and our next steps are outlined.</abstract>
      <url hash="a92e3a87">2025.latechclfl-1.9</url>
      <bibkey>frenzel-stede-2025-sentence</bibkey>
    </paper>
    <paper id="10">
      <title>Argumentation in political empowerment on <fixed-case>I</fixed-case>nstagram</title>
      <author><first>Aenne</first><last>Knierim</last><affiliation>Universität Hildesheim</affiliation></author>
      <author><first>Ulrich</first><last>Heid</last><affiliation>University of Hildesheim</affiliation></author>
      <pages>97-108</pages>
      <abstract>This paper adopts a distant reading approach to analyze political empowerment on Instagram. We focus on argument mining and content classification to uncover cooccurences between aspects of political empowerment and argument components. We develop an annotation scheme based on literature in digital political empowerment, classifying content into five primary categories along the aspects of political awareness, personal e-identity and political participation. We implement the modified toulmin scheme for argument component detection. As an example discourse, we chose the German discourses #WirSindMehr and #NieWiederIstJetzt.The upheaval was targeted against right-wing extremism and antisemitism. Political awareness emerged as the dominant category, highlighting convergent public concern against antisemitism and right-wing extremism. Claims and backings often contain statements about societal change and aim to raise consciousness.Calls for participation in offline events appear mostly in non-argumentative texts.</abstract>
      <url hash="82494ba3">2025.latechclfl-1.10</url>
      <bibkey>knierim-heid-2025-argumentation</bibkey>
    </paper>
    <paper id="11">
      <title>Interpretable Models for Detecting Linguistic Variation in <fixed-case>R</fixed-case>ussian Media: Towards Unveiling Propagandistic Strategies during the Russo-<fixed-case>U</fixed-case>krainian War</title>
      <author><first>Anastasiia</first><last>Vestel</last><affiliation>Saarland University</affiliation></author>
      <author><first>Stefania</first><last>Degaetano-Ortlieb</last><affiliation>Saarland University</affiliation></author>
      <pages>109-116</pages>
      <abstract>With the start of the full-scale Russian invasion of Ukraine in February 2022, the spread of pro-Kremlin propaganda increased to justify the war, both in the official state media and social media. This position paper explores the theoretical background of propaganda detection in the given context and proposes a thorough methodology to investigate how language has been strategically manipulated to align with ideological goals and adapt to the changing narrative surrounding the invasion. Using the WarMM-2022 corpus, the study seeks to identify linguistic patterns across media types and their evolution over time. By doing so, we aim to enhance the understanding of the role of linguistic strategies in shaping propaganda narratives. The findings are intended to contribute to the broader discussion of information manipulation in politically sensitive contexts.</abstract>
      <url hash="94a7bbb5">2025.latechclfl-1.11</url>
      <bibkey>vestel-degaetano-ortlieb-2025-interpretable</bibkey>
    </paper>
    <paper id="12">
      <title>Tuning Into Bias: A Computational Study of Gender Bias in Song Lyrics</title>
      <author><first>Danqing</first><last>Chen</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Adithi</first><last>Satish</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Rasul</first><last>Khanbayov</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Carolin</first><last>Schuster</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Georg</first><last>Groh</last><affiliation>TUM</affiliation></author>
      <pages>117-129</pages>
      <abstract>The application of text mining methods is becoming increasingly prevalent, particularly within Humanities and Computational Social Sciences, as well as in a broader range of disciplines. This paper presents an analysis of gender bias in English song lyrics using topic modeling and bias measurement techniques. Leveraging BERTopic, we cluster a dataset of 537,553 English songs into distinct topics and analyze their temporal evolution. Our results reveal a significant thematic shift in song lyrics over time, transitioning from romantic themes to a heightened focus on the sexualization of women. Additionally, we observe a substantial prevalence of profanity and misogynistic content across various topics, with a particularly high concentration in the largest thematic cluster. To further analyse gender bias across topics and genres in a quantitative way, we employ the Single Category Word Embedding Association Test (SC-WEAT) to calculate bias scores for word embeddings trained on the most prominent topics as well as individual genres. The results indicate a consistent male bias in words associated with intelligence and strength, while appearance and weakness words show a female bias. Further analysis highlights variations in these biases across topics, illustrating the interplay between thematic content and gender stereotypes in song lyrics.</abstract>
      <url hash="14bc88f4">2025.latechclfl-1.12</url>
      <bibkey>chen-etal-2025-tuning</bibkey>
    </paper>
    <paper id="13">
      <title>Artificial Relationships in Fiction: A Dataset for Advancing <fixed-case>NLP</fixed-case> in Literary Domains</title>
      <author><first>Despina</first><last>Christou</last><affiliation>Aristotle University of Thessaloniki</affiliation></author>
      <author><first>Grigorios</first><last>Tsoumakas</last><affiliation>Aristotle University of Thessaloniki</affiliation></author>
      <pages>130-147</pages>
      <abstract>Relation extraction (RE) in fiction presents unique NLP challenges due to implicit, narrative-driven relationships. Unlike factual texts, fiction weaves complex connections, yet existing RE datasets focus on non-fiction. To address this, we introduce Artificial Relationships in Fiction (ARF), a synthetically annotated dataset for literary RE. Built from diverse Project Gutenberg fiction, ARF considers author demographics, publication periods, and themes. We curated an ontology for fiction-specific entities and relations, and using GPT-4o, generated artificial relationships to capture narrative complexity. Our analysis demonstrates its value for finetuning RE models and advancing computational literary studies. By bridging a critical RE gap, ARF enables deeper exploration of fictional relationships, enriching NLP research at the intersection of storytelling and AI-driven literary analysis.</abstract>
      <url hash="8589c761">2025.latechclfl-1.13</url>
      <bibkey>christou-tsoumakas-2025-artificial</bibkey>
    </paper>
    <paper id="14">
      <title>Improving Hate Speech Classification with Cross-Taxonomy Dataset Integration</title>
      <author><first>Jan</first><last>Fillies</last><affiliation>Freie Universität Berlin</affiliation></author>
      <author><first>Adrian</first><last>Paschke</last><affiliation>Fraunhofer FOKUS</affiliation></author>
      <pages>148-159</pages>
      <abstract>Algorithmic hate speech detection faces significant challenges due to the diverse definitions and datasets used in research and practice. Social media platforms, legal frameworks, and institutions each apply distinct yet overlapping definitions, complicating classification efforts. This study addresses these challenges by demonstrating that existing datasets and taxonomies can be integrated into a unified model, enhancing prediction performance and reducing reliance on multiple specialized classifiers. The work introduces a universal taxonomy and a hate speech classifier capable of detecting a wide range of definitions within a single framework. Our approach is validated by combining two widely used but differently annotated datasets, showing improved classification performance on an independent test set. This work highlights the potential of dataset and taxonomy integration in advancing hate speech detection, increasing efficiency, and ensuring broader applicability across contexts.</abstract>
      <url hash="493c1936">2025.latechclfl-1.14</url>
      <bibkey>fillies-paschke-2025-improving</bibkey>
    </paper>
    <paper id="15">
      <title>Classifying Textual Genre in Historical Magazines (1875-1990)</title>
      <author><first>Vera</first><last>Danilova</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Ylva</first><last>Söderfeldt</last><affiliation>Uppsala University</affiliation></author>
      <pages>160-171</pages>
      <abstract>Historical magazines are a valuable resource for understanding the past, offering insights into everyday life, culture, and evolving social attitudes. They often feature diverse layouts and genres. Short stories, guides, announcements, and promotions can all appear side by side on the same page. Without grouping these documents by genre, term counts and topic models may lead to incorrect interpretations.This study takes a step towards addressing this issue by focusing on genre classification within a digitized collection of European medical magazines in Swedish and German. We explore 2 scenarios: 1) leveraging the available web genre datasets for zero-shot genre prediction, 2) semi-supervised learning over the few-shot setup. This paper offers the first experimental insights in this direction.We find that 1) with a custom genre scheme tailored to historical dataset characteristics it is possible to effectively utilize categories from web genre datasets for cross-domain and cross-lingual zero-shot prediction, 2) semi-supervised training gives considerable advantages over few-shot for all models, particularly for the historical multilingual BERT.</abstract>
      <url hash="538db222">2025.latechclfl-1.15</url>
      <bibkey>danilova-soderfeldt-2025-classifying</bibkey>
    </paper>
    <paper id="16">
      <title>Lexical Semantic Change Annotation with Large Language Models</title>
      <author><first>Thora</first><last>Hagen</last><affiliation>University of Wuerzburg</affiliation></author>
      <pages>172-178</pages>
      <abstract>This paper explores the application of state-of-the-art large language models (LLMs) to the task of lexical semantic change annotation (LSCA) using the historical German DURel dataset. We evaluate five LLMs, and investigate whether retrieval-augmented generation (RAG) with historical encyclopedic knowledge enhances results. Our findings show that the Llama3.3 model achieves comparable performance to GPT-4o despite significant parameter differences, while RAG marginally improves predictions for smaller models but hampers performance for larger ones. Further analysis suggests that our additional context benefits nouns more than verbs and adjectives, demonstrating the nuances of integrating external knowledge for semantic tasks.</abstract>
      <url hash="75fdcfd1">2025.latechclfl-1.16</url>
      <bibkey>hagen-2025-lexical</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>AI</fixed-case> Conversational Interviewing: Transforming Surveys with <fixed-case>LLM</fixed-case>s as Adaptive Interviewers</title>
      <author><first>Alexander</first><last>Wuttke</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Matthias</first><last>Assenmacher</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Christopher</first><last>Klamm</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Max</first><last>Lang</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Fraue</first><last>Kreuter</last><affiliation>LMU Munich</affiliation></author>
      <pages>179-204</pages>
      <abstract>Traditional methods for eliciting people’s opinions face a trade-off between depth and scale: structured surveys enable large-scale data collection but limit respondents’ ability to voice their opinions in their own words, while conversational interviews provide deeper insights but are resource-intensive. This study explores the potential of replacing human interviewers with large language models (LLMs) to conduct scalable conversational interviews. Our goal is to assess the performance of AI Conversational Interviewing and to identify opportunities for improvement in a controlled environment. We conducted a small-scale, in-depth study with university students who were randomly assigned to a conversational interview by either AI or human interviewers, both employing identical questionnaires on political topics. Various quantitative and qualitative measures assessed interviewer adherence to guidelines, response quality, participant engagement, and overall interview efficacy. The findings indicate the viability of AI Conversational Interviewing in producing quality data comparable to traditional methods, with the added benefit of scalability. We publish our data and materials for re-use and present specific recommendations for effective implementation.</abstract>
      <url hash="87342e2b">2025.latechclfl-1.17</url>
      <bibkey>wuttke-etal-2025-ai</bibkey>
    </paper>
    <paper id="18">
      <title>Embedded Personalities: Word Embeddings and the “Big Five” Personality Model</title>
      <author><first>Oliver</first><last>Müller</last><affiliation>Saarland University</affiliation></author>
      <author><first>Stefania</first><last>Degaetano-Ortlieb</last><affiliation>Saarland University</affiliation></author>
      <pages>205-215</pages>
      <url hash="7ad21c21">2025.latechclfl-1.18</url>
      <bibkey>muller-degaetano-ortlieb-2025-embedded</bibkey>
    </paper>
    <paper id="19">
      <title>Prompting the Past: Exploring Zero-Shot Learning for Named Entity Recognition in Historical Texts Using Prompt-Answering <fixed-case>LLM</fixed-case>s</title>
      <author><first>Crina</first><last>Tudor</last><affiliation>Stockholm University</affiliation></author>
      <author><first>Beata</first><last>Megyesi</last><affiliation>uu.se</affiliation></author>
      <author><first>Robert</first><last>Östling</last><affiliation>Department of Linguistics, Stockholm University</affiliation></author>
      <pages>216-226</pages>
      <abstract>This paper investigates the application of prompt-answering Large Language Models (LLMs) for the task of Named Entity Recognition (NER) in historical texts. Historical NER presents unique challenges due to language change through time, spelling variation, limited availability of digitized data (and, in particular, labeled data), and errors introduced by Optical Character Recognition (OCR) and Handwritten Text Recognition (HTR) processes. Leveraging the zero-shot capabilities of prompt-answering LLMs, we address these challenges by prompting the model to extract entities such as persons, locations, organizations, and dates from historical documents. We then conduct an extensive error analysis of the model output in order to identify and address potential weaknesses in the entity recognition process. The results show that, while such models display ability for extracting named entities, their overall performance is lackluster. Our analysis reveals that model performance is significantly affected by hallucinations in the model output, as well as by challenges imposed by the evaluation of NER output.</abstract>
      <url hash="aedada46">2025.latechclfl-1.19</url>
      <bibkey>tudor-etal-2025-prompting</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>LLM</fixed-case>s for Translation: Historical, Low-Resourced Languages and Contemporary <fixed-case>AI</fixed-case> Models</title>
      <author><first>Merve</first><last>Tekgürler</last><affiliation>Stanford University</affiliation></author>
      <pages>227-237</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable adaptability in performing various tasks, including machine translation (MT), without explicit training. Models such as OpenAI’s GPT-4 and Google’s Gemini are frequently evaluated on translation benchmarks and utilized as translation tools due to their high performance. This paper examines Gemini’s performance in translating an 18th-century Ottoman Turkish manuscript, Prisoner of the Infidels: The Memoirs of Osman Agha of Timișoara, into English. The manuscript recounts the experiences of Osman Agha, an Ottoman subject who spent 11 years as a prisoner of war in Austria, and includes his accounts of warfare and violence. Our analysis reveals that Gemini’s safety mechanisms flagged between 14% and 23% of the manuscript as harmful, resulting in untranslated passages. These safety settings, while effective in mitigating potential harm, hinder the model’s ability to provide complete and accurate translations of historical texts. Through real historical examples, this study highlights the inherent challenges and limitations of current LLM safety implementations in the handling of sensitive and context-rich materials. These real-world instances underscore potential failures of LLMs in contemporary translation scenarios, where accurate and comprehensive translations are crucial—for example, translating the accounts of modern victims of war for legal proceedings or humanitarian documentation.</abstract>
      <url hash="7c89c385">2025.latechclfl-1.20</url>
      <bibkey>tekgurler-2025-llms</bibkey>
    </paper>
    <paper id="21">
      <title>Optimizing Cost-Efficiency with <fixed-case>LLM</fixed-case>-Generated Training Data for Conversational Semantic Frame Analysis</title>
      <author><first>Shiho</first><last>Matta</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Yin Jou</first><last>Huang</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Fei</first><last>Cheng</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Hirokazu</first><last>Kiyomaru</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Yugo</first><last>Murawaki</last><affiliation>Kyoto University</affiliation></author>
      <pages>238-251</pages>
      <abstract>Recent studies have shown that few-shot learning enables large language models (LLMs) to generate training data for supervised models at a low cost. However, for complex tasks, the quality of LLM-generated data often falls short compared to human-labeled data. This presents a critical challenge: how should one balance the trade-off between the higher quality but more expensive human-annotated data and the lower quality yet significantly cheaper LLM-generated data? In this paper, we tackle this question for a demanding task: conversational semantic frame analysis (SFA). To address this, we propose a novel method for synthesizing training data tailored to this complex task. Through experiments conducted across a wide range of budget levels, we find that smaller budgets favor a higher reliance on LLM-generated data to achieve optimal cost-efficiency.</abstract>
      <url hash="622c2cb1">2025.latechclfl-1.21</url>
      <bibkey>matta-etal-2025-optimizing</bibkey>
    </paper>
    <paper id="22">
      <title>Don’t stop pretraining! Efficiently building specialised language models in resource-constrained settings.</title>
      <author><first>Sven</first><last>Najem-Meyer</last><affiliation>EPFL</affiliation></author>
      <author><first>Frédéric</first><last>Kaplan</last><affiliation>EPFL</affiliation></author>
      <author><first>Matteo</first><last>Romanello</last><affiliation>École polytechnique fédérale de Lausanne / Deutsches Archäologisches Institut</affiliation></author>
      <pages>252-260</pages>
      <abstract>Developing specialised language models for low-resource domains typically involves a trade-off between two specialisation strategies: adapting a general-purpose model through continued pretraining or retraining a model from scratch. While adapting preserves the model’s linguistic knowledge, retraining benefits from the flexibility of an in-domain tokeniser – a potentially significant advantage when handling rare languages. This study investigates the impact of tokenisation, specialisation strategy, and pretraining data availability using classical scholarship – a multilingual, code-switching and highly domain-specific field – as a case study. Through extensive experiments, we assess whether domain-specific tokenisation improves model performance, whether character-based models provide a viable alternative to subword-based models, and which specialisation strategy is optimal given the constraints of limited pretraining data. Contrary to prior findings, our results show that in-domain tokenisation does not necessarily enhance performance. Most notably, adaptation consistently outperforms retraining, even with limited data, confirming its efficiency as the preferred strategy for resource-constrained domains. These insights provide valuable guidelines for developing specialised models in fields with limited textual resources.</abstract>
      <url hash="826d0dca">2025.latechclfl-1.22</url>
      <bibkey>najem-meyer-etal-2025-dont</bibkey>
    </paper>
    <paper id="23">
      <title>‘... like a needle in a haystack”: Annotation and Classification of Comparative Statements</title>
      <author><first>Pritha</first><last>Majumdar</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Franziska</first><last>Pannach</last><affiliation>University of Göttingen</affiliation></author>
      <author><first>Arianna</first><last>Graciotti</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Johan</first><last>Bos</last><affiliation>University of Groningen</affiliation></author>
      <pages>261-271</pages>
      <abstract>We present a clear distinction between the phenomena of comparisons and similes along with a fine-grained annotation guideline that facilitates the structural annotation and assessment of the two classes, with three major contributions: 1) a publicly available annotated data set of 100 comparative statements; 2) theoretically grounded annotation guidelines for human annotators; and 3) results of machine learning experiments to establish how the–often subtle–distinction between the two phenomena can be automated.</abstract>
      <url hash="d1ded200">2025.latechclfl-1.23</url>
      <bibkey>majumdar-etal-2025-like</bibkey>
    </paper>
    <paper id="24">
      <title>Identifying Small Talk in Natural Conversations</title>
      <author><first>Steffen</first><last>Frenzel</last><affiliation>University of Potsdam</affiliation></author>
      <author><first>Annette</first><last>Hautli-Janisz</last><affiliation>University of Passau</affiliation></author>
      <pages>272-277</pages>
      <abstract>Small talk is part and parcel of human interaction and is rather employed to communicate values and opinions than pure information. Despite small talk being an omnipresent phenomenon in spoken language, it is difficult to identify: Small talk is situated, i.e., for interpreting a string of words or discourse units, outside references such as the context of the interlocutors and their previous experiences have to be interpreted.In this paper, we present a dataset of natural conversation annotated with a theoretically well-motivated distillation of what constitutes small talk. This dataset comprises of verbatim transcribed public service encounters in German authorities and are the basis for empirical work in administrative policy on how the satisfaction of the citizen manifests itself in the communication with the authorities. We show that statistical models achieve comparable results to those of state-of-the-art LLMs.</abstract>
      <url hash="1e20dca0">2025.latechclfl-1.24</url>
      <bibkey>frenzel-hautli-janisz-2025-identifying</bibkey>
    </paper>
    <paper id="25">
      <title>Why Novels (Don’t) Break Through: Dynamics of Canonicity in the <fixed-case>D</fixed-case>anish Modern Breakthrough (1870-1900)</title>
      <author><first>Alie</first><last>Lassche</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Pascale</first><last>Feldkamp</last><affiliation>Center for Humanities Computing, Aarhus University</affiliation></author>
      <author><first>Yuri</first><last>Bizzoni</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Katrine</first><last>Baunvig</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Kristoffer</first><last>Nielbo</last><affiliation>Center for Humanities Computing, Aarhus University</affiliation></author>
      <pages>278-290</pages>
      <abstract>Recent studies suggest that canonical works possess unique textual profiles, often tied to innovation and higher cognitive demands. However, recent work on Danish 19th century literary novels has shown that some non-canonical works shared similar textual qualities with canonical works, underscoring the role of text-extrinsic factors in shaping canonicity. The present study examines the same corpus (more than 800 Danish novels from the Modern Breakthrough era (1870–1900)) to explore socio-economic and institutional factors, as well as demographic features, specifically, book prices, publishers, and the author’s nationality – in determining canonical status. We combine expert-based and national definitions of canon to set up a classification experiment to test the predictive power of these external features, and to understand how they relate to that of text-intrinsic features. We show that the canonization process is influenced by external factors – such as publisher and nationality – but that text-intrinsic features nevertheless maintain predictive power in a dynamic interplay of text and context.</abstract>
      <url hash="f06aa314">2025.latechclfl-1.25</url>
      <bibkey>lassche-etal-2025-novels</bibkey>
    </paper>
    <paper id="26">
      <title>Adapting Multilingual Embedding Models to Historical <fixed-case>L</fixed-case>uxembourgish</title>
      <author><first>Andrianos</first><last>Michail</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Corina</first><last>Raclé</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Juri</first><last>Opitz</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Simon</first><last>Clematide</last><affiliation>University of Zurich</affiliation></author>
      <pages>291-298</pages>
      <abstract>The growing volume of digitized historical texts requires effective semantic search using text embeddings. However, pre-trained multilingual models face challenges with historical content due to OCR noise and outdated spellings. This study examines multilingual embeddings for cross-lingual semantic search in historical Luxembourgish (LB), a low-resource language. We collect historical Luxembourgish news articles from various periods and use GPT-4o for sentence segmentation and translation, generating 20,000 parallel training sentences per language pair. Additionally, we create a semantic search (Historical LB Bitext Mining) evaluation set and find that existing models perform poorly on cross-lingual search for historical Luxembourgish. Using our historical and additional modern parallel training data, we adapt several multilingual embedding models through contrastive learning or knowledge distillation and increase accuracy significantly for all models. We release our adapted models and historical Luxembourgish-German/French/English bitexts to support further research.</abstract>
      <url hash="3b564a8c">2025.latechclfl-1.26</url>
      <bibkey>michail-etal-2025-adapting</bibkey>
    </paper>
  </volume>
</collection>
