<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.peoples">
  <volume id="1" ingest-date="2020-11-29">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media</booktitle>
      <editor><first>Malvina</first><last>Nissim</last></editor>
      <editor><first>Viviana</first><last>Patti</last></editor>
      <editor><first>Barbara</first><last>Plank</last></editor>
      <editor><first>Esin</first><last>Durmus</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Barcelona, Spain (Online)</address>
      <month>December</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="9f683407">2020.peoples-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Inferring Neuroticism of <fixed-case>T</fixed-case>witter Users by Utilizing their Following Interests</title>
      <author><first>Joran</first><last>Cornelisse</last></author>
      <pages>1–10</pages>
      <abstract>Twitter is a medium where, when used adequately, users’ interests can be derived from what he follows. This characteristic can make it attractive for a source of personality derivation. We set out to test the hypothesis that, analogous to the Lexical hypothesis, which posits that word use should reveal personality, following behavior on social media should reveal personality aspects. We used a two-step approach, wherein the first stage, we selected accounts for whom it was possible to infer personality profiles to some extent using available literature on personality and interests. On these accounts, we trained a regression model and segmented the derived features using hierarchical cluster analysis. In the second stage, we obtained a small sample of users’ personalities via a questionnaire and tested whether the model from stage 1 correlated with the users from step 2. The the explained variance for the neurotic and neutral neuroticism groups indicated significant results (R2 = .131, p = .0205; R2 = .22, p = .0044). Confirming the hypothesis that following behavior should be correlated with one’s interests and that interests are correlated with the neuroticism personality dimension.</abstract>
      <url hash="08b5e811">2020.peoples-1.1</url>
    </paper>
    <paper id="2">
      <title>Matching Theory and Data with Personal-<fixed-case>ITY</fixed-case>: What a Corpus of <fixed-case>I</fixed-case>talian <fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ube Comments Reveals About Personality</title>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <author><first>Viviana</first><last>Patti</last></author>
      <pages>11–22</pages>
      <abstract>As a contribution to personality detection in languages other than English, we rely on distant supervision to create Personal-ITY, a novel corpus of YouTube comments in Italian, where authors are labelled with personality traits. The traits are derived from one of the mainstream personality theories in psychology research, named MBTI. Using personality prediction experiments, we (i) study the task of personality prediction in itself on our corpus as well as on TWISTY, a Twitter dataset also annotated with MBTI labels; (ii) carry out an extensive, in-depth analysis of the features used by the classifier, and view them specifically under the light of the original theory that we used to create the corpus in the first place. We observe that no single model is best at personality detection, and that while some traits are easier than others to detect, and also to match back to theory, for other, less frequent traits the picture is much more blurred.</abstract>
      <url hash="3fafa6c0">2020.peoples-1.2</url>
    </paper>
    <paper id="3">
      <title>Red Is Open-Minded, Blue Is Conscientious: Predicting User Traits From <fixed-case>I</fixed-case>nstagram Image Data</title>
      <author><first>Lisa</first><last>Branz</last></author>
      <author><first>Patricia</first><last>Brockmann</last></author>
      <author><first>Annika</first><last>Hinze</last></author>
      <pages>23–28</pages>
      <abstract>Various studies have addressed the connection between a user’s traits and their social media content. This paper explores the relationship between gender, age and Big Five personality traits of 179 university students from Germany and their Instagram images. With regards to both image features and image content, significant differences between genders as well as preferences related to age and personality traits emerged. Gender, age and personality traits are predicted using machine learning classification and regression methods. This work is the first of its kind to focus on data from European Instagram users, as well as to predict age from Instagram image features and content on a fine-grained level.</abstract>
      <url hash="066f21e1">2020.peoples-1.3</url>
    </paper>
    <paper id="4">
      <title>Persuasiveness of News Editorials depending on Ideology and Personality</title>
      <author><first>Roxanne</first><last>El Baff</last></author>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Benno</first><last>Stein</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <pages>29–40</pages>
      <abstract>News editorials aim to shape the opinions of their readership and the general public on timely controversial issues. The impact of an editorial on the reader’s opinion does not only depend on its content and style, but also on the reader’s profile. Previous work has studied the effect of editorial style depending on general political ideologies (liberals vs.conservatives). In our work, we dig deeper into the persuasiveness of both content and style, exploring the role of the intensity of an ideology (lean vs.extreme) and the reader’s personality traits (agreeableness, conscientiousness, extraversion, neuroticism, and openness). Concretely, we train content- and style-based models on New York Times editorials for different ideology- and personality-specific groups. Our results suggest that particularly readers with extreme ideology and non “role model” personalities are impacted by style. We further analyze the importance of various text features with respect to the editorials’ impact, the readers’ profile, and the editorials’ geographical scope.</abstract>
      <url hash="735e8007">2020.peoples-1.4</url>
    </paper>
    <paper id="5">
      <title><fixed-case>H</fixed-case>ope<fixed-case>EDI</fixed-case>: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion</title>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <pages>41–53</pages>
      <abstract>Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff’s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.</abstract>
      <url hash="d8687ffb">2020.peoples-1.5</url>
    </paper>
    <paper id="6">
      <title><fixed-case>K</fixed-case>an<fixed-case>CMD</fixed-case>: <fixed-case>K</fixed-case>annada <fixed-case>C</fixed-case>ode<fixed-case>M</fixed-case>ixed Dataset for Sentiment Analysis and Offensive Language Detection</title>
      <author><first>Adeep</first><last>Hande</last></author>
      <author><first>Ruba</first><last>Priyadharshini</last></author>
      <author><first>Bharathi Raja</first><last>Chakravarthi</last></author>
      <pages>54–63</pages>
      <abstract>We introduce Kannada CodeMixed Dataset (KanCMD), a multi-task learning dataset for sentiment analysis and offensive language identification. The KanCMD dataset highlights two real-world issues from the social media text. First, it contains actual comments in code mixed text posted by users on YouTube social media, rather than in monolingual text from the textbook. Second, it has been annotated for two tasks, namely sentiment analysis and offensive language detection for under-resourced Kannada language. Hence, KanCMD is meant to stimulate research in under-resourced Kannada language on real-world code-mixed social media text and multi-task learning. KanCMD was obtained by crawling the YouTube, and a minimum of three annotators annotates each comment. We release KanCMD 7,671 comments for multitask learning research purpose.</abstract>
      <url hash="5df5d2e3">2020.peoples-1.6</url>
    </paper>
    <paper id="7">
      <title>Contextual Augmentation of Pretrained Language Models for Emotion Recognition in Conversations</title>
      <author><first>Jonggu</first><last>Kim</last></author>
      <author><first>Hyeonmok</first><last>Ko</last></author>
      <author><first>Seoha</first><last>Song</last></author>
      <author><first>Saebom</first><last>Jang</last></author>
      <author><first>Jiyeon</first><last>Hong</last></author>
      <pages>64–73</pages>
      <abstract>Since language model pretraining to learn contextualized word representations has been proposed, pretrained language models have made success in many natural language processing tasks. That is because it is helpful to use individual contextualized representations of self-attention layers as to initialize parameters for downstream tasks. Yet, unfortunately, use of pretrained language models for emotion recognition in conversations has not been studied enough. We firstly use ELECTRA which is a state-of-the-art pretrained language model and validate the performance on emotion recognition in conversations. Furthermore, we propose contextual augmentation of pretrained language models for emotion recognition in conversations, which is to consider not only previous utterances, but also conversation-related information such as speakers, speech acts and topics. We classify information based on what the information is related to, and propose position of words corresponding to the information in the entire input sequence. To validate the proposed method, we conduct experiments on the DailyDialog dataset which contains abundant annotated information of conversations. The experiments show that the proposed method achieves state-of-the-art F1 scores on the dataset and significantly improves the performance.</abstract>
      <url hash="c1492b63">2020.peoples-1.7</url>
    </paper>
    <paper id="8">
      <title>Social Media Unrest Prediction during the <fixed-case>COVID</fixed-case>-19 Pandemic: Neural Implicit Motive Pattern Recognition as Psychometric Signs of Severe Crises</title>
      <author><first>Dirk</first><last>Johannßen</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>74–86</pages>
      <abstract>The COVID-19 pandemic has caused international social tension and unrest. Besides the crisis itself, there are growing signs of rising conflict potential of societies around the world. Indicators of global mood changes are hard to detect and direct questionnaires suffer from social desirability biases. However, so-called implicit methods can reveal humans intrinsic desires from e.g. social media texts. We present psychologically validated social unrest predictors and replicate scalable and automated predictions, setting a new state of the art on a recent German shared task dataset. We employ this model to investigate a change of language towards social unrest during the COVID-19 pandemic by comparing established psychological predictors on samples of tweets from spring 2019 with spring 2020. The results show a significant increase of the conflict indicating psychometrics. With this work, we demonstrate the applicability of automated NLP-based approaches to quantitative psychological research.</abstract>
      <url hash="58fd7497">2020.peoples-1.8</url>
    </paper>
    <paper id="9">
      <title>Topic and Emotion Development among <fixed-case>D</fixed-case>utch <fixed-case>COVID</fixed-case>-19 <fixed-case>T</fixed-case>witter Communities in the early Pandemic</title>
      <author><first>Boris</first><last>Marinov</last></author>
      <author><first>Jennifer</first><last>Spenader</last></author>
      <author><first>Tommaso</first><last>Caselli</last></author>
      <pages>87–98</pages>
      <abstract>The paper focuses on a large collection of Dutch tweets from the Netherlands to get an insight into the perception and reactions of users during the early months of the COVID-19 pandemic. We focused on five major user communities of users: government and health organizations, news media, politicians, the general public and conspiracy theory supporters, investigating differences among them in topic dominance and the expressions of emotions. Through topic modeling we monitor the evolution of the conversation about COVID-19 among these communities. Our results indicate that the national focus on COVID-19 shifted from the virus itself to its impact on the economy between February and April. Surprisingly, the overall emotional public response appears to be substantially positive and expressing trust, although differences can be observed in specific group of users.</abstract>
      <url hash="592c4567">2020.peoples-1.9</url>
    </paper>
    <paper id="10">
      <title>Sentiments in <fixed-case>R</fixed-case>ussian Medical Professional Discourse during the Covid-19 Pandemic</title>
      <author><first>Irina</first><last>Ovchinnikova</last></author>
      <author><first>Liana</first><last>Ermakova</last></author>
      <author><first>Diana</first><last>Nurbakova</last></author>
      <pages>99–108</pages>
      <abstract>Medical discourse within the professional community has undeservingly received very sparse researchers’ attention. Medical professional discourse exists offline and online. We carried out sentiment analysis on titles and text descriptions of materials published on the Russian portal Mir Vracha (90,000 word forms approximately). The texts were generated by and for physicians. The materials include personal narratives describing participants’ professional experience, participants’ opinions about pandemic news and events in the professional sphere, and Russian reviews and discussion of papers published in international journals in English. We present the first results and discussion of the sentiment analysis of Russian online medical discourse. Based on the results of sentiment analysis and discourse analysis, we described the emotions expressed in the forum and the linguistic means the forum participants used to verbalise their attitudes and emotions while discussing the Covid-19 pandemic. The results showed prevalence of neutral texts in the publications since the medical professionals are interested in research materials and outcomes. In the discussions and personal narratives, the forum participants expressed negative sentiments by colloquial words and figurative language.</abstract>
      <url hash="62d33347">2020.peoples-1.10</url>
    </paper>
    <paper id="11">
      <title>Multilingual Emoticon Prediction of Tweets about <fixed-case>COVID</fixed-case>-19</title>
      <author><first>Stefanos</first><last>Stoikos</last></author>
      <author><first>Mike</first><last>Izbicki</last></author>
      <pages>109–118</pages>
      <abstract>Emojis are a widely used tool for encoding emotional content in informal messages such as tweets,and predicting which emoji corresponds to a piece of text can be used as a proxy for measuring the emotional content in the text. This paper presents the first model for predicting emojis in highly multilingual text.Our BERTmoticon model is a fine-tuned version of the BERT model,and it can predict emojis for text written in 102 different languages.We trained our BERTmoticon model on 54.2 million geolocated tweets sent in the first 6 months of 2020,and we apply the model to a case study analyzing the emotional reaction of Twitter users to news about the coronavirus. Example findings include a spike in sadness when the World Health Organization (WHO) declared that coronavirus was a global pandemic, and a spike in anger and disgust when the number of COVID-19 related deaths in the United States surpassed one hundred thousand. We provide an easy-to-use and open source python library for predicting emojis with BERTmoticon so that the model can easily be applied to other data mining tasks.</abstract>
      <url hash="e5732990">2020.peoples-1.11</url>
    </paper>
    <paper id="12">
      <title>Experiencers, Stimuli, or Targets: Which Semantic Roles Enable Machine Learning to Infer the Emotions?</title>
      <author><first>Laura Ana Maria</first><last>Oberländer</last></author>
      <author><first>Kevin</first><last>Reich</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>119–128</pages>
      <abstract>Emotion recognition is predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory (e.g., fear, joy, anger, disgust, sadness, surprise, trust, anticipation). More recently, semantic role labeling approaches have been developed to extract structures from the text to answer questions like: “who is described to feel the emotion?” (experiencer), “what causes this emotion?” (stimulus), and at which entity is it directed?” (target). Though it has been shown that jointly modeling stimulus and emotion category prediction is beneficial for both subtasks, it remains unclear which of these semantic roles enables a classifier to infer the emotion. Is it the experiencer, because the identity of a person is biased towards a particular emotion (X is always happy)? Is it a particular target (everybody loves X) or a stimulus (doing X makes everybody sad)? We answer these questions by training emotion classification models on five available datasets annotated with at least one semantic role by masking the fillers of these roles in the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification.</abstract>
      <url hash="51a4fc17">2020.peoples-1.12</url>
    </paper>
    <paper id="13">
      <title>Learning Emotion from 100 Observations: Unexpected Robustness of Deep Learning under Strong Data Limitations</title>
      <author><first>Sven</first><last>Buechel</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <author><first>H. Andrew</first><last>Schwartz</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>129–139</pages>
      <abstract>One of the major downsides of Deep Learning is its supposed need for vast amounts of training data. As such, these techniques appear ill-suited for NLP areas where annotated data is limited, such as less-resourced languages or emotion analysis, with its many nuanced and hard-to-acquire annotation formats. We conduct a questionnaire study indicating that indeed the vast majority of researchers in emotion analysis deems neural models inferior to traditional machine learning when training data is limited. In stark contrast to those survey results, we provide empirical evidence for English, Polish, and Portuguese that commonly used neural architectures can be trained on surprisingly few observations, outperforming n-gram based ridge regression on only 100 data points. Our analysis suggests that high-quality, pre-trained word embeddings are a main factor for achieving those results.</abstract>
      <url hash="46a97324">2020.peoples-1.13</url>
    </paper>
    <paper id="14">
      <title>Cross-lingual Emotion Intensity Prediction</title>
      <author><first>Irean</first><last>Navas Alejo</last></author>
      <author><first>Toni</first><last>Badia</last></author>
      <author><first>Jeremy</first><last>Barnes</last></author>
      <pages>140–152</pages>
      <abstract>Emotion intensity prediction determines the degree or intensity of an emotion that the author expresses in a text, extending previous categorical approaches to emotion detection. While most previous work on this topic has concentrated on English texts, other languages would also benefit from fine-grained emotion classification, preferably without having to recreate the amount of annotated data available in English in each new language. Consequently, we explore cross-lingual transfer approaches for fine-grained emotion detection in Spanish and Catalan tweets. To this end we annotate a test set of Spanish and Catalan tweets using Best-Worst scaling. We compare six cross-lingual approaches, e.g., machine translation and cross-lingual embeddings, which have varying requirements for parallel data – from millions of parallel sentences to completely unsupervised. The results show that on this data, methods with low parallel-data requirements perform surprisingly better than methods that use more parallel data, which we explain through an in-depth error analysis. We make the dataset and the code available at <url>https://github.com/jerbarnes/fine-grained_cross-lingual_emotion</url>.</abstract>
      <url hash="1470e37a">2020.peoples-1.14</url>
    </paper>
    <paper id="15">
      <title>The <fixed-case>L</fixed-case>i<fixed-case>L</fixed-case>a<fixed-case>H</fixed-case> Emotion Lexicon of <fixed-case>C</fixed-case>roatian, <fixed-case>D</fixed-case>utch and <fixed-case>S</fixed-case>lovene</title>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Ilia</first><last>Markov</last></author>
      <author><first>Darja</first><last>Fišer</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>153–157</pages>
      <abstract>In this paper, we present emotion lexicons of Croatian, Dutch and Slovene, based on manually corrected automatic translations of the English NRC Emotion lexicon. We evaluate the impact of the translation changes by measuring the change in supervised classification results of socially unacceptable utterances when lexicon information is used for feature construction. We further showcase the usage of the lexicons by calculating the difference in emotion distributions in texts containing and not containing socially unacceptable discourse, comparing them across four languages (English, Croatian, Dutch, Slovene) and two topics (migrants and LGBT). We show significant and consistent improvements in automatic classification across all languages and topics, as well as consistent (and expected) emotion distributions across all languages and topics, proving for the manually corrected lexicons to be a useful addition to the severely lacking area of emotion lexicons, the crucial resource for emotive analysis of text.</abstract>
      <url hash="c5f84e9c">2020.peoples-1.15</url>
    </paper>
  </volume>
</collection>
