<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.sealp">
  <volume id="1" ingest-date="2025-01-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second Workshop in South East Asian Language Processing</booktitle>
      <editor id="derry-tanti-wijaya"><first>Derry</first><last>Wijaya</last></editor>
      <editor><first>Alham Fikri</first><last>Aji</last></editor>
      <editor><first>Clara</first><last>Vania</last></editor>
      <editor id="genta-indra-winata"><first>Genta Indra</first><last>Winata</last></editor>
      <editor><first>Ayu</first><last>Purwarianti</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>January</month>
      <year>2025</year>
      <url hash="40bc7cc2">2025.sealp-1</url>
      <venue>sealp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="1e2603a0">2025.sealp-1.0</url>
      <bibkey>sealp-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>b<fixed-case>AI</fixed-case>-b<fixed-case>AI</fixed-case>: A Context-Aware Transliteration System for Baybayin Scripts</title>
      <author><first>Jacob Simon D.</first><last>Bernardo</last></author>
      <author><first>Maria Regina Justina E.</first><last>Estuar</last></author>
      <pages>1–9</pages>
      <abstract>Baybayin, a pre-colonial writing system from the Philippines, has seen a resurgence in recent years. Research in computational linguistics has shown an increasing interest in Baybayin OCR, which focuses on the recognition and classification of script characters. However, existing studies face challenges with ambiguous Baybayin words that have multiple possible transliterations. This study introduces a disambiguation technique that employs word embeddings (WE) for contextual analysis and uses part-of-speech (POS) tagging as an initial filtering step. This approach is compared with an LLM method that prompts GPT-4o mini to determine the most appropriate transliteration given a sentence input. The proposed disambiguation process is integrated into existing Baybayin OCR systems to develop bAI-bAI, a context-aware Baybayin transliteration system capable of handling ambiguous words. Results show that incorporating POS as a filter does not significantly affect performance. The WE-Only method yields an accuracy of 77.46% and takes 5.35ms to process one sample while leveraging GPT-4o mini peaks at a higher accuracy of 90.52% but with a much longer runtime of 3280ms per sample. These findings present an opportunity to further explore and improve NLP approaches in disambiguation methods.</abstract>
      <url hash="8a8833f9">2025.sealp-1.1</url>
      <bibkey>bernardo-estuar-2025-bai</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>N</fixed-case>usa<fixed-case>BERT</fixed-case>: Teaching <fixed-case>I</fixed-case>ndo<fixed-case>BERT</fixed-case> to be Multilingual and Multicultural</title>
      <author><first>Wilson</first><last>Wongso</last></author>
      <author><first>David Samuel</first><last>Setiawan</last></author>
      <author><first>Steven</first><last>Limcorn</last></author>
      <author><first>Ananto</first><last>Joyoadikusumo</last></author>
      <pages>10–26</pages>
      <abstract>We present NusaBERT, a multilingual model built on IndoBERT and tailored for Indonesia’s diverse languages. By expanding vocabulary and pre-training on a regional corpus, NusaBERT achieves state-of-the-art performance on Indonesian NLU benchmarks, enhancing IndoBERT’s multilingual capability. This study also addresses NusaBERT’s limitations and encourages further research on Indonesia’s underrepresented languages.</abstract>
      <url hash="f8eb0844">2025.sealp-1.2</url>
      <bibkey>wongso-etal-2025-nusabert</bibkey>
    </paper>
    <paper id="3">
      <title>Evaluating Sampling Strategies for Similarity-Based Short Answer Scoring: a Case Study in <fixed-case>T</fixed-case>hailand</title>
      <author><first>Pachara</first><last>Boonsarngsuk</last></author>
      <author><first>Pacharapon</first><last>Arpanantikul</last></author>
      <author><first>Supakorn</first><last>Hiranwipas</last></author>
      <author><first>Wipu</first><last>Watcharakajorn</last></author>
      <author><first>Ekapol</first><last>Chuangsuwanich</last></author>
      <pages>27–41</pages>
      <abstract>Automatic short answer scoring is a task whose aim is to help grade written works by learners of some subject matter. In niche subject domains with small examples, existing methods primarily utilized similarity-based scoring, relying on predefined reference answers to grade each student’s answer based on the similarity to the reference. However, these reference answers are often generated from a randomly selected set of graded student answer, which may fail to represent the full range of scoring variations. We propose a semi-automatic scoring framework that enhances the selective sampling strategy for defining the reference answers through a K-center-based and a K-means-based sampling method. Our results demonstrate that our framework outperforms previous similarity-based scoring methods on a dataset with Thai and English. Moreover, it achieves competitive performance compared to human reference performance and LLMs.</abstract>
      <url hash="9a8d509f">2025.sealp-1.3</url>
      <bibkey>boonsarngsuk-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>T</fixed-case>hai <fixed-case>W</fixed-case>inograd Schemas: A Benchmark for <fixed-case>T</fixed-case>hai Commonsense Reasoning</title>
      <author><first>Phakphum</first><last>Artkaew</last></author>
      <pages>42–51</pages>
      <abstract>Commonsense reasoning is one of the important aspects of natural language understanding, with several benchmarks developed to evaluate it. However, only a few of these benchmarks are available in languages other than English. Developing parallel benchmarks facilitates cross-lingual evaluation, enabling a better understanding of different languages. This research introduces a collection of Winograd Schemas in Thai, a novel dataset designed to evaluate commonsense reasoning capabilities in the context of the Thai language. Through a methodology involving native speakers, professional translators, and thorough validation, the schemas aim to closely reflect Thai language nuances, idioms, and cultural references while maintaining ambiguity and commonsense challenges. We evaluate the performance of popular large language models on this benchmark, revealing their strengths, limitations, and providing insights into the current state-of-the-art. Results indicate that while models like GPT-4 and Claude-3-Opus achieve high accuracy in English, their performance significantly drops in Thai, highlighting the need for further advancements in multilingual commonsense reasoning.</abstract>
      <url hash="10fcb870">2025.sealp-1.4</url>
      <bibkey>artkaew-2025-thai</bibkey>
    </paper>
    <paper id="5">
      <title>Anak Baik: A Low-Cost Approach to Curate <fixed-case>I</fixed-case>ndonesian Ethical and Unethical Instructions</title>
      <author><first>Sulthan Abiyyu</first><last>Hakim</last></author>
      <author><first>Rizal Setya</first><last>Perdana</last></author>
      <author><first>Tirana Noor</first><last>Fatyanosa</last></author>
      <pages>52–62</pages>
      <abstract>This study explores the ethical challenges faced by Indonesian Large Language Models (LLMs), particularly focusing on their ability to distinguish between ethical and unethical instructions. As LLMs become increasingly integrated into sensitive applications, ensuring their ethical operation is crucial. A key contribution of this study is the introduction of the Anak Baik dataset, a resource designed to enhance the ethical reasoning capabilities of Indonesian LLMs. The phrase “Anak Baik”, meaning “Good Boy”, symbolizes the ideal of ethical behavior, as a well-behaved child refrains from engaging in harmful actions. The dataset comprises instruction-response pairs in Indonesian, crafted for Supervised Fine-Tuning (SFT) tasks. It includes examples of both ethical and unethical responses to guide models in learning to generate responses that uphold moral standards. Leveraging Low-Rank Adaptation (LoRA) on models such as Komodo and Cendol shows a significant improvement in ethical decision-making processes. This enhanced performance is quantitatively validated through substantial increases in BLEU and ROUGE scores, indicating a stronger alignment with socially responsible behavior.</abstract>
      <url hash="d73b1ab5">2025.sealp-1.5</url>
      <bibkey>hakim-etal-2025-anak</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>I</fixed-case>ndonesian Speech Content De-Identification in Low Resource Transcripts</title>
      <author><first>Rifqi Naufal</first><last>Abdjul</last></author>
      <author><first>Dessi</first><last>Puji Lestari</last></author>
      <author><first>Ayu</first><last>Purwarianti</last></author>
      <author><first>Candy Olivia</first><last>Mawalim</last></author>
      <author><first>Sakriani</first><last>Sakti</last></author>
      <author><first>Masashi</first><last>Unoki</last></author>
      <pages>63–71</pages>
      <abstract>Advancements in technology and the increased use of digital data threaten individual privacy, especially in speech containing Personally Identifiable Information (PII). Therefore, systems that can remove or process privacy-sensitive data in speech are needed, particularly for low-resource transcripts. These transcripts are minimally annotated or labeled automatically, which is less precise than human annotation. However, using them can simplify the development of de-identification systems in any language. In this study, we develop and evaluate an efficient speech de-identification system. We create an Indonesian speech dataset containing sensitive private information and design a system with three main components: speech recognition, information extraction, and masking. To enhance performance in low-resource settings, we incorporate transcription data in training, use data augmentation, and apply weakly supervised learning. Our results show that our techniques significantly improve privacy detection performance, with approximately 29% increase in F1 score, 20% in precision, and 30% in recall with minimally labeled data.</abstract>
      <url hash="a541c544">2025.sealp-1.6</url>
      <bibkey>abdjul-etal-2025-indonesian</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>I</fixed-case>ndo<fixed-case>M</fixed-case>orph: a Morphology Engine for <fixed-case>I</fixed-case>ndonesian</title>
      <author><first>Ian</first><last>Kamajaya</last></author>
      <author><first>David</first><last>Moeljadi</last></author>
      <pages>72–81</pages>
      <abstract>Indonesian is an agglutinative language and rich in morphology. Although it has more than 250 million speakers, it is a low resource language in NLP field. Many Indonesian NLP resources are scattered, undocumented, and not publicly available. In this paper we address the issue of analyzing morphology as well as generating Indonesian words. We introduce IndoMorph, a morphology analyzer and word generator for Indonesian. In an agglutinative language, morphology deconstruction can be crucial to understand the structure and meaning of words. IndoMorph can be useful for language modeling and testing certain analyses. In addition, it can be employed to make a new Indonesian subword representation resource such as Indonesian morphology dictionary (IMD), used as a language education tool, or embedded in various applications such as text analysis applications. We hope that IndoMorph can be employed not only in the Indonesian NLP research development, but also in the NLP research of any agglutinative languages.</abstract>
      <url hash="ccdb09a4">2025.sealp-1.7</url>
      <bibkey>kamajaya-moeljadi-2025-indomorph</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>N</fixed-case>usa<fixed-case>D</fixed-case>ialogue: Dialogue Summarization and Generation for Underrepresented and Extremely Low-Resource Languages</title>
      <author><first>Ayu</first><last>Purwarianti</last></author>
      <author><first>Dea</first><last>Adhista</last></author>
      <author><first>Agung</first><last>Baptiso</last></author>
      <author><first>Miftahul</first><last>Mahfuzh</last></author>
      <author><first>Yusrina</first><last>Sabila</last></author>
      <author><first>Aulia</first><last>Adila</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <pages>82–100</pages>
      <abstract>Developing dialogue summarization for extremely low-resource languages is a challenging task. We introduce NusaDialogue, a dialogue summarization dataset for three underrepresented languages in the Malayo-Polynesian language family: Minangkabau, Balinese, and Buginese. NusaDialogue covers 17 topics and 185 subtopics, with annotations provided by 73 native speakers. Additionally, we conducted experiments using fine-tuning on a specifically designed medium-sized language model for Indonesian, as well as zero- and few-shot learning on various multilingual large language models (LLMs). The results indicate that, for extremely low-resource languages such as Minangkabau, Balinese, and Buginese, the fine-tuning approach yields significantly higher performance compared to zero- and few-shot prompting, even when applied to LLMs with considerably larger parameter sizes.</abstract>
      <url hash="7ceddae7">2025.sealp-1.8</url>
      <bibkey>purwarianti-etal-2025-nusadialogue</bibkey>
    </paper>
  </volume>
</collection>
