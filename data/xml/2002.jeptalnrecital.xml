<?xml version='1.0' encoding='UTF-8'?>
<collection id="2002.jeptalnrecital">
  <volume id="invite" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 9ème conférence sur le Traitement Automatique des Langues Naturelles. Conférences invitées</booktitle>
      <editor><first>Jean-Marie</first><last>Pierrel</last></editor>
      <publisher>ATALA</publisher>
      <address>Nancy, France</address>
      <month>June</month>
      <year>2002</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="6a4f8e29">2002.jeptalnrecital-invite.0</url>
      <bibkey>jep-taln-recital-2002-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Maurice Gross : une refondation de la linguistique au crible de l’analyse automatique</title>
      <author><first>Amr</first><last>Helmy Ibrahim</last></author>
      <pages>5–30</pages>
      <abstract>Qu’il s’adresse à un Prix Nobel ou à un étudiant de première année Maurice Gross ne craignait jamais d’être trop élémentaire. C’était à chaque fois comme si, entreprenant d’écrire un livre de mathématiques il ne pouvait rien démontrer avant d’avoir reconstruit les données les plus primitives du calcul et du raisonnement qui l’accompagne. Et il arrivait souvent que ceux qui l’écoutaient ou le lisaient pour la première fois, manquant par leur impatience le détail qui faisait que ses évidences n’avaient rien d’évident, s’imaginent qu’il les prenait pour des imbéciles. Parce qu’il avait l’expression littéraire et philosophique, la langue du style, la forme de l’émotion, dans les tripes – il pouvait citer sans discontinuer des poètes français ou anglais du XVIe siècle à nos jours et discuter longuement des formulations exactes d’un René Descartes ou d’un Charles Sanders Pierce, deux de ses deux philosophes préférés - il n’a jamais fait recette auprès des littéraires, des psycho-socios, des sémio-machins, des politiques et des pouvoirs académiques chez qui le raccourci, la connotation, le clin d’oeil, dont tout le monde a oublié sur quelles complicités exactes ils se fondent, tiennent lieu de découverte quand ce n’est pas de pensée. La complexité qui l’intéressait était d’une tout autre nature et autrement plus complexe. Elle avait pour horizon la phrase simple. Même pas l’énoncé, juste la phrase. Et simple c’est-à-dire constituée d’une seule proposition. Contrairement à ceux qui voyaient dans les processus de récursivité propositionnelle – relatives notamment – une source de complexité et de créativité, il y voyait un mécanisme très banal1. La vraie complexité, celle qu’aucune machine construite à ce jour ne contrôle vraiment, il l’a exposée avec une simplicité désarmante en un peu moins de deux pages au début de Méthodes en syntaxe (1975: 17-19) dans le chapitre intitulé La créativité du langage. Elle porte sur les combinaisons possibles ou impossibles au sein d’une structure de neuf constituants formant une phrase simple. Mais ces possibilités “limitées à 1050 cas” et qui peuvent donc “être considérées comme intuitivement infinies” sans qu’il soit nécessaire “de faire appel à des mécanismes infinis pour rendre compte de leur richesse” ne sont qu’un horizon virtuel.</abstract>
      <url hash="68adfe3d">2002.jeptalnrecital-invite.1</url>
      <language>fra</language>
      <bibkey>helmy-ibrahim-2002-maurice</bibkey>
    </paper>
  </volume>
  <volume id="long" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 9ème conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Jean-Marie</first><last>Pierrel</last></editor>
      <publisher>ATALA</publisher>
      <address>Nancy, France</address>
      <month>June</month>
      <year>2002</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="fd7a1b0f">2002.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2002-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Analyse Factorielle Neuronale pour Documents Textuels</title>
      <author><first>Mathieu</first><last>Delichère</last></author>
      <author><first>Daniel</first><last>Memmi</last></author>
      <pages>33–42</pages>
      <abstract>En recherche documentaire, on représente souvent les documents textuels par des vecteurs lexicaux de grande dimension qui sont redondants et coûteux. Il est utile de réduire la dimension des ces représentations pour des raisons à la fois techniques et sémantiques. Cependant les techniques classiques d’analyse factorielle comme l’ACP ne permettent pas de traiter des vecteurs de très grande dimension. Nous avons alors utilisé une méthode adaptative neuronale (GHA) qui s’est révélée efficace pour calculer un nombre réduit de nouvelles dimensions représentatives des données. L’approche nous a permis de classer un corpus réel de pages Web avec de bons résultats.</abstract>
      <url hash="3ebfb809">2002.jeptalnrecital-long.1</url>
      <language>fra</language>
      <bibkey>delichere-memmi-2002-analyse</bibkey>
    </paper>
    <paper id="2">
      <title>Ressources terminologiques et traduction probabiliste: premiers pas positifs vers un système adaptatif</title>
      <author><first>Philippe</first><last>Langlais</last></author>
      <pages>43–52</pages>
      <abstract>Cette dernière décennie a été le témoin d’importantes avancées dans le domaine de la traduction statistique (TS). Aucune évaluation fine n’a cependant été proposée pour mesurer l’adéquation de l’approche statistique dans un contexte applicatif réel. Dans cette étude, nous étudions le comportement d’un engin de traduction probabiliste lorsqu’il traduit un texte de nature très éloignée de celle du corpus utilisé lors de l’entraînement. Nous quantifions en particulier la baisse de performance du système et développons l’idée que l’intégration de ressources terminologiques dans le processus est une solution naturelle et salutaire à la traduction. Nous décrivons cette intégration et évaluons son potentiel.</abstract>
      <url hash="c39b9656">2002.jeptalnrecital-long.2</url>
      <language>fra</language>
      <bibkey>langlais-2002-ressources</bibkey>
    </paper>
    <paper id="3">
      <title>Accentuation de mots inconnus : application au thesaurus biomédical <fixed-case>M</fixed-case>e<fixed-case>SH</fixed-case></title>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <pages>53–62</pages>
      <abstract>Certaines ressources textuelles ou terminologiques sont écrites sans signes diacritiques, ce qui freine leur utilisation pour le traitement automatique des langues. Dans un domaine spécialisé comme la médecine, il est fréquent que les mots rencontrés ne se trouvent pas dans les lexiques électroniques disponibles. Se pose alors la question de l’accentuation de mots inconnus : c’est le sujet de ce travail. Nous proposons deux méthodes d’accentuation de mots inconnus fondées sur un apprentissage par observation des contextes d’occurrence des lettres à accentuer dans un ensemble de mots d’entraînement, l’une adaptée de l’étiquetage morphosyntaxique, l’autre adaptée d’une méthode d’apprentissage de règles morphologiques. Nous présentons des résultats expérimentaux pour la lettre e sur un thesaurus biomédical en français : le MeSH. Ces méthodes obtiennent une précision de 86 à 96 % (+-4 %) pour un rappel allant de 72 à 86 %.</abstract>
      <url hash="cf41668a">2002.jeptalnrecital-long.3</url>
      <language>fra</language>
      <bibkey>zweigenbaum-grabar-2002-accentuation</bibkey>
    </paper>
    <paper id="4">
      <title>Une méthode pour l’analyse descendante et calculatoire de corpus multilingues : application au calcul des relations sujet-verbe</title>
      <author><first>Jacques</first><last>Vergne</last></author>
      <pages>63–74</pages>
      <abstract>Nous présentons une méthode d’analyse descendante et calculatoire. La démarche d’analyse est descendante du document à la proposition, en passant par la phrase. Le prototype présenté prend en entrée des documents en anglais, français, italien, espagnol, ou allemand. Il segmente les phrases en propositions, et calcule les relations sujet-verbe dans les propositions. Il est calculatoire, car il exécute un petit nombre d’opérations sur les données. Il utilise très peu de ressources (environ 200 mots et locutions par langue), et le traitement de la phrase fait environ 60 Ko de Perl, ressources lexicales comprises. La méthode présentée se situe dans le cadre d’une recherche plus générale du Groupe Syntaxe et Ingénierie Multilingue du GREYC sur l’exploration de solutions minimales et multilingues, ajustées à une tâche donnée, exploitant peu de propriétés linguistiques profondes, la généricité allant de pair avec l’efficacité.</abstract>
      <url hash="3397acc5">2002.jeptalnrecital-long.4</url>
      <language>fra</language>
      <bibkey>vergne-2002-une</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>UPERY</fixed-case> : un outil d’analyse distributionnelle étendue pour la construction d’ontologies à partir de corpus</title>
      <author><first>Didier</first><last>Bourigault</last></author>
      <pages>75–84</pages>
      <abstract>Nous présentons un module mettant en oeuvre une méthode d’analyse distributionnelle dite “étendue”. L’analyseur syntaxique de corpus SYNTEX effectue l’analyse en dépendance de chacune des phrases du corpus, puis construit un réseau de mots et syntagmes, dans lequel chaque syntagme est relié à sa tête et à ses expansions. A partir de ce réseau, le module d’analyse distributionnelle UPERY construit pour chaque terme du réseau l’ensemble de ses contextes syntaxiques. Les termes et les contextes syntaxiques peuvent être simples ou complexes. Le module rapproche ensuite les termes, ainsi que les contextes syntaxiques, sur la base de mesures de proximité distributionnelle. L’ensemble de ces résultats est utilisé comme aide à la construction d’ontologie à partir de corpus spécialisés.</abstract>
      <url hash="45ae303e">2002.jeptalnrecital-long.5</url>
      <language>fra</language>
      <bibkey>bourigault-2002-upery</bibkey>
    </paper>
    <paper id="6">
      <title>Construire des analyseurs avec <fixed-case>D</fixed-case>y<fixed-case>AL</fixed-case>og</title>
      <author><first>Éric</first><last>Villemonte De La Clergerie</last></author>
      <pages>85–94</pages>
      <abstract>Cet article survole les fonctionnalités offertes par le système DyALog pour construire des analyseurs syntaxiques tabulaires. Offrant la richesse d’un environnement de programmation en logique, DyALog facilite l’écriture de grammaires, couvre plusieurs formalismes et permet le paramétrage de stratégies d’analyse.</abstract>
      <url hash="8343f69b">2002.jeptalnrecital-long.6</url>
      <language>fra</language>
      <bibkey>villemonte-de-la-clergerie-2002-construire</bibkey>
    </paper>
    <paper id="7">
      <title>Une grammaire hors-contexte valuée pour l’analyse syntaxique</title>
      <author><first>Antoine</first><last>Rozenknop</last></author>
      <pages>95–104</pages>
      <abstract>Les grammaires hors-contexte stochastiques sont exploitées par des algorithmes particulièrement efficaces dans des tâches de reconnaissance de la parole et d’analyse syntaxique. Cet article propose une autre probabilisation de ces grammaires, dont les propriétés mathématiques semblent intuitivement plus adaptées à ces tâches que celles des SCFG (Stochastique CFG), sans nécessiter d’algorithme d’analyse spécifique. L’utilisation de ce modèle en analyse sur du texte provenant du corpus Susanne peut réduire de 33% le nombre d’analyses erronées, en comparaison avec une SCFG entraînée dans les mêmes conditions.</abstract>
      <url hash="81ce8402">2002.jeptalnrecital-long.7</url>
      <language>fra</language>
      <bibkey>rozenknop-2002-une</bibkey>
    </paper>
    <paper id="8">
      <title>Extraction d’informations à partir de corpus dégradés</title>
      <author><first>Fabrice</first><last>Even</last></author>
      <author><first>Chantal</first><last>Enguehard</last></author>
      <pages>105–115</pages>
      <abstract>Nous présentons une méthode automatique d’extraction d’information à partir d’un corpus mono-domaine de mauvaise qualité, sur lequel il est impossible d’appliquer les méthodes classiques de traitement de la langue naturelle. Cette approche se fonde sur la construction d’une ontologie semi-formelle (modélisant les informations contenues dans le corpus et les relations entre elles). Notre méthode se déroule en trois phases : 1) la normalisation du corpus, 2) la construction de l’ontologie, et 3) sa formalisation sous la forme d’une grammaire. L’extraction d’information à proprement parler exploite un étiquetage utilisant les règles définies par la grammaire. Nous illustrons notre démarche d’une application sur un corpus bancaire.</abstract>
      <url hash="ae2db300">2002.jeptalnrecital-long.8</url>
      <language>fra</language>
      <bibkey>even-enguehard-2002-extraction</bibkey>
    </paper>
    <paper id="9">
      <title>Identification thématique hiérarchique : Application aux forums de discussions</title>
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Kamel</first><last>Smaïli</last></author>
      <pages>116–125</pages>
      <abstract>Les modèles statistiques du langage ont pour but de donner une représentation statistique de la langue mais souffrent de nombreuses imperfections. Des travaux récents ont montré que ces modèles peuvent être améliorés s’ils peuvent bénéficier de la connaissance du thème traité, afin de s’y adapter. Le thème du document est alors obtenu par un mécanisme d’identification thématique, mais les thèmes ainsi traités sont souvent de granularité différente, c’est pourquoi il nous semble opportun qu’ils soient organisés dans une hiérarchie. Cette structuration des thèmes implique la mise en place de techniques spécifiques d’identification thématique. Cet article propose un modèle statistique à base d’unigrammes pour identifier automatiquement le thème d’un document parmi une arborescence prédéfinie de thèmes possibles. Nous présentons également un critère qui permet au modèle de donner un degré de fiabilité à la décision prise. L’ensemble des expérimentations a été réalisé sur des données extraites du groupe ’fr’ des forums de discussion.</abstract>
      <url hash="2fd5e9f2">2002.jeptalnrecital-long.9</url>
      <language>fra</language>
      <bibkey>bigi-smaili-2002-identification</bibkey>
    </paper>
    <paper id="10">
      <title>Vers l’apprentissage automatique, pour et par les vecteurs conceptuels, de fonctions lexicales. L’exemple de l’antonymie</title>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Violaine</first><last>Prince</last></author>
      <pages>126–135</pages>
      <abstract>Dans le cadre de recherches sur le sens en traitement automatique du langage, nous nous concentrons sur la représentation de l’aspect thématique des segments textuels à l’aide de vecteurs conceptuels. Les vecteurs conceptuels sont automatiquement appris à partir de définitions issues de dictionnaires à usage humain (Schwab, 2001). Un noyau de termes manuellement indexés est nécessaire pour l’amorçage de cette analyse. Lorsque l’item défini s’y prête, ces définitions sont complétées par des termes en relation avec lui. Ces relations sont des fonctions lexicales (Mel’cuk and al, 95) comme l’hyponymie, l’hyperonymie, la synonymie ou l’antonymie. Cet article propose d’améliorer la fonction d’antonymie naïve exposée dans (Schwab, 2001) et (Schwab and al, 2002) grâce à ces informations. La fonction s’auto-modifie, par révision de listes, en fonction des relations d’antonymie avérées entre deux items. Nous exposons la méthode utilisée, quelques résultats puis nous concluons sur les perspectives ouvertes.</abstract>
      <url hash="9fe3f6fc">2002.jeptalnrecital-long.10</url>
      <language>fra</language>
      <bibkey>schwab-etal-2002-vers</bibkey>
    </paper>
    <paper id="11">
      <title>Filtrages syntaxiques de co-occurrences pour la représentation vectorielle de documents</title>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Martin</first><last>Rajman</last></author>
      <pages>136–145</pages>
      <abstract>L’intégration de co-occurrences dans les modèles de représentation vectorielle de documents s’est avérée une source d’amélioration de la pertinence des mesures de similarités textuelles calculées dans le cadre de ces modèles (Rajman et al., 2000; Besançon, 2001). Dans cette optique, la définition des contextes pris en compte pour les co-occurrences est cruciale, par son influence sur les performances des modèles à base de co-occurrences. Dans cet article, nous proposons d’étudier deux méthodes de filtrage des co-occurrences fondées sur l’utilisation d’informations syntaxiques supplémentaires. Nous présentons également une évaluation de ces méthodes dans le cadre de la tâche de la recherche documentaire.</abstract>
      <url hash="1bbecc34">2002.jeptalnrecital-long.11</url>
      <language>fra</language>
      <bibkey>besancon-rajman-2002-filtrages</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>WSIM</fixed-case> : une méthode de détection de thème fondée sur la similarité entre mots</title>
      <author><first>Armelle</first><last>Brun</last></author>
      <author><first>Kamel</first><last>Smaïli</last></author>
      <author><first>Jean-Paul</first><last>Haton</last></author>
      <pages>146–155</pages>
      <abstract>L’adaptation des modèles de langage dans les systèmes de reconnaissance de la parole est un des enjeux importants de ces dernières années. Elle permet de poursuivre la reconnaissance en utilisant le modèle de langage adéquat : celui correspondant au thème identifié. Dans cet article nous proposons une méthode originale de détection de thème fondée sur des vocabulaires caractéristiques de thèmes et sur la similarité entre mots et thèmes. Cette méthode dépasse la méthode classique (TFIDF) de 14%, ce qui représente un gain important en terme d’identification. Nous montrons également l’intérêt de choisir un vocabulaire adéquat. Notre méthode de détermination des vocabulaires atteint des performances 3 fois supérieures à celles obtenues avec des vocabulaires construits sur la fréquence des mots.</abstract>
      <url hash="9fe9bf38">2002.jeptalnrecital-long.12</url>
      <language>fra</language>
      <bibkey>brun-etal-2002-wsim</bibkey>
    </paper>
    <paper id="13">
      <title>Segmenter et structurer thématiquement des textes par l’utilisation conjointe de collocations et de la récurrence lexicale</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>156–166</pages>
      <abstract>Nous exposons dans cet article une méthode réalisant de façon intégrée deux tâches de l’analyse thématique : la segmentation et la détection de liens thématiques. Cette méthode exploite conjointement la récurrence des mots dans les textes et les liens issus d’un réseau de collocations afin de compenser les faiblesses respectives des deux approches. Nous présentons son évaluation concernant la segmentation sur un corpus en français et un corpus en anglais et nous proposons une mesure d’évaluation spécifiquement adaptée à ce type de systèmes.</abstract>
      <url hash="c8704d72">2002.jeptalnrecital-long.13</url>
      <language>fra</language>
      <bibkey>ferret-2002-segmenter</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>LOGUS</fixed-case> : un système formel de compréhension du français parlé spontané-présentation et évaluation</title>
      <author><first>Jeanne</first><last>Villaneau</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Olivier</first><last>Ridoux</last></author>
      <pages>167–176</pages>
      <abstract>Le système de compréhension présenté dans cet article propose une approche logique et lexicalisée associant syntaxe et sémantique pour une analyse non sélective et hors-cadres sémantiques prédéterminés. L’analyse se déroule suivant deux grandes étapes ; un chunking est suivi d’une mise en relation des chunks qui aboutit à la construction de la représentation sémantique finale : formule logique ou graphe conceptuel. Nous montrons comment le formalisme a dû évoluer pour accroître l’importance de la syntaxe et améliorer la généricité des règles. Malgré l’utilisation d’une connaissance pragmatico-sémantique liée à l’application, la spécificité du système est circonscrite au choix des mots du lexique et à la définition de cette connaissance. Les résultats d’une campagne d’évaluation ont mis en évidence une bonne tolérance aux inattendus et aux phénomènes complexes, prouvant ainsi la validité de l’approche.</abstract>
      <url hash="6930dc4b">2002.jeptalnrecital-long.14</url>
      <language>fra</language>
      <bibkey>villaneau-etal-2002-logus</bibkey>
    </paper>
    <paper id="15">
      <title>Etude des relations entre pauses et ponctuations pour la synthèse de la parole à partir de texte</title>
      <author><first>Estelle</first><last>Campione</last></author>
      <author><first>Jean</first><last>Véronis</last></author>
      <pages>177–186</pages>
      <abstract>Nous présentons dans cette communication la première étude à grande échelle de la relation entre pauses et ponctuations, à l’aide de l’analyse de plusieurs milliers de pauses dans un corpus comportant près de 5 heures de parole lue en cinq langues, faisant intervenir 50 locuteurs des deux sexes. Nos résultats remettent en cause l’idée reçue de rapports bi-univoques entre pauses et ponctuations. Nous mettons en évidence une proportion importante de pauses hors ponctuation, qui délimitent des constituants, mais aussi un pourcentage élevé de ponctuations faibles réalisées sans pauses. Nous notons également une très grande variabilité inter-locuteur, ainsi que des différences importantes entre langues. Enfin, nous montrons que la durée des pauses est liée au sexe des locuteurs.</abstract>
      <url hash="540eea23">2002.jeptalnrecital-long.15</url>
      <language>fra</language>
      <bibkey>campione-veronis-2002-etude</bibkey>
    </paper>
    <paper id="16">
      <title>Génération automatique d’exercices contextuels de vocabulaire</title>
      <author><first>Thierry</first><last>Selva</last></author>
      <pages>187–196</pages>
      <abstract>Cet article explore l’utilisation de ressources lexicales et textuelles ainsi que d’outils issus du TAL dans le domaine de l’apprentissage des langues assisté par ordinateur (ALAO). Il aborde le problème de la génération automatique ou semi-automatique d’exercices contextuels de vocabulaire à partir d’un corpus de textes et de données lexicales au moyen d’un étiqueteur et d’un parseur. Sont étudiées les caractéristiques et les limites de ces exercices.</abstract>
      <url hash="24b0d85a">2002.jeptalnrecital-long.16</url>
      <language>fra</language>
      <bibkey>selva-2002-generation</bibkey>
    </paper>
    <paper id="17">
      <title>Les analyseurs syntaxiques : atouts pour une analyse des questions dans un système de question-réponse ?</title>
      <author><first>Laura</first><last>Monceaux</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <pages>197–206</pages>
      <abstract>Cet article montre que pour une application telle qu’un système de question – réponse, une analyse par mots clés de la question est insuffisante et qu’une analyse plus détaillée passant par une analyse syntaxique permet de fournir des caractéristiques permettant une meilleure recherche de la réponse.</abstract>
      <url hash="b67c8d4a">2002.jeptalnrecital-long.17</url>
      <language>fra</language>
      <bibkey>monceaux-robba-2002-les</bibkey>
    </paper>
    <paper id="18">
      <title>Variabilité et dépendances des composants linguistiques</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <author><first>Albert</first><last>Di Cristo</last></author>
      <pages>207–216</pages>
      <abstract>Nous présentons dans cet article un cadre d’explication des relations entre les différents composants de l’analyse linguistique (prosodie, syntaxe, sémantique, etc.). Nous proposons un principe spécifiant un équilibre pour un objet linguistique donné entre ces différents composants sous la forme d’un poids (précisant l’aspect marqué de l’objet décrit) défini pour chacun d’entre eux et d’un seuil (correspondant à la somme de ces poids) à atteindre. Une telle approche permet d’expliquer certains phénomènes de variabilité : le choix d’une “tournure” à l’intérieur d’un des composants peut varier à condition que son poids n’empêche pas d’atteindre le seuil spécifié. Ce type d’information, outre son intérêt purement linguistique, constitue le premier élément de réponse pour l’introduction de la variabilité dans des applications comme les systèmes de génération ou de synthèse de la parole.</abstract>
      <url hash="9e33350d">2002.jeptalnrecital-long.18</url>
      <language>fra</language>
      <bibkey>blache-di-cristo-2002-variabilite</bibkey>
    </paper>
    <paper id="19">
      <title>Groupes prépositionnels arguments ou circonstants : vers un repérage automatique en corpus</title>
      <author><first>Cécile</first><last>Fabre</last></author>
      <author><first>Cécile</first><last>Frérot</last></author>
      <pages>217–226</pages>
      <abstract>Dans cette étude, menée dans le cadre de la réalisation d’un analyseur syntaxique de corpus spécialisés, nous nous intéressons à la question des arguments et circonstants et à leur repérage automatique en corpus. Nous proposons une mesure simple pour distinguer automatiquement, au sein des groupes prépositionnels rattachés au verbe, des types de compléments différents. Nous réalisons cette distinction sur corpus, en mettant en oeuvre une stratégie endogène, et en utilisant deux mesures de productivité : la productivité du recteur verbal vis à vis de la préposition évalue le degré de cohésion entre le verbe et son groupe prépositionnel (GP), tandis que la productivité du régi vis à vis de la préposition permet d’évaluer le degré de cohésion interne du GP. Cet article présente ces deux mesures, commente les données obtenues, et détermine dans quelle mesure cette partition recouvre la distinction traditionnelle entre arguments et circonstants.</abstract>
      <url hash="bc6b5de2">2002.jeptalnrecital-long.19</url>
      <language>fra</language>
      <bibkey>fabre-frerot-2002-groupes</bibkey>
    </paper>
    <paper id="20">
      <title>Évaluation des taux de synonymie et de polysémie dans un texte</title>
      <author><first>Claude</first><last>De Loupy</last></author>
      <pages>227–236</pages>
      <abstract>La polysémie et la synonymie sont deux aspects fondamentaux de la langue. Nous présentons ici une évaluation de l’importance de ces deux phénomènes à l’aide de statistiques basées sur le lexique WordNet et sur le SemCor. Ainsi, on a un taux de polysémie théorique de 5 sens par mot dans le SemCor. Mais si on regarde les occurrences réelles, moins de 50 % des sens possibles sont utilisés. De même, s’il y a, en moyenne, 2,7 mots possibles pour désigner un concept qui apparaît dans le corpus, plus de la moitié d’entre eux ne sont jamais utilisés. Ces résultats relativisent l’utilité de telles ressources sémantiques pour le traitement de la langue.</abstract>
      <url hash="570f3eec">2002.jeptalnrecital-long.20</url>
      <language>fra</language>
      <bibkey>de-loupy-2002-evaluation</bibkey>
    </paper>
    <paper id="21">
      <title>Acquisition automatique de sens à partir d’opérations morphologiques en français : études de cas</title>
      <author><first>Fiammetta</first><last>Namer</last></author>
      <pages>237–246</pages>
      <abstract>Cet article propose une méthode de codage automatique de traits lexicaux sémantiques en français. Cette approche exploite les relations fixées par l’instruction sémantique d’un opérateur de construction morphologique entre la base et le mot construit. En cela, la réflexion s’inspire des travaux de Marc Light (Light 1996) tout en exploitant le fonctionnement d’un système d’analyse morphologique existant : l’analyseur DériF. A ce jour, l’analyse de 12 types morphologiques conduit à l’étiquetage d’environ 10 % d’un lexique composé de 99000 lemmes. L’article s’achève par la description de deux techniques utilisées pour valider les traits sémantiques.</abstract>
      <url hash="fd3e3584">2002.jeptalnrecital-long.21</url>
      <language>fra</language>
      <bibkey>namer-2002-acquisition</bibkey>
    </paper>
    <paper id="22">
      <title>Webaffix : un outil d’acquisition morphologique dérivationnelle à partir du Web</title>
      <author><first>Ludovic</first><last>Tanguy</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <pages>247–256</pages>
      <abstract>L’article présente Webaffix, un outil d’acquisition de couples de lexèmes morphologiquement apparentés à partir du Web. La méthode utilisé est inductive et indépendante des langues particulières. Webaffix (1) utilise un moteur de recherche pour collecter des formes candidates qui contiennent un suffixe graphémique donné, (2) prédit les bases potentielles de ces candidats et (3) recherche sur le Web des cooccurrences des candidats et de leurs bases prédites. L’outil a été utilisé pour enrichir Verbaction, un lexique de liens entre verbes et noms d’action ou d’événement correspondants. L’article inclut une évaluation des liens morphologiques acquis.</abstract>
      <url hash="1bfb8a2d">2002.jeptalnrecital-long.22</url>
      <language>fra</language>
      <bibkey>tanguy-hathout-2002-webaffix</bibkey>
    </paper>
    <paper id="23">
      <title>Évaluer l’acquisition semi-automatique de classes sémantiques</title>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <author><first>Dominique</first><last>Dutoit</last></author>
      <author><first>Sophie</first><last>Bizouard</last></author>
      <pages>257–266</pages>
      <abstract>Cet article vise à évaluer deux approches différentes pour la constitution de classes sémantiques. Une approche endogène (acquisition à partir d’un corpus) est contrastée avec une approche exogène (à travers un réseau sémantique riche). L’article présente une évaluation fine de ces deux techniques.</abstract>
      <url hash="cb9725cf">2002.jeptalnrecital-long.23</url>
      <language>fra</language>
      <bibkey>poibeau-etal-2002-evaluer</bibkey>
    </paper>
    <paper id="24">
      <title>Nemesis, un système de reconnaissance incrémentielle des entités nommées pour le français</title>
      <author><first>Nordine</first><last>Fourour</last></author>
      <pages>267–276</pages>
      <abstract>Cet article présente une étude des conflits engendrés par la reconnaissance des entités nommées (EN) pour le français, ainsi que quelques indices pour les résoudre. Cette reconnaissance est réalisée par le système Nemesis, dont les spécifications ont été élaborées conséquemment à une étude en corpus. Nemesis se base sur des règles de grammaire, exploite des lexiques spécialisés et comporte un module d’apprentissage. Les performances atteintes par Nemesis, sur les anthroponymes et les toponymes, sont de 90% pour le rappel et 95% pour la précision.</abstract>
      <url hash="58c4be4e">2002.jeptalnrecital-long.24</url>
      <language>fra</language>
      <bibkey>fourour-2002-nemesis</bibkey>
    </paper>
    <paper id="25">
      <title>La coédition langue↔<fixed-case>UNL</fixed-case> pour partager la révision entre les langues d’un document multilingue : un concept unificateur</title>
      <author><first>Christian</first><last>Boitet</last></author>
      <author><first>Wang-Ju</first><last>Tsai</last></author>
      <pages>277–288</pages>
      <abstract>La coédition d’un texte en langue naturelle et de sa représentation dans une forme interlingue semble le moyen le meilleur et le plus simple de partager la révision du texte vers plusieurs langues. Pour diverses raisons, les graphes UNL sont les meilleurs candidats dans ce contexte. Nous développons un prototype où, dans le scénario avec partage le plus simple, des utilisateurs “naïfs” interagissent directement avec le texte dans leur langue (L0), et indirectement avec le graphe associé pour corriger les erreurs. Le graphe modifié est ensuite envoyé au déconvertisseur UNL-L0 et le résultat est affiché. S’il est satisfaisant, les erreurs étaient probablement dues au graphe et non au déconvertisseur, et le graphe est envoyé aux déconvertisseurs vers d’autres langues. Les versions dans certaines autres langues connues de l’utilisateur peuvent être affichées, de sorte que le partage de l’amélioration soit visible et encourageant. Comme les nouvelles versions sont ajoutées dans le document multilingue original avec des balises et des attributs appropriés, rien n’est jamais perdu, et le travail coopératif sur un même document est rendu possible. Du côté interne, des liaisons sont établies entre des éléments du texte et du graphe en utilisant des ressources largement disponibles comme un dictionnaire L0-anglais, ou mieux L0-UNL, un analyseur morphosyntaxique de L0, et une transformation canonique de graphe UNL à arbre. On peut établir une “meilleure” correspondance entre “l’arbre-UNL+L0” et la “structure MS-L0”, une treille, en utilisant le dictionnaire et en cherchant à aligner l’arbre et une trajectoire avec aussi peu que possible de croisements de liaisons. Un but central de cette recherche est de fusionner les approches de la TA par pivot, de la TA interactive, et de la génération multilingue de texte.</abstract>
      <url hash="4f12b19a">2002.jeptalnrecital-long.25</url>
      <language>fra</language>
      <bibkey>boitet-tsai-2002-la</bibkey>
    </paper>
    <paper id="26">
      <title>Traduction automatique ancrée dans l’analyse linguistique</title>
      <author><first>Jessie</first><last>Pinkham</last></author>
      <author><first>Martine</first><last>Smets</last></author>
      <pages>289–298</pages>
      <abstract>Nous présentons dans cet article le système de traduction français-anglais MSR-MT développé à Microsoft dans le groupe de recherche sur le traitement du language (NLP). Ce système est basé sur des analyseurs sophistiqués qui produisent des formes logiques, dans la langue source et la langue cible. Ces formes logiques sont alignées pour produire la base de données du transfert, qui contient les correspondances entre langue source et langue cible, utilisées lors de la traduction. Nous présentons différents stages du développement de notre système, commencé en novembre 2000. Nous montrons que les performances d’octobre 2001 de notre système sont meilleures que celles du système commercial Systran, pour le domaine technique, et décrivons le travail linguistique qui nous a permis d’arriver à cette performance. Nous présentons enfin les résultats préliminaires sur un corpus plus général, les débats parlementaires du corpus du Hansard. Quoique nos résultats ne soient pas aussi concluants que pour le domaine technique, nous sommes convaincues que la résolution des problèmes d’analyse que nous avons identifiés nous permettra d’améliorer notre performance.</abstract>
      <url hash="e9189087">2002.jeptalnrecital-long.26</url>
      <language>fra</language>
      <bibkey>pinkham-smets-2002-traduction</bibkey>
    </paper>
    <paper id="27">
      <title>Descriptions d’arbres avec polarités : les Grammaires d’Interaction</title>
      <author><first>Guy</first><last>Perrier</last></author>
      <pages>299–308</pages>
      <abstract>Nous présentons un nouveau formalisme linguistique, les Grammaires d’Interaction, dont les objets syntaxiques de base sont des descriptions d’arbres, c’est-à-dire des formules logiques spécifiant partiellement des arbres syntaxiques. Dans ce contexte, l’analyse syntaxique se traduit par la construction de modèles de descriptions sous la forme d’arbres syntaxiques complètement spécifiés. L’opération de composition syntaxique qui permet cette construction pas à pas est contrôlée par un système de traits polarisés agissant comme des charges électrostatiques.</abstract>
      <url hash="c02d39a1">2002.jeptalnrecital-long.27</url>
      <language>fra</language>
      <bibkey>perrier-2002-descriptions</bibkey>
    </paper>
    <paper id="28">
      <title>Recherche de la réponse fondée sur la reconnaissance du focus de la question</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Martine</first><last>Hurault-Plantet</last></author>
      <author><first>Gabriel</first><last>Illouz</last></author>
      <author><first>Laura</first><last>Monceaux</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>309–318</pages>
      <abstract>Le système de question-réponse QALC utilise les documents sélectionnés par un moteur de recherche pour la question posée, les sépare en phrases afin de comparer chaque phrase avec la question, puis localise la réponse soit en détectant l’entité nommée recherchée, soit en appliquant des patrons syntaxiques d’extraction de la réponse, sortes de schémas figés de réponse pour un type donné de question. Les patrons d’extraction que nous avons définis se fondent sur la notion de focus, qui est l’élément important de la question, celui qui devra se trouver dans la phrase réponse. Dans cet article, nous décrirons comment nous déterminons le focus dans la question, puis comment nous l’utilisons dans l’appariement question-phrase et pour la localisation de la réponse dans les phrases les plus pertinentes retenues.</abstract>
      <url hash="a43a3836">2002.jeptalnrecital-long.28</url>
      <language>fra</language>
      <bibkey>ferret-etal-2002-recherche</bibkey>
    </paper>
  </volume>
  <volume id="poster" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 9ème conférence sur le Traitement Automatique des Langues Naturelles. Posters</booktitle>
      <editor><first>Jean-Marie</first><last>Pierrel</last></editor>
      <publisher>ATALA</publisher>
      <address>Nancy, France</address>
      <month>June</month>
      <year>2002</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="4d01ade7">2002.jeptalnrecital-poster.0</url>
      <bibkey>jep-taln-recital-2002-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Corpus <fixed-case>OTG</fixed-case> et <fixed-case>ECOLE</fixed-case>

_<fixed-case>MASSY</fixed-case> : vers la constitution d’une collection de corpus francophones de dialogue oral diffusés librement</title>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Sabine</first><last>Letellier-Zarshenas</last></author>
      <author><first>Pascale</first><last>Nicolas</last></author>
      <author><first>Igor</first><last>Schadle</last></author>
      <author><first>Jean</first><last>Caelen</last></author>
      <pages>319–324</pages>
      <abstract>Cet article présente deux corpus francophones de dialogue oral (OTG et ECOLE

_MASSY) mis librement à la disposition de la communauté scientifique. Ces deux corpus constituent la première livraison du projet Parole Publique initié par le laboratoire VALORIA. Ce projet vise la constitution d’une collection de corpus de dialogue oral enrichis par annotation morpho-syntaxique. Ces corpus de dialogue finalisé sont essentiellement destinés à une utilisation en communication homme-machine.</abstract>
      <url hash="9a1f5dcb">2002.jeptalnrecital-poster.1</url>
      <language>fra</language>
      <bibkey>antoine-etal-2002-corpus</bibkey>
    </paper>
    <paper id="2">
      <title>Relatifs et référents inclus dans un <fixed-case>SN</fixed-case> : des paramètres pour présélectionner la saisie</title>
      <author><first>Laurence</first><last>Kister</last></author>
      <pages>325–330</pages>
      <abstract>Notre objectif est de repérer l’existence de régularités pour prévoir l’attachement du relatif et de son référent introduit par un SN de la forme dét. N1 de (dét.) N2 en vue d’un traitement automatique. Pour évaluer les préférences, nous avons entrepris une analyse sur corpus. L’examen des occurrences examinées laisse entrevoir des variations en fonction de paramètres d’ordre fonctionnel, syntaxique et sémantiques : déterminants, traits sémantiques, saillance, relation établie par l’utilisation de de, position grammaticale du SN qui introduit le référent dans le discours...</abstract>
      <url hash="5d5ac7d1">2002.jeptalnrecital-poster.2</url>
      <language>fra</language>
      <bibkey>kister-2002-relatifs</bibkey>
    </paper>
    <paper id="3">
      <title>L’analyse sémantique latente et l’identification des métaphores</title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <author><first>Anne-Françoise</first><last>Cabiaux</last></author>
      <pages>331–337</pages>
      <abstract>Après avoir présenté le modèle computationnel de l’interprétation de métaphores proposé par Kintsch (2000), nous rapportons une étude préliminaire qui évalue son efficacité dans le traitement de métaphores littéraires et la possibilité de l’employer pour leur identification.</abstract>
      <url hash="a30ce802">2002.jeptalnrecital-poster.3</url>
      <language>fra</language>
      <bibkey>bestgen-cabiaux-2002-lanalyse</bibkey>
    </paper>
    <paper id="4">
      <title>An Example-Based Semantic Parser for Natural Language</title>
      <author><first>Michel</first><last>Généreux</last></author>
      <pages>338–343</pages>
      <abstract>This paper presents a method for guiding semantic parsers based on a statistical model. The parser is example driven, that is, it learns how to interpret a new utterance by looking at some examples. It is mainly predicated on the idea that similarities exist between contexts in which individual parsing actions take place. Those similarities are then used to compute the degree of certainty of a particular parse. The treatment of word order and the disambiguation of meanings can therefore be learned.</abstract>
      <url hash="0bc60aea">2002.jeptalnrecital-poster.4</url>
      <bibkey>genereux-2002-example</bibkey>
    </paper>
    <paper id="5">
      <title>Distinguer les termes des collocations : étude sur corpus du patron &lt;Adjectif – Nom&gt; en anglais médical</title>
      <author><first>François</first><last>Maniez</last></author>
      <pages>344–349</pages>
      <abstract>Un bon nombre des applications de traitement automatique des langues qui ont pour domaine les langues de spécialité sont des outils d’extraction terminologique. Elles se concentrent donc naturellement sur l’identification des groupes nominaux et des groupes prépositionnels ou prémodificateurs qui leur sont associés. En nous fondant sur un corpus composé d’articles de recherche médicale de langue anglaise, nous proposons un modèle d’extraction phraséologique semi-automatisée. Afin de distinguer, dans le cas des expressions de patron syntaxique &lt;Adjectif – Nom&gt;, les termes de la langue médicale des simples collocations, nous nous sommes livré au repérage des adjectifs entrant en cooccurrence avec les adverbes. Cette méthode, qui permet l’élimination de la plupart des adjectifs relationnels, s’avère efficace en termes de précision. L’amélioration de son rappel nécessite toutefois l’utilisation de corpus de grande taille ayant subi un étiquetage morpho-syntaxique préalable.</abstract>
      <url hash="7ae88163">2002.jeptalnrecital-poster.5</url>
      <language>fra</language>
      <bibkey>maniez-2002-distinguer</bibkey>
    </paper>
    <paper id="6">
      <title>Une <fixed-case>M</fixed-case>éta<fixed-case>G</fixed-case>rammaire pour les adjectifs du français</title>
      <author><first>Nicolas</first><last>Barrier</last></author>
      <pages>350–356</pages>
      <abstract>Initialement développée au sein de l’université Paris VII depuis maintenant près de quinze ans, la grammaire FTAG, une implémentation du modèle des grammaires d’arbres adjoints pour le français, a connu ses dernières années, diverses évolutions majeures. (Candito, 1996) a ainsi réalisé l’intégration d’un modèle de représentation compact et hiérarchique d’informations redondantes que peut contenir une grammaire, au sein d’un système déjà existant. Ce modèle, que nous appelons MétaGammaire (MG) nous a permis, en pratique, de générer semi-automatiquement des arbres élémentaires, et par là même, d’augmenter de façon considérable les différents phénomènes syntaxiques couverts par notre grammaire. Un soin tout particulier a donc été apporté pour traiter les prédicats verbaux, en laissant cependant (partiellement) de côté le prédicat adjectival. Nous présentons donc ici une nouvelle implémentation de ce prédicat dans le cadre d’une extension de la grammaire FTAG existante.</abstract>
      <url hash="94aef2e5">2002.jeptalnrecital-poster.6</url>
      <language>fra</language>
      <bibkey>barrier-2002-une</bibkey>
    </paper>
    <paper id="7">
      <title>Polynomial Tree Substitution Grammars: Characterization and New Examples</title>
      <author><first>Jean-Cédric</first><last>Chappelier</last></author>
      <author><first>Martin</first><last>Rajman</last></author>
      <author><first>Antoine</first><last>Rozenknop</last></author>
      <pages>357–362</pages>
      <abstract>Polynomial Tree Substitution Grammars, a subclass of STSGs for which finding the most probable parse is no longer NP-hard but polynomial, are defined and characterized in terms of general properties on the elementary trees in the grammar. Various sufficient and easy to compute properties for a STSG to be polynomial are presented. The min-max selection principle is shown to be one such sufficient property. In addition, another, new, instance of a sufficient property, based on lexical heads, is presented. The performances of both models are evaluated on several corpora.</abstract>
      <url hash="f96c797d">2002.jeptalnrecital-poster.7</url>
      <bibkey>chappelier-etal-2002-polynomial</bibkey>
    </paper>
    <paper id="8">
      <title>Reformuler des expressions multimodales</title>
      <author><first>Élisabeth</first><last>Godbert</last></author>
      <pages>363–368</pages>
      <abstract>Le domaine des “Interfaces Utilisateur Intelligentes” a vu ces dernières années la réalisation de systèmes complexes mettant en oeuvre une interaction multimodale dans laquelle les différentes techniques de communication (textes, gestes, parole, sélection graphique) sont coordonnées en entrée et/ou en sortie. Nous nous intéressons ici aux systèmes qui prennent en entrée des expressions multimodales et en produisent une reformulation en une expression unimodale sémantiquement équivalente. Nous proposons une modélisation du processus de traduction d’expressions multimodales en expressions unimodales, et nous décrivons la mise en oeuvre d’un processus de ce type dans un logiciel d’aide à l’apprentissage du langage.</abstract>
      <url hash="3ad1b04a">2002.jeptalnrecital-poster.8</url>
      <language>fra</language>
      <bibkey>godbert-2002-reformuler</bibkey>
    </paper>
    <paper id="9">
      <title>Un modèle de dialogue par les attentes du locuteur</title>
      <author><first>Yannick</first><last>Fouquet</last></author>
      <pages>369–375</pages>
      <abstract>Dans cet article, nous aborderons la notion d’attentes, vue du côté du locuteur, afin d’améliorer la modélisation du dialogue. Nous présenterons notre définition des attentes ainsi que notre notation, fondée sur une approche pragmatique du dialogue. Nous comparerons deux approches, l’une (uniquement stochastique) fondée sur la prédiction d’actes de parole, l’autre mettant en jeu les attentes du locuteur et leur gestion.</abstract>
      <url hash="3b5d09c8">2002.jeptalnrecital-poster.9</url>
      <language>fra</language>
      <bibkey>fouquet-2002-un</bibkey>
    </paper>
    <paper id="10">
      <title>Segmentation en thèmes de conversations téléphoniques : traitement en amont pour l’extraction d’information</title>
      <author><first>Narjès</first><last>Boufaden</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <author><first>Yoshua</first><last>Bengio</last></author>
      <pages>376–381</pages>
      <abstract>Nous présentons une approche de découpage thématique que nous utiliserons pour faciliter l’extraction d’information à partir de conversations téléphoniques transcrites. Nous expérimentons avec un modèle de Markov caché utilisant des informations de différents niveaux linguistiques, des marques d’extra-grammaticalités et les entités nommées comme source additionnelle d’information. Nous comparons le modèle obtenu avec notre modèle de base utilisant uniquement les marques linguistiques et les extra-grammaticalités. Les résultats montrent l’efficacité de l’approche utilisant les entités nommées.</abstract>
      <url hash="ffbe0a33">2002.jeptalnrecital-poster.10</url>
      <language>fra</language>
      <bibkey>boufaden-etal-2002-segmentation</bibkey>
    </paper>
    <paper id="11">
      <title>Discours et compositionnalité</title>
      <author><first>Laurent</first><last>Roussarie</last></author>
      <author><first>Pascal</first><last>Amsili</last></author>
      <pages>382–387</pages>
      <abstract>Partant du principe que certaines phrases peuvent réaliser plusieurs actes de langage, i.e., dans une interface sémantique–pragmatique, plusieurs constituants de discours séparés, nous proposons, dans le cadre de la SDRT, un algorithme de construction de représentations sémantiques qui prend en compte tous les aspects discursifs dès que possible et de façon compositionnelle.</abstract>
      <url hash="87586055">2002.jeptalnrecital-poster.11</url>
      <language>fra</language>
      <bibkey>roussarie-amsili-2002-discours</bibkey>
    </paper>
    <paper id="12">
      <title>Compréhension Automatique de la Parole et <fixed-case>TAL</fixed-case> : une approche syntaxico-sémantique pour le traitement des inattendus structuraux du français parlé</title>
      <author><first>Jérôme</first><last>Goulian</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Franck</first><last>Poirier</last></author>
      <pages>388–393</pages>
      <abstract>Dans cet article, nous présentons un système de Compréhension Automatique de la Parole dont l’un des objectifs est de permettre un traitement fiable et robuste des inattendus structuraux du français parlé (hésitations, répétitions et corrections). L’analyse d’un énoncé s’effectue en deux étapes : une première étape générique d’analyse syntaxique de surface suivie d’une seconde étape d’analyse sémantico-pragmatique, dépendante du domaine d’application et reposant sur un formalisme lexicalisé : les grammaires de liens. Les résultats de l’évaluation de ce système lors de la campagne d’évaluation du Groupe de Travail Compréhension Robuste du GDR I3 du CNRS nous permettent de discuter de l’intérêt et des limitations de l’approche adoptée.</abstract>
      <url hash="2abf55a0">2002.jeptalnrecital-poster.12</url>
      <language>fra</language>
      <bibkey>goulian-etal-2002-comprehension</bibkey>
    </paper>
    <paper id="13">
      <title>Automatic Item Text Generation in Educational Assessment</title>
      <author><first>Cédrick</first><last>Fairon</last></author>
      <author><first>David M.</first><last>Williamson</last></author>
      <pages>394–400</pages>
      <abstract>We present an automatic text generation system (ATG) developed for the generation of natural language text for automatically produced test items. This ATG has been developed to work with an automatic item generation system for analytical reasoning items for use in tests with high-stakes outcomes (such as college admissions decisions). As such, the development and implementation of this ATG is couched in the context and goals of automated item generation for educational assessment.</abstract>
      <url hash="c34389cf">2002.jeptalnrecital-poster.13</url>
      <bibkey>fairon-williamson-2002-automatic</bibkey>
    </paper>
  </volume>
  <volume id="tutoriel" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 9ème conférence sur le Traitement Automatique des Langues Naturelles. Tutoriels</booktitle>
      <editor><first>Jean-Marie</first><last>Pierrel</last></editor>
      <publisher>ATALA</publisher>
      <address>Nancy, France</address>
      <month>June</month>
      <year>2002</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="1fe8e593">2002.jeptalnrecital-tutoriel.0</url>
      <bibkey>jep-taln-recital-2002-actes-de-la-9eme</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Un ensemble de ressources informatisées et intégrées pour l’étude du français : <fixed-case>FRANTEXT</fixed-case>, <fixed-case>TLF</fixed-case>i, Dictionnaires de l’Académie et logiciel Stella, présentation et apprentissage de leurs exploitations</title>
      <author><first>Pascale</first><last>Bernard</last></author>
      <author><first>Jacques</first><last>Dendien</last></author>
      <author><first>Josette</first><last>Lecomte</last></author>
      <author><first>Jean-Marie</first><last>Pierrel</last></author>
      <pages>3–36</pages>
      <abstract>Nous proposons de présenter quelques-unes des ressources linguistiques informatisées que le laboratoire ATILF propose sur la toile et leurs diversités d’exploitation potentielle. Ces importantes ressources sur la langue française regroupent un ensemble de divers dictionnaires et lexiques, et de bases de données dont les plus importants sont le TLFi (Trésor de la Langue Française informatisé) et Frantext (plus de 3500 textes, dont la plupart catégorisés). Elles exploitent, pour la plupart, les fonctionnalités du logiciel Stella, qui correspond à un véritable moteur de recherche dédié aux bases textuelles s’appuyant sur une nouvelle théorie des objets textuels. Tous les spécialistes de traitement automatique de la langue ainsi que tous les linguistes, syntacticiens aussi bien que sémanticiens, stylisticiens et autres peuvent exploiter avec bonheur les possibilités offertes par Stella sur le TLFi et autres ressources offertes par l’ATILF. Ces recherches peuvent s’articuler autour des axes suivants : études en vue de repérer des cooccurrences et collocations, extraction de sous-lexiques, études morphologiques, études de syntaxe locale, études de sémantique, études de stylistique, etc. Nous proposons de démystifier le maniement des requêtes sur le TLFi, FRANTEXT et nos autres ressources à l’aide du logiciel Stella, et d’expliquer et de montrer comment interroger au mieux ces ressources et utiliser l’hyper-navigation mise en place entre ces ressources pour en tirer les meilleurs bénéfices.</abstract>
      <url hash="cc44da51">2002.jeptalnrecital-tutoriel.1</url>
      <language>fra</language>
      <bibkey>bernard-etal-2002-un</bibkey>
    </paper>
    <paper id="2">
      <title>Modélisation des liens lexicaux au moyen des fonctions lexicales</title>
      <author><first>Alain</first><last>Polguère</last></author>
      <pages>37–60</pages>
      <abstract>Ce tutoriel est une introduction à la modélisation lexicographique des liens lexicaux au moyen des fonctions lexicales de la théorie Sens-Texte. Il s’agit donc d’examiner un sous-ensemble des tâches effectuées en lexicographie formelle basée sur la lexicologie explicative et combinatoire. Plutôt que de viser l’introduction de toutes les fonctions lexicales identifiées par la théorie Sens- Texte, je vais m’attacher à introduire la notion de fonction lexicale de façon méthodique, en présentant d’abord les notions linguistiques plus générales sur lesquelles elle s’appuie (lexie, prédicat, actant, dérivation sémantique, collocation, etc.). Ce document vise essentiellement à récapituler les définitions des notions linguistiques qui vont être vues dans le tutoriel de façon pratique, par le biais d’exercices à caractère lexicographique.</abstract>
      <url hash="5b7f7439">2002.jeptalnrecital-tutoriel.2</url>
      <language>fra</language>
      <bibkey>polguere-2002-modelisation</bibkey>
    </paper>
    <paper id="3">
      <title>Word Formation in Computational Linguistics</title>
      <author><first>Pius</first><last>Ten Hacken</last></author>
      <pages>61–87</pages>
      <abstract/>
      <url hash="4666841d">2002.jeptalnrecital-tutoriel.3</url>
      <bibkey>ten-hacken-2002-word</bibkey>
    </paper>
    <paper id="4">
      <title>Tutoriel : Open Agent Architecture Développement d’applications de <fixed-case>TALN</fixed-case> distribuées, multiagents et multiplates-formes</title>
      <author><first>Antonio</first><last>Balvet</last></author>
      <author><first>Olivier</first><last>Grisvard</last></author>
      <author><first>Pascal</first><last>Bisson</last></author>
      <pages>88–107</pages>
      <abstract>Nous présenterons tout d’abord la philosophie « Agents » en général, afin d’en montrer les avantages pour le domaine du TALN, qui se caractérise par une hétérogénéité avérée des systèmes existants (multiplicité des langages de programmation), ainsi qu’une forte demande en ressources (mémoire notamment). Nous ferons ensuite une présentation des principales plate-formes orientées agents, puis nous examinerons de plus près la plate-forme développée au Standford Research Institute (SRI) : OAA (licence libre). Nous clôturerons le tutoriel sur des exemples commentés d’applications industrielles utilisant OAA, permettant de donner toutes les clés nécessaires au développement d’applications distribuées (intra/internet), multiagents et multiplates-formes (plusieurs langages de programmation/systèmes d’exploitation).</abstract>
      <url hash="16b80c6f">2002.jeptalnrecital-tutoriel.4</url>
      <language>fra</language>
      <bibkey>balvet-etal-2002-tutoriel</bibkey>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 9ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Azim</first><last>Roussanaly</last></editor>
      <publisher>ATALA</publisher>
      <address>Nancy, France</address>
      <month>June</month>
      <year>2002</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="3f4d8d2c">2002.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2002-actes-de-la-9eme-sur</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Méthodologie pour la création d’un dictionnaire distributionnel dans une perspective d’étiquetage lexical semi-automatique</title>
      <author><first>Delphine</first><last>Reymond</last></author>
      <pages>405–414</pages>
      <abstract>Des groupes de recherche de plus en plus nombreux s’intéressent à l’étiquetage lexical ou la désambiguïsation du sens. La tendance actuelle est à l’exploitation de très grands corpus de textes qui, grâce à l’utilisation d’outils lexicographiques appropriés, peuvent fournir un ensemble de données initiales aux systèmes. A leur tour ces systèmes peuvent être utilisés pour extraire plus d’informations des corpus, qui peuvent ensuite être réinjectées dans les systèmes, dans un processus récursif. Dans cet article, nous présentons une méthodologie qui aborde la résolution de l’ambiguïté lexicale comme le résultat de l’interaction de divers indices repérables de manière semi-automatique au niveau syntaxique (valence), sémantique (collocations, classes d’objets) avec la mise en oeuvre de tests manuels.</abstract>
      <url hash="29add592">2002.jeptalnrecital-recital.1</url>
      <language>fra</language>
      <bibkey>reymond-2002-methodologie</bibkey>
    </paper>
    <paper id="2">
      <title>Etude des critères de désambiguïsation sémantique automatique : présentation et premiers résultats sur les cooccurrences</title>
      <author><first>Laurent</first><last>Audibert</last></author>
      <pages>415–424</pages>
      <abstract>Nous présentons dans cet article les débuts d’un travail visant à rechercher et à étudier systématiquement les critères de désambiguïsation sémantique automatique. Cette étude utilise un corpus français étiqueté sémantiquement dans le cadre du projet SyntSem. Le critère ici étudié est celui des cooccurrences. Nous présentons une série de résultats sur le pouvoir désambiguïsateur des cooccurrences en fonction de leur catégorie grammaticale et de leur éloignement du mot à désambiguïser.</abstract>
      <url hash="2e17e150">2002.jeptalnrecital-recital.2</url>
      <language>fra</language>
      <bibkey>audibert-2002-etude</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>LIZARD</fixed-case>, un assistant pour le développement de ressources linguistiques à base de cascades de transducteurs</title>
      <author><first>Antonio</first><last>Balvet</last></author>
      <pages>425–434</pages>
      <abstract>Nous présentons un outil visant à assister les développeurs de ressources linguistiques en automatisant la fouille de corpus. Cet outil, est guidé par les principes de l’analyse distributionnelle sur corpus spécialisés, étendue grâce à des ressources lexicales génériques. Nous présentons une évaluation du gain de performances dû à l’intégration de notre outil à une application de filtrage d’information et nous élargissons le champ d’application de l’assistant aux études sur corpus menées à l’aide de cascades de transducteurs à états finis.</abstract>
      <url hash="068b33ca">2002.jeptalnrecital-recital.3</url>
      <language>fra</language>
      <bibkey>balvet-2002-lizard</bibkey>
    </paper>
    <paper id="4">
      <title>Conceptualisation d’un système d’informations lexicales, une interface paramétrable pour le <fixed-case>T</fixed-case>.<fixed-case>A</fixed-case>.<fixed-case>L</fixed-case></title>
      <author><first>Djamé</first><last>Seddah</last></author>
      <author><first>Evelyne</first><last>Jacquey</last></author>
      <pages>435–444</pages>
      <abstract>La nécessité de ressources lexicales normalisées et publiques est avérée dans le domaine du TAL. Cet article vise à montrer comment, sur la base d’une partie du lexique MULTEXT disponible sur le serveur ABU, il serait possible de construire une architecture permettant tout à la fois l’accès aux ressources avec des attentes différentes (lemmatiseur, parseur, extraction d’informations, prédiction, etc.) et la mise à jour par un groupe restreint de ces ressources. Cette mise à jour consistant en l’intégration et la modification, automatique ou manuelle, de données existantes. Pour ce faire, nous cherchons à prendre en compte à la fois les besoins et les données accessibles. Ce modèle est évalué conceptuellement dans un premier temps en fonction des systèmes utilisés dans notre équipe : un analyseur TAG, un constructeur de grammaires TAGs, un extracteur d’information.</abstract>
      <url hash="297c07ba">2002.jeptalnrecital-recital.4</url>
      <language>fra</language>
      <bibkey>seddah-jacquey-2002-conceptualisation</bibkey>
    </paper>
    <paper id="5">
      <title>Problèmes posés par la reconnaissance de gestes en Langue des Signes</title>
      <author><first>Bruno</first><last>Bossard</last></author>
      <pages>445–454</pages>
      <abstract>Le but de cet article est d’expliciter certains des problèmes rencontrés lorsque l’on cherche à concevoir un système de reconnaissance de gestes de la Langue des Signes et de proposer des solutions adaptées. Les trois aspects traités ici concernent la simultanéïté d’informations véhiculées par les gestes des mains, la synchronisation éventuelle entre les deux mains et le fait que différentes classes de signes peuvent se rencontrer dans une phrase en Langue des Signes.</abstract>
      <url hash="447dcb6e">2002.jeptalnrecital-recital.5</url>
      <language>fra</language>
      <bibkey>bossard-2002-problemes</bibkey>
    </paper>
    <paper id="6">
      <title>Annotation des descriptions définies : le cas des reprises par les rôles thématiques</title>
      <author><first>Hélène</first><last>Manuélian</last></author>
      <pages>455–464</pages>
      <abstract>Nous présentons dans cet article un cas particulier de description définie où la description reprend le rôle thématique d’un argument (implicite ou explicite) d’un événement mentionné dans le contexte linguistique. Nous commençons par montrer que les schémas d’annotation proposés (MATE) et utilisés (Poesio et Vieira 2000) ne permettent pas une caractérisation uniforme ni, partant, un repérage facile de ces reprises. Nous proposons une extension du schéma MATE qui pallie cette difficulté.</abstract>
      <url hash="15592016">2002.jeptalnrecital-recital.6</url>
      <language>fra</language>
      <bibkey>manuelian-2002-annotation</bibkey>
    </paper>
    <paper id="7">
      <title>Etude des répétitions en français parlé spontané pour les technologies de la parole</title>
      <author><first>Sandrine</first><last>Henry</last></author>
      <pages>465–474</pages>
      <abstract>Cet article rapporte les résultats d’une étude quantitative des répétitions menée à partir d’un corpus de français parlé spontané d’un million de mots, étude réalisée dans le cadre de notre première année de thèse. L’étude linguistique pourra aider à l’amélioration des systèmes de reconnaissance de la parole et de l’étiquetage grammatical automatique de corpus oraux. Ces technologies impliquent la prise en compte et l’étude des répétitions de performance (en opposition aux répétitions de compétence, telles que nous nous sujet + complément) afin de pouvoir, par la suite, les « gommer » avant des traitements ultérieurs. Nos résultats montrent que les répétitions de performance concernent principalement les mots-outils et apparaissent à des frontières syntaxiques majeures.</abstract>
      <url hash="5ff1adcb">2002.jeptalnrecital-recital.7</url>
      <language>fra</language>
      <bibkey>henry-2002-etude</bibkey>
    </paper>
    <paper id="8">
      <title>Normalisation de documents par analyse du contenu à l’aide d’un modèle sémantique et d’un générateur</title>
      <author><first>Aurélien</first><last>Max</last></author>
      <pages>475–484</pages>
      <abstract>La problématique de la normalisation de documents est introduite et illustrée par des exemples issus de notices pharmaceutiques. Un paradigme pour l’analyse du contenu des documents est proposé. Ce paradigme se base sur la spécification formelle de la sémantique des documents et utilise une notion de similarité floue entre les prédictions textuelles d’un générateur de texte et le texte du document à analyser. Une implémentation initiale du paradigme est présentée.</abstract>
      <url hash="645f7ea0">2002.jeptalnrecital-recital.8</url>
      <language>fra</language>
      <bibkey>max-2002-normalisation</bibkey>
    </paper>
  </volume>
  <volume id="recitalposter" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 9ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues (Posters)</booktitle>
      <editor><first>Azim</first><last>Roussanaly</last></editor>
      <publisher>ATALA</publisher>
      <address>Nancy, France</address>
      <month>June</month>
      <year>2002</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="d0afa4b6">2002.jeptalnrecital-recitalposter.0</url>
      <bibkey>jep-taln-recital-2002-actes-de-la-9eme-sur-le</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Un Modèle Distribué d’Interprétation de Requêtes fondé sur la notion d’Observateur</title>
      <author><first>Guillaume</first><last>Pitel</last></author>
      <pages>489–498</pages>
      <abstract>Nous proposons un modèle de conception d’agent conversationnel pour l’assistance d’interface. Notre but est d’obtenir un système d’interprétation de requêtes en contexte, générique pour l’indépendance vis-à-vis de la tâche, extensible pour sa capacité à intégrer des connaissances sur un nouveau domaine sans remettre en cause les connaissances antérieures et unifié dans le sens où tous les aspects du traitement de la langue naturelle, syntaxe, sémantique, ou pragmatique doivent s’exprimer dans un même formalisme. L’originalité de notre système est de permettre de représenter des connaissances d’interprétation de niveaux de granularité divers sous une même forme, réduisant la problématique de communication entre sources de connaissances qui existe dans les systèmes modulaires. Nous adoptons l’approche des micro-systèmes suivant laquelle l’interprétation de la langue se fait selon un processus non stratifié, et où absolument tous les niveaux peuvent interagir entre eux. Pour cela, nous introduisons et définissons un type d’entité que nous avons nommé observateur.</abstract>
      <url hash="e635c971">2002.jeptalnrecital-recitalposter.1</url>
      <language>fra</language>
      <bibkey>pitel-2002-un</bibkey>
    </paper>
    <paper id="2">
      <title>Extraction et classification automatique de matériaux textuels pour la création de tests de langue</title>
      <author><first>Murielle</first><last>Marchand</last></author>
      <pages>499–506</pages>
      <abstract>Nous présentons l’état de développement d’un outil d’extraction et de classification automatique de phrases pour la création de tests de langue. Cet outil de TAL est conçu pour, dans un premier temps, localiser et extraire de larges corpus en ligne du matériel textuel (phrases) possédant des propriétés linguistiques bien spécifiques. Il permet, dans un deuxième temps, de classifier automatiquement ces phrases-candidates d’après le type d’erreurs qu’elles sont en mesure de contenir. Le développement de cet outil s’inscrit dans un contexte d’optimalisation du processus de production d’items pour les tests d’évaluation. Pour répondre aux exigences croissantes de production, les industries de développement de tests de compétences doivent être capable de développer rapidement de grandes quantités de tests. De plus, pour des raisons de sécurité, les items doivent être continuellement remplacés, ce qui crée un besoin d’approvisionnement constant. Ces exigences de production et révision sont, pour ces organisations, coûteuses en temps et en personnel. Les bénéfices à retirer du développement et de l’implantation d’un outil capable d’automatiser la majeure partie du processus de production de ces items sont par conséquents considérables.</abstract>
      <url hash="829767d7">2002.jeptalnrecital-recitalposter.2</url>
      <language>fra</language>
      <bibkey>marchand-2002-extraction</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>M</fixed-case>em<fixed-case>L</fixed-case>abor, un environnement de création, de gestion et de manipulation de corpus de textes</title>
      <author><first>Vincent</first><last>Perlerin</last></author>
      <pages>507–516</pages>
      <abstract>Nous présentons dans cet article un logiciel d’étude permettant la création, la gestion et la manipulation de corpus de textes. Ce logiciel appelé MemLabor se veut un outil ouvert et open-source adaptable à toutes les opérations possibles que l’on peut effectuer sur ce type de matériau. Dans une première partie, nous présenterons les principes généraux de l’outil. Dans une seconde, nous en proposerons une utilisation dans le cadre d’une acquisition supervisée de classes sémantiques.</abstract>
      <url hash="70391444">2002.jeptalnrecital-recitalposter.3</url>
      <language>fra</language>
      <bibkey>perlerin-2002-memlabor</bibkey>
    </paper>
    <paper id="4">
      <title>Classification Automatique de messages : une approche hybride</title>
      <author><first>Omar</first><last>Nouali</last></author>
      <pages>517–522</pages>
      <abstract>Les systèmes actuels de filtrage de l’information sont basés d’une façon directe ou indirecte sur les techniques traditionnelles de recherche d’information (Malone, Kenneth, 1987), (Kilander, Takkinen, 1996). Notre approche consiste à séparer le processus de classification du filtrage proprement dit. Il s’agit d’effectuer un traitement reposant sur une compréhension primitive du message permettant d’effectuer des opérations de classement. Cet article décrit une solution pour classer des messages en se basant sur les propriétés linguistiques véhiculées par ces messages. Les propriétés linguistiques sont modélisées par un réseau de neurone. A l’aide d’un module d’apprentissage, le réseau est amélioré progressivement au fur et à mesure de son utilisation. Nous présentons à la fin les résultats d’une expérience d’évaluation.</abstract>
      <url hash="59ddbbda">2002.jeptalnrecital-recitalposter.4</url>
      <language>fra</language>
      <bibkey>nouali-2002-classification</bibkey>
    </paper>
  </volume>
</collection>
