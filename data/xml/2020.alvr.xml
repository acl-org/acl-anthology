<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.alvr">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the First Workshop on Advances in Language and Vision Research</booktitle>
      <editor><first>Xin</first><last>Wang</last></editor>
      <editor><first>Jesse</first><last>Thomason</last></editor>
      <editor><first>Ronghang</first><last>Hu</last></editor>
      <editor><first>Xinlei</first><last>Chen</last></editor>
      <editor><first>Peter</first><last>Anderson</last></editor>
      <editor><first>Qi</first><last>Wu</last></editor>
      <editor><first>Asli</first><last>Celikyilmaz</last></editor>
      <editor><first>Jason</first><last>Baldridge</last></editor>
      <editor><first>William Yang</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="7f4feb1f">2020.alvr-1</url>
      <venue>alvr</venue>
    </meta>
    <frontmatter>
      <url hash="30a10a5a">2020.alvr-1.0</url>
      <bibkey>alvr-2020-advances</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Extending <fixed-case>I</fixed-case>mage<fixed-case>N</fixed-case>et to <fixed-case>A</fixed-case>rabic using <fixed-case>A</fixed-case>rabic <fixed-case>W</fixed-case>ord<fixed-case>N</fixed-case>et</title>
      <author><first>Abdulkareem</first><last>Alsudais</last></author>
      <pages>1–6</pages>
      <abstract>ImageNet has millions of images that are labeled with English WordNet synsets. This paper investigates the extension of ImageNet to Arabic using Arabic WordNet. The objective is to discover if Arabic synsets can be found for synsets used in ImageNet. The primary finding is the identification of Arabic synsets for 1,219 of the 21,841 synsets used in ImageNet, which represents 1.1 million images. By leveraging the parent-child structure of synsets in ImageNet, this dataset is extended to 10,462 synsets (and 7.1 million images) that have an Arabic label, which is either a match or a direct hypernym, and to 17,438 synsets (and 11 million images) when a hypernym of a hypernym is included. When all hypernyms for a node are considered, an Arabic synset is found for all but four synsets. This represents the major contribution of this work: a dataset of images that have Arabic labels for 99.9% of the images in ImageNet.</abstract>
      <url hash="c4cc8c92">2020.alvr-1.1</url>
      <doi>10.18653/v1/2020.alvr-1.1</doi>
      <video href="http://slideslive.com/38929757"/>
      <bibkey>alsudais-2020-extending</bibkey>
      <pwccode url="https://github.com/alsudais/ImageNet_to_AWN" additional="false">alsudais/ImageNet_to_AWN</pwccode>
    </paper>
    <paper id="2">
      <title>Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment</title>
      <author><first>Woo Suk</first><last>Choi</last></author>
      <author><first>Kyoung-Woon</first><last>On</last></author>
      <author><first>Yu-Jung</first><last>Heo</last></author>
      <author><first>Byoung-Tak</first><last>Zhang</last></author>
      <pages>7–11</pages>
      <abstract>Scene graph is a graph representation that explicitly represents high-level semantic knowledge of an image such as objects, attributes of objects and relationships between objects. Various tasks have been proposed for the scene graph, but the problem is that they have a limited vocabulary and biased information due to their own hypothesis. Therefore, results of each task are not generalizable and difficult to be applied to other down-stream tasks. In this paper, we propose Entity Synset Alignment(ESA), which is a method to create a general scene graph by aligning various semantic knowledge efficiently to solve this bias problem. The ESA uses a large-scale lexical database, WordNet and Intersection of Union (IoU) to align the object labels in multiple scene graphs/semantic knowledge. In experiment, the integrated scene graph is applied to the image-caption retrieval task as a down-stream task. We confirm that integrating multiple scene graphs helps to get better representations of images.</abstract>
      <url hash="af689e3c">2020.alvr-1.2</url>
      <doi>10.18653/v1/2020.alvr-1.2</doi>
      <video href="http://slideslive.com/38929758"/>
      <bibkey>choi-etal-2020-toward</bibkey>
    </paper>
    <paper id="3">
      <title>Visual Question Generation from Radiology Images</title>
      <author><first>Mourad</first><last>Sarrouti</last></author>
      <author><first>Asma</first><last>Ben Abacha</last></author>
      <author><first>Dina</first><last>Demner-Fushman</last></author>
      <pages>12–18</pages>
      <abstract>Visual Question Generation (VQG), the task of generating a question based on image contents, is an increasingly important area that combines natural language processing and computer vision. Although there are some recent works that have attempted to generate questions from images in the open domain, the task of VQG in the medical domain has not been explored so far. In this paper, we introduce an approach to generation of visual questions about radiology images called VQGR, i.e. an algorithm that is able to ask a question when shown an image. VQGR first generates new training data from the existing examples, based on contextual word embeddings and image augmentation techniques. It then uses the variational auto-encoders model to encode images into a latent space and decode natural language questions. Experimental automatic evaluations performed on the VQA-RAD dataset of clinical visual questions show that VQGR achieves good performances compared with the baseline system. The source code is available at https://github.com/sarrouti/vqgr.</abstract>
      <url hash="0dcc0d39">2020.alvr-1.3</url>
      <doi>10.18653/v1/2020.alvr-1.3</doi>
      <video href="http://slideslive.com/38929760"/>
      <bibkey>sarrouti-etal-2020-visual</bibkey>
      <pwccode url="https://github.com/sarrouti/vqgr" additional="false">sarrouti/vqgr</pwccode>
    </paper>
    <paper id="4">
      <title>On the role of effective and referring questions in <fixed-case>G</fixed-case>uess<fixed-case>W</fixed-case>hat?!</title>
      <author><first>Mauricio</first><last>Mazuecos</last></author>
      <author><first>Alberto</first><last>Testoni</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <author><first>Luciana</first><last>Benotti</last></author>
      <pages>19–25</pages>
      <abstract>Task success is the standard metric used to evaluate referential visual dialogue systems. In this paper we propose two new metrics that evaluate how each question contributes to the goal. First, we measure how effective each question is by evaluating whether the question discards objects that are not the referent. Second, we define referring questions as those that univocally identify one object in the image. We report the new metrics for human dialogues and for state of the art publicly available models on GuessWhat?!. Regarding our first metric, we find that successful dialogues do not have a higher percentage of effective questions for most models. With respect to the second metric, humans make questions at the end of the dialogue that are referring, confirming their guess before guessing. Human dialogues that use this strategy have a higher task success but models do not seem to learn it.</abstract>
      <url hash="61627744">2020.alvr-1.4</url>
      <doi>10.18653/v1/2020.alvr-1.4</doi>
      <bibkey>mazuecos-etal-2020-role</bibkey>
    </paper>
    <paper id="5">
      <title>Latent Alignment of Procedural Concepts in Multimodal Recipes</title>
      <author><first>Hossein</first><last>Rajaby Faghihi</last></author>
      <author><first>Roshanak</first><last>Mirzaee</last></author>
      <author><first>Sudarshan</first><last>Paliwal</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>26–31</pages>
      <abstract>We propose a novel alignment mechanism to deal with procedural reasoning on a newly released multimodal QA dataset, named RecipeQA. Our model is solving the textual cloze task which is a reading comprehension on a recipe containing images and instructions. We exploit the power of attention networks, cross-modal representations, and a latent alignment space between instructions and candidate answers to solve the problem. We introduce constrained max-pooling which refines the max pooling operation on the alignment matrix to impose disjoint constraints among the outputs of the model. Our evaluation result indicates a 19% improvement over the baselines.</abstract>
      <url hash="1aa3a317">2020.alvr-1.5</url>
      <doi>10.18653/v1/2020.alvr-1.5</doi>
      <video href="http://slideslive.com/38929759"/>
      <bibkey>rajaby-faghihi-etal-2020-latent</bibkey>
      <pwccode url="https://github.com/HLR/LatentAlignmentProcedural" additional="false">HLR/LatentAlignmentProcedural</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/recipeqa">RecipeQA</pwcdataset>
    </paper>
  </volume>
</collection>
