<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.sdp">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Scholarly Document Processing</booktitle>
      <editor><first>Iz</first><last>Beltagy</last></editor>
      <editor><first>Arman</first><last>Cohan</last></editor>
      <editor><first>Guy</first><last>Feigenblat</last></editor>
      <editor><first>Dayne</first><last>Freitag</last></editor>
      <editor><first>Tirthankar</first><last>Ghosal</last></editor>
      <editor><first>Keith</first><last>Hall</last></editor>
      <editor><first>Drahomira</first><last>Herrmannova</last></editor>
      <editor><first>Petr</first><last>Knoth</last></editor>
      <editor><first>Kyle</first><last>Lo</last></editor>
      <editor><first>Philipp</first><last>Mayr</last></editor>
      <editor><first>Robert M.</first><last>Patton</last></editor>
      <editor><first>Michal</first><last>Shmueli-Scheuer</last></editor>
      <editor><first>Anita</first><last>de Waard</last></editor>
      <editor><first>Kuansan</first><last>Wang</last></editor>
      <editor><first>Lucy Lu</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.sdp-1</url>
    </meta>
    <frontmatter>
      <url hash="f7ce0583">2021.sdp-1.0</url>
      <bibkey>sdp-2021-scholarly</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Determining the Credibility of Science Communication</title>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>1–6</pages>
      <abstract>Most work on scholarly document processing assumes that the information processed is trust-worthy and factually correct. However, this is not always the case. There are two core challenges, which should be addressed: 1) ensuring that scientific publications are credible – e.g. that claims are not made without supporting evidence, and that all relevant supporting evidence is provided; and 2) that scientific findings are not misrepresented, distorted or outright misreported when communicated by journalists or the general public. I will present some first steps towards addressing these problems and outline remaining challenges.</abstract>
      <url hash="501264fb">2021.sdp-1.1</url>
      <doi>10.18653/v1/2021.sdp-1.1</doi>
      <bibkey>augenstein-2021-determining</bibkey>
    </paper>
    <paper id="2">
      <title>Unsupervised Document Expansion for Information Retrieval with Stochastic Text Generation</title>
      <author><first>Soyeong</first><last>Jeong</last></author>
      <author><first>Jinheon</first><last>Baek</last></author>
      <author><first>ChaeHun</first><last>Park</last></author>
      <author><first>Jong</first><last>Park</last></author>
      <pages>7–17</pages>
      <abstract>One of the challenges in information retrieval (IR) is the vocabulary mismatch problem, which happens when the terms between queries and documents are lexically different but semantically similar. While recent work has proposed to expand the queries or documents by enriching their representations with additional relevant terms to address this challenge, they usually require a large volume of query-document pairs to train an expansion model. In this paper, we propose an Unsupervised Document Expansion with Generation (UDEG) framework with a pre-trained language model, which generates diverse supplementary sentences for the original document without using labels on query-document pairs for training. For generating sentences, we further stochastically perturb their embeddings to generate more diverse sentences for document expansion. We validate our framework on two standard IR benchmark datasets. The results show that our framework significantly outperforms relevant expansion baselines for IR.</abstract>
      <url hash="60c76cbd">2021.sdp-1.2</url>
      <doi>10.18653/v1/2021.sdp-1.2</doi>
      <bibkey>jeong-etal-2021-unsupervised</bibkey>
    </paper>
    <paper id="3">
      <title>Task Definition and Integration For Scientific-Document Writing Support</title>
      <author><first>Hiromi</first><last>Narimatsu</last></author>
      <author><first>Kohei</first><last>Koyama</last></author>
      <author><first>Kohji</first><last>Dohsaka</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <author><first>Yasuhiro</first><last>Minami</last></author>
      <author><first>Hirotoshi</first><last>Taira</last></author>
      <pages>18–26</pages>
      <abstract>With the increase in the number of published academic papers, growing expectations have been placed on research related to supporting the writing process of scientific papers. Recently, research has been conducted on various tasks such as citation worthiness (judging whether a sentence requires citation), citation recommendation, and citation-text generation. However, since each task has been studied and evaluated using data that has been independently developed, it is currently impossible to verify whether such tasks can be successfully pipelined to effective use in scientific-document writing. In this paper, we first define a series of tasks related to scientific-document writing that can be pipelined. Then, we create a dataset of academic papers that can be used for the evaluation of each task as well as a series of these tasks. Finally, using the dataset, we evaluate the tasks of citation worthiness and citation recommendation as well as both of these tasks integrated. The results of our evaluations show that the proposed approach is promising.</abstract>
      <url hash="b7851166">2021.sdp-1.3</url>
      <doi>10.18653/v1/2021.sdp-1.3</doi>
      <bibkey>narimatsu-etal-2021-task</bibkey>
    </paper>
    <paper id="4">
      <title>Detecting Anatomical and Functional Connectivity Relations in Biomedical Literature via Language Representation Models</title>
      <author><first>Ibrahim Burak</first><last>Ozyurt</last></author>
      <author><first>Joseph</first><last>Menke</last></author>
      <author><first>Anita</first><last>Bandrowski</last></author>
      <author><first>Maryann</first><last>Martone</last></author>
      <pages>27–35</pages>
      <abstract>Understanding of nerve-organ interactions is crucial to facilitate the development of effective bioelectronic treatments. Towards the end of developing a systematized and computable wiring diagram of the autonomic nervous system (ANS), we introduce a curated ANS connectivity corpus together with several neural language representation model based connectivity relation extraction systems. We also show that active learning guided curation for labeled corpus expansion significantly outperforms randomly selecting connectivity relation candidates minimizing curation effort. Our final relation extraction system achieves <tex-math>F_1</tex-math> = 72.8% on anatomical connectivity and <tex-math>F_1</tex-math> = 74.6% on functional connectivity relation extraction.</abstract>
      <url hash="f5014d33">2021.sdp-1.4</url>
      <doi>10.18653/v1/2021.sdp-1.4</doi>
      <bibkey>ozyurt-etal-2021-detecting</bibkey>
    </paper>
    <paper id="5">
      <title>The Biomaterials Annotator: a system for ontology-based concept annotation of biomaterials text</title>
      <author><first>Javier</first><last>Corvi</last></author>
      <author><first>Carla</first><last>Fuenteslópez</last></author>
      <author><first>José</first><last>Fernández</last></author>
      <author><first>Josep</first><last>Gelpi</last></author>
      <author><first>Maria-Pau</first><last>Ginebra</last></author>
      <author><first>Salvador</first><last>Capella-Guitierrez</last></author>
      <author><first>Osnat</first><last>Hakimi</last></author>
      <pages>36–48</pages>
      <abstract>Biomaterials are synthetic or natural materials used for constructing artificial organs, fabricating prostheses, or replacing tissues. The last century saw the development of thousands of novel biomaterials and, as a result, an exponential increase in scientific publications in the field. Large-scale analysis of biomaterials and their performance could enable data-driven material selection and implant design. However, such analysis requires identification and organization of concepts, such as materials and structures, from published texts. To facilitate future information extraction and the application of machine-learning techniques, we developed a semantic annotator specifically tailored for the biomaterials literature. The Biomaterials Annotator has been implemented following a modular organization using software containers for the different components and orchestrated using Nextflow as workflow manager. Natural language processing (NLP) components are mainly developed in Java. This set-up has allowed named entity recognition of seventeen classes relevant to the biomaterials domain. Here we detail the development, evaluation and performance of the system, as well as the release of the first collection of annotated biomaterials abstracts. We make both the corpus and system available to the community to promote future efforts in the field and contribute towards its sustainability.</abstract>
      <url hash="e7299c63">2021.sdp-1.5</url>
      <doi>10.18653/v1/2021.sdp-1.5</doi>
      <bibkey>corvi-etal-2021-biomaterials</bibkey>
    </paper>
    <paper id="6">
      <title>Keyphrase Extraction from Scientific Articles via Extractive Summarization</title>
      <author><first>Chrysovalantis Giorgos</first><last>Kontoulis</last></author>
      <author><first>Eirini</first><last>Papagiannopoulou</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <pages>49–55</pages>
      <abstract>Automatically extracting keyphrases from scholarly documents leads to a valuable concise representation that humans can understand and machines can process for tasks, such as information retrieval, article clustering and article classification. This paper is concerned with the parts of a scientific article that should be given as input to keyphrase extraction methods. Recent deep learning methods take titles and abstracts as input due to the increased computational complexity in processing long sequences, whereas traditional approaches can also work with full-texts. Titles and abstracts are dense in keyphrases, but often miss important aspects of the articles, while full-texts on the other hand are richer in keyphrases but much noisier. To address this trade-off, we propose the use of extractive summarization models on the full-texts of scholarly documents. Our empirical study on 3 article collections using 3 keyphrase extraction methods shows promising results.</abstract>
      <url hash="ab1057a1">2021.sdp-1.6</url>
      <doi>10.18653/v1/2021.sdp-1.6</doi>
      <bibkey>kontoulis-etal-2021-keyphrase</bibkey>
    </paper>
    <paper id="7">
      <title>Argument Mining for Scholarly Document Processing: Taking Stock and Looking Ahead</title>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Tirthankar</first><last>Ghosal</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Anita</first><last>de Waard</last></author>
      <author><first>Dayne</first><last>Freitag</last></author>
      <pages>56–65</pages>
      <abstract>Argument mining targets structures in natural language related to interpretation and persuasion which are central to scientific communication. Most scholarly discourse involves interpreting experimental evidence and attempting to persuade other scientists to adopt the same conclusions. While various argument mining studies have addressed student essays and news articles, those that target scientific discourse are still scarce. This paper surveys existing work in argument mining of scholarly discourse, and provides an overview of current models, data, tasks, and applications. We identify a number of key challenges confronting argument mining in the scientific domain, and suggest some possible solutions and future directions.</abstract>
      <url hash="f6d39cb0">2021.sdp-1.7</url>
      <doi>10.18653/v1/2021.sdp-1.7</doi>
      <bibkey>al-khatib-etal-2021-argument</bibkey>
    </paper>
    <paper id="8">
      <title>Bootstrapping Multilingual Metadata Extraction: A Showcase in <fixed-case>C</fixed-case>yrillic</title>
      <author><first>Johan</first><last>Krause</last></author>
      <author><first>Igor</first><last>Shapiro</last></author>
      <author><first>Tarek</first><last>Saier</last></author>
      <author><first>Michael</first><last>Färber</last></author>
      <pages>66–72</pages>
      <abstract>Applications based on scholarly data are of ever increasing importance. This results in disadvantages for areas where high-quality data and compatible systems are not available, such as non-English publications. To advance the mitigation of this imbalance, we use Cyrillic script publications from the CORE collection to create a high-quality data set for metadata extraction. We utilize our data for training and evaluating sequence labeling models to extract title and author information. Retraining GROBID on our data, we observe significant improvements in terms of precision and recall and achieve even better results with a self developed model. We make our data set covering over 15,000 publications as well as our source code freely available.</abstract>
      <url hash="70da7581">2021.sdp-1.8</url>
      <doi>10.18653/v1/2021.sdp-1.8</doi>
      <bibkey>krause-etal-2021-bootstrapping</bibkey>
    </paper>
    <paper id="9">
      <title>The Effect of Pretraining on Extractive Summarization for Scientific Documents</title>
      <author><first>Yash</first><last>Gupta</last></author>
      <author><first>Pawan Sasanka</first><last>Ammanamanchi</last></author>
      <author><first>Shikha</first><last>Bordia</last></author>
      <author><first>Arjun</first><last>Manoharan</last></author>
      <author><first>Deepak</first><last>Mittal</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Maneesh</first><last>Singh</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Preethi</first><last>Jyothi</last></author>
      <pages>73–82</pages>
      <abstract>Large pretrained models have seen enormous success in extractive summarization tasks. In this work, we investigate the influence of pretraining on a BERT-based extractive summarization system for scientific documents. We derive significant performance improvements using an intermediate pretraining step that leverages existing summarization datasets and report state-of-the-art results on a recently released scientific summarization dataset, SciTLDR. We systematically analyze the intermediate pretraining step by varying the size and domain of the pretraining corpus, changing the length of the input sequence in the target task and varying target tasks. We also investigate how intermediate pretraining interacts with contextualized word embeddings trained on different domains.</abstract>
      <url hash="826fc418">2021.sdp-1.9</url>
      <doi>10.18653/v1/2021.sdp-1.9</doi>
      <bibkey>gupta-etal-2021-effect</bibkey>
    </paper>
    <paper id="10">
      <title>Finding Pragmatic Differences Between Disciplines</title>
      <author><first>Lee</first><last>Kezar</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <pages>83–90</pages>
      <abstract>Scholarly documents have a great degree of variation, both in terms of content (semantics) and structure (pragmatics). Prior work in scholarly document understanding emphasizes semantics through document summarization and corpus topic modeling but tends to omit pragmatics such as document organization and flow. Using a corpus of scholarly documents across 19 disciplines and state-of-the-art language modeling techniques, we learn a fixed set of domain-agnostic descriptors for document sections and “retrofit” the corpus to these descriptors (also referred to as “normalization”). Then, we analyze the position and ordering of these descriptors across documents to understand the relationship between discipline and structure. We report within-discipline structural archetypes, variability, and between-discipline comparisons, supporting the hypothesis that scholarly communities, despite their size, diversity, and breadth, share similar avenues for expressing their work. Our findings lay the foundation for future work in assessing research quality, domain style transfer, and further pragmatic analysis.</abstract>
      <url hash="6724d9e5">2021.sdp-1.10</url>
      <doi>10.18653/v1/2021.sdp-1.10</doi>
      <bibkey>kezar-pujara-2021-finding</bibkey>
    </paper>
    <paper id="11">
      <title>Extractive Research Slide Generation Using Windowed Labeling Ranking</title>
      <author><first>Athar</first><last>Sefid</last></author>
      <author><first>Prasenjit</first><last>Mitra</last></author>
      <author><first>Jian</first><last>Wu</last></author>
      <author><first>C Lee</first><last>Giles</last></author>
      <pages>91–96</pages>
      <abstract>Presentation slides generated from original research papers provide an efficient form to present research innovations. Manually generating presentation slides is labor-intensive. We propose a method to automatically generates slides for scientific articles based on a corpus of 5000 paper-slide pairs compiled from conference proceedings websites. The sentence labeling module of our method is based on SummaRuNNer, a neural sequence model for extractive summarization. Instead of ranking sentences based on semantic similarities in the whole document, our algorithm measures the importance and novelty of sentences by combining semantic and lexical features within a sentence window. Our method outperforms several baseline methods including SummaRuNNer by a significant margin in terms of ROUGE score.</abstract>
      <url hash="3b9dcf40">2021.sdp-1.11</url>
      <doi>10.18653/v1/2021.sdp-1.11</doi>
      <bibkey>sefid-etal-2021-extractive</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>L</fixed-case>ong<fixed-case>S</fixed-case>umm 2021: Session based automatic summarization model for scientific document</title>
      <author><first>Senci</first><last>Ying</last></author>
      <author><first>Zheng</first><last>Yan Zhao</last></author>
      <author><first>Wuhe</first><last>Zou</last></author>
      <pages>97–102</pages>
      <abstract>Most summarization task focuses on generating relatively short summaries. Such a length constraint might not be appropriate when summarizing scientific work. The LongSumm task needs participants generate long summary for scientific document. This task usual can be solved by language model. But an important problem is that model like BERT is limit to memory, and can not deal with a long input like a document. Also generate a long output is hard. In this paper, we propose a session based automatic summarization model(SBAS) which using a session and ensemble mechanism to generate long summary. And our model achieves the best performance in the LongSumm task.</abstract>
      <url hash="e13f7cbf">2021.sdp-1.12</url>
      <attachment type="OptionalSupplementaryCode" hash="7d3b2a40">2021.sdp-1.12.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.sdp-1.12</doi>
      <bibkey>ying-etal-2021-longsumm</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>CNLP</fixed-case>-<fixed-case>NITS</fixed-case> @ <fixed-case>L</fixed-case>ong<fixed-case>S</fixed-case>umm 2021: <fixed-case>T</fixed-case>ext<fixed-case>R</fixed-case>ank Variant for Generating Long Summaries</title>
      <author><first>Darsh</first><last>Kaushik</last></author>
      <author><first>Abdullah Faiz Ur Rahman</first><last>Khilji</last></author>
      <author><first>Utkarsh</first><last>Sinha</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <pages>103–109</pages>
      <abstract>The huge influx of published papers in the field of machine learning makes the task of summarization of scholarly documents vital, not just to eliminate the redundancy but also to provide a complete and satisfying crux of the content. We participated in LongSumm 2021: The <tex-math>2^{nd}</tex-math> Shared Task on Generating Long Summaries for scientific documents, where the task is to generate long summaries for scientific papers provided by the organizers. This paper discusses our extractive summarization approach to solve the task. We used TextRank algorithm with the BM25 score as a similarity function. Even after being a graph-based ranking algorithm that does not require any learning, TextRank produced pretty decent results with minimal compute power and time. We attained <tex-math>3^{rd}</tex-math> rank according to ROUGE-1 scores (0.5131 for F-measure and 0.5271 for recall) and performed decently as shown by the ROUGE-2 scores.</abstract>
      <url hash="02632e8d">2021.sdp-1.13</url>
      <doi>10.18653/v1/2021.sdp-1.13</doi>
      <bibkey>kaushik-etal-2021-cnlp</bibkey>
    </paper>
    <paper id="14">
      <title>Unsupervised document summarization using pre-trained sentence embeddings and graph centrality</title>
      <author><first>Juan</first><last>Ramirez-Orta</last></author>
      <author><first>Evangelos</first><last>Milios</last></author>
      <pages>110–115</pages>
      <abstract>This paper describes our submission for the LongSumm task in SDP 2021. We propose a method for incorporating sentence embeddings produced by deep language models into extractive summarization techniques based on graph centrality in an unsupervised manner.The proposed method is simple, fast, can summarize any kind of document of any size and can satisfy any length constraints for the summaries produced. The method offers competitive performance to more sophisticated supervised methods and can serve as a proxy for abstractive summarization techniques</abstract>
      <url hash="b96b587b">2021.sdp-1.14</url>
      <doi>10.18653/v1/2021.sdp-1.14</doi>
      <bibkey>ramirez-orta-milios-2021-unsupervised</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>QMUL</fixed-case>-<fixed-case>SDS</fixed-case> at <fixed-case>SCIVER</fixed-case>: Step-by-Step Binary Classification for Scientific Claim Verification</title>
      <author><first>Xia</first><last>Zeng</last></author>
      <author><first>Arkaitz</first><last>Zubiaga</last></author>
      <pages>116–123</pages>
      <abstract>Scientific claim verification is a unique challenge that is attracting increasing interest. The SCIVER shared task offers a benchmark scenario to test and compare claim verification approaches by participating teams and consists in three steps: relevant abstract selection, rationale selection and label prediction. In this paper, we present team QMUL-SDS’s participation in the shared task. We propose an approach that performs scientific claim verification by doing binary classifications step-by-step. We trained a BioBERT-large classifier to select abstracts based on pairwise relevance assessments for each &lt;claim, title of the abstract&gt; and continued to train it to select rationales out of each retrieved abstract based on &lt;claim, sentence&gt;. We then propose a two-step setting for label prediction, i.e. first predicting “NOT_ENOUGH_INFO” or “ENOUGH_INFO”, then label those marked as “ENOUGH_INFO” as either “SUPPORT” or “CONTRADICT”. Compared to the baseline system, we achieve substantial improvements on the dev set. As a result, our team is the No. 4 team on the leaderboard.</abstract>
      <url hash="c8ee47ef">2021.sdp-1.15</url>
      <doi>10.18653/v1/2021.sdp-1.15</doi>
      <bibkey>zeng-zubiaga-2021-qmul</bibkey>
    </paper>
  </volume>
</collection>
