<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.fieldmatters">
  <volume id="1" ingest-date="2025-08-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on NLP Applications to Field Linguistics</booktitle>
      <editor><first>Éric</first><last>Le Ferrand</last></editor>
      <editor><first>Elena</first><last>Klyachko</last></editor>
      <editor><first>Anna</first><last>Postnikova</last></editor>
      <editor><first>Tatiana</first><last>Shavrina</last></editor>
      <editor><first>Oleg</first><last>Serikov</last></editor>
      <editor><first>Ekaterina</first><last>Voloshina</last></editor>
      <editor><first>Ekaterina</first><last>Vylomova</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="eadaab11">2025.fieldmatters-1</url>
      <venue>fieldmatters</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-282-4</isbn>
    </meta>
    <frontmatter>
      <url hash="620abaea">2025.fieldmatters-1.0</url>
      <bibkey>fieldmatters-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Automatic Phone Alignment of Code-switched <fixed-case>U</fixed-case>rum–<fixed-case>R</fixed-case>ussian Field Data</title>
      <author><first>Emily</first><last>Ahn</last></author>
      <author><first>Eleanor</first><last>Chodroff</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Gina-Anne</first><last>Levow</last><affiliation>University of Washington</affiliation></author>
      <pages>1-14</pages>
      <abstract>Code-switching, using multiple languages in a single utterance, is a common means of communication.In the language documentation process, speakers may code-switch between the target language and a language of broader communication; however, how to handle this mixed speech data is not always clearly addressed for speech research and specifically for a corpus phonetics pipeline.This paper investigates best practices for conducting phone-level forced alignment of code-switched field data using the Urum speech dataset from DoReCo. This dataset comprises 117 minutes of narrative utterances, of which 42% contain code-switched Urum–Russian speech.We demonstrate that the inclusion of Russian speech and Russian pretrained acoustic models can aid the alignment of Urum phones.Beyond using boundary alignment precision and accuracy metrics, we also discovered that the method of acoustic modeling impacted a downstream corpus phonetics investigation of code-switched Urum–Russian.</abstract>
      <url hash="1dcd4a49">2025.fieldmatters-1.1</url>
      <bibkey>ahn-etal-2025-automatic</bibkey>
    </paper>
    <paper id="2">
      <title>What Causes Knowledge Loss in Multilingual Language Models?</title>
      <author><first>Maria</first><last>Khelli</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last><affiliation>Cohere</affiliation></author>
      <author><first>Ayu</first><last>Purwarianti</last><affiliation>Institut Teknologi Bandung</affiliation></author>
      <author><first>Genta Indra</first><last>Winata</last><affiliation>Capital One</affiliation></author>
      <pages>15-25</pages>
      <abstract>Cross-lingual transfer in natural language processing (NLP) models enhances multilingual performance by leveraging shared linguistic knowledge. However, traditional methods that process all data simultaneously often fail to mimic real-world scenarios, leading to challenges like catastrophic forgetting, where fine-tuning on new tasks degrades performance on previously learned ones. Our study explores this issue in multilingual contexts, focusing on linguistic differences affecting representational learning rather than just model parameters. We experiment with 52 languages using LoRA adapters of varying ranks to evaluate non-shared, partially shared, and fully shared parameters. Our aim is to see if parameter sharing through adapters can mitigate forgetting while preserving prior knowledge. We find that languages using non-Latin scripts are more susceptible to catastrophic forgetting, whereas those written in Latin script facilitate more effective cross-lingual transfer.</abstract>
      <url hash="25e8e394">2025.fieldmatters-1.2</url>
      <bibkey>khelli-etal-2025-causes</bibkey>
    </paper>
    <paper id="3">
      <title>Breaking the Transcription Bottleneck: Fine-tuning <fixed-case>ASR</fixed-case> Models for Extremely Low-Resource Fieldwork Languages</title>
      <author><first>Siyu</first><last>Liang</last></author>
      <author><first>Gina-Anne</first><last>Levow</last><affiliation>University of Washington</affiliation></author>
      <pages>26-37</pages>
      <abstract>The development of Automatic Speech Recognition (ASR) has yielded impressive results, but its use in linguistic fieldwork remains limited. Recordings collected in fieldwork contexts present unique challenges, including spontaneous speech, environmental noise, and severely constrained datasets from under-documented languages. In this paper, we benchmark the performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five typologically diverse low-resource languages with control of training data duration. Our findings show that MMS is best suited when extremely small amounts of training data are available, whereas XLS-R shows parity performance once training data exceed one hour. We provide linguistically grounded analysis for further provide insights towards practical guidelines for field linguists, highlighting reproducible ASR adaptation approaches to mitigate the transcription bottleneck in language documentation.</abstract>
      <url hash="b9881306">2025.fieldmatters-1.3</url>
      <bibkey>liang-levow-2025-breaking</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>K</fixed-case>az<fixed-case>B</fixed-case>ench-<fixed-case>KK</fixed-case>: A Cultural-Knowledge Benchmark for <fixed-case>K</fixed-case>azakh</title>
      <author><first>Sanzhar</first><last>Umbet</last><affiliation>Horde Research</affiliation></author>
      <author><first>Sanzhar</first><last>Murzakhmetov</last><affiliation>Horde Research</affiliation></author>
      <author><first>Beksultan</first><last>Sagyndyk</last><affiliation>Horde Research</affiliation></author>
      <author><first>Kirill</first><last>Yakunin</last><affiliation>Horde Research</affiliation></author>
      <author><first>Timur</first><last>Akishev</last><affiliation>KIMEP University</affiliation></author>
      <author><first>Pavel</first><last>Zubitski</last><affiliation>Horde Research</affiliation></author>
      <pages>38-57</pages>
      <abstract>We introduce KazBench-KK, a comprehensive 7,111-question multiple-choice benchmark designed to assess large language models’ understanding of culturally grounded Kazakh knowledge. By combining expert-curated topics with LLM-assisted web mining, we create a diverse dataset spanning 17 culturally salient domains, including pastoral traditions, social hierarchies, and contemporary politics. Beyond evaluation, KazBench-KK serves as a practical tool for field linguists, enabling rapid lexical elicitation, glossing, and topic prioritization. Our benchmarking of various open-source LLMs reveals that reinforcement-tuned models outperform others, but smaller, domain-focused fine-tunes can rival larger models in specific cultural contexts.</abstract>
      <url hash="1dc92a02">2025.fieldmatters-1.4</url>
      <bibkey>umbet-etal-2025-kazbench</bibkey>
    </paper>
    <paper id="5">
      <title>Searchable Language Documentation Corpora: <fixed-case>D</fixed-case>o<fixed-case>R</fixed-case>e<fixed-case>C</fixed-case>o meets <fixed-case>TEITOK</fixed-case></title>
      <author><first>Maarten</first><last>Janssen</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Frank</first><last>Seifart</last><affiliation>CNRS</affiliation></author>
      <pages>58-64</pages>
      <abstract>In this paper, we describe a newly created searchable interface for DoReCo, a database that contains spoken corpora from a world-wide sample of 53, mostly lesser described languages, with audio, transcription, translation, and - for most languages - interlinear morpheme glosses. Until now, DoReCo data were available for download via the DoReCo website and via the Nakala repository in a number of different formats, but not directly accessible online. We created a graphical interface to view, listen to, and search these data online, providing direct and intuitive access for linguists and laypeople. The new interface uses the TEITOK corpus infrastructure to provide a number of different visualizations on individual documents in DoReCo and provides a search interface to perform detailed searches on individual languages. The use of TEITOK also enables the corpus for use with NLP pipelines, either using the data to train NLP models, or to use NLP models to enrich the data.</abstract>
      <url hash="e8d21e79">2025.fieldmatters-1.5</url>
      <bibkey>janssen-seifart-2025-searchable</bibkey>
    </paper>
    <paper id="6">
      <title>A Practical Tool to Help Automate Interlinear Glossing: a Study on Mukrī <fixed-case>K</fixed-case>urdish</title>
      <author><first>Hiwa</first><last>Asadpour</last><affiliation>Johann Wolfgang Goethe Universität Frankfurt am Main</affiliation></author>
      <author><first>Shu</first><last>Okabe</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>65-75</pages>
      <abstract>Interlinear gloss generation aims to predict linguistic annotations (gloss) for a sentence in a language that is usually under ongoing documentation. Such output is a first draft for the linguist to work with and should reduce the manual workload.This article studies a simple glossing pipeline based on a Conditional Random Field and applies it to a small fieldwork corpus in Mukrī Kurdish, a variety of Central Kurdish.We mainly focus on making the tool as accessible as possible for field linguists, so it can run on standard computers without the need for GPUs. Our pipeline predicts common grammatical patterns robustly and, more generally, frequent combinations of morphemes and glosses. Although more advanced neural models do reach better results, our feature-based system still manages to be competitive and to provide interpretability.To foster further collaboration between field linguistics and NLP, we also provide some recommendations regarding documentation endeavours and release our pipeline code alongside.</abstract>
      <url hash="c1d01d7f">2025.fieldmatters-1.6</url>
      <bibkey>asadpour-etal-2025-practical</bibkey>
    </paper>
    <paper id="7">
      <title>Field to Model: Pairing Community Data Collection with Scalable <fixed-case>NLP</fixed-case> through the <fixed-case>L</fixed-case>i<fixed-case>FE</fixed-case> Suite</title>
      <author><first>Karthick Narayanan</first><last>R</last><affiliation>UnReaL-TecE</affiliation></author>
      <author><first>Siddharth</first><last>Singh</last></author>
      <author><first>Saurabh</first><last>Singh</last><affiliation>Unreal Tece LLP</affiliation></author>
      <author><first>Aryan</first><last>Mathur</last></author>
      <author><first>Ritesh</first><last>Kumar</last><affiliation>Dr. Bhimrao Ambedkar University</affiliation></author>
      <author><first>Shyam</first><last>Ratan</last></author>
      <author><first>Bornini</first><last>Lahiri</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Benu</first><last>Pareek</last><affiliation>NA</affiliation></author>
      <author><first>Neerav</first><last>Mathur</last><affiliation>Dr Bhimrao Ambedkar University, Agra</affiliation></author>
      <author><first>Amalesh</first><last>Gope</last><affiliation>NA</affiliation></author>
      <author><first>Meiraba</first><last>Takhellambam</last><affiliation>Manipur University</affiliation></author>
      <author><first>Yogesh</first><last>Dawer</last><affiliation>NA</affiliation></author>
      <pages>76-84</pages>
      <abstract>We present LiFE Suite as a “Field-to-Model” pipeline, designed to bridge community-centred data collection with scalable language model development. This paper describes the various tools integrated into the LiFE Suite that make this unified pipeline possible. Atekho, a mobile-first data collection platform, is designed to empower communities to assert their rights over their data. MATra-Lab, a web-based data processing and annotation tool, supports the management of field data and the creation of NLP-ready datasets with support from existing state-of-the-art NLP models. LiFE Model Studio, built on top of Hugging Face AutoTrain, offers a no-code solution for building scalable language models using the field data. This end-to-end integration ensures that every dataset collected in the field retains its linguistic, cultural, and metadata context, all the way through to deployable AI models and archive-ready datasets.</abstract>
      <url hash="96198518">2025.fieldmatters-1.7</url>
      <bibkey>r-etal-2025-field</bibkey>
    </paper>
    <paper id="8">
      <title>Low-resource Buryat-<fixed-case>R</fixed-case>ussian neural machine translation</title>
      <author><first>Dari</first><last>Baturova</last><affiliation>Siberian Networks</affiliation></author>
      <author><first>Sarana</first><last>Abidueva</last><affiliation>Asian Pacific Bank</affiliation></author>
      <author><first>Dmitrii</first><last>Lichko</last><affiliation>Moscow State Institute of Steel and Alloys</affiliation></author>
      <author><first>Ivan</first><last>Bondarenko</last><affiliation>Novosibirsk State University</affiliation></author>
      <pages>85-93</pages>
      <abstract>This paper presents a study on the development of a neural machine translation (NMT) system for the Russian-Buryat language pair, focusing on addressing the challenges of low-resource translation.We also present a parallel corpus, constructed by processing existing texts and organizing the translation process, supplemented by data augmentation techniques to enhance model training. We managed to achieve BLEU score of 20 and 35 for translation to Buryat andRussian respectively. Native speakers have evaluated the translations as acceptable.Future directions include expanding and cleaning the dataset, improving model training techniques, and exploring dialectal variations within the Buryat language.</abstract>
      <url hash="4bf07132">2025.fieldmatters-1.8</url>
      <bibkey>baturova-etal-2025-low</bibkey>
    </paper>
  </volume>
</collection>
