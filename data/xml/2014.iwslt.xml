<?xml version='1.0' encoding='UTF-8'?>
<collection id="2014.iwslt">
  <volume id="keynotes" ingest-date="2021-09-06">
    <meta>
      <booktitle>Proceedings of the 11th International Workshop on Spoken Language Translation: Keynotes</booktitle>
      <address>Lake Tahoe, California</address>
      <month>December 4-5</month>
      <year>2014</year>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <editor><first>Sebastian</first><last>Stüker</last></editor>
      <editor><first>François</first><last>Yvon</last></editor>
      <venue>iwslt</venue>
    </meta>
    <paper id="1">
      <title>Speech translation for everyone – breaking down the barriers</title>
      <author><first>Arul</first><last>Menezes</last></author>
      <url hash="cb2784fb">2014.iwslt-keynotes.1</url>
      <bibkey>menezes-2014-speech</bibkey>
    </paper>
  </volume>
  <volume id="evaluation" ingest-date="2021-08-08">
    <meta>
      <booktitle>Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign</booktitle>
      <address>Lake Tahoe, California</address>
      <month>December 4-5</month>
      <year>2014</year>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <editor><first>Sebastian</first><last>Stüker</last></editor>
      <editor><first>François</first><last>Yvon</last></editor>
      <venue>iwslt</venue>
    </meta>
    <frontmatter>
      <url hash="57513364">2014.iwslt-evaluation.0</url>
      <bibkey>iwslt-2014</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Report on the 11th <fixed-case>IWSLT</fixed-case> evaluation campaign</title>
      <author><first>Mauro</first><last>Cettolo</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <pages>2-17</pages>
      <url hash="81bd4bce">2014.iwslt-evaluation.1</url>
      <abstract>The paper overviews the 11th evaluation campaign organized by the IWSLT workshop. The 2014 evaluation offered multiple tracks on lecture transcription and translation based on the TED Talks corpus. In particular, this year IWSLT included three automatic speech recognition tracks, on English, German and Italian, five speech translation tracks, from English to French, English to German, German to English, English to Italian, and Italian to English, and five text translation track, also from English to French, English to German, German to English, English to Italian, and Italian to English. In addition to the official tracks, speech and text translation optional tracks were offered, globally involving 12 other languages: Arabic, Spanish, Portuguese (B), Hebrew, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, Russian. Overall, 21 teams participated in the evaluation, for a total of 76 primary runs submitted. Participants were also asked to submit runs on the 2013 test set (progress test set), in order to measure the progress of systems with respect to the previous year. All runs were evaluated with objective metrics, and submissions for two of the official text translation tracks were also evaluated with human post-editing.</abstract>
      <bibkey>cettolo-etal-2014-report</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>FBK</fixed-case> @ <fixed-case>IWSLT</fixed-case> 2014 – <fixed-case>ASR</fixed-case> track</title>
      <author><first>B.</first><last>BabaAli</last></author>
      <author><first>R.</first><last>Serizel</last></author>
      <author><first>S.</first><last>Jalalvand</last></author>
      <author><first>R.</first><last>Gretter</last></author>
      <author><first>D.</first><last>Giuliani</last></author>
      <pages>18-25</pages>
      <url hash="8e6931d3">2014.iwslt-evaluation.2</url>
      <abstract>This paper reports on the participation of FBK in the IWSLT 2014 evaluation campaign for Automatic Speech Recognition (ASR), which focused on the transcription of TED talks. The outputs of primary and contrastive systems were submitted for three languages, namely English, German and Italian. Most effort went into the development of the English transcription system. The primary system is based on the ROVER combination of the output of 5 transcription subsystems which are all based on the Deep Neural Network Hidden Markov Model (DNN-HMM) hybrid. Before combination, word lattices generated by each sub-system are rescored using an efficient interpolation of 4-gram and Recurrent Neural Network (RNN) language models. The primary system achieves a Word Error Rate (WER) of 14.7% and 11.4% on the 2013 and 2014 official IWSLT English test sets, respectively. The subspace Gaussian mixture model (SGMM) system developed for German achieves 39.5% WER on the 2014 IWSLT German test sets. For Italian, the primary transcription system was based on hidden Markov models and achieves 23.8% WER on the 2014 IWSLT Italian test set.</abstract>
      <bibkey>babaali-etal-2014-fbk</bibkey>
    </paper>
    <paper id="3">
      <title>The <fixed-case>UEDIN</fixed-case> <fixed-case>ASR</fixed-case> systems for the <fixed-case>IWSLT</fixed-case> 2014 evaluation</title>
      <author><first>Peter</first><last>Bell</last></author>
      <author><first>Pawel</first><last>Swietojanski</last></author>
      <author><first>Joris</first><last>Driesen</last></author>
      <author><first>Mark</first><last>Sinclair</last></author>
      <author><first>Fergus</first><last>McInnes</last></author>
      <author><first>Steve</first><last>Renals</last></author>
      <pages>26-33</pages>
      <url hash="804c5357">2014.iwslt-evaluation.3</url>
      <abstract>This paper describes the University of Edinburgh (UEDIN) ASR systems for the 2014 IWSLT Evaluation. Notable features of the English system include deep neural network acoustic models in both tandem and hybrid configuration with the use of multi-level adaptive networks, LHUC adaptation and Maxout units. The German system includes lightly supervised training and a new method for dictionary generation. Our voice activity detection system now uses a semi-Markov model to incorporate a prior on utterance lengths. There are improvements of up to 30% relative WER on the tst2013 English test set.</abstract>
      <bibkey>bell-etal-2014-uedin</bibkey>
    </paper>
    <paper id="4">
      <title>Improving <fixed-case>MEANT</fixed-case> based semantically tuned <fixed-case>SMT</fixed-case></title>
      <author><first>Meriem</first><last>Beloucif</last></author>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Dekai</first><last>Wu</last></author>
      <pages>34-41</pages>
      <url hash="7bb73ba0">2014.iwslt-evaluation.4</url>
      <abstract>We discuss various improvements to our MEANT tuned system, previously presented at IWSLT 2013. In our 2014 system, we incorporate this year’s improved version of MEANT, improved Chinese word segmentation, Chinese named entity recognition and dedicated proper name translation, and number expression handling. This results in a significant performance jump compared to last year’s system. We also ran preliminary experiments on tuning to IMEANT, our new ITG based variant of MEANT. The performance of tuning to IMEANT is comparable to tuning on MEANT (differences are statistically insignificant). We are presently investigating if tuning on IMEANT can produce even better results, since IMEANT was actually shown to correlate with human adequacy judgment more closely than MEANT. Finally, we ran experiments applying our new architectural improvements to a contrastive system tuned to BLEU. We observed a slightly higher jump in comparison to last year, possibly due to mismatches of MEANT’s similarity models to our new entity handling.</abstract>
      <bibkey>beloucif-etal-2014-improving</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>FBK</fixed-case>’s machine translation and speech translation systems for the <fixed-case>IWSLT</fixed-case> 2014 evaluation campaign</title>
      <author><first>Nicola</first><last>Bertoldi</last></author>
      <author><first>Prashanu</first><last>Mathur</last></author>
      <author><first>Nicolas</first><last>Ruiz</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <pages>42-48</pages>
      <url hash="c8564adc">2014.iwslt-evaluation.5</url>
      <abstract>This paper describes the systems submitted by FBK for the MT and SLT tracks of IWSLT 2014. We participated in the English-French and German-English machine translation tasks, as well as the English-French speech translation task. We report improvements in our English-French MT systems over last year’s baselines, largely due to improved techniques of combining translation and language models, and using huge language models. For our German-English system, we experimented with a novel domain adaptation technique. For both language pairs we also applied a novel word triggerbased model which shows slight improvements on EnglishFrench and German-English systems. Our English-French SLT system utilizes MT-based punctuation insertion, recasing, and ASR-like synthesized MT training data.</abstract>
      <bibkey>bertoldi-etal-2014-fbks</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>E</fixed-case>dinburgh <fixed-case>SLT</fixed-case> and <fixed-case>MT</fixed-case> system description for the <fixed-case>IWSLT</fixed-case> 2014 evaluation</title>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Matthias</first><last>Huck</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>49-56</pages>
      <url hash="90fc26b9">2014.iwslt-evaluation.6</url>
      <abstract>This paper describes the University of Edinburgh’s spoken language translation (SLT) and machine translation (MT) systems for the IWSLT 2014 evaluation campaign. In the SLT track, we participated in the German↔English and English→French tasks. In the MT track, we participated in the German↔English, English→French, Arabic↔English, Farsi→English, Hebrew→English, Spanish↔English, and Portuguese-Brazil↔English tasks. For our SLT submissions, we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination.</abstract>
      <bibkey>birch-etal-2014-edinburgh</bibkey>
    </paper>
    <paper id="7">
      <title>Combined spoken language translation</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Joern</first><last>Wuebker</last></author>
      <author><first>Stephan</first><last>Peitz</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <author><first>Matthias</first><last>Huck</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Mohammed</first><last>Mediani</last></author>
      <author><first>Isabel</first><last>Slawik</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Eunach</first><last>Cho</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <author><first>Nicola</first><last>Bertoldi</last></author>
      <author><first>Mauro</first><last>Cettolo</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <pages>57-64</pages>
      <url hash="4b92bfd7">2014.iwslt-evaluation.7</url>
      <abstract>EU-BRIDGE is a European research project which is aimed at developing innovative speech translation technology. One of the collaborative efforts within EU-BRIDGE is to produce joint submissions of up to four different partners to the evaluation campaign at the 2014 International Workshop on Spoken Language Translation (IWSLT). We submitted combined translations to the German→English spoken language translation (SLT) track as well as to the German→English, English→German and English→French machine translation (MT) tracks. In this paper, we present the techniques which were applied by the different individual translation systems of RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show the combination approach developed at RWTH Aachen University which combined the individual systems. The consensus translations yield empirical gains of up to 2.3 points in BLEU and 1.2 points in TER compared to the best individual system.</abstract>
      <bibkey>freitag-etal-2014-combined</bibkey>
    </paper>
    <paper id="8">
      <title>The <fixed-case>MITLL</fixed-case>-<fixed-case>AFRL</fixed-case> <fixed-case>IWSLT</fixed-case> 2014 <fixed-case>MT</fixed-case> system</title>
      <author><first>Michaeel</first><last>Kazi</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Brian</first><last>Thompson</last></author>
      <author><first>Jessica</first><last>Ray</last></author>
      <author><first>Michael</first><last>Coury</last></author>
      <author><first>Tim</first><last>Anderson</last></author>
      <author><first>Grant</first><last>Erdmann</last></author>
      <author><first>Jeremy</first><last>Gwinnup</last></author>
      <author><first>Katherine</first><last>Young</last></author>
      <author><first>Brian</first><last>Ore</last></author>
      <author><first>Michael</first><last>Hutt</last></author>
      <pages>65-72</pages>
      <url hash="ac39cb20">2014.iwslt-evaluation.8</url>
      <abstract>This report summarizes the MITLL-AFRL MT and ASR systems and the experiments run using them during the 2014 IWSLT evaluation campaign. Our MT system is much improved over last year, owing to integration of techniques such as PRO and DREM optimization, factored language models, neural network joint model rescoring, multiple phrase tables, and development set creation. We focused our eforts this year on the tasks of translating from Arabic, Russian, Chinese, and Farsi into English, as well as translating from English to French. ASR performance also improved, partly due to increased eforts with deep neural networks for hybrid and tandem systems. Work focused on both the English and Italian ASR tasks.</abstract>
      <bibkey>kazi-etal-2014-mitll</bibkey>
    </paper>
    <paper id="9">
      <title>The 2014 <fixed-case>KIT</fixed-case> <fixed-case>IWSLT</fixed-case> speech-to-text systems for <fixed-case>E</fixed-case>nglish, <fixed-case>G</fixed-case>erman and <fixed-case>I</fixed-case>talian</title>
      <author><first>Kevin</first><last>Kilgour</last></author>
      <author><first>Michael</first><last>Heck</last></author>
      <author><first>Markus</first><last>Müller</last></author>
      <author><first>Matthias</first><last>Sperber</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <pages>73-79</pages>
      <url hash="1d71b80b">2014.iwslt-evaluation.9</url>
      <abstract>This paper describes our German, Italian and English Speech-to-Text (STT) systems for the 2014 IWSLT TED ASR track. Our setup uses ROVER and confusion network combination from various subsystems to achieve a good overall performance. The individual subsystems are built by using different front-ends, (e.g., MVDR-MFCC or lMel), acoustic models (GMM or modular DNN) and phone sets and by training on various subsets of the training data. Decoding is performed in two stages, where the GMM systems are adapted in an unsupervised manner on the combination of the first stage outputs using VTLN, MLLR, and cMLLR. The combination setup produces a final hypothesis that has a significantly lower WER than any of the individual subsystems.</abstract>
      <bibkey>kilgour-etal-2014-2014</bibkey>
    </paper>
    <paper id="10">
      <title>A topic-based approach for post-processing correction of automatic translations</title>
      <author><first>Mohamed</first><last>Morchid</last></author>
      <author><first>Stéphane</first><last>Huet</last></author>
      <author><first>Richard</first><last>Dufour</last></author>
      <pages>80-85</pages>
      <url hash="fe72c4f4">2014.iwslt-evaluation.10</url>
      <abstract>We present the LIA systems for the machine translation evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2014 for the English-to-Slovene and English-to-Polish translation tasks. The proposed approach takes into account word context; first, it maps sentences into a latent Dirichlet allocation (LDA) topic space, then it chooses from this space words that are thematically and grammatically close to mistranslated words. This original post-processing approach is compared with a factored translation system built with MOSES. While this postprocessing method does not allow us to achieve better results than a state-of-the-art system, this should be an interesting way to explore, for example by adding this topic space information at an early stage in the translation process.</abstract>
      <bibkey>morchid-etal-2014-topic</bibkey>
    </paper>
    <paper id="11">
      <title>The <fixed-case>USFD</fixed-case> <fixed-case>SLT</fixed-case> system for <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Raymond W. M.</first><last>Ng</last></author>
      <author><first>Mortaza</first><last>Doulaty</last></author>
      <author><first>Rama</first><last>Doddipatla</last></author>
      <author><first>Wilker</first><last>Aziz</last></author>
      <author><first>Kashif</first><last>Shah</last></author>
      <author><first>Oscar</first><last>Saz</last></author>
      <author><first>Madina</first><last>Hasan</last></author>
      <author><first>Ghada</first><last>AlHaribi</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Thomas</first><last>Hain</last></author>
      <pages>86-91</pages>
      <url hash="ec704cce">2014.iwslt-evaluation.11</url>
      <abstract>The University of Sheffield (USFD) participated in the International Workshop for Spoken Language Translation (IWSLT) in 2014. In this paper, we will introduce the USFD SLT system for IWSLT. Automatic speech recognition (ASR) is achieved by two multi-pass deep neural network systems with adaptation and rescoring techniques. Machine translation (MT) is achieved by a phrase-based system. The USFD primary system incorporates state-of-the-art ASR and MT techniques and gives a BLEU score of 23.45 and 14.75 on the English-to-French and English-to-German speech-to-text translation task with the IWSLT 2014 data. The USFD contrastive systems explore the integration of ASR and MT by using a quality estimation system to rescore the ASR outputs, optimising towards better translation. This gives a further 0.54 and 0.26 BLEU improvement respectively on the IWSLT 2012 and 2014 evaluation data.</abstract>
      <bibkey>ng-etal-2014-usfd</bibkey>
    </paper>
    <paper id="12">
      <title>The speech recognition systems of <fixed-case>IOIT</fixed-case> for <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Quoc Bao</first><last>Nguyen</last></author>
      <author><first>Tat Thang</first><last>Vu</last></author>
      <author><first>Chi Mai</first><last>Luong</last></author>
      <pages>92-95</pages>
      <url hash="c82dccc7">2014.iwslt-evaluation.12</url>
      <abstract>This paper describes the speech recognition systems of IOIT for IWSLT 2014 TED ASR track. This year, we focus on improving acoustic model for the systems using two main approaches of deep neural network which are hybrid and bottleneck feature systems. These two subsystems are combined using lattice Minimum Bayes-Risk decoding. On the 2013 evaluations set, which serves as a progress test set, we were able to reduce the word error rate of our transcription systems from 27.2% to 24.0%, a relative reduction of 11.7%.</abstract>
      <bibkey>nguyen-etal-2014-speech</bibkey>
    </paper>
    <paper id="13">
      <title>Phrase-based language modelling for statistical machine translation</title>
      <author><first>Achraf Ben</first><last>Romdhane</last></author>
      <author><first>Salma</first><last>Jamoussi</last></author>
      <author><first>Abdelmajid Ben</first><last>Hamadou</last></author>
      <author><first>Kamel</first><last>Smaïli</last></author>
      <pages>96-99</pages>
      <url hash="26576ce5">2014.iwslt-evaluation.13</url>
      <abstract>In this paper, we present our submitted MT system for the IWSLT2014 Evaluation Campaign. We participated in the English-French translation task. In this article we focus on one of the most important component of SMT: the language model. The idea is to use a phrase-based language model. For that, sequences from the source and the target language models are retrieved and used to calculate a phrase n-gram language model. These phrases are used to rewrite the parallel corpus which is then used to calculate a new translation model.</abstract>
      <bibkey>romdhane-etal-2014-phrase</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>LIUM</fixed-case> <fixed-case>E</fixed-case>nglish-to-<fixed-case>F</fixed-case>rench spoken language translation system and the Vecsys/<fixed-case>LIUM</fixed-case> automatic speech recognition system for <fixed-case>I</fixed-case>talian language for <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Anthony</first><last>Rousseau</last></author>
      <author><first>Loïc</first><last>Barrault</last></author>
      <author><first>Paul</first><last>Deléglise</last></author>
      <author><first>Yannick</first><last>Estève</last></author>
      <author><first>Holger</first><last>Schwenk</last></author>
      <author><first>Samir</first><last>Bennacef</last></author>
      <author><first>Armando</first><last>Muscariello</last></author>
      <author><first>Stephan</first><last>Vanni</last></author>
      <pages>100-105</pages>
      <url hash="350a9c42">2014.iwslt-evaluation.14</url>
      <abstract>This paper describes the Spoken Language Translation system developed by the LIUM for the IWSLT 2014 evaluation campaign. We participated in two of the proposed tasks: (i) the Automatic Speech Recognition task (ASR) in two languages, Italian with the Vecsys company, and English alone, (ii) the English to French Spoken Language Translation task (SLT). We present the approaches and specificities found in our systems, as well as the results from the evaluation campaign.</abstract>
      <bibkey>rousseau-etal-2014-lium</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>LIMSI</fixed-case> <fixed-case>E</fixed-case>nglish-<fixed-case>F</fixed-case>rench speech translation system</title>
      <author><first>Natalia</first><last>Segal</last></author>
      <author><first>Hélène</first><last>Bonneau-Maynard</last></author>
      <author><first>Quoc Khanh</first><last>Do</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <author><first>Jean-Luc</first><last>Gauvain</last></author>
      <author><first>Lori</first><last>Lamel</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>106-112</pages>
      <url hash="74b78226">2014.iwslt-evaluation.15</url>
      <abstract>This paper documents the systems developed by LIMSI for the IWSLT 2014 speech translation task (English→French). The main objective of this participation was twofold: adapting different components of the ASR baseline system to the peculiarities of TED talks and improving the machine translation quality on the automatic speech recognition output data. For the latter task, various techniques have been considered: punctuation and number normalization, adaptation to ASR errors, as well as the use of structured output layer neural network models for speech data.</abstract>
      <bibkey>segal-etal-2014-limsi</bibkey>
    </paper>
    <paper id="16">
      <title>The <fixed-case>NCT</fixed-case> <fixed-case>ASR</fixed-case> system for <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Peng</first><last>Shen</last></author>
      <author><first>Yugang</first><last>Lu</last></author>
      <author><first>Xinhui</first><last>Hu</last></author>
      <author><first>Naoyuki</first><last>Kanda</last></author>
      <author><first>Masahiro</first><last>Saiko</last></author>
      <author><first>Chiori</first><last>Hori</last></author>
      <pages>113-118</pages>
      <url hash="a695d955">2014.iwslt-evaluation.16</url>
      <abstract>This paper describes our automatic speech recognition system for IWSLT2014 evaluation campaign. The system is based on weighted finite-state transducers and a combination of multiple subsystems which consists of four types of acoustic feature sets, four types of acoustic models, and N-gram and recurrent neural network language models. Compared with our system used in last year, we added additional subsystems based on deep neural network modeling on filter bank feature and convolutional deep neural network modeling on filter bank feature with tonal features. In addition, modifications and improvements on automatic acoustic segmentation and deep neural network speaker adaptation were applied. Compared with our last year’s system on speech recognition experiments, our new system achieved 21.5% relative improvement on word error rate on the 2013 English test data set.</abstract>
      <bibkey>shen-etal-2014-nct</bibkey>
    </paper>
    <paper id="17">
      <title>The <fixed-case>KIT</fixed-case> translation systems for <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Isabel</first><last>Slawik</last></author>
      <author><first>Mohammed</first><last>Mediani</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Yuqi</first><last>Zhang</last></author>
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>Teresa</first><last>Herrmann</last></author>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <pages>119-126</pages>
      <url hash="fd7fed03">2014.iwslt-evaluation.17</url>
      <abstract>In this paper, we present the KIT systems participating in the TED translation tasks of the IWSLT 2014 machine translation evaluation. We submitted phrase-based translation systems for all three official directions, namely English→German, German→English, and English→French, as well as for the optional directions English→Chinese and English→Arabic. For the official directions we built systems both for the machine translation as well as the spoken language translation track. This year we improved our systems’ performance over last year through n-best list rescoring using neural network-based translation and language models and novel preordering rules based on tree information of multiple syntactic levels. Furthermore, we could successfully apply a novel phrase extraction algorithm and transliteration of unknown words for Arabic. We also submitted a contrastive system for German→English built with stemmed German adjectives. For the SLT tracks, we used a monolingual translation system to translate the lowercased ASR hypotheses with all punctuation stripped to truecased, punctuated output as a preprocessing step to our usual translation system.</abstract>
      <bibkey>slawik-etal-2014-kit</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>NTT</fixed-case>-<fixed-case>NAIST</fixed-case> syntax-based <fixed-case>SMT</fixed-case> systems for <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Katsuhiko</first><last>Hayashi</last></author>
      <pages>127-133</pages>
      <url hash="1fb8bbf8">2014.iwslt-evaluation.18</url>
      <abstract>This paper presents NTT-NAIST SMT systems for English-German and German-English MT tasks of the IWSLT 2014 evaluation campaign. The systems are based on generalized minimum Bayes risk system combination of three SMT systems using the forest-to-string, syntactic preordering, and phrase-based translation formalisms. Individual systems employ training data selection for domain adaptation, truecasing, compound word splitting (for GermanEnglish), interpolated n-gram language models, and hypotheses rescoring using recurrent neural network language models.</abstract>
      <bibkey>sudoh-etal-2014-ntt</bibkey>
    </paper>
    <paper id="19">
      <title>The <fixed-case>USTC</fixed-case> machine translation system for <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Shijin</first><last>Wang</last></author>
      <author><first>Yuguang</first><last>Wang</last></author>
      <author><first>Jianfeng</first><last>Li</last></author>
      <author><first>Yiming</first><last>Cui</last></author>
      <author><first>Lirong</first><last>Dai</last></author>
      <pages>134-138</pages>
      <url hash="4fbda34d">2014.iwslt-evaluation.19</url>
      <bibkey>wang-etal-2014-ustc</bibkey>
    </paper>
    <paper id="20">
      <title>The <fixed-case>NICT</fixed-case> translation system for <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Xiaolin</first><last>Wang</last></author>
      <author><first>Andrew</first><last>Finch</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>139-142</pages>
      <url hash="3dad3114">2014.iwslt-evaluation.20</url>
      <abstract>This paper describes NICT’s participation in the IWSLT 2014 evaluation campaign for the TED Chinese-English translation shared-task. Our approach used a combination of phrase-based and hierarchical statistical machine translation (SMT) systems. Our focus was in several areas, specifically system combination, word alignment, and various language modeling techniques including the use of neural network joint models. Our experiments on the test set from the 2013 shared task, showed that an improvement in BLEU score can be gained in translation performance through all of these techniques, with the largest improvements coming from using large data sizes to train the language model.</abstract>
      <bibkey>wang-etal-2014-nict</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>P</fixed-case>olish-<fixed-case>E</fixed-case>nglish speech statistical machine translation systems for the <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Krzysztof</first><last>Wolk</last></author>
      <author><first>Krzysztof</first><last>Marasek</last></author>
      <pages>143-149</pages>
      <url hash="1060c980">2014.iwslt-evaluation.21</url>
      <abstract>This research explores effects of various training settings between Polish and English Statistical Machine Translation systems for spoken language. Various elements of the TED parallel text corpora for the IWSLT 2014 evaluation campaign were used as the basis for training of language models, and for development, tuning and testing of the translation system as well as Wikipedia based comparable corpora prepared by us. The BLEU, NIST, METEOR and TER metrics were used to evaluate the effects of data preparations on translation results. Our experiments included systems, which use lemma and morphological information on Polish words. We also conducted a deep analysis of provided Polish data as preparatory work for the automatic data correction and cleaning phase.</abstract>
      <bibkey>wolk-marasek-2014-polish</bibkey>
    </paper>
    <paper id="22">
      <title>The <fixed-case>RWTH</fixed-case> <fixed-case>A</fixed-case>achen machine translation systems for <fixed-case>IWSLT</fixed-case> 2014</title>
      <author><first>Joern</first><last>Wuebker</last></author>
      <author><first>Stephan</first><last>Peitz</last></author>
      <author><first>Andreas</first><last>Guta</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>150-154</pages>
      <url hash="1bce5725">2014.iwslt-evaluation.22</url>
      <abstract>This work describes the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign International Workshop on Spoken Language Translation (IWSLT) 2014. We participated in both the MT and SLT tracks for the English→French and German→English language pairs and applied the identical training pipeline and models on both language pairs. Our state-of-the-art phrase-based baseline systems are augmented with maximum expected BLEU training for phrasal, lexical and reordering models. Further, we apply rescoring with novel recurrent neural language and translation models. The same systems are used for the SLT track, where we additionally perform punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. We are able to improve RWTH’s 2013 evaluation systems by 1.7-1.8% BLEU absolute.</abstract>
      <bibkey>wuebker-etal-2014-rwth</bibkey>
    </paper>
  </volume>
  <volume id="papers" ingest-date="2021-08-08">
    <meta>
      <booktitle>Proceedings of the 11th International Workshop on Spoken Language Translation: Papers</booktitle>
      <address>Lake Tahoe, California</address>
      <month>December 4-5</month>
      <year>2014</year>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <editor><first>Sebastian</first><last>Stüker</last></editor>
      <editor><first>François</first><last>Yvon</last></editor>
      <venue>iwslt</venue>
    </meta>
    <paper id="1">
      <title>Advances in dialectal <fixed-case>A</fixed-case>rabic speech recognition: a study using <fixed-case>T</fixed-case>witter to improve <fixed-case>E</fixed-case>gyptian <fixed-case>ASR</fixed-case></title>
      <author><first>Ahmed</first><last>Ali</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Stephan</first><last>Vogel</last></author>
      <pages>156-162</pages>
      <url hash="32f17e1e">2014.iwslt-papers.1</url>
      <abstract>This paper reports results in building an Egyptian Arabic speech recognition system as an example for under-resourced languages. We investigated different approaches to build the system using 10 hours for training the acoustic model, and results for both grapheme system and phoneme system using MADA. The phoneme-based system shows better results than the grapheme-based system. In this paper, we explore the use of tweets written in dialectal Arabic. Using 880K Egyptian tweets reduced the Out Of Vocabulary (OOV) rate from 15.1% to 3.2% and the WER from 59.6% to 44.7%, a relative gain 25% in WER.</abstract>
      <bibkey>ali-etal-2014-advances</bibkey>
    </paper>
    <paper id="2">
      <title>Towards simultaneous interpreting: the timing of incremental machine translation and speech synthesis</title>
      <author><first>Timo</first><last>Baumann</last></author>
      <author><first>Srinivas</first><last>Bangalore</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <pages>163-168</pages>
      <url hash="71d2a2d5">2014.iwslt-papers.2</url>
      <abstract>In simultaneous interpreting, human experts incrementally construct and extend partial hypotheses about the source speaker’s message, and start to verbalize a corresponding message in the target language, based on a partial translation – which may have to be corrected occasionally. They commence the target utterance in the hope that they will be able to finish understanding the source speaker’s message and determine its translation in time for the unfolding delivery. Of course, both incremental understanding and translation by humans can be garden-pathed, although experts are able to optimize their delivery so as to balance the goals of minimal latency, translation quality and high speech fluency with few corrections. We investigate the temporal properties of both translation input and output to evaluate the tradeoff between low latency and translation quality. In addition, we estimate the improvements that can be gained with a tempo-elastic speech synthesizer.</abstract>
      <bibkey>baumann-etal-2014-towards</bibkey>
    </paper>
    <paper id="3">
      <title>Word confidence estimation for speech translation</title>
      <author><first>L.</first><last>Besacier</last></author>
      <author><first>B.</first><last>Lecouteux</last></author>
      <author><first>N. Q.</first><last>Luong</last></author>
      <author><first>K.</first><last>Hour</last></author>
      <author><first>M.</first><last>Hadjsalah</last></author>
      <pages>169-175</pages>
      <url hash="ba27309c">2014.iwslt-papers.3</url>
      <abstract>Word Confidence Estimation (WCE) for machine translation (MT) or automatic speech recognition (ASR) consists in judging each word in the (MT or ASR) hypothesis as correct or incorrect by tagging it with an appropriate label. In the past, this task has been treated separately in ASR or MT contexts and we propose here a joint estimation of word confidence for a spoken language translation (SLT) task involving both ASR and MT. This research work is possible because we built a specific corpus which is first presented. This corpus contains 2643 speech utterances for which a quintuplet containing: ASR output (src-asr), verbatim transcript (src-ref), text translation output (tgt-mt), speech translation output (tgt-slt) and post-edition of translation (tgt-pe), is made available. The rest of the paper illustrates how such a corpus (made available to the research community) can be used for evaluating word confidence estimators in ASR, MT or SLT scenarios. WCE for SLT could help rescoring SLT output graphs, improving translators productivity (for translation of lectures or movie subtitling) or it could be useful in interactive speech-to-speech translation scenarios.</abstract>
      <bibkey>besacier-etal-2014-word</bibkey>
    </paper>
    <paper id="4">
      <title>Machine translation of multi-party meetings: segmentation and disfluency removal strategies</title>
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <pages>176-183</pages>
      <url hash="01c2b606">2014.iwslt-papers.4</url>
      <abstract>Translating meetings presents a challenge since multi-speaker speech shows a variety of disfluencies. In this paper we investigate the importance of transforming speech into well-written input prior to translating multi-party meetings. We first analyze the characteristics of this data and establish oracle scores. Sentence segmentation and punctuation are performed using a language model, turn information, or a monolingual translation system. Disfluencies are removed by a CRF model trained on in-domain and out-of-domain data. For comparison, we build a combined CRF model for punctuation insertion and disfluency removal. By applying these models, multi-party meetings are transformed into fluent input for machine translation. We evaluate the models with regard to translation performance and are able to achieve an improvement of 2.1 to 4.9 BLEU points depending on the availability of turn information.</abstract>
      <bibkey>cho-etal-2014-machine</bibkey>
    </paper>
    <paper id="5">
      <title>Empircal dependency-based head finalization for statistical <fixed-case>C</fixed-case>hinese-, <fixed-case>E</fixed-case>nglish-, and <fixed-case>F</fixed-case>rench-to-<fixed-case>M</fixed-case>yanmar (<fixed-case>B</fixed-case>urmese) machine translation</title>
      <author><first>Chenchen</first><last>Ding</last></author>
      <author><first>Ye Kyaw</first><last>Thu</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Andrew</first><last>Finch</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>184-191</pages>
      <url hash="3c560afd">2014.iwslt-papers.5</url>
      <abstract>We conduct dependency-based head finalization for statistical machine translation (SMT) for Myanmar (Burmese). Although Myanmar is an understudied language, linguistically it is a head-final language with similar syntax to Japanese and Korean. So, applying the efficient techniques of Japanese and Korean processing to Myanmar is a natural idea. Our approach is a combination of two approaches. The first is a head-driven phrase structure grammar (HPSG) based head finalization for English-to-Japanese translation, the second is dependency-based pre-ordering originally designed for English-to-Korean translation. We experiment on Chinese-, English-, and French-to-Myanmar translation, using a statistical pre-ordering approach as a comparison method. Experimental results show the dependency-based head finalization was able to consistently improve a baseline SMT system, for different source languages and different segmentation schemes for the Myanmar language.</abstract>
      <bibkey>ding-etal-2014-empircal</bibkey>
    </paper>
    <paper id="6">
      <title>Discriminative adaptation of continuous space translation models</title>
      <author><first>Quoc-Khanh</first><last>Do</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>192-199</pages>
      <url hash="82fdaac0">2014.iwslt-papers.6</url>
      <abstract>In this paper we explore various adaptation techniques for continuous space translation models (CSTMs). We consider the following practical situation: given a large scale, state-of-the-art SMT system containing a CSTM, the task is to adapt the CSTM to a new domain using a (relatively) small in-domain parallel corpus. Our method relies on the definition of a new discriminative loss function for the CSTM that borrows from both the max-margin and pair-wise ranking approaches. In our experiments, the baseline out-of-domain SMT system is initially trained for the WMT News translation task, and the CSTM is to be adapted to the lecture translation task as defined by IWSLT evaluation campaign. Experimental results show that an improvement of 1.5 BLEU points can be achieved with the proposed adaptation method.</abstract>
      <bibkey>do-etal-2014-discriminative</bibkey>
    </paper>
    <paper id="7">
      <title>Extracting translation pairs from social network content</title>
      <author><first>Matthias</first><last>Eck</last></author>
      <author><first>Yuri</first><last>Zemlyanskiy</last></author>
      <author><first>Joy</first><last>Zhang</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <pages>200-205</pages>
      <url hash="1186a64a">2014.iwslt-papers.7</url>
      <abstract>We introduce two methods to collect additional training data for statistical machine translation systems from public social network content. The first method identifies multilingual content where the author self-translated their own post to reach additional friends, fans or customers. Once identified, we can split the post in the language segments and extract translation pairs from this content. The second methods considers web links (URLs) that users add as part of their post to point the reader to a video, article or website. If the same URL is shared from different language users, there is a chance they might give the same comment in their respective language. We use a support vector machine (SVM) as a classifier to identify true translations from all candidate pairs. We collected additional translation pairs using both methods for the language pairs Spanish-English and Portuguese-English. Testing the collected data as additional training data for statistical machine translations on in-domain test sets resulted in very significant improvements of up to 5 BLEU.</abstract>
      <bibkey>eck-etal-2014-extracting</bibkey>
    </paper>
    <paper id="8">
      <title>An exploration of segmentation strategies in stream decoding</title>
      <author><first>Andrew</first><last>Finch</last></author>
      <author><first>Xiaolin</first><last>Wang</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <url hash="a5f473d2">2014.iwslt-papers.8</url>
      <abstract>In this paper we explore segmentation strategies for the stream decoder a method for decoding from a continuous stream of input tokens, rather than the traditional method of decoding from sentence segmented text. The behavior of the decoder is analyzed and modifications to the decoding algorithm are proposed to improve its performance. The experimental results show our proposed decoding strategies to be effective, and add support to the original findings that this approach is capable of approaching the performance of the underlying phrase-based machine translation decoder, at useful levels of latency. Our experiments evaluated the stream decoder on a broader set of language pairs than in previous work. We found most European language pairs were similar in character, and report results on English-Chinese and English-German pairs which are of interest due to the reordering required.</abstract>
      <pages>206-213</pages>
      <bibkey>finch-etal-2014-exploration</bibkey>
    </paper>
    <paper id="9">
      <title>Incremental development of statistical machine translation systems</title>
      <author><first>Li</first><last>Gong</last></author>
      <author><first>Aurélien</first><last>Max</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <url hash="99b767ae">2014.iwslt-papers.9</url>
      <abstract>Statistical Machine Translation produces results that make it a competitive option in most machine-assisted translation scenarios. However, these good results often come at a very high computational cost and correspond to training regimes which are unfit to many practical contexts, where the ability to adapt to users and domains and to continuously integrate new data (eg. in post-edition contexts) are of primary importance. In this article, we show how these requirements can be met using a strategy for on-demand word alignment and model estimation. Most remarkably, our incremental system development framework is shown to deliver top quality translation performance even in the absence of tuning, and to surpass a strong baseline when performing online tuning. All these results obtained with great computational savings as compared to conventional systems.</abstract>
      <pages>214-222</pages>
      <bibkey>gong-etal-2014-incremental</bibkey>
    </paper>
    <paper id="10">
      <title>Lexical translation model using a deep neural network architecture</title>
      <author><first>Thanh-Le</first><last>Ha</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <pages>223-229</pages>
      <url hash="a8c842dd">2014.iwslt-papers.10</url>
      <abstract>In this paper we combine the advantages of a model using global source sentence contexts, the Discriminative Word Lexicon, and neural networks. By using deep neural networks instead of the linear maximum entropy model in the Discriminative Word Lexicon models, we are able to leverage dependencies between different source words due to the non-linearity. Furthermore, the models for different target words can share parameters and therefore data sparsity problems are effectively reduced. By using this approach in a state-of-the-art translation system, we can improve the performance by up to 0.5 BLEU points for three different language pairs on the TED translation task.</abstract>
      <bibkey>ha-etal-2014-lexical</bibkey>
    </paper>
    <paper id="11">
      <title>Anticipatory translation model adaptation for bilingual conversations</title>
      <author><first>Sanjika</first><last>Hewavitharana</last></author>
      <author><first>Dennis</first><last>Mehay</last></author>
      <author><first>Sankaranarayanan</first><last>Ananthakrishnan</last></author>
      <author><first>Rohit</first><last>Kumar</last></author>
      <author><first>John</first><last>Makhoul</last></author>
      <pages>230-235</pages>
      <url hash="301a3fd8">2014.iwslt-papers.11</url>
      <abstract>Conversational spoken language translation (CSLT) systems facilitate bilingual conversations in which the two participants speak different languages. Bilingual conversations provide additional contextual information that can be used to improve the underlying machine translation system. In this paper, we describe a novel translation model adaptation method that anticipates a participant’s response in the target language, based on his counterpart’s prior turn in the source language. Our proposed strategy uses the source language utterance to perform cross-language retrieval on a large corpus of bilingual conversations in order to obtain a set of potentially relevant target responses. The responses retrieved are used to bias translation choices towards anticipated responses. On an Iraqi-to-English CSLT task, our method achieves a significant improvement over the baseline system in terms of BLEU, TER and METEOR metrics.</abstract>
      <bibkey>hewavitharana-etal-2014-anticipatory</bibkey>
    </paper>
    <paper id="12">
      <title>Offline extraction of overlapping phrases for hierarchical phrase-based translation</title>
      <author><first>Sariya</first><last>Karimova</last></author>
      <author><first>Patrick</first><last>Simianer</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <pages>236-243</pages>
      <url hash="9343b1ce">2014.iwslt-papers.12</url>
      <abstract>Standard SMT decoders operate by translating disjoint spans of input words, thus discarding information in form of overlapping phrases that is present at phrase extraction time. The use of overlapping phrases in translation may enhance fluency in positions that would otherwise be phrase boundaries, they may provide additional statistical support for long and rare phrases, and they may generate new phrases that have never been seen in the training data. We show how to extract overlapping phrases offline for hierarchical phrasebased SMT, and how to extract features and tune weights for the new phrases. We find gains of 0.3 − 0.6 BLEU points over discriminatively trained hierarchical phrase-based SMT systems on two datasets for German-to-English translation.</abstract>
      <bibkey>karimova-etal-2014-offline</bibkey>
    </paper>
    <paper id="13">
      <title>Translations of the Callhome <fixed-case>E</fixed-case>gyptian <fixed-case>A</fixed-case>rabic corpus for conversational speech translation</title>
      <author><first>Gaurav</first><last>Kumar</last></author>
      <author><first>Yuan</first><last>Cao</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Daniel</first><last>Povey</last></author>
      <author><first>Sanjeev</first><last>Khudanpur</last></author>
      <pages>244-248</pages>
      <url hash="8de79459">2014.iwslt-papers.13</url>
      <abstract>Translation of the output of automatic speech recognition (ASR) systems, also known as speech translation, has received a lot of research interest recently. This is especially true for programs such as DARPA BOLT which focus on improving spontaneous human-human conversation across languages. However, this research is hindered by the dearth of datasets developed for this explicit purpose. For Egyptian Arabic-English, in particular, no parallel speechtranscription-translation dataset exists in the same domain. In order to support research in speech translation, we introduce the Callhome Egyptian Arabic-English Speech Translation Corpus. This supplements the existing LDC corpus with four reference translations for each utterance in the transcripts. The result is a three-way parallel dataset of Egyptian Arabic Speech, transcriptions and English translations.</abstract>
      <bibkey>kumar-etal-2014-translations</bibkey>
    </paper>
    <paper id="14">
      <title>Improving in-domain data selection for small in-domain sets</title>
      <author><first>Mohammed</first><last>Mediani</last></author>
      <author><first>Joshua</first><last>Winebarger</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>249-256</pages>
      <url hash="901c57a4">2014.iwslt-papers.14</url>
      <abstract>Finding sufficient in-domain text data for language modeling is a recurrent challenge. Some methods have already been proposed for selecting parts of out-of-domain text data most closely resembling the in-domain data using a small amount of the latter. Including this new “near-domain” data in training can potentially lead to better language model performance, while reducing training resources relative to incorporating all data. One popular, state-of-the-art selection process based on cross-entropy scores makes use of in-domain and out-ofdomain language models. In order to compensate for the limited availability of the in-domain data required for this method, we introduce enhancements to two of its steps. Firstly, we improve the procedure for drawing the outof-domain sample data used for selection. Secondly, we use word-associations in order to extend the underlying vocabulary of the sample language models used for scoring. These enhancements are applied to selecting text for language modeling of talks given in a technical subject area. Besides comparing perplexity, we judge the resulting language models by their performance in automatic speech recognition and machine translation tasks. We evaluate our method in different contexts. We show that it yields consistent improvements, up to 2% absolute reduction in word error rate and 0.3 Bleu points. We achieve these improvements even given a much smaller in-domain set.</abstract>
      <bibkey>mediani-etal-2014-improving</bibkey>
    </paper>
    <paper id="15">
      <title>Multilingual deep bottle neck features: a study on language selection and training techniques</title>
      <author><first>Markus</first><last>Müller</last></author>
      <author><first>Sebastian</first><last>Stüker</last></author>
      <author><first>Zaid</first><last>Sheikh</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <pages>257-264</pages>
      <url hash="863f0c97">2014.iwslt-papers.15</url>
      <abstract>Previous work has shown that training the neural networks for bottle neck feature extraction in a multilingual way can lead to improvements in word error rate and average term weighted value in a telephone key word search task. In this work we conduct a systematic study on a) which multilingual training strategy to employ, b) the effect of language selection and amount of multilingual training data used and c) how to find a suitable combination for languages. We conducted our experiment on the key word search task and the languages of the IARPA BABEL program. In a first step, we assessed the performance of a single language out of all available languages in combination with the target language. Based on these results, we then combined a multitude of languages. We also examined the influence of the amount of training data per language, as well as different techniques for combining the languages during network training. Our experiments show that data from arbitrary additional languages does not necessarily increase the performance of a system. But when combining a suitable set of languages, a significant gain in performance can be achieved.</abstract>
      <bibkey>muller-etal-2014-multilingual</bibkey>
    </paper>
    <paper id="16">
      <title>The <fixed-case>NAIST</fixed-case>-<fixed-case>NTT</fixed-case> <fixed-case>TED</fixed-case> talk treebank</title>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Katsuhiro</first><last>Sudoh</last></author>
      <author><first>Yusuke</first><last>Oda</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Hajime</first><last>Tsukuda</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <pages>265-270</pages>
      <url hash="b66dafe2">2014.iwslt-papers.16</url>
      <abstract>Syntactic parsing is a fundamental natural language processing technology that has proven useful in machine translation, language modeling, sentence segmentation, and a number of other applications related to speech translation. However, there is a paucity of manually annotated syntactic parsing resources for speech, and particularly for the lecture speech that is the current target of the IWSLT translation campaign. In this work, we present a new manually annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and these speechrelated applications. The first version of the corpus includes 1,217 sentences and 23,158 words manually annotated with parse trees, and aligned with translations in 26-43 different languages. In this paper we describe the collection of the corpus, and an analysis of its various characteristics.</abstract>
      <bibkey>neubig-etal-2014-naist</bibkey>
    </paper>
    <paper id="17">
      <title>Better punctuation prediction with hierarchical phrase-based translation</title>
      <author><first>Stephan</first><last>Peitz</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>271-278</pages>
      <url hash="489a6e88">2014.iwslt-papers.17</url>
      <abstract>Punctuation prediction is an important task in spoken language translation and can be performed by using a monolingual phrase-based translation system to translate from unpunctuated to text with punctuation. However, a punctuation prediction system based on phrase-based translation is not able to capture long-range dependencies between words and punctuation marks. In this paper, we propose to employ hierarchical translation in place of phrase-based translation and show that this approach is more robust for unseen word sequences. Furthermore, we analyze different optimization criteria for tuning the scaling factors of a monolingual statistical machine translation system. In our experiments, we compare the new approach with other punctuation prediction methods and show improvements in terms of F1-Score and BLEU on the IWSLT 2014 German→English and English→French translation tasks.</abstract>
      <bibkey>peitz-etal-2014-better</bibkey>
    </paper>
    <paper id="18">
      <title>Rule-based preordering on multiple syntactic levels in statistical machine translation</title>
      <author><first>Ge</first><last>Wu</last></author>
      <author><first>Yuqi</first><last>Zhang</last></author>
      <author><first>Alexander</first><last>Waibel</last></author>
      <pages>279-286</pages>
      <url hash="f2153747">2014.iwslt-papers.18</url>
      <abstract>We propose a novel data-driven rule-based preordering approach, which uses the tree information of multiple syntactic levels. This approach extend the tree-based reordering from one level into multiple levels, which has the capability to process more complicated reordering cases. We have conducted experiments in English-to-Chinese and Chinese-to-English translation directions. Our results show that the approach has led to improved translation quality both when it was applied separately or when it was combined with some other reordering approaches. As our reordering approach was used alone, it showed an improvement of 1.61 in BLEU score in the English-to-Chinese translation direction and an improvement of 2.16 in BLEU score in the Chinese-to-English translation direction, in comparison with the baseline, which used no word reordering. As our preordering approach were combined with the short rule [1], long rule [2] and tree rule [3] based preordering approaches, it showed further improvements of up to 0.43 in BLEU score in the English-to-Chinese translation direction and further improvements of up to 0.3 in BLEU score in the Chinese-to-English translation direction. Through the translations that used our preordering approach, we have also found many translation examples with improved syntactic structures.</abstract>
      <bibkey>wu-etal-2014-rule</bibkey>
    </paper>
  </volume>
</collection>
