<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.clinj">
  <volume id="13" ingest-date="2025-09-29" type="journal">
    <meta>
      <booktitle>Computational Linguistics in the Netherlands Journal</booktitle>
      <editor><first>Walter</first><last>Daelemans</last></editor>
      <editor><first>Gertjan</first><last>van Noord</last></editor>
      <publisher>CLIN journal</publisher>
      <month>March</month>
      <year>2024</year>
      <venue>clinj</venue>
      <issn>2211-4009</issn>
      <journal-volume>13</journal-volume>
    </meta>
    <paper id="1">
      <title>Preface</title>
      <author><first>Jens</first><last>Lemmens</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Lisa</first><last>Hilte</last><affiliation>Flemish AI Academy</affiliation></author>
      <author><first>Jens</first><last>Van Nooten</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Maxime</first><last>De Bruyn</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Pieter</first><last>Fivez</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Ine</first><last>Gevers</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Jeska</first><last>Buhmann</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Jens</first><last>Lemmens</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Ehsan</first><last>Lotfi</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Nicolae</first><last>Banari</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Nerses</first><last>Yuzbashyan</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Walter</first><last>Daelemans</last><affiliation>University of Antwerp</affiliation></author>
      <pages>1-4</pages>
      <url hash="c015387f">2024.clinj-13.1</url>
      <bibkey>lemmens-etal-2024-preface</bibkey>
    </paper>
    <paper id="2">
      <title>Personality Style Recognition via Machine Learning: Identifying Anaclitic and Introjective Personality Styles from Patients’ Speech</title>
      <author><first>Semere Kiros</first><last>Bitew</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Vincent</first><last>Schelstraete</last><affiliation>Tech Wolf (Ghent, Belgium)</affiliation></author>
      <author><first>Klim</first><last>Zaporojets</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Kimberly</first><last>Van Nieuwenhove</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Reitske</first><last>Meganck</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Chris</first><last>Develder</last><affiliation>University of Ghent</affiliation></author>
      <pages>5-27</pages>
      <abstract>In disentangling the heterogeneity observed in psychopathology, personality of the patients is considered crucial. While it has been demonstrated that personality traits are reflected in the language used by a patient, we hypothesize that this enables automatic inference of the personality type directly from speech utterances, potentially more accurately than through a traditional questionnaire-based approach explicitly designed for personality classification. To validate this hypothesis, we adopt natural language processing (NLP) and standard machine learning tools for classification. We test this on a dataset of recorded clinical diagnostic interviews (CDI) on a sample of 79 patients diagnosed with major depressive disorder (MDD) – a condition for which differentiated treatment based on personality styles has been advocated – and classified into anaclitic and introjective personality styles. We start by analyzing the interviews to see which linguistic features are associated with each style, in order to gain a better understanding of the styles. Then, we develop automatic classifiers based on (a) standardized questionnaire responses; (b) basic text features, i.e., TF-IDF scores of words and word sequences; (c) more advanced text features, using LIWC (linguistic inquiry and word count) and context-aware features using BERT (bidirectional encoder representations from transformers); (d) audio features. We find that automated classification with language-derived features (i.e., based on LIWC) significantly outperforms questionnaire-based classification models. Furthermore, the best performance is achieved by combining LIWC with the questionnaire features. This suggests that more work should be put into developing linguistically based automated techniques for characterizing personality, however questionnaires still to some extent complement such methods.</abstract>
      <url hash="b3715cc4">2024.clinj-13.2</url>
      <bibkey>bitew-etal-2024-personality</bibkey>
    </paper>
    <paper id="3">
      <title>Controllable Sentence Simplification in <fixed-case>D</fixed-case>utch</title>
      <author><first>Theresa</first><last>Seidl</last><affiliation>Catholic University of Leuven</affiliation></author>
      <author><first>Vincent</first><last>Vandeghinste</last><affiliation>Dutch Language Institute</affiliation></author>
      <pages>28-58</pages>
      <abstract>Text simplification aims to reduce complexity in vocabulary and syntax, enhancing the readability and comprehension of text. This paper presents a supervised sentence simplification approach for Dutch using a pre-trained large language model (T5). Given the absence of a parallel corpus in Dutch, a synthetic dataset is generated from established parallel corpora. The implementation incorporates a sentence-level discrete parametrization mechanism, enabling control over the simplification features. The model’s output can be tailored to different simplification scenarios and target audiences by incorporating control tokens into the training data. The controlled attributes include sentence length, word length, paraphrasing, and lexical and syntactic complexity. This work contributes a dedicated set of control tokens tailored to the Dutch language. It shows that significant simplification can be achieved using a synthetic dataset with as few as 2000 parallel rows, although optimal performance requires a minimum of 10,000 rows. The fine-tuned model achieves a 36.85 SARI score on the test set, supporting its effectiveness in the simplification process. This research contributes to the field of sentence simplification by discussing the implementation of a supervised simplification approach for Dutch. The findings highlight the potential of synthetic datasets and control tokens in achieving effective simplification, despite the lack of a parallel corpus in the target language.</abstract>
      <url hash="62f9ecef">2024.clinj-13.3</url>
      <bibkey>seidl-vandeghinste-2024-controllable</bibkey>
    </paper>
    <paper id="4">
      <title>Benchmarking Zero-Shot Text Classification for <fixed-case>D</fixed-case>utch</title>
      <author><first>Loic</first><last>De Langhe</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Aaron</first><last>Maladry</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Bram</first><last>Vanroy</last><affiliation>Catholic University of Leuven</affiliation></author>
      <author><first>Luna</first><last>De Bruyne</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Pranaydeep</first><last>Singh</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Els</first><last>Lefever</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Orphée</first><last>De Clercq</last><affiliation>University of Ghent</affiliation></author>
      <pages>59-86</pages>
      <abstract>The advent and popularisation of Large Language Models (LLMs) have given rise to promptbased Natural Language Processing (NLP) techniques which eliminate the need for large manually annotated corpora and computationally expensive supervised training or fine-tuning processes. Zero-shot learning in particular presents itself as an attractive alternative to the classical train-development-test paradigm for many downstream tasks as it provides a quick and inexpensive way of directly leveraging the implicitly encoded knowledge in LLMs. Despite the large interest in zero-shot applications within the domain of NLP as a whole, there is often no consensus on the methodology, analysis and evaluation of zero-shot pipelines. As a tentative step towards finding such a consensus, this work provides a detailed overview of available methods, resources, and caveats for zero-shot prompting within the Dutch language domain. At the same time, we present centralised zero-shot benchmark results on a large variety of Dutch NLP tasks using a series of standardised datasets. These tasks vary in subjectivity and domain, ranging from more social information extraction tasks (sentiment, emotion and irony detection for social media) to factual tasks (news topic classification and event coreference resolution). To ensure that the benchmark results are representative, we investigated a selection of zero-shot methodologies for a variety of state-of-the-art Dutch Natural Language Inference models (NLI), Masked Language models (MLM), and autoregressive language models. The output on each test set was compared to the best performance achieved using supervised methods. Our findings indicate that task-specific fine-tuning delivers superior performance in all but one (emotion detection) task. In the zero-shot settings it could be observed that large generative models through prompting seem to outperform NLI models, which in turn perform better than the MLM approach. Finally, we note several caveats and challenges tied to using zero-shot learning in application settings. These include, but are not limited to, properly streamlining evaluation of zero-shot output, parameter efficiency compared to standard finetuned models and prompt optimization.</abstract>
      <url hash="9783fb57">2024.clinj-13.4</url>
      <bibkey>de-langhe-etal-2024-benchmarking</bibkey>
    </paper>
    <paper id="5">
      <title>Comparative Evaluation of Topic Detection: Humans vs. <fixed-case>LLM</fixed-case>s</title>
      <author><first>Andriy</first><last>Kosar</last><affiliation>Textgain / University of Antwerp</affiliation></author>
      <author><first>Guy</first><last>De Pauw</last><affiliation>Textgain</affiliation></author>
      <author><first>Walter</first><last>Daelemans</last><affiliation>University of Antwerp</affiliation></author>
      <pages>87-116</pages>
      <abstract>This research explores topic detection and naming in news texts, conducting a comparative study involving human participants from Ukraine, Belgium, and the USA, alongside Large Language Models (LLMs). In the first experiment, 109 participants from diverse backgrounds assigned topics to three news texts each. The findings revealed significant variations in topic assignment and naming, emphasizing the need for nuanced evaluative metrics beyond simple binary matches. The second experiment engaged eight native speakers and six LLMs to determine and name topics for seven news texts. A jury of four experts anonymously assessed these topic names, evaluating them based on criteria such as relevance, completeness, clarity, and correctness. Detailed results shed light on the potential of LLMs in topic detection, stressing the importance of acknowledging and accommodating the inherent diversity and subjectivity in topic identification, while also proposing criteria for evaluating their application in both detecting and naming topics.</abstract>
      <url hash="543661dd">2024.clinj-13.5</url>
      <bibkey>kosar-etal-2024-comparative</bibkey>
    </paper>
    <paper id="6">
      <title>Detecting Dialect Features Using Normalised Pointwise Information</title>
      <author><first>H. W. Matthew</first><last>Sung</last><affiliation>University of Leiden</affiliation></author>
      <author><first>Jelena</first><last>Prokić</last><affiliation>University of Leiden</affiliation></author>
      <pages>117-141</pages>
      <abstract>Feature extraction refers to the identification of important features which differentiate one dialect group from another. It is an important step in understanding the dialectal variation, a step which has traditionally been done manually. However, manual extraction of important features is susceptible to the following problems, namely it is a time-consuming task; there is a risk of overlooking certain features and lastly, every analyst can come up with a different set of features. In this paper we compare two earlier automatic approaches to dialect feature extraction, namely Factor Analysis (Pickl 2016) and Proki ́c et al.’s (2012) method based on Fisher’s Linear Discriminant. We also introduce a new method based on Normalised Pointwise Mutual Information (nPMI), which outperforms other methods on the tested data set.</abstract>
      <url hash="2ecdd7df">2024.clinj-13.6</url>
      <bibkey>sung-prokic-2024-detecting</bibkey>
    </paper>
    <paper id="7">
      <title>Historical <fixed-case>D</fixed-case>utch Spelling Normalization with Pretrained Language Models</title>
      <author><first>Andre</first><last>Wolters</last><affiliation/></author>
      <author><first>Andreas</first><last>Van Craenenburgh</last><affiliation>University of Groningen</affiliation></author>
      <pages>142-166</pages>
      <abstract>The Dutch language has undergone several spelling reforms since the 19th century. Normalizing 19th-century Dutch spelling to its modern equivalent can increase performance on various NLP tasks, such as machine translation or entity tagging. Van Cranenburgh and van Noord (2022) presented a rule-based system to normalize historical Dutch texts to their modern equivalent, but building and extending such a system requires careful engineering to ensure good coverage while not introducing incorrect normalizations. Recently, pretrained language models have become state-of-the-art for most NLP tasks. In this paper, we combine these approaches by building sequence-to-sequence language models trained on automatically corrected texts from the rule-based system (i.e., silver data). We experiment with several types of language models and approaches. First, we finetune two T5 models: Flan-T5 (Chung et al., 2022), an instruction-fine-tuned multilingual version of the original T5, and ByT5 (Xue et al., 2022), a token-free model which operates directly on the raw text and characters. Second, we pretrain ByT5 with the pretraining data used for BERTje (de Vries et al., 2019) and finetune this model afterward. For evaluation, we use three manually-corrected novels from the same source and compare all trained models with the original rule-based system used to generate the training data. This allows for a direct comparison between the rule-based and pretrained language models to analyze which yields the best performance. Our pretrained ByT5 model finetuned with our largest finetuning dataset achieved the best results on all three novels. This model not only outperformed the rule-based system, but also also made generalizations beyond the training data. In addition to an intrinsic evaluation of the spelling normalization itself, we also perform an extrinsic evaluation on downstream tasks, namely parsing and coreference. Results show that the neural system tends to outperform the rule-based method, although the differences are small. All code, data, and models used in this paper are available at https://github.com/andreasvc/neuralspellnorm.</abstract>
      <url hash="2187c814">2024.clinj-13.7</url>
      <bibkey>wolters-van-craenenburgh-2024-historical</bibkey>
    </paper>
    <paper id="8">
      <title>Exploring <fixed-case>LLM</fixed-case>s’ Capabilities for Error Detection in <fixed-case>D</fixed-case>utch <fixed-case>L</fixed-case>1 and <fixed-case>L</fixed-case>2 Writing Products</title>
      <author><first>Joni</first><last>Kruijsbergen</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Serafina</first><last>Van Geertruyen</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Véronique</first><last>Hoste</last><affiliation>University of Ghent</affiliation></author>
      <author><first>Orphée</first><last>De Clercq</last><affiliation>University of Ghent</affiliation></author>
      <pages>167-185</pages>
      <abstract>This research examines the capabilities of Large Language Models for writing error detection, which can be seen as a first step towards automated writing support. Our work focuses on Dutch writing error detection, targeting two envisaged end-users: L1 and L2 adult speakers of Dutch. We relied on proprietary L1 and L2 datasets comprising writing products annotated with a variety of writing errors. Following the recent paradigms in NLP research, we experimented with both a fine-tuning approach combining different mono- (BERTje, RobBERT) and multilingual (mBERT, XLM-RoBERTa) models, as well as a zero-shot approach through prompting a generative autoregressive language model (GPT-3.5). The results reveal that the fine-tuning approach outperforms zero-shotting to a large extent, both for L1 and L2, even though there is much room left for improvement.</abstract>
      <url hash="92d88d59">2024.clinj-13.8</url>
      <bibkey>kruijsbergen-etal-2024-exploring</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>R</fixed-case>ob<fixed-case>BERT</fixed-case>-2023: Keeping <fixed-case>D</fixed-case>utch Language Models Up-To-Date at a Lower Cost Thanks to Model Conversion</title>
      <author><first>Pieter</first><last>Delobelle</last><affiliation>Catholic University of Leuven</affiliation></author>
      <author><first>François</first><last>Remy</last><affiliation>University of Ghent</affiliation></author>
      <pages>186-196</pages>
      <abstract>Pre-training large transformer-based language models on gigantic corpora and later repurposing them as base models for finetuning on downstream tasks has proven instrumental to the recent advances in computational linguistics. However, the prohibitively high cost associated with pretraining often hampers the regular updates of base models to incorporate the latest linguistic developments. To address this issue, we present an innovative approach for efficiently producing more powerful and up-to-date versions of RobBERT, our series of cutting-edge Dutch language models, by leveraging existing language models designed for high-resource languages. Unlike the prior versions of RobBERT, which relied on the training methodology of RoBERTa but required a fresh weight initialization, our two RobBERT-2023 models (base and large) are entirely initialized using the RoBERTa-family of models. To initialize an embedding table tailored to the newly devised Dutch tokenizer, we rely on a token translation strategy introduced by Remy et al. (2023). Along with our RobBERT-2023 release, we deliver a freshly pre-trained Dutch tokenizer using the latest version of the Dutch OSCAR corpus. This corpus incorporates new high-frequency terms, such as those related to the COVID-19 pandemic, cryptocurrencies, and the ongoing energy crisis, while mitigating the inclusion of previously over-represented terms from adult-oriented content. To assess the value of RobBERT-2023, we evaluate its performance using the same benchmarks employed for the state-of-the-art RobBERT-2022 model, as well as the newly-released Dutch Model Benchmark. Our experimental results demonstrate that RobBERT-2023 not only surpasses its predecessor in various aspects but also achieves these enhancements at a significantly reduced training cost. This work represents a significant step forward in keeping Dutch language models up-to-date and demonstrates the potential of model conversion techniques for reducing the environmental footprint of NLP research.</abstract>
      <url hash="8fd6d4e6">2024.clinj-13.9</url>
      <bibkey>delobelle-remy-2024-robbert</bibkey>
    </paper>
    <paper id="10">
      <title>Beyond Perplexity: Examining Temporal Generalization in<fixed-case>L</fixed-case>arge Language Models via Definition Generation</title>
      <author><first>Iris</first><last>Luden</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Mario</first><last>Giulianelli</last><affiliation>ETH Zürich</affiliation></author>
      <author><first>Raquel</first><last>Fernández</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>197-224</pages>
      <abstract>The advent of large language models (LLMs) has significantly improved performance across various Natural Language Processing tasks. However, the performance of LLMs has been shown to deteriorate over time, indicating a lack of temporal generalization. To date, performance deterioration of LLMs is primarily attributed to the factual changes in the real world over time. However, not only the facts of the world, but also the language we use to describe it constantly changes. Recent studies have indicated a relationship between performance deterioration and semantic change. This is typically measured using perplexity scores and relative performance on downstream tasks. Yet, perplexity and accuracy do not explain the effects of temporally shifted data on LLMs in practice. In this work, we propose to assess lexico-semantic temporal generalization of a language model by exploiting the task of contextualized word definition generation. This in-depth semantic assessment enables interpretable insights into the possible mistakes a model may perpetrate due to meaning shift, and can be used to complement more coarse-grained measures like perplexity scores. To assess how semantic change impacts performance, we design the task by differentiating between semantically stable, changing, and emerging target words, and experiment with T5-base, fine-tuned for contextualized definition generation. Our results indicate that (i) the model’s performance deteriorates for the task of contextualized word definition generation, (ii) the performance deteriorates more for semantically changing words compared to semantically stable words, (iii) the model exhibits significantly lower performance and potential bias for emerging words, and (iv) the performance does not correlate with cross-entropy or (pseudo)-perplexity scores. Overall, our results show that definition generation can be a promising task to assess a model’s capacity for temporal generalization with respect to semantic change.</abstract>
      <url hash="b77c23e5">2024.clinj-13.10</url>
      <bibkey>luden-etal-2024-beyond</bibkey>
    </paper>
    <paper id="11">
      <title>The <fixed-case>CLIN</fixed-case>33 Shared Task on the Detection of Text Generated by Large Language Models</title>
      <author><first>Pieter</first><last>Fivez</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Walter</first><last>Daelemans</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Tim</first><last>Van de Cruys</last><affiliation>Catholic University of Leuven</affiliation></author>
      <author><first>Yury</first><last>Kashnitsky</last><affiliation>Elsevier</affiliation></author>
      <author><first>Savvas</first><last>Chamezopoulos</last><affiliation>Elsevier</affiliation></author>
      <author><first>Hadi</first><last>Mohammadi</last><affiliation>University of Utrecht</affiliation></author>
      <author><first>Anastasia</first><last>Giachanou</last><affiliation>University of Utrecht</affiliation></author>
      <author><first>Ayoub</first><last>Bagheri</last><affiliation>University of Utrecht</affiliation></author>
      <author><first>Wessel</first><last>Poelman</last><affiliation>Catholic University of Leuven</affiliation></author>
      <author><first>Juraj</first><last>Vladika</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Esther</first><last>Ploeger</last><affiliation>Aalborg University</affiliation></author>
      <author><first>Johannes</first><last>Bjerva</last><affiliation>Aalborg University</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Hans</first><last>Van Halteren</last><affiliation>Radboud University</affiliation></author>
      <pages>225-251</pages>
      <abstract>The Shared Task for CLIN33 focuses on a relatively novel yet societally relevant task: the detection of text generated by Large Language Models (LLMs). We frame this detection task as a binary classification problem (LLM-generated or not), using test data from up to 6 different domains and text genres for both Dutch and English. Part of this test data was held out entirely from the contestants, including a ”mystery genre” which belonged to an unknown domain (later revealed to be columns). Four teams submitted 11 runs with substantially different models and features. This paper gives an overview of our task setup and contains the evaluation and detailed descriptions of the participating systems. Notably, included in the winning systems are both deep learning models as well as more traditional machine learning models leveraging task-specific feature engineering.</abstract>
      <url hash="34856f6d">2024.clinj-13.11</url>
      <bibkey>fivez-etal-2024-clin33</bibkey>
    </paper>
  </volume>
</collection>
