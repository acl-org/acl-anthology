<?xml version='1.0' encoding='UTF-8'?>
<collection id="Q14">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 2</booktitle>
      <year>2014</year>
    </meta>
    <frontmatter/>
    <paper id="1">
      <title>Heterogeneous Networks and Their Applications: Scientometrics, Name Disambiguation, and Topic Modeling</title>
      <author><first>Ben</first><last>King</last></author>
      <author><first>Rahul</first><last>Jha</last></author>
      <author><first>Dragomir R.</first><last>Radev</last></author>
      <doi>10.1162/tacl_a_00161</doi>
      <abstract>We present heterogeneous networks as a way to unify lexical networks with relational data. We build a unified ACL Anthology network, tying together the citation, author collaboration, and term-cooccurence networks with affiliation and venue relations. This representation proves to be convenient and allows problems such as name disambiguation, topic modeling, and the measurement of scientific impact to be easily solved using only this network and off-the-shelf graph algorithms.</abstract>
      <pages>1–14</pages>
      <url hash="63b474cd">Q14-1001</url>
    </paper>
    <paper id="2">
      <title><fixed-case>FLORS</fixed-case>: Fast and Simple Domain Adaptation for Part-of-Speech Tagging</title>
      <author><first>Tobias</first><last>Schnabel</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <doi>10.1162/tacl_a_00162</doi>
      <abstract>We present FLORS, a new part-of-speech tagger for domain adaptation. FLORS uses robust representations that work especially well for unknown words and for known words with unseen tags. FLORS is simpler and faster than previous domain adaptation methods, yet it has significantly better accuracy than several baselines.</abstract>
      <pages>15–26</pages>
      <url hash="464ac48e">Q14-1002</url>
    </paper>
    <paper id="3">
      <title>Automatic Detection and Language Identification of Multilingual Documents</title>
      <author><first>Marco</first><last>Lui</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <doi>10.1162/tacl_a_00163</doi>
      <abstract>Language identification is the task of automatically detecting the language(s) present in a document based on the content of the document. In this work, we address the problem of detecting documents that contain text from more than one language (multilingual documents). We introduce a method that is able to detect that a document is multilingual, identify the languages present, and estimate their relative proportions. We demonstrate the effectiveness of our method over synthetic data, as well as real-world multilingual documents collected from the web.</abstract>
      <pages>27–40</pages>
      <url hash="678d54a4">Q14-1003</url>
    </paper>
    <paper id="4">
      <title>A Crossing-Sensitive Third-Order Factorization for Dependency Parsing</title>
      <author><first>Emily</first><last>Pitler</last></author>
      <doi>10.1162/tacl_a_00164</doi>
      <abstract>Parsers that parametrize over wider scopes are generally more accurate than edge-factored models. For graph-based non-projective parsers, wider factorizations have so far implied large increases in the computational complexity of the parsing problem. This paper introduces a “crossing-sensitive” generalization of a third-order factorization that trades off complexity in the model structure (i.e., scoring with features over multiple edges) with complexity in the output structure (i.e., producing crossing edges). Under this model, the optimal 1-Endpoint-Crossing tree can be found in O(n4) time, matching the asymptotic run-time of both the third-order projective parser and the edge-factored 1-Endpoint-Crossing parser. The crossing-sensitive third-order parser is significantly more accurate than the third-order projective parser under many experimental settings and significantly less accurate on none.</abstract>
      <pages>41–54</pages>
      <url hash="435f2f5c">Q14-1004</url>
    </paper>
    <paper id="5">
      <title>Cross-lingual Projected Expectation Regularization for Weakly Supervised Learning</title>
      <author><first>Mengqiu</first><last>Wang</last></author>
      <author><first>Christopher D.</first><last>Manning</last></author>
      <doi>10.1162/tacl_a_00165</doi>
      <abstract>We consider a multilingual weakly supervised learning scenario where knowledge from annotated corpora in a resource-rich language is transferred via bitext to guide the learning in other languages. Past approaches project labels across bitext and use them as features or gold labels for training. We propose a new method that projects model expectations rather than labels, which facilities transfer of model uncertainty across language boundaries. We encode expectations as constraints and train a discriminative CRF model using Generalized Expectation Criteria (Mann and McCallum, 2010). Evaluated on standard Chinese-English and German-English NER datasets, our method demonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences. Furthermore, when combined with labeled examples, our method yields significant improvements over state-of-the-art supervised methods, achieving best reported numbers to date on Chinese OntoNotes and German CoNLL-03 datasets.</abstract>
      <pages>55–66</pages>
      <url hash="51cce8fb">Q14-1005</url>
    </paper>
    <paper id="6">
      <title>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
      <author><first>Peter</first><last>Young</last></author>
      <author><first>Alice</first><last>Lai</last></author>
      <author><first>Micah</first><last>Hodosh</last></author>
      <author><first>Julia</first><last>Hockenmaier</last></author>
      <doi>10.1162/tacl_a_00166</doi>
      <abstract>We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.</abstract>
      <pages>67–78</pages>
      <url hash="581eefb0">Q14-1006</url>
    </paper>
    <paper id="7">
      <title>The Language Demographics of <fixed-case>A</fixed-case>mazon <fixed-case>M</fixed-case>echanical <fixed-case>T</fixed-case>urk</title>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Ann</first><last>Irvine</last></author>
      <author><first>Dmitry</first><last>Kachaev</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <doi>10.1162/tacl_a_00167</doi>
      <abstract>We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers’ self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems.</abstract>
      <pages>79–92</pages>
      <url hash="947e2996">Q14-1007</url>
    </paper>
    <paper id="8">
      <title>Exploring the Role of Stress in <fixed-case>B</fixed-case>ayesian Word Segmentation using <fixed-case>A</fixed-case>daptor <fixed-case>G</fixed-case>rammars</title>
      <author><first>Benjamin</first><last>Börschinger</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <doi>10.1162/tacl_a_00168</doi>
      <abstract>Stress has long been established as a major cue in word segmentation for English infants. We show that enabling a current state-of-the-art Bayesian word segmentation model to take advantage of stress cues noticeably improves its performance. We find that the improvements range from 10 to 4%, depending on both the use of phonotactic cues and, to a lesser extent, the amount of evidence available to the learner. We also find that in particular early on, stress cues are much more useful for our model than phonotactic cues by themselves, consistent with the finding that children do seem to use stress cues before they use phonotactic cues. Finally, we study how the model’s knowledge about stress patterns evolves over time. We not only find that our model correctly acquires the most frequent patterns relatively quickly but also that the Unique Stress Constraint that is at the heart of a previously proposed model does not need to be built in but can be acquired jointly with word segmentation.</abstract>
      <pages>93–104</pages>
      <url hash="38a83b8e">Q14-1008</url>
    </paper>
    <paper id="9">
      <title>Parallel Algorithms for Unsupervised Tagging</title>
      <author><first>Sujith</first><last>Ravi</last></author>
      <author><first>Sergei</first><last>Vassilivitskii</last></author>
      <author><first>Vibhor</first><last>Rastogi</last></author>
      <doi>10.1162/tacl_a_00169</doi>
      <abstract>We propose a new method for unsupervised tagging that finds minimal models which are then further improved by Expectation Maximization training. In contrast to previous approaches that rely on manually specified and multi-step heuristics for model minimization, our approach is a simple greedy approximation algorithm DMLC (Distributed-Minimum-Label-Cover) that solves this objective in a single step. We extend the method and show how to efficiently parallelize the algorithm on modern parallel computing platforms while preserving approximation guarantees. The new method easily scales to large data and grammar sizes, overcoming the memory bottleneck in previous approaches. We demonstrate the power of the new algorithm by evaluating on various sequence labeling tasks: Part-of-Speech tagging for multiple languages (including low-resource languages), with complete and incomplete dictionaries, and supertagging, a complex sequence labeling task, where the grammar size alone can grow to millions of entries. Our results show that for all of these settings, our method achieves state-of-the-art scalable performance that yields high quality tagging outputs.</abstract>
      <pages>105–118</pages>
      <url hash="63676895">Q14-1009</url>
    </paper>
    <paper id="10">
      <title>A Tabular Method for Dynamic Oracles in Transition-Based Parsing</title>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Francesco</first><last>Sartorio</last></author>
      <author><first>Giorgio</first><last>Satta</last></author>
      <doi>10.1162/tacl_a_00170</doi>
      <abstract>We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages.</abstract>
      <pages>119–130</pages>
      <url hash="c74b04e3">Q14-1010</url>
    </paper>
    <paper id="11">
      <title>Joint Incremental Disfluency Detection and Dependency Parsing</title>
      <author><first>Matthew</first><last>Honnibal</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <doi>10.1162/tacl_a_00171</doi>
      <abstract>We present an incremental dependency parsing model that jointly performs disfluency detection. The model handles speech repairs using a novel non-monotonic transition system, and includes several novel classes of features. For comparison, we evaluated two pipeline systems, using state-of-the-art disfluency detectors. The joint model performed better on both tasks, with a parse accuracy of 90.5% and 84.0% accuracy at disfluency detection. The model runs in expected linear time, and processes over 550 tokens a second.</abstract>
      <pages>131–142</pages>
      <url hash="75e6ba60">Q14-1011</url>
    </paper>
    <paper id="12">
      <title>Temporal Annotation in the Clinical Domain</title>
      <author><first>William F.</first><last>Styler IV</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Sean</first><last>Finan</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Sameer</first><last>Pradhan</last></author>
      <author><first>Piet C</first><last>de Groen</last></author>
      <author><first>Brad</first><last>Erickson</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>Chen</first><last>Lin</last></author>
      <author><first>Guergana</first><last>Savova</last></author>
      <author><first>James</first><last>Pustejovsky</last></author>
      <doi>10.1162/tacl_a_00172</doi>
      <abstract>This article discusses the requirements of a formal specification for the annotation of temporal information in clinical narratives. We discuss the implementation and extension of ISO-TimeML for annotating a corpus of clinical notes, known as the THYME corpus. To reflect the information task and the heavily inference-based reasoning demands in the domain, a new annotation guideline has been developed, “the THYME Guidelines to ISO-TimeML (THYME-TimeML)”. To clarify what relations merit annotation, we distinguish between linguistically-derived and inferentially-derived temporal orderings in the text. We also apply a top performing TempEval 2013 system against this new resource to measure the difficulty of adapting systems to the clinical domain. The corpus is available to the community and has been proposed for use in a SemEval 2015 task.</abstract>
      <pages>143–154</pages>
      <url hash="4ef568ca">Q14-1012</url>
    </paper>
    <paper id="13">
      <title>Senti-<fixed-case>LSSVM</fixed-case>: Sentiment-Oriented Multi-Relation Extraction with Latent Structural <fixed-case>SVM</fixed-case></title>
      <author><first>Lizhen</first><last>Qu</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Lili</first><last>Jiang</last></author>
      <author><first>Rainer</first><last>Gemulla</last></author>
      <author><first>Gerhard</first><last>Weikum</last></author>
      <doi>10.1162/tacl_a_00173</doi>
      <abstract>Extracting instances of sentiment-oriented relations from user-generated web documents is important for online marketing analysis. Unlike previous work, we formulate this extraction task as a structured prediction problem and design the corresponding inference as an integer linear program. Our latent structural SVM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest. The empirical evaluation shows that our approach significantly outperforms state-of-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community.</abstract>
      <pages>155–168</pages>
      <url hash="1b140893">Q14-1013</url>
    </paper>
    <paper id="14">
      <title>Segmentation for Efficient Supervised Language Annotation with an Explicit Cost-Utility Tradeoff</title>
      <author><first>Matthias</first><last>Sperber</last></author>
      <author><first>Mirjam</first><last>Simantzik</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <author><first>Alex</first><last>Waibel</last></author>
      <doi>10.1162/tacl_a_00174</doi>
      <abstract>In this paper, we study the problem of manually correcting automatic annotations of natural language in as efficient a manner as possible. We introduce a method for automatically segmenting a corpus into chunks such that many uncertain labels are grouped into the same chunk, while human supervision can be omitted altogether for other segments. A tradeoff must be found for segment sizes. Choosing short segments allows us to reduce the number of highly confident labels that are supervised by the annotator, which is useful because these labels are often already correct and supervising correct labels is a waste of effort. In contrast, long segments reduce the cognitive effort due to context switches. Our method helps find the segmentation that optimizes supervision efficiency by defining user models to predict the cost and utility of supervising each segment and solving a constrained optimization problem balancing these contradictory objectives. A user study demonstrates noticeable gains over pre-segmented, confidence-ordered baselines on two natural language processing tasks: speech transcription and word segmentation.</abstract>
      <pages>169–180</pages>
      <url hash="0f7469b8">Q14-1014</url>
    </paper>
    <paper id="15">
      <title>Dynamic Language Models for Streaming Text</title>
      <author><first>Dani</first><last>Yogatama</last></author>
      <author><first>Chong</first><last>Wang</last></author>
      <author><first>Bryan R.</first><last>Routledge</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Eric P.</first><last>Xing</last></author>
      <doi>10.1162/tacl_a_00175</doi>
      <abstract>We present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features. These context features serve as important indicators of language changes that are otherwise difficult to capture using text data by itself. We learn our model in an efficient online fashion that is scalable for large, streaming data. With five streaming datasets from two different genres—economics news articles and social media—we evaluate our model on the task of sequential language modeling. Our model consistently outperforms competing models.</abstract>
      <pages>181–192</pages>
      <url hash="54deea67">Q14-1015</url>
    </paper>
    <paper id="16">
      <title>Discriminative Lexical Semantic Segmentation with Gaps: Running the <fixed-case>MWE</fixed-case> Gamut</title>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Emily</first><last>Danchik</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <doi>10.1162/tacl_a_00176</doi>
      <abstract>We present a novel representation, evaluation measure, and supervised models for the task of identifying the multiword expressions (MWEs) in a sentence, resulting in a lexical semantic segmentation. Our approach generalizes a standard chunking representation to encode MWEs containing gaps, thereby enabling efficient sequence tagging algorithms for feature-rich discriminative models. Experiments on a new dataset of English web text offer the first linguistically-driven evaluation of MWE identification with truly heterogeneous expression types. Our statistical sequence model greatly outperforms a lookup-based segmentation procedure, achieving nearly 60% F1 for MWE identification.</abstract>
      <pages>193–206</pages>
      <url hash="7b3c0a3f">Q14-1016</url>
    </paper>
    <paper id="17">
      <title>Grounded Compositional Semantics for Finding and Describing Images with Sentences</title>
      <author><first>Richard</first><last>Socher</last></author>
      <author><first>Andrej</first><last>Karpathy</last></author>
      <author><first>Quoc V.</first><last>Le</last></author>
      <author><first>Christopher D.</first><last>Manning</last></author>
      <author><first>Andrew Y.</first><last>Ng</last></author>
      <doi>10.1162/tacl_a_00177</doi>
      <abstract>Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.</abstract>
      <pages>207–218</pages>
      <url hash="54aa1604">Q14-1017</url>
    </paper>
    <paper id="18">
      <title>Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence</title>
      <author><first>Md Arafat</first><last>Sultan</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Tamara</first><last>Sumner</last></author>
      <doi>10.1162/tacl_a_00178</doi>
      <abstract>We present a simple, easy-to-replicate monolingual aligner that demonstrates state-of-the-art performance while relying on almost no supervision and a very small number of external resources. Based on the hypothesis that words with similar meanings represent potential pairs for alignment if located in similar contexts, we propose a system that operates by finding such pairs. In two intrinsic evaluations on alignment test data, our system achieves F1 scores of 88–92%, demonstrating 1–3% absolute improvement over the previous best system. Moreover, in two extrinsic evaluations our aligner outperforms existing aligners, and even a naive application of the aligner approaches state-of-the-art performance in each extrinsic task.</abstract>
      <pages>219–230</pages>
      <url hash="a2074048">Q14-1018</url>
    </paper>
    <paper id="19">
      <title>Entity Linking meets Word Sense Disambiguation: a Unified Approach</title>
      <author><first>Andrea</first><last>Moro</last></author>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <doi>10.1162/tacl_a_00179</doi>
      <abstract>Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language. But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense. In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations. Our experiments show state-of-the-art performances on both tasks on 6 different datasets, including a multilingual setting. Babelfy is online at http://babelfy.org</abstract>
      <pages>231–244</pages>
      <url hash="726c4f64">Q14-1019</url>
    </paper>
    <paper id="20">
      <title>Crosslingual and Multilingual Construction of Syntax-Based Vector Space Models</title>
      <author><first>Jason</first><last>Utt</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <doi>10.1162/tacl_a_00180</doi>
      <abstract>Syntax-based distributional models of lexical semantics provide a flexible and linguistically adequate representation of co-occurrence information. However, their construction requires large, accurately parsed corpora, which are unavailable for most languages. In this paper, we develop a number of methods to overcome this obstacle. We describe (a) a crosslingual approach that constructs a syntax-based model for a new language requiring only an English resource and a translation lexicon; and (b) multilingual approaches that combine crosslingual with monolingual information, subject to availability. We evaluate on two lexical semantic benchmarks in German and Croatian. We find that the models exhibit complementary profiles: crosslingual models yield higher accuracies while monolingual models provide better coverage. In addition, we show that simple multilingual models can successfully combine their strengths.</abstract>
      <pages>245–258</pages>
      <url hash="aca8ea16">Q14-1020</url>
    </paper>
    <paper id="21">
      <title>Entity Linking on Microblogs with Spatial and Temporal Signals</title>
      <author><first>Yuan</first><last>Fang</last></author>
      <author><first>Ming-Wei</first><last>Chang</last></author>
      <doi>10.1162/tacl_a_00181</doi>
      <abstract>Microblogs present an excellent opportunity for monitoring and analyzing world happenings. Given that words are often ambiguous, entity linking becomes a crucial step towards understanding microblogs. In this paper, we re-examine the problem of entity linking on microblogs. We first observe that spatiotemporal (i.e., spatial and temporal) signals play a key role, but they are not utilized in existing approaches. Thus, we propose a novel entity linking framework that incorporates spatiotemporal signals through a weakly supervised process. Using entity annotations on real-world data, our experiments show that the spatiotemporal model improves F1 by more than 10 points over existing systems. Finally, we present a qualitative study to visualize the effectiveness of our approach.</abstract>
      <pages>259–272</pages>
      <url hash="2b03fa22">Q14-1021</url>
    </paper>
    <paper id="22">
      <title>Dense Event Ordering with a Multi-Pass Architecture</title>
      <author><first>Nathanael</first><last>Chambers</last></author>
      <author><first>Taylor</first><last>Cassidy</last></author>
      <author><first>Bill</first><last>McDowell</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <doi>10.1162/tacl_a_00182</doi>
      <abstract>The past 10 years of event ordering research has focused on learning partial orderings over document events and time expressions. The most popular corpus, the TimeBank, contains a small subset of the possible ordering graph. Many evaluations follow suit by only testing certain pairs of events (e.g., only main verbs of neighboring sentences). This has led most research to focus on specific learners for partial labelings. This paper attempts to nudge the discussion from identifying some relations to all relations. We present new experiments on strongly connected event graphs that contain ∼10 times more relations per document than the TimeBank. We also describe a shift away from the single learner to a sieve-based architecture that naturally blends multiple learners into a precision-ranked cascade of sieves. Each sieve adds labels to the event graph one at a time, and earlier sieves inform later ones through transitive closure. This paper thus describes innovations in both approach and task. We experiment on the densest event graphs to date and show a 14% gain over state-of-the-art.</abstract>
      <pages>273–284</pages>
      <url hash="a8f5f3ab">Q14-1022</url>
    </paper>
    <paper id="23">
      <title>Multi-Modal Models for Concrete and Abstract Concept Meaning</title>
      <author><first>Felix</first><last>Hill</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <doi>10.1162/tacl_a_00183</doi>
      <abstract>Multi-modal models that learn semantic representations from both linguistic and perceptual input outperform language-only models on a range of evaluations, and better reflect human concept acquisition. Most perceptual input to such models corresponds to concrete noun concepts and the superiority of the multi-modal approach has only been established when evaluating on such concepts. We therefore investigate which concepts can be effectively learned by multi-modal models. We show that concreteness determines both which linguistic features are most informative and the impact of perceptual input in such models. We then introduce ridge regression as a means of propagating perceptual information from concrete nouns to more abstract concepts that is more robust than previous approaches. Finally, we present weighted gram matrix combination, a means of combining representations from distinct modalities that outperforms alternatives when both modalities are sufficiently rich.</abstract>
      <pages>285–296</pages>
      <url hash="6afdbf44">Q14-1023</url>
    </paper>
    <paper id="24">
      <title>Exploiting Social Network Structure for Person-to-Person Sentiment Analysis</title>
      <author><first>Robert</first><last>West</last></author>
      <author><first>Hristo S.</first><last>Paskov</last></author>
      <author><first>Jure</first><last>Leskovec</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <doi>10.1162/tacl_a_00184</doi>
      <abstract>Person-to-person evaluations are prevalent in all kinds of discourse and important for establishing reputations, building social bonds, and shaping public opinion. Such evaluations can be analyzed separately using signed social networks and textual sentiment analysis, but this misses the rich interactions between language and social context. To capture such interactions, we develop a model that predicts individual A’s opinion of individual B by synthesizing information from the signed social network in which A and B are embedded with sentiment analysis of the evaluative texts relating A to B. We prove that this problem is NP-hard but can be relaxed to an efficiently solvable hinge-loss Markov random field, and we show that this implementation outperforms text-only and network-only versions in two very different datasets involving community-level decision-making: the Wikipedia Requests for Adminship corpus and the Convote U.S. Congressional speech corpus.</abstract>
      <pages>297–310</pages>
      <url hash="de5b6c2c">Q14-1024</url>
    </paper>
    <paper id="25">
      <title>The Benefits of a Model of Annotation</title>
      <author><first>Rebecca J.</first><last>Passonneau</last></author>
      <author><first>Bob</first><last>Carpenter</last></author>
      <doi>10.1162/tacl_a_00185</doi>
      <abstract>Standard agreement measures for interannotator reliability are neither necessary nor sufficient to ensure a high quality corpus. In a case study of word sense annotation, conventional methods for evaluating labels from trained annotators are contrasted with a probabilistic annotation model applied to crowdsourced data. The annotation model provides far more information, including a certainty measure for each gold standard label; the crowdsourced data was collected at less than half the cost of the conventional approach.</abstract>
      <pages>311–326</pages>
      <url hash="4b7c56a4">Q14-1025</url>
      <erratum id="1" hash="6bfc7b9e">Q14-1025e1</erratum>
    </paper>
    <paper id="26">
      <title>Improved <fixed-case>CCG</fixed-case> Parsing with Semi-supervised Supertagging</title>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <doi>10.1162/tacl_a_00186</doi>
      <abstract>Current supervised parsers are limited by the size of their labelled training data, making improving them with unlabelled data an important goal. We show how a state-of-the-art CCG parser can be enhanced, by predicting lexical categories using unsupervised vector-space embeddings of words. The use of word embeddings enables our model to better generalize from the labelled data, and allows us to accurately assign lexical categories without depending on a POS-tagger. Our approach leads to substantial improvements in dependency parsing results over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8%), Wikipedia (1.8%) and biomedical (3.4%) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy.</abstract>
      <pages>327–338</pages>
      <url hash="9740cd0f">Q14-1026</url>
    </paper>
    <paper id="27">
      <title>2-Slave Dual Decomposition for Generalized Higher Order <fixed-case>CRF</fixed-case>s</title>
      <author><first>Xian</first><last>Qian</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <doi>10.1162/tacl_a_00187</doi>
      <abstract>We show that the decoding problem in generalized Higher Order Conditional Random Fields (CRFs) can be decomposed into two parts: one is a tree labeling problem that can be solved in linear time using dynamic programming; the other is a supermodular quadratic pseudo-Boolean maximization problem, which can be solved in cubic time using a minimum cut algorithm. We use dual decomposition to force their agreement. Experimental results on Twitter named entity recognition and sentence dependency tagging tasks show that our method outperforms spanning tree based dual decomposition.</abstract>
      <pages>339–350</pages>
      <url hash="e8fa695f">Q14-1027</url>
    </paper>
    <paper id="28">
      <title><fixed-case>T</fixed-case>ree<fixed-case>T</fixed-case>alk: Composition and Compression of Trees for Image Descriptions</title>
      <author><first>Polina</first><last>Kuznetsova</last></author>
      <author><first>Vicente</first><last>Ordonez</last></author>
      <author><first>Tamara L.</first><last>Berg</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <doi>10.1162/tacl_a_00188</doi>
      <abstract>We present a new tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions. We investigate two related tasks: image caption generalization and generation, where the former is an optional subtask of the latter. The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions, then to compose a new description by selectively combining the extracted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions.</abstract>
      <pages>351–362</pages>
      <url hash="0efeb544">Q14-1028</url>
    </paper>
    <paper id="29">
      <title>Unsupervised Discovery of Biographical Structure from Text</title>
      <author><first>David</first><last>Bamman</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <doi>10.1162/tacl_a_00189</doi>
      <abstract>We present a method for discovering abstract event classes in biographies, based on a probabilistic latent-variable model. Taking as input timestamped text, we exploit latent correlations among events to learn a set of event classes (such as Born, Graduates High School, and Becomes Citizen), along with the typical times in a person’s life when those events occur. In a quantitative evaluation at the task of predicting a person’s age for a given event, we find that our generative model outperforms a strong linear regression baseline, along with simpler variants of the model that ablate some features. The abstract event classes that we learn allow us to perform a large-scale analysis of 242,970 Wikipedia biographies. Though it is known that women are greatly underrepresented on Wikipedia—not only as editors (Wikipedia, 2011) but also as subjects of articles (Reagle and Rhue, 2011)—we find that there is a bias in their characterization as well, with biographies of women containing significantly more emphasis on events of marriage and divorce than biographies of men.</abstract>
      <pages>363–376</pages>
      <url hash="28517f1c">Q14-1029</url>
    </paper>
    <paper id="30">
      <title>Large-scale Semantic Parsing without Question-Answer Pairs</title>
      <author><first>Siva</first><last>Reddy</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <doi>10.1162/tacl_a_00190</doi>
      <abstract>In this paper we introduce a novel semantic parsing approach to query Freebase in natural language without requiring manual annotations or question-answer pairs. Our key insight is to represent natural language via semantic graphs whose topology shares many commonalities with Freebase. Given this representation, we conceptualize semantic parsing as a graph matching problem. Our model converts sentences to semantic graphs using CCG and subsequently grounds them to Freebase guided by denotations as a form of weak supervision. Evaluation experiments on a subset of the Free917 and WebQuestions benchmark datasets show our semantic parser improves over the state of the art.</abstract>
      <pages>377–392</pages>
      <url hash="528446f1">Q14-1030</url>
    </paper>
    <paper id="31">
      <title>Locally Non-Linear Learning for Statistical Machine Translation via Discretization and Structured Regularization</title>
      <author><first>Jonathan H.</first><last>Clark</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <author><first>Alon</first><last>Lavie</last></author>
      <doi>10.1162/tacl_a_00191</doi>
      <abstract>Linear models, which support efficient learning and inference, are the workhorses of statistical machine translation; however, linear decision rules are less attractive from a modeling perspective. In this work, we introduce a technique for learning arbitrary, rule-local, non-linear feature transforms that improve model expressivity, but do not sacrifice the efficient inference and learning associated with linear models. To demonstrate the value of our technique, we discard the customary log transform of lexical probabilities and drop the phrasal translation probability in favor of raw counts. We observe that our algorithm learns a variation of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers.</abstract>
      <pages>393–404</pages>
      <url hash="6aa559db">Q14-1031</url>
    </paper>
    <paper id="32">
      <title>A New Parsing Algorithm for <fixed-case>C</fixed-case>ombinatory <fixed-case>C</fixed-case>ategorial <fixed-case>G</fixed-case>rammar</title>
      <author><first>Marco</first><last>Kuhlmann</last></author>
      <author><first>Giorgio</first><last>Satta</last></author>
      <doi>10.1162/tacl_a_00192</doi>
      <abstract>We present a polynomial-time parsing algorithm for CCG, based on a new decomposition of derivations into small, shareable parts. Our algorithm has the same asymptotic complexity, O(n6), as a previous algorithm by Vijay-Shanker and Weir (1993), but is easier to understand, implement, and prove correct.</abstract>
      <pages>405–418</pages>
      <url hash="1567a7bd">Q14-1032</url>
    </paper>
    <paper id="33">
      <title>Building a State-of-the-Art Grammatical Error Correction System</title>
      <author><first>Alla</first><last>Rozovskaya</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <doi>10.1162/tacl_a_00193</doi>
      <abstract>This paper identifies and examines the key principles underlying building a state-of-the-art grammatical error correction system. We do this by analyzing the Illinois system that placed first among seventeen teams in the recent CoNLL-2013 shared task on grammatical error correction. The system focuses on five different types of errors common among non-native English writers. We describe four design principles that are relevant for correcting all of these errors, analyze the system along these dimensions, and show how each of these dimensions contributes to the performance.</abstract>
      <pages>419–434</pages>
      <url hash="68e13250">Q14-1033</url>
    </paper>
    <paper id="34">
      <title>Extracting Lexically Divergent Paraphrases from <fixed-case>T</fixed-case>witter</title>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>William B.</first><last>Dolan</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <doi>10.1162/tacl_a_00194</doi>
      <abstract>We present MultiP (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent variable model alone, we achieve the performance competitive with a state-of-the-art method which combines a latent space model with a feature-based supervised classifier. Our model also captures lexically divergent paraphrases that differ from yet complement previous methods; combining our model with previous work significantly outperforms the state-of-the-art. In addition, we present a novel annotation methodology that has allowed us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community.</abstract>
      <pages>435–448</pages>
      <url hash="582ea570">Q14-1034</url>
    </paper>
    <paper id="35">
      <title>It’s All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation</title>
      <author><first>David</first><last>Jurgens</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <doi>10.1162/tacl_a_00195</doi>
      <abstract>Annotated data is prerequisite for many NLP applications. Acquiring large-scale annotated corpora is a major bottleneck, requiring significant time and resources. Recent work has proposed turning annotation into a game to increase its appeal and lower its cost; however, current games are largely text-based and closely resemble traditional annotation tasks. We propose a new linguistic annotation paradigm that produces annotations from playing graphical video games. The effectiveness of this design is demonstrated using two video games: one to create a mapping from WordNet senses to images, and a second game that performs Word Sense Disambiguation. Both games produce accurate results. The first game yields annotation quality equal to that of experts and a cost reduction of 73% over equivalent crowdsourcing; the second game provides a 16.3% improvement in accuracy over current state-of-the-art sense disambiguation games with WordNet.</abstract>
      <pages>449–464</pages>
      <url hash="ebf246fb">Q14-1035</url>
      <video href="https://vimeo.com/150290361" tag="video"/>
    </paper>
    <paper id="36">
      <title>Online <fixed-case>A</fixed-case>daptor <fixed-case>G</fixed-case>rammars with Hybrid Inference</title>
      <author><first>Ke</first><last>Zhai</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <doi>10.1162/tacl_a_00196</doi>
      <abstract>Adaptor grammars are a flexible, powerful formalism for defining nonparametric, unsupervised models of grammar productions. This flexibility comes at the cost of expensive inference. We address the difficulty of inference through an online algorithm which uses a hybrid of Markov chain Monte Carlo and variational inference. We show that this inference strategy improves scalability without sacrificing performance on unsupervised word segmentation and topic modeling tasks.</abstract>
      <pages>465–476</pages>
      <url hash="8ab1cbe0">Q14-1036</url>
    </paper>
    <paper id="37">
      <title>A Joint Model for Entity Analysis: Coreference, Typing, and Linking</title>
      <author><first>Greg</first><last>Durrett</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <doi>10.1162/tacl_a_00197</doi>
      <abstract>We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.</abstract>
      <pages>477–490</pages>
      <url hash="a510903b">Q14-1037</url>
    </paper>
    <paper id="38">
      <title>Learning Strictly Local Subsequential Functions</title>
      <author><first>Jane</first><last>Chandlee</last></author>
      <author><first>Rémi</first><last>Eyraud</last></author>
      <author><first>Jeffrey</first><last>Heinz</last></author>
      <doi>10.1162/tacl_a_00198</doi>
      <abstract>We define two proper subclasses of subsequential functions based on the concept of Strict Locality (McNaughton and Papert, 1971; Rogers and Pullum, 2011; Rogers et al., 2013) for formal languages. They are called Input and Output Strictly Local (ISL and OSL). We provide an automata-theoretic characterization of the ISL class and theorems establishing how the classes are related to each other and to Strictly Local languages. We give evidence that local phonological and morphological processes belong to these classes. Finally we provide a learning algorithm which provably identifies the class of ISL functions in the limit from positive data in polynomial time and data. We demonstrate this learning result on appropriately synthesized artificial corpora. We leave a similar learning result for OSL functions for future work and suggest future directions for addressing non-local phonological processes.</abstract>
      <pages>491–504</pages>
      <url hash="61e2d4ce">Q14-1038</url>
    </paper>
    <paper id="39">
      <title>Joint Modeling of Opinion Expression Extraction and Attribute Classification</title>
      <author><first>Bishan</first><last>Yang</last></author>
      <author><first>Claire</first><last>Cardie</last></author>
      <doi>10.1162/tacl_a_00199</doi>
      <abstract>In this paper, we study the problems of opinion expression extraction and expression-level polarity and intensity classification. Traditional fine-grained opinion analysis systems address these problems in isolation and thus cannot capture interactions among the textual spans of opinion expressions and their opinion-related properties. We present two types of joint approaches that can account for such interactions during 1) both learning and inference or 2) only during inference. Extensive experiments on a standard dataset demonstrate that our approaches provide substantial improvements over previously published results. By analyzing the results, we gain some insight into the advantages of different joint models.</abstract>
      <pages>505–516</pages>
      <url hash="2f0e7c8d">Q14-1039</url>
    </paper>
    <paper id="40">
      <title>Predicting the Difficulty of Language Proficiency Tests</title>
      <author><first>Lisa</first><last>Beinborn</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/tacl_a_00200</doi>
      <abstract>Language proficiency tests are used to evaluate and compare the progress of language learners. We present an approach for automatic difficulty prediction of C-tests that performs on par with human experts. On the basis of detailed analysis of newly collected data, we develop a model for C-test difficulty introducing four dimensions: solution difficulty, candidate ambiguity, inter-gap dependency, and paragraph difficulty. We show that cues from all four dimensions contribute to C-test difficulty.</abstract>
      <pages>517–530</pages>
      <url hash="e8b27b62">Q14-1040</url>
    </paper>
    <paper id="41">
      <title>A Large Scale Evaluation of Distributional Semantic Models: Parameters, Interactions and Model Selection</title>
      <author><first>Gabriella</first><last>Lapesa</last></author>
      <author><first>Stefan</first><last>Evert</last></author>
      <doi>10.1162/tacl_a_00201</doi>
      <abstract>This paper presents the results of a large-scale evaluation study of window-based Distributional Semantic Models on a wide variety of tasks. Our study combines a broad coverage of model parameters with a model selection methodology that is robust to overfitting and able to capture parameter interactions. We show that our strategy allows us to identify parameter configurations that achieve good performance across different datasets and tasks.</abstract>
      <pages>531–546</pages>
      <url hash="f95d1830">Q14-1041</url>
    </paper>
    <paper id="42">
      <title>A New Corpus and Imitation Learning Framework for Context-Dependent Semantic Parsing</title>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <author><first>Stephen</first><last>Clark</last></author>
      <doi>10.1162/tacl_a_00202</doi>
      <abstract>Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation. Most approaches to this task have been evaluated on a small number of existing corpora which assume that all utterances must be interpreted according to a database and typically ignore context. In this paper we present a new, publicly available corpus for context-dependent semantic parsing. The MRL used for the annotation was designed to support a portable, interactive tourist information system. We develop a semantic parser for this corpus by adapting the imitation learning algorithm DAgger without requiring alignment information during training. DAgger improves upon independently trained classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively.</abstract>
      <pages>547–560</pages>
      <url hash="4c266b76">Q14-1042</url>
    </paper>
    <paper id="43">
      <title>Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment</title>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <author><first>Tao</first><last>Lei</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <author><first>Amir</first><last>Globerson</last></author>
      <doi>10.1162/tacl_a_00203</doi>
      <abstract>Prepositional phrase (PP) attachment disambiguation is a known challenge in syntactic parsing. The lexical sparsity associated with PP attachments motivates research in word representations that can capture pertinent syntactic and semantic features of the word. One promising solution is to use word vectors induced from large amounts of raw text. However, state-of-the-art systems that employ such representations yield modest gains in PP attachment accuracy. In this paper, we show that word vector representations can yield significant PP attachment performance gains. This is achieved via a non-linear architecture that is discriminatively trained to maximize PP attachment accuracy. The architecture is initialized with word vectors trained from unlabeled data, and relearns those to maximize attachment accuracy. We obtain additional performance gains with alternative representations such as dependency-based word vectors. When tested on both English and Arabic datasets, our method outperforms both a strong SVM classifier and state-of-the-art parsers. For instance, we achieve 82.6% PP attachment accuracy on Arabic, while the Turbo and Charniak self-trained parsers obtain 76.7% and 80.8% respectively.</abstract>
      <pages>561–572</pages>
      <url hash="c939cc93">Q14-1043</url>
      <erratum id="1" hash="bab82a07">Q14-1043e1</erratum>
    </paper>
  </volume>
</collection>
