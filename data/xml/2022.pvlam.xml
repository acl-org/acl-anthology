<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.pvlam">
  <volume id="1" ingest-date="2022-09-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on People in Vision, Language, and the Mind</booktitle>
      <editor><first>Patrizia</first><last>Paggio</last></editor>
      <editor><first>Albert</first><last>Gatt</last></editor>
      <editor><first>Marc</first><last>Tanti</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>June</month>
      <year>2022</year>
      <url hash="c634e653">2022.pvlam-1</url>
      <venue>pvlam</venue>
    </meta>
    <frontmatter>
      <url hash="18332a1f">2022.pvlam-1.0</url>
      <bibkey>pvlam-2022-people</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Exploring the <fixed-case>GLIDE</fixed-case> model for Human Action Effect Prediction</title>
      <author><first>Fangjun</first><last>Li</last></author>
      <author><first>David C.</first><last>Hogg</last></author>
      <author><first>Anthony G.</first><last>Cohn</last></author>
      <pages>1–5</pages>
      <abstract>We address the following action-effect prediction task. Given an image depicting an initial state of the world and an action expressed in text, predict an image depicting the state of the world following the action. The prediction should have the same scene context as the input image. We explore the use of the recently proposed GLIDE model for performing this task. GLIDE is a generative neural network that can synthesize (inpaint) masked areas of an image, conditioned on a short piece of text. Our idea is to mask-out a region of the input image where the effect of the action is expected to occur. GLIDE is then used to inpaint the masked region conditioned on the required action. In this way, the resulting image has the same background context as the input image, updated to show the effect of the action. We give qualitative results from experiments using the EPIC dataset of ego-centric videos labelled with actions.</abstract>
      <url hash="3922de7c">2022.pvlam-1.1</url>
      <bibkey>li-etal-2022-exploring</bibkey>
    </paper>
    <paper id="2">
      <title>Do Multimodal Emotion Recognition Models Tackle Ambiguity?</title>
      <author><first>Hélène</first><last>Tran</last></author>
      <author><first>Issam</first><last>Falih</last></author>
      <author><first>Xavier</first><last>Goblet</last></author>
      <author><first>Engelbert</first><last>Mephu Nguifo</last></author>
      <pages>6–11</pages>
      <abstract>Most databases used for emotion recognition assign a single emotion to data samples. This does not match with the complex nature of emotions: we can feel a wide range of emotions throughout our lives with varying degrees of intensity. We may even experience multiple emotions at once. Furthermore, each person physically expresses emotions differently, which makes emotion recognition even more challenging: we call this emotional ambiguity. This paper investigates the problem as a review of ambiguity in multimodal emotion recognition models. To lay the groundwork, the main representations of emotions along with solutions for incorporating ambiguity are described, followed by a brief overview of ambiguity representation in multimodal databases. Thereafter, only models trained on a database that incorporates ambiguity have been studied in this paper. We conclude that although databases provide annotations with ambiguity, most of these models do not fully exploit them, showing that there is still room for improvement in multimodal emotion recognition systems.</abstract>
      <url hash="7cb31a30">2022.pvlam-1.2</url>
      <bibkey>tran-etal-2022-multimodal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
    </paper>
    <paper id="3">
      <title>Development of a <fixed-case>M</fixed-case>ulti<fixed-case>M</fixed-case>odal Annotation Framework and Dataset for Deep Video Understanding</title>
      <author><first>Erika</first><last>Loc</last></author>
      <author><first>Keith</first><last>Curtis</last></author>
      <author><first>George</first><last>Awad</last></author>
      <author><first>Shahzad</first><last>Rajput</last></author>
      <author><first>Ian</first><last>Soboroff</last></author>
      <pages>12–16</pages>
      <abstract>In this paper we introduce our approach and methods for collecting and annotating a new dataset for deep video understanding. The proposed dataset is composed of 3 seasons (15 episodes) of the BBC Land Girls TV Series in addition to 14 Creative Common movies with total duration of 28.5 hr. The main contribution of this paper is a novel annotation framework on the movie and scene levels to support an automatic query generation process that can capture the high-level movie features (e.g. how characters and locations are related to each other) as well as fine grained scene-level features (e.g. character interactions, natural language descriptions, and sentiments). Movie-level annotations include constructing a global static knowledge graph (KG) to capture major relationships, while the scene-level annotations include constructing a sequence of knowledge graphs (KGs) to capture fine-grained features. The annotation framework supports generating multiple query types. The objective of the framework is to provide a guide to annotating long duration videos to support tasks and challenges in the video and multimedia understanding domains. These tasks and challenges can support testing automatic systems on their ability to learn and comprehend a movie or long video in terms of actors, entities, events, interactions and their relationship to each other.</abstract>
      <url hash="5d2aae92">2022.pvlam-1.3</url>
      <bibkey>loc-etal-2022-development</bibkey>
    </paper>
    <paper id="4">
      <title>Cognitive States and Types of Nods</title>
      <author><first>Taiga</first><last>Mori</last></author>
      <author><first>Kristiina</first><last>Jokinen</last></author>
      <author><first>Yasuharu</first><last>Den</last></author>
      <pages>17–25</pages>
      <abstract>In this paper we will study how different types of nods are related to the cognitive states of the listener. The distinction is made between nods with movement starting upwards (up-nods) and nods with movement starting downwards (down-nods) as well as between single or repetitive nods. The data is from Japanese multiparty conversations, and the results accord with the previous findings indicating that up-nods are related to the change in the listener’s cognitive state after hearing the partner’s contribution, while down-nods convey the meaning that the listener’s cognitive state is not changed.</abstract>
      <url hash="19ad83e4">2022.pvlam-1.4</url>
      <bibkey>mori-etal-2022-cognitive</bibkey>
    </paper>
    <paper id="5">
      <title>Examining the Effects of Language-and-Vision Data Augmentation for Generation of Descriptions of Human Faces</title>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Rafal</first><last>Černiavski</last></author>
      <author><first>Eva Elžbieta</first><last>Sventickaitė</last></author>
      <author><first>Viktorija</first><last>Buzaitė</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>26–40</pages>
      <abstract>We investigate how different augmentation techniques on both textual and visual representations affect the performance of the face description generation model. Specifically, we provide the model with either original images, sketches of faces, facial composites or distorted images. In addition, on the language side, we experiment with different methods to augment the original dataset with paraphrased captions, which are semantically equivalent to the original ones, but differ in terms of their form. We also examine if augmenting the dataset with descriptions from a different domain (e.g., image captions of real-world images) has an effect on the performance of the models. We train models on different combinations of visual and linguistic features and perform both (i) automatic evaluation of generated captions and (ii) examination of how useful different visual features are for the task of facial feature classification. Our results show that although original images encode the best possible representation for the task, the model trained on sketches can still perform relatively well. We also observe that augmenting the dataset with descriptions from a different domain can boost performance of the model. We conclude that face description generation systems are more susceptible to language rather than vision data augmentation. Overall, we demonstrate that face caption generation models display a strong imbalance in the utilisation of language and vision modalities, indicating a lack of proper information fusion. We also describe ethical implications of our study and argue that future work on human face description generation should create better, more representative datasets.</abstract>
      <url hash="803a7b44">2022.pvlam-1.5</url>
      <bibkey>ilinykh-etal-2022-examining</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/celeba-hq">CelebA-HQ</pwcdataset>
    </paper>
    <paper id="6">
      <title><fixed-case>F</fixed-case>ace2<fixed-case>T</fixed-case>ext revisited: Improved data set and baseline results</title>
      <author><first>Marc</first><last>Tanti</last></author>
      <author><first>Shaun</first><last>Abdilla</last></author>
      <author><first>Adrian</first><last>Muscat</last></author>
      <author><first>Claudia</first><last>Borg</last></author>
      <author><first>Reuben A.</first><last>Farrugia</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <pages>41–47</pages>
      <abstract>Current image description generation models do not transfer well to the task of describing human faces. To encourage the development of more human-focused descriptions, we developed a new data set of facial descriptions based on the CelebA image data set. We describe the properties of this data set, and present results from a face description generator trained on it, which explores the feasibility of using transfer learning from VGGFace/ResNet CNNs. Comparisons are drawn through both automated metrics and human evaluation by 76 English-speaking participants. The descriptions generated by the VGGFace-LSTM + Attention model are closest to the ground truth according to human evaluation whilst the ResNet-LSTM + Attention model obtained the highest CIDEr and CIDEr-D results (1.252 and 0.686 respectively). Together, the new data set and these experimental results provide data and baselines for future work in this area.</abstract>
      <url hash="14a4fbb2">2022.pvlam-1.6</url>
      <bibkey>tanti-etal-2022-face2text</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/celeba">CelebA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">MS COCO</pwcdataset>
    </paper>
  </volume>
</collection>
