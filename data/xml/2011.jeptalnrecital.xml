<?xml version='1.0' encoding='UTF-8'?>
<collection id="2011.jeptalnrecital">
  <volume id="invite" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 18e conférence sur le Traitement Automatique des Langues Naturelles. Conférences invitées</booktitle>
      <editor><first>Mathieu</first><last>Lafourcade</last></editor>
      <editor><first>Violaine</first><last>Prince</last></editor>
      <publisher>ATALA</publisher>
      <address>Montpellier, France</address>
      <month>June</month>
      <year>2011</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="0a23573c">2011.jeptalnrecital-invite.0</url>
      <bibkey>jep-taln-recital-2011-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Les perspectives révélées par la théorie des K-représentations pour la bioinformatique et le Web sémantique (The prospects revealed by the theory of K-representations for bioinformatics and Semantic Web)</title>
      <author><first>Vladimir</first><last>A. Fomichov</last></author>
      <pages>1–16</pages>
      <abstract>L’article décrit la structure et les applications possibles de la théorie des K-représentations (représentation des connaissances) dans la bioinformatique afin de développer un Réseau Sémantique d’une génération nouvelle. La théorie des K-répresentations est une théorie originale du développement des analyseurs sémantico–syntactiques avec l’utilisation large des moyens formels pour décrire les données d’entrée, intermédiaires et de sortie. Cette théorie est décrit dans la monographie de V. Fomichov (Springer, 2010). La première partie de la théorie est un modèle formel d’un système qui est composé de dix opérations sur les structures conceptuelles. Ce modèle définit une classe nouvelle des langages formels – la classe des SK-langages. Les possibilités larges de construire des répresentations sémantiques des discours compliqués en rapport à la biologie sont manifestes. Une approche formelle nouvelle de l’élaboration des analysateurs multilinguistiques sémantico-syntactiques est décrite. Cet approche a été implémentée sous la forme d’un programme en langage PYTHON.</abstract>
      <url hash="61a4d8e8">2011.jeptalnrecital-invite.1</url>
      <language>fra</language>
      <bibkey>a-fomichov-2011-les</bibkey>
    </paper>
    <paper id="2">
      <title>Theorie et Praxis Une optique sur les travaux en <fixed-case>TAL</fixed-case> sur le discours et le dialogue (Theory and Praxis A view on the <fixed-case>NLP</fixed-case> works in discourse and dialogue)</title>
      <author><first>Nicholas</first><last>Asher</last></author>
      <pages>17–17</pages>
      <abstract/>
      <url hash="1625c504">2011.jeptalnrecital-invite.2</url>
      <language>fra</language>
      <bibkey>asher-2011-theorie</bibkey>
    </paper>
    <paper id="3">
      <title>Génération de phrase : entrée, algorithmes et applications (Sentence Generation: Input, Algorithms and Applications)</title>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>18–18</pages>
      <abstract/>
      <url hash="ccf7b17a">2011.jeptalnrecital-invite.3</url>
      <language>fra</language>
      <bibkey>gardent-2011-generation</bibkey>
    </paper>
  </volume>
  <volume id="long" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 18e conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Mathieu</first><last>Lafourcade</last></editor>
      <editor><first>Violaine</first><last>Prince</last></editor>
      <publisher>ATALA</publisher>
      <address>Montpellier, France</address>
      <month>June</month>
      <year>2011</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="217668fd">2011.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2011-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Patrons de phrase, raccourcis pour apprendre rapidement à parler une nouvelle langue (Sentence patterns, shortcuts to quickly learn to speak a new language)</title>
      <author><first>Michael</first><last>Zock</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <pages>1–12</pages>
      <abstract>Nous décrivons la création d’un environnement web pour aider des apprenants (adolescents ou adultes) à acquérir les automatismes nécessaires pour produire à un débit “normal” les structures fondamentales d’une langue. Notre point de départ est une base de données de phrases, glanées sur le web ou issues de livres scolaires ou de livres de phrases. Ces phrases ont été généralisées (remplacement de mots par des variables) et indexées en termes de buts pour former une arborescence de patrons. Ces deux astuces permettent de motiver l’usage des patrons et de crééer des phrases structurellement identiques à celles rencontrées, tout en étant sémantiquement différentes. Si les notions de ‘patrons’ ou de ‘phrases à trou implicitement typées’ ne sont pas nouvelles, le fait de les avoir portées sur ordinateur pour apprendre des langues l’est. Le système étant conçu pour être ouvert, il permet aux utilisateurs, concepteurs ou apprenants, des changements sur de nombreux points importants : le nom des variables, leurs valeurs, le laps de temps entre une question et sa réponse, etc. La version initiale a été développée pour l’anglais et le japonais. Pour tester la généricité de notre approche nous y avons ajouté relativement facilement le français et le chinois.</abstract>
      <url hash="d2caec54">2011.jeptalnrecital-long.1</url>
      <language>fra</language>
      <bibkey>zock-lapalme-2011-patrons</bibkey>
    </paper>
    <paper id="2">
      <title>Génération automatique de motifs de détection d’entités nommées en utilisant des contenus encyclopédiques (Automatic generation of named entity detection patterns using encyclopedic contents)</title>
      <author><first>Eric</first><last>Charton</last></author>
      <author><first>Michel</first><last>Gagnon</last></author>
      <author><first>Benoit</first><last>Ozell</last></author>
      <pages>13–24</pages>
      <abstract>Les encyclopédies numériques contiennent aujourd’hui de vastes inventaires de formes d’écritures pour des noms de personnes, de lieux, de produits ou d’organisation. Nous présentons un système hybride de détection d’entités nommées qui combine un classifieur à base de Champs Conditionnel Aléatoires avec un ensemble de motifs de détection extraits automatiquement d’un contenu encyclopédique. Nous proposons d’extraire depuis des éditions en plusieurs langues de l’encyclopédie Wikipédia de grandes quantités de formes d’écriture que nous utilisons en tant que motifs de détection des entités nommées. Nous décrivons une méthode qui nous assure de ne conserver dans cette ressources que des formes non ambiguës susceptibles de venir renforcer un système de détection d’entités nommées automatique. Nous procédons à un ensemble d’expériences qui nous permettent de comparer un système d’étiquetage à base de CRF avec un système utilisant exclusivement des motifs de détection. Puis nous fusionnons les résultats des deux systèmes et montrons qu’un gain de performances est obtenu grâce à cette proposition.</abstract>
      <url hash="b98c91fc">2011.jeptalnrecital-long.2</url>
      <language>fra</language>
      <bibkey>charton-etal-2011-generation</bibkey>
    </paper>
    <paper id="3">
      <title>Approche de construction automatique de titres courts par des méthodes de Fouille du Web (An automatic short title construction approach by web mining methods)</title>
      <author><first>Cédric</first><last>Lopez</last></author>
      <author><first>Mathieu</first><last>Roche</last></author>
      <pages>25–36</pages>
      <abstract>Le titrage automatique de documents textuels est une tâche essentielle pour plusieurs applications (titrage de mails, génération automatique de sommaires, synthèse de documents, etc.). Cette étude présente une méthode de construction de titres courts appliquée à un corpus d’articles journalistiques via des méthodes de Fouille du Web. Il s’agit d’une première étape cruciale dans le but de proposer une méthode de construction de titres plus complexes. Dans cet article, nous présentons une méthode proposant des titres tenant compte de leur cohérence par rapport au texte, par rapport au Web, ainsi que de leur contexte dynamique. L’évaluation de notre approche indique que nos titres construits automatiquement sont informatifs et/ou accrocheurs.</abstract>
      <url hash="f84afb93">2011.jeptalnrecital-long.3</url>
      <language>fra</language>
      <bibkey>lopez-roche-2011-approche</bibkey>
    </paper>
    <paper id="4">
      <title>Une approche faiblement supervisée pour l’extraction de relations à large échelle (A weakly supervised approach to large scale relation extraction)</title>
      <author><first>Ludovic</first><last>Jean-Louis</last></author>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Adrien</first><last>Durand</last></author>
      <pages>37–48</pages>
      <abstract>Les systèmes d’extraction d’information traditionnels se focalisent sur un domaine spécifique et un nombre limité de relations. Les travaux récents dans ce domaine ont cependant vu émerger la problématique des systèmes d’extraction d’information à large échelle. À l’instar des systèmes de question-réponse en domaine ouvert, ces systèmes se caractérisent à la fois par le traitement d’un grand nombre de relations et par une absence de restriction quant aux domaines abordés. Dans cet article, nous présentons un système d’extraction d’information à large échelle fondé sur un apprentissage faiblement supervisé de patrons d’extraction de relations. Cet apprentissage repose sur la donnée de couples d’entités en relation dont la projection dans un corpus de référence permet de constituer la base d’exemples de relations support de l’induction des patrons d’extraction. Nous présentons également les résultats de l’application de cette approche dans le cadre d’évaluation défini par la tâche KBP de l’évaluation TAC 2010.</abstract>
      <url hash="a741ce5b">2011.jeptalnrecital-long.4</url>
      <language>fra</language>
      <bibkey>jean-louis-etal-2011-une</bibkey>
    </paper>
    <paper id="5">
      <title>Utilisation d’un score de qualité de traduction pour le résumé multi-document cross-lingue (Using translation quality scores for cross-language multi-document summarization)</title>
      <author><first>Stéphane</first><last>Huet</last></author>
      <author><first>Florian</first><last>Boudin</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <pages>49–58</pages>
      <abstract>Le résumé automatique cross-lingue consiste à générer un résumé rédigé dans une langue différente de celle utilisée dans les documents sources. Dans cet article, nous proposons une approche de résumé automatique multi-document, basée sur une représentation par graphe, qui prend en compte des scores de qualité de traduction lors du processus de sélection des phrases. Nous évaluons notre méthode sur un sous-ensemble manuellement traduit des données utilisées lors de la campagne d’évaluation internationale DUC 2004. Les résultats expérimentaux indiquent que notre approche permet d’améliorer la lisibilité des résumés générés, sans pour autant dégrader leur informativité.</abstract>
      <url hash="7cb63a24">2011.jeptalnrecital-long.5</url>
      <language>fra</language>
      <bibkey>huet-etal-2011-utilisation</bibkey>
    </paper>
    <paper id="6">
      <title>Accès au contenu sémantique en langue de spécialité : extraction des prescriptions et concepts médicaux (Accessing the semantic content in a specialized language: extracting prescriptions and medical concepts)</title>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Louise</first><last>Deléger</last></author>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>59–70</pages>
      <abstract>Pourtant essentiel pour appréhender rapidement et globalement l’état de santé des patients, l’accès aux informations médicales liées aux prescriptions médicamenteuses et aux concepts médicaux par les outils informatiques se révèle particulièrement difficile. Ces informations sont en effet généralement rédigées en texte libre dans les comptes rendus hospitaliers et nécessitent le développement de techniques dédiées. Cet article présente les stratégies mises en oeuvre pour extraire les prescriptions médicales et les concepts médicaux dans des comptes rendus hospitaliers rédigés en anglais. Nos systèmes, fondés sur des approches à base de règles et d’apprentissage automatique, obtiennent une F1-mesure globale de 0,773 dans l’extraction des prescriptions médicales et dans le repérage et le typage des concepts médicaux.</abstract>
      <url hash="6204f977">2011.jeptalnrecital-long.6</url>
      <language>fra</language>
      <bibkey>grouin-etal-2011-acces</bibkey>
    </paper>
    <paper id="7">
      <title>Comparaison et combinaison d’approches pour la portabilité vers une nouvelle langue d’un système de compréhension de l’oral (Comparison and combination of approaches for the portability to a new language of an oral comprehension system)</title>
      <author><first>Bassam</first><last>Jabaian</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Fabrice</first><last>Lefèvre</last></author>
      <pages>71–82</pages>
      <abstract>Dans cet article, nous proposons plusieurs approches pour la portabilité du module de compréhension de la parole (SLU) d’un système de dialogue d’une langue vers une autre. On montre que l’utilisation des traductions automatiques statistiques (SMT) aide à réduire le temps et le cout de la portabilité d’un tel système d’une langue source vers une langue cible. Pour la tache d’étiquetage sémantique on propose d’utiliser soit les champs aléatoires conditionnels (CRF), soit l’approche à base de séquences (PH-SMT). Les résultats expérimentaux montrent l’efficacité des méthodes proposées pour une portabilité rapide du SLU vers une nouvelle langue. On propose aussi deux méthodes pour accroître la robustesse du SLU aux erreurs de traduction. Enfin on montre que la combinaison de ces approches réduit les erreurs du système. Ces travaux sont motivés par la disponibilité du corpus MEDIA français et de la traduction manuelle vers l’italien d’une sous partie de ce corpus.</abstract>
      <url hash="0eebc883">2011.jeptalnrecital-long.7</url>
      <language>fra</language>
      <bibkey>jabaian-etal-2011-comparaison</bibkey>
    </paper>
    <paper id="8">
      <title>Qui êtes-vous ? Catégoriser les questions pour déterminer le rôle des locuteurs dans des conversations orales (Who are you? Categorize questions to determine the role of speakers in oral conversations)</title>
      <author><first>Thierry</first><last>Bazillon</last></author>
      <author><first>Benjamin</first><last>Maza</last></author>
      <author><first>Mickael</first><last>Rouvier</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <author><first>Alexis</first><last>Nasr</last></author>
      <pages>83–93</pages>
      <abstract>La fouille de données orales est un domaine de recherche visant à caractériser un flux audio contenant de la parole d’un ou plusieurs locuteurs, à l’aide de descripteurs liés à la forme et au contenu du signal. Outre la transcription automatique en mots des paroles prononcées, des informations sur le type de flux audio traité ainsi que sur le rôle et l’identité des locuteurs sont également cruciales pour permettre des requêtes complexes telles que : « chercher des débats sur le thème X », « trouver toutes les interviews de Y », etc. Dans ce cadre, et en traitant des conversations enregistrées lors d’émissions de radio ou de télévision, nous étudions la manière dont les locuteurs expriment des questions dans les conversations, en partant de l’intuition initiale que la forme des questions posées est une signature du rôle du locuteur dans la conversation (présentateur, invité, auditeur, etc.). En proposant une classification du type des questions et en utilisant ces informations en complément des descripteurs généralement utilisés dans la littérature pour classer les locuteurs par rôle, nous espérons améliorer l’étape de classification, et valider par la même occasion notre intuition initiale.</abstract>
      <url hash="149d221c">2011.jeptalnrecital-long.8</url>
      <language>fra</language>
      <bibkey>bazillon-etal-2011-qui</bibkey>
    </paper>
    <paper id="9">
      <title>Recherche d’information et temps linguistique : une heuristique pour calculer la pertinence des expressions calendaires (Information retrieval and linguistic time: a heuristic to calculate the relevance of calendar expressions)</title>
      <author><first>Charles</first><last>Teissèdre</last></author>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Jean-Luc</first><last>Minel</last></author>
      <pages>94–105</pages>
      <abstract>A rebours de bon nombre d’applications actuelles offrant des services de recherche d’information selon des critères temporels - applications qui reposent, à y regarder de près, sur une approche consistant à filtrer les résultats en fonction de leur inclusion dans une fenêtre de temps, nous souhaitons illustrer dans cet article l’intérêt d’un service s’appuyant sur un calcul de similarité entre des expressions adverbiales calendaires. Nous décrivons une heuristique pour mesurer la pertinence d’un fragment de texte en prenant en compte la sémantique des expressions calendaires qui y sont présentes. A travers la mise en oeuvre d’un système de recherche d’information, nous montrons comment il est possible de tirer profit de l’indexation d’expressions calendaires présentes dans les textes en définissant des scores de pertinence par rapport à une requête. L’objectif est de faciliter la recherche d’information en offrant la possibilité de croiser des critères de recherche thématique avec des critères temporels.</abstract>
      <url hash="0f2f0bc8">2011.jeptalnrecital-long.9</url>
      <language>fra</language>
      <bibkey>teissedre-etal-2011-recherche</bibkey>
    </paper>
    <paper id="10">
      <title>Extraction de patrons sémantiques appliquée à la classification d’Entités Nommées (Extraction of semantic patterns applied to the classification of named entities)</title>
      <author><first>Ismaïl</first><last>El Maarouf</last></author>
      <author><first>Jeanne</first><last>Villaneau</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <pages>106–116</pages>
      <abstract>La variabilité des corpus constitue un problème majeur pour les systèmes de reconnaissance d’entités nommées. L’une des pistes possibles pour y remédier est l’utilisation d’approches linguistiques pour les adapter à de nouveaux contextes : la construction de patrons sémantiques peut permettre de désambiguïser les entités nommées en structurant leur environnement syntaxico-sémantique. Cet article présente une première réalisation sur un corpus de presse d’un système de correction. Après une étape de segmentation sur des critères discursifs de surface, le système extrait et pondère les patrons liés à une classe d’entité nommée fournie par un analyseur. Malgré des modèles encore relativement élémentaires, les résultats obtenus sont encourageants et montrent la nécessité d’un traitement plus approfondi de la classe Organisation.</abstract>
      <url hash="cdceab96">2011.jeptalnrecital-long.10</url>
      <language>fra</language>
      <bibkey>el-maarouf-etal-2011-extraction</bibkey>
    </paper>
    <paper id="11">
      <title>Désambiguïsation lexicale par propagation de mesures sémantiques locales par algorithmes à colonies de fourmis (Lexical disambiguation by propagation of local semantic measures using ant colony algorithms)</title>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Jérôme</first><last>Goulian</last></author>
      <author><first>Nathan</first><last>Guillaume</last></author>
      <pages>117–128</pages>
      <abstract>Effectuer une tâche de désambiguïsation lexicale peut permettre d’améliorer de nombreuses applications du traitement automatique des langues comme l’extraction d’informations multilingues, ou la traduction automatique. Schématiquement, il s’agit de choisir quel est le sens le plus approprié pour chaque mot d’un texte. Une des approches classiques consiste à estimer la proximité sémantique qui existe entre deux sens de mots puis de l’étendre à l’ensemble du texte. La méthode la plus directe donne un score à toutes les paires de sens de mots puis choisit la chaîne de sens qui a le meilleur score. La complexité de cet algorithme est exponentielle et le contexte qu’il est calculatoirement possible d’utiliser s’en trouve réduit. Il ne s’agit donc pas d’une solution viable. Dans cet article, nous nous intéressons à une autre méthode, l’adaptation d’un algorithme à colonies de fourmis. Nous présentons ses caractéristiques et montrons qu’il permet de propager à un niveau global les résultats des algorithmes locaux et de tenir compte d’un contexte plus long et plus approprié en un temps raisonnable.</abstract>
      <url hash="33bc8b2c">2011.jeptalnrecital-long.11</url>
      <language>fra</language>
      <bibkey>schwab-etal-2011-desambiguisation</bibkey>
    </paper>
    <paper id="12">
      <title>Un turc mécanique pour les ressources linguistiques : critique de la myriadisation du travail parcellisé (<fixed-case>M</fixed-case>echanical <fixed-case>T</fixed-case>urk for linguistic resources: review of the crowdsourcing of parceled work)</title>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Gilles</first><last>Adda</last></author>
      <author><first>Joseph</first><last>Mariani</last></author>
      <author><first>Bernard</first><last>Lang</last></author>
      <pages>129–140</pages>
      <abstract>Cet article est une prise de position concernant les plate-formes de type Amazon Mechanical Turk, dont l’utilisation est en plein essor depuis quelques années dans le traitement automatique des langues. Ces plateformes de travail en ligne permettent, selon le discours qui prévaut dans les articles du domaine, de faire développer toutes sortes de ressources linguistiques de qualité, pour un prix imbattable et en un temps très réduit, par des gens pour qui il s’agit d’un passe-temps. Nous allons ici démontrer que la situation est loin d’être aussi idéale, que ce soit sur le plan de la qualité, du prix, du statut des travailleurs ou de l’éthique. Nous rappellerons ensuite les solutions alternatives déjà existantes ou proposées. Notre but est ici double : informer les chercheurs, afin qu’ils fassent leur choix en toute connaissance de cause, et proposer des solutions pratiques et organisationnelles pour améliorer le développement de nouvelles ressources linguistiques en limitant les risques de dérives éthiques et légales, sans que cela se fasse au prix de leur coût ou de leur qualité.</abstract>
      <url hash="8f071146">2011.jeptalnrecital-long.12</url>
      <language>fra</language>
      <bibkey>sagot-etal-2011-un</bibkey>
    </paper>
    <paper id="13">
      <title>Degré de comparabilité, extraction lexicale bilingue et recherche d’information interlingue (Degree of comparability, bilingual lexical extraction and cross-language information retrieval)</title>
      <author id="bo-li"><first>Bo</first><last>Li</last></author>
      <author><first>Eric</first><last>Gaussier</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Amir</first><last>Hazem</last></author>
      <pages>141–152</pages>
      <abstract>Nous étudions dans cet article le problème de la comparabilité des documents composant un corpus comparable afin d’améliorer la qualité des lexiques bilingues extraits et les performances des systèmes de recherche d’information interlingue. Nous proposons une nouvelle approche qui permet de garantir un certain degré de comparabilité et d’homogénéité du corpus tout en préservant une grande part du vocabulaire du corpus d’origine. Nos expériences montrent que les lexiques bilingues que nous obtenons sont d’une meilleure qualité que ceux obtenus avec les approches précédentes, et qu’ils peuvent être utilisés pour améliorer significativement les systèmes de recherche d’information interlingue.</abstract>
      <url hash="b636c26c">2011.jeptalnrecital-long.13</url>
      <language>fra</language>
      <bibkey>li-etal-2011-degre</bibkey>
    </paper>
    <paper id="14">
      <title>Identification de mots germes pour la construction d’un lexique de valence au moyen d’une procédure supervisée (Identification of seed words for building a valence lexicon using a supervised procedure)</title>
      <author><first>Nadja</first><last>Vincze</last></author>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>153–164</pages>
      <abstract>De nombreuses méthodes automatiques de classification de textes selon les sentiments qui y sont exprimés s’appuient sur un lexique dans lequel à chaque entrée est associée une valence. Le plus souvent, ce lexique est construit à partir d’un petit nombre de mots, choisis arbitrairement, qui servent de germes pour déterminer automatiquement la valence d’autres mots. La question de l’optimalité de ces mots germes a bien peu retenu l’attention. Sur la base de la comparaison de cinq méthodes automatiques de construction de lexiques de valence, dont une qui, à notre connaissance, n’a jamais été adaptée au français et une autre développée spécifiquement pour la présente étude, nous montrons l’importance du choix de ces mots germes et l’intérêt de les identifier au moyen d’une procédure d’apprentissage supervisée.</abstract>
      <url hash="d9f19719">2011.jeptalnrecital-long.14</url>
      <language>fra</language>
      <bibkey>vincze-bestgen-2011-identification</bibkey>
    </paper>
    <paper id="15">
      <title>Comparaison d’une approche miroir et d’une approche distributionnelle pour l’extraction de mots sémantiquement reliés (Comparing a mirror approach and a distributional approach for extracting semantically related words)</title>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <pages>165–176</pages>
      <abstract>Dans (Muller &amp; Langlais, 2010), nous avons comparé une approche distributionnelle et une variante de l’approche miroir proposée par Dyvik (2002) sur une tâche d’extraction de synonymes à partir d’un corpus en français. Nous présentons ici une analyse plus fine des relations extraites automatiquement en nous intéressant cette fois-ci à la langue anglaise pour laquelle de plus amples ressources sont disponibles. Différentes façons d’évaluer notre approche corroborent le fait que l’approche miroir se comporte globalement mieux que l’approche distributionnelle décrite dans (Lin, 1998), une approche de référence dans le domaine.</abstract>
      <url hash="8da1c62f">2011.jeptalnrecital-long.15</url>
      <language>fra</language>
      <bibkey>muller-langlais-2011-comparaison</bibkey>
    </paper>
    <paper id="16">
      <title>Une approche holiste et unifiée de l’alignement et de la mesure d’accord inter-annotateurs (A holistic and unified approach to aligning and measuring inter-annotator agreement)</title>
      <author><first>Yann</first><last>Mathet</last></author>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <pages>177–188</pages>
      <abstract>L’alignement et la mesure d’accord sur des textes multi-annotés sont des enjeux majeurs pour la constitution de corpus de référence. Nous défendons dans cet article l’idée que ces deux tâches sont par essence interdépendantes, la mesure d’accord nécessitant de s’appuyer sur des annotations alignées, tandis que les choix d’alignements ne peuvent se faire qu’à l’aune de la mesure qu’ils induisent. Nous proposons des principes formels relevant cette gageure, qui s’appuient notamment sur la notion de désordre du système constitué par l’ensemble des jeux d’annotations d’un texte. Nous posons que le meilleur alignement est celui qui minimise ce désordre, et que la valeur de désordre obtenue rend compte simultanément du taux d’accord. Cette approche, qualifiée d’holiste car prenant en compte l’intégralité du système pour opérer, est algorithmiquement lourde, mais nous sommes parvenus à produire une implémentation d’une version légèrement dégradée de cette dernière, et l’avons intégrée à la plate-forme d’annotation Glozz.</abstract>
      <url hash="de5a1756">2011.jeptalnrecital-long.16</url>
      <language>fra</language>
      <bibkey>mathet-widlocher-2011-une</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>F</fixed-case>rench <fixed-case>T</fixed-case>ime<fixed-case>B</fixed-case>ank : un corpus de référence sur la temporalité en français (<fixed-case>F</fixed-case>rench <fixed-case>T</fixed-case>ime<fixed-case>B</fixed-case>ank: a reference corpus on temporality in <fixed-case>F</fixed-case>rench)</title>
      <author><first>André</first><last>Bittar</last></author>
      <author><first>Pascal</first><last>Amsili</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <pages>189–199</pages>
      <abstract>Cet article a un double objectif : d’une part, il s’agit de présenter à la communauté un corpus récemment rendu public, le French Time Bank (FTiB), qui consiste en une collection de textes journalistiques annotés pour les temps et les événements selon la norme ISO-TimeML ; d’autre part, nous souhaitons livrer les résultats et réflexions méthodologiques que nous avons pu tirer de la réalisation de ce corpus de référence, avec l’idée que notre expérience pourra s’avérer profitable au-delà de la communauté intéressée par le traitement de la temporalité.</abstract>
      <url hash="4991d222">2011.jeptalnrecital-long.17</url>
      <language>fra</language>
      <bibkey>bittar-etal-2011-french-timebank</bibkey>
    </paper>
    <paper id="18">
      <title>Acquisition automatique de terminologie à partir de corpus de texte (Automatic terminology acquisition from text corpora)</title>
      <author><first>Edmond</first><last>Lassalle</last></author>
      <pages>200–210</pages>
      <abstract>Les applications de recherche d’informations chez Orange sont confrontées à des flux importants de données textuelles, recouvrant des domaines larges et évoluant très rapidement. Un des problèmes à résoudre est de pouvoir analyser très rapidement ces flux, à un niveau élevé de qualité. Le recours à un modèle d’analyse sémantique, comme solution, n’est viable qu’en s’appuyant sur l’apprentissage automatique pour construire des grandes bases de connaissances dédiées à chaque application. L’extraction terminologique décrite dans cet article est un composant amont de ce dispositif d’apprentissage. Des nouvelles méthodes d’acquisition, basée sur un modèle hybride (analyse par grammaires de chunking et analyse statistique à deux niveaux), ont été développées pour répondre aux contraintes de performance et de qualité.</abstract>
      <url hash="dc0db7aa">2011.jeptalnrecital-long.18</url>
      <language>fra</language>
      <bibkey>lassalle-2011-acquisition</bibkey>
    </paper>
    <paper id="19">
      <title>Métarecherche pour l’extraction lexicale bilingue à partir de corpus comparables (Metasearch for bilingual lexical extraction from comparable corpora)</title>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Sebastián</first><last>Peña Saldarriaga</last></author>
      <pages>211–221</pages>
      <abstract>Nous présentons dans cet article une nouvelle manière d’aborder le problème de l’acquisition automatique de paires de mots en relation de traduction à partir de corpus comparables. Nous décrivons tout d’abord les approches standard et par similarité interlangue traditionnellement dédiées à cette tâche. Nous réinterprétons ensuite la méthode par similarité interlangue et motivons un nouveau modèle pour reformuler cette approche inspirée par les métamoteurs de recherche d’information. Les résultats empiriques que nous obtenons montrent que les performances de notre modèle sont toujours supérieures à celles obtenues avec l’approche par similarité interlangue, mais aussi comme étant compétitives par rapport à l’approche standard.</abstract>
      <url hash="a3ea5639">2011.jeptalnrecital-long.19</url>
      <language>fra</language>
      <bibkey>hazem-etal-2011-metarecherche</bibkey>
    </paper>
    <paper id="20">
      <title>Évaluation et consolidation d’un réseau lexical via un outil pour retrouver le mot sur le bout de la langue (Evaluation and consolidation of a lexical network via a tool to find the word on the tip of the tongue)</title>
      <author><first>Alain</first><last>Joubert</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Michael</first><last>Zock</last></author>
      <pages>222–233</pages>
      <abstract>Depuis septembre 2007, un réseau lexical de grande taille pour le Français est en cours de construction à l’aide de méthodes fondées sur des formes de consensus populaire obtenu via des jeux (projet JeuxDeMots). L’intervention d’experts humains est marginale en ce qu’elle représente moins de 0,5% des relations du réseau et se limite à des corrections, à des ajustements ainsi qu’à la validation des sens de termes. Pour évaluer la qualité de cette ressource construite par des participants de jeu (utilisateurs non experts) nous adoptons une démarche similaire à celle de sa construction, à savoir, la ressource doit être validée sur un vocabulaire de classe ouverte, par des non-experts, de façon stable (persistante dans le temps). Pour ce faire, nous proposons de vérifier si notre ressource est capable de servir de support à la résolution du problème nommé ‘Mot sur le Bout de la Langue’ (MBL). A l’instar de JeuxdeMots, l’outil développé peut être vu comme un jeu en ligne. Tout comme ce dernier, il permet d’acquérir de nouvelles relations, constituant ainsi un enrichissement de notre réseau lexical.</abstract>
      <url hash="dea72691">2011.jeptalnrecital-long.20</url>
      <language>fra</language>
      <bibkey>joubert-etal-2011-evaluation</bibkey>
    </paper>
    <paper id="21">
      <title>Identifier la cible d’un passage d’opinion dans un corpus multithématique (Identifying the target of an opinion transition in a thematic corpus)</title>
      <author><first>Matthieu</first><last>Vernier</last></author>
      <author><first>Laura</first><last>Monceaux</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <pages>234–245</pages>
      <abstract>L’identification de la cible d’une d’opinion fait l’objet d’une attention récente en fouille d’opinion. Les méthodes existantes ont été testées sur des corpus monothématiques en anglais. Elles permettent principalement de traiter les cas où la cible se situe dans la même phrase que l’opinion. Dans cet article, nous abordons cette problématique pour le français dans un corpus multithématique et nous présentons une nouvelle méthode pour identifier la cible d’une opinion apparaissant hors du contexte phrastique. L’évaluation de la méthode montre une amélioration des résultats par rapport à l’existant.</abstract>
      <url hash="896ea7c8">2011.jeptalnrecital-long.21</url>
      <language>fra</language>
      <bibkey>vernier-etal-2011-identifier</bibkey>
    </paper>
    <paper id="22">
      <title>Intégrer des connaissances linguistiques dans un <fixed-case>CRF</fixed-case> : application à l’apprentissage d’un segmenteur-étiqueteur du français (Integrating linguistic knowledge in a <fixed-case>CRF</fixed-case>: application to learning a segmenter-tagger of <fixed-case>F</fixed-case>rench)</title>
      <author><first>Matthieu</first><last>Constant</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <author><first>Denys</first><last>Duchier</last></author>
      <author><first>Yoann</first><last>Dupont</last></author>
      <author><first>Anthony</first><last>Sigogne</last></author>
      <author><first>Sylvie</first><last>Billot</last></author>
      <pages>246–257</pages>
      <abstract>Dans cet article, nous synthétisons les résultats de plusieurs séries d’expériences réalisées à l’aide de CRF (Conditional Random Fields ou “champs markoviens conditionnels”) linéaires pour apprendre à annoter des textes français à partir d’exemples, en exploitant diverses ressources linguistiques externes. Ces expériences ont porté sur l’étiquetage morphosyntaxique intégrant l’identification des unités polylexicales. Nous montrons que le modèle des CRF est capable d’intégrer des ressources lexicales riches en unités multi-mots de différentes manières et permet d’atteindre ainsi le meilleur taux de correction d’étiquetage actuel pour le français.</abstract>
      <url hash="b2b972bf">2011.jeptalnrecital-long.22</url>
      <language>fra</language>
      <bibkey>constant-etal-2011-integrer</bibkey>
    </paper>
    <paper id="23">
      <title>Segmentation et induction de lexique non-supervisées du mandarin (Unsupervised segmentation and induction of mandarin lexicon)</title>
      <author><first>Pierre</first><last>Magistry</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>258–269</pages>
      <abstract>Pour la plupart des langues utilisant l’alphabet latin, le découpage d’un texte selon les espaces et les symboles de ponctuation est une bonne approximation d’un découpage en unités lexicales. Bien que cette approximation cache de nombreuses difficultés, elles sont sans comparaison avec celles que l’on rencontre lorsque l’on veut traiter des langues qui, comme le chinois mandarin, n’utilisent pas l’espace. Un grand nombre de systèmes de segmentation ont été proposés parmi lesquels certains adoptent une approche non-supervisée motivée linguistiquement. Cependant les méthodes d’évaluation communément utilisées ne rendent pas compte de toutes les propriétés de tels systèmes. Dans cet article, nous montrons qu’un modèle simple qui repose sur une reformulation en termes d’entropie d’une hypothèse indépendante de la langue énoncée par Harris (1955), permet de segmenter un corpus et d’en extraire un lexique. Testé sur le corpus de l’Academia Sinica, notre système permet l’induction d’une segmentation et d’un lexique qui ont de bonnes propriétés intrinsèques et dont les caractéristiques sont similaires à celles du lexique sous-jacent au corpus segmenté manuellement. De plus, on constate une certaine corrélation entre les résultats du modèle de segmentation et les structures syntaxiques fournies par une sous-partie arborée corpus.</abstract>
      <url hash="a3aa7611">2011.jeptalnrecital-long.23</url>
      <language>fra</language>
      <bibkey>magistry-sagot-2011-segmentation</bibkey>
    </paper>
    <paper id="24">
      <title>Évaluer la pertinence de la morphologie constructionnelle dans les systèmes de Question-Réponse (Evaluating the relevance of constructional morphology in question-answering systems)</title>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <author><first>Delphine</first><last>Tribout</last></author>
      <pages>270–281</pages>
      <abstract>Les connaissances morphologiques sont fréquemment utilisées en Question-Réponse afin de faciliter l’appariement entre mots de la question et mots du passage contenant la réponse. Il n’existe toutefois pas d’étude qualitative et quantitative sur les phénomènes morphologiques les plus pertinents pour ce cadre applicatif. Dans cet article, nous présentons une analyse détaillée des phénomènes de morphologie constructionnelle permettant de faire le lien entre question et réponse. Pour ce faire, nous avons constitué et annoté un corpus de paires de questions-réponses, qui nous a permis de construire une ressource de référence, utile pour l’évaluation de la couverture de ressources et d’outils d’analyse morphologique. Nous détaillons en particulier les phénomènes de dérivation et de composition et montrons qu’il reste un nombre important de relations morphologiques dérivationnelles pour lesquelles il n’existe pas encore de ressource exploitable pour le français.</abstract>
      <url hash="c4211eed">2011.jeptalnrecital-long.24</url>
      <language>fra</language>
      <bibkey>bernhard-etal-2011-evaluer</bibkey>
    </paper>
    <paper id="25">
      <title>Structure des trigrammes inconnus et lissage par analogie (Structure of unknown trigrams and smoothing by analogy)</title>
      <author><first>Julien</first><last>Gosme</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>282–293</pages>
      <abstract>Nous montrons dans une série d’expériences sur quatre langues, sur des échantillons du corpus Europarl, que, dans leur grande majorité, les trigrammes inconnus d’un jeu de test peuvent être reconstruits par analogie avec des trigrammes hapax du corpus d’entraînement. De ce résultat, nous dérivons une méthode de lissage simple pour les modèles de langue par trigrammes et obtenons de meilleurs résultats que les lissages de Witten-Bell, Good-Turing et Kneser-Ney dans des expériences menées en onze langues sur la partie commune d’Europarl, sauf pour le finnois et, dans une moindre mesure, le français.</abstract>
      <url hash="6be51b8f">2011.jeptalnrecital-long.25</url>
      <language>fra</language>
      <bibkey>gosme-lepage-2011-structure</bibkey>
    </paper>
    <paper id="26">
      <title>Modèles génératif et discriminant en analyse syntaxique : expériences sur le corpus arboré de <fixed-case>P</fixed-case>aris 7 (Generative and discriminative models in parsing: experiments on the <fixed-case>P</fixed-case>aris 7 Treebank)</title>
      <author><first>Joseph</first><last>Le Roux</last></author>
      <author><first>Benoît</first><last>Favre</last></author>
      <author><first>Seyed</first><last>Abolghasem Mirroshandel</last></author>
      <author><first>Alexis</first><last>Nasr</last></author>
      <pages>294–305</pages>
      <abstract>Nous présentons une architecture pour l’analyse syntaxique en deux étapes. Dans un premier temps un analyseur syntagmatique construit, pour chaque phrase, une liste d’analyses qui sont converties en arbres de dépendances. Ces arbres sont ensuite réévalués par un réordonnanceur discriminant. Cette méthode permet de prendre en compte des informations auxquelles l’analyseur n’a pas accès, en particulier des annotations fonctionnelles. Nous validons notre approche par une évaluation sur le corpus arboré de Paris 7. La seconde étape permet d’améliorer significativement la qualité des analyses retournées, quelle que soit la métrique utilisée.</abstract>
      <url hash="89949f58">2011.jeptalnrecital-long.26</url>
      <language>fra</language>
      <bibkey>le-roux-etal-2011-modeles</bibkey>
    </paper>
    <paper id="27">
      <title>Apport de la syntaxe pour l’extraction de relations en domaine médical (Contribution of syntax for relation extraction in the medical domain)</title>
      <author><first>Anne-Lyse</first><last>Minard</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <pages>306–316</pages>
      <abstract>Dans cet article, nous nous intéressons à l’identification de relations entre entités en domaine de spécialité, et étudions l’apport d’informations syntaxiques. Nous nous plaçons dans le domaine médical, et analysons des relations entre concepts dans des comptes-rendus médicaux, tâche évaluée dans la campagne i2b2 en 2010. Les relations étant exprimées par des formulations très variées en langue, nous avons procédé à l’analyse des phrases en extrayant des traits qui concourent à la reconnaissance de la présence d’une relation et nous avons considéré l’identification des relations comme une tâche de classification multi-classes, chaque catégorie de relation étant considérée comme une classe. Notre système de référence est celui qui a participé à la campagne i2b2, dont la F-mesure est d’environ 0,70. Nous avons évalué l’apport de la syntaxe pour cette tâche, tout d’abord en ajoutant des attributs syntaxiques à notre classifieur, puis en utilisant un apprentissage fondé sur la structure syntaxique des phrases (apprentissage à base de tree kernels) ; cette dernière méthode améliore les résultats de la classification de 3%.</abstract>
      <url hash="04e6e8bd">2011.jeptalnrecital-long.27</url>
      <language>fra</language>
      <bibkey>minard-etal-2011-apport</bibkey>
    </paper>
    <paper id="28">
      <title>Enrichissement de structures en dépendances par réécriture de graphes (Dependency structure enrichment using graph rewriting)</title>
      <author><first>Guillaume</first><last>Bonfante</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Mathieu</first><last>Morey</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <pages>317–328</pages>
      <abstract>Nous montrons comment enrichir une annotation en dépendances syntaxiques au format du French Treebank de Paris 7 en utilisant la réécriture de graphes, en vue du calcul de sa représentation sémantique. Le système de réécriture est composé de règles grammaticales et lexicales structurées en modules. Les règles lexicales utilisent une information de contrôle extraite du lexique des verbes français Dicovalence.</abstract>
      <url hash="dc76eb9b">2011.jeptalnrecital-long.28</url>
      <language>fra</language>
      <bibkey>bonfante-etal-2011-enrichissement</bibkey>
    </paper>
    <paper id="29">
      <title>Classification en polarité de sentiments avec une représentation textuelle à base de sous-graphes d’arbres de dépendances (Sentiment polarity classification using a textual representation based on subgraphs of dependency trees)</title>
      <author><first>Alexander</first><last>Pak</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>329–339</pages>
      <abstract>Les approches classiques à base de n-grammes en analyse supervisée de sentiments ne peuvent pas correctement identifier les expressions complexes de sentiments à cause de la perte d’information induite par l’approche « sac de mots » utilisée pour représenter les textes. Dans notre approche, nous avons recours à des sous-graphes extraits des graphes de dépendances syntaxiques comme traits pour la classification de sentiments. Nous représentons un texte par un vecteur composé de ces sous-graphes syntaxiques et nous employons un classifieurs SVM état-de-l’art pour identifier la polarité d’un texte. Nos évaluations expérimentales sur des critiques de jeux vidéo montrent que notre approche à base de sous-graphes est meilleure que les approches standard à modèles « sac de mots » et n-grammes. Dans cet article nous avons travaillé sur le français, mais notre approche peut facilement être adaptée à d’autres langues.</abstract>
      <url hash="3a421710">2011.jeptalnrecital-long.29</url>
      <language>fra</language>
      <bibkey>pak-paroubek-2011-classification</bibkey>
    </paper>
    <paper id="30">
      <title>Une modélisation des dites alternances de portée des quantifieurs par des opérations de combinaison des groupes nominaux (A model of called alternations of quantifiers scope by combination of nominal groups operations)</title>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <pages>340–351</pages>
      <abstract>Nous montrons que les différentes interprétations d’une combinaison de plusieurs GN peuvent être modélisées par deux opérations de combinaison sur les référents de ces GN, appelées combinaison cumulative et combinaison distributive. Nous étudions aussi bien les GN définis et indéfinis que les GN quantifiés ou pluriels et nous montrons comment la combinaison d’un GN avec d’autres éléments peut induire des interprétations collective ou individualisante. Selon la façon dont un GN se combine avec d’autres GN, le calcul de son référent peut être fonction de ces derniers ; ceci définit une relation d’ancrage de chaque GN, qui induit un ordre partiel sur les GN. Considérer cette relation plutôt que la relation converse de portée simplifie le calcul de l’interprétation des GN et des énoncés. Des représentations sémantiques graphiques et algébriques sans considération de la portée sont proposées pour les dites alternances de portée.</abstract>
      <url hash="33409bb2">2011.jeptalnrecital-long.30</url>
      <language>fra</language>
      <bibkey>kahane-2011-une</bibkey>
    </paper>
    <paper id="31">
      <title>Analyse automatique de la modalité et du niveau de certitude : application au domaine médical (Automatic analysis of modality and level of certainty: application to the medical domain)</title>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <pages>352–363</pages>
      <abstract>De nombreux phénomènes linguistiques visent à exprimer le doute ou l’incertitude de l’énonciateur, ainsi que la subjectivité potentielle du point de vue. La prise en compte de ces informations sur le niveau de certitude est primordiale pour de nombreuses applications du traitement automatique des langues, en particulier l’extraction d’information dans le domaine médical. Dans cet article, nous présentons deux systèmes qui analysent automatiquement les niveaux de certitude associés à des problèmes médicaux mentionnés dans des compte-rendus cliniques en anglais. Le premier système procède par apprentissage supervisé et obtient une f-mesure de 0,93. Le second système utilise des règles décrivant des déclencheurs linguistiques spécifiques et obtient une f-mesure de 0,90.</abstract>
      <url hash="b713d7e2">2011.jeptalnrecital-long.31</url>
      <language>fra</language>
      <bibkey>bernhard-ligozat-2011-analyse</bibkey>
    </paper>
    <paper id="32">
      <title>Analyse discursive et informations de factivité (Discursive analysis and information factivity)</title>
      <author><first>Laurence</first><last>Danlos</last></author>
      <pages>364–375</pages>
      <abstract>Les annotations discursives proposées dans le cadre de théories discursives comme RST (Rhetorical Structure Theory) ou SDRT (Segmented Dicourse Representation Theory) ont comme point fort de construire une structure discursive globale liant toutes les informations données dans un texte. Les annotations discursives proposées dans le PDTB (Penn Discourse Tree Bank) ont comme point fort d’identifier la “source” de chaque information du texte—répondant ainsi à la question qui a dit ou pense quoi ? Nous proposons une approche unifiée pour les annotations discursives alliant les points forts de ces deux courants de recherche. Cette approche unifiée repose crucialement sur des information de factivité, telles que celles qui sont annotées dans le corpus (anglais) FactBank.</abstract>
      <url hash="6ce0551c">2011.jeptalnrecital-long.32</url>
      <language>fra</language>
      <bibkey>danlos-2011-analyse</bibkey>
    </paper>
    <paper id="33">
      <title>Paraphrases et modifications locales dans l’historique des révisions de Wikipédia (Paraphrases and local changes in the revision history of <fixed-case>W</fixed-case>ikipedia)</title>
      <author><first>Camille</first><last>Dutrey</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Aurélien</first><last>Max</last></author>
      <pages>376–387</pages>
      <abstract>Dans cet article, nous analysons les modifications locales disponibles dans l’historique des révisions de la version française de Wikipédia. Nous définissons tout d’abord une typologie des modifications fondée sur une étude détaillée d’un large corpus de modifications. Puis, nous détaillons l’annotation manuelle d’une partie de ce corpus afin d’évaluer le degré de complexité de la tâche d’identification automatique de paraphrases dans ce genre de corpus. Enfin, nous évaluons un outil d’identification de paraphrases à base de règles sur un sous-ensemble de notre corpus.</abstract>
      <url hash="57703046">2011.jeptalnrecital-long.33</url>
      <language>fra</language>
      <bibkey>dutrey-etal-2011-paraphrases</bibkey>
    </paper>
    <paper id="34">
      <title>&lt;<fixed-case>T</fixed-case>ext<fixed-case>C</fixed-case>oop&gt;: un analyseur de discours basé sur les grammaires logiques (&lt;<fixed-case>T</fixed-case>ext<fixed-case>C</fixed-case>oop&gt;: a discourse analyzer based on logical grammars)</title>
      <author><first>Patrick</first><last>Saint-Dizier</last></author>
      <pages>388–399</pages>
      <abstract>Dans ce document, nous présentons les principales caractéristiques de &lt;TextCoop&gt;, un environnement basé sur les grammaires logiques dédié à l’analyse de structures discursives. Nous étudions en particulier le langage DisLog qui fixe la structure des règles et des spécifications qui les accompagnent. Nous présentons la structure du moteur de &lt;TextCoop&gt; en indiquant au fur et à mesure du texte l’état du travail, les performances et les orientations en particulier en matière d’environnement, d’aide à l’écriture de règles et de développement applicatif.</abstract>
      <url hash="4790f4fe">2011.jeptalnrecital-long.34</url>
      <language>fra</language>
      <bibkey>saint-dizier-2011-textcoop</bibkey>
    </paper>
    <paper id="35">
      <title>Intégration de la parole et du geste déictique dans une grammaire multimodale (Integration of Speech and Deictic Gesture in a Multimodal Grammar)</title>
      <author><first>Katya</first><last>Alahverdzhieva</last></author>
      <author><first>Alex</first><last>Lascarides</last></author>
      <pages>400–411</pages>
      <abstract>Dans cet article, nous présentons une analyse à base de contraintes de la relation forme-sens des gestes déictiques et de leur signal de parole synchrone. En nous basant sur une étude empirique de corpus multimodaux, nous définissons quels énoncés multimodaux sont bien formés, et lesquels ne pourraient jamais produire le sens voulu dans la situation communicative. Plus précisément, nous formulons une grammaire multimodale dont les règles de construction utilisent la prosodie, la syntaxe et la sémantique de la parole, la forme et le sens du signal déictique, ainsi que la performance temporelle de la parole et la deixis afin de contraindre la production d’un arbre de syntaxe combinant parole et gesture déictique ainsi que la représentation unifiée du sens pour l’action multimodale correspondant à cet arbre. La contribution de notre projet est double : nous ajoutons aux ressources existantes pour le TAL un corpus annoté de parole et de gestes, et nous créons un cadre théorique pour la grammaire au sein duquel la composition sémantique d’un énoncé découle de la synchronie entre geste et parole.</abstract>
      <url hash="deab206f">2011.jeptalnrecital-long.35</url>
      <language>fra</language>
      <bibkey>alahverdzhieva-lascarides-2011-integration</bibkey>
    </paper>
    <paper id="36">
      <title>Généralisation de l’alignement sous-phrastique par échantillonnage (Generalization of sub-sentential alignment by sampling)</title>
      <author><first>Adrien</first><last>Lardilleux</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>412–423</pages>
      <abstract>L’alignement sous-phrastique consiste à extraire des traductions d’unités textuelles de grain inférieur à la phrase à partir de textes multilingues parallèles alignés au niveau de la phrase. Un tel alignement est nécessaire, par exemple, pour entraîner des systèmes de traduction statistique. L’approche standard pour réaliser cette tâche implique l’estimation successive de plusieurs modèles probabilistes de complexité croissante et l’utilisation d’heuristiques qui permettent d’aligner des mots isolés, puis, par extension, des groupes de mots. Dans cet article, nous considérons une approche alternative, initialement proposée dans (Lardilleux &amp; Lepage, 2008), qui repose sur un principe beaucoup plus simple, à savoir la comparaison des profils d’occurrences dans des souscorpus obtenus par échantillonnage. Après avoir analysé les forces et faiblesses de cette approche, nous montrons comment améliorer la détection d’unités de traduction longues, et évaluons ces améliorations sur des tâches de traduction automatique.</abstract>
      <url hash="399d7b49">2011.jeptalnrecital-long.36</url>
      <language>fra</language>
      <bibkey>lardilleux-etal-2011-generalisation</bibkey>
    </paper>
    <paper id="37">
      <title>Estimation d’un modèle de traduction à partir d’alignements mot-à-mot non-déterministes (Estimating a translation model from non-deterministic word-to-word alignments)</title>
      <author><first>Nadi</first><last>Tomeh</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>424–435</pages>
      <abstract>Dans les systèmes de traduction statistique à base de segments, le modèle de traduction est estimé à partir d’alignements mot-à-mot grâce à des heuristiques d’extraction et de valuation. Bien que ces alignements mot-à-mot soient construits par des modèles probabilistes, les processus d’extraction et de valuation utilisent ces modèles en faisant l’hypothèse que ces alignements sont déterministes. Dans cet article, nous proposons de lever cette hypothèse en considérant l’ensemble de la matrice d’alignement, d’une paire de phrases, chaque association étant valuée par sa probabilité. En comparaison avec les travaux antérieurs, nous montrons qu’en utilisant un modèle exponentiel pour estimer de manière discriminante ces probabilités, il est possible d’obtenir des améliorations significatives des performances de traduction. Ces améliorations sont mesurées à l’aide de la métrique BLEU sur la tâche de traduction de l’arabe vers l’anglais de l’évaluation NIST MT’09, en considérant deux types de conditions selon la taille du corpus de données parallèles utilisées.</abstract>
      <url hash="dad590a6">2011.jeptalnrecital-long.37</url>
      <language>fra</language>
      <bibkey>tomeh-etal-2011-estimation</bibkey>
    </paper>
    <paper id="38">
      <title>Combinaison d’informations pour l’alignement monolingue (Information combination for monolingual alignment)</title>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Aurélien</first><last>Max</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>436–447</pages>
      <abstract>Dans cet article, nous décrivons une nouvelle méthode d’alignement automatique de paraphrases d’énoncés. Nous utilisons des méthodes développées précédemment afin de produire différentes approches hybrides (hybridations). Ces différentes méthodes permettent d’acquérir des équivalences textuelles à partir d’un corpus monolingue parallèle. L’hybridation combine des informations obtenues par diverses techniques : alignements statistiques, approche symbolique, fusion d’arbres syntaxiques et alignement basé sur des distances d’édition. Nous avons évalué l’ensemble de ces résultats et nous constatons une amélioration sur l’acquisition de paraphrases sous-phrastiques.</abstract>
      <url hash="14c42b96">2011.jeptalnrecital-long.38</url>
      <language>fra</language>
      <bibkey>bouamor-etal-2011-combinaison</bibkey>
    </paper>
  </volume>
  <volume id="court" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 18e conférence sur le Traitement Automatique des Langues Naturelles. Articles courts</booktitle>
      <editor><first>Mathieu</first><last>Lafourcade</last></editor>
      <editor><first>Violaine</first><last>Prince</last></editor>
      <publisher>ATALA</publisher>
      <address>Montpellier, France</address>
      <month>June</month>
      <year>2011</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="a0a48795">2011.jeptalnrecital-court.0</url>
      <bibkey>jep-taln-recital-2011-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Evaluation de la détection des émotions, des opinions ou des sentiments : dictature de la majorité ou respect de la diversité d’opinions ? (Evaluation of the detection of emotions, opinions or sentiments: majority dictatorship or respect for opinion diversity?)</title>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Marc</first><last>Le Tallec</last></author>
      <author><first>Jeanne</first><last>Villaneau</last></author>
      <pages>1–6</pages>
      <abstract>Détection d’émotion, fouille d’opinion et analyse des sentiments sont généralement évalués par comparaison des réponses du système concerné par rapport à celles contenues dans un corpus de référence. Les questions posées dans cet article concernent à la fois la définition de la référence et la fiabilité des métriques les plus fréquemment utilisées pour cette comparaison. Les expérimentations menées pour évaluer le système de détection d’émotions EmoLogus servent de base de réflexion pour ces deux problèmes. L’analyse des résultats d’EmoLogus et la comparaison entre les différentes métriques remettent en cause le choix du vote majoritaire comme référence. Par ailleurs elles montrent également la nécessité de recourir à des outils statistiques plus évolués que ceux généralement utilisés pour obtenir des évaluations fiables de systèmes qui travaillent sur des données intrinsèquement subjectives et incertaines.</abstract>
      <url hash="2007ee3a">2011.jeptalnrecital-court.1</url>
      <language>fra</language>
      <bibkey>antoine-etal-2011-evaluation</bibkey>
    </paper>
    <paper id="2">
      <title>Une approche de résumé automatique basée sur les collocations (A Collocation-Driven Approach to Text Summarization)</title>
      <author><first>Violeta</first><last>Seretan</last></author>
      <pages>7–12</pages>
      <abstract>Dans cet article, nous décrivons une nouvelle approche pour la création de résumés extractifs – tâche qui consiste à créer automatiquement un résumé pour un document en sélectionnant un sous-ensemble de ses phrases – qui exploite des informations collocationnelles spécifiques à un domaine, acquises préalablement à partir d’un corpus de développement. Un extracteur de collocations fondé sur l’analyse syntaxique est utilisé afin d’inférer un modèle de contenu qui est ensuite appliqué au document à résumer. Cette approche a été utilisée pour la création des versions simples pour les articles de Wikipedia en anglais, dans le cadre d’un projet visant la création automatique d’articles simplifiées, similaires aux articles recensées dans Simple English Wikipedia. Une évaluation du système développé reste encore à faire. Toutefois, les résultats préalables obtenus pour les articles sur des villes montrent le potentiel de cette approche guidée par collocations pour la sélection des phrases pertinentes.</abstract>
      <url hash="eb522250">2011.jeptalnrecital-court.2</url>
      <language>fra</language>
      <bibkey>seretan-2011-une</bibkey>
    </paper>
    <paper id="3">
      <title>Quel apport des unités polylexicales dans une formule de lisibilité pour le français langue étrangère (What is the contribution of multi-word expressions in a readability formula for the <fixed-case>F</fixed-case>rench as a foreign language)</title>
      <author><first>Thomas</first><last>François</last></author>
      <author><first>Patrick</first><last>Watrin</last></author>
      <pages>13–18</pages>
      <abstract>Cette étude envisage l’emploi des unités polylexicales (UPs) comme prédicteurs dans une formule de lisibilité pour le français langue étrangère. À l’aide d’un extracteur d’UPs combinant une approche statistique à un filtre linguistique, nous définissons six variables qui prennent en compte la densité et la probabilité des UPs nominales, mais aussi leur structure interne. Nos expérimentations concluent à un faible pouvoir prédictif de ces six variables et révèlent qu’une simple approche basée sur la probabilité moyenne des n-grammes des textes est plus efficace.</abstract>
      <url hash="6f80db5e">2011.jeptalnrecital-court.3</url>
      <language>fra</language>
      <bibkey>francois-watrin-2011-quel</bibkey>
    </paper>
    <paper id="4">
      <title>Coopération de méthodes statistiques et symboliques pour l’adaptation non-supervisée d’un système d’étiquetage en entités nommées (Statistical and symbolic methods cooperation for the unsupervised adaptation of a named entity recognition system)</title>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Rosa</first><last>Stern</last></author>
      <pages>19–24</pages>
      <abstract>La détection et le typage des entités nommées sont des tâches pour lesquelles ont été développés à la fois des systèmes symboliques et probabilistes. Nous présentons les résultats d’une expérience visant à faire interagir le système à base de règles NP, développé sur des corpus provenant de l’AFP, intégrant la base d’entités Aleda et qui a une bonne précision, et le système LIANE, entraîné sur des transcriptions de l’oral provenant du corpus ESTER et qui a un bon rappel. Nous montrons qu’on peut adapter à un nouveau type de corpus, de manière non supervisée, un système probabiliste tel que LIANE grâce à des corpus volumineux annotés automatiquement par NP. Cette adaptation ne nécessite aucune annotation manuelle supplémentaire et illustre la complémentarité des méthodes numériques et symboliques pour la résolution de tâches linguistiques.</abstract>
      <url hash="d0f6e66b">2011.jeptalnrecital-court.4</url>
      <language>fra</language>
      <bibkey>bechet-etal-2011-cooperation</bibkey>
    </paper>
    <paper id="5">
      <title>Création de clusters sémantiques dans des familles morphologiques à partir du <fixed-case>TLF</fixed-case>i (Creating semantic clusters in morphological families from the <fixed-case>TLF</fixed-case>i)</title>
      <author><first>Nuria</first><last>Gala</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <author><first>Alexis</first><last>Nasr</last></author>
      <author><first>Véronique</first><last>Rey</last></author>
      <author><first>Selja</first><last>Seppälä</last></author>
      <pages>25–30</pages>
      <abstract>La constitution de ressources linguistiques est une tâche longue et coûteuse. C’est notamment le cas pour les ressources morphologiques. Ces ressources décrivent de façon approfondie et explicite l’organisation morphologique du lexique complétée d’informations sémantiques exploitables dans le domaine du TAL. Le travail que nous présentons dans cet article s’inscrit dans cette perspective et, plus particulièrement, dans l’optique d’affiner une ressource existante en s’appuyant sur des informations sémantiques obtenues automatiquement. Notre objectif est de caractériser sémantiquement des familles morpho-phonologiques (des mots partageant une même racine et une continuité de sens). Pour ce faire, nous avons utilisé des informations extraites du TLFi annoté morpho-syntaxiquement. Les premiers résultats de ce travail seront analysés et discutés.</abstract>
      <url hash="4bc7b9b8">2011.jeptalnrecital-court.5</url>
      <language>fra</language>
      <bibkey>gala-etal-2011-creation</bibkey>
    </paper>
    <paper id="6">
      <title>Génération automatique de questions à partir de textes en français (Automatic generation of questions from texts in <fixed-case>F</fixed-case>rench)</title>
      <author><first>Louis</first><last>de Viron</last></author>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <pages>31–36</pages>
      <abstract>Nous présentons dans cet article un générateur automatique de questions pour le français. Le système de génération procède par transformation de phrases déclaratives en interrogatives et se base sur une analyse syntaxique préalable de la phrase de base. Nous détaillons les différents types de questions générées. Nous présentons également une évaluation de l’outil, qui démontre que 41 % des questions générées par le système sont parfaitement bien formées.</abstract>
      <url hash="53ae0e71">2011.jeptalnrecital-court.6</url>
      <language>fra</language>
      <bibkey>de-viron-etal-2011-generation</bibkey>
    </paper>
    <paper id="7">
      <title>Sélection de réponses à des questions dans un corpus Web par validation (Selection of answers to questions in a web corpus by validation)</title>
      <author><first>Arnaud</first><last>Grappy</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Mathieu-Henri</first><last>Falco</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>37–42</pages>
      <abstract>Les systèmes de questions réponses recherchent la réponse à une question posée en langue naturelle dans un ensemble de documents. Les collectionsWeb diffèrent des articles de journaux de par leurs structures et leur style. Pour tenir compte de ces spécificités nous avons développé un système fondé sur une approche robuste de validation où des réponses candidates sont extraites à partir de courts passages textuels puis ordonnées par apprentissage. Les résultats montrent une amélioration du MRR (Mean Reciprocal Rank) de 48% par rapport à la baseline.</abstract>
      <url hash="b116568b">2011.jeptalnrecital-court.7</url>
      <language>fra</language>
      <bibkey>grappy-etal-2011-selection</bibkey>
    </paper>
    <paper id="8">
      <title>Filtrage de relations pour l’extraction d’information non supervisée (Filtering relations for unsupervised information extraction)</title>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <pages>43–48</pages>
      <abstract>Le domaine de l’extraction d’information s’est récemment développé en limitant les contraintes sur la définition des informations à extraire, ouvrant la voie à des applications de veille plus ouvertes. Dans ce contexte de l’extraction d’information non supervisée, nous nous intéressons à l’identification et la caractérisation de nouvelles relations entre des types d’entités fixés. Un des défis de cette tâche est de faire face à la masse importante de candidats pour ces relations lorsque l’on considère des corpus de grande taille. Nous présentons dans cet article une approche pour le filtrage des relations combinant méthode heuristique et méthode par apprentissage. Nous évaluons ce filtrage de manière intrinsèque et par son impact sur un regroupement sémantique des relations.</abstract>
      <url hash="95626e12">2011.jeptalnrecital-court.8</url>
      <language>fra</language>
      <bibkey>wang-etal-2011-filtrage</bibkey>
    </paper>
    <paper id="9">
      <title>Un lexique pondéré des noms d’événements en français (A weighted lexicon of event names in <fixed-case>F</fixed-case>rench)</title>
      <author><first>Béatrice</first><last>Arnulphy</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>49–54</pages>
      <abstract>Cet article décrit une étude sur l’annotation automatique des noms d’événements dans les textes en français. Plusieurs lexiques existants sont utilisés, ainsi que des règles syntaxiques d’extraction, et un lexique composé de façon automatique, permettant de fournir une valeur sur le niveau d’ambiguïté du mot en tant qu’événement. Cette nouvelle information permettrait d’aider à la désambiguïsation des noms d’événements en contexte.</abstract>
      <url hash="4aa45d80">2011.jeptalnrecital-court.9</url>
      <language>fra</language>
      <bibkey>arnulphy-etal-2011-un</bibkey>
    </paper>
    <paper id="10">
      <title>Alignement automatique pour la compréhension littérale de l’oral par approche segmentale (Automatic alignment for the literal oral understanding using a segmental approach)</title>
      <author><first>Stéphane</first><last>Huet</last></author>
      <author><first>Fabrice</first><last>Lefèvre</last></author>
      <pages>55–60</pages>
      <abstract>Les approches statistiques les plus performantes actuellement pour la compréhension automatique du langage naturel nécessitent une annotation segmentale des données d’entraînement. Nous étudions dans cet article une alternative permettant d’obtenir de façon non-supervisée un alignement segmental d’unités conceptuelles sur les mots. L’impact de l’alignement automatique sur les performances du système de compréhension est évalué sur une tâche de dialogue oral.</abstract>
      <url hash="64ad0533">2011.jeptalnrecital-court.10</url>
      <language>fra</language>
      <bibkey>huet-lefevre-2011-alignement</bibkey>
    </paper>
    <paper id="11">
      <title>Ajout d’informations contextuelles pour la recherche de passages au sein de Wikipédia (Integrating contextual information for passage retrieval in <fixed-case>W</fixed-case>ikipedia)</title>
      <author><first>Romain</first><last>Deveaud</last></author>
      <author><first>Eric</first><last>Sanjuan</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <pages>61–66</pages>
      <abstract>La recherche de passages consiste à extraire uniquement des passages pertinents par rapport à une requête utilisateur plutôt qu’un ensemble de documents entiers. Cette récupération de passages est souvent handicapée par le manque d’informations complémentaires concernant le contexte de la recherche initiée par l’utilisateur. Des études montrent que l’ajout d’informations contextuelles par l’utilisateur peut améliorer les performances des systèmes de recherche de passages. Nous confirmons ces observations dans cet article, et nous introduisons également une méthode d’enrichissement de la requête à partir d’informations contextuelles issues de documents encyclopédiques. Nous menons des expérimentations en utilisant la collection et les méthodes d’évaluation proposées par la campagne INEX. Les résultats obtenus montrent que l’ajout d’informations contextuelles permet d’améliorer significativement les performances de notre système de recherche de passages. Nous observons également que notre approche automatique obtient les meilleurs résultats parmi les différentes approches que nous évaluons.</abstract>
      <url hash="c4281bba">2011.jeptalnrecital-court.11</url>
      <language>fra</language>
      <bibkey>deveaud-etal-2011-ajout</bibkey>
    </paper>
    <paper id="12">
      <title>Construction d’un lexique des adjectifs dénominaux (Construction of a lexicon of denominal adjectives)</title>
      <author><first>Jana</first><last>Strnadová</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>67–72</pages>
      <abstract>Après une brève analyse linguistique des adjectifs dénominaux en français, nous décrivons le processus automatique que nous avons mis en place à partir de lexiques et de corpus volumineux pour construire un lexique d’adjectifs dénominaux dérivés de manière régulière. Nous estimons à la fois la précision et la couverture du lexique dérivationnel obtenu. À terme, ce lexique librement disponible aura été validé manuellement et contiendra également les adjectifs dénominaux à base supplétive.</abstract>
      <url hash="5dc7de69">2011.jeptalnrecital-court.12</url>
      <language>fra</language>
      <bibkey>strnadova-sagot-2011-construction</bibkey>
    </paper>
    <paper id="13">
      <title>Développement de ressources pour le persan : <fixed-case>P</fixed-case>er<fixed-case>L</fixed-case>ex 2, nouveau lexique morphologique et <fixed-case>ME</fixed-case>ltfa, étiqueteur morphosyntaxique (Development of resources for <fixed-case>P</fixed-case>ersian: <fixed-case>P</fixed-case>er<fixed-case>L</fixed-case>ex 2, a new morphological lexicon and <fixed-case>ME</fixed-case>ltfa, a morphosyntactic tagger)</title>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Géraldine</first><last>Walther</last></author>
      <author><first>Pegah</first><last>Faghiri</last></author>
      <author><first>Pollet</first><last>Samvelian</last></author>
      <pages>73–78</pages>
      <abstract>Nous présentons une nouvelle version de PerLex, lexique morphologique du persan, une version corrigée et partiellement réannotée du corpus étiqueté BijanKhan (BijanKhan, 2004) et MEltfa, un nouvel étiqueteur morphosyntaxique librement disponible pour le persan. Après avoir développé une première version de PerLex (Sagot &amp; Walther, 2010), nous en proposons donc ici une version améliorée. Outre une validation manuelle partielle, PerLex 2 repose désormais sur un inventaire de catégories linguistiquement motivé. Nous avons également développé une nouvelle version du corpus BijanKhan : elle contient des corrections significatives de la tokenisation ainsi qu’un réétiquetage à l’aide des nouvelles catégories. Cette nouvelle version du corpus a enfin été utilisée pour l’entraînement de MEltfa, notre étiqueteur morphosyntaxique pour le persan librement disponible, s’appuyant à la fois sur ce nouvel inventaire de catégories, sur PerLex 2 et sur le système d’étiquetage MElt (Denis &amp; Sagot, 2009).</abstract>
      <url hash="07a115c3">2011.jeptalnrecital-court.13</url>
      <language>fra</language>
      <bibkey>sagot-etal-2011-developpement</bibkey>
    </paper>
    <paper id="14">
      <title>Identification de cognats à partir de corpus parallèles français-roumain (Identification of cognates from <fixed-case>F</fixed-case>rench-<fixed-case>R</fixed-case>omanian parallel corpora)</title>
      <author><first>Mirabela</first><last>Navlea</last></author>
      <author><first>Amalia</first><last>Todiraşcu</last></author>
      <pages>79–84</pages>
      <abstract>Cet article présente une méthode hybride d’identification de cognats français - roumain. Cette méthode exploite des corpus parallèles alignés au niveau propositionnel, lemmatisés et étiquetés (avec des propriétés morphosyntaxiques). Notre méthode combine des techniques statistiques et des informations linguistiques pour améliorer les résultats obtenus. Nous évaluons le module d’identification de cognats et nous faisons une comparaison avec des méthodes statistiques pures, afin d’étudier l’impact des informations linguistiques utilisées sur la qualité des résultats obtenus. Nous montrons que l’utilisation des informations linguistiques augmente significativement la performance de la méthode.</abstract>
      <url hash="28bcaeeb">2011.jeptalnrecital-court.14</url>
      <language>fra</language>
      <bibkey>navlea-todirascu-2011-identification</bibkey>
    </paper>
    <paper id="15">
      <title>Le <fixed-case>TAL</fixed-case> au service de l’<fixed-case>ALAO</fixed-case>/<fixed-case>ELAO</fixed-case> L’exemple des exercices de dictée automatisés (The use of <fixed-case>NLP</fixed-case> in <fixed-case>CALL</fixed-case> The example of automated dictation exercises)</title>
      <author><first>Richard</first><last>Beaufort</last></author>
      <author><first>Sophie</first><last>Roekhaut</last></author>
      <pages>85–90</pages>
      <abstract>Ce papier s’inscrit dans le cadre général de l’Apprentissage et de l’Enseignement des Langues Assistés par Ordinateur, et concerne plus particulièrement l’automatisation des exercices de dictée. Il présente une méthode de correction des copies d’apprenants qui se veut originale en deux points. Premièrement, la méthode exploite la composition d’automates à états finis pour détecter et pour analyser les erreurs. Deuxièmement, elle repose sur une analyse morphosyntaxique automatique de l’original de la dictée, ce qui facilite la production de diagnostics.</abstract>
      <url hash="a2eae114">2011.jeptalnrecital-court.15</url>
      <language>fra</language>
      <bibkey>beaufort-roekhaut-2011-le</bibkey>
    </paper>
    <paper id="16">
      <title>Une analyse basée sur la <fixed-case>S</fixed-case>-<fixed-case>DRT</fixed-case> pour la modélisation de dialogues pathologiques (An analysis based on the <fixed-case>S</fixed-case>-<fixed-case>DRT</fixed-case> for modeling pathological dialogues)</title>
      <author><first>Maxime</first><last>Amblard</last></author>
      <author><first>Michel</first><last>Musiol</last></author>
      <author><first>Manuel</first><last>Rebuschi</last></author>
      <pages>91–96</pages>
      <abstract>Dans cet article, nous présentons la définition et l’étude d’un corpus de dialogues entre un schizophrène et un interlocuteur ayant pour objectif la conduite et le maintien de l’échange. Nous avons identifié des discontinuités significatives chez les schizophrènes paranoïdes. Une représentation issue de la S-DRT (sa partie pragmatique) permet de rendre compte des ces usages non standards.</abstract>
      <url hash="d846068f">2011.jeptalnrecital-court.16</url>
      <language>fra</language>
      <bibkey>amblard-etal-2011-une</bibkey>
    </paper>
    <paper id="17">
      <title>Le corpus <fixed-case>T</fixed-case>ext+<fixed-case>B</fixed-case>erg Une ressource parallèle alpin français-allemand (The <fixed-case>T</fixed-case>ext+<fixed-case>B</fixed-case>erg Corpus An Alpine <fixed-case>F</fixed-case>rench-<fixed-case>G</fixed-case>erman Parallel Resource)</title>
      <author><first>Anne</first><last>Göhring</last></author>
      <author><first>Martin</first><last>Volk</last></author>
      <pages>97–102</pages>
      <abstract>Cet article présente un corpus parallèle français-allemand de plus de 4 millions de mots issu de la numérisation d’un corpus alpin multilingue. Ce corpus est une précieuse ressource pour de nombreuses études de linguistique comparée et du patrimoine culturel ainsi que pour le développement d’un système statistique de traduction automatique dans un domaine spécifique. Nous avons annoté un échantillon de ce corpus parallèle et aligné les structures arborées au niveau des mots, des constituants et des phrases. Cet “alpine treebank” est le premier corpus arboré parallèle français-allemand de haute qualité (manuellement contrôlé), de libre accès et dans un domaine et un genre nouveau : le récit d’alpinisme.</abstract>
      <url hash="08e25e76">2011.jeptalnrecital-court.17</url>
      <language>fra</language>
      <bibkey>gohring-volk-2011-le</bibkey>
    </paper>
    <paper id="18">
      <title>Ordonner un résumé automatique multi-documents fondé sur une classification des phrases en classes lexicales (Ordering a multi-document summary based on sentences subtopic clustering)</title>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <author><first>Émilie</first><last>Guimier De Neef</last></author>
      <pages>103–108</pages>
      <abstract>Nous présentons différentes méthodes de réordonnancement de phrases pour le résumé automatique fondé sur une classification des phrases à résumer en classes thématiques. Nous comparons ces méthodes à deux baselines : ordonnancement des phrases selon leur pertinence et ordonnancement selon la date et la position dans le document d’origine. Nous avons fait évaluer les résumés obtenus sur le corpus RPM2 par 4 annotateurs et présentons les résultats.</abstract>
      <url hash="9d6211cd">2011.jeptalnrecital-court.18</url>
      <language>fra</language>
      <bibkey>bossard-guimier-de-neef-2011-ordonner</bibkey>
    </paper>
    <paper id="19">
      <title>Construction d’une grammaire d’arbres adjoints pour la langue arabe (Construction of a tree adjoining grammar for the <fixed-case>A</fixed-case>rabic language)</title>
      <author><first>Fériel</first><last>Ben Fraj</last></author>
      <pages>109–115</pages>
      <abstract>La langue arabe présente des spécificités qui la rendent plus ambigüe que d’autres langues naturelles. Sa morphologie, sa syntaxe ainsi que sa sémantique sont en corrélation et se complètent l’une l’autre. Dans le but de construire une grammaire qui soit adaptée à ces spécificités, nous avons conçu et développé une application d’aide à la création des règles syntaxiques licites suivant le formalisme d’arbres adjoints. Cette application est modulaire et enrichie par des astuces de contrôle de la création et aussi d’une interface conviviale pour assister l’utilisateur final dans la gestion des créations prévues.</abstract>
      <url hash="0b778275">2011.jeptalnrecital-court.19</url>
      <language>fra</language>
      <bibkey>ben-fraj-2011-construction</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>F</fixed-case>re<fixed-case>D</fixed-case>ist : Construction automatique d’un thésaurus distributionnel pour le Français (<fixed-case>F</fixed-case>re<fixed-case>D</fixed-case>ist : Automatic construction of distributional thesauri for <fixed-case>F</fixed-case>rench)</title>
      <author><first>Enrique</first><last>Henestroza Anguiano</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <pages>116–121</pages>
      <abstract>Dans cet article, nous présentons FreDist, un logiciel libre pour la construction automatique de thésaurus distributionnels à partir de corpus de texte, ainsi qu’une évaluation des différents ressources ainsi produites. Suivant les travaux de (Lin, 1998) et (Curran, 2004), nous utilisons un corpus journalistique de grande taille et implémentons différentes options pour : le type de relation contexte lexical, la fonction de poids, et la fonction de mesure de similarité. Prenant l’EuroWordNet français et le WOLF comme références, notre évaluation révèle, de manière originale, que c’est l’approche qui combine contextes linéaires (ici, de type bigrammes) et contextes syntaxiques qui semble fournir le meilleur thésaurus. Enfin, nous espérons que notre logiciel, distribué avec nos meilleurs thésaurus pour le français, seront utiles à la communauté TAL.</abstract>
      <url hash="53dc1262">2011.jeptalnrecital-court.20</url>
      <language>fra</language>
      <bibkey>henestroza-anguiano-denis-2011-fredist</bibkey>
    </paper>
    <paper id="21">
      <title>Utilisation de critères linguistiques de surface pour l’extraction de relation dans les textes bio-médicaux (Using shallow linguistic features for relation extraction in bio-medical texts)</title>
      <author><first>Ali</first><last>Reza Ebadat</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <author><first>Pascale</first><last>Sébillot</last></author>
      <pages>122–127</pages>
      <abstract>Dans cet article, nous proposons de modéliser la tâche d’extraction de relations à partir de corpus textuels comme un problème de classification. Nous montrons que, dans ce cadre, des représentations fondées sur des informations linguistiques de surface sont suffisantes pour que des algorithmes d’apprentissage artificiel standards les exploitant rivalisent avec les meilleurs systèmes d’extraction de relations reposant sur des connaissances issues d’analyses profondes (analyses syntaxiques ou sémantiques). Nous montrons également qu’en prenant davantage en compte les spécificités de la tâche d’extraction à réaliser et des données disponibles, il est possible d’obtenir des méthodes encore plus efficaces tout en exploitant ces informations simples. La technique originale à base d’apprentissage « paresseux » et de modèles de langue que nous évaluons en extraction d’interactions géniques sur les données du challenge LLL2005 dépasse les résultats de l’état de l’art.</abstract>
      <url hash="da940413">2011.jeptalnrecital-court.21</url>
      <language>fra</language>
      <bibkey>reza-ebadat-etal-2011-utilisation</bibkey>
    </paper>
    <paper id="22">
      <title>Vers une prise en charge approfondie des phénomènes itératifs par <fixed-case>T</fixed-case>ime<fixed-case>ML</fixed-case> (Toward a comprehensive support of iterative phenomenons in <fixed-case>T</fixed-case>ime<fixed-case>ML</fixed-case>)</title>
      <author><first>Julien</first><last>Lebranchu</last></author>
      <author><first>Yann</first><last>Mathet</last></author>
      <pages>128–133</pages>
      <abstract>Les travaux menés ces dernières années autour de l’itération en langue, tant par la communauté linguistique que par celle du TAL, ont mis au jour des phénomènes particuliers, non réductibles aux représentations temporelles classiques. En particulier, une itération ne saurait structurellement être réduite à une simple énumération de procès, et du point de vue de l’aspect, met en jeu simultanément deux visées aspectuelles indépendantes. Le formalisme TimeML, qui a vocation à annoter les informations temporelles portées par un texte, intègre déjà des éléments relatifs aux itérations, mais ne prend pas en compte ces dernières avancées. C’est ce que nous entreprenons de faire dans cet article, en proposant une extension à ce formalisme.</abstract>
      <url hash="037d15a8">2011.jeptalnrecital-court.22</url>
      <language>fra</language>
      <bibkey>lebranchu-mathet-2011-vers</bibkey>
    </paper>
    <paper id="23">
      <title>Une procédure pour identifier les modifieurs de la valence affective d’un mot dans des textes (A procedure to identify modifiers of the word emotional valence in texts)</title>
      <author><first>Noémi</first><last>Boubel</last></author>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>134–139</pages>
      <abstract>Cette recherche s’inscrit dans le champ de la fouille d’opinion et, plus particulièrement, dans celui de l’analyse de la polarité d’une phrase ou d’un syntagme. Dans ce cadre, la prise en compte du contexte linguistique dans lequel apparaissent les mots porteurs de valence est particulièrement importante. Nous proposons une méthodologie pour extraire automatiquement de corpus de textes de telles expressions linguistiques. Cette approche s’appuie sur un corpus de textes, ou d’extraits de textes, dont la valence est connue, sur un lexique de valence construit à partir de ce corpus au moyen d’une procédure automatique et sur un analyseur syntaxique. Une étude exploratoire, limitée à la seule relation syntaxique associant un adverbe à un adjectif, laisse entrevoir les potentialités de l’approche.</abstract>
      <url hash="b2421803">2011.jeptalnrecital-court.23</url>
      <language>fra</language>
      <bibkey>boubel-bestgen-2011-une</bibkey>
    </paper>
    <paper id="24">
      <title>Stratégie d’exploration de corpus multi-annotés avec <fixed-case>G</fixed-case>lozz<fixed-case>QL</fixed-case> (Multi-annotated corpus exploration strategy with <fixed-case>G</fixed-case>lozz<fixed-case>QL</fixed-case>)</title>
      <author><first>Yann</first><last>Mathet</last></author>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <pages>140–145</pages>
      <abstract>La multiplication des travaux sur corpus, en linguistique computationnelle et en TAL, conduit à la multiplication des campagnes d’annotation et des corpus multi-annotés, porteurs d’informations relatives à des phénomènes variés, envisagés par des annotateurs multiples, parfois automatiques. Pour mieux comprendre les phénomènes que ces campagnes prennent pour objets, ou pour contrôler les données en vue de l’établissement d’un corpus de référence, il est nécessaire de disposer d’outils permettant d’explorer les annotations. Nous présentons une stratégie possible et son opérationalisation dans la plate-forme Glozz par le langage GlozzQL.</abstract>
      <url hash="eb7ef37c">2011.jeptalnrecital-court.24</url>
      <language>fra</language>
      <bibkey>mathet-widlocher-2011-strategie</bibkey>
    </paper>
    <paper id="25">
      <title>Attribution de rôles sémantiques aux actants des lexies verbales (Assigning semantic roles to actants of verbal lexical units)</title>
      <author><first>Fadila</first><last>Hadouche</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <author><first>Marie-Claude</first><last>L’Homme</last></author>
      <pages>146–151</pages>
      <abstract>Dans cet article, nous traitons de l’attribution des rôles sémantiques aux actants de lexies verbales en corpus spécialisé en français. Nous proposons une classification de rôles sémantiques par apprentissage machine basée sur un corpus de lexies verbales annotées manuellement du domaine de l’informatique et d’Internet. Nous proposons également une méthode de partitionnement semi-supervisé pour prendre en compte l’annotation de nouvelles lexies ou de nouveaux rôles sémantiques et de les intégrés dans le système. Cette méthode de partitionnement permet de regrouper les instances d’actants selon les valeurs communes correspondantes aux traits de description des actants dans des groupes d’instances d’actants similaires. La classification de rôles sémantique a obtenu une F-mesure de 93% pour Patient, de 90% pour Agent, de 85% pour Destination et de 76% pour les autres rôles pris ensemble. Quand au partitionnement en regroupant les instances selon leur similarité donne une F-mesure de 88% pour Patient, de 81% pour Agent, de 58% pour Destination et de 46% pour les autres rôles.</abstract>
      <url hash="7081007e">2011.jeptalnrecital-court.25</url>
      <language>fra</language>
      <bibkey>hadouche-etal-2011-attribution</bibkey>
    </paper>
    <paper id="26">
      <title>Utiliser l’amorçage pour améliorer une mesure de similarité sémantique (Using bootstrapping to improve a measure of semantic similarity)</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>152–157</pages>
      <abstract>Les travaux sur les mesures de similarité sémantique de nature distributionnelle ont abouti à un certain consensus quant à leurs performances et ont montré notamment que leurs résultats sont surtout intéressants pour des mots de forte fréquence et une similarité sémantique étendue, non restreinte aux seuls synonymes. Dans cet article, nous proposons une méthode d’amélioration d’une mesure de similarité classique permettant de rééquilibrer ses résultats pour les mots de plus faible fréquence. Cette méthode est fondée sur un mécanisme d’amorçage : un ensemble d’exemples et de contre-exemples de mots sémantiquement liés sont sélectionnés de façon non supervisée à partir des résultats de la mesure initiale et servent à l’entraînement d’un classifieur supervisé. Celui-ci est ensuite utilisé pour réordonner les voisins sémantiques initiaux. Nous évaluons l’intérêt de ce réordonnancement pour un large ensemble de noms anglais couvrant différents domaines fréquentiels.</abstract>
      <url hash="e6934220">2011.jeptalnrecital-court.26</url>
      <language>fra</language>
      <bibkey>ferret-2011-utiliser</bibkey>
    </paper>
    <paper id="27">
      <title>Un calcul de termes typés pour la pragmatique lexicale: chemins et voyageurs fictifs dans un corpus de récits de voyage (A calculation of typed terms for lexical pragmatics: paths and fictional travellers in a travel stories corpus)</title>
      <author><first>Richard</first><last>Moot</last></author>
      <author><first>Laurent</first><last>Prévot</last></author>
      <author><first>Christian</first><last>Retoré</last></author>
      <pages>158–163</pages>
      <abstract>Ce travail s’inscrit dans l’analyse automatique d’un corpus de récits de voyage. À cette fin, nous raffinons la sémantique de Montague pour rendre compte des phénomènes d’adaptation du sens des mots au contexte dans lequel ils apparaissent. Ici, nous modélisons les constructions de type ‘le chemin descend pendant une demi-heure’ où ledit chemin introduit un voyageur fictif qui le parcourt, en étendant des idées que le dernier auteur a développé avec Bassac et Mery. Cette introduction du voyageur utilise la montée de type afin que le quantificateur introduisant le voyageur porte sur toute la phrase et que les propriétés du chemin ne deviennent pas des propriétés du voyageur, fût-il fictif. Cette analyse sémantique (ou plutôt sa traduction en lambda-DRT) est d’ores et déjà implantée pour une partie du lexique de Grail.</abstract>
      <url hash="2fcc2034">2011.jeptalnrecital-court.27</url>
      <language>fra</language>
      <bibkey>moot-etal-2011-un</bibkey>
    </paper>
    <paper id="28">
      <title>Catégoriser les réponses aux interruptions dans les débats politiques (Categorizing responses to disruptions in political debates)</title>
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Cristel</first><last>Portes</last></author>
      <author><first>Agnès</first><last>Steuckardt</last></author>
      <author><first>Marion</first><last>Tellier</last></author>
      <pages>164–169</pages>
      <abstract>Cet article traite de l’analyse de débats politiques selon une orientation multimodale. Nous étudions plus particulièrement les réponses aux interruptions lors d’un débat à l’Assemblée nationale. Nous proposons de procéder à l’analyse via des annotations systématiques de différentes modalités. L’analyse argumentative nous a amenée à proposer une typologie de ces réponses. Celle-ci a été mise à l’épreuve d’une classification automatique. La difficulté dans la construction d’un tel système réside dans la nature même des données : multimodales, parfois manquantes et incertaines.</abstract>
      <url hash="3bab56f4">2011.jeptalnrecital-court.28</url>
      <language>fra</language>
      <bibkey>bigi-etal-2011-categoriser</bibkey>
    </paper>
    <paper id="29">
      <title>Mesure non-supervisée du degré d’appartenance d’une entité à un type (An unsupervised measure of the degree of belonging of an entity to a type)</title>
      <author><first>Ludovic</first><last>Bonnefoy</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <author><first>Michel</first><last>Benoit</last></author>
      <pages>170–175</pages>
      <abstract>La recherche d’entités nommées a été le sujet de nombreux travaux. Cependant, la construction des ressources nécessaires à de tels systèmes reste un problème majeur. Dans ce papier, nous proposons une méthode complémentaire aux outils capables de reconnaître des entités de types larges, dont l’objectif est de déterminer si une entité est d’un type donné, et ce de manière non-supervisée et quel que soit le type. Nous proposons pour cela une approche basée sur la comparaison de modèles de langage estimés à partir du Web. L’intérêt de notre approche est validé par une évaluation sur 100 entités et 273 types différents.</abstract>
      <url hash="582e138f">2011.jeptalnrecital-court.29</url>
      <language>fra</language>
      <bibkey>bonnefoy-etal-2011-mesure</bibkey>
    </paper>
    <paper id="30">
      <title>Traduction (automatique) des connecteurs de discours ((Machine) Translation of discourse connectors)</title>
      <author><first>Laurence</first><last>Danlos</last></author>
      <author><first>Charlotte</first><last>Roze</last></author>
      <pages>176–181</pages>
      <abstract>En nous appuyant sur des données fournies par le concordancier bilingue TransSearch qui intègre un alignement statistique au niveau des mots, nous avons effectué une annotation semi-manuelle de la traduction anglaise de deux connecteurs du français. Les résultats de cette annotation montrent que les traductions de ces connecteurs ne correspondent pas aux « transpots » identifiés par TransSearch et encore moins à ce qui est proposé dans les dictionnaires bilingues.</abstract>
      <url hash="cb0c081e">2011.jeptalnrecital-court.30</url>
      <language>fra</language>
      <bibkey>danlos-roze-2011-traduction</bibkey>
    </paper>
    <paper id="31">
      <title>Découverte de patrons paraphrastiques en corpus comparable: une approche basée sur les n-grammes (Extracting paraphrastic patterns comparable corpus: an approach based on n-grams)</title>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <author><first>Louise</first><last>Deléger</last></author>
      <pages>182–187</pages>
      <abstract>Cet article présente l’utilisation d’un corpus comparable pour l’extraction de patrons de paraphrases. Nous présentons une méthode empirique basée sur l’appariement de n-grammes, permettant d’extraire des patrons de paraphrases dans des corpus comparables d’une même langue (le français), du même domaine (la médecine) mais de registres de langues différents (spécialisé ou grand public). Cette méthode confirme les résultats précédents basés sur des méthodes à base de patrons, et permet d’identifier de nouveaux patrons, apportant également un regard nouveau sur les différences entre les discours de langue générale et spécialisée.</abstract>
      <url hash="b8f46a83">2011.jeptalnrecital-court.31</url>
      <language>fra</language>
      <bibkey>cartoni-deleger-2011-decouverte</bibkey>
    </paper>
    <paper id="32">
      <title>Prise en compte de la sous-catégorisation verbale dans un lexique bilingue anglais-japonais (Verbal subcategorization in an <fixed-case>E</fixed-case>nglish-<fixed-case>J</fixed-case>apanese bilingual lexicon)</title>
      <author><first>Alexis</first><last>Kauffmann</last></author>
      <pages>188–193</pages>
      <abstract>Dans cet article, nous présentons une méthode de détection des correspondances bilingues de sous-catégorisation verbale à partir de données lexicales monolingues. Nous évoquons également la structure de ces lexiques et leur utilisation en traduction automatique (TA) à base linguistique anglais-japonais. Les lexiques sont utilisés par un programme de TA fonctionnant selon une architecture classique dite “à transfert”, et leur structure permet une classification précise des sous-catégorisations verbales. Nos travaux ont permis une amélioration des données de sous-catégorisation des lexiques pour les verbes japonais et leurs équivalents anglais, en utilisant des données linguistiques compilées à partir d’un corpus de textes extrait du web. De plus, le fonctionnement du programme de TA a pu ^etre amélioré en utilisant ces données.</abstract>
      <url hash="cdf11681">2011.jeptalnrecital-court.32</url>
      <language>fra</language>
      <bibkey>kauffmann-2011-prise</bibkey>
    </paper>
    <paper id="33">
      <title>Extraction non-supervisée de relations basée sur la dualité de la représentation (Unsupervised relation extraction based on the dual representation)</title>
      <author><first>Yayoi</first><last>Nakamura-Delloye</last></author>
      <pages>194–199</pages>
      <abstract>Nous proposons dans cet article une méthode non-supervisée d’extraction des relations entre entités nommées. La méthode proposée se caractérise par l’utilisation de résultats d’analyses syntaxiques, notamment les chemins syntaxiques reliant deux entités nommées dans des arbres de dépendance. Nous avons également exploité la dualité de la représentation des relations sémantiques et le résultat de notre expérience comparative a montré que cette approche améliorait les rappels.</abstract>
      <url hash="42d2e5e9">2011.jeptalnrecital-court.33</url>
      <language>fra</language>
      <bibkey>nakamura-delloye-2011-extraction</bibkey>
    </paper>
    <paper id="34">
      <title>Vers la détection des dislocations à gauche dans les transcriptions automatiques du Français parlé (Towards automatic recognition of left dislocation in transcriptions of Spoken <fixed-case>F</fixed-case>rench)</title>
      <author><first>Corinna</first><last>Anderson</last></author>
      <author><first>Christophe</first><last>Cerisara</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>200–205</pages>
      <abstract>Ce travail prend place dans le cadre plus général du développement d’une plate-forme d’analyse syntaxique du français parlé. Nous décrivons la conception d’un modèle automatique pour résoudre le lien anaphorique présent dans les dislocations à gauche dans un corpus de français parlé radiophonique. La détection de ces structures devrait permettre à terme d’améliorer notre analyseur syntaxique en enrichissant les informations prises en compte dans nos modèles automatiques. La résolution du lien anaphorique est réalisée en deux étapes : un premier niveau à base de règles filtre les configurations candidates, et un second niveau s’appuie sur un modèle appris selon le critère du maximum d’entropie. Une évaluation expérimentale réalisée par validation croisée sur un corpus annoté manuellement donne une F-mesure de l’ordre de 40%.</abstract>
      <url hash="a5f807f0">2011.jeptalnrecital-court.34</url>
      <language>fra</language>
      <bibkey>anderson-etal-2011-vers</bibkey>
    </paper>
    <paper id="35">
      <title>Règles et paradigmes en morphologie informatique lexématique (Rules and paradigms in lexematic computer morphology)</title>
      <author><first>Nabil</first><last>Hathout</last></author>
      <author><first>Fiammetta</first><last>Namer</last></author>
      <pages>206–211</pages>
      <abstract>Les familles de mots produites par deux analyseurs morphologiques, DériF (basé sur des règles) et Morphonette (basé sur l’analogie), appliqués à un même corpus lexical, sont comparées. Cette comparaison conduit à l’examen de trois sous-ensembles : - un sous-ensemble commun aux deux systèmes dont la taille montre que, malgré leurs différences, les approches expérimentées par chaque système sont valides et décrivent en partie la même réalité morphologique. - un sous-ensemble propre à DériF et un autre à Morphonette. Ces ensembles (a) nous renseignent sur les caractéristiques propres à chaque système, et notamment sur ce que l’autre ne peut pas produire, (b) ils mettent en évidence les erreurs d’un système, en ce qu’elles n’apparaissent pas dans l’autre, (c) ils font apparaître certaines limites de la description, notamment celles qui sont liées aux objets et aux notions théoriques comme les familles morphologiques, les bases, l’existence de RCL « transversales » entre les lexèmes qui n’ont pas de relation d’ascendance ou de descendance.</abstract>
      <url hash="78597df8">2011.jeptalnrecital-court.35</url>
      <language>fra</language>
      <bibkey>hathout-namer-2011-regles</bibkey>
    </paper>
    <paper id="36">
      <title>Classification de séquences bidirectionnelles pour des tâches d’étiquetage par apprentissage guidé (Bidirectional Sequence Classification for Tagging Tasks with Guided Learning)</title>
      <author><first>Andrea</first><last>Gesmundo</last></author>
      <pages>212–217</pages>
      <abstract>Dans cet article nous présentons une série d’adaptations de l’algorithme du “cadre d’apprenstissage guidé” pour résoudre différentes tâches d’étiquetage. La spécificité du système proposé réside dans sa capacité à apprendre l’ordre de l’inférence avec les paramètres du classifieur local au lieu de la forcer dans un ordre pré-défini (de gauche à droite). L’algorithme d’entraînement est basé sur l’algorithme du “perceptron”. Nous appliquons le système à différents types de tâches d’étiquetage pour atteindre des résultats au niveau de l’état de l’art en un court temps d’exécution.</abstract>
      <url hash="0c7afe5f">2011.jeptalnrecital-court.36</url>
      <language>fra</language>
      <bibkey>gesmundo-2011-classification</bibkey>
    </paper>
    <paper id="37">
      <title>Calcul de réseaux phrastiques pour l’analyse et la navigation textuelle (Computing sentence networks for textual analysis and navigation)</title>
      <author><first>Dominique</first><last>Legallois</last></author>
      <author><first>Peggy</first><last>Cellier</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>218–223</pages>
      <abstract>Le travail présente une méthode de navigation dans les textes, fondée sur la répétition lexicale. La méthode choisie est celle développée par le linguiste Hoey. Son application manuelle à des textes de grandeur conséquente est problématique. Nous proposons dans cet article un processus automatique qui permet d’analyser selon cette méthode des textes de grande taille ; des expériences ont été menées appliquant le processus à différents types de textes (narratif, expositif) et montrant l’intérêt de l’approche.</abstract>
      <url hash="a715a7d5">2011.jeptalnrecital-court.37</url>
      <language>fra</language>
      <bibkey>legallois-etal-2011-calcul</bibkey>
    </paper>
    <paper id="38">
      <title>Exploitation d’un corpus arboré pour non spécialistes par des requêtes guidées et des requêtes sémantiques (Exploiting a Treebank for non-specialists by guided queries and semantic queries)</title>
      <author><first>Achille</first><last>Falaise</last></author>
      <author><first>Agnès</first><last>Tutin</last></author>
      <author><first>Olivier</first><last>Kraif</last></author>
      <pages>224–229</pages>
      <abstract>L’exploitation de corpus analysés syntaxiquement (ou corpus arborés) pour le public non spécialiste n’est pas un problème trivial. Si la communauté du TAL souhaite mettre à la disposition des chercheurs non-informaticiens des corpus comportant des annotations linguistiques complexes, elle doit impérativement développer des interfaces simples à manipuler mais permettant des recherches fines. Dans cette communication, nous présentons les modes de recherche « grand public » développé(e)s dans le cadre du projet Scientext, qui met à disposition un corpus d’écrits scientifiques interrogeable par partie textuelle, par partie du discours et par fonction syntaxique. Les modes simples sont décrits : un mode libre et guidé, où l’utilisateur sélectionne lui-même les éléments de la requête, et un mode sémantique, qui comporte des grammaires locales préétablies à l’aide des fonctions syntaxiques.</abstract>
      <url hash="60e855f8">2011.jeptalnrecital-court.38</url>
      <language>fra</language>
      <bibkey>falaise-etal-2011-exploitation</bibkey>
    </paper>
    <paper id="39">
      <title>Communautés <fixed-case>I</fixed-case>nternet comme sources de préterminologie (<fixed-case>I</fixed-case>nternet communities as sources of preterminology)</title>
      <author><first>Mohammad</first><last>Daoud</last></author>
      <author><first>Christian</first><last>Boitet</last></author>
      <pages>230–235</pages>
      <abstract>Cet article décrit deux expériences sur la construction de ressources terminologiques multilingues (preterminologies) préliminaires, mais grandes, grâce à des communautés Internet, et s’appuie sur ces expériences pour cibler des données terminologiques plus raffinées venant de communautés Internet et d’applications Web 2.0. La première expérience est une passerelle de contribution pour le site Web de la Route de la Soie numérique (DSR). Les visiteurs contribuent en effet à un référentiel lexical multilingue dédié, pendant qu’ils visitent et lisent les livres archivés, parce qu’ils sont intéressés par le domaine et ont tendance à être polygottes. Nous avons recueilli 1400 contributions lexicales en 4 mois. La seconde expérience est basée sur le JeuxDeMots arabe, où les joueurs en ligne contribuent à un réseau lexical arabe. L’expérience a entraîné une croissance régulière du nombre de joueurs et de contributions, ces dernières contenant des termes absents et des mots de dialectes oraux.</abstract>
      <url hash="e6688b30">2011.jeptalnrecital-court.39</url>
      <language>fra</language>
      <bibkey>daoud-boitet-2011-communautes</bibkey>
    </paper>
    <paper id="40">
      <title>Évaluation de <fixed-case>G</fixed-case>-<fixed-case>L</fixed-case>ex<fixed-case>A</fixed-case>r pour la traduction automatique statistique (Evaluation of <fixed-case>G</fixed-case>-Lexar for statistical machine translation)</title>
      <author><first>Wigdan</first><last>Mekki</last></author>
      <author><first>Julien</first><last>Gosme</last></author>
      <author><first>Fathi</first><last>Debili</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <author><first>Nadine</first><last>Lucas</last></author>
      <pages>236–241</pages>
      <abstract>G-LexAr est un analyseur morphologique de l’arabe qui a récemment reçu des améliorations substantielles. Cet article propose une évaluation de cet analyseur en tant qu’outil de pré-traitement pour la traduction automatique statistique, ce dont il n’a encore jamais fait l’objet. Nous étudions l’impact des différentes formes proposées par son analyse (voyellation, lemmatisation et segmentation) sur un système de traduction arabe-anglais, ainsi que l’impact de la combinaison de ces formes. Nos expériences montrent que l’utilisation séparée de chacune de ces formes n’a que peu d’influence sur la qualité des traductions obtenues, tandis que leur combinaison y contribue de façon très bénéfique.</abstract>
      <url hash="17218c80">2011.jeptalnrecital-court.40</url>
      <language>fra</language>
      <bibkey>mekki-etal-2011-evaluation</bibkey>
    </paper>
    <paper id="41">
      <title>Enrichir la notion de patron par la prise en compte de la structure textuelle - Application à la construction d’ontologie (Enriching the notion of pattern by taking into account the textual structure - Application to ontology construction)</title>
      <author><first>Marion</first><last>Laignelet</last></author>
      <author><first>Mouna</first><last>Kamel</last></author>
      <author><first>Nathalie</first><last>Aussenac-Gilles</last></author>
      <pages>242–247</pages>
      <abstract>La projection de patrons lexico-syntaxiques sur corpus est une des manières privilégiées pour identifier des relations sémantiques précises entre éléments lexicaux. Dans cet article, nous proposons d’étendre la notion de patron en prenant en compte la sémantique que véhiculent les éléments de structure d’un document (définitions, titres, énumérations) dans l’identification de relations. Nous avons testé cette hypothèse dans le cadre de la construction d’ontologies à partir de textes fortement structurés du domaine de la cartographie.</abstract>
      <url hash="39e7fbe9">2011.jeptalnrecital-court.41</url>
      <language>fra</language>
      <bibkey>laignelet-etal-2011-enrichir</bibkey>
    </paper>
    <paper id="42">
      <title>La traduction automatique des séquences clitiques dans un traducteur à base de règles (Automatic translation clitic sequences in a rule-based <fixed-case>MT</fixed-case> system)</title>
      <author><first>Lorenza</first><last>Russo</last></author>
      <author><first>Éric</first><last>Wehrli</last></author>
      <pages>248–253</pages>
      <abstract>Dans cet article, nous discutons la méthodologie utilisée par Its-2, un système de traduction à base de règles, pour la traduction des pronoms clitiques. En particulier, nous nous focalisons sur les séquences clitiques, pour la traduction automatique entre le français et l’anglais. Une évaluation basée sur un corpus de phrases construites montre le potentiel de notre approche pour des traductions de bonne qualité.</abstract>
      <url hash="f5b81e2d">2011.jeptalnrecital-court.42</url>
      <language>fra</language>
      <bibkey>russo-wehrli-2011-la</bibkey>
    </paper>
    <paper id="43">
      <title>Étude inter-langues de la distribution et des ambiguïtés syntaxiques des pronoms (A study of cross-language distribution and syntactic ambiguities of pronouns)</title>
      <author><first>Lorenza</first><last>Russo</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <author><first>Luka</first><last>Nerima</last></author>
      <author><first>Éric</first><last>Wehrli</last></author>
      <pages>254–259</pages>
      <abstract>Ce travail décrit la distribution des pronoms selon le style de texte (littéraire ou journalistique) et selon la langue (français, anglais, allemand et italien). Sur la base d’un étiquetage morpho-syntaxique effectué automatiquement puis vérifié manuellement, nous pouvons constater que la proportion des différents types de pronoms varie selon le type de texte et selon la langue. Nous discutons les catégories les plus ambiguës de manière détaillée. Comme nous avons utilisé l’analyseur syntaxique Fips pour l’étiquetage des pronoms, nous l’avons également évalué et obtenu une précision moyenne de plus de 95%.</abstract>
      <url hash="beef3092">2011.jeptalnrecital-court.43</url>
      <language>fra</language>
      <bibkey>russo-etal-2011-etude</bibkey>
    </paper>
    <paper id="44">
      <title>La traduction automatique des pronoms. Problèmes et perspectives (Automatic translation of pronouns. Problems and perspectives)</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Lorenza</first><last>Russo</last></author>
      <author><first>Jean-Philippe</first><last>Goldman</last></author>
      <author><first>Sharid</first><last>Loáiciga</last></author>
      <author><first>Luka</first><last>Nerima</last></author>
      <author><first>Éric</first><last>Wehrli</last></author>
      <pages>260–265</pages>
      <abstract>Dans cette étude, notre système de traduction automatique, Its-2, a fait l’objet d’une évaluation manuelle de la traduction des pronoms pour cinq paires de langues et sur deux corpus : un corpus littéraire et un corpus de communiqués de presse. Les résultats montrent que les pourcentages d’erreurs peuvent atteindre 60% selon la paire de langues et le corpus. Nous discutons ainsi deux pistes de recherche pour l’amélioration des performances de Its-2 : la résolution des ambiguïtés d’analyse et la résolution des anaphores pronominales.</abstract>
      <url hash="13c56c27">2011.jeptalnrecital-court.44</url>
      <language>fra</language>
      <bibkey>scherrer-etal-2011-la</bibkey>
    </paper>
    <paper id="45">
      <title>Ressources lexicales pour une sémantique inférentielle : un exemple, le mot « quitter » (Lexical resources for semantic inference: an example, the word “quitter”)</title>
      <author><first>Daniel</first><last>Kayser</last></author>
      <pages>266–271</pages>
      <abstract>On étudie environ 500 occurrences du verbe « quitter » en les classant selon les inférences qu’elles suggèrent au lecteur. On obtient ainsi 43 « schémas inférentiels ». Ils ne s’excluent pas l’un l’autre : si plusieurs d’entre eux s’appliquent, les inférences produites se cumulent ; cependant, comme l’auteur sait que le lecteur dispose de tels schémas, s’il veut l’orienter vers une seule interprétation, il fournit des indices permettant d’éliminer les autres. On conjecture que ces schémas présentent des régularités observables sur des familles de mots, que ces régularités proviennent du fonctionnement d’opérations génériques, et qu’il est donc sans gravité de ne pas être exhaustif, dans la mesure où ces opérations permettent d’engendrer les schémas manquants en cas de besoin.</abstract>
      <url hash="dc140451">2011.jeptalnrecital-court.45</url>
      <language>fra</language>
      <bibkey>kayser-2011-ressources</bibkey>
    </paper>
    <paper id="46">
      <title>Un système de détection d’opinions fondé sur l’analyse syntaxique profonde (An opinion detection system based on deep syntactic analysis)</title>
      <author><first>Caroline</first><last>Brun</last></author>
      <pages>272–277</pages>
      <abstract>Dans cet article, nous présentons un système de détection d’opinions construit à partir des sorties d’un analyseur syntaxique robuste produisant des analyses profondes. L’objectif de ce système est l’extraction d’opinions associées à des produits (les concepts principaux) ainsi qu’aux concepts qui leurs sont associés (en anglais «features-based opinion extraction»). Suite à une étude d’un corpus cible, notre analyseur syntaxique est enrichi par l’ajout de polarité aux éléments pertinents du lexique et par le développement de règles génériques et spécialisées permettant l’extraction de relations sémantiques d’opinions, qui visent à alimenter un modèle de représentation des opinions. Une première évaluation montre des résultats très encourageants, mais de nombreuses perspectives restent à explorer.</abstract>
      <url hash="53f714bc">2011.jeptalnrecital-court.46</url>
      <language>fra</language>
      <bibkey>brun-2011-un</bibkey>
    </paper>
    <paper id="47">
      <title>Développement d’un système de détection des infections associées aux soins à partir de l’analyse de comptes-rendus d’hospitalisation (Development of a system that detects occurrences of healthcare-associated infections from the analysis of hospitalization reports)</title>
      <author><first>Caroline</first><last>Hagège</last></author>
      <author><first>Denys</first><last>Proux</last></author>
      <author><first>Quentin</first><last>Gicquel</last></author>
      <author><first>Stéfan</first><last>Darmoni</last></author>
      <author><first>Suzanne</first><last>Pereira</last></author>
      <author><first>Frédérique</first><last>Segond</last></author>
      <author><first>Marie-Helène</first><last>Metzger</last></author>
      <pages>278–283</pages>
      <abstract>Cet article décrit la première version et les résultats de l’évaluation d’un système de détection des épisodes d’infections associées aux soins. Cette détection est basée sur l’analyse automatique de comptes-rendus d’hospitalisation provenant de différents hôpitaux et différents services. Ces comptes-rendus sont sous forme de texte libre. Le système de détection a été développé à partir d’un analyseur linguistique que nous avons adapté au domaine médical et extrait à partir des documents des indices pouvant conduire à une suspicion d’infection. Un traitement de la négation et un traitement temporel des textes sont effectués permettant de restreindre et de raffiner l’extraction d’indices. Nous décrivons dans cet article le système que nous avons développé et donnons les résultats d’une évaluation préliminaire.</abstract>
      <url hash="1ccd5a63">2011.jeptalnrecital-court.47</url>
      <language>fra</language>
      <bibkey>hagege-etal-2011-developpement</bibkey>
    </paper>
  </volume>
  <volume id="demonstration" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 18e conférence sur le Traitement Automatique des Langues Naturelles. Démonstrations</booktitle>
      <editor><first>Mathieu</first><last>Lafourcade</last></editor>
      <editor><first>Violaine</first><last>Prince</last></editor>
      <publisher>ATALA</publisher>
      <address>Montpellier, France</address>
      <month>June</month>
      <year>2011</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="e83deff6">2011.jeptalnrecital-demonstration.0</url>
      <bibkey>jep-taln-recital-2011-actes-de-la-18e</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>PLATON</fixed-case>, Plateforme d’apprentissage et d’enseignement de l’orthographe sur le Net (<fixed-case>PLATON</fixed-case>, Spelling learning and teaching platform on the net)</title>
      <author><first>Richard</first><last>Beaufort</last></author>
      <author><first>Sophie</first><last>Roekhaut</last></author>
      <pages>1–1</pages>
      <abstract/>
      <url hash="2dcf4d3b">2011.jeptalnrecital-demonstration.1</url>
      <language>fra</language>
      <bibkey>beaufort-roekhaut-2011-platon</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>S</fixed-case>pati<fixed-case>A</fixed-case>nn, un outil pour annoter l’utilisation de l’espace dans les corpus vidéo (<fixed-case>S</fixed-case>pati<fixed-case>A</fixed-case>nn, a tool for annotating the use of space in video corpora)</title>
      <author><first>Annelies</first><last>Braffort</last></author>
      <author><first>Laurence</first><last>Bolot</last></author>
      <pages>2–2</pages>
      <abstract/>
      <url hash="9430a7a3">2011.jeptalnrecital-demonstration.2</url>
      <language>fra</language>
      <bibkey>braffort-bolot-2011-spatiann</bibkey>
    </paper>
    <paper id="3">
      <title>Libellex : une plateforme multiservices pour la gestion des contenus multilingues (Libellex: a multi-service platform for managing multilingual content)</title>
      <author><first>François</first><last>Brown de Colstoun</last></author>
      <author><first>Estelle</first><last>Delpech</last></author>
      <author><first>Etienne</first><last>Monneret</last></author>
      <pages>3–3</pages>
      <abstract/>
      <url hash="8f44823d">2011.jeptalnrecital-demonstration.3</url>
      <language>fra</language>
      <bibkey>brown-de-colstoun-etal-2011-libellex</bibkey>
    </paper>
    <paper id="4">
      <title>Une application de la grammaire structurelle: L’analyseur syntaxique du français <fixed-case>SYGFRAN</fixed-case> (An application of structural grammar: the <fixed-case>SYGFRAN</fixed-case> syntactic analyser)</title>
      <author><first>Jacques</first><last>Chauché</last></author>
      <pages>4–4</pages>
      <abstract>La démonstration présentée produit une analyse syntaxique du français. Elle est écrite en SYGMART, fournie avec les actes, exécutable à l’adresse : http ://www.lirmm.fr/ chauche/ExempleAnl.html et téléchargeable à l’adresse : http ://www.sygtext.fr.</abstract>
      <url hash="aa7ebbf7">2011.jeptalnrecital-demonstration.4</url>
      <language>fra</language>
      <bibkey>chauche-2011-une</bibkey>
    </paper>
    <paper id="5">
      <title>Proxem Ubiq : une solution d’e-réputation par analyse de feedbacks clients (Proxem Ubiq: a solution of e-reputation by analyzing customer feedback)</title>
      <author><first>François-Régis</first><last>Chaumartin</last></author>
      <pages>5–5</pages>
      <abstract/>
      <url hash="a9a19c8d">2011.jeptalnrecital-demonstration.5</url>
      <language>fra</language>
      <bibkey>chaumartin-2011-proxem</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>TTC</fixed-case> <fixed-case>T</fixed-case>erm<fixed-case>S</fixed-case>uite : une chaîne de traitement pour la fouille terminologique multilingue (<fixed-case>TTC</fixed-case> <fixed-case>T</fixed-case>erm<fixed-case>S</fixed-case>uite: a processing chain for multilingual terminology mining)</title>
      <author><first>Béatrice</first><last>Daille</last></author>
      <author><first>Christine</first><last>Jacquin</last></author>
      <author><first>Laura</first><last>Monceaux</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Jérome</first><last>Rocheteau</last></author>
      <pages>6–6</pages>
      <abstract/>
      <url hash="aa31fcbc">2011.jeptalnrecital-demonstration.6</url>
      <language>fra</language>
      <bibkey>daille-etal-2011-ttc</bibkey>
    </paper>
    <paper id="7">
      <title>Une Suite d’interaction de fouille basée sur la compréhension du langage naturel (An Interaction Mining Suite Based On Natural Language Understanding)</title>
      <author><first>Rodolfo</first><last>Delmonte</last></author>
      <author><first>Vincenzo</first><last>Pallotta</last></author>
      <author><first>Violeta</first><last>Seretan</last></author>
      <author><first>Lammert</first><last>Vrieling</last></author>
      <author><first>David</first><last>Walker</last></author>
      <pages>7–7</pages>
      <abstract/>
      <url hash="1c9aaede">2011.jeptalnrecital-demonstration.7</url>
      <language>fra</language>
      <bibkey>delmonte-etal-2011-une</bibkey>
    </paper>
    <paper id="8">
      <title>Démonstration de l’<fixed-case>API</fixed-case> de <fixed-case>NLG</fixed-case>b<fixed-case>A</fixed-case>se (Demonstration of the <fixed-case>NLG</fixed-case>b<fixed-case>A</fixed-case>se <fixed-case>API</fixed-case>)</title>
      <author><first>François-Xavier</first><last>Desmarais</last></author>
      <author><first>Éric</first><last>Charton</last></author>
      <pages>8–8</pages>
      <abstract/>
      <url hash="690955ad">2011.jeptalnrecital-demonstration.8</url>
      <language>fra</language>
      <bibkey>desmarais-charton-2011-demonstration</bibkey>
    </paper>
    <paper id="9">
      <title>Système d’analyse de la polarité de dépêches financières (A polarity sentiment analysis system financial news items)</title>
      <author><first>Michel</first><last>Généreux</last></author>
      <pages>9–9</pages>
      <abstract>Nous présentons un système pour la classification en continu de dépêches financières selon une polarité positive ou négative. La démonstration permettra ainsi d’observer quelles sont les dépêches les plus à même de faire varier la valeur d’actions cotées en bourse, au moment même de la démonstration. Le système traitera de dépêches écrites en anglais et en français.</abstract>
      <url hash="8b7749b4">2011.jeptalnrecital-demonstration.9</url>
      <language>fra</language>
      <bibkey>genereux-2011-systeme</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>R</fixed-case>ef<fixed-case>G</fixed-case>en, outil d’identification automatique des chaînes de référence en français (<fixed-case>R</fixed-case>ef<fixed-case>G</fixed-case>en, an automatic identification tool of reference chains in <fixed-case>F</fixed-case>rench)</title>
      <author><first>Laurence</first><last>Longo</last></author>
      <author><first>Amalia</first><last>Todirascu</last></author>
      <pages>10–10</pages>
      <abstract/>
      <url hash="c9daf65b">2011.jeptalnrecital-demonstration.10</url>
      <language>fra</language>
      <bibkey>longo-todirascu-2011-refgen</bibkey>
    </paper>
    <paper id="11">
      <title>Babouk – exploration orientée du web pour la constitution de corpus et de terminologies (Babouk – oriented exploration of the web for the construction of corpora and terminologies)</title>
      <author><first>Clément</first><last>de Groc</last></author>
      <author><first>Javier</first><last>Couto</last></author>
      <author><first>Helena</first><last>Blancafort</last></author>
      <author><first>Claude</first><last>de Loupy</last></author>
      <pages>11–11</pages>
      <abstract/>
      <url hash="a5d0ece2">2011.jeptalnrecital-demonstration.11</url>
      <language>fra</language>
      <bibkey>de-groc-etal-2011-babouk</bibkey>
    </paper>
    <paper id="12">
      <title>Extraction d’informations médicales au <fixed-case>LIMSI</fixed-case> (Medical information extraction at <fixed-case>LIMSI</fixed-case>)</title>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Louise</first><last>Deléger</last></author>
      <author><first>Anne-Lyse</first><last>Minard</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Asma</first><last>Ben Abacha</last></author>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>12–12</pages>
      <abstract/>
      <url hash="3959ab46">2011.jeptalnrecital-demonstration.12</url>
      <language>fra</language>
      <bibkey>grouin-etal-2011-extraction</bibkey>
    </paper>
    <paper id="13">
      <title>Système d’analyse catégorielle <fixed-case>ACCG</fixed-case> : adéquation au traitement de problèmes syntaxiques complexes (<fixed-case>ACCG</fixed-case> categorical analysis system: adequacy to the treatment of complex syntactic problems)</title>
      <author><first>Juyeon</first><last>Kang</last></author>
      <author><first>Jean-Pierre</first><last>Desclés</last></author>
      <pages>13–13</pages>
      <abstract/>
      <url hash="93dc0939">2011.jeptalnrecital-demonstration.13</url>
      <language>fra</language>
      <bibkey>kang-descles-2011-systeme</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>LOL</fixed-case> : Langage objet dédié à la programmation linguistique (<fixed-case>LOL</fixed-case>: Object-oriented language dedicated to linguistic programming)</title>
      <author><first>Jimmy</first><last>Ma</last></author>
      <author><first>Mickaël</first><last>Mounier</last></author>
      <author><first>Helena</first><last>Blancafort</last></author>
      <author><first>Javier</first><last>Couto</last></author>
      <author><first>Claude</first><last>de Loupy</last></author>
      <pages>14–14</pages>
      <abstract/>
      <url hash="9a09b0c6">2011.jeptalnrecital-demonstration.14</url>
      <language>fra</language>
      <bibkey>ma-etal-2011-lol</bibkey>
    </paper>
    <paper id="15">
      <title>Aligner : un outil d’alignement et de mesure d’accord inter-annotateurs (Aligner: a tool for aligning and measuring inter-annotator agreement)</title>
      <author><first>Yann</first><last>Mathet</last></author>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <pages>15–15</pages>
      <abstract/>
      <url hash="e221c6b1">2011.jeptalnrecital-demonstration.15</url>
      <language>fra</language>
      <bibkey>mathet-widlocher-2011-aligner</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>G</fixed-case>lozz<fixed-case>QL</fixed-case> : un langage de requêtes incrémental pour les textes annotés (<fixed-case>G</fixed-case>lozz<fixed-case>QL</fixed-case>: an incremental query language for annotated texts)</title>
      <author><first>Yann</first><last>Mathet</last></author>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <pages>16–16</pages>
      <abstract/>
      <url hash="7ca652eb">2011.jeptalnrecital-demonstration.16</url>
      <language>fra</language>
      <bibkey>mathet-widlocher-2011-glozzql</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>EASYTEXT</fixed-case> : un système opérationnel de génération de textes (<fixed-case>EASYTEXT</fixed-case>: an operational system for text generation)</title>
      <author><first>Frédéric</first><last>Meunier</last></author>
      <author><first>Laurence</first><last>Danlos</last></author>
      <author><first>Vanessa</first><last>Combet</last></author>
      <pages>17–17</pages>
      <abstract/>
      <url hash="6ef6be8d">2011.jeptalnrecital-demonstration.17</url>
      <language>fra</language>
      <bibkey>meunier-etal-2011-easytext</bibkey>
    </paper>
    <paper id="18">
      <title>Restad : un logiciel d’indexation et de stockage relationnel de contenus <fixed-case>XML</fixed-case> (Restad: an indexing and relational storing software for <fixed-case>XML</fixed-case> content)</title>
      <author><first>Yoann</first><last>Moreau</last></author>
      <author><first>Eric</first><last>SanJuan</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <pages>18–18</pages>
      <abstract/>
      <url hash="22d9fa6b">2011.jeptalnrecital-demonstration.18</url>
      <language>fra</language>
      <bibkey>moreau-etal-2011-restad</bibkey>
    </paper>
    <paper id="19">
      <title>Une chaîne d’analyse des e-mails pour l’aide à la gestion de sa messagerie (An analysis chain for helping managing emails)</title>
      <author><first>Gaëlle</first><last>Recourcé</last></author>
      <pages>19–19</pages>
      <abstract/>
      <url hash="cde00619">2011.jeptalnrecital-demonstration.19</url>
      <language>fra</language>
      <bibkey>recource-2011-une</bibkey>
    </paper>
    <paper id="20">
      <title>Démonstration d’un outil de « Calcul Littéraire » (Demonstration of the tool « Calcul Littéraire »)</title>
      <author><first>Jean</first><last>Rohmer</last></author>
      <pages>20–20</pages>
      <abstract/>
      <url hash="2ab02753">2011.jeptalnrecital-demonstration.20</url>
      <language>fra</language>
      <bibkey>rohmer-2011-demonstration</bibkey>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 18e conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Cédric</first><last>Lopez</last></editor>
      <publisher>ATALA</publisher>
      <address>Montpellier, France</address>
      <month>June</month>
      <year>2011</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="604d6e83">2011.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2011-actes-de-la-18e-sur</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Analyse de l’ambiguïté des requêtes utilisateurs par catégorisation thématique</title>
      <author><first>Fanny</first><last>Lalleman</last></author>
      <pages>1–11</pages>
      <abstract>Dans cet article, nous cherchons à identifier la nature de l’ambiguïté des requêtes utilisateurs issues d’un moteur de recherche dédié à l’actualité, 2424actu.fr, en utilisant une tâche de catégorisation. Dans un premier temps, nous verrons les différentes formes de l’ambiguïté des requêtes déjà décrites dans les travaux de TAL. Nous confrontons la vision lexicographique de l’ambiguïté à celle décrite par les techniques de classification appliquées à la recherche d’information. Dans un deuxième temps, nous appliquons une méthode de catégorisation thématique afin d’explorer l’ambiguïté des requêtes, celle-ci nous permet de conduire une analyse sémantique de ces requêtes, en intégrant la dimension temporelle propre au contexte des news. Nous proposons une typologie des phénomènes d’ambiguïté basée sur notre analyse sémantique. Enfin, nous comparons l’exploration par catégorisation à une ressource comme Wikipédia, montrant concrètement les divergences des deux approches.</abstract>
      <url hash="178eb4ac">2011.jeptalnrecital-recital.1</url>
      <language>fra</language>
      <bibkey>lalleman-2011-analyse</bibkey>
    </paper>
    <paper id="2">
      <title>Extraction Automatique d’Informations Pédagogiques Pertinentes à partir de Documents Textuels</title>
      <author><first>Boutheina</first><last>Smine</last></author>
      <author><first>Rim</first><last>Faiz</last></author>
      <author><first>Jean-Pierre</first><last>Desclés</last></author>
      <pages>12–23</pages>
      <abstract>Plusieurs utilisateurs ont souvent besoin d’informations pédagogiques pour les intégrer dans leurs ressources pédagogiques, ou pour les utiliser dans un processus d’apprentissage. Une indexation de ces informations s’avère donc utile en vue d’une extraction des informations pédagogiques pertinentes en réponse à une requête utilisateur. La plupart des systèmes d’extraction d’informations pédagogiques existants proposent une indexation basée sur une annotation manuelle ou semi-automatique des informations pédagogiques, tâche qui n’est pas préférée par les utilisateurs. Dans cet article, nous proposons une approche d’indexation d’objets pédagogiques (Définition, Exemple, Exercice, etc.) basée sur une annotation sémantique par Exploration Contextuelle des documents. L’index généré servira à une extraction des objets pertinents répondant à une requête utilisateur sémantique. Nous procédons, ensuite, à un classement des objets extraits selon leur pertinence en utilisant l’algorithme Rocchio. Notre objectif est de mettre en valeur une indexation à partir de contextes sémantiques et non pas à partir de seuls termes linguistiques.</abstract>
      <url hash="c3a17197">2011.jeptalnrecital-recital.2</url>
      <language>fra</language>
      <bibkey>smine-etal-2011-extraction</bibkey>
    </paper>
    <paper id="3">
      <title>Des outils de <fixed-case>TAL</fixed-case> en support aux experts de sûreté industrielle pour l’exploitation de bases de données de retour d’expérience</title>
      <author><first>Nikola</first><last>Tulechki</last></author>
      <pages>24–35</pages>
      <abstract>Cet article présente des applications d’outils et méthodes du traitement automatique des langues (TAL) à la maîtrise du risque industriel grâce à l’analyse de données textuelles issues de volumineuses bases de retour d’expérience (REX). Il explicite d’abord le domaine de la gestion de la sûreté, ses aspects politiques et sociaux ainsi que l’activité des experts en sûreté et les besoins qu’ils expriment. Dans un deuxième temps il présente une série de techniques, comme la classification automatique de documents, le repérage de subjectivité, et le clustering, adaptées aux données REX visant à répondre à ces besoins présents et à venir, sous forme d’outils, en support à l’activité des experts.</abstract>
      <url hash="3c3e0366">2011.jeptalnrecital-recital.3</url>
      <language>fra</language>
      <bibkey>tulechki-2011-des</bibkey>
    </paper>
    <paper id="4">
      <title>Vers une algèbre des relations de discours pour la comparaison de structures discursives</title>
      <author><first>Charlotte</first><last>Roze</last></author>
      <pages>36–47</pages>
      <abstract>Nous proposons une méthodologie pour la construction de règles de déduction de relations de discours, destinées à être intégrées dans une algèbre de ces relations. La construction de ces règles a comme principal objectif de pouvoir calculer la fermeture discursive d’une structure de discours, c’est-à-dire de déduire toutes les relations que la structure contient implicitement. Calculer la fermeture des structures discursives peut permettre d’améliorer leur comparaison, notamment dans le cadre de l’évaluation de systèmes d’analyse automatique du discours. Nous présentons la méthodologie adoptée, que nous illustrons par l’étude d’une règle de déduction.</abstract>
      <url hash="7b2e664b">2011.jeptalnrecital-recital.4</url>
      <language>fra</language>
      <bibkey>roze-2011-vers</bibkey>
    </paper>
    <paper id="5">
      <title>Alignment of Monolingual Corpus by Reduction of the Search Space</title>
      <author><first>Prajol</first><last>Shrestha</last></author>
      <pages>48–56</pages>
      <abstract>Monolingual comparable corpora annotated with alignments between text segments (paragraphs, sentences, etc.) based on similarity are used in a wide range of natural language processing applications like plagiarism detection, information retrieval, summarization and so on. The drawback wanting to use them is that there aren’t many standard corpora which are aligned. Due to this drawback, the corpus is manually created, which is a time consuming and costly task. In this paper, we propose a method to significantly reduce the search space for manual alignment of the monolingual comparable corpus which in turn makes the alignment process faster and easier. This method can be used in making alignments on different levels of text segments. Using this method we create our own gold corpus aligned on the level of paragraph, which will be used for testing and building our algorithms for automatic alignment. We also present some experiments for the reduction of search space on the basis of stem overlap, word overlap, and cosine similarity measure which help us automatize the process to some extent and reduce human effort for alignment.</abstract>
      <url hash="d4f5ffc9">2011.jeptalnrecital-recital.5</url>
      <bibkey>shrestha-2011-alignment</bibkey>
    </paper>
  </volume>
  <volume id="recitalcourt" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 18e conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues (articles courts)</booktitle>
      <editor><first>Cédric</first><last>Lopez</last></editor>
      <publisher>ATALA</publisher>
      <address>Montpellier, France</address>
      <month>June</month>
      <year>2011</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="5d741ab9">2011.jeptalnrecital-recitalcourt.0</url>
      <bibkey>jep-taln-recital-2011-actes-de-la-18e-sur-le</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Corpus-Based methods for Short Text Similarity</title>
      <author><first>Prajol</first><last>Shrestha</last></author>
      <pages>1–6</pages>
      <abstract>This paper presents corpus-based methods to find similarity between short text (sentences, paragraphs, ...) which has many applications in the field of NLP. Previous works on this problem have been based on supervised methods or have used external resources such as WordNet, British National Corpus etc. Our methods are focused on unsupervised corpus-based methods. We present a new method, based on Vector Space Model, to capture the contextual behavior, senses and correlation, of terms and show that this method performs better than the baseline method that uses vector based cosine similarity measure. The performance of existing document similarity measures, Dice and Resemblance, are also evaluated which in our knowledge have not been used for short text similarity. We also show that the performance of the vector-based baseline method is improved when using stems instead of words and using the candidate sentences for computing the parameters rather than some external resource.</abstract>
      <url hash="b0594767">2011.jeptalnrecital-recitalcourt.1</url>
      <bibkey>shrestha-2011-corpus</bibkey>
    </paper>
    <paper id="2">
      <title>Ressources lexicales au service de recherche et d’indexation des images</title>
      <author><first>Inga</first><last>Gheorghita</last></author>
      <pages>7–14</pages>
      <abstract>Cet article présente une méthodologie d’utilisation du Trésor de la Langue Française informatisée (TLFi) pour l’indexation et la recherche des images fondée sur l’annotation textuelle. Nous utilisons les définitions du TLFi pour la création automatique et l’enrichissement d’un thésaurus à partir des mots-clés de la requête de recherche et des mots-clés attribués à l’image lors de l’indexation. Plus précisement il s’agit d’associer, de façon automatisé, à chaque mot-clé de l’image une liste des mots extraits de ses définitions TLFi pour un domaine donné, en construisant ainsi un arbre hiérarchique. L’approche proposée permet une catégorisation très précise des images, selon les domaines, une indexation de grandes quantités d’images et une recherche rapide.</abstract>
      <url hash="41e9a3c8">2011.jeptalnrecital-recitalcourt.2</url>
      <language>fra</language>
      <bibkey>gheorghita-2011-ressources</bibkey>
    </paper>
    <paper id="3">
      <title>Repérer les phrases évaluatives dans les articles de presse à partir d’indices et de stéréotypes d’écriture</title>
      <author><first>Mathias</first><last>Lambert</last></author>
      <pages>15–20</pages>
      <abstract>Ce papier présente une méthode de recherche des phrases évaluatives dans les articles de presse économique et financière à partir de marques et d’indices stéréotypés, propres au style journalistique, apparaissant de manière concomitante à l’expression d’évaluation(s) dans les phrases. Ces marques et indices ont été dégagés par le biais d’une annotation manuelle. Ils ont ensuite été implémentés, en vue d’une phase-test d’annotation automatique, sous forme de grammaires DCG/GULP permettant, par filtrage, de matcher les phrases les contenant. Les résultats de notre première tentative d’annotation automatique sont présentés dans cet article. Enfin les perspectives offertes par cette méthode relativement peu coûteuse en ressources (à base d’indices non intrinsèquement évaluatifs) font l’objet d’une discussion.</abstract>
      <url hash="676b5063">2011.jeptalnrecital-recitalcourt.3</url>
      <language>fra</language>
      <bibkey>lambert-2011-reperer</bibkey>
    </paper>
    <paper id="4">
      <title>La complexité linguistique Méthode d’analyse</title>
      <author><first>Adrien</first><last>Barbaresi</last></author>
      <pages>21–26</pages>
      <abstract>La complexité linguistique regroupe différents phénomènes dont il s’agit de modéliser le rapport. Le travail en cours que je décris ici propose une réflexion sur les approches linguistiques et techniques de cette notion et la mise en application d’un balayage des textes qui s’efforce de contribuer à leur enrichissement. Ce traitement en surface effectué suivant une liste de critères qui représentent parfois des approximations de logiques plus élaborées tente de fournir une image “raisonnable” de la complexité.</abstract>
      <url hash="70686ee7">2011.jeptalnrecital-recitalcourt.4</url>
      <language>fra</language>
      <bibkey>barbaresi-2011-la</bibkey>
    </paper>
  </volume>
</collection>
