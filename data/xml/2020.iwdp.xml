<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.iwdp">
  <volume id="1" ingest-date="2020-12-02">
    <meta>
      <booktitle>Proceedings of the Second International Workshop of Discourse Processing</booktitle>
      <editor><first>Qun</first><last>Liu</last></editor>
      <editor><first>Deyi</first><last>Xiong</last></editor>
      <editor><first>Shili</first><last>Ge</last></editor>
      <editor><first>Xiaojun</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
      <venue>iwdp</venue>
    </meta>
    <frontmatter>
      <url hash="64d9f206">2020.iwdp-1.0</url>
      <bibkey>iwdp-2020-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Research on Discourse Parsing: from the Dependency View</title>
      <author><first>Sujian</first><last>Li</last></author>
      <pages>1–3</pages>
      <abstract>Discourse parsing aims to comprehensively acquire the logical structure of the whole text which may be helpful to some downstream applications such as summarization, reading comprehension, QA and so on. One important issue behind discourse parsing is the representation of discourse structure. Up to now, many discourse structures have been proposed, and the correponding parsing methods are designed, promoting the development of discourse research. In this paper, we mainly introduce our recent discourse research and its preliminary application from the dependency view.</abstract>
      <url hash="c64faad8">2020.iwdp-1.1</url>
      <bibkey>li-2020-research</bibkey>
    </paper>
    <paper id="2">
      <title>A Review of Discourse-level Machine Translation</title>
      <author><first>Xiaojun</first><last>Zhang</last></author>
      <pages>4–12</pages>
      <abstract>Machine translation (MT) models usually translate a text at sentence level by considering isolated sentences, which is based on a strict assumption that the sentences in a text are independent of one another. However, the fact is that the texts at discourse level have properties going beyond individual sentences. These properties reveal texts in the frequency and distribution of words, word senses, referential forms and syntactic structures. Dissregarding dependencies across sentences will harm translation quality especially in terms of coherence, cohesion, and consistency. To solve these problems, several approaches have previously been investigated for conventional statistical machine translation (SMT). With the fast growth of neural machine translation (NMT), discourse-level NMT has drawn increasing attention from researchers. In this work, we review major works on addressing discourse related problems for both SMT and NMT models with a survey of recent trends in the fields.</abstract>
      <url hash="0b0dc392">2020.iwdp-1.2</url>
      <bibkey>zhang-2020-review</bibkey>
    </paper>
    <paper id="3">
      <title>A Test Suite for Evaluating Discourse Phenomena in Document-level Neural Machine Translation</title>
      <author><first>Xinyi</first><last>Cai</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>13–17</pages>
      <abstract>The need to evaluate the ability of context-aware neural machine translation (NMT) models in dealing with specific discourse phenomena arises in document-level NMT. However, test sets that satisfy this need are rare. In this paper, we propose a test suite to evaluate three common discourse phenomena in English-Chinese translation: pronoun, discourse connective and ellipsis where discourse divergences lie across the two languages. The test suite contains 1,200 instances, 400 for each type of discourse phenomena. We perform both automatic and human evaluation with three state-of-the-art context-aware NMT models on the proposed test suite. Results suggest that our test suite can be used as a challenging benchmark test bed for evaluating document-level NMT. The test suite will be publicly available soon.</abstract>
      <url hash="a99692c6">2020.iwdp-1.3</url>
      <bibkey>cai-xiong-2020-test</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="4">
      <title>Comparison of the effects of attention mechanism on translation tasks of different lengths of ambiguous words</title>
      <author><first>Yue</first><last>Hu</last></author>
      <author><first>Jiahao</first><last>Qin</last></author>
      <author><first>Zemeiqi</first><last>Chen</last></author>
      <author><first>Jingshi</first><last>Zhou</last></author>
      <author><first>Xiaojun</first><last>Zhang</last></author>
      <pages>18–21</pages>
      <abstract>In recent years, attention mechanism has been widely used in various neural machine translation tasks based on encoder decoder. This paper focuses on the performance of encoder decoder attention mechanism in word sense disambiguation task with different text length, trying to find out the influence of context marker on attention mechanism in word sense disambiguation task. We hypothesize that attention mechanisms have similar performance when translating texts of different lengths. Our conclusion is that the alignment effect of attention mechanism is magnified in short text translation tasks with ambiguous nouns, while the effect of attention mechanism is far less than expected in long-text tasks, which means that attention mechanism is not the main mechanism for NMT model to feed WSD to integrate context information. This may mean that attention mechanism pays more attention to ambiguous nouns than context markers. The experimental results show that with the increase of text length, the performance of NMT model using attention mechanism will gradually decline.</abstract>
      <url hash="5535965f">2020.iwdp-1.4</url>
      <bibkey>hu-etal-2020-comparison</bibkey>
    </paper>
    <paper id="5">
      <title>Context-Aware Word Segmentation for <fixed-case>C</fixed-case>hinese Real-World Discourse</title>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Junpeng</first><last>Liu</last></author>
      <author><first>Jingxiang</first><last>Cao</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <pages>22–28</pages>
      <abstract>Previous neural approaches achieve significant progress for Chinese word segmentation (CWS) as a sentence-level task, but it suffers from limitations on real-world scenario. In this paper, we address this issue with a context-aware method and optimize the solution at document-level. This paper proposes a three-step strategy to improve the performance for discourse CWS. First, the method utilizes an auxiliary segmenter to remedy the limitation on pre-segmenter. Then the context-aware algorithm computes the confidence of each split. The maximum probability path is reconstructed via this algorithm. Besides, in order to evaluate the performance in discourse, we build a new benchmark consisting of the latest news and Chinese medical articles. Extensive experiments on this benchmark show that our proposed method achieves a competitive performance on a document-level real-world scenario for CWS.</abstract>
      <url hash="061c4146">2020.iwdp-1.5</url>
      <attachment type="Dataset" hash="ba4de41c">2020.iwdp-1.5.Dataset.rar</attachment>
      <bibkey>huang-etal-2020-context</bibkey>
    </paper>
    <paper id="6">
      <title>Neural Abstractive Multi-Document Summarization: Hierarchical or Flat Structure?</title>
      <author><first>Ye</first><last>Ma</last></author>
      <author><first>Lu</first><last>Zong</last></author>
      <pages>29–37</pages>
      <abstract>With regards to WikiSum (CITATION) that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops two hierarchical Transformers (HT) that describe both the cross-token and cross-document dependencies, at the same time allow extended length of input documents. By incorporating word- and paragraph-level multi-head attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT &amp;VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively. A comprehensive evaluation is conducted on WikiSum to compare PHT &amp;VHT with established models and to answer the question whether hierarchical structures offer more promising performances than flat structures in the MDS task. The results suggest that our hierarchical models generate summaries of higher quality by better capturing cross-document relationships, and save more memory spaces in comparison to flat-structure models. Moreover, we recommend PHT given its practical value of higher inference speed and greater memory-saving capacity.</abstract>
      <url hash="0b7d8d4d">2020.iwdp-1.6</url>
      <bibkey>ma-zong-2020-neural</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
    </paper>
    <paper id="7">
      <title>Intent Segmentation of User Queries Via Discourse Parsing</title>
      <author><first>Vicente Ivan</first><last>Sanchez Carmona</last></author>
      <author><first>Yibing</first><last>Yang</last></author>
      <author><first>Ziyue</first><last>Wen</last></author>
      <author><first>Ruosen</first><last>Li</last></author>
      <author><first>Xiaohua</first><last>Wang</last></author>
      <author><first>Changjian</first><last>Hu</last></author>
      <pages>38–47</pages>
      <abstract>In this paper, we explore a new approach based on discourse analysis for the task of intent segmentation. Our target texts are user queries from a real-world chatbot. Our results show the feasibility of our approach with an F1-score of 82.97 points, and some advantages and disadvantages compared to two machine learning baselines: BERT and LSTM+CRF.</abstract>
      <url hash="e9ad8719">2020.iwdp-1.7</url>
      <bibkey>sanchez-carmona-etal-2020-intent</bibkey>
    </paper>
    <paper id="8">
      <title>Bridging Question Answering and Discourse The case of Multi-Sentence Questions</title>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>48</pages>
      <abstract>In human question-answering (QA), questions are often expressed in the form of multiple sentences. One can see this in both spoken QA interactions, when one person asks a question of another, and written QA, such as are found on-line in FAQs and in what are called ”Community Question-Answering Forums”. Computer-based QA has taken the challenge of these ”multi-sentence questions” to be that of breaking them into an appropriately ordered sequence of separate questions, with both the previous questions and their answers serving as context for the next question. This can be seen, for example, in two recent workshops at AAAI called ”Reasoning for Complex QA” [https://rcqa-ws.github.io/program/]. We claim that, while appropriate for some types of ”multi-sentence questions” (MSQs), it is not appropriate for all, because they are essentially different types of discourse. To support this claim, we need to provide evidence that: • different types of MSQs are answered differently in written or spoken QA between people; • people can (and do) distinguish these different types of MSQs; • systems can be made to both distinguish different types of MSQs and provide appropriate answers.</abstract>
      <url hash="0c0d8441">2020.iwdp-1.8</url>
      <bibkey>webber-2020-bridging</bibkey>
    </paper>
    <paper id="9">
      <title>Component Sharing in <fixed-case>E</fixed-case>nglish and <fixed-case>C</fixed-case>hinese Clause Complex</title>
      <author><first>Shili</first><last>Ge</last></author>
      <author><first>Xiaoping</first><last>Lin</last></author>
      <author><first>Rou</first><last>Song</last></author>
      <pages>49–53</pages>
      <abstract>NT Clause Complex Framework defines a clause complex as a combination of NT clauses through component sharing and logic-semantic relationship. This paper clarifies the existence of component sharing mechanism in both English and Chinese clause complexes, illustrates the differences in component sharing between the two languages, and introduces a formal annotation scheme to represent clause-complex level structural transformations. Under the guidance of the annotation scheme, the English-Chinese Clause Alignment Corpus is built. It is believed that this corpus will aid comparative linguistic studies, translation studies and machine translation studies by providing abundant formal and computable samples for English-Chinese structural transformations on the clause complex level.</abstract>
      <url hash="fdc38338">2020.iwdp-1.9</url>
      <bibkey>ge-etal-2020-component</bibkey>
    </paper>
    <paper id="10">
      <title>Referential Cohesion A Challenge for Machine Translation Evaluation</title>
      <author><first>Christian</first><last>Hardmeier</last></author>
      <pages>54</pages>
      <abstract>Connected texts are characterised by the presence of linguistic elements relating to shared referents throughout the text. These elements together form a structure that lends cohesion to the text. The realisation of those cohesive structures is subject to different constraints and varying preferences in different languages. We regularly observe mismatches of cohesive structures across languages in parallel texts. This can be a result of either a divergence of language-internal constraints or of effects of the translation process. As fully automatic high-quality MT is starting to look achievable, the question arises how cohesive elements should be handled in MT evaluation, since the common assumption of 1:1 correspondence between referring expressions is a poor match for what we find in corpus data. Focusing on the translation of pronouns, I discuss different approaches to evaluating a particular type of cohesive elements in MT output and the trade-offs they make between evaluation cost, validity, specificity and coverage. I suggest that a meaningful evaluation of cohesive structures in translation is difficult to achieve simply by appealing to the intuition of human annotators, but requires a more structured approach that forces us to make up our minds about the standards we expect the translation output to adhere to.</abstract>
      <url hash="45fc2672">2020.iwdp-1.10</url>
      <bibkey>hardmeier-2020-referential</bibkey>
    </paper>
  </volume>
</collection>
