<?xml version='1.0' encoding='UTF-8'?>
<collection id="2018.lilt">
  <volume id="16">
    <meta>
      <booktitle>Linguistic Issues in Language Technology, Volume 16, 2018</booktitle>
      <publisher>CSLI Publications</publisher>
      <year>2018</year>
      <month>July</month>
      <venue>lilt</venue>
    </meta>
    <paper id="1">
      <title>Can Recurrent Neural Networks Learn Nested Recursion?</title>
      <author><first>Jean-Phillipe</first><last>Bernardy</last></author>
      <abstract>Context-free grammars (CFG) were one of the first formal tools used to model natural languages, and they remain relevant today as the basis of several frameworks. A key ingredient of CFG is the presence of nested recursion. In this paper, we investigate experimentally the capability of several recurrent neural networks (RNNs) to learn nested recursion. More precisely, we measure an upper bound of their capability to do so, by simplifying the task to learning a generalized Dyck language, namely one composed of matching parentheses of various kinds. To do so, we present the RNNs with a set of random strings having a given maximum nesting depth and test its ability to predict the kind of closing parenthesis when facing deeper nested strings. We report mixed results: when generalizing to deeper nesting levels, the accuracy of standard RNNs is significantly higher than random, but still far from perfect. Additionally, we propose some non-standard stack-based models which can approach perfect accuracy, at the cost of robustness.</abstract>
      <issue>1</issue>
      <url hash="a5d8242e">2018.lilt-16.1</url>
      <bibkey>bernardy-2018-recurrent</bibkey>
    </paper>
  </volume>
</collection>
