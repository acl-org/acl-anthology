<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.autosimtrans">
  <volume id="1" ingest-date="2022-06-29">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Automatic Simultaneous Translation</booktitle>
      <editor><first>Julia</first><last>Ive</last></editor>
      <editor><first>Ruiqing</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2022</year>
      <url hash="eb34e6f4">2022.autosimtrans-1</url>
      <venue>autosimtrans</venue>
    </meta>
    <frontmatter>
      <url hash="85266f0a">2022.autosimtrans-1.0</url>
      <bibkey>autosimtrans-2022-automatic</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the Third Workshop on Automatic Simultaneous Translation</title>
      <author><first>Ruiqing</first><last>Zhang</last></author>
      <author><first>Chuanqiang</first><last>Zhang</last></author>
      <author><first>Zhongjun</first><last>He</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Liang</first><last>Huang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Wolfgang</first><last>Macherey</last></author>
      <pages>1-11</pages>
      <abstract>This paper reports the results of the shared task we hosted on the Third Workshop of Automatic Simultaneous Translation (AutoSimTrans). The shared task aims to promote the development of text-to-text and speech-to-text simultaneous translation, and includes Chinese-English and English-Spanish tracks. The number of systems submitted this year has increased fourfold compared with last year. Additionally, the top 1 ranked system in the speech-to-text track is the first end-to-end submission we have received in the past three years, which has shown great potential. This paper reports the results and descriptions of the 14 participating teams, compares different evaluation metrics, and revisits the ranking method.</abstract>
      <url hash="3926445e">2022.autosimtrans-1.1</url>
      <bibkey>zhang-etal-2022-findings</bibkey>
      <doi>10.18653/v1/2022.autosimtrans-1.1</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/aishell-1">AISHELL-1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bstc">BSTC</pwcdataset>
    </paper>
    <paper id="2">
      <title>Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for Simultaneous Speech Translation</title>
      <author><first>Sara</first><last>Papi</last></author>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>12-17</pages>
      <abstract>Simultaneous speech translation (SimulST) systems aim at generating their output with the lowest possible latency, which is normally computed in terms of Average Lagging (AL). In this paper we highlight that, despite its widespread adoption, AL provides underestimated scores for systems that generate longer predictions compared to the corresponding references. We also show that this problem has practical relevance, as recent SimulST systems have indeed a tendency to over-generate. As a solution, we propose LAAL (Length-Adaptive Average Lagging), a modified version of the metric that takes into account the over-generation phenomenon and allows for unbiased evaluation of both under-/over-generating systems.</abstract>
      <url hash="2210de52">2022.autosimtrans-1.2</url>
      <bibkey>papi-etal-2022-generation</bibkey>
      <doi>10.18653/v1/2022.autosimtrans-1.2</doi>
      <pwccode url="https://github.com/hlt-mt/fbk-fairseq" additional="false">hlt-mt/fbk-fairseq</pwccode>
    </paper>
    <paper id="3">
      <title>System Description on Automatic Simultaneous Translation Workshop</title>
      <author><first>Zecheng</first><last>Li</last></author>
      <author><first>Yue</first><last>Sun</last></author>
      <author><first>Haoze</first><last>Li</last></author>
      <pages>18-21</pages>
      <abstract>This paper describes our system submitted on the third automatic simultaneous translation workshop at NAACL2022. We participate in the Chinese audio-&gt;English text direction of Chinese-to-English translation. Our speech-to-text system is a pipeline system, in which we resort to rhymological features for audio split, ASRT model for speech recoginition, STACL model for streaming text translation. To translate streaming text, we use wait-k policy trained to generate the target sentence concurrently with the source sentence, but always k words behind. We propose a competitive simultaneous translation system and rank 3rd in the audio input track. The code will release soon.</abstract>
      <url hash="9101567a">2022.autosimtrans-1.3</url>
      <bibkey>li-etal-2022-system</bibkey>
      <doi>10.18653/v1/2022.autosimtrans-1.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/aishell-1">AISHELL-1</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/thchs-30">THCHS-30</pwcdataset>
    </paper>
    <paper id="4">
      <title>System Description on Third Automatic Simultaneous Translation Workshop</title>
      <author><first>Zhang</first><last>Yiqiao</last></author>
      <pages>22-24</pages>
      <abstract>This paper shows my submission to the Third Automatic Simultaneous Translation Workshop at NAACL2022.The submission includes Chinese audio to English text task, Chinese text to English text tast, and English text to Spanish text task.For the two text-to-text tasks, I use the STACL model of PaddleNLP. As for the audio-to-text task, I first use DeepSpeech2 to translate the audio into text, then apply the STACL model to handle the text-to-text task.The submission results show that the used method can get low delay with a few training samples.</abstract>
      <url hash="261361ac">2022.autosimtrans-1.4</url>
      <bibkey>yiqiao-2022-system</bibkey>
      <doi>10.18653/v1/2022.autosimtrans-1.4</doi>
    </paper>
    <paper id="5">
      <title>End-to-End Simultaneous Speech Translation with Pretraining and Distillation: Huawei <fixed-case>N</fixed-case>oah’s System for <fixed-case>A</fixed-case>uto<fixed-case>S</fixed-case>im<fixed-case>T</fixed-case>ran<fixed-case>S</fixed-case> 2022</title>
      <author><first>Xingshan</first><last>Zeng</last></author>
      <author><first>Pengfei</first><last>Li</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>25-33</pages>
      <abstract>This paper describes the system submitted to AutoSimTrans 2022 from Huawei Noah’s Ark Lab, which won the first place in the audio input track of the Chinese-English translation task. Our system is based on RealTranS, an end-to-end simultaneous speech translation model. We enhance the model with pretraining, by initializing the acoustic encoder with ASR encoder, and the semantic encoder and decoder with NMT encoder and decoder, respectively. To relieve the data scarcity, we further construct pseudo training corpus as a kind of knowledge distillation with ASR data and the pretrained NMT model. Meanwhile, we also apply several techniques to improve the robustness and domain generalizability, including punctuation removal, token-level knowledge distillation and multi-domain finetuning. Experiments show that our system significantly outperforms the baselines at all latency and also verify the effectiveness of our proposed methods.</abstract>
      <url hash="8bcdac35">2022.autosimtrans-1.5</url>
      <bibkey>zeng-etal-2022-end</bibkey>
      <doi>10.18653/v1/2022.autosimtrans-1.5</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/bstc">BSTC</pwcdataset>
    </paper>
    <paper id="6">
      <title><fixed-case>BIT</fixed-case>-Xiaomi’s System for <fixed-case>A</fixed-case>uto<fixed-case>S</fixed-case>im<fixed-case>T</fixed-case>rans 2022</title>
      <author><first>Mengge</first><last>Liu</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Bao</first><last>Chen</last></author>
      <author><first>Yanzhi</first><last>Tian</last></author>
      <author><first>Tianwei</first><last>Lan</last></author>
      <author><first>Silin</first><last>Li</last></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <author><first>Jian</first><last>Luan</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <pages>34-42</pages>
      <abstract>This system paper describes the BIT-Xiaomi simultaneous translation system for Autosimtrans 2022 simultaneous translation challenge. We participated in three tracks: the Zh-En text-to-text track, the Zh-En audio-to-text track and the En-Es test-to-text track. In our system, wait-k is employed to train prefix-to-prefix translation models. We integrate streaming chunking to detect boundaries as the source streaming read in. We further improve our system with data selection, data-augmentation and R-drop training methods. Results show that our wait-k implementation outperforms organizer’s baseline by 8 BLEU score at most, and our proposed streaming chunking method further improves about 2 BLEU in low latency regime.</abstract>
      <url hash="04afefe1">2022.autosimtrans-1.6</url>
      <bibkey>liu-etal-2022-bit</bibkey>
      <doi>10.18653/v1/2022.autosimtrans-1.6</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/bstc">BSTC</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>USST</fixed-case>’s System for <fixed-case>A</fixed-case>uto<fixed-case>S</fixed-case>im<fixed-case>T</fixed-case>rans 2022</title>
      <author><first>Zhu</first><last>Hui</last></author>
      <author><first>Yu</first><last>Jun</last></author>
      <pages>43-49</pages>
      <abstract>This paper describes our submitted text-to-text Simultaneous translation (ST) system, which won the second place in the Chinese→English streaming translation task of AutoSimTrans 2022. Our baseline system is a BPE-based Transformer model trained with the PaddlePaddle framework. In our experiments, we employ data synthesis and ensemble approaches to enhance the base model. In order to bridge the gap between general domain and spoken domain, we select in-domain data from general corpus and mixed then with spoken corpus for mixed fine tuning. Finally, we adopt fixed wait-k policy to transfer our full-sentence translation model to simultaneous translation model. Experiments on the development data show that our system outperforms than the baseline system.</abstract>
      <url hash="6cb6f915">2022.autosimtrans-1.7</url>
      <bibkey>hui-jun-2022-ussts</bibkey>
      <doi>10.18653/v1/2022.autosimtrans-1.7</doi>
      <pwccode url="https://github.com/tyy2022/usst_autosimultrans2022" additional="false">tyy2022/usst_autosimultrans2022</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bstc">BSTC</pwcdataset>
    </paper>
  </volume>
</collection>
