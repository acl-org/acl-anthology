<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.americasnlp">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</booktitle>
      <editor><first>Manuel</first><last>Mager</last></editor>
      <editor><first>Arturo</first><last>Oncevay</last></editor>
      <editor><first>Annette</first><last>Rios</last></editor>
      <editor><first>Ivan Vladimir Meza</first><last>Ruiz</last></editor>
      <editor><first>Alexis</first><last>Palmer</last></editor>
      <editor><first>Graham</first><last>Neubig</last></editor>
      <editor><first>Katharina</first><last>Kann</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.americasnlp-1</url>
      <venue>americasnlp</venue>
    </meta>
    <frontmatter>
      <url hash="beb9eed7">2021.americasnlp-1.0</url>
      <bibkey>americasnlp-2021-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>qxo<fixed-case>R</fixed-case>ef 1.0: A coreference corpus and mention-pair baseline for coreference resolution in Conchucos <fixed-case>Q</fixed-case>uechua</title>
      <author><first>Elizabeth</first><last>Pankratz</last></author>
      <pages>1–9</pages>
      <abstract>This paper introduces qxoRef 1.0, the first coreference corpus to be developed for a Quechuan language, and describes a baseline mention-pair coreference resolution system developed for this corpus. The evaluation of this system will illustrate that earlier steps in the NLP pipeline, in particular syntactic parsing, should be in place before a complex task like coreference resolution can truly succeed. qxoRef 1.0 is freely available under a CC-BY-NC-SA 4.0 license.</abstract>
      <url hash="c6243b82">2021.americasnlp-1.1</url>
      <doi>10.18653/v1/2021.americasnlp-1.1</doi>
      <bibkey>pankratz-2021-qxoref</bibkey>
      <pwccode url="https://github.com/epankratz/qxoref" additional="false">epankratz/qxoref</pwccode>
    </paper>
    <paper id="2">
      <title>A corpus of K’iche’ annotated for morphosyntactic structure</title>
      <author><first>Francis</first><last>Tyers</last></author>
      <author><first>Robert</first><last>Henderson</last></author>
      <pages>10–20</pages>
      <abstract>This article describes a collection of sentences in K’iche’ annotated for morphology and syntax. K’iche’ is a language in the Mayan language family, spoken in Guatemala. The annotation is done according to the guidelines of the Universal Dependencies project. The corpus consists of a total of 1,433 sentences containing approximately 10,000 tokens and is released under a free/open-source licence. We present a comparison of parsing systems for K’iche’ using this corpus and describe how it can be used for mining linguistic examples.</abstract>
      <url hash="98959e76">2021.americasnlp-1.2</url>
      <doi>10.18653/v1/2021.americasnlp-1.2</doi>
      <bibkey>tyers-henderson-2021-corpus</bibkey>
    </paper>
    <paper id="3">
      <title>Investigating variation in written forms of <fixed-case>N</fixed-case>ahuatl using character-based language models</title>
      <author><first>Robert</first><last>Pugh</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>21–27</pages>
      <abstract>We describe experiments with character-based language modeling for written variants of Nahuatl. Using a standard LSTM model and publicly available Bible translations, we explore how character language models can be applied to the tasks of estimating mutual intelligibility, identifying genetic similarity, and distinguishing written variants. We demonstrate that these simple language models are able to capture similarities and differences that have been described in the linguistic literature.</abstract>
      <url hash="ba4bb55f">2021.americasnlp-1.3</url>
      <doi>10.18653/v1/2021.americasnlp-1.3</doi>
      <bibkey>pugh-tyers-2021-investigating</bibkey>
      <pwccode url="https://github.com/lguyogiro/nahuatl-variant-charlms-americasnlp" additional="false">lguyogiro/nahuatl-variant-charlms-americasnlp</pwccode>
    </paper>
    <paper id="4">
      <title><fixed-case>A</fixed-case>purinã <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies Treebank</title>
      <author><first>Jack</first><last>Rueter</last></author>
      <author><first>Marília</first><last>Fernanda Pereira de Freitas</last></author>
      <author><first>Sidney</first><last>Da Silva Facundes</last></author>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Niko</first><last>Partanen</last></author>
      <pages>28–33</pages>
      <abstract>This paper presents and discusses the first Universal Dependencies treebank for the Apurinã language. The treebank contains 76 fully annotated sentences, applies 14 parts-of-speech, as well as seven augmented or new features — some of which are unique to Apurinã. The construction of the treebank has also served as an opportunity to develop finite-state description of the language and facilitate the transfer of open-source infrastructure possibilities to an endangered language of the Amazon. The source materials used in the initial treebank represent fieldwork practices where not all tokens of all sentences are equally annotated. For this reason, establishing regular annotation practices for the entire Apurinã treebank is an ongoing project.</abstract>
      <url hash="580f8060">2021.americasnlp-1.4</url>
      <doi>10.18653/v1/2021.americasnlp-1.4</doi>
      <bibkey>rueter-etal-2021-apurina</bibkey>
    </paper>
    <paper id="5">
      <title>Automatic Interlinear Glossing for <fixed-case>O</fixed-case>tomi language</title>
      <author><first>Diego</first><last>Barriga Martínez</last></author>
      <author><first>Victor</first><last>Mijangos</last></author>
      <author><first>Ximena</first><last>Gutierrez-Vasques</last></author>
      <pages>34–43</pages>
      <abstract>In linguistics, interlinear glossing is an essential procedure for analyzing the morphology of languages. This type of annotation is useful for language documentation, and it can also provide valuable data for NLP applications. We perform automatic glossing for Otomi, an under-resourced language. Our work also comprises the pre-processing and annotation of the corpus. We implement different sequential labelers. CRF models represented an efficient and good solution for our task. Two main observations emerged from our work: 1) models with a higher number of parameters (RNNs) performed worse in our low-resource scenario; and 2) the information encoded in the CRF feature function plays an important role in the prediction of labels; however, even in cases where POS tags are not available it is still possible to achieve competitive results.</abstract>
      <url hash="6cf4e4ee">2021.americasnlp-1.5</url>
      <doi>10.18653/v1/2021.americasnlp-1.5</doi>
      <bibkey>barriga-martinez-etal-2021-automatic</bibkey>
    </paper>
    <paper id="6">
      <title>A survey of part-of-speech tagging approaches applied to K’iche’</title>
      <author><first>Francis</first><last>Tyers</last></author>
      <author><first>Nick</first><last>Howell</last></author>
      <pages>44–52</pages>
      <abstract>We study the performance of several popular neural part-of-speech taggers from the Universal Dependencies ecosystem on Mayan languages using a small corpus of 1435 annotated K’iche’ sentences consisting of approximately 10,000 tokens, with encouraging results: <tex-math>F_1</tex-math> scores 93%+ on lemmatisation, part-of-speech and morphological feature assignment. The high performance motivates a cross-language part-of-speech tagging study, where K’iche’-trained models are evaluated on two other Mayan languages, Kaqchikel and Uspanteko: performance on Kaqchikel is good, 63-85%, and on Uspanteko modest, 60-71%. Supporting experiments lead us to conclude the relative diversity of morphological features as a plausible explanation for the limiting factors in cross-language tagging performance, providing some direction for future sentence annotation and collection work to support these and other Mayan languages.</abstract>
      <url hash="a94c2792">2021.americasnlp-1.6</url>
      <doi>10.18653/v1/2021.americasnlp-1.6</doi>
      <bibkey>tyers-howell-2021-survey</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>H</fixed-case>ighland <fixed-case>P</fixed-case>uebla <fixed-case>N</fixed-case>ahuatl Speech Translation Corpus for Endangered Language Documentation</title>
      <author><first>Jiatong</first><last>Shi</last></author>
      <author><first>Jonathan D.</first><last>Amith</last></author>
      <author><first>Xuankai</first><last>Chang</last></author>
      <author><first>Siddharth</first><last>Dalmia</last></author>
      <author><first>Brian</first><last>Yan</last></author>
      <author><first>Shinji</first><last>Watanabe</last></author>
      <pages>53–63</pages>
      <abstract>Documentation of endangered languages (ELs) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of EL audio via automatic speech recognition (ASR), machine translation (MT), or speech translation (ST). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278), an EL spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments, we observed that state-of-the-art end-to-end ST models could outperform a cascaded ST (ASR &gt; MT) pipeline when translating endangered language documentation materials.</abstract>
      <url hash="1fd693b4">2021.americasnlp-1.7</url>
      <doi>10.18653/v1/2021.americasnlp-1.7</doi>
      <bibkey>shi-etal-2021-highland</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="8">
      <title>End-to-End Automatic Speech Recognition: Its Impact on the Workflowin Documenting Yoloxóchitl <fixed-case>M</fixed-case>ixtec</title>
      <author><first>Jonathan D.</first><last>Amith</last></author>
      <author><first>Jiatong</first><last>Shi</last></author>
      <author><first>Rey</first><last>Castillo García</last></author>
      <pages>64–80</pages>
      <abstract>This paper describes three open access Yoloxóchitl Mixtec corpora and presents the results and implications of end-to-end automatic speech recognition for endangered language documentation. Two issues are addressed. First, the advantage for ASR accuracy of targeting informational (BPE) units in addition to, or in substitution of, linguistic units (word, morpheme, morae) and then using ROVER for system combination. BPE units consistently outperform linguistic units although the best results are obtained by system combination of different BPE targets. Second, a case is made that for endangered language documentation, ASR contributions should be evaluated according to extrinsic criteria (e.g., positive impact on downstream tasks) and not simply intrinsic metrics (e.g., CER and WER). The extrinsic metric chosen is the level of reduction in the human effort needed to produce high-quality transcriptions for permanent archiving.</abstract>
      <url hash="2b6025a1">2021.americasnlp-1.8</url>
      <doi>10.18653/v1/2021.americasnlp-1.8</doi>
      <bibkey>amith-etal-2021-end</bibkey>
    </paper>
    <paper id="9">
      <title>A finite-state morphological analyser for <fixed-case>P</fixed-case>araguayan <fixed-case>G</fixed-case>uaraní</title>
      <author><first>Anastasia</first><last>Kuznetsova</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>81–89</pages>
      <abstract>This article describes the development of morphological analyser for Paraguayan Guaraní, agglutinative indigenous language spoken by nearly 6 million people in South America. The implementation of our analyser uses HFST (Helsiki Finite State Technology) and two-level transducer that covers morphotactics and phonological processes occurring in Guaraní. We assess the efficacy of the approach on publicly available Wikipedia and Bible corpora and the naive coverage of analyser reaches 86% on Wikipedia and 91% on Bible corpora.</abstract>
      <url hash="d6bf1547">2021.americasnlp-1.9</url>
      <doi>10.18653/v1/2021.americasnlp-1.9</doi>
      <bibkey>kuznetsova-tyers-2021-finite</bibkey>
    </paper>
    <paper id="10">
      <title>Morphological Segmentation for <fixed-case>S</fixed-case>eneca</title>
      <author><first>Zoey</first><last>Liu</last></author>
      <author><first>Robert</first><last>Jimerson</last></author>
      <author><first>Emily</first><last>Prud’hommeaux</last></author>
      <pages>90–101</pages>
      <abstract>This study takes up the task of low-resource morphological segmentation for Seneca, a critically endangered and morphologically complex Native American language primarily spoken in what is now New York State and Ontario. The labeled data in our experiments comes from two sources: one digitized from a publicly available grammar book and the other collected from informal sources. We treat these two sources as distinct domains and investigate different evaluation designs for model selection. The first design abides by standard practices and evaluate models with the in-domain development set, while the second one carries out evaluation using a development domain, or the out-of-domain development set. Across a series of monolingual and crosslinguistic training settings, our results demonstrate the utility of neural encoder-decoder architecture when coupled with multi-task learning.</abstract>
      <url hash="371b8485">2021.americasnlp-1.10</url>
      <doi>10.18653/v1/2021.americasnlp-1.10</doi>
      <bibkey>liu-etal-2021-morphological</bibkey>
      <pwccode url="https://github.com/zoeyliu18/seneca" additional="false">zoeyliu18/seneca</pwccode>
    </paper>
    <paper id="11">
      <title>Representation of <fixed-case>Y</fixed-case>ine [<fixed-case>A</fixed-case>rawak] Morphology by Finite State Transducer Formalism</title>
      <author><first>Adriano</first><last>Ingunza Torres</last></author>
      <author><first>John</first><last>Miller</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Roberto</first><last>Zariquiey Biondi</last></author>
      <pages>102–112</pages>
      <abstract>We represent the complexity of Yine (Arawak) morphology with a finite state transducer (FST) based morphological analyzer. Yine is a low-resource indigenous polysynthetic Peruvian language spoken by approximately 3,000 people and is classified as ‘definitely endangered’ by UNESCO. We review Yine morphology focusing on morphophonology, possessive constructions and verbal predicates. Then we develop FSTs to model these components proposing techniques to solve challenging problems such as complex patterns of incorporating open and closed category arguments. This is a work in progress and we still have more to do in the development and verification of our analyzer. Our analyzer will serve both as a tool to better document the Yine language and as a component of natural language processing (NLP) applications such as spell checking and correction.</abstract>
      <url hash="fa1d4d59">2021.americasnlp-1.11</url>
      <attachment type="OptionalSupplementaryCode" hash="3b960847">2021.americasnlp-1.11.OptionalSupplementaryCode.zip</attachment>
      <doi>10.18653/v1/2021.americasnlp-1.11</doi>
      <bibkey>ingunza-torres-etal-2021-representation</bibkey>
    </paper>
    <paper id="12">
      <title>Leveraging <fixed-case>E</fixed-case>nglish Word Embeddings for Semi-Automatic Semantic Classification in Nêhiyawêwin (<fixed-case>P</fixed-case>lains <fixed-case>C</fixed-case>ree)</title>
      <author><first>Atticus</first><last>Harrigan</last></author>
      <author><first>Antti</first><last>Arppe</last></author>
      <pages>113–121</pages>
      <abstract>This paper details a semi-automatic method of word clustering for the Algonquian language, Nêhiyawêwin (Plains Cree). Although this method worked well, particularly for nouns, it required some amount of manual postprocessing. The main benefit of this approach over implementing an existing classification ontology is that this method approaches the language from an endogenous point of view, while performing classification quicker than in a fully manual context.</abstract>
      <url hash="0cb0928a">2021.americasnlp-1.12</url>
      <doi>10.18653/v1/2021.americasnlp-1.12</doi>
      <bibkey>harrigan-arppe-2021-leveraging</bibkey>
    </paper>
    <paper id="13">
      <title>Restoring the Sister: Reconstructing a Lexicon from Sister Languages using Neural Machine Translation</title>
      <author><first>Remo</first><last>Nitschke</last></author>
      <pages>122–130</pages>
      <abstract>The historical comparative method has a long history in historical linguists. It describes a process by which historical linguists aim to reverse-engineer the historical developments of language families in order to reconstruct proto-forms and familial relations between languages. In recent years, there have been multiple attempts to replicate this process through machine learning, especially in the realm of cognate detection (List et al., 2016; Ciobanu and Dinu, 2014; Rama et al., 2018). So far, most of these experiments aimed at actual reconstruction have attempted the prediction of a proto-form from the forms of the daughter languages (Ciobanu and Dinu, 2018; Meloni et al., 2019).. Here, we propose a reimplementation that uses modern related languages, or sisters, instead, to reconstruct the vocabulary of a target language. In particular, we show that we can reconstruct vocabulary of a target language by using a fairly small data set of parallel cognates from different sister languages, using a neural machine translation (NMT) architecture with a standard encoder-decoder setup. This effort is directly in furtherance of the goal to use machine learning tools to help under-served language communities in their efforts at reclaiming, preserving, or reconstructing their own languages.</abstract>
      <url hash="939ed1c3">2021.americasnlp-1.13</url>
      <doi>10.18653/v1/2021.americasnlp-1.13</doi>
      <bibkey>nitschke-2021-restoring</bibkey>
      <pwccode url="https://github.com/remo-help/restoring_the_sister" additional="false">remo-help/restoring_the_sister</pwccode>
    </paper>
    <paper id="14">
      <title>Expanding <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies for Polysynthetic Languages: A Case of <fixed-case>S</fixed-case>t. <fixed-case>L</fixed-case>awrence <fixed-case>I</fixed-case>sland <fixed-case>Y</fixed-case>upik</title>
      <author><first>Hyunji Hayley</first><last>Park</last></author>
      <author><first>Lane</first><last>Schwartz</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>131–142</pages>
      <abstract>This paper describes the development of the first Universal Dependencies (UD) treebank for St. Lawrence Island Yupik, an endangered language spoken in the Bering Strait region. While the UD guidelines provided a general framework for our annotations, language-specific decisions were made necessary by the rich morphology of the polysynthetic language. Most notably, we annotated a corpus at the morpheme level as well as the word level. The morpheme level annotation was conducted using an existing morphological analyzer and manual disambiguation. By comparing the two resulting annotation schemes, we argue that morpheme-level annotation is essential for polysynthetic languages like St. Lawrence Island Yupik. Word-level annotation results in degenerate trees for some Yupik sentences and often fails to capture syntactic relations that can be manifested at the morpheme level. Dependency parsing experiments provide further support for morpheme-level annotation. Implications for UD annotation of other polysynthetic languages are discussed.</abstract>
      <url hash="49a8ac83">2021.americasnlp-1.14</url>
      <doi>10.18653/v1/2021.americasnlp-1.14</doi>
      <bibkey>park-etal-2021-expanding</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="15">
      <title>The More Detail, the Better? – Investigating the Effects of Semantic Ontology Specificity on Vector Semantic Classification with a <fixed-case>P</fixed-case>lains <fixed-case>C</fixed-case>ree / nêhiyawêwin Dictionary</title>
      <author><first>Daniel</first><last>Dacanay</last></author>
      <author><first>Atticus</first><last>Harrigan</last></author>
      <author><first>Arok</first><last>Wolvengrey</last></author>
      <author><first>Antti</first><last>Arppe</last></author>
      <pages>143–152</pages>
      <abstract>One problem in the task of automatic semantic classification is the problem of determining the level on which to group lexical items. This is often accomplished using pre-made, hierarchical semantic ontologies. The following investigation explores the computational assignment of semantic classifications on the contents of a dictionary of nêhiyawêwin / Plains Cree (ISO: crk, Algonquian, Western Canada and United States), using a semantic vector space model, and following two semantic ontologies, WordNet and SIL’s Rapid Words, and compares how these computational results compare to manual classifications with the same two ontologies.</abstract>
      <url hash="203761c7">2021.americasnlp-1.15</url>
      <doi>10.18653/v1/2021.americasnlp-1.15</doi>
      <bibkey>dacanay-etal-2021-detail</bibkey>
    </paper>
    <paper id="16">
      <title>Experiments on a <fixed-case>G</fixed-case>uarani Corpus of News and Social Media</title>
      <author><first>Santiago</first><last>Góngora</last></author>
      <author><first>Nicolás</first><last>Giossa</last></author>
      <author><first>Luis</first><last>Chiruzzo</last></author>
      <pages>153–158</pages>
      <abstract>While Guarani is widely spoken in South America, obtaining a large amount of Guarani text from the web is hard. We present the building process of a Guarani corpus composed of a parallel Guarani-Spanish set of news articles, and a monolingual set of tweets. We perform some word embeddings experiments aiming at evaluating the quality of the Guarani split of the corpus, finding encouraging results but noticing that more diversity in text domains might be needed for further improvements.</abstract>
      <url hash="1d798f81">2021.americasnlp-1.16</url>
      <doi>10.18653/v1/2021.americasnlp-1.16</doi>
      <bibkey>gongora-etal-2021-experiments</bibkey>
    </paper>
    <paper id="17">
      <title>Towards a First Automatic Unsupervised Morphological Segmentation for <fixed-case>I</fixed-case>nuinnaqtun</title>
      <author><first>Ngoc Tan</first><last>Le</last></author>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <pages>159–162</pages>
      <abstract>Low-resource polysynthetic languages pose many challenges in NLP tasks, such as morphological analysis and Machine Translation, due to available resources and tools, and the morphologically complex languages. This research focuses on the morphological segmentation while adapting an unsupervised approach based on Adaptor Grammars in low-resource setting. Experiments and evaluations on Inuinnaqtun, one of Inuit language family in Northern Canada, considered a language that will be extinct in less than two generations, have shown promising results.</abstract>
      <url hash="3814cdb3">2021.americasnlp-1.17</url>
      <doi>10.18653/v1/2021.americasnlp-1.17</doi>
      <bibkey>le-sadat-2021-towards</bibkey>
    </paper>
    <paper id="18">
      <title>Toward Creation of <fixed-case>A</fixed-case>ncash Lexical Resources from <fixed-case>OCR</fixed-case></title>
      <author><first>Johanna</first><last>Cordova</last></author>
      <author><first>Damien</first><last>Nouvel</last></author>
      <pages>163–167</pages>
      <abstract>The Quechua linguistic family has a limited number of NLP resources, most of them being dedicated to Southern Quechua, whereas the varieties of Central Quechua have, to the best of our knowledge, no specific resources (software, lexicon or corpus). Our work addresses this issue by producing two resources for the Ancash Quechua: a full digital version of a dictionary, and an OCR model adapted to the considered variety. In this paper, we describe the steps towards this goal: we first measure performances of existing models for the task of digitising a Quechua dictionary, then adapt a model for the Ancash variety, and finally create a reliable resource for NLP in XML-TEI format. We hope that this work will be a basis for initiating NLP projects for Central Quechua, and that it will encourage digitisation initiatives for under-resourced languages.</abstract>
      <url hash="0bb39377">2021.americasnlp-1.18</url>
      <doi>10.18653/v1/2021.americasnlp-1.18</doi>
      <bibkey>cordova-nouvel-2021-toward</bibkey>
    </paper>
    <paper id="19">
      <title>Ayuuk-<fixed-case>S</fixed-case>panish Neural Machine Translator</title>
      <author><first>Delfino</first><last>Zacarías Márquez</last></author>
      <author><first>Ivan Vladimir</first><last>Meza Ruiz</last></author>
      <pages>168–172</pages>
      <abstract>This paper presents the first neural machine translator system for the Ayuuk language. In our experiments we translate from Ayuuk to Spanish, and fromSpanish to Ayuuk. Ayuuk is a language spoken in the Oaxaca state of Mexico by the Ayuukjä’äy people (in Spanish commonly known as Mixes. We use different sources to create a low-resource parallel corpus, more than 6,000 phrases. For some of these resources we rely on automatic alignment. The proposed system is based on the Transformer neural architecture and it uses sub-word level tokenization as the input. We show the current performance given the resources we have collected for the San Juan Güichicovi variant, they are promising, up to 5 BLEU. We based our development on the Masakhane project for African languages.</abstract>
      <url hash="9bee916e">2021.americasnlp-1.19</url>
      <doi>10.18653/v1/2021.americasnlp-1.19</doi>
      <bibkey>zacarias-marquez-meza-ruiz-2021-ayuuk</bibkey>
    </paper>
    <paper id="20">
      <title>Explicit Tone Transcription Improves <fixed-case>ASR</fixed-case> Performance in Extremely Low-Resource Languages: A Case Study in <fixed-case>B</fixed-case>ribri</title>
      <author><first>Rolando</first><last>Coto-Solano</last></author>
      <pages>173–184</pages>
      <abstract>Linguistic tone is transcribed for input into ASR systems in numerous ways. This paper shows a systematic test of several transcription styles, using as an example the Chibchan language Bribri, an extremely low-resource language from Costa Rica. The most successful models separate the tone from the vowel, so that the ASR algorithms learn tone patterns independently. These models showed improvements ranging from 4% to 25% in character error rate (CER), and between 3% and 23% in word error rate (WER). This is true for both traditional GMM/HMM and end-to-end CTC algorithms. This paper also presents the first attempt to train ASR models for Bribri. The best performing models had a CER of 33% and a WER of 50%. Despite the disadvantage of using hand-engineered representations, these models were trained on only 68 minutes of data, and therefore show the potential of ASR to generate further training materials and aid in the documentation and revitalization of the language.</abstract>
      <url hash="d54e742a">2021.americasnlp-1.20</url>
      <doi>10.18653/v1/2021.americasnlp-1.20</doi>
      <bibkey>coto-solano-2021-explicit</bibkey>
    </paper>
    <paper id="21">
      <title>Towards a morphological transducer and orthography converter for <fixed-case>W</fixed-case>estern <fixed-case>T</fixed-case>lacolula <fixed-case>V</fixed-case>alley <fixed-case>Z</fixed-case>apotec</title>
      <author><first>Jonathan</first><last>Washington</last></author>
      <author><first>Felipe</first><last>Lopez</last></author>
      <author><first>Brook</first><last>Lillehaugen</last></author>
      <pages>185–193</pages>
      <abstract>This paper presents work towards a morphological transducer and orthography converter for Dizhsa, or San Lucas Quiaviní Zapotec, an endangered Western Tlacolula Valley Zapotec language. The implementation of various aspects of the language’s morphology is presented, as well as the transducer’s ability to perform analysis in two orthographies and convert between them. Potential uses of the transducer for language maintenance and issues of licensing are also discussed. Evaluation of the transducer shows that it is fairly robust although incomplete, and evaluation of orthographic conversion shows that this method is strongly affected by the coverage of the transducer.</abstract>
      <url hash="c2d6b978">2021.americasnlp-1.21</url>
      <doi>10.18653/v1/2021.americasnlp-1.21</doi>
      <bibkey>washington-etal-2021-towards</bibkey>
      <pwccode url="https://github.com/apertium/apertium-zab" additional="false">apertium/apertium-zab</pwccode>
    </paper>
    <paper id="22">
      <title><fixed-case>P</fixed-case>eru is Multilingual, Its Machine Translation Should Be Too?</title>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <pages>194–201</pages>
      <abstract>Peru is a multilingual country with a long history of contact between the indigenous languages and Spanish. Taking advantage of this context for machine translation is possible with multilingual approaches for learning both unsupervised subword segmentation and neural machine translation models. The study proposes the first multilingual translation models for four languages spoken in Peru: Aymara, Ashaninka, Quechua and Shipibo-Konibo, providing both many-to-Spanish and Spanish-to-many models and outperforming pairwise baselines in most of them. The task exploited a large English-Spanish dataset for pre-training, monolingual texts with tagged back-translation, and parallel corpora aligned with English. Finally, by fine-tuning the best models, we also assessed the out-of-domain capabilities in two evaluation datasets for Quechua and a new one for Shipibo-Konibo.</abstract>
      <url hash="e12bb0e2">2021.americasnlp-1.22</url>
      <doi>10.18653/v1/2021.americasnlp-1.22</doi>
      <bibkey>oncevay-2021-peru</bibkey>
      <pwccode url="https://github.com/aoncevay/mt-peru" additional="false">aoncevay/mt-peru</pwccode>
    </paper>
    <paper id="23">
      <title>Findings of the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2021 Shared Task on Open Machine Translation for Indigenous Languages of the <fixed-case>A</fixed-case>mericas</title>
      <author><first>Manuel</first><last>Mager</last></author>
      <author><first>Arturo</first><last>Oncevay</last></author>
      <author><first>Abteen</first><last>Ebrahimi</last></author>
      <author><first>John</first><last>Ortega</last></author>
      <author><first>Annette</first><last>Rios</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Ximena</first><last>Gutierrez-Vasques</last></author>
      <author><first>Luis</first><last>Chiruzzo</last></author>
      <author><first>Gustavo</first><last>Giménez-Lugo</last></author>
      <author><first>Ricardo</first><last>Ramos</last></author>
      <author><first>Ivan Vladimir</first><last>Meza Ruiz</last></author>
      <author><first>Rolando</first><last>Coto-Solano</last></author>
      <author><first>Alexis</first><last>Palmer</last></author>
      <author><first>Elisabeth</first><last>Mager-Hois</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>202–217</pages>
      <abstract>This paper presents the results of the 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas. The shared task featured two independent tracks, and participants submitted machine translation systems for up to 10 indigenous languages. Overall, 8 teams participated with a total of 214 submissions. We provided training sets consisting of data collected from various sources, as well as manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages.</abstract>
      <url hash="ac069c84">2021.americasnlp-1.23</url>
      <doi>10.18653/v1/2021.americasnlp-1.23</doi>
      <bibkey>mager-etal-2021-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="24">
      <title>Open Machine Translation for Low Resource <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>merican Languages (<fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2021 Shared Task Contribution)</title>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Subhadarshi</first><last>Panda</last></author>
      <author><first>Amulya</first><last>Dash</last></author>
      <author><first>Esau</first><last>Villatoro-Tello</last></author>
      <author><first>A. Seza</first><last>Doğruöz</last></author>
      <author><first>Rosa M.</first><last>Ortega-Mendoza</last></author>
      <author><first>Amadeo</first><last>Hernández</last></author>
      <author><first>Yashvardhan</first><last>Sharma</last></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <pages>218–223</pages>
      <abstract>This paper describes the team (“Tamalli”)’s submission to AmericasNLP2021 shared task on Open Machine Translation for low resource South American languages. Our goal was to evaluate different Machine Translation (MT) techniques, statistical and neural-based, under several configuration settings. We obtained the second-best results for the language pairs “Spanish-Bribri”, “Spanish-Asháninka”, and “Spanish-Rarámuri” in the category “Development set not used for training”. Our performed experiments will serve as a point of reference for researchers working on MT with low-resource languages.</abstract>
      <url hash="a84a26a0">2021.americasnlp-1.24</url>
      <doi>10.18653/v1/2021.americasnlp-1.24</doi>
      <bibkey>parida-etal-2021-open</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>NRC</fixed-case>-<fixed-case>CNRC</fixed-case> Machine Translation Systems for the 2021 <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> Shared Task</title>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <author><first>Darlene</first><last>Stewart</last></author>
      <author><first>Samuel</first><last>Larkin</last></author>
      <author><first>Patrick</first><last>Littell</last></author>
      <pages>224–233</pages>
      <abstract>We describe the NRC-CNRC systems submitted to the AmericasNLP shared task on machine translation. We submitted systems translating from Spanish into Wixárika, Nahuatl, Rarámuri, and Guaraní. Our best neural machine translation systems used multilingual pretraining, ensembling, finetuning, training on parts of the development data, and subword regularization. We also submitted translation memory systems as a strong baseline.</abstract>
      <url hash="b1a5cfcc">2021.americasnlp-1.25</url>
      <doi>10.18653/v1/2021.americasnlp-1.25</doi>
      <bibkey>knowles-etal-2021-nrc</bibkey>
    </paper>
    <paper id="26">
      <title>Low-Resource Machine Translation Using Cross-Lingual Language Model Pretraining</title>
      <author><first>Francis</first><last>Zheng</last></author>
      <author><first>Machel</first><last>Reid</last></author>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <pages>234–240</pages>
      <abstract>This paper describes UTokyo’s submission to the AmericasNLP 2021 Shared Task on machine translation systems for indigenous languages of the Americas. We present a low-resource machine translation system that improves translation accuracy using cross-lingual language model pretraining. Our system uses an mBART implementation of fairseq to pretrain on a large set of monolingual data from a diverse set of high-resource languages before finetuning on 10 low-resource indigenous American languages: Aymara, Bribri, Asháninka, Guaraní, Wixarika, Náhuatl, Hñähñu, Quechua, Shipibo-Konibo, and Rarámuri. On average, our system achieved BLEU scores that were 1.64 higher and chrF scores that were 0.0749 higher than the baseline.</abstract>
      <url hash="44ac1d19">2021.americasnlp-1.26</url>
      <doi>10.18653/v1/2021.americasnlp-1.26</doi>
      <bibkey>zheng-etal-2021-low</bibkey>
    </paper>
    <paper id="27">
      <title>The <fixed-case>REPU</fixed-case> <fixed-case>CS</fixed-case>’ <fixed-case>S</fixed-case>panish–<fixed-case>Q</fixed-case>uechua Submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2021 Shared Task on Open Machine Translation</title>
      <author><first>Oscar</first><last>Moreno</last></author>
      <pages>241–247</pages>
      <abstract>We present the submission of REPUcs to the AmericasNLP machine translation shared task for the low resource language pair Spanish–Quechua. Our neural machine translation system ranked first in Track two (development set not used for training) and third in Track one (training includes development data). Our contribution is focused on: (i) the collection of new parallel data from different web sources (poems, lyrics, lexicons, handbooks), and (ii) using large Spanish–English data for pre-training and then fine-tuning the Spanish–Quechua system. This paper describes the new parallel corpora and our approach in detail.</abstract>
      <url hash="0d16c89a">2021.americasnlp-1.27</url>
      <doi>10.18653/v1/2021.americasnlp-1.27</doi>
      <bibkey>moreno-2021-repu</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>M</fixed-case>oses and the Character-Based Random Babbling Baseline: <fixed-case>C</fixed-case>o<fixed-case>AS</fixed-case>ta<fixed-case>L</fixed-case> at <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2021 Shared Task</title>
      <author><first>Marcel</first><last>Bollmann</last></author>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Héctor</first><last>Murrieta Bello</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>248–254</pages>
      <abstract>We evaluated a range of neural machine translation techniques developed specifically for low-resource scenarios. Unsuccessfully. In the end, we submitted two runs: (i) a standard phrase-based model, and (ii) a random babbling baseline using character trigrams. We found that it was surprisingly hard to beat (i), in spite of this model being, in theory, a bad fit for polysynthetic languages; and more interestingly, that (ii) was better than several of the submitted systems, highlighting how difficult low-resource machine translation for polysynthetic languages is.</abstract>
      <url hash="f7694ac1">2021.americasnlp-1.28</url>
      <doi>10.18653/v1/2021.americasnlp-1.28</doi>
      <bibkey>bollmann-etal-2021-moses</bibkey>
    </paper>
    <paper id="29">
      <title>The <fixed-case>H</fixed-case>elsinki submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> shared task</title>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Sami</first><last>Virpioja</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>255–264</pages>
      <abstract>The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects: (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail.</abstract>
      <url hash="c92b8108">2021.americasnlp-1.29</url>
      <doi>10.18653/v1/2021.americasnlp-1.29</doi>
      <bibkey>vazquez-etal-2021-helsinki</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>I</fixed-case>nd<fixed-case>T</fixed-case>5: A Text-to-Text Transformer for 10 Indigenous Languages</title>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Wei-Rui</first><last>Chen</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Hasan</first><last>Cavusoglu</last></author>
      <pages>265–271</pages>
      <abstract>Transformer language models have become fundamental components of NLP based pipelines. Although several Transformer have been introduced to serve many languages, there is a shortage of models pre-trained for low-resource and Indigenous languages in particular. In this work, we introduce IndT5, the first Transformer language model for Indigenous languages. To train IndT5, we build IndCorpus, a new corpus for 10 Indigenous languages and Spanish. We also present the application of IndT5 to machine translation by investigating different approaches to translate between Spanish and the Indigenous languages as part of our contribution to the AmericasNLP 2021 Shared Task on Open Machine Translation. IndT5 and IndCorpus are publicly available for research.</abstract>
      <url hash="35edb80c">2021.americasnlp-1.30</url>
      <doi>10.18653/v1/2021.americasnlp-1.30</doi>
      <bibkey>nagoudi-etal-2021-indt5</bibkey>
    </paper>
  </volume>
</collection>
