<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.bionlp">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing</booktitle>
      <editor><first>Dina</first><last>Demner-Fushman</last></editor>
      <editor><first>Kevin Bretonnel</first><last>Cohen</last></editor>
      <editor><first>Sophia</first><last>Ananiadou</last></editor>
      <editor><first>Junichi</first><last>Tsujii</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="bea3611e">2020.bionlp-1</url>
    </meta>
    <frontmatter>
      <url hash="be6d2b3b">2020.bionlp-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Quantifying 60 Years of Gender Bias in Biomedical Research with Word Embeddings</title>
      <author><first>Anthony</first><last>Rios</last></author>
      <author><first>Reenam</first><last>Joshi</last></author>
      <author><first>Hejin</first><last>Shin</last></author>
      <pages>1–13</pages>
      <abstract>Gender bias in biomedical research can have an adverse impact on the health of real people. For example, there is evidence that heart disease-related funded research generally focuses on men. Health disparities can form between men and at-risk groups of women (i.e., elderly and low-income) if there is not an equal number of heart disease-related studies for both genders. In this paper, we study temporal bias in biomedical research articles by measuring gender differences in word embeddings. Specifically, we address multiple questions, including, How has gender bias changed over time in biomedical research, and what health-related concepts are the most biased? Overall, we find that traditional gender stereotypes have reduced over time. However, we also find that the embeddings of many medical conditions are as biased today as they were 60 years ago (e.g., concepts related to drug addiction and body dysmorphia).</abstract>
      <url hash="04caffd3">2020.bionlp-1.1</url>
      <doi>10.18653/v1/2020.bionlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Sequence-to-Set Semantic Tagging for Complex Query Reformulation and Automated Text Categorization in Biomedical <fixed-case>IR</fixed-case> using Self-Attention</title>
      <author><first>Manirupa</first><last>Das</last></author>
      <author><first>Juanxi</first><last>Li</last></author>
      <author><first>Eric</first><last>Fosler-Lussier</last></author>
      <author><first>Simon</first><last>Lin</last></author>
      <author><first>Steve</first><last>Rust</last></author>
      <author><first>Yungui</first><last>Huang</last></author>
      <author><first>Rajiv</first><last>Ramnath</last></author>
      <pages>14–27</pages>
      <abstract>Novel contexts, comprising a set of terms referring to one or more concepts, may often arise in complex querying scenarios such as in evidence-based medicine (EBM) involving biomedical literature. These may not explicitly refer to entities or canonical concept forms occurring in a fact-based knowledge source, e.g. the UMLS ontology. Moreover, hidden associations between related concepts meaningful in the current context, may not exist within a single document, but across documents in the collection. Predicting semantic concept tags of documents can therefore serve to associate documents related in unseen contexts, or categorize them, in information filtering or retrieval scenarios. Thus, inspired by the success of sequence-to-sequence neural models, we develop a novel sequence-to-set framework with attention, for learning document representations in a unique unsupervised setting, using no human-annotated document labels or external knowledge resources and only corpus-derived term statistics to drive the training, that can effect term transfer within a corpus for semantically tagging a large collection of documents. Our sequence-to-set modeling approach to predict semantic tags, gives to the best of our knowledge, the state-of-the-art for both, an unsupervised query expansion (QE) task for the TREC CDS 2016 challenge dataset when evaluated on an Okapi BM25–based document retrieval system; and also over the MLTM system baseline baseline (Soleimani and Miller, 2016), for both supervised and semi-supervised multi-label prediction tasks on the del.icio.us and Ohsumed datasets. We make our code and data publicly available.</abstract>
      <url hash="e38458d8">2020.bionlp-1.2</url>
      <doi>10.18653/v1/2020.bionlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Interactive Extractive Search over Biomedical Corpora</title>
      <author><first>Hillel</first><last>Taub Tabib</last></author>
      <author><first>Micah</first><last>Shlain</last></author>
      <author><first>Shoval</first><last>Sadde</last></author>
      <author><first>Dan</first><last>Lahav</last></author>
      <author><first>Matan</first><last>Eyal</last></author>
      <author><first>Yaara</first><last>Cohen</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>28–37</pages>
      <abstract>We present a system that allows life-science researchers to search a linguistically annotated corpus of scientific texts using patterns over dependency graphs, as well as using patterns over token sequences and a powerful variant of boolean keyword queries. In contrast to previous attempts to dependency-based search, we introduce a light-weight query language that does not require the user to know the details of the underlying linguistic representations, and instead to query the corpus by providing an example sentence coupled with simple markup. Search is performed at an interactive speed due to efficient linguistic graph-indexing and retrieval engine. This allows for rapid exploration, development and refinement of user queries. We demonstrate the system using example workflows over two corpora: the PubMed corpus including 14,446,243 PubMed abstracts and the CORD-19 dataset, a collection of over 45,000 research papers focused on COVID-19 research. The system is publicly available at https://allenai.github.io/spike</abstract>
      <url hash="f34737e0">2020.bionlp-1.3</url>
      <doi>10.18653/v1/2020.bionlp-1.3</doi>
      <video tag="video" href="http://slideslive.com/38929643"/>
    </paper>
    <paper id="4">
      <title>Improving Biomedical Analogical Retrieval with Embedding of Structural Dependencies</title>
      <author><first>Amandalynne</first><last>Paullada</last></author>
      <author><first>Bethany</first><last>Percha</last></author>
      <author><first>Trevor</first><last>Cohen</last></author>
      <pages>38–48</pages>
      <abstract>Inferring the nature of the relationships between biomedical entities from text is an important problem due to the difficulty of maintaining human-curated knowledge bases in rapidly evolving fields. Neural word embeddings have earned attention for an apparent ability to encode relational information. However, word embedding models that disregard syntax during training are limited in their ability to encode the structural relationships fundamental to cognitive theories of analogy. In this paper, we demonstrate the utility of encoding dependency structure in word embeddings in a model we call Embedding of Structural Dependencies (ESD) as a way to represent biomedical relationships in two analogical retrieval tasks: a relationship retrieval (RR) task, and a literature-based discovery (LBD) task meant to hypothesize plausible relationships between pairs of entities unseen in training. We compare our model to skip-gram with negative sampling (SGNS), using 19 databases of biomedical relationships as our evaluation data, with improvements in performance on 17 (LBD) and 18 (RR) of these sets. These results suggest embeddings encoding dependency path information are of value for biomedical analogy retrieval.</abstract>
      <url hash="fa2e9527">2020.bionlp-1.4</url>
      <doi>10.18653/v1/2020.bionlp-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>D</fixed-case>e<fixed-case>S</fixed-case>pin: a prototype system for detecting spin in biomedical publications</title>
      <author><first>Anna</first><last>Koroleva</last></author>
      <author><first>Sanjay</first><last>Kamath</last></author>
      <author><first>Patrick</first><last>Bossuyt</last></author>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <pages>49–59</pages>
      <abstract>Improving the quality of medical research reporting is crucial to reduce avoidable waste in research and to improve the quality of health care. Despite various initiatives aiming at improving research reporting – guidelines, checklists, authoring aids, peer review procedures, etc. – overinterpretation of research results, also known as spin, is still a serious issue in research reporting. In this paper, we propose a Natural Language Processing (NLP) system for detecting several types of spin in biomedical articles reporting randomized controlled trials (RCTs). We use a combination of rule-based and machine learning approaches to extract important information on trial design and to detect potential spin. The proposed spin detection system includes algorithms for text structure analysis, sentence classification, entity and relation extraction, semantic similarity assessment. Our algorithms achieved operational performance for the these tasks, F-measure ranging from 79,42 to 97.86% for different tasks. The most difficult task is extracting reported outcomes. Our tool is intended to be used as a semi-automated aid tool for assisting both authors and peer reviewers to detect potential spin. The tool incorporates a simple interface that allows to run the algorithms and visualize their output. It can also be used for manual annotation and correction of the errors in the outputs. The proposed tool is the first tool for spin detection. The tool and the annotated dataset are freely available.</abstract>
      <url hash="d606a19f">2020.bionlp-1.5</url>
      <doi>10.18653/v1/2020.bionlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Towards Visual Dialog for Radiology</title>
      <author><first>Olga</first><last>Kovaleva</last></author>
      <author><first>Chaitanya</first><last>Shivade</last></author>
      <author><first>Satyananda</first><last>Kashyap</last></author>
      <author><first>Karina</first><last>Kanjaria</last></author>
      <author><first>Joy</first><last>Wu</last></author>
      <author><first>Deddeh</first><last>Ballah</last></author>
      <author><first>Adam</first><last>Coy</last></author>
      <author><first>Alexandros</first><last>Karargyris</last></author>
      <author><first>Yufan</first><last>Guo</last></author>
      <author><first>David Beymer</first><last>Beymer</last></author>
      <author><first>Anna</first><last>Rumshisky</last></author>
      <author><first>Vandana Mukherjee</first><last>Mukherjee</last></author>
      <pages>60–69</pages>
      <abstract>Current research in machine learning for radiology is focused mostly on images. There exists limited work in investigating intelligent interactive systems for radiology. To address this limitation, we introduce a realistic and information-rich task of Visual Dialog in radiology, specific to chest X-ray images. Using MIMIC-CXR, an openly available database of chest X-ray images, we construct both a synthetic and a real-world dataset and provide baseline scores achieved by state-of-the-art models. We show that incorporating medical history of the patient leads to better performance in answering questions as opposed to conventional visual question answering model which looks only at the image. While our experiments show promising results, they indicate that the task is extremely challenging with significant scope for improvement. We make both the datasets (synthetic and gold standard) and the associated code publicly available to the research community.</abstract>
      <url hash="30ad8cab">2020.bionlp-1.6</url>
      <doi>10.18653/v1/2020.bionlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>A <fixed-case>BERT</fixed-case>-based One-Pass Multi-Task Model for Clinical Temporal Relation Extraction</title>
      <author><first>Chen</first><last>Lin</last></author>
      <author><first>Timothy</first><last>Miller</last></author>
      <author><first>Dmitriy</first><last>Dligach</last></author>
      <author><first>Farig</first><last>Sadeque</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <author><first>Guergana</first><last>Savova</last></author>
      <pages>70–75</pages>
      <abstract>Recently BERT has achieved a state-of-the-art performance in temporal relation extraction from clinical Electronic Medical Records text. However, the current approach is inefficient as it requires multiple passes through each input sequence. We extend a recently-proposed one-pass model for relation classification to a one-pass model for relation extraction. We augment this framework by introducing global embeddings to help with long-distance relation inference, and by multi-task learning to increase model performance and generalizability. Our proposed model produces results on par with the state-of-the-art in temporal relation extraction on the THYME corpus and is much “greener” in computational cost.</abstract>
      <url hash="6120b644">2020.bionlp-1.7</url>
      <doi>10.18653/v1/2020.bionlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Experimental Evaluation and Development of a Silver-Standard for the <fixed-case>MIMIC</fixed-case>-<fixed-case>III</fixed-case> Clinical Coding Dataset</title>
      <author><first>Thomas</first><last>Searle</last></author>
      <author><first>Zina</first><last>Ibrahim</last></author>
      <author><first>Richard</first><last>Dobson</last></author>
      <pages>76–85</pages>
      <abstract>Clinical coding is currently a labour-intensive, error-prone, but a critical administrative process whereby hospital patient episodes are manually assigned codes by qualified staff from large, standardised taxonomic hierarchies of codes. Automating clinical coding has a long history in NLP research and has recently seen novel developments setting new benchmark results. A popular dataset used in this task is MIMIC-III, a large database of clinical free text notes and their associated codes amongst other data. We argue for the reconsideration of the validity MIMIC-III’s assigned codes, as MIMIC-III has not undergone secondary validation. This work presents an open-source, reproducible experimental methodology for assessing the validity of EHR discharge summaries. We exemplify the methodology with MIMIC-III discharge summaries and show the most frequently assigned codes in MIMIC-III are undercoded up to 35%.</abstract>
      <url hash="5bcfc46e">2020.bionlp-1.8</url>
      <doi>10.18653/v1/2020.bionlp-1.8</doi>
      <video tag="video" href="http://slideslive.com/38929649"/>
    </paper>
    <paper id="9">
      <title>Comparative Analysis of Text Classification Approaches in Electronic Health Records</title>
      <author><first>Aurelie</first><last>Mascio</last></author>
      <author><first>Zeljko</first><last>Kraljevic</last></author>
      <author><first>Daniel</first><last>Bean</last></author>
      <author><first>Richard</first><last>Dobson</last></author>
      <author><first>Robert</first><last>Stewart</last></author>
      <author><first>Rebecca</first><last>Bendayan</last></author>
      <author><first>Angus</first><last>Roberts</last></author>
      <pages>86–94</pages>
      <abstract>Text classification tasks which aim at harvesting and/or organizing information from electronic health records are pivotal to support clinical and translational research. However these present specific challenges compared to other classification tasks, notably due to the particular nature of the medical lexicon and language used in clinical records. Recent advances in embedding methods have shown promising results for several clinical tasks, yet there is no exhaustive comparison of such approaches with other commonly used word representations and classification models. In this work, we analyse the impact of various word representations, text pre-processing and classification algorithms on the performance of four different text classification tasks. The results show that traditional approaches, when tailored to the specific language and structure of the text inherent to the classification task, can achieve or exceed the performance of more recent ones based on contextual embeddings such as BERT.</abstract>
      <url hash="a505a102">2020.bionlp-1.9</url>
      <doi>10.18653/v1/2020.bionlp-1.9</doi>
      <attachment type="Dataset" hash="47149249">2020.bionlp-1.9.Dataset.pdf</attachment>
      <video tag="video" href="http://slideslive.com/38929648"/>
    </paper>
    <paper id="10">
      <title>Noise Pollution in Hospital Readmission Prediction: Long Document Classification with Reinforcement Learning</title>
      <author><first>Liyan</first><last>Xu</last></author>
      <author><first>Julien</first><last>Hogan</last></author>
      <author><first>Rachel E.</first><last>Patzer</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>95–104</pages>
      <abstract>This paper presents a reinforcement learning approach to extract noise in long clinical documents for the task of readmission prediction after kidney transplant. We face the challenges of developing robust models on a small dataset where each document may consist of over 10K tokens with full of noise including tabular text and task-irrelevant sentences. We first experiment four types of encoders to empirically decide the best document representation, and then apply reinforcement learning to remove noisy text from the long documents, which models the noise extraction process as a sequential decision problem. Our results show that the old bag-of-words encoder outperforms deep learning-based encoders on this task, and reinforcement learning is able to improve upon baseline while pruning out 25% text segments. Our analysis depicts that reinforcement learning is able to identify both typical noisy tokens and task-specific noisy text.</abstract>
      <url hash="de916f91">2020.bionlp-1.10</url>
      <doi>10.18653/v1/2020.bionlp-1.10</doi>
    </paper>
    <paper id="11">
      <title>Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author id="fei-liu-unimelb"><first>Fei</first><last>Liu</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>105–111</pages>
      <abstract>In this paper, we apply pre-trained language models to the Semantic Textual Similarity (STS) task, with a specific focus on the clinical domain. In low-resource setting of clinical STS, these large models tend to be impractical and prone to overfitting. Building on BERT, we study the impact of a number of model design choices, namely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS 1 demonstrate substantial improvements, validating the utility of the proposed methods.</abstract>
      <url hash="83d7c2e8">2020.bionlp-1.11</url>
      <doi>10.18653/v1/2020.bionlp-1.11</doi>
      <video tag="video" href="http://slideslive.com/38929642"/>
    </paper>
    <paper id="12">
      <title>Entity-Enriched Neural Models for Clinical Question Answering</title>
      <author><first>Bhanu Pratap Singh</first><last>Rawat</last></author>
      <author><first>Wei-Hung</first><last>Weng</last></author>
      <author><first>So Yeon</first><last>Min</last></author>
      <author><first>Preethi</first><last>Raghavan</last></author>
      <author><first>Peter</first><last>Szolovits</last></author>
      <pages>112–122</pages>
      <abstract>We explore state-of-the-art neural models for question answering on electronic medical records and improve their ability to generalize better on previously unseen (paraphrased) questions at test time. We enable this by learning to predict logical forms as an auxiliary task along with the main task of answer span detection. The predicted logical forms also serve as a rationale for the answer. Further, we also incorporate medical entity information in these models via the ERNIE architecture. We train our models on the large-scale emrQA dataset and observe that our multi-task entity-enriched models generalize to paraphrased questions ~5% better than the baseline BERT model.</abstract>
      <url hash="d54da020">2020.bionlp-1.12</url>
      <doi>10.18653/v1/2020.bionlp-1.12</doi>
      <video tag="video" href="http://slideslive.com/38929646"/>
    </paper>
    <paper id="13">
      <title>Evidence Inference 2.0: More Data, Better Models</title>
      <author><first>Jay</first><last>DeYoung</last></author>
      <author><first>Eric</first><last>Lehman</last></author>
      <author><first>Benjamin</first><last>Nye</last></author>
      <author><first>Iain</first><last>Marshall</last></author>
      <author><first>Byron C.</first><last>Wallace</last></author>
      <pages>123–132</pages>
      <abstract>How do we most effectively treat a disease or condition? Ideally, we could consult a database of evidence gleaned from clinical trials to answer such questions. Unfortunately, no such database exists; clinical trial results are instead disseminated primarily via lengthy natural language articles. Perusing all such articles would be prohibitively time-consuming for healthcare practitioners; they instead tend to depend on manually compiled <i>systematic reviews</i> of medical literature to inform care. NLP may speed this process up, and eventually facilitate immediate consult of published evidence. The <i>Evidence Inference</i> dataset was recently released to facilitate research toward this end. This task entails inferring the comparative performance of two treatments, with respect to a given outcome, from a particular article (describing a clinical trial) and identifying supporting evidence. For instance: Does this article report that <i>chemotherapy</i> performed better than <i>surgery</i> for <i>five-year survival rates</i> of operable cancers? In this paper, we collect additional annotations to expand the Evidence Inference dataset by 25%, provide stronger baseline models, systematically inspect the errors that these make, and probe dataset quality. We also release an <i>abstract only</i> (as opposed to full-texts) version of the task for rapid model prototyping. The updated corpus, documentation, and code for new baselines and evaluations are available at <url>http://evidence-inference.ebm-nlp.com/</url>.</abstract>
      <url hash="672114b7">2020.bionlp-1.13</url>
      <doi>10.18653/v1/2020.bionlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Personalized Early Stage <fixed-case>A</fixed-case>lzheimer’s Disease Detection: A Case Study of President <fixed-case>R</fixed-case>eagan’s Speeches</title>
      <author><first>Ning</first><last>Wang</last></author>
      <author><first>Fan</first><last>Luo</last></author>
      <author><first>Vishal</first><last>Peddagangireddy</last></author>
      <author><first>Koduvayur</first><last>Subbalakshmi</last></author>
      <author><first>Rajarathnam</first><last>Chandramouli</last></author>
      <pages>133–139</pages>
      <abstract>Alzheimer’s disease (AD)-related global healthcare cost is estimated to be $1 trillion by 2050. Currently, there is no cure for this disease; however, clinical studies show that early diagnosis and intervention helps to extend the quality of life and inform technologies for personalized mental healthcare. Clinical research indicates that the onset and progression of Alzheimer’s disease lead to dementia and other mental health issues. As a result, the language capabilities of patient start to decline. In this paper, we show that machine learning-based unsupervised clustering of and anomaly detection with linguistic biomarkers are promising approaches for intuitive visualization and personalized early stage detection of Alzheimer’s disease. We demonstrate this approach on 10 year’s (1980 to 1989) of President Ronald Reagan’s speech data set. Key linguistic biomarkers that indicate early-stage AD are identified. Experimental results show that Reagan had early onset of Alzheimer’s sometime between 1983 and 1987. This finding is corroborated by prior work that analyzed his interviews using a statistical technique. The proposed technique also identifies the exact speeches that reflect linguistic biomarkers for early stage AD.</abstract>
      <url hash="7b1338af">2020.bionlp-1.14</url>
      <doi>10.18653/v1/2020.bionlp-1.14</doi>
      <video tag="video" href="http://slideslive.com/38929645"/>
    </paper>
    <paper id="15">
      <title><fixed-case>B</fixed-case>io<fixed-case>MRC</fixed-case>: A Dataset for Biomedical Machine Reading Comprehension</title>
      <author><first>Dimitris</first><last>Pappas</last></author>
      <author><first>Petros</first><last>Stavropoulos</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <author><first>Ryan</first><last>McDonald</last></author>
      <pages>140–149</pages>
      <abstract>We introduceBIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.</abstract>
      <url hash="ef622367">2020.bionlp-1.15</url>
      <doi>10.18653/v1/2020.bionlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Neural Transduction of Letter Position Dyslexia using an Anagram Matrix Representation</title>
      <author><first>Avi</first><last>Bleiweiss</last></author>
      <pages>150–155</pages>
      <abstract>Research on analyzing reading patterns of dyslectic children has mainly been driven by classifying dyslexia types offline. We contend that a framework to remedy reading errors inline is more far-reaching and will help to further advance our understanding of this impairment. In this paper, we propose a simple and intuitive neural model to reinstate migrating words that transpire in letter position dyslexia, a visual analysis deficit to the encoding of character order within a word. Introduced by the anagram matrix representation of an input verse, the novelty of our work lies in the expansion from one to a two dimensional context window for training. This warrants words that only differ in the disposition of letters to remain interpreted semantically similar in the embedding space. Subject to the apparent constraints of the self-attention transformer architecture, our model achieved a unigram BLEU score of 40.6 on our reconstructed dataset of the Shakespeare sonnets.</abstract>
      <url hash="ab96024b">2020.bionlp-1.16</url>
      <doi>10.18653/v1/2020.bionlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Domain Adaptation and Instance Selection for Disease Syndrome Classification over Veterinary Clinical Notes</title>
      <author><first>Brian</first><last>Hur</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <author><first>Laura</first><last>Hardefeldt</last></author>
      <author><first>James</first><last>Gilkerson</last></author>
      <pages>156–166</pages>
      <abstract>Identifying the reasons for antibiotic administration in veterinary records is a critical component of understanding antimicrobial usage patterns. This informs antimicrobial stewardship programs designed to fight antimicrobial resistance, a major health crisis affecting both humans and animals in which veterinarians have an important role to play. We propose a document classification approach to determine the reason for administration of a given drug, with particular focus on domain adaptation from one drug to another, and instance selection to minimize annotation effort.</abstract>
      <url hash="b5dc1509">2020.bionlp-1.17</url>
      <doi>10.18653/v1/2020.bionlp-1.17</doi>
      <video tag="video" href="http://slideslive.com/38929650"/>
    </paper>
    <paper id="18">
      <title>Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings</title>
      <author><first>David</first><last>Chang</last></author>
      <author><first>Ivana</first><last>Balažević</last></author>
      <author><first>Carl</first><last>Allen</last></author>
      <author><first>Daniel</first><last>Chawla</last></author>
      <author><first>Cynthia</first><last>Brandt</last></author>
      <author><first>Andrew</first><last>Taylor</last></author>
      <pages>167–176</pages>
      <abstract>Much of biomedical and healthcare data is encoded in discrete, symbolic form such as text and medical codes. There is a wealth of expert-curated biomedical domain knowledge stored in knowledge bases and ontologies, but the lack of reliable methods for learning knowledge representation has limited their usefulness in machine learning applications. While text-based representation learning has significantly improved in recent years through advances in natural language processing, attempts to learn biomedical concept embeddings so far have been lacking. A recent family of models called knowledge graph embeddings have shown promising results on general domain knowledge graphs, and we explore their capabilities in the biomedical domain. We train several state-of-the-art knowledge graph embedding models on the SNOMED-CT knowledge graph, provide a benchmark with comparison to existing methods and in-depth discussion on best practices, and make a case for the importance of leveraging the multi-relational nature of knowledge graphs for learning biomedical knowledge representation. The embeddings, code, and materials will be made available to the community.</abstract>
      <url hash="c674e165">2020.bionlp-1.18</url>
      <attachment type="Software" hash="7b45845f">2020.bionlp-1.18.Software.zip</attachment>
      <doi>10.18653/v1/2020.bionlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Extensive Error Analysis and a Learning-Based Evaluation of Medical Entity Recognition Systems to Approximate User Experience</title>
      <author><first>Isar</first><last>Nejadgholi</last></author>
      <author><first>Kathleen C.</first><last>Fraser</last></author>
      <author><first>Berry</first><last>de Bruijn</last></author>
      <pages>177–186</pages>
      <abstract>When comparing entities extracted by a medical entity recognition system with gold standard annotations over a test set, two types of mismatches might occur, label mismatch or span mismatch. Here we focus on span mismatch and show that its severity can vary from a serious error to a fully acceptable entity extraction due to the subjectivity of span annotations. For a domain-specific BERT-based NER system, we showed that 25% of the errors have the same labels and overlapping span with gold standard entities. We collected expert judgement which shows more than 90% of these mismatches are accepted or partially accepted by the user. Using the training set of the NER system, we built a fast and lightweight entity classifier to approximate the user experience of such mismatches through accepting or rejecting them. The decisions made by this classifier are used to calculate a learning-based F-score which is shown to be a better approximation of a forgiving user’s experience than the relaxed F-score. We demonstrated the results of applying the proposed evaluation metric for a variety of deep learning medical entity recognition models trained with two datasets.</abstract>
      <url hash="89a62594">2020.bionlp-1.19</url>
      <doi>10.18653/v1/2020.bionlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>A Data-driven Approach for Noise Reduction in Distantly Supervised Biomedical Relation Extraction</title>
      <author><first>Saadullah</first><last>Amin</last></author>
      <author><first>Katherine Ann</first><last>Dunfield</last></author>
      <author><first>Anna</first><last>Vechkaeva</last></author>
      <author><first>Guenter</first><last>Neumann</last></author>
      <pages>187–194</pages>
      <abstract>Fact triples are a common form of structured knowledge used within the biomedical domain. As the amount of unstructured scientific texts continues to grow, manual annotation of these texts for the task of relation extraction becomes increasingly expensive. Distant supervision offers a viable approach to combat this by quickly producing large amounts of labeled, but considerably noisy, data. We aim to reduce such noise by extending an entity-enriched relation classification BERT model to the problem of multiple instance learning, and defining a simple data encoding scheme that significantly reduces noise, reaching state-of-the-art performance for distantly-supervised biomedical relation extraction. Our approach further encodes knowledge about the direction of relation triples, allowing for increased focus on relation learning by reducing noise and alleviating the need for joint learning with knowledge graph completion.</abstract>
      <url hash="51024790">2020.bionlp-1.20</url>
      <attachment type="Software" hash="c59c9acc">2020.bionlp-1.20.Software.zip</attachment>
      <doi>10.18653/v1/2020.bionlp-1.20</doi>
      <video tag="video" href="http://slideslive.com/38929644"/>
    </paper>
    <paper id="21">
      <title>Global Locality in Biomedical Relation and Event Extraction</title>
      <author><first>Elaheh</first><last>ShafieiBavani</last></author>
      <author><first>Antonio</first><last>Jimeno Yepes</last></author>
      <author><first>Xu</first><last>Zhong</last></author>
      <author><first>David</first><last>Martinez Iraola</last></author>
      <pages>195–204</pages>
      <abstract>Due to the exponential growth of biomedical literature, event and relation extraction are important tasks in biomedical text mining. Most work only focus on relation extraction, and detect a single entity pair mention on a short span of text, which is not ideal due to long sentences that appear in biomedical contexts. We propose an approach to both relation and event extraction, for simultaneously predicting relationships between all mention pairs in a text. We also perform an empirical study to discuss different network setups for this purpose. The best performing model includes a set of multi-head attentions and convolutions, an adaptation of the transformer architecture, which offers self-attention the ability to strengthen dependencies among related elements, and models the interaction between features extracted by multiple attention heads. Experiment results demonstrate that our approach outperforms the state of the art on a set of benchmark biomedical corpora including BioNLP 2009, 2011, 2013 and BioCreative 2017 shared tasks.</abstract>
      <url hash="3cb56dbd">2020.bionlp-1.21</url>
      <doi>10.18653/v1/2020.bionlp-1.21</doi>
    </paper>
    <paper id="22">
      <title>An Empirical Study of Multi-Task Learning on <fixed-case>BERT</fixed-case> for Biomedical Text Mining</title>
      <author><first>Yifan</first><last>Peng</last></author>
      <author><first>Qingyu</first><last>Chen</last></author>
      <author><first>Zhiyong</first><last>Lu</last></author>
      <pages>205–214</pages>
      <abstract>Multi-task learning (MTL) has achieved remarkable success in natural language processing applications. In this work, we study a multi-task learning model with multiple decoders on varieties of biomedical and clinical natural language processing tasks such as text similarity, relation extraction, named entity recognition, and text inference. Our empirical results demonstrate that the MTL fine-tuned models outperform state-of-the-art transformer models (e.g., BERT and its variants) by 2.0% and 1.3% in biomedical and clinical domain adaptation, respectively. Pairwise MTL further demonstrates more details about which tasks can improve or decrease others. This is particularly helpful in the context that researchers are in the hassle of choosing a suitable model for new problems. The code and models are publicly available at https://github.com/ncbi-nlp/bluebert.</abstract>
      <url hash="a8dc691e">2020.bionlp-1.22</url>
      <doi>10.18653/v1/2020.bionlp-1.22</doi>
    </paper>
  </volume>
</collection>
