<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.arabicnlp">
  <volume id="main" ingest-date="2025-10-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of The Third Arabic Natural Language Processing Conference</booktitle>
      <editor><first>Kareem</first><last>Darwish</last></editor>
      <editor><first>Ahmed</first><last>Ali</last></editor>
      <editor><first>Ibrahim</first><last>Abu Farha</last></editor>
      <editor><first>Samia</first><last>Touileb</last></editor>
      <editor><first>Imed</first><last>Zitouni</last></editor>
      <editor><first>Ahmed</first><last>Abdelali</last></editor>
      <editor><first>Sharefah</first><last>Al-Ghamdi</last></editor>
      <editor><first>Sakhar</first><last>Alkhereyf</last></editor>
      <editor><first>Wajdi</first><last>Zaghouani</last></editor>
      <editor><first>Salam</first><last>Khalifa</last></editor>
      <editor><first>Badr</first><last>AlKhamissi</last></editor>
      <editor><first>Rawan</first><last>Almatham</last></editor>
      <editor><first>Injy</first><last>Hamed</last></editor>
      <editor><first>Zaid</first><last>Alyafeai</last></editor>
      <editor><first>Areeb</first><last>Alowisheq</last></editor>
      <editor><first>Go</first><last>Inoue</last></editor>
      <editor><first>Khalil</first><last>Mrini</last></editor>
      <editor><first>Waad</first><last>Alshammari</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>November</month>
      <year>2025</year>
      <url hash="287a013b">2025.arabicnlp-main</url>
      <venue>arabicnlp</venue>
      <isbn>979-8-89176-352-4</isbn>
      <doi>10.18653/v1/2025.arabicnlp-main</doi>
    </meta>
    <frontmatter>
      <url hash="86137428">2025.arabicnlp-main.0</url>
      <bibkey>arabicnlp-ws-2025-main</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Adapting Falcon3-7<fixed-case>B</fixed-case> Language Model for <fixed-case>A</fixed-case>rabic: Methods, Challenges, and Outcomes</title>
      <author><first>Basma El Amel</first><last>Boussaha</last><affiliation>Technology and Innovation Institute</affiliation></author>
      <author><first>Mohammed</first><last>Alyafeai</last></author>
      <author id="ahmed-alzubaidi" orcid="0000-0002-7313-5649"><first>Ahmed</first><last>Alzubaidi</last><affiliation>TII</affiliation></author>
      <author><first>Leen</first><last>Al Qadi</last></author>
      <author><first>Shaikha</first><last>Alsuwaidi</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author id="hakim-hacid" orcid="0000-0003-2265-9343"><first>Hakim</first><last>Hacid</last><affiliation>TII</affiliation></author>
      <pages>1-15</pages>
      <abstract>Under-represented languages suffer from a lack of data, and as a result, there are few LLMs that support them. Extending an existing LLM to a new language is a practical option for startups, university labs, and organizations with limited budgets. This process involves several steps. In this paper, we describe how we adapted the Falcon3-7B model to Arabic, covering everything from data collection and training to evaluation. Falcon-Arabic was trained exclusively on native data to better capture the cultural and linguistic aspects of the language. Our evaluations show that Falcon-Arabic achieves state-of-the-art results on a range of Arabic benchmarks.</abstract>
      <url hash="e3d2e7ab">2025.arabicnlp-main.1</url>
      <bibkey>boussaha-etal-2025-adapting</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>A</fixed-case>rab<fixed-case>J</fixed-case>obs: A Multinational Corpus of <fixed-case>A</fixed-case>rabic Job Ads</title>
      <author id="mo-el-haj" orcid="0000-0002-6136-3898"><first>Mo</first><last>El-Haj</last><affiliation>VinUniversity and Lancaster University</affiliation></author>
      <pages>16-25</pages>
      <abstract>ArabJobs is a publicly available corpus of Arabic job advertisements collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates. Comprising over 8,500 postings and more than 550,000 words, the dataset captures linguistic, regional, and socio-economic variation in the Arab labour market. We present analyses of gender representation and occupational structure, and highlight dialectal variation across ads, which offers opportunities for future research. We also demonstrate applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification. The findings show the utility of ArabJobs for fairness-aware Arabic NLP and labour market research. The dataset is publicly available on GitHub: https://github.com/drelhaj/ArabJobs.</abstract>
      <url hash="2ece545e">2025.arabicnlp-main.2</url>
      <bibkey>el-haj-2025-arabjobs</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>S</fixed-case>emitic Root Encoding: Tokenization Based on the Templatic Morphology of <fixed-case>S</fixed-case>emitic Languages in <fixed-case>NMT</fixed-case></title>
      <author><first>Brendan T.</first><last>Hatch</last><affiliation>Brigham Young University and The MITRE Corporation</affiliation></author>
      <author><first>Stephen D.</first><last>Richardson</last><affiliation>Brigham Young University</affiliation></author>
      <pages>26-41</pages>
      <abstract>The morphological structure of Semitic languages, such as Arabic, is based on non-concatenative roots and templates. This complex word structure used by humans is obscured to neural models that employ traditional tokenization algorithms, such as byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994). In this work, we present and evaluate Semitic Root Encoding (SRE), a tokenization method that represents both concatenative and non-concatenative structures in Semitic words with sequences of root, template stem, and BPE tokens. We apply the method to neural machine translation (NMT) and find that SRE tokenization yields an average increase of 1.15 BLEU over the baseline. SRE tokenization is also robust against generating combinations of roots with template stems that do not occur in nature. Finally, we compare the performance of SRE to tokenization based on non-linguistic root and template structures and tokenization based on stems, providing evidence that NMT models are capable of leveraging tokens based on non-concatenative Semitic morphology.</abstract>
      <url hash="56041e20">2025.arabicnlp-main.3</url>
      <bibkey>hatch-richardson-2025-semitic</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.3</doi>
    </paper>
    <paper id="4">
      <title>3<fixed-case>LM</fixed-case>: Bridging <fixed-case>A</fixed-case>rabic, <fixed-case>STEM</fixed-case>, and Code through Benchmarking</title>
      <author><first>Basma El Amel</first><last>Boussaha</last><affiliation>Technology and Innovation Institute</affiliation></author>
      <author><first>Leen</first><last>Al Qadi</last></author>
      <author><first>Mugariya</first><last>Farooq</last></author>
      <author><first>Shaikha</first><last>Alsuwaidi</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Giulia</first><last>Campesan</last></author>
      <author id="ahmed-alzubaidi" orcid="0000-0002-7313-5649"><first>Ahmed</first><last>Alzubaidi</last><affiliation>TII</affiliation></author>
      <author><first>Mohammed</first><last>Alyafeai</last></author>
      <author id="hakim-hacid" orcid="0000-0003-2265-9343"><first>Hakim</first><last>Hacid</last><affiliation>TII</affiliation></author>
      <pages>42-63</pages>
      <abstract>Arabic is one of the most widely spoken languages in the world, yet efforts to develop and evaluate Large Language Models (LLMs) for Arabic remain relatively limited. Most existing Arabic benchmarks focus on linguistic, cultural, or religious content, leaving a significant gap in areas like STEM and coding domains that are increasingly relevant for real-world LLM applications. To help bridge this gap, we present <b>3LM</b>, a suite of <b>three</b> benchmarks designed specifically for Arabic. The first is a set of STEM-related question-answer pairs, naturally sourced from Arabic textbooks and educational worksheets. The second consists of synthetically generated STEM questions, created using the same sources. The third benchmark focuses on code generation, built through a careful translation of two widely used code benchmarks, incorporating a human-in-the-loop process with several rounds of review to ensure high-quality and faithful translations. We release all three benchmarks publicly to support the growth of Arabic LLM research in these essential but underrepresented areas.</abstract>
      <url hash="612aad74">2025.arabicnlp-main.4</url>
      <bibkey>boussaha-etal-2025-3lm</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>T</fixed-case>uni<fixed-case>F</fixed-case>ra: A <fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabic Speech Corpus with Orthographic Transcriptions and <fixed-case>F</fixed-case>rench Translations</title>
      <author><first>Alex</first><last>Choux</last></author>
      <author><first>Marko</first><last>Avila</last><affiliation>Systran</affiliation></author>
      <author id="josep-m-crego" orcid="0009-0006-8034-3581"><first>Josep</first><last>Crego</last><affiliation>SYSTRAN By ChapsVision</affiliation></author>
      <author><first>Fethi</first><last>Bougares</last><affiliation>elyadata</affiliation></author>
      <author id="antoine-laurent" orcid="0000-0002-2653-1008"><first>Antoine</first><last>Laurent</last><affiliation>Universite du Maine, Université du Maine</affiliation></author>
      <pages>64-68</pages>
      <abstract>We introduce TuniFra, a novel and comprehensive corpus developed to advance research in Automatic Speech Recognition (ASR) and Speech-to-Text Translation (STT) for Tunisian Arabic, a notably low-resourced language variety. The TuniFra corpus comprises 15 hours of native Tunisian Arabic speech, carefully transcribed and manually translated into French. While the development of ASR and STT systems for major languages is supported by extensive datasets, low-resource languages such as Tunisian Arabic face significant challenges due to limited training data, particularly for speech technologies. TuniFra addresses this gap by offering a valuable resource tailored for both ASR and STT tasks in the Tunisian dialect. We describe our methodology for data collection, transcription, and annotation, and present initial baseline results for both Tunisian Arabic speech recognition and Tunisian Arabic–French speech translation.</abstract>
      <url hash="b87c8ff5">2025.arabicnlp-main.5</url>
      <bibkey>choux-etal-2025-tunifra</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.5</doi>
    </paper>
    <paper id="6">
      <title>The Cross-Lingual Cost: Retrieval Biases in <fixed-case>RAG</fixed-case> over <fixed-case>A</fixed-case>rabic-<fixed-case>E</fixed-case>nglish Corpora</title>
      <author id="chen-amiraz" orcid="0000-0001-7539-4903"><first>Chen</first><last>Amiraz</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Yaroslav</first><last>Fyodorov</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Elad</first><last>Haramaty</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author><first>Zohar</first><last>Karnin</last><affiliation>tii</affiliation></author>
      <author><first>Liane</first><last>Lewin-Eytan</last><affiliation>TII</affiliation></author>
      <pages>69-83</pages>
      <abstract>Cross-lingual retrieval-augmented generation (RAG) is a critical capability for retrieving and generating answers across languages. Prior work in this context has mostly focused on generation and relied on benchmarks derived from open-domain sources, most notably Wikipedia. In such settings, retrieval challenges often remain hidden due to language imbalances, overlap with pretraining data, and memorized content. To address this gap, we study Arabic-English RAG in a domain-specific setting using benchmarks derived from real-world corporate datasets. Our benchmarks include all combinations of languages for the user query and the supporting document, drawn independently and uniformly at random. This enables a systematic study of multilingual retrieval behavior.Our findings reveal that retrieval is a critical bottleneck in cross-lingual domain-specific scenarios, with substantial performance drops occurring when the user query and supporting document languages differ. A key insight is that these failures stem primarily from the retriever’s difficulty in ranking documents across languages. Finally, we propose two simple retrieval strategies that address this source of failure by enforcing equal retrieval from both languages or by translating the query, resulting in substantial improvements in cross-lingual and overall performance. These results highlight meaningful opportunities for improving multilingual retrieval, particularly in practical, real-world RAG applications.</abstract>
      <url hash="3abc3453">2025.arabicnlp-main.6</url>
      <bibkey>amiraz-etal-2025-cross</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.6</doi>
    </paper>
    <paper id="7">
      <title>Open-domain <fixed-case>A</fixed-case>rabic Conversational Question Answering with Question Rewriting</title>
      <author><first>Mariam E.</first><last>Hassib</last></author>
      <author id="nagwa-m-el-makky" orcid="0000-0002-9571-7543"><first>Nagwa</first><last>El-Makky</last></author>
      <author><first>Marwan</first><last>Torki</last><affiliation>Alexandria University</affiliation></author>
      <pages>84-96</pages>
      <abstract>Conversational question-answering (CQA) plays a crucial role in bridging the gap between human language and machine understanding, enabling more natural and interactive interactions with AI systems. In this work, we present the first results on open-domain Arabic CQA using deep learning. We introduce AraQReCC, a large-scale Arabic CQA dataset containing 9K conversations with 62K question-answer pairs, created by translating a subset of the QReCC dataset. To ensure data quality, we used COMET-based filtering and manual ratings from large language models (LLMs), such as GPT-4 and LLaMA, selecting conversations with COMET scores, along with LLM ratings of 4 or more. AraQReCC facilitates advanced research in Arabic CQA, improving clarity and relevance through question rewriting. We applied AraT5 for question rewriting and used BM25 and Dense Passage Retrieval (DPR) for passage retrieval. AraT5 is also used for question answering, completing the end-to-end system. Our experiments show that the best performance is achieved with DPR, attaining an F1 score of 21.51% on the test set. While this falls short of the human upper bound of 40.22%, it underscores the importance of question rewriting and quality-controlled data in enhancing system performance.</abstract>
      <url hash="455dea7b">2025.arabicnlp-main.7</url>
      <bibkey>hassib-etal-2025-open</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>ATHAR</fixed-case>: A High-Quality and Diverse Dataset for Classical <fixed-case>A</fixed-case>rabic to <fixed-case>E</fixed-case>nglish Translation</title>
      <author><first>Mohammed Sabry</first><last>Mohammed</last></author>
      <author><first>Mohammed</first><last>Khalil</last></author>
      <pages>97-106</pages>
      <abstract>Classical Arabic represents a significant era that encompasses the golden age of Arab culture, philosophy, and scientific literature. With a broad consensus on the importance of translating these literatures to enrich knowledge dissemination across communities, the advent of large language models (LLMs) and translation systems offers promising tools to facilitate this goal. However, we have identified a scarcity of translation datasets in Classical Arabic, which are often limited in scope and topics, hindering the development of high-quality translation systems. In response, we present the ATHAR dataset, which comprises 66,000 high-quality classical Arabic to English translation samples that cover a wide array of topics including science, culture, and philosophy. Furthermore, we assess the performance of current state-of-the-art LLMs under various settings, concluding that there is a need for such datasets in current systems. Our findings highlight how models can benefit from fine-tuning or incorporating this dataset into their pretraining pipelines. The dataset is publicly available on the HuggingFace Data Hub. To preserve anonymity during review, we additionally provide an anonymized snapshot at https://drive.google.com/drive/folders/1c_ElsblaOJzQ0TW_M1DugjR2o3Xv9RUo.</abstract>
      <url hash="f04e2ab9">2025.arabicnlp-main.8</url>
      <bibkey>mohammed-khalil-2025-athar</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.8</doi>
    </paper>
    <paper id="9">
      <title><tex-math>\mathbf{A-SEA}^3\mathbf{L}</tex-math>-<fixed-case>QA</fixed-case>: A Fully Automated Self-Evolving, Adversarial Workflow for <fixed-case>A</fixed-case>rabic Long-Context Question-Answer Generation</title>
      <author><first>Kesen</first><last>Wang</last></author>
      <author><first>Daulet</first><last>Toibazar</last><affiliation>Humain AI</affiliation></author>
      <author><first>Pedro J</first><last>Moreno Mengibar</last></author>
      <pages>107-116</pages>
      <abstract>We present an end-to-end, self-evolving adversarial workflow for long-context Question-Answer (QA) Generation in Arabic. By orchestrating multiple specialized LVLMs: a question generator, an evaluator, and a swarm of answer generators, our system iteratively refines its own performance without any human intervention. Starting from raw, multi-page Arabic documents across diverse domains, the question generator produces fine-grained, context-aware queries to be tackled by the answer generator swarm, and the evaluator assesses and feeds back quality metrics. This closed-loop cycle enables continuous learning: low-confidence outputs trigger automated re-generation and model updates, progressively enhancing question difficulty and relevance. Moreover, we set the quality metrics as a tunable hyperparameter, enabling question generation at controllable and customizable difficulty levels. We release <i>
          <b>AraLongBench</b></i>, a large-scale Arabic benchmark of single- and multi-page challenges spanning hundreds of pages, and demonstrate that our self-evolving workflow substantially outperform static pipelines, markedly boosting the long-context comprehension capabilities of leading Arabic Large Vision Language Models (LVLMs). Lastly, we also meticulously architect a fully automated agentic workflow for long-context Arabic document collection.</abstract>
      <url hash="e8e756ed">2025.arabicnlp-main.9</url>
      <bibkey>wang-etal-2025-sea3l</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.9</doi>
    </paper>
    <paper id="10">
      <title>Lemmatizing Dialectal <fixed-case>A</fixed-case>rabic with Sequence-to-Sequence Models</title>
      <author><first>Mostafa</first><last>Saeed</last></author>
      <author id="nizar-habash" orcid="0000-0002-1831-3457"><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>117-129</pages>
      <abstract>Lemmatization for dialectal Arabic poses many challenges due to the lack of orthographic standards and limited morphological analyzers. This work explores the effectiveness of Seq2Seq models for lemmatizing dialectal Arabic, both without analyzers and with their integration. We assess how well these models generalize across dialects and benefit from related varieties. Focusing on Egyptian, Gulf, and Levantine dialects with varying resource levels, our analysis highlights both the potential and limitations of data-driven approaches. The proposed method achieves significant gains over baselines, performing well in both low-resource and dialect-rich scenarios.</abstract>
      <url hash="695209e1">2025.arabicnlp-main.10</url>
      <bibkey>saeed-habash-2025-lemmatizing</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.10</doi>
    </paper>
    <paper id="11">
      <title>Saudi-Alignment Benchmark: Assessing <fixed-case>LLM</fixed-case>s Alignment with Cultural Norms and Domain Knowledge in the Saudi Context</title>
      <author><first>Manal</first><last>Alhassoun</last><affiliation>King Abdulaziz City for Science and Technology</affiliation></author>
      <author><first>Imaan Mohammed</first><last>Alkhanen</last><affiliation>AI and robotics</affiliation></author>
      <author><first>Nouf</first><last>Alshalawi</last><affiliation>King Abdulaziz City for Science and Technology</affiliation></author>
      <author id="ibtehal-baazeem" orcid="0000-0002-7531-0666"><first>Ibtehal</first><last>Baazeem</last><affiliation>KACST</affiliation></author>
      <author><first>Waleed</first><last>Alsanie</last><affiliation>King Abdulaziz City for Science and Technology (KACST)</affiliation></author>
      <pages>130-147</pages>
      <abstract>For effective use in specific countries, Large Language Models (LLMs) need a strong grasp of local culture and core knowledge to ensure socially appropriate, context-aware, and factually correct responses. Existing Arabic and Saudi benchmarks are limited, focusing mainly on dialects or lifestyle, with little attention to deeper cultural or domain-specific alignment from authoritative sources. To address this gap and the challenge LLMs face with non-Western cultural nuance, this study introduces the Saudi-Alignment Benchmark. It consists of 874 manually curated questions across two core cultural dimensions: Saudi Cultural and Ethical Norms, and Saudi Domain Knowledge. These questions span multiple subcategories and use three formats to assess different goals with verified sources. Our evaluation reveals significant variance in LLM alignment. GPT-4 achieved the highest overall accuracy (83.3%), followed by ALLaM-7B (81.8%) and Llama-3.3-70B (81.6%), whereas Jais-30B exhibited a pronounced shortfall at 21.9%. Furthermore, multilingual LLMs excelled in norms; ALLaM-7B in domain knowledge. Considering the effect of question format, LLMs generally excelled in selected-response formats but showed weaker results on generative tasks, indicating that recognition-based benchmarks alone may overestimate cultural and contextual alignment. These findings highlight the need for tailored benchmarks and reveal LLMs’ limitations in achieving cultural grounding, particularly in underrepresented contexts like Saudi Arabia.</abstract>
      <url hash="2258a60f">2025.arabicnlp-main.11</url>
      <bibkey>alhassoun-etal-2025-saudi</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.11</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>allu<fixed-case>E</fixed-case>val: A Fine-grained Hallucination Evaluation Framework for <fixed-case>A</fixed-case>rabic <fixed-case>LLM</fixed-case>s</title>
      <author><first>Aisha</first><last>Alansari</last></author>
      <author id="hamzah-luqman" orcid="0000-0001-7944-5093"><first>Hamzah</first><last>Luqman</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <pages>148-161</pages>
      <abstract>Recently, extensive research on the hallucination of the large language models (LLMs) has mainly focused on the English language. Despite the growing number of multilingual and Arabic-specific LLMs, evaluating LLMs’ hallucination in the Arabic context remains relatively underexplored. The knowledge gap is particularly pressing given Arabic’s widespread use across many regions and its importance in global communication and media. This paper presents the first comprehensive hallucination evaluation of Arabic and multilingual LLMs on two critical Arabic natural language generation tasks: generative question answering (GQA) and summarization. This study evaluates a total of 12 LLMs, including 4 Arabic pre-trained models, 4 multilingual models, and 4 reasoning-based models. To assess the factual consistency and faithfulness of LLMs’ outputs, we developed a fine-grained hallucination evaluation framework consisting of 12 fine-grained hallucination indicators that represent the varying characteristics of each task. The results reveal that factual hallucinations are more prevalent than faithfulness errors across all models and tasks. Notably, the Arabic pre-trained model Allam consistently demonstrates lower hallucination rates than multilingual models and a comparative performance with reasoning-based models. The code is available at: https://github.com/aishaalansari57/AraHalluEval</abstract>
      <url hash="730aed46">2025.arabicnlp-main.12</url>
      <bibkey>alansari-luqman-2025-arahallueval</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.12</doi>
    </paper>
    <paper id="13">
      <title>Evaluating Prompt Relevance in <fixed-case>A</fixed-case>rabic Automatic Essay Scoring: Insights from Synthetic and Real-World Data</title>
      <author><first>Chatrine</first><last>Qwaider</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Chalmers University of Technology</affiliation></author>
      <author id="kirill-chirkunov" orcid="0000-0002-2328-6035"><first>Kirill</first><last>Chirkunov</last></author>
      <author id="bashar-alhafni"><first>Bashar</first><last>Alhafni</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author id="nizar-habash" orcid="0000-0002-1831-3457"><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author id="ted-briscoe"><first>Ted</first><last>Briscoe</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>162-178</pages>
      <abstract>Prompt relevance is a critical yet underexplored dimension in Arabic Automated Essay Scoring (AES). We present the first systematic study of binary prompt-essay relevance classification, supporting both AES scoring and dataset annotation. To address data scarcity, we built a synthetic dataset of on-topic and off-topic pairs and evaluated multiple models, including threshold-based classifiers, SVMs, causal LLMs, and a fine-tuned masked SBERT model. For real-data evaluation, we combined QAES with ZAEBUC, creating off-topic pairs via mismatched prompts. We also tested prompt expansion strategies using AraVec, CAMeL, and GPT-4o. Our fine-tuned SBERT achieved 98% F1 on synthetic data and strong results on QAES+ZAEBUC, outperforming SVMs and threshold-based baselines and offering a resource-efficient alternative to LLMs. This work establishes the first benchmark for Arabic prompt relevance and provides practical strategies for low-resource AES.</abstract>
      <url hash="caaf3487">2025.arabicnlp-main.13</url>
      <bibkey>qwaider-etal-2025-evaluating</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>W</fixed-case>ojood<fixed-case>O</fixed-case>ntology: Ontology-Driven <fixed-case>LLM</fixed-case> Prompting for Unified Information Extraction Tasks</title>
      <author><first>Alaa</first><last>Aljabari</last><affiliation>Birzeit University</affiliation></author>
      <author id="nagham-hamad" orcid="0000-0003-3898-1548"><first>Nagham</first><last>Hamad</last><affiliation>Birzeit University and Palestine Technical University - Kadoorie</affiliation></author>
      <author id="mohammed-khalilia" orcid="0000-0001-6086-6795"><first>Mohammed</first><last>Khalilia</last><affiliation>Birzeit University and Qualtrics XM</affiliation></author>
      <author id="mustafa-jarrar" orcid="0000-0003-4351-4207"><first>Mustafa</first><last>Jarrar</last><affiliation>Birzeit University</affiliation></author>
      <pages>179-193</pages>
      <abstract>Information Extraction tasks such as Named Entity Recognition and Relation Extraction are often developed using diverse tagsets and annotation guidelines. This presents major challenges for model generalization, cross-dataset evaluation, tool interoperability, and broader industry adoption. To address these issues, we propose an information extraction ontology, , which covers a wide range of named entity types and relations. serves as a semantic mediation framework that facilitates alignment across heterogeneous tagsets and annotation guidelines. We propose two ontology-based mapping methods: (i) as a set of mapping rules for uni-directional tagset alignment; and (ii) as ontology-based prompting, which incorporates the ontology concepts directly into prompts, enabling large language models (LLMs) to perform more effective and bi-directional mappings. Our experiments show a 15% improvement in out-of-domain mapping accuracy when using ontology-based prompting compared to rule-based methods. Furthermore, is aligned with Schema.org and Wikidata, enabling interoperability with knowledge graphs and facilitating broader industry adoption. The is open source and available at <url>https://sina.birzeit.edu/wojood</url>.</abstract>
      <url hash="063e8e5f">2025.arabicnlp-main.14</url>
      <bibkey>aljabari-etal-2025-wojoodontology</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.14</doi>
    </paper>
    <paper id="15">
      <title>Tahḏīb: A Rhythm-Aware Phrase Insertion for Classical <fixed-case>A</fixed-case>rabic Poetry Composition</title>
      <author><first>Mohamad</first><last>Elzohbi</last><affiliation>University of Calgary</affiliation></author>
      <author><first>Richard</first><last>Zhao</last><affiliation>University of Calgary</affiliation></author>
      <pages>194-202</pages>
      <abstract>This paper presents a methodology for inserting phrases in Arabic poems to conform to a specific rhythm using ByT5, a byte-level multilingual transformer-based model. Our work discusses a rule-based grapheme-to-beat transformation tailored for extracting the rhythm from fully diacritized Arabic script. Our approach employs a conditional denoising objective to fine-tune ByT5, where the model reconstructs masked words to match a target rhythm. We adopt a curriculum learning strategy, pre-training on a general Arabic dataset before fine-tuning on poetic dataset, and explore cross-lingual transfer from English to Arabic. Experimental results demonstrate that our models achieve high rhythmic alignment while maintaining semantic coherence. The proposed model has the potential to be used in co-creative applications in the process of composing classical Arabic poems.</abstract>
      <url hash="9f8b844c">2025.arabicnlp-main.15</url>
      <bibkey>elzohbi-zhao-2025-tahdib</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.15</doi>
    </paper>
    <paper id="16">
      <title>Can <fixed-case>LLM</fixed-case>s Directly Retrieve Passages for Answering Questions from Qur’an?</title>
      <author><first>Sohaila</first><last>Eltanbouly</last><affiliation>University of Qatar</affiliation></author>
      <author><first>Salam</first><last>Albatarni</last></author>
      <author id="shaimaa-hassanein" orcid="0000-0003-2727-5461"><first>Shaimaa</first><last>Hassanein</last></author>
      <author><first>Tamer</first><last>Elsayed</last><affiliation>Qatar University</affiliation></author>
      <pages>203-210</pages>
      <abstract>The Holy Qur’an provides timeless guidance, addressing modern challenges and offering answers to many important questions. The Qur’an QA 2023 shared task introduced the Qur’anic Passage Retrieval (QPR) task, which involves retrieving relevant passages in response to MSA questions. In this work, we evaluate the ability of seven pre-trained large language models (LLMs) to retrieve relevant passages from the Qur’an in response to given questions, considering zero-shot and several few-shot scenarios. Our experiments show that the best model, Claude, significantly outperforms the state-of-the-art QPR model by 28 points on MAP and 38 points on MRR, exhibiting an impressive improvement of about 113% and 82%, respectively.</abstract>
      <url hash="08682d22">2025.arabicnlp-main.16</url>
      <bibkey>eltanbouly-etal-2025-llms</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>A</fixed-case>rab<fixed-case>E</fixed-case>mo<fixed-case>N</fixed-case>et: A Lightweight Hybrid 2<fixed-case>D</fixed-case> <fixed-case>CNN</fixed-case>-<fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> Model with Attention for Robust <fixed-case>A</fixed-case>rabic Speech Emotion Recognition</title>
      <author id="ali-abouzeid" orcid="0009-0007-6694-0416"><first>Ali</first><last>Abouzeid</last></author>
      <author id="bilal-elbouardi" orcid="0000-0003-3558-167X"><first>Bilal</first><last>Elbouardi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Mohamed</first><last>Maged</last></author>
      <author id="shady-shehata" orcid="0000-0002-3258-6734"><first>Shady</first><last>Shehata</last><affiliation>University of Waterloo</affiliation></author>
      <pages>211-218</pages>
      <abstract>Speech emotion recognition is vital for human-computer interaction, particularly for low-resource languages like Arabic, which face challenges due to limited data and research. We introduce ArabEmoNet, a lightweight architecture designed to overcome these limitations and deliver state-of-the-art performance. Unlike previous systems relying on discrete MFCC features and 1D convolutions, which miss nuanced spectro-temporal patterns, ArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving critical emotional cues often lost in traditional methods. While recent models favor large-scale architectures with millions of parameters, ArabEmoNet achieves superior results with just 1 million parameters—90 times smaller than HuBERT base and 74 times smaller than Whisper. This efficiency makes it ideal for resource-constrained environments. ArabEmoNet advances Arabic speech emotion recognition, offering exceptional performance and accessibility for real-world applications.</abstract>
      <url hash="91aa545b">2025.arabicnlp-main.17</url>
      <bibkey>abouzeid-etal-2025-arabemonet</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.17</doi>
    </paper>
    <paper id="18">
      <title>Capturing Intra-Dialectal Variation in Qatari <fixed-case>A</fixed-case>rabic: A Corpus of Cultural and Gender Dimensions</title>
      <author><first>Houda</first><last>Bouamor</last><affiliation>Carnegie Mellon University, Qatar</affiliation></author>
      <author id="sara-al-emadi" orcid="0009-0003-9377-5293"><first>Sara</first><last>Al-Emadi</last><affiliation>Hamad Bin Khalifa University, Qatar</affiliation></author>
      <author><first>Zeinab</first><last>Ibrahim</last><affiliation>Carnegie Mellon University, Qatar</affiliation></author>
      <author><first>Hany</first><last>Fazzaa</last><affiliation>Georgetown University, Qatar</affiliation></author>
      <author><first>Aisha</first><last>Al-Sultan</last><affiliation>Doha International Family Institute</affiliation></author>
      <pages>219-230</pages>
      <abstract>We present the first publicly available, multidimensional corpus of Qatari Arabic that captures intra-dialectal variation across Urban and Bedouin speakers. While often grouped under the label of “Gulf Arabic”, Qatari Arabic exhibits rich phonological, lexical, and discourse-level differences shaped by gender, age, and sociocultural identity. Our dataset includes aligned speech and transcriptions from 255 speakers, stratified by gender and age, and collected through structured interviews on culturally salient topics such as education, heritage, and social norms. The corpus reveals systematic variation in pronunciation, vocabulary, and narrative style, offering insights for both sociolinguistic analysis and computational modeling. We also demonstrate its utility through preliminary experiments in the prediction of dialects and genders. This work provides the first large-scale, demographically balanced corpus of Qatari Arabic, laying a foundation for both sociolinguistic research and the development of dialect-aware NLP systems.</abstract>
      <url hash="a98882d1">2025.arabicnlp-main.18</url>
      <bibkey>bouamor-etal-2025-capturing</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.18</doi>
    </paper>
    <paper id="19">
      <title>Feature Engineering is not Dead: A Step Towards State of the Art for <fixed-case>A</fixed-case>rabic Automated Essay Scoring</title>
      <author><first>Marwan</first><last>Sayed</last></author>
      <author><first>Sohaila</first><last>Eltanbouly</last><affiliation>University of Qatar</affiliation></author>
      <author><first>May</first><last>Bashendy</last><affiliation>University of Qatar</affiliation></author>
      <author><first>Tamer</first><last>Elsayed</last><affiliation>Qatar University</affiliation></author>
      <pages>231-245</pages>
      <abstract>Automated Essay Scoring (AES) has shown significant advancements in educational assessment. However, under-resourced languages like Arabic have received limited attention. To bridge this gap and enable robust Arabic AES, this paper introduces the <tex-math>\textit{first publicly-available}</tex-math> comprehensive set of engineered features tailored for Arabic AES, covering surface-level, readability, lexical, syntactic, and semantic features. Experiments are conducted on a dataset of 620 Arabic essays, each annotated with both holistic and trait-specific scores. Our findings demonstrate that the proposed feature set is effective across different models and competitive with recent NLP advances including LLMs, establishing the state-of-the-art performance and providing strong baselines for future Arabic AES research. Moroever, the resulting feature set offers a reusable and foundational resource, contributing towards the development of more effective Arabic AES systems.</abstract>
      <url hash="97bf60be">2025.arabicnlp-main.19</url>
      <bibkey>sayed-etal-2025-feature</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.19</doi>
    </paper>
    <paper id="20">
      <title>Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation</title>
      <author id="abdessalam-bouchekif"><first>Abdessalam</first><last>Bouchekif</last><affiliation>Hamad Bin Khalifa University, Qatar</affiliation></author>
      <author><first>Samer</first><last>Rashwani</last></author>
      <author><first>Heba</first><last>Sbahi</last><affiliation>Hamad Bin Khalifa University, Qatar</affiliation></author>
      <author id="shahd-gaben" orcid="0009-0001-8554-9777"><first>Shahd</first><last>Gaben</last><affiliation>Hamad Bin Khalifa University, Qatar</affiliation></author>
      <author><first>Mutaz</first><last>Al Khatib</last><affiliation>Hamad Bin Khalifa University, Qatar</affiliation></author>
      <author><first>Mohammed</first><last>Ghaly</last><affiliation>Hamad Bin Khalifa University, Qatar</affiliation></author>
      <pages>246-257</pages>
      <abstract>This paper evaluates the knowledge and reasoning capabilities of Large Language Models in Islamic inheritance law, ʿilm al-mawārīth. We assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice questions covering diverse inheritance scenarios, designed to test each model’s ability—from understanding the inheritance context to computing the distribution of shares prescribed by Islamic jurisprudence. The results show a wide performance gap among models. o3 and Gemini 2.5 achieved accuracies above 90%, while ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect important differences in reasoning ability and domain adaptation.We conduct a detailed error analysis to identify recurring failure patterns across models, including misunderstandings of inheritance scenarios, incorrect application of legal rules, and insufficient domain knowledge. Our findings highlight the limitations of current models in handling structured legal reasoning and suggest directions for improving their performance in Islamic legal reasoning.</abstract>
      <url hash="e2cb471d">2025.arabicnlp-main.20</url>
      <bibkey>bouchekif-etal-2025-assessing</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>BALSAM</fixed-case>: A Platform for Benchmarking <fixed-case>A</fixed-case>rabic Large Language Models</title>
      <author><first>Rawan Nasser</first><last>Almatham</last></author>
      <author><first>Kareem Mohamed</first><last>Darwish</last><affiliation>aiXplain Inc</affiliation></author>
      <author><first>Raghad</first><last>Al-Rasheed</last><affiliation>King Salman global academy</affiliation></author>
      <author id="waad-thuwaini-alshammari" orcid="0000-0002-9166-5619"><first>Waad Thuwaini</first><last>Alshammari</last><affiliation>King Salman Global Academy for Arabic Language</affiliation></author>
      <author id="muneera-alhoshan" orcid="0000-0001-5705-3590"><first>Muneera</first><last>Alhoshan</last><affiliation>King Salman International Academy for the Arabic Language</affiliation></author>
      <author><first>Amal</first><last>Almazrua</last><affiliation>King Abdulaziz City for Science and Technology (KACST)</affiliation></author>
      <author id="asma-al-wazrah" orcid="0000-0001-5495-1694"><first>Asma</first><last>Al Wazrah</last></author>
      <author id="mais-alheraki" orcid="0000-0002-4793-8544"><first>Mais</first><last>Alheraki</last></author>
      <author id="firoj-alam" orcid="0000-0001-7172-1997"><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author id="preslav-nakov" orcid="0000-0002-3600-1510"><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author id="norah-a-alzahrani" orcid="0009-0005-4136-3195"><first>Norah A.</first><last>Alzahrani</last></author>
      <author><first>Eman</first><last>Albilali</last></author>
      <author id="nizar-habash" orcid="0000-0002-1831-3457"><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author><first>Abdelrahman Mustafa</first><last>El-Sheikh</last></author>
      <author><first>Muhammad</first><last>Elmallah</last><affiliation>aixplain</affiliation></author>
      <author id="hamdy-mubarak" orcid="0000-0003-4828-6098"><first>Hamdy</first><last>Mubarak</last></author>
      <author id="zaid-alyafeai" orcid="0009-0003-1774-5236"><first>Zaid</first><last>Alyafeai</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Mohamed</first><last>Anwar</last><affiliation>NA</affiliation></author>
      <author id="haonan-li" orcid="0000-0001-6623-5089"><first>Haonan</first><last>Li</last></author>
      <author><first>Ahmed</first><last>Abdelali</last><affiliation>Humain</affiliation></author>
      <author><first>Nora</first><last>Altwairesh</last><affiliation>Humain</affiliation></author>
      <author id="maram-hasanain" orcid="0000-0002-7466-178X"><first>Maram</first><last>Hasanain</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author id="abdulmohsen-al-thubaity" orcid="0000-0003-2376-0849"><first>Abdulmohsen</first><last>Al-Thubaity</last><affiliation>HUMAIN</affiliation></author>
      <author id="shady-shehata" orcid="0000-0002-3258-6734"><first>Shady</first><last>Shehata</last><affiliation>University of Waterloo</affiliation></author>
      <author id="bashar-alhafni"><first>Bashar</first><last>Alhafni</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Injy</first><last>Hamed</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Go</first><last>Inoue</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Khalid N.</first><last>Elmadani</last><affiliation>New York University, Abu Dhabi</affiliation></author>
      <author><first>Ossama</first><last>Obeid</last><affiliation>New York University</affiliation></author>
      <author><first>Fatima</first><last>Haouari</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Tamer</first><last>Elsayed</last><affiliation>Qatar University</affiliation></author>
      <author><first>Emad A.</first><last>Alghamdi</last><affiliation>Humain</affiliation></author>
      <author><first>Khalid</first><last>Almubarak</last><affiliation>Humain</affiliation></author>
      <author><first>Saied</first><last>Alshahrani</last><affiliation>University of Bisha</affiliation></author>
      <author><first>Ola</first><last>Aljareh</last><affiliation>King Salman Global Academy For Arabic Language, KSA</affiliation></author>
      <author><first>Safa</first><last>Alajlan</last><affiliation>King Salman Global Academy For Arabic Language, KSA</affiliation></author>
      <author><first>Areej</first><last>Alshaqarawi</last><affiliation>King Salman Global Academy For Arabic Language, KSA</affiliation></author>
      <author><first>Maryam</first><last>Alshihri</last><affiliation>King Salman Global Academy For Arabic Language, KSA</affiliation></author>
      <author><first>Sultana</first><last>Alghurabi</last><affiliation>King Salman Global Academy For Arabic Language, KSA</affiliation></author>
      <author><first>Atikah</first><last>Alzeghayer</last><affiliation>King Salman Global Academy For Arabic Language, KSA</affiliation></author>
      <author><first>Afrah</first><last>Altamimi</last><affiliation>King Salman Global Academy For Arabic Language, KSA</affiliation></author>
      <author><first>Abdullah</first><last>Alfaifi</last><affiliation>Al-Imam Mohamed Ibn Saud Islamic University and Al-Imam Mohamed Ibn Saud Islamic University</affiliation></author>
      <author><first>Abdulrahman M</first><last>Alosaimy</last><affiliation>Al-Imam Mohamed Ibn Saud Islamic University</affiliation></author>
      <pages>258-277</pages>
      <abstract>The impressive advancement of Large Language Models (LLMs) in English has not been matched across all languages. In particular, LLM performance in Arabic lags behind, due to data scarcity, linguistic diversity of Arabic and its dialects, morphological complexity, etc. Progress is further hindered by the quality of Arabic benchmarks, which typically rely on static, publicly available data, lack comprehensive task coverage, or do not provide dedicated platforms with blind test sets. This makes it challenging to measure actual progress and to mitigate data contamination. Here, we aim to bridge these gaps. In particular, we introduce BALSAM, a comprehensive, community-driven benchmark aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP tasks from 14 broad categories, with 52K examples divided into 37K test and 15K development, and a centralized, transparent platform for blind evaluation. We envision BALSAM as a unifying platform that sets standards and promotes collaborative research to advance Arabic LLM capabilities.</abstract>
      <url hash="9d8b3e44">2025.arabicnlp-main.21</url>
      <bibkey>almatham-etal-2025-balsam</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>TED</fixed-case>x<fixed-case>TN</fixed-case>: A Three-way Speech Translation Corpus for Code-Switched <fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabic - <fixed-case>E</fixed-case>nglish</title>
      <author><first>Fethi</first><last>Bougares</last><affiliation>elyadata</affiliation></author>
      <author id="salima-mdhaffar" orcid="0000-0002-8472-6890"><first>Salima</first><last>Mdhaffar</last><affiliation>Université d’Avignon</affiliation></author>
      <author><first>Haroun</first><last>Elleuch</last><affiliation>Elyadata</affiliation></author>
      <author id="yannick-esteve" orcid="0000-0002-3656-8883"><first>Yannick</first><last>Estève</last><affiliation>University of Avignon</affiliation></author>
      <pages>278-287</pages>
      <abstract>In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The. collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research studying Tunisian Dialect.</abstract>
      <url hash="18a7a1d2">2025.arabicnlp-main.22</url>
      <bibkey>bougares-etal-2025-tedxtn</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>A</fixed-case>uto<fixed-case>A</fixed-case>rabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks</title>
      <author><first>Mohamed</first><last>Eltahir</last></author>
      <author><first>Osamah</first><last>Sarraj</last></author>
      <author><first>Abdulrahman M.</first><last>Alfrihidi</last></author>
      <author><first>Taha</first><last>Alshatiri</last></author>
      <author><first>Mohammed</first><last>Khurd</last></author>
      <author><first>Mohammed</first><last>Bremoo</last></author>
      <author id="tanveer-hussain" orcid="0000-0003-4861-8347"><first>Tanveer</first><last>Hussain</last><affiliation>Edge Hill University</affiliation></author>
      <pages>288-297</pages>
      <abstract>Video-to-text and text-to-video retrieval are dominated by English benchmarks (e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet Arabic remains underserved, lacking localized evaluation metrics. We introduce a three-stage framework, AutoArabic, utilizing state-of-the-art large language models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic, reducing the manual revision required by nearly fourfold. The framework incorporates an error detection module that automatically flags potential translation errors with 97% accuracy. Applying the framework to DiDeMo, a video retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent Arabic descriptions. An analysis of the translation errors is provided and organized into an insightful taxonomy to guide future Arabic localization efforts. We train a CLIP-style baseline with identical hyperparameters on the Arabic and English variants of the benchmark, finding a moderate performance gap (<tex-math>\Delta \approx 3</tex-math>pp at Recall@1), indicating that Arabic localization preserves benchmark difficulty. We evaluate three post-editing budgets (zero/ flagged-only/ full) and find that performance improves monotonically with more post-editing, while the raw LLM output (zero-budget) remains usable. To ensure reproducibility to other languages, we made the code available at https://github.com/Tahaalshatiri/AutoArabic.</abstract>
      <url hash="cfeb08ce">2025.arabicnlp-main.23</url>
      <bibkey>eltahir-etal-2025-autoarabic</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.23</doi>
    </paper>
    <paper id="24">
      <title>Zero-Shot and Fine-Tuned Evaluation of Generative <fixed-case>LLM</fixed-case>s for <fixed-case>A</fixed-case>rabic Word Sense Disambiguation</title>
      <author><first>Yossra</first><last>Noureldien</last><affiliation>University of Khartoum</affiliation></author>
      <author><first>Abdelrazig</first><last>Mohamed</last></author>
      <author><first>Farah</first><last>Attallah</last></author>
      <pages>298-305</pages>
      <abstract>Arabic presents unique challenges for sense level language understanding due to its rich morphology and semantic ambiguity. This paper benchmarks large generative language models (LLMs) for Arabic Word Sense Disambiguation (WSD) under both zero-shot and fine-tuning conditions. We evaluate one proprietary model (GPT-4o) and three opensource models (LLaMA 3.1-8B, Qwen 2.5-7B, and Gemma 2-9B) on two publicly available datasets. In zero-shot settings, GPT-4o achieved the highest overall performance, with comparable results across both datasets, reaching 79% accuracy and an average macro-F1 score of 66.08%. Fine-tuning, however, notably elevated all open models beyond GPT4o’s zero-shot results. Qwen achieved the top scores on one dataset, with an accuracy of 90.77% and a macro-F1 score of 83.98%, while LLaMA scored highest on the other, reaching an accuracy of 88.51% and a macroF1 score of 69.41%. These findings demonstrate that parameter-efficient supervised adaptation can close much of the performance gap and establish strong, reproducible baselines for Arabic WSD using open-source, relatively medium-sized models. Full code is publicly available.</abstract>
      <url hash="b2e0af54">2025.arabicnlp-main.24</url>
      <bibkey>noureldien-etal-2025-zero</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.24</doi>
    </paper>
    <paper id="25">
      <title>Nile-Chat: <fixed-case>E</fixed-case>gyptian Language Models for <fixed-case>A</fixed-case>rabic and <fixed-case>L</fixed-case>atin Scripts</title>
      <author><first>Guokan</first><last>Shang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author id="hadi-abdine" orcid="0009-0001-3944-4747"><first>Hadi</first><last>Abdine</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ahmad</first><last>Chamma</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Amr</first><last>Mohamed</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Mohamed</first><last>Anwar</last></author>
      <author><first>Abdelaziz</first><last>Bounhar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Omar</first><last>El Herraoui</last></author>
      <author id="preslav-nakov" orcid="0000-0002-3600-1510"><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Michalis</first><last>Vazirgiannis</last><affiliation>Ecole Polytechnique, France</affiliation></author>
      <author id="eric-xing"><first>Eric P.</first><last>Xing</last><affiliation>Mohamed bin Zayed Univeristy of AI and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>306-322</pages>
      <abstract>We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for Egyptian dialect, uniquely designed to understand and generate texts written in both Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we introduce a novel language adaptation approach by leveraging the Branch-Train-MiX strategy to merge script-specialized experts, into a single MoE model. Our Nile-Chat models significantly outperform leading multilingual and Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced Egyptian evaluation benchmarks, which span both understanding and generative tasks. Notably, our 12B model delivers a 14.4% performance gain over Qwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly available. We believe this work presents a comprehensive methodology for adapting LLMs to a single language with dual-script usage, addressing an often overlooked aspect in contemporary LLM development.</abstract>
      <url hash="aa91c501">2025.arabicnlp-main.25</url>
      <bibkey>shang-etal-2025-nile</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.25</doi>
    </paper>
    <paper id="26">
      <title>Mind the Gap: A Review of <fixed-case>A</fixed-case>rabic Post-Training Datasets and Their Limitations</title>
      <author id="mohammed-alkhowaiter" orcid="0009-0004-6842-1300"><first>Mohammed</first><last>Alkhowaiter</last><affiliation>Prince Sattam bin Abdulaziz University</affiliation></author>
      <author><first>Norah</first><last>Alshahrani</last><affiliation>ASAS AI</affiliation></author>
      <author><first>Saied</first><last>Alshahrani</last><affiliation>University of Bisha</affiliation></author>
      <author><first>Reem I.</first><last>Masoud</last></author>
      <author id="alaa-alzahrani" orcid="0000-0002-9914-915X"><first>Alaa</first><last>Alzahrani</last><affiliation>King Salman Global Academy for Arabic</affiliation></author>
      <author id="deema-alnuhait" orcid="0000-0002-5082-8565"><first>Deema</first><last>Alnuhait</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Emad A.</first><last>Alghamdi</last><affiliation>HUMAIN</affiliation></author>
      <author><first>Khalid</first><last>Almubarak</last><affiliation>Humain</affiliation></author>
      <pages>323-337</pages>
      <abstract>Post-training has emerged as a crucial technique for aligning pre-trained Large Language Models (LLMs) with human instructions, significantly enhancing their performance across a wide range of tasks. Central to this process is the quality and diversity of post-training datasets. This paper presents a review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: (1) LLM Capabilities (e.g., Question Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation, and Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3) Alignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness. Each dataset is rigorously evaluated based on popularity, practical adoption, recency and maintenance, documentation and annotation quality, licensing transparency, and scientific contribution. Our review revealed critical gaps in the development of Arabic post-training datasets, including limited task diversity, inconsistent or missing documentation and annotation, and low adoption across the community. Finally, the paper discusses the implications of these gaps on the progress of Arabic-centric LLMs and applications while providing concrete recommendations for future efforts in Arabic post-training dataset development.</abstract>
      <url hash="12246b2e">2025.arabicnlp-main.26</url>
      <bibkey>alkhowaiter-etal-2025-mind</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.26</doi>
    </paper>
    <paper id="27">
      <title>Bridging Dialectal Gaps in <fixed-case>A</fixed-case>rabic Medical <fixed-case>LLM</fixed-case>s through Model Merging</title>
      <author id="ahmed-ibrahim" orcid="0009-0002-9217-4173"><first>Ahmed</first><last>Ibrahim</last><affiliation>Weill Cornell Medicine, Cornell University</affiliation></author>
      <author><first>Abdullah</first><last>Hosseini</last></author>
      <author id="hoda-helmy" orcid="0009-0009-8251-9351"><first>Hoda</first><last>Helmy</last></author>
      <author><first>Wafa</first><last>Lakhdhar</last><affiliation>Hamad Medical Corporation</affiliation></author>
      <author><first>Ahmed</first><last>Serag</last><affiliation>Philips Research</affiliation></author>
      <pages>338-346</pages>
      <abstract>The linguistic fragmentation of Arabic, with over 30 dialects exhibiting low mutual intelligibility, presents a critical challenge for deploying natural language processing (NLP) in healthcare. Conventional fine-tuning of large language models (LLMs) for each dialect is computationally prohibitive and operationally unsustainable. In this study, we explore model merging as a scalable alternative by integrating three pre-trained LLMs—a medical domain expert, an Egyptian Arabic model, and a Moroccan Darija model—into a unified system without additional fine-tuning. We introduce a novel evaluation framework that assesses both dialectal fidelity via dual evaluation: LLM-based automated scoring and human assessments by native speakers. Our results demonstrate that the merged model effectively handles cross-dialect medical scenarios, such as interpreting Moroccan Darija inputs for Egyptian Arabic-speaking clinicians, while maintaining high clinical relevance. The merging process reduced computational cost by over 60% compared to per-dialect fine-tuning, highlighting its viability for resource-constrained settings. This work offers a promising path for building dialect-aware medical LLMs at scale, with implications for broader deployment across linguistically diverse regions.</abstract>
      <url hash="bc174b60">2025.arabicnlp-main.27</url>
      <bibkey>ibrahim-etal-2025-bridging</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.27</doi>
    </paper>
    <paper id="28">
      <title>Tool Calling for <fixed-case>A</fixed-case>rabic <fixed-case>LLM</fixed-case>s: Data Strategies and Instruction Tuning</title>
      <author><first>Asım</first><last>Ersoy</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author id="enes-altinisik" orcid="0000-0001-9300-6564"><first>Enes</first><last>Altinisik</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Kareem Mohamed</first><last>Darwish</last><affiliation>aiXplain Inc</affiliation></author>
      <author><first>Husrev Taha</first><last>Sencar</last><affiliation>QCRI</affiliation></author>
      <pages>347-358</pages>
      <abstract>Tool calling is a critical capability that allows Large Language Models (LLMs) to interact with external systems, significantly expanding their utility. However, research and resources for tool calling are predominantly English-centric, leaving a gap in our understanding of how to enable this functionality for other languages, such as Arabic. This paper investigates three key research questions: (1) the necessity of in-language (Arabic) tool-calling data versus relying on cross-lingual transfer, (2) the effect of general-purpose instruction tuning on tool-calling performance, and (3) the value of fine-tuning on specific, high-priority tools. To address these questions, we conduct extensive experiments using base and post-trained variants of an open-weight Arabic LLM. To enable this study, we bridge the resource gap by translating and adapting two open-source tool-calling datasets into Arabic. Our findings provide crucial insights into the optimal strategies for developing robust tool-augmented agents for Arabic.</abstract>
      <url hash="2bc8d76f">2025.arabicnlp-main.28</url>
      <bibkey>ersoy-etal-2025-tool</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.28</doi>
    </paper>
    <paper id="29">
      <title>Toward Culturally-Aware <fixed-case>A</fixed-case>rabic Debate Platforms with <fixed-case>NLP</fixed-case> Support</title>
      <author id="khalid-khatib" orcid="0009-0006-7255-5349"><first>Khalid Al</first><last>Khatib</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Mohammad</first><last>Khader</last></author>
      <pages>359-374</pages>
      <abstract>Despite the growing importance of online discourse, Arabic-speaking communities lack platforms that support structured, culturally grounded debate. Mainstream social media rarely fosters constructive engagement, often leading to polarization and superficial exchanges. This paper proposes the development of a culturally aware debate platform tailored to the values and traditions of Arabic-speaking users, with a focus on leveraging advances in natural language processing (NLP). We present findings from a user survey that explores experiences with existing debate tools and expectations for future platforms. Besides, we analyze 30,000 English-language debate topics using large language models (LLMs) to assess their cultural relevance and appropriateness for Arab audiences. We further examine the ability of LLMs to generate new culturally resonant debate topics, contributing to the emerging tasks of culture-aware topic assessment and generation. Finally, we propose a theoretical and technical framework for building an NLP-supported Arabic debate platform. Our work highlights the urgent need for culturally sensitive NLP resources that foster critical thinking, digital literacy, and meaningful deliberation in Arabic.</abstract>
      <url hash="bff02850">2025.arabicnlp-main.29</url>
      <bibkey>khatib-khader-2025-toward</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.29</doi>
    </paper>
    <paper id="30">
      <title>Modeling <fixed-case>N</fixed-case>orth <fixed-case>A</fixed-case>frican Dialects from Standard Languages</title>
      <author><first>Yassine</first><last>Toughrai</last></author>
      <author id="kamel-smaili"><first>Kamel</first><last>Smaïli</last><affiliation>University of Lorraine</affiliation></author>
      <author id="david-langlois" orcid="0000-0001-7277-3668"><first>David</first><last>Langlois</last><affiliation>Université de Lorraine</affiliation></author>
      <pages>375-383</pages>
      <abstract>Processing North African Arabic dialects presents significant challenges due to high lexical variability, frequent code-switching with French, and the use of both Arabic and Latin scripts. We address this with a phonemebased normalization strategy that maps Arabic and French text into a simplified representation (Arabic rendered in Latin script), reflecting native reading patterns. Using this method, we pretrain BERTbased models on normalized Modern Standard Arabic and French only and evaluate them on Named Entity Recognition (NER) and text classification. Experiments show that normalized standard-language corpora yield competitive performance on North African dialect tasks; in zero-shot NER, Ar_20k surpasses dialectpretrained baselines. Normalization improves vocabulary alignment, indicating that normalized standard corpora can suffice for developing dialect-supportive</abstract>
      <url hash="bdb555ce">2025.arabicnlp-main.30</url>
      <bibkey>toughrai-etal-2025-modeling</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.30</doi>
    </paper>
    <paper id="31">
      <title>Learning Word Embeddings from Glosses: A Multi-Loss Framework for <fixed-case>A</fixed-case>rabic Reverse Dictionary Tasks</title>
      <author><first>Engy</first><last>Ibrahim</last></author>
      <author><first>Farhah</first><last>Adel</last></author>
      <author><first>Marwan</first><last>Torki</last><affiliation>Alexandria University</affiliation></author>
      <author id="nagwa-m-el-makky" orcid="0000-0002-9571-7543"><first>Nagwa</first><last>El-Makky</last></author>
      <pages>384-388</pages>
      <abstract>We address the task of reverse dictionary modeling in Arabic, where the goal is to retrieve a target word given its definition. The task comprises two subtasks: (1) generating embeddings for Arabic words based on Arabic glosses, and (2) a cross-lingual setting where the gloss is in English and the target embedding is for the corresponding Arabic word. Prior approaches have largely relied on BERT models such as CAMeLBERT or MARBERT trained with mean squared error loss. In contrast, we propose a novel ensemble architecture that combines MARBERTv2 with the encoder of AraBART, and we demonstrate that the choice of loss function has a significant impact on performance. We apply contrastive loss to improve representational alignment, and introduce structural and center losses to better capture the semantic distribution of the dataset. This multi-loss framework enhances the quality of the learned embeddings and leads to consistent improvements in both monolingual and cross-lingual settings. Our system achieved the best rank metric in both subtasks compared to the previous approaches. These results highlight the effectiveness of combining architectural diversity with task-specific loss functions in representational tasks for morphologically rich languages like Arabic.</abstract>
      <url hash="fde6d851">2025.arabicnlp-main.31</url>
      <bibkey>ibrahim-etal-2025-learning</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.31</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>ALARB</fixed-case>: An <fixed-case>A</fixed-case>rabic Legal Argument Reasoning Benchmark</title>
      <author><first>Harethah</first><last>Abu Shairah</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Somayah</first><last>AlHarbi</last><affiliation>Thiqah for Business Services</affiliation></author>
      <author><first>Abdulaziz</first><last>AlHussein</last></author>
      <author><first>Sameer</first><last>Alsabea</last></author>
      <author><first>Omar</first><last>Shaqaqi</last></author>
      <author><first>Hebah</first><last>AlShamlan</last><affiliation>Thiqah</affiliation></author>
      <author><first>Omar</first><last>Knio</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>George</first><last>Turkiyyah</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <pages>389-406</pages>
      <abstract>We introduce ALARB, a dataset and suite of tasks designed to evaluate the reasoning capabilities of large language models (LLMs) within the Arabic legal domain. While existing Arabic benchmarks cover some knowledge-intensive tasks such as retrieval and understanding, substantial datasets focusing specifically on multistep reasoning for Arabic LLMs, especially in open-ended contexts, are lacking. The dataset comprises over 13K commercial court cases from Saudi Arabia, with each case including the facts presented, the reasoning of the court, the verdict, as well the cited clauses extracted from the regulatory documents. We define a set of challenging tasks leveraging this dataset and reflecting the complexity of real-world legal reasoning, including verdict prediction, completion of reasoning chains in multistep legal arguments, and identification of relevant regulations based on case facts. We benchmark a representative selection of current open and closed Arabic LLMs on these tasks and demonstrate the dataset’s utility for instruction tuning. Notably, we show that instruction tuning a modest 12B parameter model using ALARB significantly enhances its performance in verdict prediction and Arabic verdict generation, reaching a level comparable to that of GPT-4o.</abstract>
      <url hash="3541d9cb">2025.arabicnlp-main.32</url>
      <bibkey>shairah-etal-2025-alarb</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.32</doi>
    </paper>
    <paper id="33">
      <title>Transfer or Translate? Argument Mining in <fixed-case>A</fixed-case>rabic with No Native Annotations</title>
      <author id="sara-nabhani" orcid="0009-0003-0599-5756"><first>Sara</first><last>Nabhani</last></author>
      <author id="khalid-khatib" orcid="0009-0006-7255-5349"><first>Khalid Al</first><last>Khatib</last><affiliation>University of Groningen</affiliation></author>
      <pages>407-416</pages>
      <abstract>Argument mining for Arabic remains underexplored, largely due to the scarcity of annotated corpora. To address this gap, we examine the effectiveness of cross-lingual transfer from English. Using the English Persuasive Essays (PE) corpus, annotated with argumentative components (Major Claim, Claim, and Premise), we explore several transfer strategies: training encoder-based multilingual and monolingual models on English data, machine-translated Arabic data, and their combination. We further assess the impact of annotation noise introduced during translation by manually correcting portions of the projected training data. In addition, we investigate the potential of prompting large language models (LLMs) for the task. Experiments on a manually corrected Arabic test set show that monolingual models trained on translated data achieve the strongest performance, with further improvements from small-scale manual correction of training examples.</abstract>
      <url hash="3116d920">2025.arabicnlp-main.33</url>
      <bibkey>nabhani-khatib-2025-transfer</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.33</doi>
    </paper>
    <paper id="34">
      <title>An Exploration of Knowledge Editing for <fixed-case>A</fixed-case>rabic</title>
      <author><first>Basel</first><last>Mousi</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author id="nadir-durrani" orcid="0000-0002-9378-4128"><first>Nadir</first><last>Durrani</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author><first>Fahim</first><last>Dalvi</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <pages>417-424</pages>
      <abstract>While Knowledge Editing (KE) has been widely explored in English, its behavior in morphologically rich languages like Arabic remains underexamined. In this work, we present the first study of Arabic KE. We evaluate four methods (ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact benchmarks, analyzing both multilingual and cross-lingual settings. Our experiments on Llama-2-7B-chat show show that parameter-based methods struggle with cross-lingual generalization, while instruction-tuned methods perform more robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show that joint Arabic-English training improves both editability and transfer. We release Arabic KE benchmarks and multilingual training for LTE data to support future research.</abstract>
      <url hash="42b76afc">2025.arabicnlp-main.34</url>
      <bibkey>mousi-etal-2025-exploration</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.34</doi>
    </paper>
    <paper id="35">
      <title>Octopus: Towards Building the <fixed-case>A</fixed-case>rabic Speech <fixed-case>LLM</fixed-case> Suite</title>
      <author id="sara-althubaiti" orcid="0000-0001-5754-8569"><first>Sara</first><last>Althubaiti</last></author>
      <author><first>Vasista Sai</first><last>Lodagala</last></author>
      <author><first>Tjad</first><last>Clark</last><affiliation>Humain</affiliation></author>
      <author><first>Yousseif Ahmed</first><last>Elshahawy</last><affiliation>Humain</affiliation></author>
      <author><first>Daniel</first><last>Izham</last><affiliation>Humain</affiliation></author>
      <author><first>Abdullah</first><last>Alrajeh</last><affiliation>Humain</affiliation></author>
      <author><first>Aljawahrah</first><last>Bin Tamran</last></author>
      <author><first>Ahmed</first><last>Ali</last><affiliation>Saudi Data and AI Authority, Saudi Data and AI Authority</affiliation></author>
      <pages>425-435</pages>
      <abstract>We present Octopus, a first family of modular speech-language models designed for Arabic-English ASR, dialect identification, and speech translation. Built on Whisper-V3 and enhanced with large language models like ALLaM, LLaMA, and DeepSeek, Octopus bridges speech and text through a lightweight projection layer and Q-Former. To broaden its scope beyond speech, Octopus integrates BEATs, a general-purpose audio encoder allowing it to understand both linguistic and acoustic events. Despite its simplicity, this dual-encoder design supports robust performance across multilingual and code-switched scenarios. We also introduce TinyOctopus, a distilled variant using smaller models (Distil-Whisper + LLaMA3-1B / DeepSeek-1.5B), achieving competitive results with just a fraction of the parameters. Fine-tuning on synthetic code-switched data further boosts its performance. Octopus demonstrates the power of compact, extensible architectures in Arabic-centric speech modeling and sets the stage for unified multilingual audio-language understanding.</abstract>
      <url hash="17f88019">2025.arabicnlp-main.35</url>
      <bibkey>althubaiti-etal-2025-octopus</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>A</fixed-case>rabic<fixed-case>W</fixed-case>eb-Edu: Educational Quality Data for <fixed-case>A</fixed-case>rabic <fixed-case>LLM</fixed-case> Training</title>
      <author id="majd-hawasly" orcid="0000-0003-1823-5580"><first>Majd</first><last>Hawasly</last></author>
      <author id="muhammad-tasnim-mohiuddin"><first>Tasnim</first><last>Mohiuddin</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <author id="hamdy-mubarak" orcid="0000-0003-4828-6098"><first>Hamdy</first><last>Mubarak</last></author>
      <author id="sabri-boughorbel" orcid="0000-0003-2734-3356"><first>Sabri</first><last>Boughorbel</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>436-447</pages>
      <abstract>The quality of training data plays a critical role in the performance of large language models (LLMs). This is especially true for low-resource languages where high-quality content is relatively scarce. Inspired by the success of FineWeb-Edu for English, we construct a native Arabic educational-quality dataset using similar methodological principles. We begin by sampling 1 million Arabic web documents from Common Crawl and labeling them into six quality classes (0–5) with Qwen-2.5-72B-Instruct model using a classification prompt adapted from FineWeb-Edu. These labeled examples are used to train a robust classifier capable of distinguishing educational content from general web text. We train a classification head on top of a multilingual 300M encoder model, then use this classifier to filter a large Arabic web corpus, discarding documents with low educational value. To evaluate the impact of this curation, we pretrain from scratch two bilingual English-Arabic 7B LLMs on 800 billion tokens using the filtered and unfiltered data and compare their performance across a suite of benchmarks. Our results show a significant improvement when using the filtered educational dataset, validating the effectiveness of quality filtering as a component in a balanced data mixture for Arabic LLM development. This work addresses the scarcity of high-quality Arabic training data and offers a scalable methodology for curating educational quality content in low-resource languages.</abstract>
      <url hash="1abab713">2025.arabicnlp-main.36</url>
      <bibkey>hawasly-etal-2025-arabicweb</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.36</doi>
    </paper>
    <paper id="37">
      <title><fixed-case>AMC</fixed-case>rawl: An <fixed-case>A</fixed-case>rabic Web-Scale Dataset of Interleaved Image-Text Documents and Image-Text Pairs</title>
      <author><first>Shahad</first><last>Aboukozzana</last></author>
      <author><first>Muhammad Kamran J</first><last>Khan</last></author>
      <author><first>Ahmed</first><last>Ali</last><affiliation>Saudi Data and AI Authority, Saudi Data and AI Authority</affiliation></author>
      <pages>448-465</pages>
      <abstract>In this paper, we present the Arabic Multimodal Crawl (AMCrawl), the first native-based Arabic multimodal dataset to our knowledge, derived from the Common Crawl corpus and rigorously filtered for quality and safety. Image-text pair datasets are the standard choice for pretraining multimodal large language models. However, they are often derived from image alt-text metadata, which is typically brief and context-poor, disconnecting images from their broader meaning. Although significant advances have been made in building interleaved image-text datasets for English, such as the OBELICS dataset, a substantial gap remains for native Arabic content. Our processing covered 8.6 million Arabic web pages, yielding 5.8 million associated images and 1.3 billion text tokens. The final dataset includes interleaved image-text documents and question-answer pairs, featuring 2.8 million high-quality interleaved documents and 5 million QA pairs. Alongside the dataset, we release the complete pipeline and code, ensuring reproducibility and encouraging further research and development. To demonstrate the effectiveness of AMCrawl, we introduce a publicly available native Arabic Vision Language model, trained with 13 billion parameters. These models achieve competitive results when benchmarked against publicly available datasets. AMCrawl bridges a critical gap in Arabic multimodal resources, providing a robust foundation for developing Arabic multimodal large language models and fostering advancements in this underrepresented area. Code: github.com/shahad-aboukozzana/AMCrawl</abstract>
      <url hash="eafdde3d">2025.arabicnlp-main.37</url>
      <bibkey>aboukozzana-etal-2025-amcrawl</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.37</doi>
    </paper>
    <paper id="38">
      <title><fixed-case>D</fixed-case>ial<fixed-case>G</fixed-case>2<fixed-case>P</fixed-case>: Dialectal Grapheme-to-Phoneme. <fixed-case>A</fixed-case>rabic as a Case Study</title>
      <author id="majd-hawasly" orcid="0000-0003-1823-5580"><first>Majd</first><last>Hawasly</last></author>
      <author id="hamdy-mubarak" orcid="0000-0003-4828-6098"><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Ahmed</first><last>Abdelali</last><affiliation>Humain</affiliation></author>
      <author><first>Ahmed</first><last>Ali</last><affiliation>Saudi Data and AI Authority, Saudi Data and AI Authority</affiliation></author>
      <pages>466-471</pages>
      <abstract>Grapheme-to-phoneme (G2P) models are essential components in text-to-speech (TTS) and pronunciation assessment applications. While standard forms of languages have gained attention in that regard, dialectal speech, which often serves as the primary means of spoken communication for many communities, as it is the case for Arabic, has not received the same level of focus. In this paper, we introduce an end-to-end dialectal G2P for Egyptian Arabic, a dialect without standard orthography. Our novel architecture accomplishes three tasks: (i) restores short vowels of the diacritical marks for the dialectal text; (ii) maps certain characters that happen only in the spoken version of the dialectal Arabic to their dialect-specific character transcriptions; and finally (iii) converts the previous step output to the corresponding phoneme sequence. We benchmark G2P on a modular cascaded system, a large language model, and our multi-task end-to-end architecture.</abstract>
      <url hash="335b681a">2025.arabicnlp-main.38</url>
      <bibkey>hawasly-etal-2025-dialg2p</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.38</doi>
    </paper>
    <paper id="39">
      <title>Shawarma Chats: A Benchmark Exact Dialogue &amp; Evaluation Platter in <fixed-case>E</fixed-case>gyptian, <fixed-case>M</fixed-case>aghrebi &amp; Modern Standard <fixed-case>A</fixed-case>rabic—<fixed-case>A</fixed-case> Triple-Dialect Feast for Hungry Language Models</title>
      <author><first>Kamyar</first><last>Zeinalipour</last></author>
      <author><first>Mohamed Zaky</first><last>Saad</last></author>
      <author id="oumaima-attafi" orcid="0009-0008-4543-7616"><first>Oumaima</first><last>Attafi</last><affiliation>University of Siena and Università degli Studi di Siena</affiliation></author>
      <author id="marco-maggini" orcid="0000-0002-6428-1265"><first>Marco</first><last>Maggini</last></author>
      <author id="marco-gori" orcid="0000-0001-6337-5430"><first>Marco</first><last>Gori</last><affiliation>University of Siena</affiliation></author>
      <pages>472-524</pages>
      <abstract>Content-grounded dialogue evaluation for Arabic remains under-resourced, particularly across Modern Standard (MSA), Egyptian, and Maghrebi varieties. We introduce Shawarma Chats, a benchmark of 30,000 six-turn conversations grounded in Wikipedia content, evenly split across the three dialects. To build this corpus, we prompt five frontier LLMs GPT-4o, Gemini 2.5 Flash, Qwen-Plus, DeepSeek-Chat, and Mistral Large to generate 1,500 seed dialogues. Native Arabic speakers evaluate these outputs to select the most effective generator and most human-aligned grader. Sub-A dialogues undergo a two-pass, rationale-driven self-repair loop where the grader critiques and the generator revises; unresolved cases are manually corrected. We apply this pipeline to 10,000 Wikipedia paragraphs to create 30,000 high-quality conversations 10,000 per dialect—at modest human cost. To validate the benchmark, we LoRA-fine-tune six open LLMs (1–24 B parameters) on Shawarma Chats and observe consistent gains in automatic-grader scores, BERTScore, BLEU and ROUGE particularly for models larger than 7 B parameters. Shawarma Chats thus establishes the first large-scale, dialect-aware, content-grounded dialogue benchmark for Arabic.</abstract>
      <url hash="32212d53">2025.arabicnlp-main.39</url>
      <bibkey>zeinalipour-etal-2025-shawarma</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-main.39</doi>
    </paper>
  </volume>
  <volume id="sharedtasks" ingest-date="2025-10-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of The Third Arabic Natural Language Processing Conference: Shared Tasks</booktitle>
      <editor><first>Kareem</first><last>Darwish</last></editor>
      <editor><first>Ahmed</first><last>Ali</last></editor>
      <editor><first>Ibrahim</first><last>Abu Farha</last></editor>
      <editor><first>Samia</first><last>Touileb</last></editor>
      <editor><first>Imed</first><last>Zitouni</last></editor>
      <editor><first>Ahmed</first><last>Abdelali</last></editor>
      <editor><first>Sharefah</first><last>Al-Ghamdi</last></editor>
      <editor><first>Sakhar</first><last>Alkhereyf</last></editor>
      <editor><first>Wajdi</first><last>Zaghouani</last></editor>
      <editor><first>Salam</first><last>Khalifa</last></editor>
      <editor><first>Badr</first><last>AlKhamissi</last></editor>
      <editor><first>Rawan</first><last>Almatham</last></editor>
      <editor><first>Injy</first><last>Hamed</last></editor>
      <editor><first>Zaid</first><last>Alyafeai</last></editor>
      <editor><first>Areeb</first><last>Alowisheq</last></editor>
      <editor><first>Go</first><last>Inoue</last></editor>
      <editor><first>Khalil</first><last>Mrini</last></editor>
      <editor><first>Waad</first><last>Alshammari</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>November</month>
      <year>2025</year>
      <url hash="b69d845d">2025.arabicnlp-sharedtasks</url>
      <venue>arabicnlp</venue>
      <isbn>979-8-89176-356-2</isbn>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks</doi>
    </meta>
    <frontmatter>
      <url hash="613c73a7">2025.arabicnlp-sharedtasks.0</url>
      <bibkey>arabicnlp-ws-2025-sharedtasks</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.0</doi>
    </frontmatter>
    <paper id="1">
      <title>The <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task on <fixed-case>A</fixed-case>rabic Authorship Style Transfer and <fixed-case>AI</fixed-case> Generated Text Detection</title>
      <author id="shadi-abudalfa" orcid="0000-0002-2028-144X"><first>Shadi</first><last>Abudalfa</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <author id="saad-ezzini" orcid="0000-0001-7657-4738"><first>Saad</first><last>Ezzini</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <author><first>Ahmed</first><last>Abdelali</last><affiliation>Humain</affiliation></author>
      <author><first>Hamza</first><last>Alami</last></author>
      <author id="abdessamad-benlahbib" orcid="0000-0002-0039-7832"><first>Abdessamad</first><last>Benlahbib</last></author>
      <author><first>Salmane</first><last>Chafik</last></author>
      <author id="mo-el-haj" orcid="0000-0002-6136-3898"><first>Mo</first><last>El-Haj</last><affiliation>VinUniversity and Lancaster University</affiliation></author>
      <author id="abdelkader-el-mahdaouy" orcid="0000-0003-4281-2472"><first>Abdelkader</first><last>El Mahdaouy</last><affiliation>Mohammed VI Polytechnic University</affiliation></author>
      <author id="mustafa-jarrar" orcid="0000-0003-4351-4207"><first>Mustafa</first><last>Jarrar</last><affiliation>Birzeit University</affiliation></author>
      <author><first>Salima</first><last>Lamsiyah</last><affiliation>University of Luxemburg</affiliation></author>
      <author id="hamzah-luqman" orcid="0000-0001-7944-5093"><first>Hamzah</first><last>Luqman</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <pages>1-13</pages>
      <abstract>We present an overview of the AraGenEval shared task, organized as part of the ArabicNLP 2025 conference. This task introduced the first benchmark suite for Arabic authorship analysis, featuring three subtasks: Authorship Style Transfer, Authorship Identification, and AI-Generated Text Detection. We curated high-quality datasets, including over 47,000 paragraphs from 21 authors and a balanced corpus of human- and AI-generated texts. The task attracted significant global participation, with 72 registered teams from 16 countries. The results highlight the effectiveness of transformer-based models, with top systems leveraging prompt engineering for style transfer, model ensembling for authorship identification, and a mix of multilingual and Arabic-specific models for AI text detection. This paper details the task design, datasets, participant systems, and key findings, establishing a foundation for future research in Arabic stylistics and trustworthy NLP.</abstract>
      <url hash="8d994c53">2025.arabicnlp-sharedtasks.1</url>
      <bibkey>abudalfa-etal-2025-arageneval</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>MISSION</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: Enhanced <fixed-case>A</fixed-case>rabic Authority Classification</title>
      <author><first>Thamer</first><last>Maseer Alharbi</last><affiliation>Institute</affiliation></author>
      <pages>14-17</pages>
      <abstract/>
      <url hash="0b0fc543">2025.arabicnlp-sharedtasks.2</url>
      <bibkey>maseer-alharbi-2025-mission</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>N</fixed-case>ojoom.<fixed-case>AI</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: <fixed-case>A</fixed-case>rabic Authorship Style Transfer</title>
      <author><first>Hafsa</first><last>Kara Achira</last><affiliation>Institute</affiliation></author>
      <author><first>Mourad</first><last>Bouache</last><affiliation>Institute</affiliation></author>
      <author><first>Mourad</first><last>Dahmane</last><affiliation>Institute</affiliation></author>
      <pages>18-25</pages>
      <abstract/>
      <url hash="2924e258">2025.arabicnlp-sharedtasks.3</url>
      <bibkey>kara-achira-etal-2025-nojoom</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.3</doi>
    </paper>
    <paper id="4">
      <title><fixed-case>LMSA</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: Ensemble-Based Detection of <fixed-case>AI</fixed-case>-Generated <fixed-case>A</fixed-case>rabic Text Using Multilingual and <fixed-case>A</fixed-case>rabic-Specific Models</title>
      <author><first>Kaoutar</first><last>Zita</last><affiliation>Institute</affiliation></author>
      <author><first>Attia</first><last>Nehar</last><affiliation>Institute</affiliation></author>
      <author><first>Abdelkader</first><last>Khelil</last><affiliation>Institute</affiliation></author>
      <author><first>Slimane</first><last>Bellaouar</last><affiliation>Institute</affiliation></author>
      <author><first>Hadda</first><last>Cherroun</last><affiliation>Institute</affiliation></author>
      <pages>26-31</pages>
      <abstract/>
      <url hash="f7205daa">2025.arabicnlp-sharedtasks.4</url>
      <bibkey>zita-etal-2025-lmsa</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.4</doi>
    </paper>
    <paper id="5">
      <title>Amr&amp;<fixed-case>M</fixed-case>ohamed<fixed-case>S</fixed-case>abaa at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val shared task: <fixed-case>A</fixed-case>rabic Authorship Identification using Term Frequency – Inverse Document Frequency Features with Supervised Machine Learning</title>
      <author><first>Amr</first><last>Sabaa</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Sabaa</last><affiliation>Institute</affiliation></author>
      <pages>32-36</pages>
      <abstract/>
      <url hash="c0d14e6b">2025.arabicnlp-sharedtasks.5</url>
      <bibkey>sabaa-sabaa-2025-amr</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>NLP</fixed-case>_wizard at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val shared task: Embedding-Based Classification for <fixed-case>AI</fixed-case> Detection and Authorship Attribution</title>
      <author><first>Mena</first><last>Hany</last><affiliation>Institute</affiliation></author>
      <pages>37-41</pages>
      <abstract/>
      <url hash="0bb42c87">2025.arabicnlp-sharedtasks.6</url>
      <bibkey>hany-2025-nlp</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>PTUK</fixed-case>-<fixed-case>HULAT</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: Fine-tuning <fixed-case>XLM</fixed-case>-<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a for <fixed-case>AI</fixed-case>-Generated <fixed-case>A</fixed-case>rabic News Detection</title>
      <author><first>Tasneem</first><last>Duridi</last><affiliation>Institute</affiliation></author>
      <author><first>Areej</first><last>Jaber</last><affiliation>Institute</affiliation></author>
      <author><first>Paloma</first><last>Martínez</last><affiliation>Institute</affiliation></author>
      <pages>42-48</pages>
      <abstract/>
      <url hash="fe9dfd0f">2025.arabicnlp-sharedtasks.7</url>
      <bibkey>duridi-etal-2025-ptuk</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>ANLP</fixed-case>ers at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: Descriptive Author Tokens for Transparent <fixed-case>A</fixed-case>rabic Authorship Style Transfer</title>
      <author><first>Omer</first><last>Nacar</last><affiliation>Institute</affiliation></author>
      <author><first>Mahmoud</first><last>Reda</last><affiliation>Institute</affiliation></author>
      <author><first>Serry</first><last>Sibaee</last><affiliation>Institute</affiliation></author>
      <author><first>Yasser</first><last>Alhabashi</last><affiliation>Institute</affiliation></author>
      <author><first>Adel</first><last>Ammar</last><affiliation>Institute</affiliation></author>
      <author><first>Wadii</first><last>Boulila</last><affiliation>Institute</affiliation></author>
      <pages>49-53</pages>
      <abstract/>
      <url hash="4bbe0f78">2025.arabicnlp-sharedtasks.8</url>
      <bibkey>nacar-etal-2025-anlpers</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.8</doi>
    </paper>
    <paper id="9">
      <title>Athership at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: Identifying <fixed-case>A</fixed-case>rabic Authorship with a Dual-Model Logit Fusion</title>
      <author><first>Mohamed</first><last>Amin</last><affiliation>Institute</affiliation></author>
      <author><first>Mahmoud</first><last>Rady</last><affiliation>Institute</affiliation></author>
      <author><first>Mariam</first><last>Hossam</last><affiliation>Institute</affiliation></author>
      <author><first>Sara</first><last>Gaballa</last><affiliation>Institute</affiliation></author>
      <author><first>Eman</first><last>Samir</last><affiliation>Institute</affiliation></author>
      <author><first>Maria</first><last>Bassem</last><affiliation>Institute</affiliation></author>
      <author><first>Nisreen</first><last>Hisham</last><affiliation>Institute</affiliation></author>
      <author><first>Ayman</first><last>Khalafallah</last><affiliation>Institute</affiliation></author>
      <pages>54-58</pages>
      <abstract/>
      <url hash="bf2f78ba">2025.arabicnlp-sharedtasks.9</url>
      <bibkey>amin-etal-2025-athership</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.9</doi>
    </paper>
    <paper id="10">
      <title>Sebaweh at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: <fixed-case>BERENSE</fixed-case> - <fixed-case>BER</fixed-case>t based <fixed-case>ENSE</fixed-case>mbler for <fixed-case>A</fixed-case>rabic Authorship Identification</title>
      <author><first>Muhammad</first><last>Helmy</last><affiliation>Institute</affiliation></author>
      <author><first>Batool</first><last>Najeh Balah</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmed</first><last>Mohamed Sallam</last><affiliation>Institute</affiliation></author>
      <author><first>Ammar</first><last>Sherif</last><affiliation>Institute</affiliation></author>
      <pages>59-64</pages>
      <abstract/>
      <url hash="0865062d">2025.arabicnlp-sharedtasks.10</url>
      <bibkey>helmy-etal-2025-sebaweh</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>CUET</fixed-case>-<fixed-case>NLP</fixed-case>_<fixed-case>T</fixed-case>eam_<fixed-case>SS</fixed-case>306 at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: A Transformer-based Framework for Detecting <fixed-case>AI</fixed-case>-Generated <fixed-case>A</fixed-case>rabic Text</title>
      <author><first>Sowrav</first><last>Nath</last><affiliation>Institute</affiliation></author>
      <author><first>Shadman</first><last>Saleh</last><affiliation>Institute</affiliation></author>
      <author><first>Kawsar</first><last>Ahmed</last><affiliation>Institute</affiliation></author>
      <author><first>Mohammed</first><last>Moshiul Hoque</last><affiliation>Institute</affiliation></author>
      <pages>65-71</pages>
      <abstract/>
      <url hash="78e5b6ad">2025.arabicnlp-sharedtasks.11</url>
      <bibkey>nath-etal-2025-cuet</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.11</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>BUSTED</fixed-case> at <fixed-case>ARATECT</fixed-case> Shared Task: A Comparative Study of Transformer-Based Models for <fixed-case>A</fixed-case>rabic <fixed-case>AI</fixed-case>-Generated Text Detection</title>
      <author><first>Ali</first><last>Zain</last><affiliation>Institute</affiliation></author>
      <author><first>Sareem</first><last>Farooqui</last><affiliation>Institute</affiliation></author>
      <author><first>Muhammad</first><last>Rafi</last><affiliation>Institute</affiliation></author>
      <pages>72-76</pages>
      <abstract/>
      <url hash="2c9a36d4">2025.arabicnlp-sharedtasks.12</url>
      <bibkey>zain-etal-2025-busted</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>CIOL</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val shared task: Authorship Identification and <fixed-case>AI</fixed-case> Generated Text Detection in <fixed-case>A</fixed-case>rabic using Pretrained Models</title>
      <author><first>Sadia</first><last>Tasnim Meem</last><affiliation>Institute</affiliation></author>
      <author><first>Azmine</first><last>Toushik Wasi</last><affiliation>Institute</affiliation></author>
      <pages>77-81</pages>
      <abstract/>
      <url hash="0acb25e9">2025.arabicnlp-sharedtasks.13</url>
      <bibkey>tasnim-meem-toushik-wasi-2025-ciol</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.13</doi>
    </paper>
    <paper id="14">
      <title>Osint at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val shared task: Fine-Tuned Modeling for Tracking Style Signatures and <fixed-case>AI</fixed-case> Generation in <fixed-case>A</fixed-case>rabic Texts</title>
      <author><first>Shifali</first><last>Agrahari</last><affiliation>Institute</affiliation></author>
      <author><first>Hemanth</first><last>Prakash Simhadri</last><affiliation>Institute</affiliation></author>
      <author><first>Ashutosh</first><last>Kumar Verma</last><affiliation>Institute</affiliation></author>
      <author><first>Ranbir</first><last>Singh Sanasam</last><affiliation>Institute</affiliation></author>
      <pages>82-87</pages>
      <abstract/>
      <url hash="9477501f">2025.arabicnlp-sharedtasks.14</url>
      <bibkey>agrahari-etal-2025-osint</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>M</fixed-case>arsad<fixed-case>L</fixed-case>ab at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: <fixed-case>LLM</fixed-case>-Based Approaches to <fixed-case>A</fixed-case>rabic Authorship Style Transfer and Identification</title>
      <author><first>Md.</first><last>Rafiul Biswas</last><affiliation>Institute</affiliation></author>
      <author><first>Mabrouka</first><last>Bessghaier</last><affiliation>Institute</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Institute</affiliation></author>
      <author><first>Wajdi</first><last>Zaghouani</last><affiliation>Institute</affiliation></author>
      <pages>88-93</pages>
      <abstract/>
      <url hash="d3bb8da3">2025.arabicnlp-sharedtasks.15</url>
      <bibkey>rafiul-biswas-etal-2025-marsadlab</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>REGLAT</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val shared task: Morphology-Aware <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> for Detecting <fixed-case>A</fixed-case>rabic <fixed-case>AI</fixed-case>-Generated Text</title>
      <author><first>Mariam</first><last>Labib</last><affiliation>Institute</affiliation></author>
      <author><first>Nsrin</first><last>Ashraf</last><affiliation>Institute</affiliation></author>
      <author><first>Mohammed</first><last>Aldawsari</last><affiliation>Institute</affiliation></author>
      <author><first>Hamada</first><last>Nayel</last><affiliation>Institute</affiliation></author>
      <pages>94-98</pages>
      <abstract/>
      <url hash="93284bea">2025.arabicnlp-sharedtasks.16</url>
      <bibkey>labib-etal-2025-reglat</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>J</fixed-case>enin at <fixed-case>A</fixed-case>ra<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val Shared Task: Parameter-Efficient Fine-Tuning and Layer-Wise Analysis of <fixed-case>A</fixed-case>rabic <fixed-case>LLM</fixed-case>s for Authorship Style Transfer and Classification</title>
      <author><first>Huthayfa</first><last>Malhis</last><affiliation>Institute</affiliation></author>
      <author><first>Mohammad</first><last>Tami</last><affiliation>Institute</affiliation></author>
      <author><first>Huthaifa</first><last>I. Ashqar</last><affiliation>Institute</affiliation></author>
      <pages>99-106</pages>
      <abstract/>
      <url hash="f10f39da">2025.arabicnlp-sharedtasks.17</url>
      <bibkey>malhis-etal-2025-jenin</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.17</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> 2025: The First Shared Task on <fixed-case>A</fixed-case>rabic Health Question Answering</title>
      <author><first>Hassan</first><last>Alhuzali</last><affiliation>Umm Al-Qura University</affiliation></author>
      <author><first>Walid</first><last>Al-Eisawi</last></author>
      <author id="muhammad-abdul-mageed" orcid="0000-0002-8590-2040"><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Chaimae</first><last>Abouzahir</last></author>
      <author><first>Mouath</first><last>Abu-Daoud</last></author>
      <author><first>Ashwag</first><last>Alasmari</last><affiliation>King Khaled University</affiliation></author>
      <author><first>Renad</first><last>Al-Monef</last></author>
      <author><first>Ali</first><last>Alqahtani</last></author>
      <author id="lama-ayash" orcid="0000-0002-6268-9072"><first>Lama</first><last>Ayash</last></author>
      <author><first>Leen</first><last>Kharouf</last></author>
      <author><first>Farah E.</first><last>Shamout</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author id="nizar-habash" orcid="0000-0002-1831-3457"><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>107-118</pages>
      <abstract>We introduce AraHealthQA 2025, the Comprehensive Arabic Health Question Answering Shared Task, held in conjunction with ArabicNLP 2025 co-located with EMNLP 2025. This shared task addresses the paucity of high-quality Arabic medical QA resources by offering two complementary tracks: MentalQA, focusing on Arabic mental health Q&amp;A (e.g., anxiety, depression, stigma reduction), and MedArabiQ, covering broader medical domains such as internal medicine, pediatrics, and clinical decision making. Each track comprises multiple subtasks, evaluation datasets, and standardized metrics, facilitating fair benchmarking. The task was structured to promote modeling under realistic, multilingual, and culturally nuanced healthcare contexts. We outline the dataset creation, task design and evaluation framework, participation statistics, baseline systems, and summarize the overall outcomes. We conclude with reflections on the performance trends observed and prospects for future iterations in Arabic health QA.</abstract>
      <url hash="09258095">2025.arabicnlp-sharedtasks.18</url>
      <bibkey>alhuzali-etal-2025-arahealthqa</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>NYUAD</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> Shared Task: Benchmarking the Medical Understanding and Reasoning of Large Language Models in <fixed-case>A</fixed-case>rabic Healthcare Tasks</title>
      <author><first>Nouar</first><last>AlDahoul</last><affiliation>Institute</affiliation></author>
      <author><first>Yasir</first><last>Zaki</last><affiliation>Institute</affiliation></author>
      <pages>119-125</pages>
      <abstract/>
      <url hash="0ea72344">2025.arabicnlp-sharedtasks.19</url>
      <bibkey>aldahoul-zaki-2025-nyuad</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>M</fixed-case>ed<fixed-case>L</fixed-case>ingua at <fixed-case>M</fixed-case>ed<fixed-case>A</fixed-case>rabi<fixed-case>Q</fixed-case>2025: Zero- and Few-Shot Prompting of Large Language Models for <fixed-case>A</fixed-case>rabic Medical <fixed-case>QA</fixed-case></title>
      <author><first>Fatimah</first><last>Mohamed Emad Elden</last><affiliation>Institute</affiliation></author>
      <author><first>Mumina</first><last>Ab. Abukar</last><affiliation>Institute</affiliation></author>
      <pages>126-139</pages>
      <abstract/>
      <url hash="145d1d6b">2025.arabicnlp-sharedtasks.20</url>
      <bibkey>mohamed-emad-elden-ab-abukar-2025-medlingua</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.20</doi>
    </paper>
    <paper id="21">
      <title>Sakinah-<fixed-case>AI</fixed-case> at <fixed-case>M</fixed-case>ental<fixed-case>QA</fixed-case>: A Comparative Study of Few-Shot, Optimized, and Ensemble Methods for <fixed-case>A</fixed-case>rabic Mental Health Question Classification</title>
      <author><first>Fatimah</first><last>Mohamed Emad Elden</last><affiliation>Institute</affiliation></author>
      <author><first>Mumina</first><last>Ab. Abukar</last><affiliation>Institute</affiliation></author>
      <pages>140-148</pages>
      <abstract/>
      <url hash="2d5fbf7f">2025.arabicnlp-sharedtasks.21</url>
      <bibkey>mohamed-emad-elden-ab-abukar-2025-sakinah</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>M</fixed-case>ind<fixed-case>LLM</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> 2025 Track 1: Leveraging Large Language Models for Mental Health Question Answering</title>
      <author><first>Nejood</first><last>Abdulaziz Bin Eshaq</last><affiliation>Institute</affiliation></author>
      <pages>149-154</pages>
      <abstract/>
      <url hash="1e2e8bf7">2025.arabicnlp-sharedtasks.22</url>
      <bibkey>abdulaziz-bin-eshaq-2025-mindllm</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.22</doi>
    </paper>
    <paper id="23">
      <title>Quasar at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> Track 1 : Leveraging Zero-Shot Large Language Models for Question and Answer Categorization in <fixed-case>A</fixed-case>rabic Mental Health</title>
      <author><first>Adiba</first><last>Fairooz Chowdhury</last><affiliation>Institute</affiliation></author>
      <author><first>Md</first><last>Sagor Chowdhury</last><affiliation>Institute</affiliation></author>
      <pages>155-163</pages>
      <abstract/>
      <url hash="40f7f6b1">2025.arabicnlp-sharedtasks.23</url>
      <bibkey>fairooz-chowdhury-sagor-chowdhury-2025-quasar</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.23</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>B</fixed-case>inary_<fixed-case>B</fixed-case>unch at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> Track 1: <fixed-case>A</fixed-case>rabic Mental Health <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case> Classification Using Data Augmentation and Transformer Models</title>
      <author><first>Sajib</first><last>Bhattacharjee</last><affiliation>Institute</affiliation></author>
      <author><first>Ratnajit</first><last>Dhar</last><affiliation>Institute</affiliation></author>
      <author><first>Kawsar</first><last>Ahmed</last><affiliation>Institute</affiliation></author>
      <author><first>Mohammed</first><last>Moshiul Hoque</last><affiliation>Institute</affiliation></author>
      <pages>164-175</pages>
      <abstract/>
      <url hash="0077c9d9">2025.arabicnlp-sharedtasks.24</url>
      <bibkey>bhattacharjee-etal-2025-binary</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.24</doi>
    </paper>
    <paper id="25">
      <title>!<fixed-case>MSA</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> 2025 Shared Task: Enhancing <fixed-case>LLM</fixed-case> Performance for <fixed-case>A</fixed-case>rabic Clinical Question Answering through Prompt Engineering and Ensemble Learning</title>
      <author><first>Mohamed</first><last>Younes</last><affiliation>Institute</affiliation></author>
      <author><first>Seif</first><last>Ahmed</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Basem</last><affiliation>Institute</affiliation></author>
      <pages>176-183</pages>
      <abstract/>
      <url hash="bf59d971">2025.arabicnlp-sharedtasks.25</url>
      <bibkey>younes-etal-2025-msa</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.25</doi>
    </paper>
    <paper id="26">
      <title>Sindbad at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> Track 1: Leveraging Large Language Models for Mental Health <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case></title>
      <author><first>AbdulRahman</first><last>A. Morsy</last><affiliation>Institute</affiliation></author>
      <author><first>Saad</first><last>Mankarious</last><affiliation>Institute</affiliation></author>
      <author><first>Ayah</first><last>Zirikly</last><affiliation>Institute</affiliation></author>
      <pages>184-191</pages>
      <abstract/>
      <url hash="f040bc8c">2025.arabicnlp-sharedtasks.26</url>
      <bibkey>a-morsy-etal-2025-sindbad</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.26</doi>
    </paper>
    <paper id="27">
      <title><fixed-case>A</fixed-case>rabic Mental Health Question Answering: A Multi-Task Approach with Advanced Retrieval-Augmented Generation</title>
      <author><first>AbdelAziz</first><last>Amr</last><affiliation>Institute</affiliation></author>
      <author><first>Mamdouh</first><last>Koritam</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Youssef</last><affiliation>Institute</affiliation></author>
      <author><first>Marwa</first><last>Aldeeb</last><affiliation>Institute</affiliation></author>
      <author><first>Ensaf H.</first><last>Mohamed</last><affiliation>Institute</affiliation></author>
      <pages>192-197</pages>
      <abstract/>
      <url hash="19d98bae">2025.arabicnlp-sharedtasks.27</url>
      <bibkey>amr-abdelaziz-etal-2025-arabic</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.27</doi>
    </paper>
    <paper id="28">
      <title><fixed-case>A</fixed-case>ra<fixed-case>M</fixed-case>inds at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> 2025: A Retrieval-Augmented Generation System for Fine-Grained Classification and Answer Generation of <fixed-case>A</fixed-case>rabic Mental Health <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case></title>
      <author><first>Mohamed</first><last>Zaytoon</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmed</first><last>Mahmoud Salem</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmed</first><last>Sakr</last><affiliation>Institute</affiliation></author>
      <author><first>Hossam</first><last>Elkordi</last><affiliation>Institute</affiliation></author>
      <pages>198-203</pages>
      <abstract/>
      <url hash="cb3910a2">2025.arabicnlp-sharedtasks.28</url>
      <bibkey>zaytoon-etal-2025-araminds</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.28</doi>
    </paper>
    <paper id="29">
      <title>Fahmni at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> Track 1: Multi-Agent Retrieval-Augmented Generation and Multi-Label Classification for <fixed-case>A</fixed-case>rabic Mental Health <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case></title>
      <author><first>Caroline</first><last>Sabty</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamad</first><last>Rasmy</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Eyad Badran</last><affiliation>Institute</affiliation></author>
      <author><first>Nourhan</first><last>Sakr</last><affiliation>Institute</affiliation></author>
      <author><first>Alia</first><last>El Bolock</last><affiliation>Institute</affiliation></author>
      <pages>204-212</pages>
      <abstract/>
      <url hash="4b346ac7">2025.arabicnlp-sharedtasks.29</url>
      <bibkey>sabty-etal-2025-fahmni</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.29</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>M</fixed-case>ed<fixed-case>G</fixed-case>ap<fixed-case>G</fixed-case>ab at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case>: Modular <fixed-case>LLM</fixed-case> Assignment for Gaps and Gabs in <fixed-case>A</fixed-case>rabic Medical Question Answering</title>
      <author><first>Baraa</first><last>Hikal</last><affiliation>Institute</affiliation></author>
      <pages>213-221</pages>
      <abstract/>
      <url hash="5185e015">2025.arabicnlp-sharedtasks.30</url>
      <bibkey>hikal-2025-medgapgab</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.30</doi>
    </paper>
    <paper id="31">
      <title>Egyhealth at General <fixed-case>A</fixed-case>rabic Health <fixed-case>QA</fixed-case> (<fixed-case>M</fixed-case>ed<fixed-case>A</fixed-case>rabi<fixed-case>Q</fixed-case>): An Enhanced <fixed-case>RAG</fixed-case> Framework with Large-Scale <fixed-case>A</fixed-case>rabic <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case> Medical Data</title>
      <author><first>Hossam</first><last>Amer</last><affiliation>Institute</affiliation></author>
      <author><first>Rawan</first><last>Tarek Taha</last><affiliation>Institute</affiliation></author>
      <author><first>Gannat</first><last>Elsayed</last><affiliation>Institute</affiliation></author>
      <author><first>Ensaf</first><last>Hussein Mohamed</last><affiliation>Institute</affiliation></author>
      <pages>222-225</pages>
      <abstract/>
      <url hash="f282fea1">2025.arabicnlp-sharedtasks.31</url>
      <bibkey>amer-etal-2025-egyhealth</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.31</doi>
    </paper>
    <paper id="32">
      <title>muc<fixed-case>AI</fixed-case> at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case> 2025: Explain–Retrieve–Verify (<fixed-case>ERV</fixed-case>) Workflow for Multi-Label <fixed-case>A</fixed-case>rabic Health <fixed-case>QA</fixed-case> Classification</title>
      <author><first>Ahmed</first><last>Abdou</last><affiliation>Institute</affiliation></author>
      <pages>226-232</pages>
      <abstract/>
      <url hash="d51c096a">2025.arabicnlp-sharedtasks.32</url>
      <bibkey>abdou-2025-mucai</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>M</fixed-case>arsad<fixed-case>L</fixed-case>ab at <fixed-case>A</fixed-case>ra<fixed-case>H</fixed-case>ealth<fixed-case>QA</fixed-case>: Hybrid Contextual–Lexical Fusion with <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> for Question and Answer Categorization</title>
      <author><first>Mabrouka</first><last>Bessghaier</last><affiliation>Institute</affiliation></author>
      <author><first>Shimaa</first><last>Ibrahim</last><affiliation>Institute</affiliation></author>
      <author><first>Md.</first><last>Rafiul Biswas</last><affiliation>Institute</affiliation></author>
      <author><first>Wajdi</first><last>Zaghouani</last><affiliation>Institute</affiliation></author>
      <pages>233-238</pages>
      <abstract/>
      <url hash="79262e8a">2025.arabicnlp-sharedtasks.33</url>
      <bibkey>bessghaier-etal-2025-marsadlab</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.33</doi>
    </paper>
    <paper id="34">
      <title><fixed-case>BAREC</fixed-case> Shared Task 2025 on <fixed-case>A</fixed-case>rabic Readability Assessment</title>
      <author><first>Khalid N.</first><last>Elmadani</last><affiliation>New York University, Abu Dhabi</affiliation></author>
      <author id="bashar-alhafni"><first>Bashar</first><last>Alhafni</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author id="hanada-taha-thomure" orcid="0000-0003-4111-0137"><first>Hanada</first><last>Taha</last></author>
      <author id="nizar-habash" orcid="0000-0002-1831-3457"><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>239-252</pages>
      <abstract>We present the results and findings of the BAREC Shared Task 2025 on Arabic Readability Assessment, organized as part of The Third Arabic Natural Language Processing Conference (ArabicNLP 2025). The BAREC 2025 shared task focuses on automatic readability assessment using BAREC Corpus, addressing fine-grained classification into 19 readability levels. The shared task includes two sub-tasks: sentence-level classification and document-level classification, and three tracks: (1) Strict Track, where only BAREC Corpus is allowed; (2) Constrained Track, restricted to the BAREC Corpus, SAMER Corpus, and SAMER Lexicon, and (3) Open Track, allowing any external resources. A total of 22 teams from 12 countries registered for the task. Among these, 17 teams submitted system description papers. The winning team achieved 87.5 QWK on the sentence-level task and 87.4 QWK on the document-level task.</abstract>
      <url hash="18ae444b">2025.arabicnlp-sharedtasks.34</url>
      <bibkey>elmadani-etal-2025-barec</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.34</doi>
    </paper>
    <paper id="35">
      <title>Syntaxa at <fixed-case>BAREC</fixed-case> Shared Task 2025: <fixed-case>BERT</fixed-case>n<fixed-case>P</fixed-case>arse - Fusion of <fixed-case>BERT</fixed-case> and Dependency Graphs for Readability Prediction</title>
      <author><first>Ahmed</first><last>Bahloul</last><affiliation>Institute</affiliation></author>
      <pages>253-260</pages>
      <abstract/>
      <url hash="0375d067">2025.arabicnlp-sharedtasks.35</url>
      <bibkey>bahloul-2025-syntaxa</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>GNN</fixed-case>injas at <fixed-case>BAREC</fixed-case> Shared Task 2025: Lexicon-Enriched Graph Modeling for <fixed-case>A</fixed-case>rabic Document Readability Prediction</title>
      <author><first>Passant</first><last>Elchafei</last><affiliation>Institute</affiliation></author>
      <author><first>Mayar</first><last>Osama</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamad</first><last>Rageh</last><affiliation>Institute</affiliation></author>
      <author><first>Mervat</first><last>Abu-Elkheir</last><affiliation>Institute</affiliation></author>
      <pages>261-265</pages>
      <abstract/>
      <url hash="ccacecbc">2025.arabicnlp-sharedtasks.36</url>
      <bibkey>elchafei-etal-2025-gnninjas</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.36</doi>
    </paper>
    <paper id="37">
      <title><fixed-case>ZAI</fixed-case> at <fixed-case>BAREC</fixed-case> Shared Task 2025: <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> <fixed-case>CORAL</fixed-case> for Fine Grained <fixed-case>A</fixed-case>rabic Readability</title>
      <author><first>Ahmad</first><last>M. Nazzal</last><affiliation>Institute</affiliation></author>
      <pages>266-268</pages>
      <abstract/>
      <url hash="eed43c5e">2025.arabicnlp-sharedtasks.37</url>
      <bibkey>m-nazzal-2025-zai</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.37</doi>
    </paper>
    <paper id="38">
      <title><fixed-case>ANLP</fixed-case>ers at <fixed-case>BAREC</fixed-case> Shared Task 2025: Readability of Embeddings Training Neural Readability Classifiers on the <fixed-case>BAREC</fixed-case> Corpus</title>
      <author><first>Serry</first><last>Sibaee</last><affiliation>Institute</affiliation></author>
      <author><first>Omer</first><last>Nacar</last><affiliation>Institute</affiliation></author>
      <author><first>Yasser</first><last>Alhabashi</last><affiliation>Institute</affiliation></author>
      <author><first>Adel</first><last>Ammar</last><affiliation>Institute</affiliation></author>
      <author><first>Wadii</first><last>Boulila</last><affiliation>Institute</affiliation></author>
      <pages>269-273</pages>
      <abstract/>
      <url hash="0e127dc3">2025.arabicnlp-sharedtasks.38</url>
      <bibkey>sibaee-etal-2025-anlpers</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.38</doi>
    </paper>
    <paper id="39">
      <title><fixed-case>M</fixed-case>arsad<fixed-case>L</fixed-case>ab at <fixed-case>BAREC</fixed-case> Shared Task 2025: Strict-Track Readability Prediction with Specialized <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> Models on <fixed-case>BAREC</fixed-case></title>
      <author><first>Shimaa</first><last>Ibrahim</last><affiliation>Institute</affiliation></author>
      <author><first>Md.</first><last>Rafiul Biswas</last><affiliation>Institute</affiliation></author>
      <author><first>Mabrouka</first><last>Bessghaier</last><affiliation>Institute</affiliation></author>
      <author><first>Wajdi</first><last>Zaghouani</last><affiliation>Institute</affiliation></author>
      <pages>274-279</pages>
      <abstract/>
      <url hash="d7197469">2025.arabicnlp-sharedtasks.39</url>
      <bibkey>ibrahim-etal-2025-marsadlab</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>SATL</fixed-case>ab at <fixed-case>BAREC</fixed-case> Shared Task 2025: Optimizing a Language-Independent System for Fine-Grained Readability Assessment</title>
      <author><first>Yves</first><last>Bestgen</last><affiliation>Institute</affiliation></author>
      <pages>280-285</pages>
      <abstract/>
      <url hash="f1958c8e">2025.arabicnlp-sharedtasks.40</url>
      <bibkey>bestgen-2025-satlab</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.40</doi>
    </paper>
    <paper id="41">
      <title><fixed-case>M</fixed-case>orpho<fixed-case>A</fixed-case>rabia at <fixed-case>BAREC</fixed-case> Shared Task 2025: A Hybrid Architecture with Morphological Analysis for <fixed-case>A</fixed-case>rabic Readability Assessment</title>
      <author><first>Fatimah</first><last>Mohamed Emad Elden</last><affiliation>Institute</affiliation></author>
      <pages>286-296</pages>
      <abstract/>
      <url hash="92ca3d3f">2025.arabicnlp-sharedtasks.41</url>
      <bibkey>mohamed-emad-elden-2025-morphoarabia</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.41</doi>
    </paper>
    <paper id="42">
      <title>!<fixed-case>MSA</fixed-case> at <fixed-case>BAREC</fixed-case> Shared Task 2025: Ensembling <fixed-case>A</fixed-case>rabic Transformers for Readability Assessment</title>
      <author><first>Mohamed</first><last>Basem</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Younes</last><affiliation>Institute</affiliation></author>
      <author><first>Seif</first><last>Ahmed</last><affiliation>Institute</affiliation></author>
      <author><first>Abdelrahman</first><last>Moustafa</last><affiliation>Institute</affiliation></author>
      <pages>297-305</pages>
      <abstract/>
      <url hash="4711f39b">2025.arabicnlp-sharedtasks.42</url>
      <bibkey>basem-etal-2025-msa</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.42</doi>
    </paper>
    <paper id="43">
      <title>Qais at <fixed-case>BAREC</fixed-case> Shared Task 2025: A Fine-Grained Approach for <fixed-case>A</fixed-case>rabic Readability Classification Using a pre-trained model</title>
      <author><first>Samar</first><last>Ahmad</last><affiliation>Institute</affiliation></author>
      <pages>306-311</pages>
      <abstract/>
      <url hash="16ca42ba">2025.arabicnlp-sharedtasks.43</url>
      <bibkey>ahmad-2025-qais</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.43</doi>
    </paper>
    <paper id="44">
      <title>muc<fixed-case>AI</fixed-case> at <fixed-case>BAREC</fixed-case> Shared Task 2025: Towards Uncertainty Aware <fixed-case>A</fixed-case>rabic Readability Assessment</title>
      <author><first>Ahmed</first><last>Abdou</last><affiliation>Institute</affiliation></author>
      <pages>312-319</pages>
      <abstract/>
      <url hash="e47ede7a">2025.arabicnlp-sharedtasks.44</url>
      <bibkey>abdou-2025-mucai-barec</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.44</doi>
    </paper>
    <paper id="45">
      <title><fixed-case>AMAR</fixed-case> at <fixed-case>BAREC</fixed-case> Shared Task 2025: <fixed-case>A</fixed-case>rabic Meta-learner for Assessing Readability</title>
      <author><first>Mostafa</first><last>Saeed</last><affiliation>Institute</affiliation></author>
      <author><first>Rana</first><last>Waly</last><affiliation>Institute</affiliation></author>
      <author><first>Abdelaziz</first><last>Ashraf Hussein</last><affiliation>Institute</affiliation></author>
      <pages>320-330</pages>
      <abstract/>
      <url hash="b188cfa4">2025.arabicnlp-sharedtasks.45</url>
      <bibkey>saeed-etal-2025-amar</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.45</doi>
    </paper>
    <paper id="46">
      <title>Noor at <fixed-case>BAREC</fixed-case> Shared Task 2025: A Hybrid Transformer-Feature Architecture for Sentence-level Readability Assessment</title>
      <author><first>Nour</first><last>Rabih</last><affiliation>Institute</affiliation></author>
      <pages>331-342</pages>
      <abstract/>
      <url hash="2c301a4e">2025.arabicnlp-sharedtasks.46</url>
      <bibkey>rabih-2025-noor</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.46</doi>
    </paper>
    <paper id="47">
      <title><fixed-case>P</fixed-case>al<fixed-case>NLP</fixed-case> at <fixed-case>BAREC</fixed-case> Shared Task 2025: Predicting <fixed-case>A</fixed-case>rabic Readability Using Ordinal Regression and K-Fold Ensemble Learning</title>
      <author><first>Mutaz</first><last>Ayesh</last><affiliation>Institute</affiliation></author>
      <pages>343-349</pages>
      <abstract/>
      <url hash="1324c2ee">2025.arabicnlp-sharedtasks.47</url>
      <bibkey>ayesh-2025-palnlp</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.47</doi>
    </paper>
    <paper id="48">
      <title>Pixels at <fixed-case>BAREC</fixed-case> Shared Task 2025: Visual <fixed-case>A</fixed-case>rabic Readability Assessment</title>
      <author><first>Ben</first><last>Sapirstein</last><affiliation>Institute</affiliation></author>
      <pages>350-356</pages>
      <abstract/>
      <url hash="c1daf8bd">2025.arabicnlp-sharedtasks.48</url>
      <bibkey>sapirstein-2025-pixels</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.48</doi>
    </paper>
    <paper id="49">
      <title>Phantoms at <fixed-case>BAREC</fixed-case> Shared Task 2025: Enhancing <fixed-case>A</fixed-case>rabic Readability Prediction with Hybrid <fixed-case>BERT</fixed-case> and Linguistic Features</title>
      <author><first>Ahmed</first><last>Alhassan</last><affiliation>Institute</affiliation></author>
      <author><first>Asim</first><last>Mohamed</last><affiliation>Institute</affiliation></author>
      <author><first>Moayad</first><last>Elamin</last><affiliation>Institute</affiliation></author>
      <pages>357-361</pages>
      <abstract/>
      <url hash="a61ca820">2025.arabicnlp-sharedtasks.49</url>
      <bibkey>alhassan-etal-2025-phantoms</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.49</doi>
    </paper>
    <paper id="50">
      <title><fixed-case>STBW</fixed-case> at <fixed-case>BAREC</fixed-case> Shared Task 2025: <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case>-v2 with <fixed-case>MSE</fixed-case>-<fixed-case>S</fixed-case>oft<fixed-case>QWK</fixed-case> Loss for Sentence-Level <fixed-case>A</fixed-case>rabic Readability</title>
      <author><first>Saoussan</first><last>Trigui</last><affiliation>Institute</affiliation></author>
      <pages>362-366</pages>
      <abstract/>
      <url hash="2846ceca">2025.arabicnlp-sharedtasks.50</url>
      <bibkey>trigui-2025-stbw</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.50</doi>
    </paper>
    <paper id="51">
      <title><fixed-case>LIS</fixed-case> at <fixed-case>BAREC</fixed-case> Shared Task 2025: Multi-Scale Curriculum Learning for <fixed-case>A</fixed-case>rabic Sentence-Level Readability Assessment Using Pre-trained Language Models</title>
      <author><first>Anya</first><last>Amel Nait Djoudi</last><affiliation>Institute</affiliation></author>
      <author id="patrice-bellot"><first>Patrice</first><last>Bellot</last><affiliation>Institute</affiliation></author>
      <author><first>Adrian-Gabriel</first><last>Chifu</last><affiliation>Institute</affiliation></author>
      <pages>367-375</pages>
      <abstract/>
      <url hash="9bfbe2ee">2025.arabicnlp-sharedtasks.51</url>
      <bibkey>amel-nait-djoudi-etal-2025-lis</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.51</doi>
    </paper>
    <paper id="52">
      <title><fixed-case>I</fixed-case>mage<fixed-case>E</fixed-case>val 2025: The First <fixed-case>A</fixed-case>rabic Image Captioning Shared Task</title>
      <author><first>Ahlam</first><last>Bashiti</last><affiliation>Birzeit University</affiliation></author>
      <author><first>Alaa</first><last>Aljabari</last><affiliation>Birzeit University</affiliation></author>
      <author><first>Hadi Khaled</first><last>Hamoud</last><affiliation>Arab Center for Research and Policy Studies</affiliation></author>
      <author id="md-rafiul-biswas" orcid="0000-0002-5145-1990"><first>Md. Rafiul</first><last>Biswas</last></author>
      <author><first>Bilal Mohammed</first><last>Shalash</last><affiliation>Birzeit University and Arab Center for Research and Policy Studies</affiliation></author>
      <author id="mustafa-jarrar" orcid="0000-0003-4351-4207"><first>Mustafa</first><last>Jarrar</last><affiliation>Birzeit University</affiliation></author>
      <author id="fadi-a-zaraket" orcid="0000-0001-5909-6375"><first>Fadi</first><last>Zaraket</last><affiliation>Arab Center for Research and Policy Studies and American University of Beirut</affiliation></author>
      <author id="george-mikros" orcid="0000-0002-4093-5973"><first>George</first><last>Mikros</last><affiliation>Hamad Bin Khalifa University and University of Massachusetts Boston</affiliation></author>
      <author><first>Ehsaneddin</first><last>Asgari</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author id="wajdi-zaghouani" orcid="0000-0003-1521-5568"><first>Wajdi</first><last>Zaghouani</last><affiliation>Northwestern University</affiliation></author>
      <pages>376-389</pages>
      <abstract>We present ImageEval 2025, the first shared task dedicated to Arabic image captioning. The task addresses the critical gap in multimodal Arabic NLP by focusing on two complementary subtasks: (1) creating the first open-source, manually-captioned Arabic image dataset through a collaborative datathon, and (2) developing and evaluating Arabic image captioning models. A total of 44 teams registered, of which eight submitted during the test phase, producing 111 valid submissions. Evaluation was conducted using automatic metrics, LLM-based judgment, and human assessment. In Subtask 1, the best-performing system achieved a cosine similarity of 65.5, while in Subtask 2, the top score was 60.0. Although these results show encouraging progress, they also confirm that Arabic image captioning remains a challenging task, particularly due to cultural grounding requirements, morphological richness, and dialectal variation. All datasets, baseline models, and evaluation tools are released publicly to support future research in Arabic multimodal NLP.</abstract>
      <url hash="0df57325">2025.arabicnlp-sharedtasks.52</url>
      <bibkey>bashiti-etal-2025-imageeval</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.52</doi>
    </paper>
    <paper id="53">
      <title>Codezone Research Group at <fixed-case>I</fixed-case>mage<fixed-case>E</fixed-case>val Shared-Task 2: <fixed-case>A</fixed-case>rabic Image Captioning Using <fixed-case>BLIP</fixed-case> and <fixed-case>M</fixed-case>2<fixed-case>M</fixed-case>100: A Two-Stage Translation Approach for <fixed-case>I</fixed-case>mage<fixed-case>E</fixed-case>val 2025</title>
      <author><first>Abdulkadir</first><last>Shehu Bichi</last><affiliation>Institute</affiliation></author>
      <pages>390-394</pages>
      <abstract/>
      <url hash="3d631e51">2025.arabicnlp-sharedtasks.53</url>
      <bibkey>shehu-bichi-2025-codezone</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.53</doi>
    </paper>
    <paper id="54">
      <title><fixed-case>BZU</fixed-case>-<fixed-case>AUM</fixed-case>@<fixed-case>I</fixed-case>mage<fixed-case>E</fixed-case>val2025: An <fixed-case>A</fixed-case>rabic Image Captioning Dataset for Conflict Narratives with Human Annotation</title>
      <author><first>Mohammed</first><last>Alkhanafseh</last><affiliation>Institute</affiliation></author>
      <author><first>Ola</first><last>Surakhi</last><affiliation>Institute</affiliation></author>
      <author><first>Abdallah</first><last>Abedaljalill</last><affiliation>Institute</affiliation></author>
      <pages>395-400</pages>
      <abstract/>
      <url hash="23dc8550">2025.arabicnlp-sharedtasks.54</url>
      <bibkey>alkhanafseh-etal-2025-bzu</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.54</doi>
    </paper>
    <paper id="55">
      <title><fixed-case>I</fixed-case>mpact<fixed-case>A</fixed-case>i at <fixed-case>I</fixed-case>mage<fixed-case>E</fixed-case>val 2025 Shared Task: Region-Aware Transformers for <fixed-case>A</fixed-case>rabic Image Captioning – A Case Study on the Palestinian Narrative</title>
      <author><first>Rabee</first><last>Al-Qasem</last><affiliation>Institute</affiliation></author>
      <author><first>Mohannad</first><last>Hendi</last><affiliation>Institute</affiliation></author>
      <pages>401-407</pages>
      <abstract/>
      <url hash="0876d698">2025.arabicnlp-sharedtasks.55</url>
      <bibkey>al-qasem-hendi-2025-impactai</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.55</doi>
    </paper>
    <paper id="56">
      <title><fixed-case>VLCAP</fixed-case> at <fixed-case>I</fixed-case>mage<fixed-case>E</fixed-case>val 2025 Shared Task: Multimodal <fixed-case>A</fixed-case>rabic Captioning with Interpretable Visual Concept Integration</title>
      <author><first>Passant</first><last>Elchafei</last><affiliation>Institute</affiliation></author>
      <author><first>Amany</first><last>Fashwan</last><affiliation>Institute</affiliation></author>
      <pages>408-413</pages>
      <abstract/>
      <url hash="f5272946">2025.arabicnlp-sharedtasks.56</url>
      <bibkey>elchafei-fashwan-2025-vlcap</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.56</doi>
    </paper>
    <paper id="57">
      <title><fixed-case>P</fixed-case>hantom<fixed-case>T</fixed-case>roupe at <fixed-case>I</fixed-case>mage<fixed-case>E</fixed-case>val 2025 Shared Task: Multimodal <fixed-case>A</fixed-case>rabic Image Captioning through Translation-Based Fine-Tuning of <fixed-case>LLM</fixed-case> Models</title>
      <author><first>Muhammad</first><last>Abu Horaira</last><affiliation>Institute</affiliation></author>
      <author><first>Farhan</first><last>Amin</last><affiliation>Institute</affiliation></author>
      <author><first>Sakibul</first><last>Hasan</last><affiliation>Institute</affiliation></author>
      <author><first>Md.</first><last>Tanvir Ahammed Shawon</last><affiliation>Institute</affiliation></author>
      <author><first>Muhammad</first><last>Ibrahim Khan</last><affiliation>Institute</affiliation></author>
      <pages>414-418</pages>
      <abstract/>
      <url hash="84d39a13">2025.arabicnlp-sharedtasks.57</url>
      <bibkey>abu-horaira-etal-2025-phantomtroupe</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.57</doi>
    </paper>
    <paper id="58">
      <title><fixed-case>NU</fixed-case>_<fixed-case>I</fixed-case>nternship team at <fixed-case>I</fixed-case>mage<fixed-case>E</fixed-case>val 2025: From Zero-Shot to Ensembles: Enhancing Grounded <fixed-case>A</fixed-case>rabic Image Captioning</title>
      <author><first>Rana</first><last>Gaber</last><affiliation>Institute</affiliation></author>
      <author><first>Seif</first><last>Eldin Amgad</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmed</first><last>Sherif Nasri</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Ibrahim Ragab</last><affiliation>Institute</affiliation></author>
      <author><first>Ensaf</first><last>Hussein Mohamed</last><affiliation>Institute</affiliation></author>
      <pages>419-431</pages>
      <abstract/>
      <url hash="3ff394a2">2025.arabicnlp-sharedtasks.58</url>
      <bibkey>gaber-etal-2025-nu</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.58</doi>
    </paper>
    <paper id="59">
      <title>Averroes at <fixed-case>I</fixed-case>mage<fixed-case>E</fixed-case>val 2025 Shared Task: Advancing <fixed-case>A</fixed-case>rabic Image Captioning with Augmentation and Two-Stage Generation</title>
      <author><first>Mariam</first><last>Saeed</last><affiliation>Institute</affiliation></author>
      <author><first>Sarah</first><last>Elshabrawy</last><affiliation>Institute</affiliation></author>
      <author><first>Abdelrahman</first><last>Hagrass</last><affiliation>Institute</affiliation></author>
      <author><first>Mazen</first><last>Yasser</last><affiliation>Institute</affiliation></author>
      <author><first>Ayman</first><last>Khalafallah</last><affiliation>Institute</affiliation></author>
      <pages>432-437</pages>
      <abstract/>
      <url hash="1665e1ef">2025.arabicnlp-sharedtasks.59</url>
      <bibkey>saeed-etal-2025-averroes</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.59</doi>
    </paper>
    <paper id="60">
      <title><fixed-case>AZLU</fixed-case> at <fixed-case>I</fixed-case>mag<fixed-case>E</fixed-case>val Shared Task: Bridging Linguistics and Cultural Gaps in <fixed-case>A</fixed-case>rabic Image Captioning</title>
      <author><first>Sarah</first><last>Yassine</last><affiliation>Institute</affiliation></author>
      <pages>438-442</pages>
      <abstract/>
      <url hash="f99e5fff">2025.arabicnlp-sharedtasks.60</url>
      <bibkey>yassine-2025-azlu</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.60</doi>
    </paper>
    <paper id="61">
      <title>Iqra’Eval: A Shared Task on Qur’anic Pronunciation Assessment</title>
      <author><first>Yassine</first><last>El Kheir</last></author>
      <author id="amit-meghanani" orcid="0000-0002-0811-274X"><first>Amit</first><last>Meghanani</last></author>
      <author id="hawau-olamide-toyin" orcid="0009-0008-3927-526X"><first>Hawau Olamide</first><last>Toyin</last></author>
      <author><first>Nada</first><last>Almarwani</last><affiliation>taibah university</affiliation></author>
      <author><first>Omnia</first><last>Ibrahim</last><affiliation>NA</affiliation></author>
      <author><first>Yousseif Ahmed</first><last>Elshahawy</last><affiliation>HUMAIN</affiliation></author>
      <author id="mostafa-shahin" orcid="0000-0002-1091-8531"><first>Mostafa</first><last>Shahin</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Ahmed</first><last>Ali</last><affiliation>Saudi Data and AI Authority, Saudi Data and AI Authority</affiliation></author>
      <pages>443-452</pages>
      <abstract>We present the findings of the first shared task on Qur’anic pronunciation assessment, which focuses on addressing the unique challenges of evaluating the precise pronunciation of Qur’anic recitation. To fill an existing research gap, the Iqra’Eval 2025 shared task introduces the first open benchmark for Mispronunciation Detection and Diagnosis (MDD) in Qur’anic recitation, using Modern Standard Arabic (MSA) reading of Qur’anic texts as its case study. The task provides a comprehensive evaluation framework with increasingly complex subtasks: error localization and detailed error diagnosis. Leveraging the recently developed QuranMB benchmark dataset along with auxiliary training resources, this shared task aims to stimulate research in an area of both linguistic and cultural significance while addressing computational challenges in pronunciation assessment.</abstract>
      <url hash="efa0f249">2025.arabicnlp-sharedtasks.61</url>
      <bibkey>el-kheir-etal-2025-iqraeval</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.61</doi>
    </paper>
    <paper id="62">
      <title><fixed-case>H</fixed-case>afs2<fixed-case>V</fixed-case>ec: A System for the <fixed-case>I</fixed-case>qra<fixed-case>E</fixed-case>val <fixed-case>A</fixed-case>rabic and Qur’anic Phoneme-level Pronunciation Assessment</title>
      <author><first>Ahmed</first><last>Ibrahim</last><affiliation>Institute</affiliation></author>
      <pages>453-456</pages>
      <abstract/>
      <url hash="27cd0ef1">2025.arabicnlp-sharedtasks.62</url>
      <bibkey>ibrahim-2025-hafs2vec</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.62</doi>
    </paper>
    <paper id="63">
      <title>Phoneme-level mispronunciation detection in <fixed-case>Q</fixed-case>uranic recitation using <fixed-case>S</fixed-case>hallow<fixed-case>T</fixed-case>ransformer</title>
      <author><first>Mohamed Nadhir</first><last>Daoud</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed Anouar</first><last>Ben Messaoud</last><affiliation>Institute</affiliation></author>
      <pages>457-463</pages>
      <abstract>Preserving the integrity of Qur’anic recitation requires accurate pronunciation, as even subtle mispronunciations can alter meaning. Automatic assessment of Qur’anic recitation at the phoneme level is therefore a critical and challenging task. We present ShallowTransformer, a lightweight and computationally efficient transformer model leveraging Wav2vec2.0 features and trained with CTC loss for phoneme-level mispronunciation detection. Evaluated on the Iqra’Eval benchmark (QuranMB.v2), our model outperforms published BiLSTM baselines on QuranMB.v1 while achieving competitive performance relative to the official Iqra’Eval challenge baselines, which are not yet fully documented. Such improvements are particularly important in assisted Qur’an learning, as accurate phonetic feedback supports correct recitation and preserves textual integrity. These results highlight the effectiveness of transformer architectures in capturing subtle pronunciation errors while remaining deployable for practical applications.</abstract>
      <url hash="88e8bb34">2025.arabicnlp-sharedtasks.63</url>
      <bibkey>nadhir-daoud-anouar-ben-messaoud-2025-phoneme</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.63</doi>
    </paper>
    <paper id="64">
      <title><fixed-case>ANPL</fixed-case>ers at <fixed-case>I</fixed-case>qra<fixed-case>E</fixed-case>val Shared task: Adapting Whisper-large-v3 as Speech-to-Phoneme for Qur’anic Recitation Mispronunciation Detection</title>
      <author><first>Nour</first><last>Qandos</last><affiliation>Institute</affiliation></author>
      <author><first>Serry</first><last>Sibaee</last><affiliation>Institute</affiliation></author>
      <author><first>Samar</first><last>Ahmad</last><affiliation>Institute</affiliation></author>
      <author><first>Omer</first><last>Nacar</last><affiliation>Institute</affiliation></author>
      <author><first>Adel</first><last>Ammar</last><affiliation>Institute</affiliation></author>
      <author><first>Wadii</first><last>Boulila</last><affiliation>Institute</affiliation></author>
      <author><first>Yasser</first><last>Alhabashi</last><affiliation>Institute</affiliation></author>
      <pages>464-468</pages>
      <abstract/>
      <url hash="0075b696">2025.arabicnlp-sharedtasks.64</url>
      <bibkey>qandos-etal-2025-anplers</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.64</doi>
    </paper>
    <paper id="65">
      <title><fixed-case>A</fixed-case>ra<fixed-case>S</fixed-case>2<fixed-case>P</fixed-case>: <fixed-case>A</fixed-case>rabic Speech-to-Phonemes System</title>
      <author><first>Bassam</first><last>Mattar</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Fayed</last><affiliation>Institute</affiliation></author>
      <author><first>Ayman</first><last>Khalafallah</last><affiliation>Institute</affiliation></author>
      <pages>469-474</pages>
      <abstract/>
      <url hash="64c1457a">2025.arabicnlp-sharedtasks.65</url>
      <bibkey>mattar-etal-2025-aras2p</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.65</doi>
    </paper>
    <paper id="66">
      <title>Metapseud at Iqra’Eval: Domain Adaptation with Multi-Stage Fine-Tuning for Phoneme-Level Qur’anic Mispronunciation Detection</title>
      <author><first>Ayman</first><last>Mansour</last><affiliation>Institute</affiliation></author>
      <pages>475-479</pages>
      <abstract/>
      <url hash="db5efb4e">2025.arabicnlp-sharedtasks.66</url>
      <bibkey>mansour-2025-metapseud</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.66</doi>
    </paper>
    <paper id="67">
      <title><fixed-case>I</fixed-case>slamic<fixed-case>E</fixed-case>val 2025: The First Shared Task of Capturing <fixed-case>LLM</fixed-case>s Hallucination in Islamic Content</title>
      <author id="hamdy-mubarak" orcid="0000-0003-4828-6098"><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Rana</first><last>Malhas</last><affiliation>University of Qatar</affiliation></author>
      <author><first>Watheq</first><last>Mansour</last></author>
      <author id="abubakr-mohamed" orcid="0000-0002-7854-7589"><first>Abubakr</first><last>Mohamed</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author><first>Mahmoud</first><last>Fawzi</last></author>
      <author id="majd-hawasly" orcid="0000-0003-1823-5580"><first>Majd</first><last>Hawasly</last></author>
      <author><first>Tamer</first><last>Elsayed</last><affiliation>Qatar University</affiliation></author>
      <author><first>Kareem Mohamed</first><last>Darwish</last><affiliation>aiXplain Inc</affiliation></author>
      <author id="walid-magdy" orcid="0000-0001-9676-1338"><first>Walid</first><last>Magdy</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>480-493</pages>
      <abstract>Hallucination in Large Language Models (LLMs) remains a significant challenge and continues to draw substantial research attention. The problem becomes especially critical when hallucinations arise in sensitive domains, such as religious discourse. To address this gap, we introduce IslamicEval 2025—the first shared task specifically focused on evaluating and detecting hallucinations in Islamic content. The task consists of two subtasks: (1) Hallucination Detection and Correction of quoted verses (Ayahs) from the Holy Quran and quoted Hadiths; and (2) Qur’an and Hadith Question Answering, which assesses retrieval models and LLMs by requiring answers to be retrieved from grounded, authoritative sources. Thirteen teams participated in the final phase of the shared task, employing a range of pipelines and frameworks. Their diverse approaches underscore both the complexity of the task and the importance of effectively managing hallucinations in Islamic discourse.</abstract>
      <url hash="31f63b6c">2025.arabicnlp-sharedtasks.67</url>
      <bibkey>mubarak-etal-2025-islamiceval</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.67</doi>
    </paper>
    <paper id="68">
      <title><fixed-case>NUR</fixed-case> at <fixed-case>I</fixed-case>slamic<fixed-case>E</fixed-case>val 2025 Shared Task: Retrieval-Augmented <fixed-case>LLM</fixed-case>s for Qur’an and <fixed-case>H</fixed-case>adith <fixed-case>QA</fixed-case></title>
      <author><first>Serag</first><last>Amin</last><affiliation>Institute</affiliation></author>
      <author><first>Ranwa</first><last>Aly</last><affiliation>Institute</affiliation></author>
      <author><first>Yara</first><last>Allam</last><affiliation>Institute</affiliation></author>
      <author><first>Yomna</first><last>Eid</last><affiliation>Institute</affiliation></author>
      <author><first>Ensaf</first><last>Hussein Mohamed</last><affiliation>Institute</affiliation></author>
      <pages>494-502</pages>
      <abstract/>
      <url hash="3c0d944b">2025.arabicnlp-sharedtasks.68</url>
      <bibkey>amin-etal-2025-nur</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.68</doi>
    </paper>
    <paper id="69">
      <title><fixed-case>B</fixed-case>urhan<fixed-case>AI</fixed-case> at <fixed-case>I</fixed-case>slamic<fixed-case>E</fixed-case>val 2025 Shared Task: Combating Hallucinations in <fixed-case>LLM</fixed-case>s for Islamic Content; Evaluation, Correction, and Retrieval-Based Solution</title>
      <author><first>Arij</first><last>Al Adel</last><affiliation>Institute</affiliation></author>
      <author><first>Abu</first><last>Bakr Soliman</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Sakher Sawan</last><affiliation>Institute</affiliation></author>
      <author><first>Rahaf</first><last>Al-Najjar</last><affiliation>Institute</affiliation></author>
      <author><first>Sameh</first><last>Amin</last><affiliation>Institute</affiliation></author>
      <pages>503-508</pages>
      <abstract/>
      <url hash="c052e488">2025.arabicnlp-sharedtasks.69</url>
      <bibkey>al-adel-etal-2025-burhanai</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.69</doi>
    </paper>
    <paper id="70">
      <title><fixed-case>HUMAIN</fixed-case> at <fixed-case>I</fixed-case>slamic<fixed-case>E</fixed-case>val 2025 Shared Task 1: A Three-Stage <fixed-case>LLM</fixed-case>-Based Pipeline for Detecting and Correcting Hallucinations in <fixed-case>Q</fixed-case>uran and <fixed-case>H</fixed-case>adith</title>
      <author><first>Arwa</first><last>Omayrah</last><affiliation>Institute</affiliation></author>
      <author><first>Sakhar</first><last>Alkhereyf</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmed</first><last>Abdelali</last><affiliation>Institute</affiliation></author>
      <author><first>Abdulmohsen</first><last>Al-Thubaity</last><affiliation>Institute</affiliation></author>
      <author><first>Jeril</first><last>Kuriakose</last><affiliation>Institute</affiliation></author>
      <author><first>Ibrahim</first><last>AbdulMajeed</last><affiliation>Institute</affiliation></author>
      <pages>509-514</pages>
      <abstract/>
      <url hash="8f7ae49a">2025.arabicnlp-sharedtasks.70</url>
      <bibkey>omayrah-etal-2025-humain</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.70</doi>
    </paper>
    <paper id="71">
      <title><fixed-case>TCE</fixed-case> at <fixed-case>I</fixed-case>slamic<fixed-case>E</fixed-case>val 2025: Retrieval-Augmented <fixed-case>LLM</fixed-case>s for <fixed-case>Q</fixed-case>uranic and <fixed-case>H</fixed-case>adith Content Identification and Verification</title>
      <author><first>Mohammed</first><last>ElKoumy</last><affiliation>Institute</affiliation></author>
      <author><first>Khalid</first><last>Allam</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmad</first><last>Tamer</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Alqablawi</last><affiliation>Institute</affiliation></author>
      <pages>515-527</pages>
      <abstract/>
      <url hash="74359637">2025.arabicnlp-sharedtasks.71</url>
      <bibkey>elkoumy-etal-2025-tce</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.71</doi>
    </paper>
    <paper id="72">
      <title><fixed-case>T</fixed-case>hink<fixed-case>D</fixed-case>rill at <fixed-case>I</fixed-case>slamic<fixed-case>E</fixed-case>val 2025 Shared Task: <fixed-case>LLM</fixed-case> Hybrid Approach for Qur’an and <fixed-case>H</fixed-case>adith Question Answering</title>
      <author><first>Eman</first><last>Elrefai</last><affiliation>Institute</affiliation></author>
      <author><first>Toka</first><last>Khaled</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmed</first><last>Soliman</last><affiliation>Institute</affiliation></author>
      <pages>528-533</pages>
      <abstract/>
      <url hash="1410d1ca">2025.arabicnlp-sharedtasks.72</url>
      <bibkey>elrefai-etal-2025-thinkdrill</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.72</doi>
    </paper>
    <paper id="73">
      <title>Burhan at <fixed-case>I</fixed-case>slamic<fixed-case>E</fixed-case>val: Fact-Augmented and <fixed-case>LLM</fixed-case>-Driven Retrieval for Islamic <fixed-case>QA</fixed-case></title>
      <author><first>Mohammad</first><last>Basheer</last><affiliation>Institute</affiliation></author>
      <author><first>Watheq</first><last>Mansour</last><affiliation>Institute</affiliation></author>
      <author><first>Abdulhamid</first><last>Touma</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmad</first><last>Qadeib Alban</last><affiliation>Institute</affiliation></author>
      <pages>534-539</pages>
      <abstract/>
      <url hash="5dd6a28d">2025.arabicnlp-sharedtasks.73</url>
      <bibkey>basheer-etal-2025-burhan</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.73</doi>
    </paper>
    <paper id="74">
      <title>Isnad <fixed-case>AI</fixed-case> at <fixed-case>I</fixed-case>slamic<fixed-case>E</fixed-case>val 2025: A Rule-Based System for Identifying Religious Texts in <fixed-case>LLM</fixed-case> Outputs</title>
      <author><first>Fatimah</first><last>Mohamed Emad Elden</last><affiliation>Institute</affiliation></author>
      <pages>540-559</pages>
      <abstract/>
      <url hash="43c6fffc">2025.arabicnlp-sharedtasks.74</url>
      <bibkey>mohamed-emad-elden-2025-isnad</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.74</doi>
    </paper>
    <paper id="75">
      <title><fixed-case>MAHED</fixed-case> Shared Task: Multimodal Detection of Hope and Hate Emotions in <fixed-case>A</fixed-case>rabic Content</title>
      <author id="wajdi-zaghouani" orcid="0000-0003-1521-5568"><first>Wajdi</first><last>Zaghouani</last><affiliation>Northwestern University</affiliation></author>
      <author id="md-rafiul-biswas" orcid="0000-0002-5145-1990"><first>Md. Rafiul</first><last>Biswas</last></author>
      <author><first>Mabrouka</first><last>Bessghaier</last><affiliation>Northwestern University In Qatar</affiliation></author>
      <author id="shimaa-ibrahim" orcid="0000-0003-4014-8424"><first>Shimaa</first><last>Ibrahim</last><affiliation>Northwestern University In Qatar</affiliation></author>
      <author id="george-mikros" orcid="0000-0002-4093-5973"><first>George</first><last>Mikros</last><affiliation>Hamad Bin Khalifa University and University of Massachusetts Boston</affiliation></author>
      <author id="abul-hasnat" orcid="0000-0003-2748-8221"><first>Abul</first><last>Hasnat</last></author>
      <author id="firoj-alam" orcid="0000-0001-7172-1997"><first>Firoj</first><last>Alam</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>560-574</pages>
      <abstract>This paper presents the MAHED 2025 Shared Task on Multimodal Detection of Hope and Hate Emotions in Arabic Content, comprising three subtasks: (1) text-based classification of Arabic content into hate and hope, (2) multi-task learning for joint prediction of emotions, offensive content, and hate speech, and (3) multimodal detection of hateful content in Arabic memes. We provide three high-quality datasets totaling over 22,000 instances sourced from social media platforms, annotated by native Arabic speakers with Cohen’s Kappa exceeding 0.85. Our evaluation attracted 46 leaderboard submissions from participants, with systems leveraging Arabic-specific pre-trained language models (AraBERT, MARBERT), large language models (GPT-4, Gemini), and multimodal fusion architectures combining CLIP vision encoders with Arabic text models. The best-performing systems achieved macro F1-scores of 0.723 (Task 1), 0.578 (Task 2), and 0.796 (Task 3), with top teams employing ensemble methods, class-weighted training, and OCR-aware multimodal fusion. Analysis reveals persistent challenges in dialectal robustness, minority class detection for hope speech, and highlights key directions for future Arabic content moderation research.</abstract>
      <url hash="8a528a2d">2025.arabicnlp-sharedtasks.75</url>
      <bibkey>zaghouani-etal-2025-mahed</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.75</doi>
    </paper>
    <paper id="76">
      <title><fixed-case>NYUAD</fixed-case> at <fixed-case>MAHED</fixed-case> Shared Task: Detecting Hope, Hate, and Emotion in <fixed-case>A</fixed-case>rabic Textual Speech and Multi-modal Memes Using Large Language Models</title>
      <author><first>Nouar</first><last>AlDahoul</last><affiliation>Institute</affiliation></author>
      <author><first>Yasir</first><last>Zaki</last><affiliation>Institute</affiliation></author>
      <pages>575-584</pages>
      <abstract/>
      <url hash="6759a5ad">2025.arabicnlp-sharedtasks.76</url>
      <bibkey>aldahoul-zaki-2025-nyuad-mahed</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.76</doi>
    </paper>
    <paper id="77">
      <title><fixed-case>N</fixed-case>guyen<fixed-case>T</fixed-case>riet at <fixed-case>MAHED</fixed-case> Shared Task: Ensemble of <fixed-case>A</fixed-case>rabic <fixed-case>BERT</fixed-case> Models with Hierarchical Prediction and Soft Voting for Text-Based Hope and Hate Detection</title>
      <author><first>Nguyen</first><last>Minh Triet</last><affiliation>Institute</affiliation></author>
      <author><first>Thìn</first><last>Đặng Văn</last><affiliation>Institute</affiliation></author>
      <pages>585-589</pages>
      <abstract/>
      <url hash="330874ff">2025.arabicnlp-sharedtasks.77</url>
      <bibkey>minh-triet-dang-van-2025-nguyentriet</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.77</doi>
    </paper>
    <paper id="78">
      <title><fixed-case>ANLP</fixed-case>ers at <fixed-case>MAHED</fixed-case>2025: From Hate to Hope: Boosting <fixed-case>A</fixed-case>rabic Text Classification</title>
      <author><first>Yasser</first><last>Alhabashi</last><affiliation>Institute</affiliation></author>
      <author><first>Serry</first><last>Sibaee</last><affiliation>Institute</affiliation></author>
      <author><first>Omer</first><last>Nacar</last><affiliation>Institute</affiliation></author>
      <author><first>Adel</first><last>Ammar</last><affiliation>Institute</affiliation></author>
      <author><first>Wadii</first><last>Boulila</last><affiliation>Institute</affiliation></author>
      <pages>590-594</pages>
      <abstract/>
      <url hash="9eef8842">2025.arabicnlp-sharedtasks.78</url>
      <bibkey>alhabashi-etal-2025-anlpers</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.78</doi>
    </paper>
    <paper id="79">
      <title><fixed-case>L</fixed-case>ove<fixed-case>H</fixed-case>eaven at <fixed-case>MAHED</fixed-case> 2025: Text-based Hate and Hope Speech Classification Using <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case>-<fixed-case>T</fixed-case>witter Ensemble</title>
      <author><first>Nguyễn</first><last>Thiên Bảo</last><affiliation>Institute</affiliation></author>
      <author><first>Dang</first><last>Van Thin</last><affiliation>Institute</affiliation></author>
      <pages>595-598</pages>
      <abstract/>
      <url hash="1c73c7cf">2025.arabicnlp-sharedtasks.79</url>
      <bibkey>thien-bao-van-thin-2025-loveheaven</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.79</doi>
    </paper>
    <paper id="80">
      <title><fixed-case>CIC</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>MAHED</fixed-case> 2025 <fixed-case>TASK</fixed-case> 1:Assessing the Role of Bigram Augmentation in Multiclass <fixed-case>A</fixed-case>rabic Hate and Hope Speech Classification</title>
      <author><first>Tolulope</first><last>Olalekan Abiola</last><affiliation>Institute</affiliation></author>
      <author><first>Oluwatobi</first><last>Joseph Abiola</last><affiliation>Institute</affiliation></author>
      <author><first>Ogunleye</first><last>Temitope Dasola</last><affiliation>Institute</affiliation></author>
      <author><first>Tewodros</first><last>Achamaleh</last><affiliation>Institute</affiliation></author>
      <author><first>Obiadoh</first><last>Augustine Ekenedilichukwu</last><affiliation>Institute</affiliation></author>
      <pages>599-602</pages>
      <abstract/>
      <url hash="bbcdafad">2025.arabicnlp-sharedtasks.80</url>
      <bibkey>olalekan-abiola-etal-2025-cic</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.80</doi>
    </paper>
    <paper id="81">
      <title><fixed-case>T</fixed-case>ran<fixed-case>T</fixed-case>ran<fixed-case>UIT</fixed-case> at <fixed-case>MAHED</fixed-case> Shared Task: Multilingual Transformer Ensemble with Advanced Data Augmentation and Optuna-based Hyperparameter Optimization</title>
      <author><first>Trinh</first><last>Tran Tran</last><affiliation>Institute</affiliation></author>
      <author><first>Thìn</first><last>Đặng Văn</last><affiliation>Institute</affiliation></author>
      <pages>603-607</pages>
      <abstract/>
      <url hash="1082a8b2">2025.arabicnlp-sharedtasks.81</url>
      <bibkey>tran-tran-dang-van-2025-trantranuit</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.81</doi>
    </paper>
    <paper id="82">
      <title><fixed-case>Y</fixed-case>assir<fixed-case>EA</fixed-case> at <fixed-case>MAHED</fixed-case> 2025: Fusion-Based Multimodal Models for <fixed-case>A</fixed-case>rabic Hate Meme Detection</title>
      <author><first>Yassir</first><last>El Attar</last><affiliation>Institute</affiliation></author>
      <pages>608-614</pages>
      <abstract/>
      <url hash="4d596bfa">2025.arabicnlp-sharedtasks.82</url>
      <bibkey>el-attar-2025-yassirea</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.82</doi>
    </paper>
    <paper id="83">
      <title><fixed-case>AAA</fixed-case> at <fixed-case>MAHED</fixed-case> Text-based Hate and Hope Speech Classification: A Systematic Encoder Evaluation for <fixed-case>A</fixed-case>rabic Hope and Hate Speech Classification</title>
      <author><first>Ahmed</first><last>Khalil Elzainy</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Amin</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmed</first><last>Samir</last><affiliation>Institute</affiliation></author>
      <author><first>Hazem</first><last>Abdelsalam</last><affiliation>Institute</affiliation></author>
      <pages>615-619</pages>
      <abstract/>
      <url hash="2b102df3">2025.arabicnlp-sharedtasks.83</url>
      <bibkey>khalil-elzainy-etal-2025-aaa</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.83</doi>
    </paper>
    <paper id="84">
      <title><fixed-case>CUET</fixed-case>-823 at <fixed-case>MAHED</fixed-case> 2025 Shared Task: Large Language Model-Based Framework for Emotion, Offensive, and Hate Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Ratnajit</first><last>Dhar</last><affiliation>Institute</affiliation></author>
      <author><first>Arpita</first><last>Mallik</last><affiliation>Institute</affiliation></author>
      <pages>620-625</pages>
      <abstract/>
      <url hash="bc8fc455">2025.arabicnlp-sharedtasks.84</url>
      <bibkey>dhar-mallik-2025-cuet</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.84</doi>
    </paper>
    <paper id="85">
      <title><fixed-case>A</fixed-case>ra<fixed-case>M</fixed-case>inds at <fixed-case>MAHED</fixed-case> 2025: Leveraging Vision-Language Models and Contrastive Multi-task Learning for Multimodal Hate Speech Detection</title>
      <author><first>Mohamed</first><last>Zaytoon</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmed</first><last>Mahmoud Salem</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmed</first><last>Sakr</last><affiliation>Institute</affiliation></author>
      <author><first>Hossam</first><last>Elkordi</last><affiliation>Institute</affiliation></author>
      <pages>626-631</pages>
      <abstract/>
      <url hash="dbbc23ca">2025.arabicnlp-sharedtasks.85</url>
      <bibkey>zaytoon-etal-2025-araminds-mahed</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.85</doi>
    </paper>
    <paper id="86">
      <title><fixed-case>D</fixed-case>es<fixed-case>C</fixed-case>artes-<fixed-case>HOPE</fixed-case> at <fixed-case>MAHED</fixed-case> Shared task 2025: Integrating Pragmatic Features for <fixed-case>A</fixed-case>rabic Hope and Hate Speech Detection</title>
      <author><first>Leila</first><last>Moudjari</last><affiliation>Institute</affiliation></author>
      <author><first>Hacène-Cherkaski</first><last>Mélissa</last><affiliation>Institute</affiliation></author>
      <author id="farah-benamara"><first>Farah</first><last>Benamara</last><affiliation>Institute</affiliation></author>
      <pages>632-638</pages>
      <abstract/>
      <url hash="f49f2129">2025.arabicnlp-sharedtasks.86</url>
      <bibkey>moudjari-etal-2025-descartes</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.86</doi>
    </paper>
    <paper id="87">
      <title><fixed-case>ANLP</fixed-case>-<fixed-case>U</fixed-case>ni<fixed-case>S</fixed-case>o at <fixed-case>MAHED</fixed-case> Shared Task: Detection of Hate and Hope Speech in <fixed-case>A</fixed-case>rabic Social Media based on <fixed-case>XLM</fixed-case>-<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a and Logistic Regression</title>
      <author><first>Yasmine</first><last>El Abed</last><affiliation>Institute</affiliation></author>
      <author><first>Mariem</first><last>Ben Arbia</last><affiliation>Institute</affiliation></author>
      <author><first>Saoussen</first><last>Ben Chaabene</last><affiliation>Institute</affiliation></author>
      <author><first>Omar</first><last>Trigui</last><affiliation>Institute</affiliation></author>
      <pages>639-644</pages>
      <abstract/>
      <url hash="c1855e7d">2025.arabicnlp-sharedtasks.87</url>
      <bibkey>el-abed-etal-2025-anlp</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.87</doi>
    </paper>
    <paper id="88">
      <title><fixed-case>REGLAT</fixed-case> at <fixed-case>MAHED</fixed-case> Shared Task: A Hybrid Ensemble-Based System for <fixed-case>A</fixed-case>rabic Hate Speech Detection</title>
      <author><first>Nsrin</first><last>Ashraf</last><affiliation>Institute</affiliation></author>
      <author><first>Mariam</first><last>Labib</last><affiliation>Institute</affiliation></author>
      <author><first>Tarek</first><last>Elshishtawy</last><affiliation>Institute</affiliation></author>
      <author><first>Hamada</first><last>Nayel</last><affiliation>Institute</affiliation></author>
      <pages>645-650</pages>
      <abstract/>
      <url hash="f12c7f5a">2025.arabicnlp-sharedtasks.88</url>
      <bibkey>ashraf-etal-2025-reglat</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.88</doi>
    </paper>
    <paper id="89">
      <title><fixed-case>HTU</fixed-case> at <fixed-case>MAHED</fixed-case> Shared Task: Ensemble-Based Classification of <fixed-case>A</fixed-case>rabic Hate and Hope Speech Using Pre-trained Dialectal <fixed-case>A</fixed-case>rabic Models</title>
      <author><first>Abdallah</first><last>Saleh</last><affiliation>Institute</affiliation></author>
      <author><first>Mariam</first><last>M Biltawi</last><affiliation>Institute</affiliation></author>
      <pages>651-657</pages>
      <abstract/>
      <url hash="3a26d6a5">2025.arabicnlp-sharedtasks.89</url>
      <bibkey>saleh-m-biltawi-2025-htu</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.89</doi>
    </paper>
    <paper id="90">
      <title><fixed-case>S</fixed-case>mol<fixed-case>L</fixed-case>ab_<fixed-case>SEU</fixed-case> at <fixed-case>MAHED</fixed-case> Shared Task: Do <fixed-case>A</fixed-case>rabic-Native Encoders Surpass Multilingual Models in Detecting the Nuances of Hope, Hate, and Emotion?</title>
      <author><first>Md</first><last>Abdur Rahman</last><affiliation>Institute</affiliation></author>
      <author><first>Md</first><last>Sabbir Dewan</last><affiliation>Institute</affiliation></author>
      <author><first>Md.</first><last>Tofael Ahmed Bhuiyan</last><affiliation>Institute</affiliation></author>
      <author><first>Md</first><last>Ashiqur Rahman</last><affiliation>Institute</affiliation></author>
      <pages>658-664</pages>
      <abstract/>
      <url hash="2d2558a5">2025.arabicnlp-sharedtasks.90</url>
      <bibkey>abdur-rahman-etal-2025-smollab</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.90</doi>
    </paper>
    <paper id="91">
      <title>Baoflowin502 at <fixed-case>MAHED</fixed-case> Shared Task: Text-based Hate and Hope Speech Classification</title>
      <author><first>Nguyen</first><last>Minh Bao</last><affiliation>Institute</affiliation></author>
      <author><first>Dang</first><last>Van Thin</last><affiliation>Institute</affiliation></author>
      <pages>665-669</pages>
      <abstract/>
      <url hash="c5904b55">2025.arabicnlp-sharedtasks.91</url>
      <bibkey>minh-bao-van-thin-2025-baoflowin502</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.91</doi>
    </paper>
    <paper id="92">
      <title><fixed-case>A</fixed-case>yah<fixed-case>V</fixed-case>erse at <fixed-case>MAHED</fixed-case> Shared Task: Fine-Tuning <fixed-case>A</fixed-case>rabic<fixed-case>BERT</fixed-case> with Preprocessing for Hope and Hate Detection</title>
      <author><first>Ibad-ur-Rehman</first><last>Rashid</last><affiliation>Institute</affiliation></author>
      <author><first>Muhammad</first><last>Hashir Khalil</last><affiliation>Institute</affiliation></author>
      <pages>670-676</pages>
      <abstract/>
      <url hash="17f352ef">2025.arabicnlp-sharedtasks.92</url>
      <bibkey>rashid-hashir-khalil-2025-ayahverse</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.92</doi>
    </paper>
    <paper id="93">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>M</fixed-case>inds at <fixed-case>MAHED</fixed-case> 2025: Multimodal and Multitask Approaches for Detecting Emotional, Hate, and Offensive Speech in <fixed-case>A</fixed-case>rabic Content</title>
      <author><first>Riddhiman</first><last>Debnath</last><affiliation>Institute</affiliation></author>
      <author><first>Abdul</first><last>Wadud Shakib</last><affiliation>Institute</affiliation></author>
      <author><first>Md</first><last>Saiful Islam</last><affiliation>Institute</affiliation></author>
      <pages>677-682</pages>
      <abstract/>
      <url hash="8a9ddd91">2025.arabicnlp-sharedtasks.93</url>
      <bibkey>debnath-etal-2025-multiminds</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.93</doi>
    </paper>
    <paper id="94">
      <title>joy_2004114 at <fixed-case>MAHED</fixed-case> Shared Task : Filtering Hate Speech from Memes using A Multimodal Fusion-based Approach</title>
      <author><first>Joy</first><last>Das</last><affiliation>Institute</affiliation></author>
      <author><first>Alamgir</first><last>Hossain</last><affiliation>Institute</affiliation></author>
      <author><first>Mohammed</first><last>Moshiul Hoque</last><affiliation>Institute</affiliation></author>
      <pages>683-691</pages>
      <abstract/>
      <url hash="31427d5b">2025.arabicnlp-sharedtasks.94</url>
      <bibkey>das-etal-2025-joy</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.94</doi>
    </paper>
    <paper id="95">
      <title>Quasar at <fixed-case>MAHED</fixed-case> Shared Task : Decoding Emotions and Offense in <fixed-case>A</fixed-case>rabic Text using <fixed-case>LLM</fixed-case> and Transformer-Based Approaches</title>
      <author><first>Md</first><last>Sagor Chowdhury</last><affiliation>Institute</affiliation></author>
      <author><first>Adiba</first><last>Fairooz Chowdhury</last><affiliation>Institute</affiliation></author>
      <pages>692-699</pages>
      <abstract/>
      <url hash="f8be7fea">2025.arabicnlp-sharedtasks.95</url>
      <bibkey>sagor-chowdhury-fairooz-chowdhury-2025-quasar</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.95</doi>
    </paper>
    <paper id="96">
      <title><fixed-case>CUET</fixed-case>_<fixed-case>Z</fixed-case>ahra_<fixed-case>D</fixed-case>uo@Mahed 2025: Hate and Hope Speech Detection in <fixed-case>A</fixed-case>rabic Social Media Content using Transformer</title>
      <author><first>Walisa</first><last>Alam</last><affiliation>Institute</affiliation></author>
      <author><first>Mehreen</first><last>Rahman</last><affiliation>Institute</affiliation></author>
      <author><first>Shawly</first><last>Ahsan</last><affiliation>Institute</affiliation></author>
      <author><first>Mohammed</first><last>Moshiul Hoque</last><affiliation>Institute</affiliation></author>
      <pages>700-705</pages>
      <abstract/>
      <url hash="bec47048">2025.arabicnlp-sharedtasks.96</url>
      <bibkey>alam-etal-2025-cuet</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.96</doi>
    </paper>
    <paper id="97">
      <title><fixed-case>A</fixed-case>ra<fixed-case>NLP</fixed-case> at <fixed-case>MAHED</fixed-case> 2025 Shared Task: Using <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> for Text-based Hate and Hope Speech Classification</title>
      <author><first>Wafaa</first><last>S. El-Kassas</last><affiliation>Institute</affiliation></author>
      <author><first>Enas</first><last>A. Hakim Khalil</last><affiliation>Institute</affiliation></author>
      <pages>706-711</pages>
      <abstract/>
      <url hash="b0617ba7">2025.arabicnlp-sharedtasks.97</url>
      <bibkey>s-el-kassas-a-hakim-khalil-2025-aranlp</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.97</doi>
    </paper>
    <paper id="98">
      <title>Thinking Nodes at <fixed-case>MAHED</fixed-case>: A Comparative Study of Multimodal Architectures for <fixed-case>A</fixed-case>rabic Hateful Meme Detection</title>
      <author><first>Itbaan</first><last>Safwan</last><affiliation>Institute</affiliation></author>
      <pages>712-719</pages>
      <abstract/>
      <url hash="de4e6e60">2025.arabicnlp-sharedtasks.98</url>
      <bibkey>safwan-2025-thinking</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.98</doi>
    </paper>
    <paper id="99">
      <title><fixed-case>NADI</fixed-case> 2025: The First Multidialectal <fixed-case>A</fixed-case>rabic Speech Processing Shared Task</title>
      <author id="bashar-talafha" orcid="0000-0002-4227-892X"><first>Bashar</first><last>Talafha</last><affiliation>University of British Columbia</affiliation></author>
      <author id="hawau-olamide-toyin" orcid="0009-0008-3927-526X"><first>Hawau Olamide</first><last>Toyin</last></author>
      <author><first>Peter</first><last>Sullivan</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>AbdelRahim A.</first><last>Elmadany</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Abdurrahman</first><last>Juma</last></author>
      <author id="amirbek-djanibekov" orcid="0000-0003-0831-4867"><first>Amirbek</first><last>Djanibekov</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author id="chiyu-zhang" orcid="0000-0002-5981-352X"><first>Chiyu</first><last>Zhang</last><affiliation>Facebook</affiliation></author>
      <author id="hamad-alshehhi" orcid="0009-0007-8636-2334"><first>Hamad</first><last>Alshehhi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author id="hanan-aldarmaki" orcid="0000-0003-1706-1777"><first>Hanan</first><last>Aldarmaki</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author id="mustafa-jarrar" orcid="0000-0003-4351-4207"><first>Mustafa</first><last>Jarrar</last><affiliation>Birzeit University</affiliation></author>
      <author id="nizar-habash" orcid="0000-0002-1831-3457"><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author id="muhammad-abdul-mageed" orcid="0000-0002-8590-2040"><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>720-733</pages>
      <abstract>We present the findings of the sixth Nuanced Arabic Dialect Identification (NADI 2025) Shared Task, which focused on Arabic speech dialect processing across three subtasks: spoken dialect identification (Subtask 1), speech recognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask 3). A total of 44 teams registered, and during the testing phase, 100 valid submissions were received from eight unique teams. The distribution was as follows: 34 submissions for Subtask 1 five teams, 47 submissions for Subtask 2 six teams, and 19 submissions for Subtask 3 two teams. The best-performing systems achieved 79.8% accuracy on Subtask 1, <tex-math>35.68/12.20</tex-math> WER/CER (overall average) on Subtask 2, and <tex-math>55/13</tex-math> WER/CER on Subtask 3. These results highlight the ongoing challenges of Arabic dialect speech processing, particularly in dialect identification, recognition, and diacritic restoration. We also summarize the methods adopted by participating teams and briefly outline directions for future editions of NADI.</abstract>
      <url hash="8b05a531">2025.arabicnlp-sharedtasks.99</url>
      <bibkey>talafha-etal-2025-nadi</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.99</doi>
    </paper>
    <paper id="100">
      <title>Munsit at <fixed-case>NADI</fixed-case> 2025 Shared Task 2: Pushing the Boundaries of Multidialectal <fixed-case>A</fixed-case>rabic <fixed-case>ASR</fixed-case> with Weakly Supervised Pretraining and Continual Supervised Fine-tuning</title>
      <author><first>Mahmoud</first><last>Salhab</last><affiliation>Institute</affiliation></author>
      <author><first>Shameed</first><last>Sait</last><affiliation>Institute</affiliation></author>
      <author><first>Mohammad</first><last>Abusheikh</last><affiliation>Institute</affiliation></author>
      <author><first>Hasan</first><last>Abusheikh</last><affiliation>Institute</affiliation></author>
      <pages>734-739</pages>
      <abstract/>
      <url hash="276932f6">2025.arabicnlp-sharedtasks.100</url>
      <bibkey>salhab-etal-2025-munsit</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.100</doi>
    </paper>
    <paper id="101">
      <title>Lahjati at <fixed-case>NADI</fixed-case> 2025 A <fixed-case>ECAPA</fixed-case>-<fixed-case>W</fixed-case>av<fixed-case>LM</fixed-case> Fusion with Multi-Stage Optimization</title>
      <author><first>Sanad</first><last>Albawwab</last><affiliation>Institute</affiliation></author>
      <author><first>Omar</first><last>Qawasmeh</last><affiliation>Institute</affiliation></author>
      <pages>740-744</pages>
      <abstract/>
      <url hash="c6caac07">2025.arabicnlp-sharedtasks.101</url>
      <bibkey>albawwab-qawasmeh-2025-lahjati</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.101</doi>
    </paper>
    <paper id="102">
      <title><fixed-case>S</fixed-case>aarland-<fixed-case>G</fixed-case>roningen at <fixed-case>NADI</fixed-case> 2025 Shared Task: Effective Dialectal <fixed-case>A</fixed-case>rabic Speech Processing under Data Constraints</title>
      <author><first>Badr</first><last>M. Abdullah</last><affiliation>Institute</affiliation></author>
      <author><first>Yusser</first><last>Al Ghussin</last><affiliation>Institute</affiliation></author>
      <author id="zena-al-khalili"><first>Zena</first><last>Al-Khalili</last><affiliation>Institute</affiliation></author>
      <author><first>Ömer</first><last>Tarik Özyilmaz</last><affiliation>Institute</affiliation></author>
      <author><first>Matias</first><last>Valdenegro-Toro</last><affiliation>Institute</affiliation></author>
      <author><first>Simon</first><last>Ostermann</last><affiliation>Institute</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last><affiliation>Institute</affiliation></author>
      <pages>745-751</pages>
      <abstract/>
      <url hash="886686ce">2025.arabicnlp-sharedtasks.102</url>
      <bibkey>m-abdullah-etal-2025-saarland</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.102</doi>
    </paper>
    <paper id="103">
      <title><fixed-case>M</fixed-case>arsad<fixed-case>L</fixed-case>ab at <fixed-case>NADI</fixed-case> Shared Task: <fixed-case>A</fixed-case>rabic Dialect Identification and Speech Recognition using <fixed-case>ECAPA</fixed-case>-<fixed-case>TDNN</fixed-case> and Whisper</title>
      <author><first>Md.</first><last>Rafiul Biswas</last><affiliation>Institute</affiliation></author>
      <author><first>Kais</first><last>Attia</last><affiliation>Institute</affiliation></author>
      <author><first>Shimaa</first><last>Ibrahim</last><affiliation>Institute</affiliation></author>
      <author><first>Mabrouka</first><last>Bessghaier</last><affiliation>Institute</affiliation></author>
      <author><first>Wajdi</first><last>Zaghouani</last><affiliation>Institute</affiliation></author>
      <pages>752-756</pages>
      <abstract/>
      <url hash="8a1e5146">2025.arabicnlp-sharedtasks.103</url>
      <bibkey>rafiul-biswas-etal-2025-marsadlab-nadi</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.103</doi>
    </paper>
    <paper id="104">
      <title>Abjad <fixed-case>AI</fixed-case> at <fixed-case>NADI</fixed-case> 2025: <fixed-case>CATT</fixed-case>-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations</title>
      <author><first>Ahmad</first><last>Ghannam</last><affiliation>Institute</affiliation></author>
      <author><first>Naif</first><last>Alharthi</last><affiliation>Institute</affiliation></author>
      <author><first>Faris</first><last>Alasmary</last><affiliation>Institute</affiliation></author>
      <author><first>Kholood</first><last>Al Tabash</last><affiliation>Institute</affiliation></author>
      <author><first>Shouq</first><last>Sadah</last><affiliation>Institute</affiliation></author>
      <author><first>Lahouari</first><last>Ghouti</last><affiliation>Institute</affiliation></author>
      <pages>757-761</pages>
      <abstract/>
      <url hash="8c908378">2025.arabicnlp-sharedtasks.104</url>
      <bibkey>ghannam-etal-2025-abjad</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.104</doi>
    </paper>
    <paper id="105">
      <title><fixed-case>ELYADATA</fixed-case> &amp; <fixed-case>LIA</fixed-case> at <fixed-case>NADI</fixed-case> 2025: <fixed-case>ASR</fixed-case> and <fixed-case>ADI</fixed-case> Subtasks</title>
      <author><first>Haroun</first><last>Elleuch</last><affiliation>Institute</affiliation></author>
      <author><first>Youssef</first><last>Saidi</last><affiliation>Institute</affiliation></author>
      <author><first>Salima</first><last>Mdhaffar</last><affiliation>Institute</affiliation></author>
      <author><first>Yannick</first><last>Estève</last><affiliation>Institute</affiliation></author>
      <author><first>Fethi</first><last>Bougares</last><affiliation>Institute</affiliation></author>
      <pages>762-766</pages>
      <abstract/>
      <url hash="34269761">2025.arabicnlp-sharedtasks.105</url>
      <bibkey>elleuch-etal-2025-elyadata</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.105</doi>
    </paper>
    <paper id="106">
      <title>Unicorn at <fixed-case>NADI</fixed-case> 2025 Subtask 3: <fixed-case>GEMM</fixed-case>3<fixed-case>N</fixed-case>-<fixed-case>DR</fixed-case>: Audio-Text Diacritic Restoration via Fine-tuning Multimodal <fixed-case>A</fixed-case>rabic <fixed-case>LLM</fixed-case></title>
      <author><first>Mohamed</first><last>Lotfy Elrefai</last><affiliation>Institute</affiliation></author>
      <pages>767-773</pages>
      <abstract/>
      <url hash="fc1f3394">2025.arabicnlp-sharedtasks.106</url>
      <bibkey>lotfy-elrefai-2025-unicorn</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.106</doi>
    </paper>
    <paper id="107">
      <title><fixed-case>P</fixed-case>alm<fixed-case>X</fixed-case> 2025: The First Shared Task on Benchmarking <fixed-case>LLM</fixed-case>s on <fixed-case>A</fixed-case>rabic and Islamic Culture</title>
      <author><first>Fakhraddin</first><last>Alwajih</last><affiliation>NA</affiliation></author>
      <author id="abdellah-el-mekki"><first>Abdellah</first><last>El Mekki</last><affiliation>University of British Columbia</affiliation></author>
      <author id="hamdy-mubarak" orcid="0000-0003-4828-6098"><first>Hamdy</first><last>Mubarak</last></author>
      <author id="majd-hawasly" orcid="0000-0003-1823-5580"><first>Majd</first><last>Hawasly</last></author>
      <author id="abubakr-mohamed" orcid="0000-0002-7854-7589"><first>Abubakr</first><last>Mohamed</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author id="muhammad-abdul-mageed" orcid="0000-0002-8590-2040"><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>774-789</pages>
      <abstract>Large Language Models (LLMs) inherently reflect the vast data distributions they encounter during their pre-training phase. As this data is predominantly sourced from the web, there is a high chance it will be skewed towards high-resourced languages and cultures, such as those of the West. Consequently, LLMs often exhibit a diminished understanding of certain communities, a gap that is particularly evident in their knowledge of Arabic and Islamic cultures. This issue becomes even more pronounced with increasingly under-represented topics. To address this critical challenge, we introduce PalmX 2025, the first shared task designed to benchmark the cultural competence of LLMs in these specific domains. The task is composed of two subtasks featuring multiple-choice questions (MCQs) in Modern Standard Arabic (MSA): General Arabic Culture and General Islamic Culture. These subtasks cover a wide range of topics, including traditions, food, history, religious practices, and language expressions from across 22 Arab countries. The initiative drew considerable interest, with 26 teams registering for Subtask 1 and 19 for Subtask 2, culminating in nine and six valid submissions, respectively. Our findings reveal that task-specific fine-tuning substantially boosts performance over baseline models. The top-performing systems achieved an accuracy of 72.15% on cultural questions and 84.22% on Islamic knowledge. Parameter-efficient fine-tuning emerged as the predominant and most effective approach among participants, while the utility of data augmentation was found to be domain-dependent. Ultimately, this benchmark provides a crucial, standardized framework to guide the development of more culturally grounded and competent Arabic LLMs. Results of the shared task demonstrate that general cultural and general religious knowledge remain challenging to LLMs, motivating us to continue to offer the shared task in the future.</abstract>
      <url hash="282d28bb">2025.arabicnlp-sharedtasks.107</url>
      <bibkey>alwajih-etal-2025-palmx</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.107</doi>
    </paper>
    <paper id="108">
      <title>Hamyaria at <fixed-case>P</fixed-case>alm<fixed-case>X</fixed-case>2025: Leveraging Large Language Models to Improve <fixed-case>A</fixed-case>rabic Multiple-Choice Questions in Cultural and Islamic Domains</title>
      <author><first>Walid</first><last>Al-Dhabyani</last><affiliation>Institute</affiliation></author>
      <author><first>Hamzah</first><last>A. Alsayadi</last><affiliation>Institute</affiliation></author>
      <pages>790-796</pages>
      <abstract/>
      <url hash="4d7a292e">2025.arabicnlp-sharedtasks.108</url>
      <bibkey>al-dhabyani-a-alsayadi-2025-hamyaria</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.108</doi>
    </paper>
    <paper id="109">
      <title><fixed-case>ISL</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>P</fixed-case>alm<fixed-case>X</fixed-case> 2025: Retrieval-Augmented Fine-Tuning for <fixed-case>A</fixed-case>rabic Cultural Question Answering</title>
      <author><first>Mohamed</first><last>Gomaa</last><affiliation>Institute</affiliation></author>
      <author><first>Noureldin</first><last>Elmadany</last><affiliation>Institute</affiliation></author>
      <pages>797-801</pages>
      <abstract/>
      <url hash="50c7da40">2025.arabicnlp-sharedtasks.109</url>
      <bibkey>gomaa-elmadany-2025-isl</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.109</doi>
    </paper>
    <paper id="110">
      <title><fixed-case>ADAPT</fixed-case>–<fixed-case>MTU</fixed-case> <fixed-case>HAI</fixed-case> at <fixed-case>P</fixed-case>alm<fixed-case>X</fixed-case> 2025: Leveraging Full and <fixed-case>P</fixed-case>arameter‐<fixed-case>E</fixed-case>fficient <fixed-case>LLM</fixed-case> <fixed-case>F</fixed-case>ine‐<fixed-case>T</fixed-case>uning for <fixed-case>A</fixed-case>rabic Cultural <fixed-case>QA</fixed-case></title>
      <author><first>Shehenaz</first><last>Hossain</last><affiliation>Institute</affiliation></author>
      <author><first>Haithem</first><last>Afli</last><affiliation>Institute</affiliation></author>
      <pages>802-808</pages>
      <abstract/>
      <url hash="7d9549cd">2025.arabicnlp-sharedtasks.110</url>
      <bibkey>hossain-afli-2025-adapt</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.110</doi>
    </paper>
    <paper id="111">
      <title><fixed-case>C</fixed-case>ultran<fixed-case>AI</fixed-case> at <fixed-case>P</fixed-case>alm<fixed-case>X</fixed-case> 2025: Data Augmentation for Cultural Knowledge Representation</title>
      <author><first>Hunzalah</first><last>Hassan Bhatti</last><affiliation>Institute</affiliation></author>
      <author><first>Youssef</first><last>Ahmed</last><affiliation>Institute</affiliation></author>
      <author><first>Md</first><last>Arid Hasan</last><affiliation>Institute</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Institute</affiliation></author>
      <pages>809-817</pages>
      <abstract/>
      <url hash="c5789698">2025.arabicnlp-sharedtasks.111</url>
      <bibkey>hassan-bhatti-etal-2025-cultranai</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.111</doi>
    </paper>
    <paper id="112">
      <title><fixed-case>M</fixed-case>arsad<fixed-case>L</fixed-case>ab at <fixed-case>P</fixed-case>alm<fixed-case>X</fixed-case> Shared Task: An <fixed-case>LLM</fixed-case> Benchmark for <fixed-case>A</fixed-case>rabic Culture and Islamic Civilization</title>
      <author><first>Md.</first><last>Rafiul Biswas</last><affiliation>Institute</affiliation></author>
      <author><first>Shimaa</first><last>Ibrahim</last><affiliation>Institute</affiliation></author>
      <author><first>Kais</first><last>Attia</last><affiliation>Institute</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Institute</affiliation></author>
      <author><first>Wajdi</first><last>Zaghouani</last><affiliation>Institute</affiliation></author>
      <pages>818-824</pages>
      <abstract/>
      <url hash="43f05785">2025.arabicnlp-sharedtasks.112</url>
      <bibkey>rafiul-biswas-etal-2025-marsadlab-palmx</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.112</doi>
    </paper>
    <paper id="113">
      <title>Star at <fixed-case>P</fixed-case>alm<fixed-case>X</fixed-case> 2025: <fixed-case>A</fixed-case>rabic Cultural Understanding via Targeted Pretraining and Lightweight Fine-tuning</title>
      <author><first>Eman</first><last>Elrefai</last><affiliation>Institute</affiliation></author>
      <author><first>Esraa</first><last>Khaled</last><affiliation>Institute</affiliation></author>
      <author><first>Alhassan</first><last>Ehab</last><affiliation>Institute</affiliation></author>
      <pages>825-829</pages>
      <abstract/>
      <url hash="21f83c4e">2025.arabicnlp-sharedtasks.113</url>
      <bibkey>elrefai-etal-2025-star</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.113</doi>
    </paper>
    <paper id="114">
      <title><fixed-case>AYA</fixed-case> at <fixed-case>P</fixed-case>alm<fixed-case>X</fixed-case> 2025: Modeling Cultural and Islamic Knowledge in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jannatul</first><last>Tajrin</last><affiliation>Institute</affiliation></author>
      <author><first>Bir</first><last>Ballav Roy</last><affiliation>Institute</affiliation></author>
      <author><first>Firoj</first><last>Alam</last><affiliation>Institute</affiliation></author>
      <pages>830-836</pages>
      <abstract/>
      <url hash="6782b500">2025.arabicnlp-sharedtasks.114</url>
      <bibkey>tajrin-etal-2025-aya</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.114</doi>
    </paper>
    <paper id="115">
      <title>Cultura-Arabica: Probing and Enhancing <fixed-case>A</fixed-case>rabic Cultural Awareness in Large Language Models via <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case></title>
      <author><first>Pulkit</first><last>Chatwal</last><affiliation>Institute</affiliation></author>
      <author><first>Santosh</first><last>Kumar Mishra</last><affiliation>Institute</affiliation></author>
      <pages>837-842</pages>
      <abstract/>
      <url hash="7e9256f8">2025.arabicnlp-sharedtasks.115</url>
      <bibkey>chatwal-kumar-mishra-2025-cultura</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.115</doi>
    </paper>
    <paper id="116">
      <title>Phoenix at Palmx: Exploring Data Augmentation for <fixed-case>A</fixed-case>rabic Cultural Question Answering</title>
      <author><first>Houdaifa</first><last>Atou</last><affiliation>Institute</affiliation></author>
      <author><first>Issam</first><last>Ait Yahia</last><affiliation>Institute</affiliation></author>
      <author><first>Ismail</first><last>Berrada</last><affiliation>Institute</affiliation></author>
      <pages>843-850</pages>
      <abstract/>
      <url hash="bf2337c3">2025.arabicnlp-sharedtasks.116</url>
      <bibkey>atou-etal-2025-phoenix</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.116</doi>
    </paper>
    <paper id="117">
      <title><fixed-case>QIAS</fixed-case> 2025: Overview of the Shared Task on Islamic Inheritance Reasoning and Knowledge Assessment</title>
      <author id="abdessalam-bouchekif"><first>Abdessalam</first><last>Bouchekif</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author><first>Samer</first><last>Rashwani</last></author>
      <author><first>Emad Soliman Ali</first><last>Mohamed</last><affiliation>Nazarbayev University</affiliation></author>
      <author><first>Mutaz</first><last>Alkhatib</last><affiliation>NA</affiliation></author>
      <author><first>Heba</first><last>Sbahi</last><affiliation>NA</affiliation></author>
      <author id="shahd-gaben" orcid="0009-0001-8554-9777"><first>Shahd</first><last>Gaben</last></author>
      <author id="wajdi-zaghouani" orcid="0000-0003-1521-5568"><first>Wajdi</first><last>Zaghouani</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Aiman</first><last>Erbad</last><affiliation>NA</affiliation></author>
      <author><first>Mohammed</first><last>Ghaly</last><affiliation>NA</affiliation></author>
      <pages>851-860</pages>
      <abstract>This paper provides a comprehensive overview of the QIAS 2025 shared task, organized as part of the ArabicNLP 2025 conference and co-located with EMNLP 2025. The task was designed for the evaluation of large language models in the complex domains of religious and legal reasoning. It comprises two subtasks: (1) Islamic Inheritance Reasoning, requiring models to compute inheritance shares according to Islamic jurisprudence, and (2) Islamic Knowledge Assessment, which covers a range of traditional Islamic disciplines. Both subtasks were structured as multiple-choice question answering challenges, with questions stratified by varying difficulty levels. The shared task attracted significant interest, with 44 teams participating in the development phase, from which 18 teams advanced to the final test phase. Of these, 6 teams submitted entries for both subtasks, 8 for Task 1 only, and two for Task 3 only. Ultimately, 16 teams submitted system description papers. Herein, we detail the task’s motivation, dataset construction, evaluation protocol, and present a summary of the participating systems and their results.</abstract>
      <url hash="e64de0d7">2025.arabicnlp-sharedtasks.117</url>
      <bibkey>bouchekif-etal-2025-qias</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.117</doi>
    </paper>
    <paper id="118">
      <title><fixed-case>NYUAD</fixed-case> at <fixed-case>QIAS</fixed-case> Shared Task: Benchmarking the Legal Reasoning of <fixed-case>LLM</fixed-case>s in <fixed-case>A</fixed-case>rabic Islamic Inheritance Cases</title>
      <author><first>Nouar</first><last>AlDahoul</last><affiliation>Institute</affiliation></author>
      <author><first>Yasir</first><last>Zaki</last><affiliation>Institute</affiliation></author>
      <pages>861-866</pages>
      <abstract/>
      <url hash="445c75bf">2025.arabicnlp-sharedtasks.118</url>
      <bibkey>aldahoul-zaki-2025-nyuad-qias</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.118</doi>
    </paper>
    <paper id="119">
      <title><fixed-case>SHA</fixed-case> at the <fixed-case>QIAS</fixed-case> Shared Task: <fixed-case>LLM</fixed-case>s for <fixed-case>A</fixed-case>rabic Islamic Inheritance Reasoning</title>
      <author><first>Shatha</first><last>Altammami</last><affiliation>Institute</affiliation></author>
      <pages>867-872</pages>
      <abstract/>
      <url hash="aa2bffa6">2025.arabicnlp-sharedtasks.119</url>
      <bibkey>altammami-2025-sha</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.119</doi>
    </paper>
    <paper id="120">
      <title><fixed-case>ANLP</fixed-case>ers at <fixed-case>QIAS</fixed-case>: <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> for Islamic Inheritance</title>
      <author><first>Serry</first><last>Sibaee</last><affiliation>Institute</affiliation></author>
      <author><first>Mahmoud</first><last>Reda</last><affiliation>Institute</affiliation></author>
      <author><first>Omer</first><last>Nacar</last><affiliation>Institute</affiliation></author>
      <author><first>Yasser</first><last>Alhabashi</last><affiliation>Institute</affiliation></author>
      <author><first>Adel</first><last>Ammar</last><affiliation>Institute</affiliation></author>
      <author><first>Wadii</first><last>Boulila</last><affiliation>Institute</affiliation></author>
      <pages>873-877</pages>
      <abstract/>
      <url hash="61003318">2025.arabicnlp-sharedtasks.120</url>
      <bibkey>sibaee-etal-2025-anlpers-qias</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.120</doi>
    </paper>
    <paper id="121">
      <title>N&amp;N at <fixed-case>QIAS</fixed-case> 2025: Chain-of-Thought Ensembles with Retrieval-Augmented framework for Classical <fixed-case>A</fixed-case>rabic Islamic</title>
      <author><first>Nourah</first><last>Alangari</last><affiliation>Institute</affiliation></author>
      <author><first>Nouf</first><last>AlShenaifi</last><affiliation>Institute</affiliation></author>
      <pages>878-882</pages>
      <abstract/>
      <url hash="6164c16c">2025.arabicnlp-sharedtasks.121</url>
      <bibkey>alangari-alshenaifi-2025-n</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.121</doi>
    </paper>
    <paper id="122">
      <title><fixed-case>HIAST</fixed-case> at <fixed-case>QIAS</fixed-case> 2025: Retrieval-Augmented <fixed-case>LLM</fixed-case>s with Top-Hit Web Evidence for <fixed-case>A</fixed-case>rabic Islamic Reasoning <fixed-case>QA</fixed-case></title>
      <author><first>Mohamed</first><last>Motasim Hamed</last><affiliation>Institute</affiliation></author>
      <author><first>Nada</first><last>Ghneim</last><affiliation>Institute</affiliation></author>
      <author><first>Riad</first><last>Sonbol</last><affiliation>Institute</affiliation></author>
      <pages>883-891</pages>
      <abstract/>
      <url hash="637726e8">2025.arabicnlp-sharedtasks.122</url>
      <bibkey>motasim-hamed-etal-2025-hiast</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.122</doi>
    </paper>
    <paper id="123">
      <title><fixed-case>QU</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>QIAS</fixed-case> 2025 Shared Task: A Two-Phase <fixed-case>LLM</fixed-case> Fine-Tuning and Retrieval-Augmented Generation Approach for Islamic Inheritance Reasoning</title>
      <author><first>Mohammad</first><last>AL-Smadi</last><affiliation>Institute</affiliation></author>
      <pages>892-898</pages>
      <abstract/>
      <url hash="bbc2d130">2025.arabicnlp-sharedtasks.123</url>
      <bibkey>al-smadi-2025-qu</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.123</doi>
    </paper>
    <paper id="124">
      <title>Transformer Tafsir at <fixed-case>QIAS</fixed-case> 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering</title>
      <author><first>Muhammad</first><last>Abu Ahmad</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamad</first><last>Ballout</last><affiliation>Institute</affiliation></author>
      <author><first>Raia</first><last>Abu Ahmad</last><affiliation>Institute</affiliation></author>
      <author><first>Elia</first><last>Bruni</last><affiliation>Institute</affiliation></author>
      <pages>899-904</pages>
      <abstract/>
      <url hash="a99722c5">2025.arabicnlp-sharedtasks.124</url>
      <bibkey>abu-ahmad-etal-2025-transformer</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.124</doi>
    </paper>
    <paper id="125">
      <title><fixed-case>P</fixed-case>ux<fixed-case>AI</fixed-case> at <fixed-case>QIAS</fixed-case> 2025: Multi-Agent Retrieval-Augmented Generation for Islamic Inheritance and Knowledge Reasoning</title>
      <author><first>Nguyen</first><last>Xuan Phuc</last><affiliation>Institute</affiliation></author>
      <author><first>Thìn</first><last>Đặng Văn</last><affiliation>Institute</affiliation></author>
      <pages>905-913</pages>
      <abstract/>
      <url hash="90f52bd0">2025.arabicnlp-sharedtasks.125</url>
      <bibkey>xuan-phuc-dang-van-2025-puxai</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.125</doi>
    </paper>
    <paper id="126">
      <title>Athar at <fixed-case>QIAS</fixed-case>2025: <fixed-case>LLM</fixed-case>-based Question Answering Systems for Islamic Inheritance and Classical Islamic Knowledge</title>
      <author><first>Yossra</first><last>Noureldien</last><affiliation>Institute</affiliation></author>
      <author><first>Hassan</first><last>Suliman</last><affiliation>Institute</affiliation></author>
      <author><first>Farah</first><last>Attallah</last><affiliation>Institute</affiliation></author>
      <author><first>Abdelrazig</first><last>Mohamed</last><affiliation>Institute</affiliation></author>
      <author><first>Sara</first><last>Abdalla</last><affiliation>Institute</affiliation></author>
      <pages>914-922</pages>
      <abstract/>
      <url hash="c1319a64">2025.arabicnlp-sharedtasks.126</url>
      <bibkey>noureldien-etal-2025-athar</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.126</doi>
    </paper>
    <paper id="127">
      <title><fixed-case>ADAPT</fixed-case>–<fixed-case>MTU</fixed-case> <fixed-case>HAI</fixed-case> at <fixed-case>QIAS</fixed-case>2025: Dual-Expert <fixed-case>LLM</fixed-case> Fine-Tuning and Constrained Decoding for <fixed-case>A</fixed-case>rabic Islamic Inheritance Reasoning</title>
      <author><first>Shehenaz</first><last>Hossain</last><affiliation>Institute</affiliation></author>
      <author><first>Haithem</first><last>Afli</last><affiliation>Institute</affiliation></author>
      <pages>923-928</pages>
      <abstract/>
      <url hash="76f4d43a">2025.arabicnlp-sharedtasks.127</url>
      <bibkey>hossain-afli-2025-adapt-mtu</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.127</doi>
    </paper>
    <paper id="128">
      <title><fixed-case>CVPD</fixed-case> at <fixed-case>QIAS</fixed-case> 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning</title>
      <author><first>Salah</first><last>Eddine Bekhouche</last><affiliation>Institute</affiliation></author>
      <author><first>Abdellah</first><last>Zakaria Sellam</last><affiliation>Institute</affiliation></author>
      <author><first>Telli</first><last>Hichem</last><affiliation>Institute</affiliation></author>
      <author><first>Cosimo</first><last>Distante</last><affiliation>Institute</affiliation></author>
      <author><first>Abdenour</first><last>Hadid</last><affiliation>Institute</affiliation></author>
      <pages>929-934</pages>
      <abstract/>
      <url hash="4776fb00">2025.arabicnlp-sharedtasks.128</url>
      <bibkey>eddine-bekhouche-etal-2025-cvpd</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.128</doi>
    </paper>
    <paper id="129">
      <title><fixed-case>CIS</fixed-case>-<fixed-case>RG</fixed-case> at <fixed-case>QIAS</fixed-case> 2025 Shared Task: Approaches for Enhancing Performance of <fixed-case>LLM</fixed-case> on Islamic Legal Reasoning and its Mathematical Calculations</title>
      <author><first>Osama</first><last>Farouk Zaki</last><affiliation>Institute</affiliation></author>
      <pages>935-939</pages>
      <abstract/>
      <url hash="503d940e">2025.arabicnlp-sharedtasks.129</url>
      <bibkey>farouk-zaki-2025-cis</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.129</doi>
    </paper>
    <paper id="130">
      <title><fixed-case>SEA</fixed-case>-Team at <fixed-case>QIAS</fixed-case> 2025: Enhancing <fixed-case>LLM</fixed-case>s for Question Answering in Islamic Texts</title>
      <author><first>Sanaa</first><last>Alowaidi</last><affiliation>Institute</affiliation></author>
      <pages>940-946</pages>
      <abstract/>
      <url hash="430ecece">2025.arabicnlp-sharedtasks.130</url>
      <bibkey>alowaidi-2025-sea</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.130</doi>
    </paper>
    <paper id="131">
      <title><fixed-case>M</fixed-case>or<fixed-case>AI</fixed-case> at <fixed-case>QIAS</fixed-case> 2025: Collaborative <fixed-case>LLM</fixed-case> via Voting and Retrieval-Augmented Generation for Solving Complex Inheritance Problems</title>
      <author><first>Jihad</first><last>R’baiti</last><affiliation>Institute</affiliation></author>
      <author><first>Chouaib</first><last>El Hachimi</last><affiliation>Institute</affiliation></author>
      <author><first>Youssef</first><last>Hmamouche</last><affiliation>Institute</affiliation></author>
      <author><first>Amal</first><last>El Fallah Seghrouchni</last><affiliation>Institute</affiliation></author>
      <pages>947-952</pages>
      <abstract/>
      <url hash="0e5afac9">2025.arabicnlp-sharedtasks.131</url>
      <bibkey>rbaiti-etal-2025-morai</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.131</doi>
    </paper>
    <paper id="132">
      <title>Gumball at <fixed-case>QIAS</fixed-case> 2025: <fixed-case>A</fixed-case>rabic <fixed-case>LLM</fixed-case> Automated Reasoning in Islamic Inheritance</title>
      <author><first>Eman</first><last>Elrefai</last><affiliation>Institute</affiliation></author>
      <author><first>Mohamed</first><last>Lotfy Elrefai</last><affiliation>Institute</affiliation></author>
      <author><first>Aml</first><last>Hassan Esmail</last><affiliation>Institute</affiliation></author>
      <pages>953-959</pages>
      <abstract/>
      <url hash="4ad584db">2025.arabicnlp-sharedtasks.132</url>
      <bibkey>elrefai-etal-2025-gumball</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.132</doi>
    </paper>
    <paper id="133">
      <title>Tokenizers United at <fixed-case>QIAS</fixed-case>-2025: <fixed-case>RAG</fixed-case>-Enhanced Question Answering for Islamic Studies by Integrating Semantic Retrieval with Generative Reasoning</title>
      <author><first>Mohamed</first><last>Samy</last></author>
      <author><first>Mayar</first><last>Boghdady</last><affiliation>Institute</affiliation></author>
      <author><first>Marwan</first><last>El Adawi</last></author>
      <author><first>Mohamed</first><last>Nassar</last></author>
      <author><first>Ensaf</first><last>Hussein</last></author>
      <pages>960-965</pages>
      <abstract/>
      <url hash="1572525d">2025.arabicnlp-sharedtasks.133</url>
      <bibkey>boghdady-2025-tokenizers</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.133</doi>
    </paper>
    <paper id="134">
      <title><fixed-case>TAQEEM</fixed-case> 2025: Overview of The First Shared Task for <fixed-case>A</fixed-case>rabic Quality Evaluation of Essays in Multi-dimensions</title>
      <author><first>May</first><last>Bashendy</last><affiliation>University of Qatar</affiliation></author>
      <author><first>Salam</first><last>Albatarni</last></author>
      <author><first>Sohaila</first><last>Eltanbouly</last><affiliation>University of Qatar</affiliation></author>
      <author id="walid-massoud" orcid="0000-0001-6023-5336"><first>Walid</first><last>Massoud</last></author>
      <author><first>Houda</first><last>Bouamor</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Tamer</first><last>Elsayed</last><affiliation>Qatar University</affiliation></author>
      <pages>966-976</pages>
      <abstract>Automated Essay Scoring (AES) has emerged as a significant research problem in natural language processing, offering valuable tools to support educators in assessing student writing. Motivated by the growing need for reliable Arabic AES systems, we organized the first shared Task for Arabic Quality Evaluation of Essays in Multi-dimensions (TAQEEM) held at the ArabicNLP 2025 conference. TAQEEM 2025 includes two subtasks: Task A on holistic scoring and Task B on trait-specific scoring. It introduces a new (and first of its kind) dataset of 1,265 Arabic essays, annotated with holistic and trait-specific scores, including relevance, organization, vocabulary, style, development, mechanics, and grammar. The main goal of TAQEEM is to address the scarcity of standardized benchmarks and high-quality resources in Arabic AES. TAQEEM 2025 attracted 11 registered teams for Task A and 10 for Task B, with a total of 5 teams, across both tasks, submitting system runs for evaluation. This paper presents an overview of the task, outlines the approaches employed, and discusses the results of the participating teams.</abstract>
      <url hash="4a9f465a">2025.arabicnlp-sharedtasks.134</url>
      <bibkey>bashendy-etal-2025-taqeem</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.134</doi>
    </paper>
    <paper id="135">
      <title><fixed-case>AR</fixed-case>x<fixed-case>HYOKA</fixed-case> at <fixed-case>TAQEEM</fixed-case>2025: Comparative Approaches to <fixed-case>A</fixed-case>rabic Essay Trait Scoring</title>
      <author><first>Mohamad</first><last>Alnajjar</last><affiliation>Institute</affiliation></author>
      <author><first>Ahmad</first><last>Almoustafa</last><affiliation>Institute</affiliation></author>
      <author><first>Tomohiro</first><last>Nishiyama</last><affiliation>Institute</affiliation></author>
      <author><first>Shoko</first><last>Wakamiya</last><affiliation>Institute</affiliation></author>
      <author><first>Eiji</first><last>Aramaki</last><affiliation>Institute</affiliation></author>
      <author><first>Takuya</first><last>Matsuzaki</last><affiliation>Institute</affiliation></author>
      <pages>977-982</pages>
      <abstract/>
      <url hash="22ef668d">2025.arabicnlp-sharedtasks.135</url>
      <bibkey>alnajjar-etal-2025-arxhyoka</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.135</doi>
    </paper>
    <paper id="136">
      <title>912 at <fixed-case>TAQEEM</fixed-case> 2025: A Distribution-aware Approach to <fixed-case>A</fixed-case>rabic Essay Scoring</title>
      <author><first>Trong-Tai</first><last>Dam Vu</last><affiliation>Institute</affiliation></author>
      <author><first>Thìn</first><last>Đặng Văn</last><affiliation>Institute</affiliation></author>
      <pages>983-988</pages>
      <abstract/>
      <url hash="dc84df13">2025.arabicnlp-sharedtasks.136</url>
      <bibkey>dam-vu-dang-van-2025-912</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.136</doi>
    </paper>
    <paper id="137">
      <title>Taibah at <fixed-case>TAQEEM</fixed-case> 2025: Leveraging <fixed-case>GPT</fixed-case>-4o for <fixed-case>A</fixed-case>rabic Essay Scoring</title>
      <author><first>Nada</first><last>Almarwani</last><affiliation>Institute</affiliation></author>
      <author><first>Alaa</first><last>Alharbi</last><affiliation>Institute</affiliation></author>
      <author><first>Samah</first><last>Aloufi</last><affiliation>Institute</affiliation></author>
      <pages>989-997</pages>
      <abstract/>
      <url hash="4a228803">2025.arabicnlp-sharedtasks.137</url>
      <bibkey>almarwani-etal-2025-taibah</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.137</doi>
    </paper>
    <paper id="138">
      <title><fixed-case>M</fixed-case>arsad<fixed-case>L</fixed-case>ab at <fixed-case>TAQEEM</fixed-case> 2025: Prompt-Aware Lexicon-Enhanced Transformer for <fixed-case>A</fixed-case>rabic Automated Essay Scoring</title>
      <author><first>Mabrouka</first><last>Bessghaier</last><affiliation>Institute</affiliation></author>
      <author><first>Md.</first><last>Rafiul Biswas</last><affiliation>Institute</affiliation></author>
      <author><first>Amira</first><last>Dhouib</last><affiliation>Institute</affiliation></author>
      <author><first>Wajdi</first><last>Zaghouani</last><affiliation>Institute</affiliation></author>
      <pages>998-1002</pages>
      <abstract/>
      <url hash="2cfc8c2a">2025.arabicnlp-sharedtasks.138</url>
      <bibkey>bessghaier-etal-2025-marsadlab-taqeem</bibkey>
      <doi>10.18653/v1/2025.arabicnlp-sharedtasks.138</doi>
    </paper>
  </volume>
</collection>
