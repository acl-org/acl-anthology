<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.figlang">
  <volume id="1" ingest-date="2020-06-21">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Figurative Language Processing</booktitle>
      <editor><first>Beata Beigman</first><last>Klebanov</last></editor>
      <editor><first>Ekaterina</first><last>Shutova</last></editor>
      <editor><first>Patricia</first><last>Lichtenstein</last></editor>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Chee</first><last>Wee</last></editor>
      <editor><first>Anna</first><last>Feldman</last></editor>
      <editor><first>Debanjan</first><last>Ghosh</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>July</month>
      <year>2020</year>
      <url hash="5a671364">2020.figlang-1</url>
    </meta>
    <frontmatter>
      <url hash="567b9601">2020.figlang-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>A Report on the 2020 Sarcasm Detection Shared Task</title>
      <author><first>Debanjan</first><last>Ghosh</last></author>
      <author><first>Avijit</first><last>Vajpayee</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>1–11</pages>
      <abstract>Detecting sarcasm and verbal irony is critical for understanding people’s actual sentiments and beliefs. Thus, the field of sarcasm analysis has become a popular research problem in natural language processing. As the community working on computational approaches for sarcasm detection is growing, it is imperative to conduct benchmarking studies to analyze the current state-of-the-art, facilitating progress in this area. We report on the shared task on sarcasm detection we conducted as a part of the 2nd Workshop on Figurative Language Processing (FigLang 2020) at ACL 2020.</abstract>
      <url hash="69c64937">2020.figlang-1.1</url>
      <doi>10.18653/v1/2020.figlang-1.1</doi>
      <video tag="video" href="http://slideslive.com/38929708"/>
    </paper>
    <paper id="2">
      <title>Augmenting Data for Sarcasm Detection with Unlabeled Conversation Context</title>
      <author><first>Hankyol</first><last>Lee</last></author>
      <author><first>Youngjae</first><last>Yu</last></author>
      <author><first>Gunhee</first><last>Kim</last></author>
      <pages>12–17</pages>
      <abstract>We present a novel data augmentation technique, CRA (Contextual Response Augmentation), which utilizes conversational context to generate meaningful samples for training. We also mitigate the issues regarding unbalanced context lengths by changing the input output format of the model such that it can deal with varying context lengths effectively. Specifically, our proposed model, trained with the proposed data augmentation technique, participated in the sarcasm detection task of FigLang2020, have won and achieves the best performance in both Reddit and Twitter datasets.</abstract>
      <url hash="6a54aa45">2020.figlang-1.2</url>
      <doi>10.18653/v1/2020.figlang-1.2</doi>
      <video tag="video" href="http://slideslive.com/38929696"/>
    </paper>
    <paper id="3">
      <title>A Report on the 2020 <fixed-case>VUA</fixed-case> and <fixed-case>TOEFL</fixed-case> Metaphor Detection Shared Task</title>
      <author><first>Chee Wee (Ben)</first><last>Leong</last></author>
      <author><first>Beata</first><last>Beigman Klebanov</last></author>
      <author><first>Chris</first><last>Hamill</last></author>
      <author><first>Egon</first><last>Stemle</last></author>
      <author><first>Rutuja</first><last>Ubale</last></author>
      <author><first>Xianyang</first><last>Chen</last></author>
      <pages>18–29</pages>
      <abstract>In this paper, we report on the shared task on metaphor identification on VU Amsterdam Metaphor Corpus and on a subset of the TOEFL Native Language Identification Corpus. The shared task was conducted as apart of the ACL 2020 Workshop on Processing Figurative Language.</abstract>
      <url hash="f6a9db03">2020.figlang-1.3</url>
      <doi>10.18653/v1/2020.figlang-1.3</doi>
      <revision id="1" href="2020.figlang-1.3v1" hash="2d8097c5"/>
      <revision id="2" href="2020.figlang-1.3v2" hash="f6a9db03" date="2020-08-07">This revision clarifies the relationship of the best shared task result with state of the art for the VUA corpus, in section 5.2, and acknowledges a reader for pointing out an error.</revision>
      <video tag="video" href="http://slideslive.com/38929731"/>
    </paper>
    <paper id="4">
      <title><fixed-case>D</fixed-case>eep<fixed-case>M</fixed-case>et: A Reading Comprehension Paradigm for Token-level Metaphor Detection</title>
      <author><first>Chuandong</first><last>Su</last></author>
      <author><first>Fumiyo</first><last>Fukumoto</last></author>
      <author><first>Xiaoxi</first><last>Huang</last></author>
      <author><first>Jiyi</first><last>Li</last></author>
      <author><first>Rongbo</first><last>Wang</last></author>
      <author><first>Zhiqun</first><last>Chen</last></author>
      <pages>30–39</pages>
      <abstract>Machine metaphor understanding is one of the major topics in NLP. Most of the recent attempts consider it as classification or sequence tagging task. However, few types of research introduce the rich linguistic information into the field of computational metaphor by leveraging powerful pre-training language models. We focus a novel reading comprehension paradigm for solving the token-level metaphor detection task which provides an innovative type of solution for this task. We propose an end-to-end deep metaphor detection model named DeepMet based on this paradigm. The proposed approach encodes the global text context (whole sentence), local text context (sentence fragments), and question (query word) information as well as incorporating two types of part-of-speech (POS) features by making use of the advanced pre-training language model. The experimental results by using several metaphor datasets show that our model achieves competitive results in the second shared task on metaphor detection.</abstract>
      <url hash="6633a9f3">2020.figlang-1.4</url>
      <doi>10.18653/v1/2020.figlang-1.4</doi>
      <video tag="video" href="http://slideslive.com/38929720"/>
    </paper>
    <paper id="5">
      <title>Context-Driven Satirical News Generation</title>
      <author><first>Zachary</first><last>Horvitz</last></author>
      <author><first>Nam</first><last>Do</last></author>
      <author><first>Michael L.</first><last>Littman</last></author>
      <pages>40–50</pages>
      <abstract>While mysterious, humor likely hinges on an interplay of entities, their relationships, and cultural connotations. Motivated by the importance of context in humor, we consider methods for constructing and leveraging contextual representations in generating humorous text. Specifically, we study the capacity of transformer-based architectures to generate funny satirical headlines, and show that both language models and summarization models can be fine-tuned to regularly generate headlines that people find funny. Furthermore, we find that summarization models uniquely support satire-generation by enabling the generation of topical humorous text. Outside of our formal study, we note that headlines generated by our model were accepted via a competitive process into a satirical newspaper, and one headline was ranked as high or better than 73% of human submissions. As part of our work, we contribute a dataset of over 15K satirical headlines paired with ranked contextual information from news articles and Wikipedia.</abstract>
      <url hash="1e81cce2">2020.figlang-1.5</url>
      <doi>10.18653/v1/2020.figlang-1.5</doi>
      <video tag="video" href="http://slideslive.com/38929710"/>
    </paper>
    <paper id="6">
      <title>Sarcasm Detection using Context Separators in Online Discourse</title>
      <author><first>Tanvi</first><last>Dadu</last></author>
      <author><first>Kartikey</first><last>Pant</last></author>
      <pages>51–55</pages>
      <abstract>Sarcasm is an intricate form of speech, where meaning is conveyed implicitly. Being a convoluted form of expression, detecting sarcasm is an assiduous problem. The difficulty in recognition of sarcasm has many pitfalls, including misunderstandings in everyday communications, which leads us to an increasing focus on automated sarcasm detection. In the second edition of the Figurative Language Processing (FigLang 2020) workshop, the shared task of sarcasm detection released two datasets, containing responses along with their context sampled from Twitter and Reddit. In this work, we use <tex-math>RoBERTa_{large}</tex-math> to detect sarcasm in both the datasets. We further assert the importance of context in improving the performance of contextual word embedding based models by using three different types of inputs - Response-only, Context-Response, and Context-Response (Separated). We show that our proposed architecture performs competitively for both the datasets. We also show that the addition of a separation token between context and target response results in an improvement of 5.13% in the F1-score in the Reddit dataset.</abstract>
      <url hash="8e901222">2020.figlang-1.6</url>
      <doi>10.18653/v1/2020.figlang-1.6</doi>
      <video tag="video" href="http://slideslive.com/38929695"/>
    </paper>
    <paper id="7">
      <title>Sarcasm Detection in Tweets with <fixed-case>BERT</fixed-case> and <fixed-case>G</fixed-case>lo<fixed-case>V</fixed-case>e Embeddings</title>
      <author><first>Akshay</first><last>Khatri</last></author>
      <author><first>Pranav</first><last>P</last></author>
      <pages>56–60</pages>
      <abstract>Sarcasm is a form of communication in which the person states opposite of what he actually means. In this paper, we propose using machine learning techniques with BERT and GloVe embeddings to detect sarcasm in tweets. The dataset is preprocessed before extracting the embeddings. The proposed model also uses all of the context provided in the dataset to which the user is reacting along with his actual response.</abstract>
      <url hash="060f0261">2020.figlang-1.7</url>
      <doi>10.18653/v1/2020.figlang-1.7</doi>
      <video tag="video" href="http://slideslive.com/38929697"/>
    </paper>
    <paper id="8">
      <title><fixed-case>C</fixed-case>-Net: Contextual Network for Sarcasm Detection</title>
      <author><first>Amit</first><last>Kumar Jena</last></author>
      <author><first>Aman</first><last>Sinha</last></author>
      <author><first>Rohit</first><last>Agarwal</last></author>
      <pages>61–66</pages>
      <abstract>Automatic Sarcasm Detection in conversations is a difficult and tricky task. Classifying an utterance as sarcastic or not in isolation can be futile since most of the time the sarcastic nature of a sentence heavily relies on its context. This paper presents our proposed model, C-Net, which takes contextual information of a sentence in a sequential manner to classify it as sarcastic or non-sarcastic. Our model showcases competitive performance in the Sarcasm Detection shared task organised on CodaLab and achieved 75.0% F1-score on the Twitter dataset and 66.3% F1-score on Reddit dataset.</abstract>
      <url hash="5ca7edd9">2020.figlang-1.8</url>
      <doi>10.18653/v1/2020.figlang-1.8</doi>
      <video tag="video" href="http://slideslive.com/38929698"/>
    </paper>
    <paper id="9">
      <title>Applying Transformers and Aspect-based Sentiment Analysis approaches on Sarcasm Detection</title>
      <author><first>Taha</first><last>Shangipour ataei</last></author>
      <author><first>Soroush</first><last>Javdan</last></author>
      <author><first>Behrouz</first><last>Minaei-Bidgoli</last></author>
      <pages>67–71</pages>
      <abstract>Sarcasm is a type of figurative language broadly adopted in social media and daily conversations. The sarcasm can ultimately alter the meaning of the sentence, which makes the opinion analysis process error-prone. In this paper, we propose to employ bidirectional encoder representations transformers (BERT), and aspect-based sentiment analysis approaches in order to extract the relation between context dialogue sequence and response and determine whether or not the response is sarcastic. The best performing method of ours obtains an F1 score of 0.73 on the Twitter dataset and 0.734 over the Reddit dataset at the second workshop on figurative language processing Shared Task 2020.</abstract>
      <url hash="fc8bf585">2020.figlang-1.9</url>
      <doi>10.18653/v1/2020.figlang-1.9</doi>
      <video tag="video" href="http://slideslive.com/38929699"/>
    </paper>
    <paper id="10">
      <title>Sarcasm Identification and Detection in Conversion Context using <fixed-case>BERT</fixed-case></title>
      <author><first>Kalaivani</first><last>A.</last></author>
      <author><first>Thenmozhi</first><last>D.</last></author>
      <pages>72–76</pages>
      <abstract>Sarcasm analysis in user conversion text is automatic detection of any irony, insult, hurting, painful, caustic, humour, vulgarity that degrades an individual. It is helpful in the field of sentimental analysis and cyberbullying. As an immense growth of social media, sarcasm analysis helps to avoid insult, hurts and humour to affect someone. In this paper, we present traditional machine learning approaches, deep learning approach (LSTM -RNN) and BERT (Bidirectional Encoder Representations from Transformers) for identifying sarcasm. We have used the approaches to build the model, to identify and categorize how much conversion context or response is needed for sarcasm detection and evaluated on the two social media forums that is twitter conversation dataset and reddit conversion dataset. We compare the performance based on the approaches and obtained the best F1 scores as 0.722, 0.679 for the twitter forums and reddit forums respectively.</abstract>
      <url hash="4c82ea89">2020.figlang-1.10</url>
      <doi>10.18653/v1/2020.figlang-1.10</doi>
      <video tag="video" href="http://slideslive.com/38929700"/>
    </paper>
    <paper id="11">
      <title>Neural Sarcasm Detection using Conversation Context</title>
      <author><first>Nikhil</first><last>Jaiswal</last></author>
      <pages>77–82</pages>
      <abstract>Social media platforms and discussion forums such as Reddit, Twitter, etc. are filled with figurative languages. Sarcasm is one such category of figurative language whose presence in a conversation makes language understanding a challenging task. In this paper, we present a deep neural architecture for sarcasm detection. We investigate various pre-trained language representation models (PLRMs) like BERT, RoBERTa, etc. and fine-tune it on the Twitter dataset. We experiment with a variety of PLRMs either on the twitter utterance in isolation or utilizing the contextual information along with the utterance. Our findings indicate that by taking into consideration the previous three most recent utterances, the model is more accurately able to classify a conversation as being sarcastic or not. Our best performing ensemble model achieves an overall F1 score of 0.790, which ranks us second on the leaderboard of the Sarcasm Shared Task 2020.</abstract>
      <url hash="64c8a878">2020.figlang-1.11</url>
      <doi>10.18653/v1/2020.figlang-1.11</doi>
      <video tag="video" href="http://slideslive.com/38929701"/>
    </paper>
    <paper id="12">
      <title>Context-Aware Sarcasm Detection Using <fixed-case>BERT</fixed-case></title>
      <author><first>Arup</first><last>Baruah</last></author>
      <author><first>Kaushik</first><last>Das</last></author>
      <author><first>Ferdous</first><last>Barbhuiya</last></author>
      <author><first>Kuntal</first><last>Dey</last></author>
      <pages>83–87</pages>
      <abstract>In this paper, we present the results obtained by BERT, BiLSTM and SVM classifiers on the shared task on Sarcasm Detection held as part of The Second Workshop on Figurative Language Processing. The shared task required the use of conversational context to detect sarcasm. We experimented by varying the amount of context used along with the response (response is the text to be classified). The amount of context used includes (i) zero context, (ii) last one, two or three utterances, and (iii) all utterances. It was found that including the last utterance in the dialogue along with the response improved the performance of the classifier for the Twitter data set. On the other hand, the best performance for the Reddit data set was obtained when using only the response without any contextual information. The BERT classifier obtained F-score of 0.743 and 0.658 for the Twitter and Reddit data set respectively.</abstract>
      <url hash="fb7ccaa3">2020.figlang-1.12</url>
      <doi>10.18653/v1/2020.figlang-1.12</doi>
      <video tag="video" href="http://slideslive.com/38929702"/>
    </paper>
    <paper id="13">
      <title>Transformers on Sarcasm Detection with Context</title>
      <author><first>Amardeep</first><last>Kumar</last></author>
      <author><first>Vivek</first><last>Anand</last></author>
      <pages>88–92</pages>
      <abstract>Sarcasm Detection with Context, a shared task of Second Workshop on Figurative Language Processing (co-located with ACL 2020), is study of effect of context on Sarcasm detection in conversations of Social media. We present different techniques and models, mostly based on transformer for Sarcasm Detection with Context. We extended latest pre-trained transformers like BERT, RoBERTa, spanBERT on different task objectives like single sentence classification, sentence pair classification, etc. to understand role of conversation context for sarcasm detection on Twitter conversations and conversation threads from Reddit. We also present our own architecture consisting of LSTM and Transformers to achieve the objective.</abstract>
      <url hash="2d2aa0b2">2020.figlang-1.13</url>
      <doi>10.18653/v1/2020.figlang-1.13</doi>
      <video tag="video" href="http://slideslive.com/38929703"/>
    </paper>
    <paper id="14">
      <title>A Novel Hierarchical <fixed-case>BERT</fixed-case> Architecture for Sarcasm Detection</title>
      <author><first>Himani</first><last>Srivastava</last></author>
      <author><first>Vaibhav</first><last>Varshney</last></author>
      <author><first>Surabhi</first><last>Kumari</last></author>
      <author><first>Saurabh</first><last>Srivastava</last></author>
      <pages>93–97</pages>
      <abstract>Online discussion platforms are often flooded with opinions from users across the world on a variety of topics. Many such posts, comments, or utterances are often sarcastic in nature, i.e., the actual intent is hidden in the sentence and is different from its literal meaning, making the detection of such utterances challenging without additional context. In this paper, we propose a novel deep learning-based approach to detect whether an utterance is sarcastic or non-sarcastic by utilizing the given contexts ina hierarchical manner. We have used datasets from two online discussion platforms - Twitter and Reddit1for our experiments. Experimental and error analysis shows that the hierarchical models can make full use of history to obtain a better representation of contexts and thus, in turn, can outperform their sequential counterparts.</abstract>
      <url hash="3be522ec">2020.figlang-1.14</url>
      <doi>10.18653/v1/2020.figlang-1.14</doi>
      <video tag="video" href="http://slideslive.com/38929704"/>
    </paper>
    <paper id="15">
      <title><fixed-case>D</fixed-case>etecting <fixed-case>S</fixed-case>arcasm in <fixed-case>C</fixed-case>onversation <fixed-case>C</fixed-case>ontext <fixed-case>U</fixed-case>sing <fixed-case>T</fixed-case>ransformer-<fixed-case>B</fixed-case>ased <fixed-case>M</fixed-case>odels</title>
      <author><first>Adithya</first><last>Avvaru</last></author>
      <author><first>Sanath</first><last>Vobilisetty</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>98–103</pages>
      <abstract>Sarcasm detection, regarded as one of the sub-problems of sentiment analysis, is a very typical task because the introduction of sarcastic words can flip the sentiment of the sentence itself. To date, many research works revolve around detecting sarcasm in one single sentence and there is very limited research to detect sarcasm resulting from multiple sentences. Current models used Long Short Term Memory (LSTM) variants with or without attention to detect sarcasm in conversations. We showed that the models using state-of-the-art Bidirectional Encoder Representations from Transformers (BERT), to capture syntactic and semantic information across conversation sentences, performed better than the current models. Based on the data analysis, we estimated that the number of sentences in the conversation that can contribute to the sarcasm and the results agrees to this estimation. We also perform a comparative study of our different versions of BERT-based model with other variants of LSTM model and XLNet (both using the estimated number of conversation sentences) and find out that BERT-based models outperformed them.</abstract>
      <url hash="f21683bd">2020.figlang-1.15</url>
      <doi>10.18653/v1/2020.figlang-1.15</doi>
    </paper>
    <paper id="16">
      <title>Using Conceptual Norms for Metaphor Detection</title>
      <author><first>Mingyu</first><last>Wan</last></author>
      <author><first>Kathleen</first><last>Ahrens</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Menghan</first><last>Jiang</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Rong</first><last>Xiang</last></author>
      <author><first>Chu-Ren</first><last>Huang</last></author>
      <pages>104–109</pages>
      <abstract>This paper reports a linguistically-enriched method of detecting token-level metaphors for the second shared task on Metaphor Detection. We participate in all four phases of competition with both datasets, i.e. Verbs and AllPOS on the VUA and the TOFEL datasets. We use the modality exclusivity and embodiment norms for constructing a conceptual representation of the nodes and the context. Our system obtains an F-score of 0.652 for the VUA Verbs track, which is 5% higher than the strong baselines. The experimental results across models and datasets indicate the salient contribution of using modality exclusivity and modality shift information for predicting metaphoricity.</abstract>
      <url hash="d539dd53">2020.figlang-1.16</url>
      <doi>10.18653/v1/2020.figlang-1.16</doi>
      <video tag="video" href="http://slideslive.com/38929723"/>
    </paper>
    <paper id="17">
      <title><fixed-case>ALBERT</fixed-case>-<fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> for Sequential Metaphor Detection</title>
      <author><first>Shuqun</first><last>Li</last></author>
      <author><first>Jingjie</first><last>Zeng</last></author>
      <author><first>Jinhui</first><last>Zhang</last></author>
      <author><first>Tao</first><last>Peng</last></author>
      <author><first>Liang</first><last>Yang</last></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>110–115</pages>
      <abstract>In our daily life, metaphor is a common way of expression. To understand the meaning of a metaphor, we should recognize the metaphor words which play important roles. In the metaphor detection task, we design a sequence labeling model based on ALBERT-LSTM-softmax. By applying this model, we carry out a lot of experiments and compare the experimental results with different processing methods, such as with different input sentences and tokens, or the methods with CRF and softmax. Then, some tricks are adopted to improve the experimental results. Finally, our model achieves a 0.707 F1-score for the all POS subtask and a 0.728 F1-score for the verb subtask on the TOEFL dataset.</abstract>
      <url hash="da48424c">2020.figlang-1.17</url>
      <doi>10.18653/v1/2020.figlang-1.17</doi>
      <video tag="video" href="http://slideslive.com/38929718"/>
    </paper>
    <paper id="18">
      <title>Character aware models with similarity learning for metaphor detection</title>
      <author><first>Tarun</first><last>Kumar</last></author>
      <author><first>Yashvardhan</first><last>Sharma</last></author>
      <pages>116–125</pages>
      <abstract>Recent work on automatic sequential metaphor detection has involved recurrent neural networks initialized with different pre-trained word embeddings and which are sometimes combined with hand engineered features. To capture lexical and orthographic information automatically, in this paper we propose to add character based word representation. Also, to contrast the difference between literal and contextual meaning, we utilize a similarity network. We explore these components via two different architectures - a BiLSTM model and a Transformer Encoder model similar to BERT to perform metaphor identification. We participate in the Second Shared Task on Metaphor Detection on both the VUA and TOFEL datasets with the above models. The experimental results demonstrate the effectiveness of our method as it outperforms all the systems which participated in the previous shared task.</abstract>
      <url hash="50d48c2c">2020.figlang-1.18</url>
      <doi>10.18653/v1/2020.figlang-1.18</doi>
      <video tag="video" href="http://slideslive.com/38929724"/>
    </paper>
    <paper id="19">
      <title>Sky + Fire = Sunset. Exploring Parallels between Visually Grounded Metaphors and Image Classifiers</title>
      <author><first>Yuri</first><last>Bizzoni</last></author>
      <author><first>Simon</first><last>Dobnik</last></author>
      <pages>126–135</pages>
      <abstract>This work explores the differences and similarities between neural image classifiers’ mis-categorisations and visually grounded metaphors - that we could conceive as intentional mis-categorisations. We discuss the possibility of using automatic image classifiers to approximate human metaphoric behaviours, and the limitations of such frame. We report two pilot experiments to study grounded metaphoricity. In the first we represent metaphors as a form of visual mis-categorisation. In the second we model metaphors as a more flexible, compositional operation in a continuous visual space generated from automatic classification systems.</abstract>
      <url hash="ce03dbb4">2020.figlang-1.19</url>
      <doi>10.18653/v1/2020.figlang-1.19</doi>
      <video tag="video" href="http://slideslive.com/38929714"/>
    </paper>
    <paper id="20">
      <title>Recognizing Euphemisms and Dysphemisms Using Sentiment Analysis</title>
      <author><first>Christian</first><last>Felt</last></author>
      <author><first>Ellen</first><last>Riloff</last></author>
      <pages>136–145</pages>
      <abstract>This paper presents the first research aimed at recognizing euphemistic and dysphemistic phrases with natural language processing. Euphemisms soften references to topics that are sensitive, disagreeable, or taboo. Conversely, dysphemisms refer to sensitive topics in a harsh or rude way. For example, “passed away” and “departed” are euphemisms for death, while “croaked” and “six feet under” are dysphemisms for death. Our work explores the use of sentiment analysis to recognize euphemistic and dysphemistic language. First, we identify near-synonym phrases for three topics (firing, lying, and stealing) using a bootstrapping algorithm for semantic lexicon induction. Next, we classify phrases as euphemistic, dysphemistic, or neutral using lexical sentiment cues and contextual sentiment analysis. We introduce a new gold standard data set and present our experimental results for this task.</abstract>
      <url hash="e06ee38a">2020.figlang-1.20</url>
      <doi>10.18653/v1/2020.figlang-1.20</doi>
      <video tag="video" href="http://slideslive.com/38929717"/>
    </paper>
    <paper id="21">
      <title><fixed-case>I</fixed-case>llini<fixed-case>M</fixed-case>et: <fixed-case>I</fixed-case>llinois System for Metaphor Detection with Contextual and Linguistic Information</title>
      <author><first>Hongyu</first><last>Gong</last></author>
      <author><first>Kshitij</first><last>Gupta</last></author>
      <author><first>Akriti</first><last>Jain</last></author>
      <author><first>Suma</first><last>Bhat</last></author>
      <pages>146–153</pages>
      <abstract>Metaphors are rhetorical use of words based on the conceptual mapping as opposed to their literal use. Metaphor detection, an important task in language understanding, aims to identify metaphors in word level from given sentences. We present IlliniMet, a system to automatically detect metaphorical words. Our model combines the strengths of the contextualized representation by the widely used RoBERTa model and the rich linguistic information from external resources such as WordNet. The proposed approach is shown to outperform strong baselines on a benchmark dataset. Our best model achieves F1 scores of 73.0% on VUA ALLPOS, 77.1% on VUA VERB, 70.3% on TOEFL ALLPOS and 71.9% on TOEFL VERB.</abstract>
      <url hash="998371bd">2020.figlang-1.21</url>
      <doi>10.18653/v1/2020.figlang-1.21</doi>
      <video tag="video" href="http://slideslive.com/38929719"/>
    </paper>
    <paper id="22">
      <title>Adaptation of Word-Level Benchmark Datasets for Relation-Level Metaphor Identification</title>
      <author><first>Omnia</first><last>Zayed</last></author>
      <author><first>John Philip</first><last>McCrae</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <pages>154–164</pages>
      <abstract>Metaphor processing and understanding has attracted the attention of many researchers recently with an increasing number of computational approaches. A common factor among these approaches is utilising existing benchmark datasets for evaluation and comparisons. The availability, quality and size of the annotated data are among the main difficulties facing the growing research area of metaphor processing. The majority of current approaches pertaining to metaphor processing concentrate on word-level processing due to data availability. On the other hand, approaches that process metaphors on the relation-level ignore the context where the metaphoric expression. This is due to the nature and format of the available data. Word-level annotation is poorly grounded theoretically and is harder to use in downstream tasks such as metaphor interpretation. The conversion from word-level to relation-level annotation is non-trivial. In this work, we attempt to fill this research gap by adapting three benchmark datasets, namely the VU Amsterdam metaphor corpus, the TroFi dataset and the TSV dataset, to suit relation-level metaphor identification. We publish the adapted datasets to facilitate future research in relation-level metaphor processing.</abstract>
      <url hash="bdc4e760">2020.figlang-1.22</url>
      <doi>10.18653/v1/2020.figlang-1.22</doi>
      <video tag="video" href="http://slideslive.com/38929709"/>
    </paper>
    <paper id="23">
      <title>Generating Ethnographic Models from Communities’ Online Data</title>
      <author><first>Tomek</first><last>Strzalkowski</last></author>
      <author><first>Anna</first><last>Newheiser</last></author>
      <author><first>Nathan</first><last>Kemper</last></author>
      <author><first>Ning</first><last>Sa</last></author>
      <author><first>Bharvee</first><last>Acharya</last></author>
      <author><first>Gregorios</first><last>Katsios</last></author>
      <pages>165–175</pages>
      <abstract>In this paper we describe computational ethnography study to demonstrate how machine learning techniques can be utilized to exploit bias resident in language data produced by communities with online presence. Specifically, we leverage the use of figurative language (i.e., the choice of metaphors) in online text (e.g., news media, blogs) produced by distinct communities to obtain models of community worldviews that can be shown to be distinctly biased and thus different from other communities’ models. We automatically construct metaphor-based community models for two distinct scenarios: gun rights and marriage equality. We then conduct a series of experiments to validate the hypothesis that the metaphors found in each community’s online language convey the bias in the community’s worldview.</abstract>
      <url hash="10dcef0a">2020.figlang-1.23</url>
      <attachment type="Software" hash="3084e139">2020.figlang-1.23.Software.zip</attachment>
      <doi>10.18653/v1/2020.figlang-1.23</doi>
      <attachment type="Dataset" hash="5d983303">2020.figlang-1.23.Dataset.pdf</attachment>
      <video tag="video" href="http://slideslive.com/38929711"/>
    </paper>
    <paper id="24">
      <title><fixed-case>O</fixed-case>xymorons: a preliminary corpus investigation</title>
      <author><first>Marta</first><last>La Pietra</last></author>
      <author><first>Francesca</first><last>Masini</last></author>
      <pages>176–185</pages>
      <abstract>This paper contains a preliminary corpus study of oxymorons, a figure of speech so far under-investigated in NLP-oriented research. The study resulted in a list of 376 oxymorons, identified by extracting a set of antonymous pairs (under various configurations) from corpora of written Italian and by manually checking the results. A complementary method is also envisaged for discovering contextual oxymorons, which are highly relevant for the detection of humor, irony and sarcasm.</abstract>
      <url hash="426f1bf0">2020.figlang-1.24</url>
      <doi>10.18653/v1/2020.figlang-1.24</doi>
      <video tag="video" href="http://slideslive.com/38929712"/>
    </paper>
    <paper id="25">
      <title>Can Humor Prediction Datasets be used for Humor Generation? Humorous Headline Generation via Style Transfer</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Nancy</first><last>Fulda</last></author>
      <author><first>Kevin</first><last>Seppi</last></author>
      <pages>186–191</pages>
      <abstract>Understanding and identifying humor has been increasingly popular, as seen by the number of datasets created to study humor. However, one area of humor research, humor generation, has remained a difficult task, with machine generated jokes failing to match human-created humor. As many humor prediction datasets claim to aid in generative tasks, we examine whether these claims are true. We focus our experiments on the most popular dataset, included in the 2020 SemEval’s Task 7, and teach our model to take normal text and “translate” it into humorous text. We evaluate our model compared to humorous human generated headlines, finding that our model is preferred equally in A/B testing with the human edited versions, a strong success for humor generation, and is preferred over an intelligent random baseline 72% of the time. We also show that our model is assumed to be human written comparable with that of the human edited headlines and is significantly better than random, indicating that this dataset does indeed provide potential for future humor generation systems.</abstract>
      <url hash="7c2f36d7">2020.figlang-1.25</url>
      <doi>10.18653/v1/2020.figlang-1.25</doi>
      <video tag="video" href="http://slideslive.com/38929713"/>
    </paper>
    <paper id="26">
      <title>Evaluating a <fixed-case>B</fixed-case>i-<fixed-case>LSTM</fixed-case> Model for Metaphor Detection in <fixed-case>TOEFL</fixed-case> Essays</title>
      <author><first>Kevin</first><last>Kuo</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>192–196</pages>
      <abstract>This paper describes systems submitted to the Metaphor Shared Task at the Second Workshop on Figurative Language Processing. In this submission, we replicate the evaluation of the Bi-LSTM model introduced by Gao et al.(2018) on the VUA corpus in a new setting: TOEFL essays written by non-native English speakers. Our results show that Bi-LSTM models outperform feature-rich linear models on this challenging task, which is consistent with prior findings on the VUA dataset. However, the Bi-LSTM models lag behind the best performing systems in the shared task.</abstract>
      <url hash="8a2b23f7">2020.figlang-1.26</url>
      <doi>10.18653/v1/2020.figlang-1.26</doi>
      <video tag="video" href="http://slideslive.com/38929721"/>
    </paper>
    <paper id="27">
      <title>Neural Metaphor Detection with a Residual bi<fixed-case>LSTM</fixed-case>-<fixed-case>CRF</fixed-case> Model</title>
      <author><first>Andrés</first><last>Torres Rivera</last></author>
      <author><first>Antoni</first><last>Oliver</last></author>
      <author><first>Salvador</first><last>Climent</last></author>
      <author><first>Marta</first><last>Coll-Florit</last></author>
      <pages>197–203</pages>
      <abstract>In this paper we present a novel resource-inexpensive architecture for metaphor detection based on a residual bidirectional long short-term memory and conditional random fields. Current approaches on this task rely on deep neural networks to identify metaphorical words, using additional linguistic features or word embeddings. We evaluate our proposed approach using different model configurations that combine embeddings, part of speech tags, and semantically disambiguated synonym sets. This evaluation process was performed using the training and testing partitions of the VU Amsterdam Metaphor Corpus. We use this method of evaluation as reference to compare the results with other current neural approaches for this task that implement similar neural architectures and features, and that were evaluated using this corpus. Results show that our system achieves competitive results with a simpler architecture compared to previous approaches.</abstract>
      <url hash="b50332bb">2020.figlang-1.27</url>
      <doi>10.18653/v1/2020.figlang-1.27</doi>
      <video tag="video" href="http://slideslive.com/38929722"/>
    </paper>
    <paper id="28">
      <title>Augmenting Neural Metaphor Detection with Concreteness</title>
      <author><first>Ghadi</first><last>Alnafesah</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>204–210</pages>
      <abstract>The idea that a shift in concreteness within a sentence indicates the presence of a metaphor has been around for a while. However, recent methods of detecting metaphor that have relied on deep neural models have ignored concreteness and related psycholinguistic information. We hypothesis that this information is not available to these models and that their addition will boost the performance of these models in detecting metaphor. We test this hypothesis on the Metaphor Detection Shared Task 2020 and find that the addition of concreteness information does in fact boost deep neural models. We also run tests on data from a previous shared task and show similar results.</abstract>
      <url hash="7777f0bb">2020.figlang-1.28</url>
      <doi>10.18653/v1/2020.figlang-1.28</doi>
      <video tag="video" href="http://slideslive.com/38929725"/>
    </paper>
    <paper id="29">
      <title>Supervised Disambiguation of <fixed-case>G</fixed-case>erman Verbal Idioms with a <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case> Architecture</title>
      <author><first>Rafael</first><last>Ehren</last></author>
      <author><first>Timm</first><last>Lichte</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Jakub</first><last>Waszczuk</last></author>
      <pages>211–220</pages>
      <abstract>Supervised disambiguation of verbal idioms (VID) poses special demands on the quality and quantity of the annotated data used for learning and evaluation. In this paper, we present a new VID corpus for German and perform a series of VID disambiguation experiments on it. Our best classifier, based on a neural architecture, yields an error reduction across VIDs of 57% in terms of accuracy compared to a simple majority baseline.</abstract>
      <url hash="ce346dcf">2020.figlang-1.29</url>
      <doi>10.18653/v1/2020.figlang-1.29</doi>
      <video tag="video" href="http://slideslive.com/38929715"/>
    </paper>
    <paper id="30">
      <title>Metaphor Detection using Context and Concreteness</title>
      <author><first>Rowan</first><last>Hall Maudslay</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>221–226</pages>
      <abstract>We report the results of our system on the Metaphor Detection Shared Task at the Second Workshop on Figurative Language Processing 2020. Our model is an ensemble, utilising contextualised and static distributional semantic representations, along with word-type concreteness ratings. Using these features, it predicts word metaphoricity with a deep multi-layer perceptron. We are able to best the state-of-the-art from the 2018 Shared Task by an average of 8.0% F1, and finish fourth in both sub-tasks in which we participate.</abstract>
      <url hash="16e2d02a">2020.figlang-1.30</url>
      <doi>10.18653/v1/2020.figlang-1.30</doi>
      <video tag="video" href="http://slideslive.com/38929726"/>
    </paper>
    <paper id="31">
      <title>Being neighbourly: Neural metaphor identification in discourse</title>
      <author><first>Verna</first><last>Dankers</last></author>
      <author><first>Karan</first><last>Malhotra</last></author>
      <author><first>Gaurav</first><last>Kudva</last></author>
      <author><first>Volodymyr</first><last>Medentsiy</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <pages>227–234</pages>
      <abstract>Existing approaches to metaphor processing typically rely on local features, such as immediate lexico-syntactic contexts or information within a given sentence. However, a large body of corpus-linguistic research suggests that situational information and broader discourse properties influence metaphor production and comprehension. In this paper, we present the first neural metaphor processing architecture that models a broader discourse through the use of attention mechanisms. Our models advance the state of the art on the all POS track of the 2018 VU Amsterdam metaphor identification task. The inclusion of discourse-level information yields further significant improvements.</abstract>
      <url hash="dd59e60d">2020.figlang-1.31</url>
      <doi>10.18653/v1/2020.figlang-1.31</doi>
      <video tag="video" href="http://slideslive.com/38929716"/>
    </paper>
    <paper id="32">
      <title>Go Figure! Multi-task transformer-based architecture for metaphor detection using idioms: <fixed-case>ETS</fixed-case> team in 2020 metaphor shared task</title>
      <author><first>Xianyang</first><last>Chen</last></author>
      <author><first>Chee Wee (Ben)</first><last>Leong</last></author>
      <author><first>Michael</first><last>Flor</last></author>
      <author><first>Beata</first><last>Beigman Klebanov</last></author>
      <pages>235–243</pages>
      <abstract>This paper describes the ETS entry to the 2020 Metaphor Detection shared task. Our contribution consists of a sequence of experiments using BERT, starting with a baseline, strengthening it by spell-correcting the TOEFL corpus, followed by a multi-task learning setting, where one of the tasks is the token-level metaphor classification as per the shared task, while the other is meant to provide additional training that we hypothesized to be relevant to the main task. In one case, out-of-domain data manually annotated for metaphor is used for the auxiliary task; in the other case, in-domain data automatically annotated for idioms is used for the auxiliary task. Both multi-task experiments yield promising results.</abstract>
      <url hash="f482f6b0">2020.figlang-1.32</url>
      <doi>10.18653/v1/2020.figlang-1.32</doi>
      <video tag="video" href="http://slideslive.com/38929727"/>
    </paper>
    <paper id="33">
      <title>Metaphor Detection using Ensembles of Bidirectional Recurrent Neural Networks</title>
      <author><first>Jennifer</first><last>Brooks</last></author>
      <author><first>Abdou</first><last>Youssef</last></author>
      <pages>244–249</pages>
      <abstract>In this paper we present our results from the Second Shared Task on Metaphor Detection, hosted by the Second Workshop on Figurative Language Processing. We use an ensemble of RNN models with bidirectional LSTMs and bidirectional attention mechanisms. Some of the models were trained on all parts of speech. Each of the other models was trained on one of four categories for parts of speech: “nouns”, “verbs”, “adverbs/adjectives”, or “other”. The models were combined into voting pools and the voting pools were combined using the logical “OR” operator.</abstract>
      <url hash="f68d3dff">2020.figlang-1.33</url>
      <doi>10.18653/v1/2020.figlang-1.33</doi>
      <video tag="video" href="http://slideslive.com/38929728"/>
    </paper>
    <paper id="34">
      <title>Metaphor Detection Using Contextual Word Embeddings From Transformers</title>
      <author><first>Jerry</first><last>Liu</last></author>
      <author><first>Nathan</first><last>O’Hara</last></author>
      <author><first>Alexander</first><last>Rubin</last></author>
      <author><first>Rachel</first><last>Draelos</last></author>
      <author><first>Cynthia</first><last>Rudin</last></author>
      <pages>250–255</pages>
      <abstract>The detection of metaphors can provide valuable information about a given text and is crucial to sentiment analysis and machine translation. In this paper, we outline the techniques for word-level metaphor detection used in our submission to the Second Shared Task on Metaphor Detection. We propose using both BERT and XLNet language models to create contextualized embeddings and a bi-directional LSTM to identify whether a given word is a metaphor. Our best model achieved F1-scores of 68.0% on VUA AllPOS, 73.0% on VUA Verbs, 66.9% on TOEFL AllPOS, and 69.7% on TOEFL Verbs, placing 7th, 6th, 5th, and 5th respectively. In addition, we outline another potential approach with a KNN-LSTM ensemble model that we did not have enough time to implement given the deadline for the competition. We show that a KNN classifier provides a similar F1-score on a validation set as the LSTM and yields different information on metaphors.</abstract>
      <url hash="0d888890">2020.figlang-1.34</url>
      <doi>10.18653/v1/2020.figlang-1.34</doi>
      <video tag="video" href="http://slideslive.com/38929729"/>
    </paper>
    <paper id="35">
      <title>Testing the role of metadata in metaphor identification</title>
      <author><first>Egon</first><last>Stemle</last></author>
      <author><first>Alexander</first><last>Onysko</last></author>
      <pages>256–263</pages>
      <abstract>This paper describes the adaptation and application of a neural network system for the automatic detection of metaphors. The LSTM BiRNN system participated in the shared task of metaphor identification that was part of the Second Workshop of Figurative Language Processing (FigLang2020) held at the Annual Conference of the Association for Computational Linguistics (ACL2020). The particular focus of our approach is on the potential influence that the metadata given in the ETS Corpus of Non-Native Written English might have on the automatic detection of metaphors in this dataset. The article first discusses the annotated ETS learner data, highlighting some of its peculiarities and inherent biases of metaphor use. A series of evaluations follow in order to test whether specific metadata influence the system performance in the task of automatic metaphor identification. The system is available under the APLv2 open-source license.</abstract>
      <url hash="b5fcbc81">2020.figlang-1.35</url>
      <doi>10.18653/v1/2020.figlang-1.35</doi>
      <video tag="video" href="http://slideslive.com/38929730"/>
    </paper>
    <paper id="36">
      <title>Sarcasm Detection Using an Ensemble Approach</title>
      <author><first>Jens</first><last>Lemmens</last></author>
      <author><first>Ben</first><last>Burtenshaw</last></author>
      <author><first>Ehsan</first><last>Lotfi</last></author>
      <author><first>Ilia</first><last>Markov</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>264–269</pages>
      <abstract>We present an ensemble approach for the detection of sarcasm in Reddit and Twitter responses in the context of The Second Workshop on Figurative Language Processing held in conjunction with ACL 2020. The ensemble is trained on the predicted sarcasm probabilities of four component models and on additional features, such as the sentiment of the comment, its length, and source (Reddit or Twitter) in order to learn which of the component models is the most reliable for which input. The component models consist of an LSTM with hashtag and emoji representations; a CNN-LSTM with casing, stop word, punctuation, and sentiment representations; an MLP based on Infersent embeddings; and an SVM trained on stylometric and emotion-based features. All component models use the two conversational turns preceding the response as context, except for the SVM, which only uses features extracted from the response. The ensemble itself consists of an adaboost classifier with the decision tree algorithm as base estimator and yields F1-scores of 67% and 74% on the Reddit and Twitter test data, respectively.</abstract>
      <url hash="a15ac2d3">2020.figlang-1.36</url>
      <doi>10.18653/v1/2020.figlang-1.36</doi>
      <video tag="video" href="http://slideslive.com/38929694"/>
    </paper>
    <paper id="37">
      <title>A Transformer Approach to Contextual Sarcasm Detection in <fixed-case>T</fixed-case>witter</title>
      <author><first>Hunter</first><last>Gregory</last></author>
      <author><first>Steven</first><last>Li</last></author>
      <author><first>Pouya</first><last>Mohammadi</last></author>
      <author><first>Natalie</first><last>Tarn</last></author>
      <author><first>Rachel</first><last>Draelos</last></author>
      <author><first>Cynthia</first><last>Rudin</last></author>
      <pages>270–275</pages>
      <abstract>Understanding tone in Twitter posts will be increasingly important as more and more communication moves online. One of the most difficult, yet important tones to detect is sarcasm. In the past, LSTM and transformer architecture models have been used to tackle this problem. We attempt to expand upon this research, implementing LSTM, GRU, and transformer models, and exploring new methods to classify sarcasm in Twitter posts. Among these, the most successful were transformer models, most notably BERT. While we attempted a few other models described in this paper, our most successful model was an ensemble of transformer models including BERT, RoBERTa, XLNet, RoBERTa-large, and ALBERT. This research was performed in conjunction with the sarcasm detection shared task section in the Second Workshop on Figurative Language Processing, co-located with ACL 2020.</abstract>
      <url hash="68cd72af">2020.figlang-1.37</url>
      <doi>10.18653/v1/2020.figlang-1.37</doi>
      <video tag="video" href="http://slideslive.com/38929706"/>
    </paper>
    <paper id="38">
      <title>Transformer-based Context-aware Sarcasm Detection in Conversation Threads from Social Media</title>
      <author><first>Xiangjue</first><last>Dong</last></author>
      <author><first>Changmao</first><last>Li</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>276–280</pages>
      <abstract>We present a transformer-based sarcasm detection model that accounts for the context from the entire conversation thread for more robust predictions. Our model uses deep transformer layers to perform multi-head attentions among the target utterance and the relevant context in the thread. The context-aware models are evaluated on two datasets from social media, Twitter and Reddit, and show 3.1% and 7.0% improvements over their baselines. Our best models give the F1-scores of 79.0% and 75.0% for the Twitter and Reddit datasets respectively, becoming one of the highest performing systems among 36 participants in this shared task.</abstract>
      <url hash="0159aa16">2020.figlang-1.38</url>
      <doi>10.18653/v1/2020.figlang-1.38</doi>
      <video tag="video" href="http://slideslive.com/38929707"/>
    </paper>
  </volume>
</collection>
