<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.nlpcss">
  <volume id="1" ingest-date="2024-06-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Sixth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS 2024)</booktitle>
      <editor><first>Dallas</first><last>Card</last></editor>
      <editor><first>Anjalie</first><last>Field</last></editor>
      <editor><first>Dirk</first><last>Hovy</last></editor>
      <editor><first>Katherine</first><last>Keith</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="e835e499">2024.nlpcss-1</url>
      <venue>nlpcss</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="c9bb0d78">2024.nlpcss-1.0</url>
      <bibkey>nlp-css-2024-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Detecting Perspective-Getting in <fixed-case>W</fixed-case>ikipedia Discussions</title>
      <author><first>Evgeny</first><last>Vasilets</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Tijs</first><last>Broek</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Anna</first><last>Wegmann</last><affiliation>Utrecht University</affiliation></author>
      <author><first>David</first><last>Abadi</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Dong</first><last>Nguyen</last><affiliation>Utrecht University</affiliation></author>
      <pages>1-15</pages>
      <abstract>Perspective-getting (i.e., the effort to obtain information about the other person’s perspective) can lead to more accurate interpersonal understanding. In this paper, we develop an approach to measure perspective-getting and apply it to English Wikipedia discussions. First, we develop a codebook based on perspective-getting theory to operationalize perspective-getting into two categories: asking questions about and attending the other’s perspective. Second, we use the codebook to annotate perspective-getting in Wikipedia discussion pages. Third, we fine-tune a RoBERTa model that achieves an average F-1 score of 0.76 on the two perspective-getting categories. Last, we test whether perspective-getting is associated with discussion outcomes. Perspective-getting was not higher in non-escalated discussions. However, discussions starting with a post attending the other’s perspective are followed by responses that are more likely to also attend the other’s perspective. Future research may use our model to study the influence of perspective-getting on the dynamics and outcomes of online discussions.</abstract>
      <url hash="5fb22f2f">2024.nlpcss-1.1</url>
      <bibkey>vasilets-etal-2024-detecting</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.1</doi>
    </paper>
    <paper id="2">
      <title>Connecting the Dots in News Analysis: Bridging the Cross-Disciplinary Disparities in Media Bias and Framing</title>
      <author><first>Gisela</first><last>Vallejo</last></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>University of Melbourne</affiliation></author>
      <pages>16-31</pages>
      <abstract>The manifestation and effect of bias in news reporting have been central topics in the social sciences for decades, and have received increasing attention in the NLP community recently. While NLP can help to scale up analyses or contribute automatic procedures to investigate the impact of biased news in society, we argue that methodologies that are currently dominant fall short of capturing the complex questions and effects addressed in theoretical media studies. This is problematic because it diminishes the validity and safety of the resulting tools and applications. Here, we review and critically compare task formulations, methods and evaluation schemes in the social sciences and NLP. We discuss open questions and suggest possible directions to close identified gaps between theory and predictive models, and their evaluation. These include model transparency, considering document-external information, and cross-document reasoning.</abstract>
      <url hash="c697af9f">2024.nlpcss-1.2</url>
      <bibkey>vallejo-etal-2024-connecting</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.2</doi>
    </paper>
    <paper id="3">
      <title>The Crime of Being Poor: Associations between Crime and Poverty on Social Media in Eight Countries</title>
      <author><first>Georgina</first><last>Curto</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Svetlana</first><last>Kiritchenko</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Kathleen</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Isar</first><last>Nejadgholi</last><affiliation>National Research Council Canada and University of Ottawa</affiliation></author>
      <pages>32-45</pages>
      <abstract>Negative public perceptions of people living in poverty can hamper policies and programs that aim to help the poor. One prominent example of social bias and discrimination against people in need is the persistent association of poverty with criminality. The phenomenon has two facets: first, the belief that poor people are more likely to engage in crime (e.g., stealing, mugging, violence) and second, the view that certain behaviors directly resulting from poverty (e.g., living outside, panhandling) warrant criminal punishment. In this paper, we use large language models (LLMs) to identify examples of crime–poverty association (CPA) in English social media texts. We analyze the online discourse on CPA across eight geographically-diverse countries, and find evidence that the CPA rates are higher within the sample obtained from the U.S. and Canada, as compared to the other countries such as South Africa, despite the latter having higher poverty, criminality, and inequality indexes. We further uncover and analyze the most common themes in CPA posts and find more negative and biased attitudes toward people living in poverty in posts from the U.S. and Canada. These results could partially be explained by cultural factors related to the tendency to overestimate the equality of opportunities and social mobility in the U.S. and Canada. These findings have consequences for policy-making and open a new path of research for poverty mitigation with the focus not only on the redistribution of wealth but also on the mitigation of bias and discrimination against people in need.</abstract>
      <url hash="7acbae10">2024.nlpcss-1.3</url>
      <bibkey>curto-etal-2024-crime</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.3</doi>
    </paper>
    <paper id="4">
      <title>Discovering Implicit Meanings of Cultural Motifs from Text</title>
      <author><first>Anurag</first><last>Acharya</last><affiliation>Pacific Northwest National Laboratory</affiliation></author>
      <author><first>Diego</first><last>Estrada</last><affiliation>Florida International University</affiliation></author>
      <author><first>Shreeja</first><last>Dahal</last><affiliation>Florida International University</affiliation></author>
      <author><first>W. Victor H.</first><last>Yarlott</last><affiliation>Florida International University</affiliation></author>
      <author><first>Diana</first><last>Gomez</last><affiliation>Florida International University</affiliation></author>
      <author><first>Mark</first><last>Finlayson</last><affiliation>Florida International University</affiliation></author>
      <pages>46-56</pages>
      <abstract>Motifs are distinctive, recurring, widely used idiom-like words or phrases, often originating in folklore and usually strongly anchored to a particular cultural or national group. Motifs are significant communicative devices across a wide range of media—including news, literature, and propaganda—because they can concisely imply a large set of culturally relevant associations. One difficulty of understanding motifs is that their meaning is usually implicit, so for an out-group person the meaning is inaccessible. We present the Motif Implicit Meaning Extractor (MIME), a proof-of-concept system designed to automatically identify a motif’s implicit meaning, as evidenced by textual uses of the motif across a large set data. MIME uses several sources (including motif indices, Wikipedia pages on the motifs, explicit explanations of motifs from in-group informants, and news/social media posts where the motif is used) and can generate a structured report of information about a motif understandable to an out-group person. In addition to a variety of examples and information drawn from structured sources, the report includes implicit information about a motif such as the type of reference (e.g., a person, an organization, etc.), it’s general connotation (strongly negative, slightly negative, neutral, etc.), and it’s associations (typically adjectives). We describe how MIME works and demonstrate its operation on a small set of manually curated motifs. We perform a qualitative evaluation of the output, and assess the difficulty of the problem, showing that explicit motif information provided by cultural informants is critical to high quality output, although mining motif usages in news and social media provides useful additional depth. A system such as MIME, appropriately scaled up, would potentially be quite useful to an out-group person trying to understand in-group usages of motifs, and has wide potential applications in domains such as literary criticism, cultural heritage, marketed and branding, and intelligence analysis.</abstract>
      <url hash="100c032c">2024.nlpcss-1.4</url>
      <bibkey>acharya-etal-2024-discovering</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.4</doi>
    </paper>
    <paper id="5">
      <title>Can Large Language Models (or Humans) Disentangle Text?</title>
      <author><first>Nicolas</first><last>Audinet de Pieuchon</last></author>
      <author><first>Adel</first><last>Daoud</last><affiliation>Linköping University</affiliation></author>
      <author><first>Connor</first><last>Jerzak</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Moa</first><last>Johansson</last><affiliation>Chalmers University of Technology</affiliation></author>
      <author><first>Richard</first><last>Johansson</last><affiliation>Chalmers University of Technology and Göteborg University</affiliation></author>
      <pages>57-67</pages>
      <abstract>We investigate the potential of large language models (LLMs) to disentangle text variables—to remove the textual traces of an undesired forbidden variable in a task sometimes known as text distillation and closely related to the fairness in AI and causal inference literature. We employ a range of various LLM approaches in an attempt to disentangle text by identifying and removing information about a target variable while preserving other relevant signals. We show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still detectable to machine learning classifiers post-LLM-disentanglement. Furthermore, we find that human annotators also struggle to disentangle sentiment while preserving other semantic content. This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of disentanglement methods that achieve statistical independence in representation space.</abstract>
      <url hash="ee753c29">2024.nlpcss-1.5</url>
      <bibkey>pieuchon-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.5</doi>
    </paper>
    <paper id="6">
      <title>Retrieval Augmented Generation of Subjective Explanations for Socioeconomic Scenarios</title>
      <author><first>Razvan-Gabriel</first><last>Dumitru</last></author>
      <author><first>Maria</first><last>Alexeeva</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Keith</first><last>Alcock</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Nargiza</first><last>Ludgate</last><affiliation>University of Florida</affiliation></author>
      <author><first>Cheonkam</first><last>Jeong</last></author>
      <author><first>Zara Fatima</first><last>Abdurahaman</last><affiliation>RAND Corporation</affiliation></author>
      <author><first>Prateek</first><last>Puri</last><affiliation>RAND Corporation</affiliation></author>
      <author><first>Brian</first><last>Kirchhoff</last><affiliation>RAND Corporation</affiliation></author>
      <author><first>Santadarshan</first><last>Sadhu</last><affiliation>RAND Corporation</affiliation></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <pages>68-85</pages>
      <abstract>We introduce a novel retrieval augmented generation approach that explicitly models causality and subjectivity. We use it to generate explanations for socioeconomic scenarios that capture beliefs of local populations. Through intrinsic and extrinsic evaluation, we show that our explanations, contextualized using causal and subjective information retrieved from local news sources, are rated higher than those produced by other large language models both in terms of mimicking the real population and the explanations quality. We also provide a discussion of the role subjectivity plays in evaluation of this natural language generation task.</abstract>
      <url hash="f7b67c5a">2024.nlpcss-1.6</url>
      <bibkey>dumitru-etal-2024-retrieval</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.6</doi>
    </paper>
    <paper id="7">
      <title>Where on Earth Do Users Say They Are?: Geo-Entity Linking for Noisy Multilingual User Input</title>
      <author><first>Tessa</first><last>Masis</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Brendan</first><last>O’Connor</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>86-98</pages>
      <abstract>Geo-entity linking is the task of linking a location mention to the real-world geographic location. In this we explore the challenging task of geo-entity linking for noisy, multilingual social media data. There are few open-source multilingual geo-entity linking tools available and existing ones are often rule-based, which break easily in social media settings, or LLM-based, which are too expensive for large-scale datasets. We present a method which represents real-world locations as averaged embeddings from labeled user-input location names and allows for selective prediction via an interpretable confidence score. We show that our approach improves geo-entity linking on a global and multilingual social media dataset, and discuss progress and problems with evaluating at different geographic granularities.</abstract>
      <url hash="3ba51669">2024.nlpcss-1.7</url>
      <bibkey>masis-oconnor-2024-earth</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.7</doi>
    </paper>
    <paper id="8">
      <title>News Deja Vu: Connecting Past and Present with Semantic Search</title>
      <author><first>Brevin</first><last>Franklin</last><affiliation>University of California, Berkeley and Harvard University</affiliation></author>
      <author><first>Emily</first><last>Silcock</last><affiliation>Department of Economics, Harvard University</affiliation></author>
      <author><first>Abhishek</first><last>Arora</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <author><first>Tom</first><last>Bryan</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <author><first>Melissa</first><last>Dell</last><affiliation>Harvard University, Harvard University</affiliation></author>
      <pages>99-112</pages>
      <abstract>Social scientists and the general public often analyze contemporary events by drawing parallels with the past, a process complicated by the vast, noisy, and unstructured nature of historical texts. For example, hundreds of millions of page scans from historical newspapers have been noisily transcribed. Traditional sparse methods for searching for relevant material in these vast corpora, e.g., with keywords, can be brittle given complex vocabularies and OCR noise. This study introduces News Deja Vu, a novel semantic search tool that leverages transformer large language models and a bi-encoder approach to identify historical news articles that are most similar to modern news queries. News Deja Vu first recognizes and masks entities, in order to focus on broader parallels rather than the specific named entities being discussed. Then, a contrastively trained, lightweight bi-encoder retrieves historical articles that are most similar semantically to a modern query, illustrating how phenomena that might seem unique to the present have varied historical precedents. Aimed at social scientists, the user-friendly News Deja Vu package is designed to be accessible for those who lack extensive familiarity with deep learning. It works with large text datasets, and we show how it can be deployed to a massive scale corpus of historical, open-source news articles. While human expertise remains important for drawing deeper insights, News Deja Vu provides a powerful tool for exploring parallels in how people have perceived past and present.</abstract>
      <url hash="e6cf2508">2024.nlpcss-1.8</url>
      <bibkey>franklin-etal-2024-news</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.8</doi>
    </paper>
    <paper id="9">
      <title>Knowledge Distillation in Automated Annotation: Supervised Text Classification with <fixed-case>LLM</fixed-case>-Generated Training Labels</title>
      <author><first>Nicholas</first><last>Pangakis</last></author>
      <author><first>Sam</first><last>Wolken</last></author>
      <pages>113-131</pages>
      <abstract>Computational social science (CSS) practitioners often rely on human-labeled data to fine-tune supervised text classifiers. We assess the potential for researchers to augment or replace human-generated training data with surrogate training labels from generative large language models (LLMs). We introduce a recommended workflow and test this LLM application by replicating 14 classification tasks and measuring performance. We employ a novel corpus of English-language text classification data sets from recent CSS articles in high-impact journals. Because these data sets are stored in password-protected archives, our analyses are less prone to issues of contamination. For each task, we compare supervised classifiers fine-tuned using GPT-4 labels against classifiers fine-tuned with human annotations and against labels from GPT-4 and Mistral-7B with few-shot in-context learning. Our findings indicate that supervised classification models fine-tuned on LLM-generated labels perform comparably to models fine-tuned with labels from human annotators. Fine-tuning models using LLM-generated labels can be a fast, efficient and cost-effective method of building supervised text classifiers.</abstract>
      <url hash="04d89673">2024.nlpcss-1.9</url>
      <bibkey>pangakis-wolken-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.9</doi>
    </paper>
    <paper id="10">
      <title>Clustering Document Parts: Detecting and Characterizing Influence Campaigns from Documents</title>
      <author><first>Zhengxiang</first><last>Wang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Owen</first><last>Rambow</last><affiliation>Stony Brook University</affiliation></author>
      <pages>132-143</pages>
      <abstract>We propose a novel clustering pipeline to detect and characterize influence campaigns from documents. This approach clusters parts of document, detects clusters that likely reflect an influence campaign, and then identifies documents linked to an influence campaign via their association with the high-influence clusters. Our approach outperforms both the direct document-level classification and the direct document-level clustering approach in predicting if a document is part of an influence campaign. We propose various novel techniques to enhance our pipeline, including using an existing event factuality prediction system to obtain document parts, and aggregating multiple clustering experiments to improve the performance of both cluster and document classification. Classifying documents after clustering not only accurately extracts the parts of the documents that are relevant to influence campaigns, but also captures influence campaigns as a coordinated and holistic phenomenon. Our approach makes possible more fine-grained and interpretable characterizations of influence campaigns from documents.</abstract>
      <url hash="04c34888">2024.nlpcss-1.10</url>
      <bibkey>wang-rambow-2024-clustering</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.10</doi>
    </paper>
    <paper id="11">
      <title>A First Step towards Measuring Interdisciplinary Engagement in Scientific Publications: A Case Study on <fixed-case>NLP</fixed-case> + <fixed-case>CSS</fixed-case> Research</title>
      <author><first>Alexandria</first><last>Leto</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Shamik</first><last>Roy</last><affiliation>Amazon</affiliation></author>
      <author><first>Alexander</first><last>Hoyle</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Daniel</first><last>Acuna</last><affiliation>Computer Science Department, University of Colorado at Boulder</affiliation></author>
      <author><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <pages>144-158</pages>
      <abstract>With the rise in the prevalence of cross-disciplinary research, there is a need to develop methods to characterize its practices. Current computational methods to evaluate interdisciplinary engagement—such as affiliation diversity, keywords, and citation patterns—are insufficient to model the degree of engagement between disciplines, as well as the way in which the complementary expertise of co-authors is harnessed. In this paper, we propose an automated framework to address some of these issues on a large scale. Our framework tracks interdisciplinary citations in scientific articles and models: 1) the section and position in which they appear, and 2) the argumentative role that they play in the writing. To showcase our framework, we perform a preliminary analysis of interdisciplinary engagement in published work at the intersection of natural language processing and computational social science in the last decade.</abstract>
      <url hash="fd05b653">2024.nlpcss-1.11</url>
      <bibkey>leto-etal-2024-first</bibkey>
      <doi>10.18653/v1/2024.nlpcss-1.11</doi>
    </paper>
  </volume>
</collection>
