<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.textgraphs">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15)</booktitle>
      <editor><first>Alexander</first><last>Panchenko</last></editor>
      <editor><first>Fragkiskos D.</first><last>Malliaros</last></editor>
      <editor><first>Varvara</first><last>Logacheva</last></editor>
      <editor><first>Abhik</first><last>Jana</last></editor>
      <editor><first>Dmitry</first><last>Ustalov</last></editor>
      <editor><first>Peter</first><last>Jansen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.textgraphs-1</url>
      <venue>textgraphs</venue>
    </meta>
    <frontmatter>
      <url hash="eeb149e0">2021.textgraphs-1.0</url>
      <bibkey>textgraphs-2021-graph</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Bootstrapping Large-Scale Fine-Grained Contextual Advertising Classifier from <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Yiping</first><last>Jin</last></author>
      <author><first>Vishakha</first><last>Kadam</last></author>
      <author><first>Dittaya</first><last>Wanvarie</last></author>
      <pages>1–9</pages>
      <abstract>Contextual advertising provides advertisers with the opportunity to target the context which is most relevant to their ads. The large variety of potential topics makes it very challenging to collect training documents to build a supervised classification model or compose expert-written rules in a rule-based classification system. Besides, in fine-grained classification, different categories often overlap or co-occur, making it harder to classify accurately. In this work, we propose wiki2cat, a method to tackle large-scaled fine-grained text classification by tapping on the Wikipedia category graph. The categories in the IAB taxonomy are first mapped to category nodes in the graph. Then the label is propagated across the graph to obtain a list of labeled Wikipedia documents to induce text classifiers. The method is ideal for large-scale classification problems since it does not require any manually-labeled document or hand-curated rules or keywords. The proposed method is benchmarked with various learning-based and keyword-based baselines and yields competitive performance on publicly available datasets and a new dataset containing more than 300 fine-grained categories.</abstract>
      <url hash="d430a08f">2021.textgraphs-1.1</url>
      <bibkey>jin-etal-2021-bootstrapping</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.1</doi>
      <pwccode url="https://github.com/YipingNUS/contextual-eval-dataset" additional="false">YipingNUS/contextual-eval-dataset</pwccode>
    </paper>
    <paper id="2">
      <title>Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs</title>
      <author><first>Martin</first><last>Schmitt</last></author>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last></author>
      <author><first>Philipp</first><last>Dufter</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>10–21</pages>
      <abstract>We present Graformer, a novel Transformer-based encoder-decoder architecture for graph-to-text generation. With our novel graph self-attention, the encoding of a node relies on all nodes in the input graph - not only direct neighbors - facilitating the detection of global patterns. We represent the relation between two nodes as the length of the shortest path between them. Graformer learns to weight these node-node relations differently for different attention heads, thus virtually learning differently connected views of the input graph. We evaluate Graformer on two popular graph-to-text generation benchmarks, AGENDA and WebNLG, where it achieves strong performance while using many fewer parameters than other approaches.</abstract>
      <url hash="42368bbb">2021.textgraphs-1.2</url>
      <bibkey>schmitt-etal-2021-modeling</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.2</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/agenda">AGENDA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="3">
      <title>Entity Prediction in Knowledge Graphs with Joint Embeddings</title>
      <author><first>Matthias</first><last>Baumgartner</last></author>
      <author><first>Daniele</first><last>Dell’Aglio</last></author>
      <author><first>Abraham</first><last>Bernstein</last></author>
      <pages>22–31</pages>
      <abstract>Knowledge Graphs (KGs) have become increasingly popular in the recent years. However, as knowledge constantly grows and changes, it is inevitable to extend existing KGs with entities that emerged or became relevant to the scope of the KG after its creation. Research on updating KGs typically relies on extracting named entities and relations from text. However, these approaches cannot infer entities or relations that were not explicitly stated. Alternatively, embedding models exploit implicit structural regularities to predict missing relations, but cannot predict missing entities. In this article, we introduce a novel method to enrich a KG with new entities given their textual description. Our method leverages joint embedding models, hence does not require entities or relations to be named explicitly. We show that our approach can identify new concepts in a document corpus and transfer them into the KG, and we find that the performance of our method improves substantially when extended with techniques from association rule mining, text mining, and active learning.</abstract>
      <url hash="698c6346">2021.textgraphs-1.3</url>
      <bibkey>baumgartner-etal-2021-entity</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="4">
      <title>Hierarchical Graph Convolutional Networks for Jointly Resolving Cross-document Coreference of Entity and Event Mentions</title>
      <author><first>Duy</first><last>Phung</last></author>
      <author><first>Tuan Ngo</first><last>Nguyen</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>32–41</pages>
      <abstract>This paper studies the problem of cross-document event coreference resolution (CDECR) that seeks to determine if event mentions across multiple documents refer to the same real-world events. Prior work has demonstrated the benefits of the predicate-argument information and document context for resolving the coreference of event mentions. However, such information has not been captured effectively in prior work for CDECR. To address these limitations, we propose a novel deep learning model for CDECR that introduces hierarchical graph convolutional neural networks (GCN) to jointly resolve entity and event mentions. As such, sentence-level GCNs enable the encoding of important context words for event mentions and their arguments while the document-level GCN leverages the interaction structures of event mentions and arguments to compute document representations to perform CDECR. Extensive experiments are conducted to demonstrate the effectiveness of the proposed model.</abstract>
      <url hash="efc21a64">2021.textgraphs-1.4</url>
      <attachment type="OptionalSupplementaryMaterial" hash="b5c92db6">2021.textgraphs-1.4.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>phung-etal-2021-hierarchical</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.4</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>GENE</fixed-case>: Global Event Network Embedding</title>
      <author><first>Qi</first><last>Zeng</last></author>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Hanghang</first><last>Tong</last></author>
      <pages>42–53</pages>
      <abstract>Current methods for event representation ignore related events in a corpus-level global context. For a deep and comprehensive understanding of complex events, we introduce a new task, Event Network Embedding, which aims to represent events by capturing the connections among events. We propose a novel framework, Global Event Network Embedding (GENE), that encodes the event network with a multi-view graph encoder while preserving the graph topology and node semantics. The graph encoder is trained by minimizing both structural and semantic losses. We develop a new series of structured probing tasks, and show that our approach effectively outperforms baseline models on node typing, argument role classification, and event coreference resolution.</abstract>
      <url hash="5b7306ce">2021.textgraphs-1.5</url>
      <bibkey>zeng-etal-2021-gene</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.5</doi>
      <pwccode url="https://github.com/pkuzengqi/gene" additional="false">pkuzengqi/gene</pwccode>
    </paper>
    <paper id="6">
      <title>Learning Clause Representation from Dependency-Anchor Graph for Connective Prediction</title>
      <author><first>Yanjun</first><last>Gao</last></author>
      <author><first>Ting-Hao</first><last>Huang</last></author>
      <author><first>Rebecca J.</first><last>Passonneau</last></author>
      <pages>54–66</pages>
      <abstract>Semantic representation that supports the choice of an appropriate connective between pairs of clauses inherently addresses discourse coherence, which is important for tasks such as narrative understanding, argumentation, and discourse parsing. We propose a novel clause embedding method that applies graph learning to a data structure we refer to as a dependency-anchor graph. The dependency anchor graph incorporates two kinds of syntactic information, constituency structure, and dependency relations, to highlight the subject and verb phrase relation. This enhances coherence-related aspects of representation. We design a neural model to learn a semantic representation for clauses from graph convolution over latent representations of the subject and verb phrase. We evaluate our method on two new datasets: a subset of a large corpus where the source texts are published novels, and a new dataset collected from students’ essays. The results demonstrate a significant improvement over tree-based models, confirming the importance of emphasizing the subject and verb phrase. The performance gap between the two datasets illustrates the challenges of analyzing student’s written text, plus a potential evaluation task for coherence modeling and an application for suggesting revisions to students.</abstract>
      <url hash="e80e22d5">2021.textgraphs-1.6</url>
      <bibkey>gao-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.6</doi>
      <pwccode url="https://github.com/serenayj/DeSSE" additional="false">serenayj/DeSSE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>W</fixed-case>iki<fixed-case>G</fixed-case>raphs: A <fixed-case>W</fixed-case>ikipedia Text - Knowledge Graph Paired Dataset</title>
      <author><first>Luyu</first><last>Wang</last></author>
      <author><first>Yujia</first><last>Li</last></author>
      <author><first>Ozlem</first><last>Aslan</last></author>
      <author><first>Oriol</first><last>Vinyals</last></author>
      <pages>67–82</pages>
      <abstract>We present a new dataset of Wikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning. Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data. Our new dataset WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark (Merity et al., 2016) with a subgraph from the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy to benchmark against other state-of-the-art text generative models that are capable of generating long paragraphs of coherent text. Both the graphs and the text data are of significantly larger scale compared to prior graph-text paired datasets. We present baseline graph neural network and transformer model results on our dataset for 3 tasks: graph -&gt; text generation, graph -&gt; text retrieval and text -&gt; graph retrieval. We show that better conditioning on the graph provides gains in generation and retrieval quality but there is still large room for improvement.</abstract>
      <url hash="09e9ab34">2021.textgraphs-1.7</url>
      <bibkey>wang-etal-2021-wikigraphs</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.7</doi>
      <pwccode url="https://github.com/deepmind/deepmind-research/tree/master/wikigraphs" additional="false">deepmind/deepmind-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikigraphs">WikiGraphs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/genwiki">GenWiki</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="8">
      <title>Selective Attention Based Graph Convolutional Networks for Aspect-Level Sentiment Classification</title>
      <author><first>Xiaochen</first><last>Hou</last></author>
      <author><first>Jing</first><last>Huang</last></author>
      <author><first>Guangtao</first><last>Wang</last></author>
      <author><first>Peng</first><last>Qi</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Bowen</first><last>Zhou</last></author>
      <pages>83–93</pages>
      <abstract>Recent work on aspect-level sentiment classification has employed Graph Convolutional Networks (GCN) over dependency trees to learn interactions between aspect terms and opinion words. In some cases, the corresponding opinion words for an aspect term cannot be reached within two hops on dependency trees, which requires more GCN layers to model. However, GCNs often achieve the best performance with two layers, and deeper GCNs do not bring any additional gain. Therefore, we design a novel selective attention based GCN model. On one hand, the proposed model enables the direct interaction between aspect terms and context words via the self-attention operation without the distance limitation on dependency trees. On the other hand, a top-k selection procedure is designed to locate opinion words by selecting k context words with the highest attention scores. We conduct experiments on several commonly used benchmark datasets and the results show that our proposed SA-GCN outperforms strong baseline models.</abstract>
      <url hash="bfa7b811">2021.textgraphs-1.8</url>
      <bibkey>hou-etal-2021-selective</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.8</doi>
    </paper>
    <paper id="9">
      <title>Keyword Extraction Using Unsupervised Learning on the Document’s Adjacency Matrix</title>
      <author><first>Eirini</first><last>Papagiannopoulou</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <author><first>Apostolos</first><last>Papadopoulos</last></author>
      <pages>94–105</pages>
      <abstract>This work revisits the information given by the graph-of-words and its typical utilization through graph-based ranking approaches in the context of keyword extraction. Recent, well-known graph-based approaches typically employ the knowledge from word vector representations during the ranking process via popular centrality measures (e.g., PageRank) without giving the primary role to vectors’ distribution. We consider the adjacency matrix that corresponds to the graph-of-words of a target text document as the vector representation of its vocabulary. We propose the distribution-based modeling of this adjacency matrix using unsupervised (learning) algorithms. The efficacy of the distribution-based modeling approaches compared to state-of-the-art graph-based methods is confirmed by an extensive experimental study according to the F1 score. Our code is available on GitHub.</abstract>
      <url hash="8dfa8412">2021.textgraphs-1.9</url>
      <bibkey>papagiannopoulou-etal-2021-keyword</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.9</doi>
    </paper>
    <paper id="10">
      <title>Improving Human Text Simplification with Sentence Fusion</title>
      <author><first>Max</first><last>Schwarzer</last></author>
      <author><first>Teerapaun</first><last>Tanprasert</last></author>
      <author><first>David</first><last>Kauchak</last></author>
      <pages>106–114</pages>
      <abstract>The quality of fully automated text simplification systems is not good enough for use in real-world settings; instead, human simplifications are used. In this paper, we examine how to improve the cost and quality of human simplifications by leveraging crowdsourcing. We introduce a graph-based sentence fusion approach to augment human simplifications and a reranking approach to both select high quality simplifications and to allow for targeting simplifications with varying levels of simplicity. Using the Newsela dataset (Xu et al., 2015) we show consistent improvements over experts at varying simplification levels and find that the additional sentence fusion simplifications allow for simpler output than the human simplifications alone.</abstract>
      <url hash="8929c272">2021.textgraphs-1.10</url>
      <bibkey>schwarzer-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.10</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
    </paper>
    <paper id="11">
      <title>Structural Realization with <fixed-case>GGNN</fixed-case>s</title>
      <author><first>Jinman</first><last>Zhao</last></author>
      <author><first>Gerald</first><last>Penn</last></author>
      <author><first>Huan</first><last>Ling</last></author>
      <pages>115–124</pages>
      <abstract>In this paper, we define an abstract task called structural realization that generates words given a prefix of words and a partial representation of a parse tree. We also present a method for solving instances of this task using a Gated Graph Neural Network (GGNN). We evaluate it with standard accuracy measures, as well as with respect to perplexity, in which its comparison to previous work on language modelling serves to quantify the information added to a lexical selection task by the presence of syntactic knowledge. That the addition of parse-tree-internal nodes to this neural model should improve the model, with respect both to accuracy and to more conventional measures such as perplexity, may seem unsurprising, but previous attempts have not met with nearly as much success. We have also learned that transverse links through the parse tree compromise the model’s accuracy at generating adjectival and nominal parts of speech.</abstract>
      <url hash="912375f9">2021.textgraphs-1.11</url>
      <bibkey>zhao-etal-2021-structural</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.11</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>MG</fixed-case>-<fixed-case>BERT</fixed-case>: Multi-Graph Augmented <fixed-case>BERT</fixed-case> for Masked Language Modeling</title>
      <author><first>Parishad</first><last>BehnamGhader</last></author>
      <author><first>Hossein</first><last>Zakerinia</last></author>
      <author><first>Mahdieh</first><last>Soleymani Baghshah</last></author>
      <pages>125–131</pages>
      <abstract>Pre-trained models like Bidirectional Encoder Representations from Transformers (BERT), have recently made a big leap forward in Natural Language Processing (NLP) tasks. However, there are still some shortcomings in the Masked Language Modeling (MLM) task performed by these models. In this paper, we first introduce a multi-graph including different types of relations between words. Then, we propose Multi-Graph augmented BERT (MG-BERT) model that is based on BERT. MG-BERT embeds tokens while taking advantage of a static multi-graph containing global word co-occurrences in the text corpus beside global real-world facts about words in knowledge graphs. The proposed model also employs a dynamic sentence graph to capture local context effectively. Experimental results demonstrate that our model can considerably enhance the performance in the MLM task.</abstract>
      <url hash="f5bd88eb">2021.textgraphs-1.12</url>
      <attachment type="OptionalSupplementaryMaterial" hash="0852fc0c">2021.textgraphs-1.12.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>behnamghader-etal-2021-mg</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.12</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>GTN</fixed-case>-<fixed-case>ED</fixed-case>: Event Detection Using Graph Transformer Networks</title>
      <author><first>Sanghamitra</first><last>Dutta</last></author>
      <author><first>Liang</first><last>Ma</last></author>
      <author><first>Tanay Kumar</first><last>Saha</last></author>
      <author><first>Di</first><last>Liu</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes</last></author>
      <pages>132–137</pages>
      <abstract>Recent works show that the graph structure of sentences, generated from dependency parsers, has potential for improving event detection. However, they often only leverage the edges (dependencies) between words, and discard the dependency labels (e.g., nominal-subject), treating the underlying graph edges as homogeneous. In this work, we propose a novel framework for incorporating both dependencies and their labels using a recently proposed technique called Graph Transformer Network (GTN). We integrate GTN to leverage dependency relations on two existing homogeneous-graph-based models and demonstrate an improvement in the F1 score on the ACE dataset.</abstract>
      <url hash="1a0a04b2">2021.textgraphs-1.13</url>
      <bibkey>dutta-etal-2021-gtn</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.13</doi>
    </paper>
    <paper id="14">
      <title>Fine-grained General Entity Typing in <fixed-case>G</fixed-case>erman using <fixed-case>G</fixed-case>erma<fixed-case>N</fixed-case>et</title>
      <author><first>Sabine</first><last>Weber</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>138–143</pages>
      <abstract>Fine-grained entity typing is important to tasks like relation extraction and knowledge base construction. We find however, that fine-grained entity typing systems perform poorly on general entities (e.g. “ex-president”) as compared to named entities (e.g. “Barack Obama”). This is due to a lack of general entities in existing training data sets. We show that this problem can be mitigated by automatically generating training data from WordNets. We use a German WordNet equivalent, GermaNet, to automatically generate training data for German general entity typing. We use this data to supplement named entity data to train a neural fine-grained entity typing system. This leads to a 10% improvement in accuracy of the prediction of level 1 FIGER types for German general entities, while decreasing named entity type prediction accuracy by only 1%.</abstract>
      <url hash="5b8372b0">2021.textgraphs-1.14</url>
      <bibkey>weber-steedman-2021-fine</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.14</doi>
      <pwccode url="https://github.com/webersab/german_general_entity_typing" additional="false">webersab/german_general_entity_typing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="15">
      <title>On Geodesic Distances and Contextual Embedding Compression for Text Classification</title>
      <author><first>Rishi</first><last>Jha</last></author>
      <author><first>Kai</first><last>Mihata</last></author>
      <pages>144–149</pages>
      <abstract>In some memory-constrained settings like IoT devices and over-the-network data pipelines, it can be advantageous to have smaller contextual embeddings. We investigate the efficacy of projecting contextual embedding data (BERT) onto a manifold, and using nonlinear dimensionality reduction techniques to compress these embeddings. In particular, we propose a novel post-processing approach, applying a combination of Isomap and PCA. We find that the geodesic distance estimations, estimates of the shortest path on a Riemannian manifold, from Isomap’s k-Nearest Neighbors graph bolstered the performance of the compressed embeddings to be comparable to the original BERT embeddings. On one dataset, we find that despite a 12-fold dimensionality reduction, the compressed embeddings performed within 0.1% of the original BERT embeddings on a downstream classification task. In addition, we find that this approach works particularly well on tasks reliant on syntactic data, when compared with linear dimensionality reduction. These results show promise for a novel geometric approach to achieve lower dimensional text embeddings from existing transformers and pave the way for data-specific and application-specific embedding compressions.</abstract>
      <url hash="5f821887">2021.textgraphs-1.15</url>
      <bibkey>jha-mihata-2021-geodesic</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.15</doi>
      <pwccode url="https://github.com/kaimihata/geo-bert" additional="false">kaimihata/geo-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="16">
      <title>Semi-Supervised Joint Estimation of Word and Document Readability</title>
      <author><first>Yoshinari</first><last>Fujinuma</last></author>
      <author><first>Masato</first><last>Hagiwara</last></author>
      <pages>150–155</pages>
      <abstract>Readability or difficulty estimation of words and documents has been investigated independently in the literature, often assuming the existence of extensive annotated resources for the other. Motivated by our analysis showing that there is a recursive relationship between word and document difficulty, we propose to jointly estimate word and document difficulty through a graph convolutional network (GCN) in a semi-supervised fashion. Our experimental results reveal that the GCN-based method can achieve higher accuracy than strong baselines, and stays robust even with a smaller amount of labeled data.</abstract>
      <url hash="919b443f">2021.textgraphs-1.16</url>
      <bibkey>fujinuma-hagiwara-2021-semi</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.16</doi>
      <pwccode url="https://github.com/akkikiki/diff_joint_estimate" additional="false">akkikiki/diff_joint_estimate</pwccode>
    </paper>
    <paper id="17">
      <title><fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration</title>
      <author><first>Mokanarangan</first><last>Thayaparan</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Peter</first><last>Jansen</last></author>
      <author><first>Dmitry</first><last>Ustalov</last></author>
      <pages>156–165</pages>
      <abstract>The Shared Task on Multi-Hop Inference for Explanation Regeneration asks participants to compose large multi-hop explanations to questions by assembling large chains of facts from a supporting knowledge base. While previous editions of this shared task aimed to evaluate explanatory completeness – finding a set of facts that form a complete inference chain, without gaps, to arrive from question to correct answer, this 2021 instantiation concentrates on the subtask of determining relevance in large multi-hop explanations. To this end, this edition of the shared task makes use of a large set of approximately 250k manual explanatory relevancy ratings that augment the 2020 shared task data. In this summary paper, we describe the details of the explanation regeneration task, the evaluation data, and the participating systems. Additionally, we perform a detailed analysis of participating systems, evaluating various aspects involved in the multi-hop inference process. The best performing system achieved an NDCG of 0.82 on this challenging task, substantially increasing performance over baseline methods by 32%, while also leaving significant room for future improvement.</abstract>
      <url hash="3b22be0b">2021.textgraphs-1.17</url>
      <bibkey>thayaparan-etal-2021-textgraphs</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.17</doi>
      <pwccode url="https://github.com/cognitiveailab/tg2021task" additional="false">cognitiveailab/tg2021task</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/worldtree">Worldtree</pwcdataset>
    </paper>
    <paper id="18">
      <title><fixed-case>D</fixed-case>eep<fixed-case>B</fixed-case>lue<fixed-case>AI</fixed-case> at <fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>raphs 2021 Shared Task: Treating Multi-Hop Inference Explanation Regeneration as A Ranking Problem</title>
      <author><first>Chunguang</first><last>Pan</last></author>
      <author><first>Bingyan</first><last>Song</last></author>
      <author><first>Zhipeng</first><last>Luo</last></author>
      <pages>166–170</pages>
      <abstract>This paper describes the winning system for TextGraphs 2021 shared task: Multi-hop inference explanation regeneration. Given a question and its corresponding correct answer, this task aims to select the facts that can explain why the answer is correct for that question and answering (QA) from a large knowledge base. To address this problem and accelerate training as well, our strategy includes two steps. First, fine-tuning pre-trained language models (PLMs) with triplet loss to recall top-K relevant facts for each question and answer pair. Then, adopting the same architecture to train the re-ranking model to rank the top-K candidates. To further improve the performance, we average the results from models based on different PLMs (e.g., RoBERTa) and different parameter settings to make the final predictions. The official evaluation shows that, our system can outperform the second best system by 4.93 points, which proves the effectiveness of our system. Our code has been open source, address is https://github.com/DeepBlueAI/TextGraphs-15</abstract>
      <url hash="51fcd941">2021.textgraphs-1.18</url>
      <bibkey>pan-etal-2021-deepblueai</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.18</doi>
      <pwccode url="https://github.com/deepblueai/textgraphs-15" additional="false">deepblueai/textgraphs-15</pwccode>
    </paper>
    <paper id="19">
      <title>A Three-step Method for Multi-Hop Inference Explanation Regeneration</title>
      <author><first>Yuejia</first><last>Xiang</last></author>
      <author><first>Yunyan</first><last>Zhang</last></author>
      <author><first>Xiaoming</first><last>Shi</last></author>
      <author><first>Bo</first><last>Liu</last></author>
      <author><first>Wandi</first><last>Xu</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <pages>171–175</pages>
      <abstract>Multi-hop inference for explanation generation is to combine two or more facts to make an inference. The task focuses on generating explanations for elementary science questions. In the task, the relevance between the explanations and the QA pairs is of vital importance. To address the task, a three-step framework is proposed. Firstly, vector distance between two texts is utilized to recall the top-K relevant explanations for each question, reducing the calculation consumption. Then, a selection module is employed to choose those most relative facts in an autoregressive manner, giving a preliminary order for the retrieved facts. Thirdly, we adopt a re-ranking module to re-rank the retrieved candidate explanations with relevance between each fact and the QA pairs. Experimental results illustrate the effectiveness of the proposed framework with an improvement of 39.78% in NDCG over the official baseline.</abstract>
      <url hash="43d165bb">2021.textgraphs-1.19</url>
      <bibkey>xiang-etal-2021-three</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.19</doi>
    </paper>
    <paper id="20">
      <title>Textgraphs-15 Shared Task System Description : Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings</title>
      <author><first>Sureshkumar</first><last>Vivek Kalyan</last></author>
      <author><first>Sam</first><last>Witteveen</last></author>
      <author><first>Martin</first><last>Andrews</last></author>
      <pages>176–180</pages>
      <abstract>Creating explanations for answers to science questions is a challenging task that requires multi-hop inference over a large set of fact sentences. This year, to refocus the Textgraphs Shared Task on the problem of gathering relevant statements (rather than solely finding a single ‘correct path’), the WorldTree dataset was augmented with expert ratings of ‘relevance’ of statements to each overall explanation. Our system, which achieved second place on the Shared Task leaderboard, combines initial statement retrieval; language models trained to predict the relevance scores; and ensembling of a number of the resulting rankings. Our code implementation is made available at https://github.com/mdda/worldtree_corpus/tree/textgraphs_2021</abstract>
      <url hash="b5b07332">2021.textgraphs-1.20</url>
      <bibkey>vivek-kalyan-etal-2021-textgraphs</bibkey>
      <doi>10.18653/v1/2021.textgraphs-1.20</doi>
      <pwccode url="https://github.com/mdda/worldtree_corpus" additional="false">mdda/worldtree_corpus</pwccode>
    </paper>
  </volume>
</collection>
