<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.bsnlp">
  <volume id="1" ingest-date="2023-04-29">
    <meta>
      <booktitle>Proceedings of the 9th Workshop on Slavic Natural Language Processing 2023 (SlavicNLP 2023)</booktitle>
      <editor><first>Jakub</first><last>Piskorski</last></editor>
      <editor><first>Michał</first><last>Marcińczuk</last></editor>
      <editor><first>Preslav</first><last>Nakov</last></editor>
      <editor><first>Maciej</first><last>Ogrodniczuk</last></editor>
      <editor><first>Senja</first><last>Pollak</last></editor>
      <editor><first>Pavel</first><last>Přibáň</last></editor>
      <editor><first>Piotr</first><last>Rybak</last></editor>
      <editor><first>Josef</first><last>Steinberger</last></editor>
      <editor><first>Roman</first><last>Yangarber</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dubrovnik, Croatia</address>
      <month>May</month>
      <year>2023</year>
      <url hash="96919c9d">2023.bsnlp-1</url>
      <venue>bsnlp</venue>
    </meta>
    <frontmatter>
      <url hash="dfa760ab">2023.bsnlp-1.0</url>
      <bibkey>bsnlp-2023-slavic</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Named Entity Recognition for Low-Resource Languages - Profiting from Language Families</title>
      <author><first>Sunna</first><last>Torge</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Andrei</first><last>Politov</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Christoph</first><last>Lehmann</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Bochra</first><last>Saffar</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Ziyan</first><last>Tao</last><affiliation>Technische Universität Dresden</affiliation></author>
      <pages>1-10</pages>
      <abstract>Machine learning drives forward the development in many areas of Natural Language Processing (NLP). Until now, many NLP systems and research are focusing on high-resource languages, i.e. languages for which many data resources exist. Recently, so-called low-resource languages increasingly come into focus. In this context, multi-lingual language models, which are trained on related languages to a target low-resource language, may enable NLP tasks on this low-resource language. In this work, we investigate the use of multi-lingual models for Named Entity Recognition (NER) for low-resource languages. We consider the West Slavic language family and the low-resource languages Upper Sorbian and Kashubian. Three RoBERTa models were trained from scratch, two mono-lingual models for Czech and Polish, and one bi-lingual model for Czech and Polish. These models were evaluated on the NER downstream task for Czech, Polish, Upper Sorbian, and Kashubian, and compared to existing state-of-the-art models such as RobeCzech, HerBERT, and XLM-R. The results indicate that the mono-lingual models perform better on the language they were trained on, and both the mono-lingual and language family models outperform the large multi-lingual model in downstream tasks. Overall, the study shows that low-resource West Slavic languages can benefit from closely related languages and their models.</abstract>
      <url hash="82ba9539">2023.bsnlp-1.1</url>
      <bibkey>torge-etal-2023-named</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>MAUPQA</fixed-case>: Massive Automatically-created <fixed-case>P</fixed-case>olish Question Answering Dataset</title>
      <author><first>Piotr</first><last>Rybak</last><affiliation>Institute of Computer Science, Polish Academy of Sciences</affiliation></author>
      <pages>11-16</pages>
      <abstract>Recently, open-domain question answering systems have begun to rely heavily on annotated datasets to train neural passage retrievers. However, manually annotating such datasets is both difficult and time-consuming, which limits their availability for less popular languages. In this work, we experiment with several methods for automatically collecting weakly labeled datasets and show how they affect the performance of the neural passage retrieval models. As a result of our work, we publish the MAUPQA dataset, consisting of nearly 400,000 question-passage pairs for Polish, as well as the HerBERT-QA neural retriever.</abstract>
      <url hash="afd0f0a6">2023.bsnlp-1.2</url>
      <bibkey>rybak-2023-maupqa</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>T</fixed-case>rel<fixed-case>BERT</fixed-case>: A pre-trained encoder for <fixed-case>P</fixed-case>olish <fixed-case>T</fixed-case>witter</title>
      <author><first>Wojciech</first><last>Szmyd</last><affiliation>Deepsense.ai</affiliation></author>
      <author><first>Alicja</first><last>Kotyla</last><affiliation>Deepsense.ai</affiliation></author>
      <author><first>Michał</first><last>Zobniów</last><affiliation>Deepsense.ai</affiliation></author>
      <author><first>Piotr</first><last>Falkiewicz</last><affiliation>Deepsense.ai</affiliation></author>
      <author><first>Jakub</first><last>Bartczuk</last><affiliation>MedSI</affiliation></author>
      <author><first>Artur</first><last>Zygadło</last><affiliation>Deepsense.ai</affiliation></author>
      <pages>17-24</pages>
      <abstract>Pre-trained Transformer-based models have become immensely popular amongst NLP practitioners. We present TrelBERT – the first Polish language model suited for application in the social media domain. TrelBERT is based on an existing general-domain model and adapted to the language of social media by pre-training it further on a large collection of Twitter data. We demonstrate its usefulness by evaluating it in the downstream task of cyberbullying detection, in which it achieves state-of-the-art results, outperforming larger monolingual models trained on general-domain corpora, as well as multilingual in-domain models, by a large margin. We make the model publicly available. We also release a new dataset for the problem of harmful speech detection.</abstract>
      <url hash="0505d23c">2023.bsnlp-1.3</url>
      <bibkey>szmyd-etal-2023-trelbert</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>C</fixed-case>roatian Film Review Dataset (Cro-<fixed-case>F</fixed-case>i<fixed-case>R</fixed-case>e<fixed-case>D</fixed-case>a): A Sentiment Annotated Dataset of Film Reviews</title>
      <author><first>Gaurish</first><last>Thakkar</last><affiliation>University of Zagreb</affiliation></author>
      <author><first>Nives</first><last>Mikelic Preradovic</last><affiliation>University of Zagreb</affiliation></author>
      <author><first>Marko</first><last>Tadić</last><affiliation>University of Zagreb, Faculty of Humanities and Social Sciences</affiliation></author>
      <pages>25-31</pages>
      <abstract>This paper introduces Cro-FiReDa, a sentiment-annotated dataset for Croatian in the domain of movie reviews. The dataset, which contains over 10,000 sentences, has been annotated at the sentence level. In addition to presentingthe overall annotation process, we also present benchmark results based on the transformer-based fine-tuning approach.</abstract>
      <url hash="4ad70184">2023.bsnlp-1.4</url>
      <bibkey>thakkar-etal-2023-croatian</bibkey>
    </paper>
    <paper id="5">
      <title>Too Many Cooks Spoil the Model: Are Bilingual Models for <fixed-case>S</fixed-case>lovene Better than a Large Multilingual Model?</title>
      <author><first>Pranaydeep</first><last>Singh</last><affiliation>LT3, University of Ghent</affiliation></author>
      <author><first>Aaron</first><last>Maladry</last><affiliation>Ghent University</affiliation></author>
      <author><first>Els</first><last>Lefever</last><affiliation>LT3, Ghent University</affiliation></author>
      <pages>32-39</pages>
      <abstract>This paper investigates whether adding data of typologically closer languages improves the performance of transformer-based models for three different downstream tasks, namely Part-of-Speech tagging, Named Entity Recognition, and Sentiment Analysis, compared to a monolingual and plain multilingual language model. For the presented pilot study, we performed experiments for the use case of Slovene, a low(er)-resourced language belonging to the Slavic language family. The experiments were carried out in a controlled setting, where a monolingual model for Slovene was compared to combined language models containing Slovene, trained with the same amount of Slovene data. The experimental results show that adding typologically closer languages indeed improves the performance of the Slovene language model, and even succeeds in outperforming the large multilingual XLM-RoBERTa model for NER and PoS-tagging. We also reveal that, contrary to intuition, distantly or unrelated languages also combine admirably with Slovene, often out-performing XLM-R as well. All the bilingual models used in the experiments are publicly available at https://github.com/pranaydeeps/BLAIR</abstract>
      <url hash="b506548d">2023.bsnlp-1.5</url>
      <bibkey>singh-etal-2023-many</bibkey>
    </paper>
    <paper id="6">
      <title>Machine-translated texts from <fixed-case>E</fixed-case>nglish to <fixed-case>P</fixed-case>olish show a potential for typological explanations in Source Language Identification</title>
      <author><first>Damiaan</first><last>Reijnaers</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Elize</first><last>Herrewijnen</last><affiliation>Utrecht University</affiliation></author>
      <pages>40-46</pages>
      <abstract>This work examines a case study that investigates (1) the achievability of extracting typological features from Polish texts, and (2) their contrastive power to discriminate between machine-translated texts from English. The findings indicate potential for a proposed method that deals with the explainable prediction of the source language of translated texts.</abstract>
      <url hash="ad4fd7bf">2023.bsnlp-1.6</url>
      <bibkey>reijnaers-herrewijnen-2023-machine</bibkey>
    </paper>
    <paper id="7">
      <title>Comparing domain-specific and domain-general <fixed-case>BERT</fixed-case> variants for inferred real-world knowledge through rare grammatical features in <fixed-case>S</fixed-case>erbian</title>
      <author><first>Sofia</first><last>Lee</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <author><first>Jelke</first><last>Bloem</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>47-60</pages>
      <abstract>Transfer learning is one of the prevailing approaches towards training language-specific BERT models. However, some languages have uncommon features that may prove to be challenging to more domain-general models but not domain-specific models. Comparing the performance of BERTić, a Bosnian-Croatian-Montenegrin-Serbian model, and Multilingual BERT on a Named-Entity Recognition (NER) task and Masked Language Modelling (MLM) task based around a rare phenomenon of indeclinable female foreign names in Serbian reveals how the different training approaches impacts their performance. Multilingual BERT is shown to perform better than BERTić in the NER task, but BERTić greatly exceeds in the MLM task. Thus, there are applications both for domain-general training and domain-specific training depending on the tasks at hand.</abstract>
      <url hash="71209746">2023.bsnlp-1.7</url>
      <bibkey>lee-bloem-2023-comparing</bibkey>
    </paper>
    <paper id="8">
      <title>Dispersing the clouds of doubt: can cosine similarity of word embeddings help identify relation-level metaphors in <fixed-case>S</fixed-case>lovene?</title>
      <author><first>Mojca</first><last>Brglez</last><affiliation>Faculty of Arts, University of Ljubljana</affiliation></author>
      <pages>61-69</pages>
      <abstract>Word embeddings and pre-trained language models have achieved great performance in many tasks due to their ability to capture both syntactic and semantic information in their representations. The vector space representations have also been used to identify figurative language shifts such as metaphors, however, the more recent contextualized models have mostly been evaluated via their performance on downstream tasks.In this article, we evaluate static and contextualized word embeddings in terms of their representation and unsupervised identification of relation-level (ADJ-NOUN, NOUN-NOUN) metaphors in Slovene on a set of 24 literal and 24 metaphorical phrases. Our experiments show very promising results for both embedding methods, however, the performance in contextual embeddings notably depends on the layer involved and the input provided to the model.</abstract>
      <url hash="3f6d0509">2023.bsnlp-1.8</url>
      <bibkey>brglez-2023-dispersing</bibkey>
    </paper>
    <paper id="9">
      <title>Automatic text simplification of <fixed-case>R</fixed-case>ussian texts using control tokens</title>
      <author><first>Anna</first><last>Dmitrieva</last><affiliation>University of Helsinki</affiliation></author>
      <pages>70-77</pages>
      <abstract>This paper describes the research on the possibilities to control automatic text simplification with special tokens that allow modifying the length, paraphrasing degree, syntactic complexity, and the CEFR (Common European Framework of Reference) grade level of the output texts, i.e. the level of language proficiency a non-native speaker would need to understand them. The project is focused on Russian texts and aims to continue and broaden the existing research on controlled Russian text simplification. It is done by exploring available datasets for monolingual Russian machine translation (paraphrasing and simplification), experimenting with various model architectures, and adding control tokens that have not been used on Russian texts previously.</abstract>
      <url hash="a58da291">2023.bsnlp-1.9</url>
      <bibkey>dmitrieva-2023-automatic</bibkey>
    </paper>
    <paper id="10">
      <title>Target Two Birds With One <fixed-case>ST</fixed-case>o<fixed-case>N</fixed-case>e: Entity-Level Sentiment and Tone Analysis in <fixed-case>C</fixed-case>roatian News Headlines</title>
      <author><first>Ana</first><last>Barić</last><affiliation>University of Zagreb, Faculty of Electrical Engineering and Computing</affiliation></author>
      <author><first>Laura</first><last>Majer</last><affiliation>University of Zagreb, Faculty of Electrical Engineering and Computing</affiliation></author>
      <author><first>David</first><last>Dukić</last><affiliation>University of Zagreb, Faculty of Electrical Engineering and Computing</affiliation></author>
      <author><first>Marijana</first><last>Grbeša-zenzerović</last><affiliation>University of Zagreb, Faculty of Political Science</affiliation></author>
      <author><first>Jan</first><last>Snajder</last><affiliation>University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, 10000 Zagreb</affiliation></author>
      <pages>78-85</pages>
      <abstract>Sentiment analysis is often used to examine how different actors are portrayed in the media, and analysis of news headlines is of particular interest due to their attention-grabbing role. We address the task of entity-level sentiment analysis from Croatian news headlines. We frame the task as targeted sentiment analysis (TSA), explicitly differentiating between sentiment toward a named entity and the overall tone of the headline. We describe SToNe, a new dataset for this task with sentiment and tone labels. We implement several neural benchmark models, utilizing single- and multi-task training, and show that TSA can benefit from tone information. Finally, we gauge the difficulty of this task by leveraging dataset cartography.</abstract>
      <url hash="12eb433f">2023.bsnlp-1.10</url>
      <bibkey>baric-etal-2023-target</bibkey>
    </paper>
    <paper id="11">
      <title>Is <fixed-case>G</fixed-case>erman secretly a <fixed-case>S</fixed-case>lavic language? What <fixed-case>BERT</fixed-case> probing can tell us about language groups</title>
      <author><first>Aleksandra</first><last>Mysiak</last><affiliation>University of Warsaw</affiliation></author>
      <author><first>Jacek</first><last>Cyranka</last><affiliation>University of Warsaw</affiliation></author>
      <pages>86-93</pages>
      <abstract>In the light of recent developments in NLP, the problem of understanding and interpreting large language models has gained a lot of urgency. Methods developed to study this area are subject to considerable scrutiny. In this work, we take a closer look at one such method, the structural probe introduced by Hewitt and Manning (2019). We run a series of experiments involving multiple languages, focusing principally on the group of Slavic languages. We show that probing results can be seen as a reflection of linguistic classification, and conclude that multilingual BERT learns facts about languages and their groups.</abstract>
      <url hash="26b126cc">2023.bsnlp-1.11</url>
      <bibkey>mysiak-cyranka-2023-german</bibkey>
    </paper>
    <paper id="12">
      <title>Resources and Few-shot Learners for In-context Learning in <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Michal</first><last>Štefánik</last><affiliation>Masaryk University</affiliation></author>
      <author><first>Marek</first><last>Kadlčík</last><affiliation>Masaryk University</affiliation></author>
      <author><first>Piotr</first><last>Gramacki</last><affiliation>Department of Artificial Intelligence, Wrocław University of Science and Technology</affiliation></author>
      <author><first>Petr</first><last>Sojka</last><affiliation>Faculty of Informatics, Masaryk University</affiliation></author>
      <pages>94-105</pages>
      <abstract>Despite the rapid recent progress in creating accurate and compact in-context learners, most recent work focuses on in-context learning (ICL) for tasks in English. However, the ability to interact with users of languages outside English presents a great potential for broadening the applicability of language technologies to non-English speakers.In this work, we collect the infrastructure necessary for training and evaluation of ICL in a selection of Slavic languages: Czech, Polish, and Russian. We link a diverse set of datasets and cast these into a unified instructional format through a set of transformations and newly-crafted templates written purely in target languages.Using the newly-curated dataset, we evaluate a set of the most recent in-context learners and compare their results to the supervised baselines. Finally, we train, evaluate and publish a set of in-context learning models that we train on the collected resources and compare their performance to previous work.We find that ICL models tuned in English are also able to learn some tasks from non-English contexts, but multilingual instruction fine-tuning consistently improves the ICL ability. We also find that the massive multitask training can be outperformed by single-task training in the target language, uncovering the potential for specializing in-context learners to the language(s) of their application.</abstract>
      <url hash="9a6b04c6">2023.bsnlp-1.12</url>
      <bibkey>stefanik-etal-2023-resources</bibkey>
    </paper>
    <paper id="13">
      <title>Analysis of Transfer Learning for Named Entity Recognition in <fixed-case>S</fixed-case>outh-<fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Nikola</first><last>Ivačič</last><affiliation>Jozef Stefan International Postgraduate School, Dropchop</affiliation></author>
      <author><first>Thi Hong Hanh</first><last>Tran</last><affiliation>La Rochelle University</affiliation></author>
      <author><first>Boshko</first><last>Koloski</last><affiliation>Jozef Stefan Institute</affiliation></author>
      <author><first>Senja</first><last>Pollak</last><affiliation>Jožef Stefan Institute</affiliation></author>
      <author><first>Matthew</first><last>Purver</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>106-112</pages>
      <abstract>This paper analyzes a Named Entity Recognition task for South-Slavic languages using the pre-trained multilingual neural network models. We investigate whether the performance of the models for a target language can be improved by using data from closely related languages. We have shown that the model performance is not influenced substantially when trained with other than a target language. While for Slovene, the monolingual setting generally performs better, for Croatian and Serbian the results are slightly better in selected cross-lingual settings, but the improvements are not large. The most significant performance improvement is shown for the Serbian language, which has the smallest corpora. Therefore, fine-tuning with other closely related languages may benefit only the “low resource” languages.</abstract>
      <url hash="a042282e">2023.bsnlp-1.13</url>
      <bibkey>ivacic-etal-2023-analysis</bibkey>
    </paper>
    <paper id="14">
      <title>Information Extraction from <fixed-case>P</fixed-case>olish Radiology Reports Using Language Models</title>
      <author><first>Aleksander</first><last>Obuchowski</last><affiliation>Gdańsk University of Technology : Faculty of Applied Physics and Mathematics</affiliation></author>
      <author><first>Barbara</first><last>Klaudel</last><affiliation>Gdańsk University of Technology : Faculty of Electronics, Telecommunications and Informatics</affiliation></author>
      <author><first>Patryk</first><last>Jasik</last><affiliation>Gdańsk University of Technology : Faculty of Applied Physics and Mathematics</affiliation></author>
      <pages>113-122</pages>
      <abstract>Radiology reports are vital elements of directing patient care. They are usually delivered in free text form, which makes them prone to errors, such as omission in reporting radiological findings and using difficult-to-comprehend mental shortcuts. Although structured reporting is the recommended method, its adoption continues to be limited. Radiologists find structured reports too limiting and burdensome. In this paper, we propose the model, which is meant to preserve the benefits of free text, while moving towards a structured report. The model automatically parametrizes Polish radiology reports based on language models. The models were trained on a large dataset of 1200 chest computed tomography (CT) reports annotated by multiple medical experts reports with 44 observation tags. Experimental analysis shows that models based on language models are able to achieve satisfactory results despite being pre-trained on general domain corpora. Overall, the model achieves an F1 score of 81{% and is able to successfully parametrize the most common radiological observations, allowing for potential adaptation in clinical practice. Our model is publically available.</abstract>
      <url hash="adb18c15">2023.bsnlp-1.14</url>
      <bibkey>obuchowski-etal-2023-information</bibkey>
    </paper>
    <paper id="15">
      <title>Can <fixed-case>BERT</fixed-case> eat <fixed-case>R</fixed-case>u<fixed-case>C</fixed-case>o<fixed-case>LA</fixed-case>? Topological Data Analysis to Explain</title>
      <author><first>Irina</first><last>Proskurina</last><affiliation>University of Lyon, Lyon 2</affiliation></author>
      <author><first>Ekaterina</first><last>Artemova</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Irina</first><last>Piontkovskaya</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <pages>123-137</pages>
      <abstract>This paper investigates how Transformer language models (LMs) fine-tuned for acceptability classification capture linguistic features. Our approach is based on best practices of topological data analysis (TDA) in NLP: we construct directed attention graphs from attention matrices, derive topological features from them and feed them to linear classifiers. We introduce two novel features, chordality and the matching number, and show that TDA-based classifiers outperform fine-tuning baselines. We experiment with two datasets, CoLA and RuCoLA, in English and Russian, which are typologically different languages. On top of that, we propose several black-box introspection techniques aimed at detecting changes in the attention mode of the LM’s during fine-tuning, defining the LM’s prediction confidences, and associating individual heads with fine-grained grammar phenomena. Our results contribute to understanding the behaviour of monolingual LMs in the acceptability classification task, provide insights into the functional roles of attention heads, and highlight the advantages of TDA-based approaches for analyzing LMs.We release the code and the experimental results for further uptake.</abstract>
      <url hash="4f6c568f">2023.bsnlp-1.15</url>
      <bibkey>proskurina-etal-2023-bert</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>W</fixed-case>iki<fixed-case>G</fixed-case>old<fixed-case>SK</fixed-case>: Annotated Dataset, Baselines and Few-Shot Learning Experiments for <fixed-case>S</fixed-case>lovak Named Entity Recognition</title>
      <author><first>David</first><last>Suba</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <author><first>Marek</first><last>Suppa</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <author><first>Jozef</first><last>Kubik</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <author><first>Endre</first><last>Hamerlik</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <author><first>Martin</first><last>Takac</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <pages>138-145</pages>
      <abstract>Named Entity Recognition (NER) is a fundamental NLP tasks with a wide range of practical applications. The performance of state-of-the-art NER methods depends on high quality manually anotated datasets which still do not exist for some languages. In this work we aim to remedy this situation in Slovak by introducing {texttt{WikiGoldSK}, the first sizable human labelled Slovak NER dataset. We benchmark it by evaluating state-of-the-art multilingual Pretrained Language Models and comparing it to the existing silver-standard Slovak NER dataset.We also conduct few-shot experiments and show that training on a sliver-standard dataset yields better results.To enable future work that can be based on Slovak NER, we release the dataset, code, as well as the trained models publicly under permissible licensing terms at https://github.com/NaiveNeuron/WikiGoldSK</abstract>
      <url hash="4406d20a">2023.bsnlp-1.16</url>
      <bibkey>suba-etal-2023-wikigoldsk</bibkey>
    </paper>
    <paper id="17">
      <title>Measuring Gender Bias in <fixed-case>W</fixed-case>est <fixed-case>S</fixed-case>lavic Language Models</title>
      <author><first>Sandra</first><last>Martinková</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Karolina</first><last>Stanczak</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>Department of Computer Science, University of Copenhagen</affiliation></author>
      <pages>146-154</pages>
      <abstract>Pre-trained language models have been known to perpetuate biases from the underlying datasets to downstream tasks. However, these findings are predominantly based on monolingual language models for English, whereas there are few investigative studies of biases encoded in language models for languages beyond English. In this paper, we fill this gap by analysing gender bias in West Slavic language models. We introduce the first template-based dataset in Czech, Polish, and Slovak for measuring gender bias towards male, female and non-binary subjects. We complete the sentences using both mono- and multilingual language models and assess their suitability for the masked language modelling objective. Next, we measure gender bias encoded in West Slavic language models by quantifying the toxicity and genderness of the generated words. We find that these language models produce hurtful completions that depend on the subject’s gender. Perhaps surprisingly, Czech, Slovak, and Polish language models produce more hurtful completions with men as subjects, which, upon inspection, we find is due to completions being related to violence, death, and sickness.</abstract>
      <url hash="49b52c36">2023.bsnlp-1.17</url>
      <bibkey>martinkova-etal-2023-measuring</bibkey>
    </paper>
    <paper id="18">
      <title>On Experiments of Detecting Persuasion Techniques in <fixed-case>P</fixed-case>olish and <fixed-case>R</fixed-case>ussian Online News: Preliminary Study</title>
      <author><first>Nikolaos</first><last>Nikolaidis</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Nicolas</first><last>Stefanovitch</last><affiliation>Joint Research Centre</affiliation></author>
      <author><first>Jakub</first><last>Piskorski</last><affiliation>Polish Academy of Sciences</affiliation></author>
      <pages>155-164</pages>
      <abstract>This paper reports on the results of preliminary experiments on the detection of persuasion techniques in online news in Polish and Russian, using a taxonomy of 23 persuasion techniques. The evaluation addresses different aspects, namely, the granularity of the persuasion technique category, i.e., coarse- (6 labels) versus fine-grained (23 labels), and the focus of the classification, i.e., at which level the labels are detected (subword, sentence, or paragraph). We compare the performance of mono- verus multi-lingual-trained state-of-the-art transformed-based models in this context.</abstract>
      <url hash="0f1c87fc">2023.bsnlp-1.18</url>
      <bibkey>nikolaidis-etal-2023-experiments</bibkey>
    </paper>
    <paper id="19">
      <title>Exploring the Use of Foundation Models for Named Entity Recognition and Lemmatization Tasks in <fixed-case>S</fixed-case>lavic Languages</title>
      <author><first>Gabriela</first><last>Pałka</last><affiliation>Snowflake</affiliation></author>
      <author><first>Artur</first><last>Nowakowski</last><affiliation>Adam Mickiewicz University</affiliation></author>
      <pages>165-171</pages>
      <abstract>This paper describes Adam Mickiewicz University’s (AMU) solution for the 4th Shared Task on SlavNER. The task involves the identification, categorization, and lemmatization of named entities in Slavic languages. Our approach involved exploring the use of foundation models for these tasks. In particular, we used models based on the popular BERT and T5 model architectures. Additionally, we used external datasets to further improve the quality of our models. Our solution obtained promising results, achieving high metrics scores in both tasks. We describe our approach and the results of our experiments in detail, showing that the method is effective for NER and lemmatization in Slavic languages. Additionally, our models for lemmatization will be available at: https://huggingface.co/amu-cai.</abstract>
      <url hash="c712ef29">2023.bsnlp-1.19</url>
      <bibkey>palka-nowakowski-2023-exploring</bibkey>
    </paper>
    <paper id="20">
      <title>Large Language Models for Multilingual <fixed-case>S</fixed-case>lavic Named Entity Linking</title>
      <author><first>Rinalds</first><last>Vīksna</last><affiliation>University of Latvia</affiliation></author>
      <author><first>Inguna</first><last>Skadiņa</last><affiliation>Tilde/ Institute of Mathematics and Computer Science, University of Latvia</affiliation></author>
      <author><first>Daiga</first><last>Deksne</last><affiliation>Tilde; University of Latvia</affiliation></author>
      <author><first>Roberts</first><last>Rozis</last><affiliation>Tilde</affiliation></author>
      <pages>172-178</pages>
      <abstract>This paper describes our submission for the 4th Shared Task on SlavNER on three Slavic languages - Czech, Polish and Russian. We use pre-trained multilingual XLM-R Language Model (Conneau et al., 2020) and fine-tune it for three Slavic languages using datasets provided by organizers. Our multilingual NER model achieves 0.896 F-score on all corpora, with the best result for Czech (0.914) and the worst for Russian (0.880). Our cross-language entity linking module achieves F-score of 0.669 in the official SlavNER 2023 evaluation.</abstract>
      <url hash="8a2b55a2">2023.bsnlp-1.20</url>
      <bibkey>viksna-etal-2023-large</bibkey>
    </paper>
    <paper id="21">
      <title>Slav-<fixed-case>NER</fixed-case>: the 4th Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across <fixed-case>S</fixed-case>lavic languages</title>
      <author><first>Roman</first><last>Yangarber</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Jakub</first><last>Piskorski</last><affiliation>Polish Academy of Sciences</affiliation></author>
      <author><first>Anna</first><last>Dmitrieva</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Michał</first><last>Marcińczuk</last><affiliation>Wrocław University of Science and Technology</affiliation></author>
      <author><first>Pavel</first><last>Přibáň</last><affiliation>University of West Bohemia, Faculty of Applied Sciences</affiliation></author>
      <author><first>Piotr</first><last>Rybak</last><affiliation>Institute of Computer Science, Polish Academy of Sciences</affiliation></author>
      <author><first>Josef</first><last>Steinberger</last><affiliation>University of West Bohemia</affiliation></author>
      <pages>179-189</pages>
      <abstract>This paper describes Slav-NER: the 4th Multilingual Named Entity Challenge in Slavic languages. The tasks involve recognizing mentions of named entities in Web documents, normalization of the names, and cross-lingual linking. This version of the Challenge covers three languages and five entity types. It is organized as part of the 9th Slavic Natural Language Processing Workshop, co-located with the EACL 2023 Conference.Seven teams registered and three participated actively in the competition. Performance for the named entity recognition and normalization tasks reached 90% F1 measure, much higher than reported in the first edition of the Challenge, but similar to the results reported in the latest edition. Performance for the entity linking task for individual language reached the range of 72-80% F1 measure. Detailed evaluation information is available on the Shared Task web page.</abstract>
      <url hash="6a7a99b7">2023.bsnlp-1.21</url>
      <bibkey>yangarber-etal-2023-slav</bibkey>
    </paper>
  </volume>
</collection>
