<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.mrl">
  <volume id="1" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Multilingual Representation Learning</booktitle>
      <editor><first>Duygu</first><last>Ataman</last></editor>
      <editor><first>Alexandra</first><last>Birch</last></editor>
      <editor><first>Alexis</first><last>Conneau</last></editor>
      <editor><first>Orhan</first><last>Firat</last></editor>
      <editor><first>Sebastian</first><last>Ruder</last></editor>
      <editor><first>Gozde Gul</first><last>Sahin</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
      <venue>mrl</venue>
    </meta>
    <frontmatter>
      <url hash="2895c85e">2021.mrl-1.0</url>
      <bibkey>mrl-2021-multilingual</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Language Models are Few-shot Multilingual Learners</title>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Rosanne</first><last>Liu</last></author>
      <author><first>Jason</first><last>Yosinski</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>1–15</pages>
      <abstract>General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models.</abstract>
      <url hash="3cbde5c7">2021.mrl-1.1</url>
      <bibkey>winata-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.mrl-1.1</doi>
      <video href="2021.mrl-1.1.mp4"/>
      <pwccode url="https://github.com/gentaiscool/few-shot-lm" additional="false">gentaiscool/few-shot-lm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="2">
      <title>Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora</title>
      <author><first>Takashi</first><last>Wada</last></author>
      <author><first>Tomoharu</first><last>Iwata</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <pages>16–31</pages>
      <abstract>We propose a new approach for learning contextualised cross-lingual word embeddings based on a small parallel corpus (e.g. a few hundred sentence pairs). Our method obtains word embeddings via an LSTM encoder-decoder model that simultaneously translates and reconstructs an input sentence. Through sharing model parameters among different languages, our model jointly trains the word embeddings in a common cross-lingual space. We also propose to combine word and subword embeddings to make use of orthographic similarities across different languages. We base our experiments on real-world data from endangered languages, namely Yongning Na, Shipibo-Konibo, and Griko. Our experiments on bilingual lexicon induction and word alignment tasks show that our model outperforms existing methods by a large margin for most language pairs. These results demonstrate that, contrary to common belief, an encoder-decoder translation model is beneficial for learning cross-lingual representations even in extremely low-resource conditions. Furthermore, our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.</abstract>
      <url hash="7c99ca7d">2021.mrl-1.2</url>
      <bibkey>wada-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.mrl-1.2</doi>
      <video href="2021.mrl-1.2.mp4"/>
      <pwccode url="https://github.com/twadada/multilingual-nlm" additional="false">twadada/multilingual-nlm</pwccode>
    </paper>
    <paper id="3">
      <title>Clustering Monolingual Vocabularies to Improve Cross-Lingual Generalization</title>
      <author><first>Riccardo</first><last>Bassani</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <author><first>Tejaswini</first><last>Deoskar</last></author>
      <pages>32–40</pages>
      <abstract>Multilingual language models exhibit better performance for some languages than for others (Singh et al., 2019), and many languages do not seem to benefit from multilingual sharing at all, presumably as a result of poor multilingual segmentation (Pyysal o et al., 2020). This work explores the idea of learning multilingual language models based on clustering of monolingual segments. We show significant improvements over standard multilingual segmentation and training across nine languages on a question answering task, both in a small model regime and for a model of the size of BERT-base.</abstract>
      <url hash="73ca47c3">2021.mrl-1.3</url>
      <bibkey>bassani-etal-2021-clustering</bibkey>
      <doi>10.18653/v1/2021.mrl-1.3</doi>
      <video href="2021.mrl-1.3.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
    </paper>
    <paper id="4">
      <title>Do not neglect related languages: The case of low-resource <fixed-case>O</fixed-case>ccitan cross-lingual word embeddings</title>
      <author><first>Lisa</first><last>Woller</last></author>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>41–50</pages>
      <abstract>Cross-lingual word embeddings (CLWEs) have proven indispensable for various natural language processing tasks, e.g., bilingual lexicon induction (BLI). However, the lack of data often impairs the quality of representations. Various approaches requiring only weak cross-lingual supervision were proposed, but current methods still fail to learn good CLWEs for languages with only a small monolingual corpus. We therefore claim that it is necessary to explore further datasets to improve CLWEs in low-resource setups. In this paper we propose to incorporate data of related high-resource languages. In contrast to previous approaches which leverage independently pre-trained embeddings of languages, we (i) train CLWEs for the low-resource and a related language jointly and (ii) map them to the target language to build the final multilingual space. In our experiments we focus on Occitan, a low-resource Romance language which is often neglected due to lack of resources. We leverage data from French, Spanish and Catalan for training and evaluate on the Occitan-English BLI task. By incorporating supporting languages our method outperforms previous approaches by a large margin. Furthermore, our analysis shows that the degree of relatedness between an incorporated language and the low-resource language is critically important.</abstract>
      <url hash="00966483">2021.mrl-1.4</url>
      <bibkey>woller-etal-2021-neglect</bibkey>
      <doi>10.18653/v1/2021.mrl-1.4</doi>
      <video href="2021.mrl-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Specializing Multilingual Language Models: An Empirical Study</title>
      <author><first>Ethan C.</first><last>Chau</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>51–61</pages>
      <abstract>Pretrained multilingual language models have become a common tool in transferring NLP capabilities to low-resource languages, often with adaptations. In this work, we study the performance, extensibility, and interaction of two such adaptations: vocabulary augmentation and script transliteration. Our evaluations on part-of-speech tagging, universal dependency parsing, and named entity recognition in nine diverse low-resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low-resource settings.</abstract>
      <url hash="22575101">2021.mrl-1.5</url>
      <bibkey>chau-smith-2021-specializing</bibkey>
      <doi>10.18653/v1/2021.mrl-1.5</doi>
      <revision id="1" href="2021.mrl-1.5v1" hash="9a564aa6"/>
      <revision id="2" href="2021.mrl-1.5v2" hash="22575101" date="2022-06-22">Updated paper due to implementation correction of paper 2020.findings-emnlp.118.</revision>
      <video href="2021.mrl-1.5.mp4"/>
      <pwccode url="https://github.com/ethch18/specializing-multilingual" additional="false">ethch18/specializing-multilingual</pwccode>
    </paper>
    <paper id="6">
      <title>Learning Cross-lingual Representations for Event Coreference Resolution with Multi-view Alignment and Optimal Transport</title>
      <author><first>Duy</first><last>Phung</last></author>
      <author><first>Hieu</first><last>Minh Tran</last></author>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>62–73</pages>
      <abstract>We study a new problem of cross-lingual transfer learning for event coreference resolution (ECR) where models trained on data from a source language are adapted for evaluations in different target languages. We introduce the first baseline model for this task based on XLM-RoBERTa, a state-of-the-art multilingual pre-trained language model. We also explore language adversarial neural networks (LANN) that present language discriminators to distinguish texts from the source and target languages to improve the language generalization for ECR. In addition, we introduce two novel mechanisms to further enhance the general representation learning of LANN, featuring: (i) multi-view alignment to penalize cross coreference-label alignment of examples in the source and target languages, and (ii) optimal transport to select close examples in the source and target languages to provide better training signals for the language discriminators. Finally, we perform extensive experiments for cross-lingual ECR from English to Spanish and Chinese to demonstrate the effectiveness of the proposed methods.</abstract>
      <url hash="5e7037d5">2021.mrl-1.6</url>
      <bibkey>phung-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.mrl-1.6</doi>
      <video href="2021.mrl-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training</title>
      <author><first>Vikram</first><last>Gupta</last></author>
      <pages>74–85</pages>
      <abstract>Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel emotion recognition has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leveraging unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on SemEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). We also improve the existing state-of-the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for understanding the impact of different layers of the contextual models.</abstract>
      <url hash="d2b14682">2021.mrl-1.7</url>
      <bibkey>gupta-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.mrl-1.7</doi>
      <video href="2021.mrl-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance</title>
      <author><first>Karthikeyan</first><last>K</last></author>
      <author><first>Aalok</first><last>Sathe</last></author>
      <author><first>Somak</first><last>Aditya</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>86–95</pages>
      <abstract>Multilingual language models achieve impressive zero-shot accuracies in many languages in complex tasks such as Natural Language Inference (NLI). Examples in NLI (and equivalent complex tasks) often pertain to various types of sub-tasks, requiring different kinds of reasoning. Certain types of reasoning have proven to be more difficult to learn in a monolingual context, and in the crosslingual context, similar observations may shed light on zero-shot transfer efficiency and few-shot sample selection. Hence, to investigate the effects of types of reasoning on transfer performance, we propose a category-annotated multilingual NLI dataset and discuss the challenges to scale monolingual annotations to multiple languages. We statistically observe interesting effects that the confluence of reasoning types and language similarities have on transfer performance.</abstract>
      <url hash="cbe4db68">2021.mrl-1.8</url>
      <bibkey>k-etal-2021-analyzing</bibkey>
      <doi>10.18653/v1/2021.mrl-1.8</doi>
      <pwccode url="https://github.com/microsoft/taxixnli" additional="false">microsoft/taxixnli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/taxinli">TaxiNLI</pwcdataset>
    </paper>
    <paper id="9">
      <title>Identifying the Importance of Content Overlap for Better Cross-lingual Embedding Mappings</title>
      <author><first>Réka</first><last>Cserháti</last></author>
      <author><first>Gábor</first><last>Berend</last></author>
      <pages>96–106</pages>
      <abstract>In this work, we analyze the performance and properties of cross-lingual word embedding models created by mapping-based alignment methods. We use several measures of corpus and embedding similarity to predict BLI scores of cross-lingual embedding mappings over three types of corpora, three embedding methods and 55 language pairs. Our experimental results corroborate that instead of mere size, the amount of common content in the training corpora is essential. This phenomenon manifests in that i) despite of the smaller corpus sizes, using only the comparable parts of Wikipedia for training the monolingual embedding spaces to be mapped is often more efficient than relying on all the contents of Wikipedia, ii) the smaller, in return less diversified Spanish Wikipedia works almost always much better as a training corpus for bilingual mappings than the ubiquitously used English Wikipedia.</abstract>
      <url hash="673ebaa6">2021.mrl-1.9</url>
      <bibkey>cserhati-berend-2021-identifying</bibkey>
      <doi>10.18653/v1/2021.mrl-1.9</doi>
      <video href="2021.mrl-1.9.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="10">
      <title>On the Cross-lingual Transferability of Contextualized Sense Embeddings</title>
      <author><first>Kiamehr</first><last>Rezaee</last></author>
      <author><first>Daniel</first><last>Loureiro</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>107–115</pages>
      <abstract>In this paper we analyze the extent to which contextualized sense embeddings, i.e., sense embeddings that are computed based on contextualized word embeddings, are transferable across languages.To this end, we compiled a unified cross-lingual benchmark for Word Sense Disambiguation. We then propose two simple strategies to transfer sense-specific knowledge across languages and test them on the benchmark.Experimental results show that this contextualized knowledge can be effectively transferred to similar languages through pre-trained multilingual language models, to the extent that they can out-perform monolingual representations learnednfrom existing language-specific data.</abstract>
      <url hash="a4b89553">2021.mrl-1.10</url>
      <bibkey>rezaee-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.mrl-1.10</doi>
      <video href="2021.mrl-1.10.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="11">
      <title>Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages</title>
      <author><first>Kelechi</first><last>Ogueji</last></author>
      <author><first>Yuxin</first><last>Zhu</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>116–126</pages>
      <abstract>Pretrained multilingual language models have been shown to work well on many languages for a variety of downstream NLP tasks. However, these models are known to require a lot of training data. This consequently leaves out a huge percentage of the world’s languages as they are under-resourced. Furthermore, a major motivation behind these models is that lower-resource languages benefit from joint training with higher-resource languages. In this work, we challenge this assumption and present the first attempt at training a multilingual language model on only low-resource languages. We show that it is possible to train competitive multilingual language models on less than 1 GB of text. Our model, named AfriBERTa, covers 11 African languages, including the first language model for 4 of these languages. Evaluations on named entity recognition and text classification spanning 10 languages show that our model outperforms mBERT and XLM-Rin several languages and is very competitive overall. Results suggest that our “small data” approach based on similar languages may sometimes work better than joint training on large datasets with high-resource languages. Code, data and models are released at https://github.com/keleog/afriberta.</abstract>
      <url hash="98baf544">2021.mrl-1.11</url>
      <bibkey>ogueji-etal-2021-small</bibkey>
      <doi>10.18653/v1/2021.mrl-1.11</doi>
      <pwccode url="https://github.com/keleog/afriberta" additional="false">keleog/afriberta</pwccode>
    </paper>
    <paper id="12">
      <title>Mr. <fixed-case>T</fixed-case>y<fixed-case>D</fixed-case>i: A Multi-lingual Benchmark for Dense Retrieval</title>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Xueguang</first><last>Ma</last></author>
      <author><first>Peng</first><last>Shi</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>127–137</pages>
      <abstract>We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual retrieval in eleven typologically diverse languages, designed to evaluate ranking with learned dense representations. The goal of this resource is to spur research in dense retrieval techniques in non-English languages, motivated by recent observations that existing techniques for representation learning perform poorly when applied to out-of-distribution data. As a starting point, we provide zero-shot baselines for this new dataset based on a multi-lingual adaptation of DPR that we call “mDPR”. Experiments show that although the effectiveness of mDPR is much lower than BM25, dense representations nevertheless appear to provide valuable relevance signals, improving BM25 results in sparse–dense hybrids. In addition to analyses of our results, we also discuss future challenges and present a research agenda in multi-lingual dense retrieval. Mr. TyDi can be downloaded at https://github.com/castorini/mr.tydi.</abstract>
      <url hash="b65d5771">2021.mrl-1.12</url>
      <bibkey>zhang-etal-2021-mr</bibkey>
      <doi>10.18653/v1/2021.mrl-1.12</doi>
      <video href="2021.mrl-1.12.mp4"/>
      <pwccode url="https://github.com/castorini/mr.tydi" additional="false">castorini/mr.tydi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mr-tydi">Mr. TYDI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="13">
      <title><fixed-case>V</fixed-case>isual<fixed-case>S</fixed-case>em: a high-quality knowledge graph for vision and language</title>
      <author><first>Houda</first><last>Alberts</last></author>
      <author><first>Ningyuan</first><last>Huang</last></author>
      <author><first>Yash</first><last>Deshpande</last></author>
      <author><first>Yibo</first><last>Liu</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Clara</first><last>Vania</last></author>
      <author><first>Iacer</first><last>Calixto</last></author>
      <pages>138–152</pages>
      <abstract>An exciting frontier in natural language understanding (NLU) and generation (NLG) calls for (vision-and-) language models that can efficiently access external structured knowledge repositories. However, many existing knowledge bases only cover limited domains, or suffer from noisy data, and most of all are typically hard to integrate into neural language pipelines. To fill this gap, we release VisualSem: a high-quality knowledge graph (KG) which includes nodes with multilingual glosses, multiple illustrative images, and visually relevant relations. We also release a neural multi-modal retrieval model that can use images or sentences as inputs and retrieves entities in the KG. This multi-modal retrieval model can be integrated into any (neural network) model pipeline. We encourage the research community to use VisualSem for data augmentation and/or as a source of grounding, among other possible uses. VisualSem as well as the multi-modal retrieval models are publicly available and can be downloaded in this URL: https://github.com/iacercalixto/visualsem.</abstract>
      <url hash="231e95c6">2021.mrl-1.13</url>
      <bibkey>alberts-etal-2021-visualsem</bibkey>
      <doi>10.18653/v1/2021.mrl-1.13</doi>
      <video href="2021.mrl-1.13.mp4"/>
      <pwccode url="https://github.com/iacercalixto/visualsem" additional="false">iacercalixto/visualsem</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="14">
      <title><fixed-case>V</fixed-case>yākarana: <fixed-case>A</fixed-case> Colorless Green Benchmark for Syntactic Evaluation in Indic Languages</title>
      <author><first>Rajaswa</first><last>Patil</last></author>
      <author><first>Jasleen</first><last>Dhillon</last></author>
      <author><first>Siddhant</first><last>Mahurkar</last></author>
      <author><first>Saumitra</first><last>Kulkarni</last></author>
      <author><first>Manav</first><last>Malhotra</last></author>
      <author><first>Veeky</first><last>Baths</last></author>
      <pages>153–165</pages>
      <abstract>While there has been significant progress towards developing NLU resources for Indic languages, syntactic evaluation has been relatively less explored. Unlike English, Indic languages have rich morphosyntax, grammatical genders, free linear word-order, and highly inflectional morphology. In this paper, we introduce Vyākarana: a benchmark of Colorless Green sentences in Indic languages for syntactic evaluation of multilingual language models. The benchmark comprises four syntax-related tasks: PoS Tagging, Syntax Tree-depth Prediction, Grammatical Case Marking, and Subject-Verb Agreement. We use the datasets from the evaluation tasks to probe five multilingual language models of varying architectures for syntax in Indic languages. Due to its prevalence, we also include a code-switching setting in our experiments. Our results show that the token-level and sentence-level representations from the Indic language models (IndicBERT and MuRIL) do not capture the syntax in Indic languages as efficiently as the other highly multilingual language models. Further, our layer-wise probing experiments reveal that while mBERT, DistilmBERT, and XLM-R localize the syntax in middle layers, the Indic language models do not show such syntactic localization.</abstract>
      <url hash="0d4edf46">2021.mrl-1.14</url>
      <bibkey>patil-etal-2021-vyakarana</bibkey>
      <doi>10.18653/v1/2021.mrl-1.14</doi>
      <video href="2021.mrl-1.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Improving the Diversity of Unsupervised Paraphrasing with Embedding Outputs</title>
      <author><first>Monisha</first><last>Jegadeesan</last></author>
      <author><first>Sachin</first><last>Kumar</last></author>
      <author><first>John</first><last>Wieting</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>166–175</pages>
      <abstract>We present a novel technique for zero-shot paraphrase generation. The key contribution is an end-to-end multilingual paraphrasing model that is trained using translated parallel corpora to generate paraphrases into “meaning spaces” – replacing the final softmax layer with word embeddings. This architectural modification, plus a training procedure that incorporates an autoencoding objective, enables effective parameter sharing across languages for more fluent monolingual rewriting, and facilitates fluency and diversity in the generated outputs. Our continuous-output paraphrase generation models outperform zero-shot paraphrasing baselines when evaluated on two languages using a battery of computational metrics as well as in human assessment.</abstract>
      <url hash="8d743a9f">2021.mrl-1.15</url>
      <bibkey>jegadeesan-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.mrl-1.15</doi>
      <pwccode url="https://github.com/monisha-jega/paraphrasing_embedding_outputs" additional="false">monisha-jega/paraphrasing_embedding_outputs</pwccode>
    </paper>
    <paper id="16">
      <title>The Effectiveness of Intermediate-Task Training for Code-Switched Natural Language Understanding</title>
      <author><first>Archiki</first><last>Prasad</last></author>
      <author><first>Mohammad Ali</first><last>Rehan</last></author>
      <author><first>Shreya</first><last>Pathak</last></author>
      <author><first>Preethi</first><last>Jyothi</last></author>
      <pages>176–190</pages>
      <abstract>While recent benchmarks have spurred a lot of new work on improving the generalization of pretrained multilingual language models on multilingual tasks, techniques to improve code-switched natural language understanding tasks have been far less explored. In this work, we propose the use of <i>bilingual intermediate pretraining</i> as a reliable technique to derive large and consistent performance gains using code-switched text on three different NLP tasks: Natural Language Inference (NLI), Question Answering (QA) and Sentiment Analysis (SA). We show consistent performance gains on four different code-switched language-pairs (Hindi-English, Spanish-English, Tamil-English and Malayalam-English) for SA and on Hindi-English for NLI and QA. We also present a code-switched masked language modeling (MLM) pretraining technique that consistently benefits SA compared to standard MLM pretraining using real code-switched text.</abstract>
      <url hash="2c5e985e">2021.mrl-1.16</url>
      <bibkey>prasad-etal-2021-effectiveness</bibkey>
      <doi>10.18653/v1/2021.mrl-1.16</doi>
      <video href="2021.mrl-1.16.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tweeteval">TweetEval</pwcdataset>
    </paper>
    <paper id="17">
      <title>Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations</title>
      <author><first>Ekaterina</first><last>Taktasheva</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <pages>191–210</pages>
      <abstract>Recent research has adopted a new experimental field centered around the concept of text perturbations which has revealed that shuffled word order has little to no impact on the downstream performance of Transformer-based language models across many NLP tasks. These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the word order is modeled with position embeddings. To this end, this paper proposes nine probing datasets organized by the type of controllable text perturbation for three Indo-European languages with a varying degree of word order flexibility: English, Swedish and Russian. Based on the probing analysis of the M-BERT and M-BART models, we report that the syntactic sensitivity depends on the language and model pre-training objectives. We also find that the sensitivity grows across layers together with the increase of the perturbation granularity. Last but not least, we show that the models barely use the positional information to induce syntactic trees from their intermediate self-attention and contextualized representations.</abstract>
      <url hash="d6f27a87">2021.mrl-1.17</url>
      <bibkey>taktasheva-etal-2021-shaking</bibkey>
      <doi>10.18653/v1/2021.mrl-1.17</doi>
      <video href="2021.mrl-1.17.mp4"/>
      <pwccode url="https://github.com/evtaktasheva/dependency_extraction" additional="false">evtaktasheva/dependency_extraction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/blimp">BLiMP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="18">
      <title>Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent Prediction and Slot Filling</title>
      <author><first>Jitin</first><last>Krishnan</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Hemant</first><last>Purohit</last></author>
      <author><first>Huzefa</first><last>Rangwala</last></author>
      <pages>211–223</pages>
      <abstract>Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding (NLU). Since annotated datasets are only available for a handful of languages, our work focuses particularly on a zero-shot scenario where the target language is unseen during training. In the context of zero-shot learning, this task is typically approached using representations from pre-trained multilingual language models such as mBERT or by fine-tuning on data automatically translated into the target language. We propose a novel method which augments monolingual source data using multilingual code-switching via random translations, to enhance generalizability of large multilingual language models when fine-tuning them for downstream tasks. Experiments on the MultiATIS++ benchmark show that our method leads to an average improvement of +4.2% in accuracy for the intent task and +1.8% in F1 for the slot-filling task over the state-of-the-art across 8 typologically diverse languages. We also study the impact of code-switching into different families of languages on downstream performance. Furthermore, we present an application of our method for crisis informatics using a new human-annotated tweet dataset of slot filling in English and Haitian Creole, collected during the Haiti earthquake.</abstract>
      <url hash="4d1a2cec">2021.mrl-1.18</url>
      <attachment type="Software" hash="075eb3db">2021.mrl-1.18.Software.zip</attachment>
      <bibkey>krishnan-etal-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.mrl-1.18</doi>
      <pwccode url="https://github.com/jitinkrishnan/Multilingual-ZeroShot-SlotFilling" additional="false">jitinkrishnan/Multilingual-ZeroShot-SlotFilling</pwccode>
    </paper>
    <paper id="19">
      <title>Analysis of Zero-Shot Crosslingual Learning between <fixed-case>E</fixed-case>nglish and <fixed-case>K</fixed-case>orean for Named Entity Recognition</title>
      <author><first>Jongin</first><last>Kim</last></author>
      <author><first>Nayoung</first><last>Choi</last></author>
      <author><first>Seunghyun</first><last>Lim</last></author>
      <author><first>Jungwhan</first><last>Kim</last></author>
      <author><first>Soojin</first><last>Chung</last></author>
      <author><first>Hyunsoo</first><last>Woo</last></author>
      <author><first>Min</first><last>Song</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>224–237</pages>
      <abstract>This paper presents a English-Korean parallel dataset that collects 381K news articles where 1,400 of them, comprising 10K sentences, are manually labeled for crosslingual named entity recognition (NER). The annotation guidelines for the two languages are developed in parallel, that yield the inter-annotator agreement scores of 91 and 88% for English and Korean respectively, indicating sublime quality annotation in our dataset. Three types of crosslingual learning approaches, direct model transfer, embedding projection, and annotation projection, are used to develop zero-shot Korean NER models. Our best model gives the F1-score of 51% that is very encouraging, considering the extremely distinct natures of these two languages. This is pioneering work that explores zero-shot cross-lingual learning between English and Korean and provides rich parallel annotation for a core NLP task such as named entity recognition.</abstract>
      <url hash="1073cb2a">2021.mrl-1.19</url>
      <bibkey>kim-etal-2021-analysis</bibkey>
      <doi>10.18653/v1/2021.mrl-1.19</doi>
      <pwccode url="https://github.com/emorynlp/mrl-2021" additional="false">emorynlp/mrl-2021</pwccode>
    </paper>
    <paper id="20">
      <title>Regularising Fisher Information Improves Cross-lingual Generalisation</title>
      <author><first>Asa</first><last>Cooper Stickland</last></author>
      <author><first>Iain</first><last>Murray</last></author>
      <pages>238–241</pages>
      <abstract>Many recent works use ‘consistency regularisation’ to improve the generalisation of fine-tuned pre-trained models, both multilingual and English-only. These works encourage model outputs to be similar between a perturbed and normal version of the input, usually via penalising the Kullback–Leibler (KL) divergence between the probability distribution of the perturbed and normal model. We believe that consistency losses may be implicitly regularizing the loss landscape. In particular, we build on work hypothesising that implicitly or explicitly regularizing trace of the Fisher Information Matrix (FIM), amplifies the implicit bias of SGD to avoid memorization. Our initial results show both empirically and theoretically that consistency losses are related to the FIM, and show that the flat minima implied by a small trace of the FIM improves performance when fine-tuning a multilingual model on additional languages. We aim to confirm these initial results on more datasets, and use our insights to develop better multilingual fine-tuning techniques.</abstract>
      <url hash="8823b1ad">2021.mrl-1.20</url>
      <bibkey>cooper-stickland-murray-2021-regularising</bibkey>
      <doi>10.18653/v1/2021.mrl-1.20</doi>
      <video href="2021.mrl-1.20.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="21">
      <title><fixed-case>DM</fixed-case>ix: Distance Constrained Interpolative Mixup</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Shrey</first><last>Pandit</last></author>
      <author><first>Debdoot</first><last>Mukherjee</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>242–244</pages>
      <abstract>Interpolation-based regularisation methods have proven to be effective for various tasks and modalities. Mixup is a data augmentation method that generates virtual training samples from convex combinations of individual inputs and labels. We extend Mixup and propose DMix, distance-constrained interpolative Mixup for sentence classification leveraging the hyperbolic space. DMix achieves state-of-the-art results on sentence classification over existing data augmentation methods across datasets in four languages.</abstract>
      <url hash="8996a24d">2021.mrl-1.21</url>
      <bibkey>sawhney-etal-2021-dmix</bibkey>
      <revision id="1" href="2021.mrl-1.21v1" hash="47db7ca9"/>
      <revision id="2" href="2021.mrl-1.21v2" hash="8996a24d" date="2021-11-16">Retracted.</revision>
      <retracted date="2021-11-16">Paper was already published elsewhere and the authors want to withdraw the paper.</retracted>
      <doi>10.18653/v1/2021.mrl-1.21</doi>
      <video href="2021.mrl-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Sequence Mixup for Zero-Shot Cross-Lingual Part-Of-Speech Tagging</title>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Vishwa</first><last>Shah</last></author>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Debdoot</first><last>Mukherjee</last></author>
      <pages>245–247</pages>
      <abstract>There have been efforts in cross-lingual transfer learning for various tasks. We present an approach utilizing an interpolative data augmentation method, Mixup, to improve the generalizability of models for part-of-speech tagging trained on a source language, improving its performance on unseen target languages. Through experiments on ten languages with diverse structures and language roots, we put forward its applicability for downstream zero-shot cross-lingual tasks.</abstract>
      <url hash="2db89a52">2021.mrl-1.22</url>
      <bibkey>thakkar-etal-2021-sequence</bibkey>
      <revision id="1" href="2021.mrl-1.22v1" hash="f6ab5075"/>
      <revision id="2" href="2021.mrl-1.22v2" hash="2db89a52" date="2021-11-16">Retracted.</revision>
      <retracted date="2021-11-16">Paper was already published elsewhere and the authors want to withdraw the paper.</retracted>
      <doi>10.18653/v1/2021.mrl-1.22</doi>
      <video href="2021.mrl-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Well-Defined Morphology is Sentence-Level Morphology</title>
      <author><first>Omer</first><last>Goldman</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <pages>248–250</pages>
      <abstract>Morphological tasks have gained decent popularity within the NLP community in the recent years, with large multi-lingual datasets providing morphological analysis of words, either in or out of context. However, the lack of a clear linguistic definition for words destines the annotative work to be incomplete and mired in inconsistencies, especially cross-linguistically. In this work we expand morphological inflection of words to inflection of sentences to provide true universality disconnected from orthographic traditions of white-space usage. To allow annotation for sentence-inflection we define a morphological annotation scheme by a fixed set of inflectional features. We present a small cross-linguistic dataset including semi-manually generated simple sentences in 4 typologically diverse languages annotated according to our suggested scheme, and show that the task of reinflection gets substantially more difficult but that the change of scope from words to well-defined sentences allows interface with contextualized language models.</abstract>
      <url hash="2be918d3">2021.mrl-1.23</url>
      <bibkey>goldman-tsarfaty-2021-well</bibkey>
      <doi>10.18653/v1/2021.mrl-1.23</doi>
      <video href="2021.mrl-1.23.mp4"/>
    </paper>
    <paper id="24">
      <title>Cross-Lingual Training of Dense Retrievers for Document Retrieval</title>
      <author><first>Peng</first><last>Shi</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>He</first><last>Bai</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>251–253</pages>
      <abstract>Dense retrieval has shown great success for passage ranking in English. However, its effectiveness for non-English languages remains unexplored due to limitation in training resources. In this work, we explore different transfer techniques for document ranking from English annotations to non-English languages. Our experiments reveal that zero-shot model-based transfer using mBERT improves search quality. We find that weakly-supervised target language transfer is competitive compared to generation-based target language transfer, which requires translation models.</abstract>
      <url hash="803e2476">2021.mrl-1.24</url>
      <bibkey>shi-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.mrl-1.24</doi>
    </paper>
  </volume>
</collection>
