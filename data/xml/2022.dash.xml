<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.dash">
  <volume id="1" ingest-date="2023-01-17">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Data Science with Human-in-the-Loop (Language Advances)</booktitle>
      <editor><first>Eduard</first><last>Dragut</last></editor>
      <editor><first>Yunyao</first><last>Li</last></editor>
      <editor><first>Lucian</first><last>Popa</last></editor>
      <editor><first>Slobodan</first><last>Vucetic</last></editor>
      <editor><first>Shashank</first><last>Srivastava</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, United Arab Emirates (Hybrid)</address>
      <month>December</month>
      <year>2022</year>
      <url hash="30d1589a">2022.dash-1</url>
      <venue>dash</venue>
    </meta>
    <frontmatter>
      <url hash="6fbd234f">2022.dash-1.0</url>
      <bibkey>dash-2022-data</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>MEGA</fixed-case>nno: Exploratory Labeling for <fixed-case>NLP</fixed-case> in Computational Notebooks</title>
      <author><first>Dan</first><last>Zhang</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Hannah</first><last>Kim</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Rafael</first><last>Li Chen</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Eser</first><last>Kandogan</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs - https://megagon.ai/</affiliation></author>
      <pages>1-7</pages>
      <abstract>We present MEGAnno, a novel exploratory annotation framework designed for NLP researchers and practitioners. Unlike existing labeling tools that focus on data labeling only, our framework aims to support a broader, iterative ML workflow including data exploration and model development. With MEGAnno’s API, users can programmatically explore the data through sophisticated search and automated suggestion functions and incrementally update task schema as their project evolve. Combined with our widget, the users can interactively sort, filter, and assign labels to multiple items simultaneously in the same notebook where the rest of the NLP project resides. We demonstrate MEGAnno’s flexible, exploratory, efficient, and seamless labeling experience through a sentiment analysis use case.</abstract>
      <url hash="d957bd8b">2022.dash-1.1</url>
      <bibkey>zhang-etal-2022-meganno</bibkey>
    </paper>
    <paper id="2">
      <title>Cross-lingual Short-text Entity Linking: Generating Features for Neuro-Symbolic Methods</title>
      <author><first>Qiuhao</first><last>Lu</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Sairam</first><last>Gurajada</last><affiliation>IBM Research - Almaden</affiliation></author>
      <author><first>Prithviraj</first><last>Sen</last><affiliation>Amazon</affiliation></author>
      <author><first>Lucian</first><last>Popa</last><affiliation>IBM Research - Almaden</affiliation></author>
      <author><first>Dejing</first><last>Dou</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Thien</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <pages>8-14</pages>
      <abstract>Entity linking (EL) on short text is crucial for a variety of industrial applications. Compared with general long-text EL, short-text EL poses particular challenges as the limited context restricts the clues one can leverage to disambiguate textual mentions. On the other hand, existing studies mostly focus on black-box neural methods and thus lack interpretability, which is critical to industrial applications in certain areas. In this study, we extend upon LNN-EL, a monolingual short-text EL method based on interpretable first-order logic, by incorporating three sets of multilingual features to enable disambiguating mentions written in languages other than English. More specifically, we use multilingual autoencoding language models (i.e., mBERT) to capture the similarities between the mention with its context and the candidate entity; we use multilingual sequence-to-sequence language models (i.e., mBART and mT5) to represent the likelihood of the text given the candidate entity. We also propose a word-level context feature to capture the semantic evidence of the co-occurring mentions. We evaluate the proposed xLNN-EL approach on the QALD-9-multilingual dataset and demonstrate the cross-linguality of the model and the effectiveness of the features.</abstract>
      <url hash="d827dcfc">2022.dash-1.2</url>
      <bibkey>lu-etal-2022-cross</bibkey>
    </paper>
    <paper id="3">
      <title>Crowdsourcing Preposition Sense Disambiguation with High Precision via a Priming Task</title>
      <author><first>Shira</first><last>Wein</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Nathan</first><last>Schneider</last><affiliation>Georgetown University</affiliation></author>
      <pages>15-22</pages>
      <abstract>The careful design of a crowdsourcing protocol is critical to eliciting highly accurate annotations from untrained workers. In this work, we explore the development of crowdsourcing protocols for a challenging word sense disambiguation task. We find that (a) selecting a similar example usage can serve as a proxy for selecting an explicit definition of the sense, and (b) priming workers with an additional, related task within the HIT improves performance on the main proxy task. Ultimately, we demonstrate the usefulness of our crowdsourcing elicitation technique as an effective alternative to previously investigated training strategies, which can be used if agreement on a challenging task is low.</abstract>
      <url hash="110d88dc">2022.dash-1.3</url>
      <bibkey>wein-schneider-2022-crowdsourcing</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>D</fixed-case>o<fixed-case>SA</fixed-case> : A System to Accelerate Annotations on Business Documents with Human-in-the-Loop</title>
      <author><first>Neelesh</first><last>Shukla</last><affiliation>State Street Corporation</affiliation></author>
      <author><first>Msp</first><last>Raja</last><affiliation>Statestreet Corporation Pvt Ltd</affiliation></author>
      <author><first>Raghu</first><last>Katikeri</last><affiliation>State Street Corporation</affiliation></author>
      <author><first>Amit</first><last>Vaid</last><affiliation>State Street Corporation</affiliation></author>
      <pages>23-27</pages>
      <abstract>Business documents come in a variety of structures, formats and information needs which makes information extraction a challenging task. Due to these variations, having a document generic model which can work well across all types of documents for all the use cases seems far-fetched. For document-specific models, we would need customized document-specific labels. We introduce DoSA (Document Specific Automated Annotations), which helps annotators in generating initial annotations automatically using our novel bootstrap approach by leveraging document generic datasets and models. These initial annotations can further be reviewed by a human for correctness. An initial document-specific model can be trained and its inference can be used as feedback for generating more automated annotations. These automated annotations can be reviewed by humanin-the-loop for the correctness and a new improved model can be trained using the current model as pre-trained model before going for the next iteration. In this paper, our scope is limited to Form like documents due to limited availability of generic annotated datasets, but this idea can be extended to a variety of other documents as more datasets are built. An opensource ready-to-use implementation is made available on GitHub.</abstract>
      <url hash="e6f29592">2022.dash-1.4</url>
      <bibkey>shukla-etal-2022-dosa</bibkey>
    </paper>
    <paper id="5">
      <title>Execution-based Evaluation for Data Science Code Generation Models</title>
      <author><first>Junjie</first><last>Huang</last><affiliation>Beihang University</affiliation></author>
      <author><first>Chenglong</first><last>Wang</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jipeng</first><last>Zhang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Cong</first><last>Yan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Haotian</first><last>Cui</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Jeevana Priya</first><last>Inala</last><affiliation>Microsoft</affiliation></author>
      <author><first>Colin</first><last>Clement</last><affiliation>Microsoft</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>28-36</pages>
      <abstract>Code generation models can benefit data scientists’ productivity by automatically generating code from context and text descriptions. An important measure of the modeling progress is whether a model can generate code that can correctly execute to solve the task. However, due to the lack of an evaluation dataset that directly supports execution-based model evaluation, existing work relies on code surface form similarity metrics (e.g., BLEU, CodeBLEU) for model selection, which can be inaccurate. To remedy this, we introduce ExeDS, an evaluation dataset for execution evaluation for data science code generation tasks. ExeDS contains a set of 534 problems from Jupyter Notebooks, each consisting of code context, task description, reference program, and the desired execution output. With ExeDS, we evaluate the execution performance of five state-of-the-art code generation models that have achieved high surface-form evaluation scores. Our experiments show that models with high surface-form scores do not necessarily perform well on execution metrics, and execution-based metrics can better capture model code generation errors. All the code and data will be released upon acceptance.</abstract>
      <url hash="2c698653">2022.dash-1.5</url>
      <bibkey>huang-etal-2022-execution</bibkey>
    </paper>
    <paper id="6">
      <title>A Gamified Approach to Frame Semantic Role Labeling</title>
      <author><first>Emily</first><last>Amspoker</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Miriam R L</first><last>Petruck</last><affiliation>International Computer Science Institute</affiliation></author>
      <pages>37-42</pages>
      <abstract>Much research has investigated the possibility of creating games with a purpose (GWAPs), i.e., online games whose purpose is gathering information to address the insufficient amount of data for training and testing of large language models (Von Ahn and Dabbish, 2008). Based on such work, this paper reports on the development of a game for frame semantic role labeling, where players have fun while using semantic frames as prompts for short story writing. This game will generate more annotations for FrameNet and original content for annotation, supporting FrameNet’s goal of characterizing the English language in terms of Frame Semantics.</abstract>
      <url hash="9fd20115">2022.dash-1.6</url>
      <bibkey>amspoker-petruck-2022-gamified</bibkey>
    </paper>
    <paper id="7">
      <title>A Comparative Analysis between Human-in-the-loop Systems and Large Language Models for Pattern Extraction Tasks</title>
      <author><first>Maeda</first><last>Hanafi</last><affiliation>Ibm</affiliation></author>
      <author><first>Yannis</first><last>Katsis</last><affiliation>IBM Research - Almaden</affiliation></author>
      <author><first>Ishan</first><last>Jindal</last><affiliation>IBM Research</affiliation></author>
      <author><first>Lucian</first><last>Popa</last><affiliation>IBM Research</affiliation></author>
      <pages>43-50</pages>
      <abstract>Building a natural language processing (NLP) model can be challenging for end-users such as analysts, journalists, investigators, etc., especially given that they will likely apply existing tools out of the box. In this article, we take a closer look at how two complementary approaches, a state-of-the-art human-in-the-loop (HITL) tool and a generative language model (GPT-3) perform out of the box, that is, without fine-tuning. Concretely, we compare these approaches when end-users with little technical background are given pattern extraction tasks from text. We discover that the HITL tool performs with higher precision, while GPT-3 requires some level of engineering in its input prompts as well as post-processing on its output before it can achieve comparable results. Future work in this space should look further into the advantages and disadvantages of the two approaches, HITL and generative language model, as well as into ways to optimally combine them.</abstract>
      <url hash="f4001571">2022.dash-1.7</url>
      <bibkey>hanafi-etal-2022-comparative</bibkey>
    </paper>
    <paper id="8">
      <title>Guiding Generative Language Models for Data Augmentation in Few-Shot Text Classification</title>
      <author><first>Aleksandra</first><last>Edwards</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Asahi</first><last>Ushio</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jose</first><last>Camacho-collados</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Helene</first><last>Ribaupierre</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Alun</first><last>Preece</last><affiliation>Cardiff University</affiliation></author>
      <pages>51-63</pages>
      <abstract>Data augmentation techniques are widely used for enhancing the performance of machine learning models by tackling class imbalance issues and data sparsity. State-of-the-art generative language models have been shown to provide significant gains across different NLP tasks. However, their applicability to data augmentation for text classification tasks in few-shot settings have not been fully explored, especially for specialised domains. In this paper, we leverage GPT-2 (Radford et al, 2019) for generating artificial training instances in order to improve classification performance. Our aim is to analyse the impact the selection process of seed training examples has over the quality of GPT-generated samples and consequently the classifier performance. We propose a human-in-the-loop approach for selecting seed samples. Further, we compare the approach to other seed selection strategies that exploit the characteristics of specialised domains such as human-created class hierarchical structure and the presence of noun phrases. Our results show that fine-tuning GPT-2 in a handful of label instances leads to consistent classification improvements and outperform competitive baselines. The seed selection strategies developed in this work lead to significant improvements over random seed selection for specialised domains. We show that guiding text generation through domain expert selection can lead to further improvements, which opens up interesting research avenues for combining generative models and active learning.</abstract>
      <url hash="41cd2aff">2022.dash-1.8</url>
      <attachment type="software" hash="ee650eb2">2022.dash-1.8.software.zip</attachment>
      <attachment type="dataset" hash="deed2b50">2022.dash-1.8.dataset.zip</attachment>
      <bibkey>edwards-etal-2022-guiding</bibkey>
    </paper>
    <paper id="9">
      <title>Partially Humanizing Weak Supervision: Towards a Better Low Resource Pipeline for Spoken Language Understanding</title>
      <author><first>Ayush</first><last>Kumar</last><affiliation>Observe.AI</affiliation></author>
      <author><first>Rishabh</first><last>Tripathi</last><affiliation>Observe.AI</affiliation></author>
      <author><first>Jithendra</first><last>Vepa</last><affiliation>Observe AI</affiliation></author>
      <pages>64-73</pages>
      <abstract>Weak Supervised Learning (WSL) is a popular technique to develop machine learning models in absence of labeled training data. WSL involves training over noisy labels which are traditionally obtained from hand-engineered semantic rules and task-specific pre-trained models. Such rules offer limited coverage and generalization over tasks. On the other hand, pre-trained models are available only for limited tasks. Thus, obtaining weak labels is a bottleneck in weak supervised learning. In this work, we propose to utilize the prompting paradigm to generate weak labels for the underlying tasks. We show that task-agnostic prompts are generalizable and can be used to obtain noisy labels for different Spoken Language Understanding (SLU) tasks such as sentiment classification, disfluency detection and emotion classification. These prompts can additionally be updated with human-in-the-loop to add task-specific contexts, thus providing flexibility to design task-specific prompts. Our proposed WSL pipeline outperforms other competitive low-resource benchmarks on zero and few-shot learning by more than 4% on Macro-F1 and a conventional rule-based WSL baseline by more than 5% across all the benchmark datasets. We demonstrate that prompt-based methods save nearly 75% of time in a weak-supervised framework and generate more reliable labels for the above SLU tasks and thus can be used as a universal strategy to obtain weak labels.</abstract>
      <url hash="85077b8d">2022.dash-1.9</url>
      <bibkey>kumar-etal-2022-partially</bibkey>
    </paper>
    <paper id="10">
      <title>Improving Human Annotation Effectiveness for Fact Collection by Identifying the Most Relevant Answers</title>
      <author><first>Pranav</first><last>Kamath</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yiwen</first><last>Sun</last><affiliation>Apple</affiliation></author>
      <author><first>Thomas</first><last>Semere</last><affiliation>Apple</affiliation></author>
      <author><first>Adam</first><last>Green</last><affiliation>Apple</affiliation></author>
      <author><first>Scott</first><last>Manley</last><affiliation>Apple</affiliation></author>
      <author><first>Xiaoguang</first><last>Qi</last><affiliation>Apple</affiliation></author>
      <author><first>Kun</first><last>Qian</last><affiliation>Apple</affiliation></author>
      <author><first>Yunyao</first><last>Li</last><affiliation>Apple</affiliation></author>
      <pages>74-80</pages>
      <abstract>Identifying and integrating missing facts is a crucial task for knowledge graph completion to ensure robustness towards downstream applications such as question answering. Adding new facts for a knowledge graph in real world system often involves human verification effort, where candidate facts are verified for accuracy by human annotators. This process is labor-intensive, time-consuming, and inefficient since only a small number of missing facts can be identified. This paper proposes a simple but effective human-in-the-loop framework for fact collection that searches for a diverse set of highly relevant candidate facts for human annotation. Empirical results presented in this work demonstrate that the proposed solution leads to both improvements in i) the quality of the candidate facts as well as ii) the ability of discovering more facts to grow the knowledge graph without requiring additional human effort.</abstract>
      <url hash="a4606d0c">2022.dash-1.10</url>
      <bibkey>kamath-etal-2022-improving</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>AVA</fixed-case>-<fixed-case>TMP</fixed-case>: A Human-in-the-Loop Multi-layer Dynamic Topic Modeling Pipeline</title>
      <author><first>Viseth</first><last>Sean</last><affiliation>Alignment Healthcare</affiliation></author>
      <author><first>Padideh</first><last>Danaee</last><affiliation>Alignment Healthcare</affiliation></author>
      <author><first>Yang</first><last>Yang</last><affiliation>Alignment Healthcare</affiliation></author>
      <author><first>Hakan</first><last>Kardes</last><affiliation>Alignment Healthcare</affiliation></author>
      <pages>81-87</pages>
      <abstract>A phone call is still one of the primary preferred channels for seniors to express their needs, ask questions, and inform potential problems to their health insurance plans. Alignment Healthis a next-generation, consumer-centric organization that is providing a variety of Medicare Advantage Products for seniors. We combine our proprietary technology platform, AVA, and our high-touch clinical model to provide seniors with care as it should be: high quality, low cost, and accompanied by a vastly improved consumer experience. Our members have the ability to connect with our member services and concierge teams 24/7 for a wide variety of ever-changing reasons through different channels, such as phone, email, and messages. We strive to provide an excellent member experience and ensure our members are getting the help and information they need at every touch —ideally, even before they reach us. This requires ongoing monitoring of reasons for contacting us, ensuring agents are equipped with the right tools and information to serve members, and coming up with proactive strategies to eliminate the need for the call when possible.We developed an NLP-based dynamic call reason tagging and reporting pipeline with an optimized human-in-the-loop approach to enable accurate call reason reporting and monitoring with the ability to see high-level trends as well as drill down into more granular sub-reasons. Our system produces 96.4% precision and 30%-50% better recall in tagging calls with proper reasons. We have also consistently achieved a 60+ Net Promoter Score (NPS) score, which illustrates high consumer satisfaction.</abstract>
      <url hash="83a946a4">2022.dash-1.11</url>
      <bibkey>sean-etal-2022-ava</bibkey>
    </paper>
    <paper id="12">
      <title>Improving Named Entity Recognition in Telephone Conversations via Effective Active Learning with Human in the Loop</title>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Cheng</first><last>Chen</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Xue-yong</first><last>Fu</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Shashi</first><last>Bhushan Tn</last><affiliation>Dialpad Inc</affiliation></author>
      <pages>88-93</pages>
      <abstract>Telephone transcription data can be very noisy due to speech recognition errors, disfluencies, etc. Not only that annotating such data is very challenging for the annotators, but also such data may have lots of annotation errors even after the annotation job is completed, resulting in a very poor model performance. In this paper, we present an active learning framework that leverages human in the loop learning to identify data samples from the annotated dataset for re-annotation that are more likely to contain annotation errors. In this way, we largely reduce the need for data re-annotation for the whole dataset. We conduct extensive experiments with our proposed approach for Named Entity Recognition and observe that by re-annotating only about 6% training instances out of the whole dataset, the F1 score for a certain entity type can be significantly improved by about 25%.</abstract>
      <url hash="3af2c524">2022.dash-1.12</url>
      <bibkey>laskar-etal-2022-improving</bibkey>
    </paper>
    <paper id="13">
      <title>Interactively Uncovering Latent Arguments in Social Media Platforms: A Case Study on the Covid-19 Vaccine Debate</title>
      <author><first>Maria Leonor</first><last>Pacheco</last><affiliation>University of Colorado Boulder / Microsoft Research</affiliation></author>
      <author><first>Tunazzina</first><last>Islam</last><affiliation>Purdue University</affiliation></author>
      <author><first>Lyle</first><last>Ungar</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Ming</first><last>Yin</last><affiliation>Purdue University</affiliation></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University</affiliation></author>
      <pages>94-111</pages>
      <abstract>Automated methods for analyzing public opinion have grown in popularity with the proliferation of social media. While supervised methods can be very good at classifying text, the dynamic nature of social media discourse results in a moving target for supervised learning. Meanwhile, traditional unsupervised techniques for extracting themes from textual repositories, such as topic models, can result in incorrect outputs that are unusable to domain experts. For this reason, a non-trivial amount of research on social media discourse still relies on manual coding techniques. In this paper, we present an interactive, humans-in-the-loop framework that strikes a balance between unsupervised techniques and manual coding for extracting latent arguments from social media discussions. We use the COVID-19 vaccination debate as a case study, and show that our methodology can be used to obtain a more accurate, interpretable set of arguments when compared to traditional topic models. We do this at a relatively low manual cost, as 3 experts take approximately 2 hours to code close to 100k tweets.</abstract>
      <url hash="a7768b8d">2022.dash-1.13</url>
      <bibkey>pacheco-etal-2022-interactively</bibkey>
    </paper>
    <paper id="14">
      <title>User or Labor: An Interaction Framework for Human-Machine Relationships in <fixed-case>NLP</fixed-case></title>
      <author><first>Ruyuan</first><last>Wan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Naome</first><last>Etori</last><affiliation>University of Minnesota</affiliation></author>
      <author><first>Karla</first><last>Badillo-urquiola</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Dongyeop</first><last>Kang</last><affiliation>University of Minnesota</affiliation></author>
      <pages>112-121</pages>
      <abstract>The bridging research between Human-Computer Interaction and Natural Language Processing is developing quickly these years. However, there is still a lack of formative guidelines to understand the human-machine interaction in the NLP loop. When researchers crossing the two fields talk about humans, they may imply a user or labor. Regarding a human as a user, the human is in control, and the machine is used as a tool to achieve the human’s goals. Considering a human as a laborer, the machine is in control, and the human is used as a resource to achieve the machine’s goals. Through a systematic literature review and thematic analysis, we present an interaction framework for understanding human-machine relationships in NLP. In the framework, we propose four types of human-machine interactions: Human-Teacher and Machine-Learner, Machine-Leading, Human-Leading, and Human-Machine Collaborators. Our analysis shows that the type of interaction is not fixed but can change across tasks as the relationship between the human and the machine develops. We also discuss the implications of this framework for the future of NLP and human-machine relationships.</abstract>
      <url hash="b51cf35d">2022.dash-1.14</url>
      <bibkey>wan-etal-2022-user</bibkey>
    </paper>
  </volume>
</collection>
