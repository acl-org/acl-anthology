<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.socialnlp">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media</booktitle>
      <editor><first>Lun-Wei</first><last>Ku</last></editor>
      <editor><first>Cheng-Te</first><last>Li</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.socialnlp-1</url>
      <venue>socialnlp</venue>
    </meta>
    <frontmatter>
      <url hash="d0f89066">2021.socialnlp-1.0</url>
      <bibkey>socialnlp-2021-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Analysis of Nuanced Stances and Sentiment Towards Entities of <fixed-case>US</fixed-case> Politicians through the Lens of Moral Foundation Theory</title>
      <author><first>Shamik</first><last>Roy</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>1–13</pages>
      <abstract>The Moral Foundation Theory suggests five moral foundations that can capture the view of a user on a particular issue. It is widely used to identify sentence-level sentiment. In this paper, we study the Moral Foundation Theory in tweets by US politicians on two politically divisive issues - Gun Control and Immigration. We define the nuanced stance of politicians on these two topics by the grades given by related organizations to the politicians. First, we identify moral foundations in tweets from a huge corpus using deep relational learning. Then, qualitative and quantitative evaluations using the corpus show that there is a strong correlation between the moral foundation usage and the politicians’ nuanced stance on a particular topic. We also found substantial differences in moral foundation usage by different political parties when they address different entities. All of these results indicate the need for more intense research in this area.</abstract>
      <url hash="d531d495">2021.socialnlp-1.1</url>
      <doi>10.18653/v1/2021.socialnlp-1.1</doi>
      <bibkey>roy-goldwasser-2021-analysis</bibkey>
    </paper>
    <paper id="2">
      <title>Content-based Stance Classification of Tweets about the 2020 <fixed-case>I</fixed-case>talian Constitutional Referendum</title>
      <author><first>Marco</first><last>Di Giovanni</last></author>
      <author><first>Marco</first><last>Brambilla</last></author>
      <pages>14–23</pages>
      <abstract>On September 2020 a constitutional referendum was held in Italy. In this work we collect a dataset of 1.2M tweets related to this event, with particular interest to the textual content shared, and we design a hashtag-based semi-automatic approach to label them as Supporters or Against the referendum. We use the labelled dataset to train a classifier based on transformers, unsupervisedly pre-trained on Italian corpora. Our model generalizes well on tweets that cannot be labeled by the hashtag-based approach. We check that no length-, lexicon- and sentiment-biases are present to affect the performance of the classifier. Finally, we discuss the discrepancy between the magnitudes of tweets expressing a specific stance, obtained using both the hashtag-based approach and our trained classifier, and the real outcome of the referendum: the referendum was approved by 70% of the voters, while the number of tweets against the referendum is four times greater than the number of tweets supporting it. We conclude that the Italian referendum was an example of event where the minority was very loud on social media, highly influencing the perception of the event. Analyzing only the activity on social media is dangerous and can lead to extremely wrong forecasts.</abstract>
      <url hash="1d70cc66">2021.socialnlp-1.2</url>
      <doi>10.18653/v1/2021.socialnlp-1.2</doi>
      <bibkey>di-giovanni-brambilla-2021-content</bibkey>
      <pwccode url="https://github.com/marco-digio/italian-referendum-2020" additional="false">marco-digio/italian-referendum-2020</pwccode>
    </paper>
    <paper id="3">
      <title>A Case Study of In-House Competition for Ranking Constructive Comments in a News Service</title>
      <author><first>Hayato</first><last>Kobayashi</last></author>
      <author><first>Hiroaki</first><last>Taguchi</last></author>
      <author><first>Yoshimune</first><last>Tabuchi</last></author>
      <author><first>Chahine</first><last>Koleejan</last></author>
      <author><first>Ken</first><last>Kobayashi</last></author>
      <author><first>Soichiro</first><last>Fujita</last></author>
      <author><first>Kazuma</first><last>Murao</last></author>
      <author><first>Takeshi</first><last>Masuyama</last></author>
      <author><first>Taichi</first><last>Yatsuka</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <author><first>Satoshi</first><last>Sekine</last></author>
      <pages>24–35</pages>
      <abstract>Ranking the user comments posted on a news article is important for online news services because comment visibility directly affects the user experience. Research on ranking comments with different metrics to measure the comment quality has shown “constructiveness” used in argument analysis is promising from a practical standpoint. In this paper, we report a case study in which this constructiveness is examined in the real world. Specifically, we examine an in-house competition to improve the performance of ranking constructive comments and demonstrate the effectiveness of the best obtained model for a commercial service.</abstract>
      <url hash="973a90ae">2021.socialnlp-1.3</url>
      <doi>10.18653/v1/2021.socialnlp-1.3</doi>
      <bibkey>kobayashi-etal-2021-case</bibkey>
    </paper>
    <paper id="4">
      <title>Quantifying the Effects of <fixed-case>COVID</fixed-case>-19 on Restaurant Reviews</title>
      <author><first>Ivy</first><last>Cao</last></author>
      <author><first>Zizhou</first><last>Liu</last></author>
      <author><first>Giannis</first><last>Karamanolakis</last></author>
      <author><first>Daniel</first><last>Hsu</last></author>
      <author><first>Luis</first><last>Gravano</last></author>
      <pages>36–60</pages>
      <abstract>The COVID-19 pandemic has implications beyond physical health, affecting society and economies. Government efforts to slow down the spread of the virus have had a severe impact on many businesses, including restaurants. Mandatory policies such as restaurant closures, bans on social gatherings, and social distancing restrictions have affected restaurant operations as well as customer preferences (e.g., prompting a demand of stricter hygiene standards). As of now, however, it is not clear how and to what extent the pandemic has affected restaurant reviews, an analysis of which could potentially inform policies for addressing this ongoing situation. In this work, we present our efforts to understand the effects of COVID-19 on restaurant reviews, with a focus on Yelp reviews produced during the pandemic for New York City and Los Angeles County restaurants. Overall, we make the following contributions. First, we assemble a dataset of 600 reviews with manual annotations of fine-grained COVID-19 aspects related to restaurants (e.g., hygiene practices, service changes, sympathy and support for local businesses). Second, we address COVID-19 aspect detection using supervised classifiers, weakly-supervised approaches based on keywords, and unsupervised topic modeling approaches, and experimentally show that classifiers based on pre-trained BERT representations achieve the best performance (F1=0.79). Third, we analyze the number and evolution of COVID-related aspects over time and show that the resulting time series have substantial correlation (Spearman’s <tex-math>\rho</tex-math>=0.84) with critical statistics related to the COVID-19 pandemic, including the number of new COVID-19 cases. To our knowledge, this is the first work analyzing the effects of COVID-19 on Yelp restaurant reviews and could potentially inform policies by public health departments, for example, to cover resource utilization.</abstract>
      <url hash="71f3d389">2021.socialnlp-1.4</url>
      <doi>10.18653/v1/2021.socialnlp-1.4</doi>
      <bibkey>cao-etal-2021-quantifying</bibkey>
    </paper>
    <paper id="5">
      <title>Assessing Cognitive Linguistic Influences in the Assignment of Blame</title>
      <author><first>Karen</first><last>Zhou</last></author>
      <author><first>Ana</first><last>Smith</last></author>
      <author><first>Lillian</first><last>Lee</last></author>
      <pages>61–69</pages>
      <abstract>Lab studies in cognition and the psychology of morality have proposed some thematic and linguistic factors that influence moral reasoning. This paper assesses how well the findings of these studies generalize to a large corpus of over 22,000 descriptions of fraught situations posted to a dedicated forum. At this social-media site, users judge whether or not an author is in the wrong with respect to the event that the author described. We find that, consistent with lab studies, there are statistically significant differences in uses of first-person passive voice, as well as first-person agents and patients, between descriptions of situations that receive different blame judgments. These features also aid performance in the task of predicting the eventual collective verdicts.</abstract>
      <url hash="1364306e">2021.socialnlp-1.5</url>
      <doi>10.18653/v1/2021.socialnlp-1.5</doi>
      <bibkey>zhou-etal-2021-assessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/scruples">Scruples</pwcdataset>
    </paper>
    <paper id="6">
      <title>Evaluating Deception Detection Model Robustness To Linguistic Variation</title>
      <author><first>Maria</first><last>Glenski</last></author>
      <author><first>Ellyn</first><last>Ayton</last></author>
      <author><first>Robin</first><last>Cosbey</last></author>
      <author><first>Dustin</first><last>Arendt</last></author>
      <author><first>Svitlana</first><last>Volkova</last></author>
      <pages>70–80</pages>
      <abstract>With the increasing use of machine-learning driven algorithmic judgements, it is critical to develop models that are robust to evolving or manipulated inputs. We propose an extensive analysis of model robustness against linguistic variation in the setting of deceptive news detection, an important task in the context of misinformation spread online. We consider two prediction tasks and compare three state-of-the-art embeddings to highlight consistent trends in model performance, high confidence misclassifications, and high impact failures. By measuring the effectiveness of adversarial defense strategies and evaluating model susceptibility to adversarial attacks using character- and word-perturbed text, we find that character or mixed ensemble models are the most effective defenses and that character perturbation-based attack tactics are more successful.</abstract>
      <url hash="725f36c6">2021.socialnlp-1.6</url>
      <doi>10.18653/v1/2021.socialnlp-1.6</doi>
      <bibkey>glenski-etal-2021-evaluating</bibkey>
    </paper>
    <paper id="7">
      <title>Reconsidering Annotator Disagreement about Racist Language: Noise or Signal?</title>
      <author><first>Savannah</first><last>Larimore</last></author>
      <author><first>Ian</first><last>Kennedy</last></author>
      <author><first>Breon</first><last>Haskett</last></author>
      <author><first>Alina</first><last>Arseniev-Koehler</last></author>
      <pages>81–90</pages>
      <abstract>An abundance of methodological work aims to detect hateful and racist language in text. However, these tools are hampered by problems like low annotator agreement and remain largely disconnected from theoretical work on race and racism in the social sciences. Using annotations of 5188 tweets from 291 annotators, we investigate how annotator perceptions of racism in tweets vary by annotator racial identity and two text features of the tweets: relevant keywords and latent topics identified through structural topic modeling. We provide a descriptive summary of our data and estimate a series of generalized linear models to determine if annotator racial identity and our 12 latent topics, alone or in combination, explain the way racial sentiment was annotated, net of relevant annotator characteristics and tweet features. Our results show that White and non-White annotators exhibit significant differences in ratings when reading tweets with high prevalence of particular, racially-charged topics. We conclude by suggesting how future methodological work can draw on our results and further incorporate social science theory into analyses.</abstract>
      <url hash="5c187275">2021.socialnlp-1.7</url>
      <doi>10.18653/v1/2021.socialnlp-1.7</doi>
      <bibkey>larimore-etal-2021-reconsidering</bibkey>
    </paper>
    <paper id="8">
      <title>Understanding and Interpreting the Impact of User Context in Hate Speech Detection</title>
      <author><first>Edoardo</first><last>Mosca</last></author>
      <author><first>Maximilian</first><last>Wich</last></author>
      <author><first>Georg</first><last>Groh</last></author>
      <pages>91–102</pages>
      <abstract>As hate speech spreads on social media and online communities, research continues to work on its automatic detection. Recently, recognition performance has been increasing thanks to advances in deep learning and the integration of user features. This work investigates the effects that such features can have on a detection model. Unlike previous research, we show that simple performance comparison does not expose the full impact of including contextual- and user information. By leveraging explainability techniques, we show (1) that user features play a role in the model’s decision and (2) how they affect the feature space learned by the model. Besides revealing that—and also illustrating why—user features are the reason for performance gains, we show how such techniques can be combined to better understand the model and to detect unintended bias.</abstract>
      <url hash="6e3a074c">2021.socialnlp-1.8</url>
      <doi>10.18653/v1/2021.socialnlp-1.8</doi>
      <bibkey>mosca-etal-2021-understanding</bibkey>
    </paper>
    <paper id="9">
      <title>Self-Contextualized Attention for Abusive Language Identification</title>
      <author><first>Horacio</first><last>Jarquín-Vásquez</last></author>
      <author><first>Hugo Jair</first><last>Escalante</last></author>
      <author><first>Manuel</first><last>Montes</last></author>
      <pages>103–112</pages>
      <abstract>The use of attention mechanisms in deep learning approaches has become popular in natural language processing due to its outstanding performance. The use of these mechanisms allows one managing the importance of the elements of a sequence in accordance to their context, however, this importance has been observed independently between the pairs of elements of a sequence (self-attention) and between the application domain of a sequence (contextual attention), leading to the loss of relevant information and limiting the representation of the sequences. To tackle these particular issues we propose the self-contextualized attention mechanism, which trades off the previous limitations, by considering the internal and contextual relationships between the elements of a sequence. The proposed mechanism was evaluated in four standard collections for the abusive language identification task achieving encouraging results. It outperformed the current attention mechanisms and showed a competitive performance with respect to state-of-the-art approaches.</abstract>
      <url hash="31c0034d">2021.socialnlp-1.9</url>
      <doi>10.18653/v1/2021.socialnlp-1.9</doi>
      <bibkey>jarquin-vasquez-etal-2021-self</bibkey>
    </paper>
    <paper id="10">
      <title>Unsupervised Domain Adaptation in Cross-corpora Abusive Language Detection</title>
      <author><first>Tulika</first><last>Bose</last></author>
      <author><first>Irina</first><last>Illina</last></author>
      <author><first>Dominique</first><last>Fohr</last></author>
      <pages>113–122</pages>
      <abstract>The state-of-the-art abusive language detection models report great in-corpus performance, but underperform when evaluated on abusive comments that differ from the training scenario. As human annotation involves substantial time and effort, models that can adapt to newly collected comments can prove to be useful. In this paper, we investigate the effectiveness of several Unsupervised Domain Adaptation (UDA) approaches for the task of cross-corpora abusive language detection. In comparison, we adapt a variant of the BERT model, trained on large-scale abusive comments, using Masked Language Model (MLM) fine-tuning. Our evaluation shows that the UDA approaches result in sub-optimal performance, while the MLM fine-tuning does better in the cross-corpora setting. Detailed analysis reveals the limitations of the UDA approaches and emphasizes the need to build efficient adaptation methods for this task.</abstract>
      <url hash="736fcd59">2021.socialnlp-1.10</url>
      <doi>10.18653/v1/2021.socialnlp-1.10</doi>
      <bibkey>bose-etal-2021-unsupervised</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech-and-offensive-language">Hate Speech and Offensive Language</pwcdataset>
    </paper>
    <paper id="11">
      <title>Using Noisy Self-Reports to Predict <fixed-case>T</fixed-case>witter User Demographics</title>
      <author><first>Zach</first><last>Wood-Doughty</last></author>
      <author><first>Paiheng</first><last>Xu</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>123–137</pages>
      <abstract>Computational social science studies often contextualize content analysis within standard demographics. Since demographics are unavailable on many social media platforms (e.g. Twitter), numerous studies have inferred demographics automatically. Despite many studies presenting proof-of-concept inference of race and ethnicity, training of practical systems remains elusive since there are few annotated datasets. Existing datasets are small, inaccurate, or fail to cover the four most common racial and ethnic groups in the United States. We present a method to identify self-reports of race and ethnicity from Twitter profile descriptions. Despite the noise of automated supervision, our self-report datasets enable improvements in classification performance on gold standard self-report survey data. The result is a reproducible method for creating large-scale training resources for race and ethnicity.</abstract>
      <url hash="c7fca580">2021.socialnlp-1.11</url>
      <doi>10.18653/v1/2021.socialnlp-1.11</doi>
      <bibkey>wood-doughty-etal-2021-using</bibkey>
      <pwccode url="https://bitbucket.org/mdredze/demographer" additional="false">mdredze/demographer</pwccode>
    </paper>
    <paper id="12">
      <title><fixed-case>PANDORA</fixed-case> Talks: Personality and Demographics on <fixed-case>R</fixed-case>eddit</title>
      <author><first>Matej</first><last>Gjurković</last></author>
      <author><first>Mladen</first><last>Karan</last></author>
      <author><first>Iva</first><last>Vukojević</last></author>
      <author><first>Mihaela</first><last>Bošnjak</last></author>
      <author><first>Jan</first><last>Snajder</last></author>
      <pages>138–152</pages>
      <abstract>Personality and demographics are important variables in social sciences and computational sociolinguistics. However, datasets with both personality and demographic labels are scarce. To address this, we present PANDORA, the first dataset of Reddit comments of 10k users partially labeled with three personality models and demographics (age, gender, and location), including 1.6k users labeled with the well-established Big 5 personality model. We showcase the usefulness of this dataset on three experiments, where we leverage the more readily available data from other personality models to predict the Big 5 traits, analyze gender classification biases arising from psycho-demographic variables, and carry out a confirmatory and exploratory analysis based on psychological theories. Finally, we present benchmark prediction models for all personality and demographic variables.</abstract>
      <url hash="05dfea1b">2021.socialnlp-1.12</url>
      <doi>10.18653/v1/2021.socialnlp-1.12</doi>
      <bibkey>gjurkovic-etal-2021-pandora</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pandora">PANDORA</pwcdataset>
    </paper>
    <paper id="13">
      <title>Room to Grow: Understanding Personal Characteristics Behind Self Improvement Using Social Media</title>
      <author><first>MeiXing</first><last>Dong</last></author>
      <author><first>Xueming</first><last>Xu</last></author>
      <author><first>Yiwei</first><last>Zhang</last></author>
      <author><first>Ian</first><last>Stewart</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>153–162</pages>
      <abstract>Many people aim for change, but not everyone succeeds. While there are a number of social psychology theories that propose motivation-related characteristics of those who persist with change, few computational studies have explored the motivational stage of personal change. In this paper, we investigate a new dataset consisting of the writings of people who manifest intention to change, some of whom persist while others do not. Using a variety of linguistic analysis techniques, we first examine the writing patterns that distinguish the two groups of people. Persistent people tend to reference more topics related to long-term self-improvement and use a more complicated writing style. Drawing on these consistent differences, we build a classifier that can reliably identify the people more likely to persist, based on their language. Our experiments provide new insights into the motivation-related behavior of people who persist with their intention to change.</abstract>
      <url hash="b1cfcb24">2021.socialnlp-1.13</url>
      <doi>10.18653/v1/2021.socialnlp-1.13</doi>
      <bibkey>dong-etal-2021-room</bibkey>
    </paper>
    <paper id="14">
      <title>Mitigating Temporal-Drift: A Simple Approach to Keep <fixed-case>NER</fixed-case> Models Crisp</title>
      <author><first>Shuguang</first><last>Chen</last></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>163–169</pages>
      <abstract>Performance of neural models for named entity recognition degrades over time, becoming stale. This degradation is due to temporal drift, the change in our target variables’ statistical properties over time. This issue is especially problematic for social media data, where topics change rapidly. In order to mitigate the problem, data annotation and retraining of models is common. Despite its usefulness, this process is expensive and time-consuming, which motivates new research on efficient model updating. In this paper, we propose an intuitive approach to measure the potential trendiness of tweets and use this metric to select the most informative instances to use for training. We conduct experiments on three state-of-the-art models on the Temporal Twitter Dataset. Our approach shows larger increases in prediction accuracy with less training data than the alternatives, making it an attractive, practical solution.</abstract>
      <url hash="5ebd2a3d">2021.socialnlp-1.14</url>
      <doi>10.18653/v1/2021.socialnlp-1.14</doi>
      <bibkey>chen-etal-2021-mitigating</bibkey>
      <pwccode url="https://github.com/RiTUAL-UH/trending_NER" additional="false">RiTUAL-UH/trending_NER</pwccode>
    </paper>
    <paper id="15">
      <title>Jujeop: <fixed-case>K</fixed-case>orean Puns for K-pop Stars on Social Media</title>
      <author><first>Soyoung</first><last>Oh</last></author>
      <author><first>Jisu</first><last>Kim</last></author>
      <author><first>Seungpeel</first><last>Lee</last></author>
      <author><first>Eunil</first><last>Park</last></author>
      <pages>170–177</pages>
      <abstract>Jujeop is a type of pun and a unique way for fans to express their love for the K-pop stars they follow using Korean. One of the unique characteristics of Jujeop is its use of exaggerated expressions to compliment K-pop stars, which contain or lead to humor. Based on this characteristic, Jujeop can be separated into four distinct types, with their own lexical collocations: (1) Fragmenting words to create a twist, (2) Homophones and homographs, (3) Repetition, and (4) Nonsense. Thus, the current study first defines the concept of Jujeop in Korean, manually labels 8.6K comments and annotates the comments to one of the four Jujeop types. With the given annotated corpus, this study presents distinctive characteristics of Jujeop comments compared to the other comments by classification task. Moreover, with the clustering approach, we proposed a structural dependency within each Jujeop type. We have made our dataset publicly available for future research of Jujeop expressions.</abstract>
      <url hash="2bd866e7">2021.socialnlp-1.15</url>
      <doi>10.18653/v1/2021.socialnlp-1.15</doi>
      <bibkey>oh-etal-2021-jujeop</bibkey>
      <pwccode url="https://github.com/merry555/jujeop" additional="false">merry555/jujeop</pwccode>
    </paper>
    <paper id="16">
      <title>Identifying Distributional Perspectives from Colingual Groups</title>
      <author><first>Yufei</first><last>Tian</last></author>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Fred</first><last>Morstatter</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>178–190</pages>
      <abstract>Discrepancies exist among different cultures or languages. A lack of mutual understanding among different colingual groups about the perspectives on specific values or events may lead to uninformed decisions or biased opinions. Thus, automatically understanding the group perspectives can provide essential back-ground for many natural language processing tasks. In this paper, we study colingual groups and use language corpora as a proxy to identify their distributional perspectives. We present a novel computational approach to learn shared understandings, and benchmark our method by building culturally-aware models for the English, Chinese, and Japanese languages. Ona held out set of diverse topics, including marriage, corruption, democracy, etc., our model achieves high correlation with human judgements regarding intra-group values and inter-group differences</abstract>
      <url hash="a4c0bd69">2021.socialnlp-1.16</url>
      <doi>10.18653/v1/2021.socialnlp-1.16</doi>
      <bibkey>tian-etal-2021-identifying</bibkey>
    </paper>
  </volume>
</collection>
