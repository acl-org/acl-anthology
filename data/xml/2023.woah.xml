<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.woah">
  <volume id="1" ingest-date="2023-07-09" type="proceedings">
    <meta>
      <booktitle>The 7th Workshop on Online Abuse and Harms (WOAH)</booktitle>
      <editor><first>Yi-ling</first><last>Chung</last></editor>
      <editor><first>Paul</first><last>R{\"ottger}</last></editor>
      <editor><first>Debora</first><last>Nozza</last></editor>
      <editor><first>Zeerak</first><last>Talat</last></editor>
      <editor><first>Aida</first><last>Mostafazadeh Davani</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada</address>
      <month>July</month>
      <year>2023</year>
      <url hash="1e1d1243">2023.woah-1</url>
      <venue>woah</venue>
    </meta>
    <paper id="1">
      <title>Identity Construction in a Misogynist Incels Forum</title>
      <author><first>Michael</first><last>Yoder</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Chloe</first><last>Perry</last><affiliation>University of Michigan</affiliation></author>
      <author><first>David</first><last>Brown</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Kathleen</first><last>Carley</last><affiliation>Carnegie Mellon University, Netanomics</affiliation></author>
      <author><first>Meredith</first><last>Pruden</last><affiliation>Kennesaw State University</affiliation></author>
      <pages>1-13</pages>
      <abstract>Online communities of involuntary celibates (incels) are a prominent source of misogynist hate speech. In this paper, we use quantitative text and network analysis approaches to examine how identity groups are discussed on incels.is, the largest black-pilled incels forum. We find that this community produces a wide range of novel identity terms and, while terms for women are most common, mentions of other minoritized identities are increasing. An analysis of the associations made with identity groups suggests an essentialist ideology where physical appearance, as well as gender and racial hierarchies, determine human value. We discuss implications for research into automated misogynist hate speech detection.</abstract>
      <url hash="1391921c">2023.woah-1.1</url>
      <bibkey>yoder-etal-2023-identity</bibkey>
      <doi>10.18653/v1/2023.woah-1.1</doi>
      <revision id="1" href="2023.woah-1.1v1" hash="f934d059"/>
      <revision id="2" href="2023.woah-1.1v2" hash="1391921c" date="2024-03-08">Author name correction.</revision>
    </paper>
    <paper id="2">
      <title><fixed-case>D</fixed-case>e<fixed-case>T</fixed-case>ex<fixed-case>D</fixed-case>: A Benchmark Dataset for Delicate Text Detection</title>
      <author><first>Serhii</first><last>Yavnyi</last><affiliation>Grammarly</affiliation></author>
      <author><first>Oleksii</first><last>Sliusarenko</last><affiliation>Grammarly</affiliation></author>
      <author><first>Jade</first><last>Razzaghi</last><affiliation>Grammarly</affiliation></author>
      <author><first>Olena</first><last>Nahorna</last><affiliation>Grammarly</affiliation></author>
      <author><first>Yichen</first><last>Mo</last><affiliation>Grammarly</affiliation></author>
      <author><first>Knar</first><last>Hovakimyan</last><affiliation>Grammarly</affiliation></author>
      <author><first>Artem</first><last>Chernodub</last><affiliation>Grammarly</affiliation></author>
      <pages>14-28</pages>
      <abstract>Over the past few years, much research has been conducted to identify and regulate toxic language. However, few studies have addressed a broader range of sensitive texts that are not necessarily overtly toxic. In this paper, we introduce and define a new category of sensitive text called “delicate text.” We provide the taxonomy of delicate text and present a detailed annotation scheme. We annotate DeTexD, the first benchmark dataset for delicate text detection. The significance of the difference in the definitions is highlighted by the relative performance deltas between models trained each definitions and corpora and evaluated on the other. We make publicly available the DeTexD Benchmark dataset, annotation guidelines, and baseline model for delicate text detection.</abstract>
      <url hash="3a69d371">2023.woah-1.2</url>
      <bibkey>chernodub-etal-2023-detexd</bibkey>
      <doi>10.18653/v1/2023.woah-1.2</doi>
    </paper>
    <paper id="3">
      <title>Towards Safer Communities: Detecting Aggression and Offensive Language in Code-Mixed Tweets to Combat Cyberbullying</title>
      <author><first>Nazia</first><last>Nafis</last><affiliation>Indian Institute of Information Technology, Lucknow</affiliation></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Naveen</first><last>Saini</last><affiliation>Indian Institute of Information Technology, Lucknow</affiliation></author>
      <author><first>Rudra</first><last>Murthy</last><affiliation>IBM India Research Limited</affiliation></author>
      <pages>29-41</pages>
      <abstract>Cyberbullying is a serious societal issue widespread on various channels and platforms, particularly social networking sites. Such platforms have proven to be exceptionally fertile grounds for such behavior. The dearth of high-quality training data for multilingual and low-resource scenarios, data that can accurately capture the nuances of social media conversations, often poses a roadblock to this task. This paper attempts to tackle cyberbullying, specifically its two most common manifestations - aggression and offensiveness. We present a novel, manually annotated dataset of a total of 10,000 English and Hindi-English code-mixed tweets, manually annotated for aggression detection and offensive language detection tasks. Our annotations are supported by inter-annotator agreement scores of 0.67 and 0.74 for the two tasks, indicating substantial agreement. We perform comprehensive fine-tuning of pre-trained language models (PTLMs) using this dataset to check its efficacy. Our challenging test sets show that the best models achieve macro F1-scores of 67.87 and 65.45 on the two tasks, respectively. Further, we perform cross-dataset transfer learning to benchmark our dataset against existing aggression and offensive language datasets. We also present a detailed quantitative and qualitative analysis of errors in prediction, and with this paper, we publicly release the novel dataset, code, and models.</abstract>
      <url hash="d23dcdd2">2023.woah-1.3</url>
      <bibkey>nafis-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.woah-1.3</doi>
    </paper>
    <paper id="4">
      <title>Towards Weakly-Supervised Hate Speech Classification Across Datasets</title>
      <author><first>Yiping</first><last>Jin</last><affiliation>Pompeu Fabra University</affiliation></author>
      <author><first>Leo</first><last>Wanner</last><affiliation>ICREA and Pompeu Fabra University</affiliation></author>
      <author><first>Vishakha</first><last>Kadam</last><affiliation>Knorex</affiliation></author>
      <author><first>Alexander</first><last>Shvets</last><affiliation>Pompeu Fabra University</affiliation></author>
      <pages>42-59</pages>
      <abstract>As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.</abstract>
      <url hash="ba926d65">2023.woah-1.4</url>
      <bibkey>jin-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.woah-1.4</doi>
    </paper>
    <paper id="6">
      <title>Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech</title>
      <author><first>Flor Miriam</first><last>Plaza-del-arco</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>60-68</pages>
      <abstract>Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.</abstract>
      <url hash="e1105ee5">2023.woah-1.6</url>
      <bibkey>plaza-del-arco-etal-2023-respectful</bibkey>
      <doi>10.18653/v1/2023.woah-1.6</doi>
    </paper>
    <paper id="7">
      <title>Benchmarking Offensive and Abusive Language in <fixed-case>D</fixed-case>utch Tweets</title>
      <author><first>Tommaso</first><last>Caselli</last><affiliation>Rijksuniversiteit Groningen</affiliation></author>
      <author><first>Hylke</first><last>Van Der Veen</last><affiliation>University of Groningen</affiliation></author>
      <pages>69-84</pages>
      <abstract>We present an extensive evaluation of different fine-tuned models to detect instances of offensive and abusive language in Dutch across three benchmarks: a standard held-out test, a task- agnostic functional benchmark, and a dynamic test set. We also investigate the use of data cartography to identify high quality training data. Our results show a relatively good quality of the manually annotated data used to train the models while highlighting some critical weakness. We have also found a good portability of trained models along the same language phenomena. As for the data cartography, we have found a positive impact only on the functional benchmark and when selecting data per annotated dimension rather than using the entire training material.</abstract>
      <url hash="f8588642">2023.woah-1.7</url>
      <bibkey>caselli-van-der-veen-2023-benchmarking</bibkey>
      <doi>10.18653/v1/2023.woah-1.7</doi>
    </paper>
    <paper id="8">
      <title>Relationality and Offensive Speech: A Research Agenda</title>
      <author><first>Razvan</first><last>Amironesei</last><affiliation>Independent</affiliation></author>
      <author><first>Mark</first><last>Diaz</last><affiliation>Google</affiliation></author>
      <pages>85-95</pages>
      <abstract>We draw from the framework of relationality as a pathway for modeling social relations to address gaps in text classification, generally, and offensive language classification, specifically. We use minoritized language, such as queer speech, to motivate a need for understanding and modeling social relations–both among individuals and among their social communities. We then point to socio-ethical style as a research area for inferring and measuring social relations as well as propose additional questions to structure future research on operationalizing social context.</abstract>
      <url hash="4c0784af">2023.woah-1.8</url>
      <bibkey>amironesei-diaz-2023-relationality</bibkey>
      <doi>10.18653/v1/2023.woah-1.8</doi>
    </paper>
    <paper id="9">
      <title>Cross-Platform and Cross-Domain Abusive Language Detection with Supervised Contrastive Learning</title>
      <author><first>Md Tawkat Islam</first><last>Khondaker</last><affiliation>The University of British Columbia</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-mageed</last><affiliation>The University of British Columbia</affiliation></author>
      <author><first>Laks</first><last>Lakshmanan, V.s.</last><affiliation>UBC</affiliation></author>
      <pages>96-112</pages>
      <abstract>The prevalence of abusive language on different online platforms has been a major concern that raises the need for automated cross-platform abusive language detection. However, prior works focus on concatenating data from multiple platforms, inherently adopting Empirical Risk Minimization (ERM) method. In this work, we address this challenge from the perspective of domain generalization objective. We design SCL-Fish, a supervised contrastive learning integrated meta-learning algorithm to detect abusive language on unseen platforms. Our experimental analysis shows that SCL-Fish achieves better performance over ERM and the existing state-of-the-art models. We also show that SCL-Fish is data-efficient and achieves comparable performance with the large-scale pre-trained models upon finetuning for the abusive language detection task.</abstract>
      <url hash="d8661f7b">2023.woah-1.9</url>
      <bibkey>khondaker-etal-2023-cross</bibkey>
      <doi>10.18653/v1/2023.woah-1.9</doi>
    </paper>
    <paper id="12">
      <title>Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor</title>
      <author><first>Svetlana</first><last>Kiritchenko</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Georgina</first><last>Curto Rex</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Isar</first><last>Nejadgholi</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Kathleen C.</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>113-125</pages>
      <abstract>While many types of hate speech and online toxicity have been the focus of extensive research in NLP, toxic language stigmatizing poor people has been mostly disregarded. Yet, aporophobia, a social bias against the poor, is a common phenomenon online, which can be psychologically damaging as well as hindering poverty reduction policy measures. We demonstrate that aporophobic attitudes are indeed present in social media and argue that the existing NLP datasets and models are inadequate to effectively address this problem. Efforts toward designing specialized resources and novel socio-technical mechanisms for confronting aporophobia are needed.</abstract>
      <url hash="a53849d9">2023.woah-1.12</url>
      <bibkey>kiritchenko-etal-2023-aporophobia</bibkey>
      <doi>10.18653/v1/2023.woah-1.12</doi>
    </paper>
    <paper id="13">
      <title>Problematic Webpage Identification: A Trilogy of Hatespeech, Search Engines and <fixed-case>GPT</fixed-case></title>
      <author><first>Ojasvin</first><last>Sood</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sandipan</first><last>Dandapat</last><affiliation>Microsoft</affiliation></author>
      <pages>126-137</pages>
      <abstract>In this paper, we introduce a fine-tuned transformer-based model focused on problematic webpage classification to identify webpages promoting hate and violence of various forms. Due to the unavailability of labelled problematic webpage data, first we propose a novel webpage data collection strategy which leverages well-studied short-text hate speech datasets. We have introduced a custom GPT-4 few-shot prompt annotation scheme taking various webpage features to label the prohibitively expensive webpage annotation task. The resulting annotated data is used to build our problematic webpage classification model. We report the accuracy (87.6% F1-score) of our webpage classification model and conduct a detailed comparison of it against other state-of-the-art hate speech classification model on problematic webpage identification task. Finally, we have showcased the importance of various webpage features in identifying a problematic webpage.</abstract>
      <url hash="b16cc094">2023.woah-1.13</url>
      <bibkey>sood-dandapat-2023-problematic</bibkey>
      <doi>10.18653/v1/2023.woah-1.13</doi>
    </paper>
    <paper id="14">
      <title>Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers</title>
      <author><first>Isar</first><last>Nejadgholi</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Svetlana</first><last>Kiritchenko</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Kathleen C.</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Esma</first><last>Balkir</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>138-149</pages>
      <abstract>Classifiers tend to learn a false causal relationship between an over-represented concept and a label, which can result in over-reliance on the concept and compromised classification accuracy. It is imperative to have methods in place that can compare different models and identify over-reliances on specific concepts. We consider three well-known abusive language classifiers trained on large English datasets and focus on the concept of negative emotions, which is an important signal but should not be learned as a sufficient feature for the label of abuse. Motivated by the definition of global sufficiency, we first examine the unwanted dependencies learned by the classifiers by assessing their accuracy on a challenge set across all decision thresholds. Further, recognizing that a challenge set might not always be available, we introduce concept-based explanation metrics to assess the influence of the concept on the labels. These explanations allow us to compare classifiers regarding the degree of false global sufficiency they have learned between a concept and a label.</abstract>
      <url hash="a95e2428">2023.woah-1.14</url>
      <bibkey>nejadgholi-etal-2023-concept</bibkey>
      <doi>10.18653/v1/2023.woah-1.14</doi>
    </paper>
    <paper id="15">
      <title>“Female Astronaut: Because sandwiches won’t make themselves up there”: Towards Multimodal misogyny detection in memes</title>
      <author><first>Smriti</first><last>Singh</last><affiliation>Manipal Institute of Technology</affiliation></author>
      <author><first>Amritha</first><last>Haridasan</last><affiliation>The University of Texas at Austin</affiliation></author>
      <author><first>Raymond</first><last>Mooney</last><affiliation>University of Texas at Austin</affiliation></author>
      <pages>150-159</pages>
      <abstract>A rise in the circulation of memes has led to the spread of a new form of multimodal hateful content. Unfortunately, the degree of hate women receive on the internet is disproportionately skewed against them. This, combined with the fact that multimodal misogyny is more challenging to detect as opposed to traditional text-based misogyny, signifies that the task of identifying misogynistic memes online is one of utmost importance. To this end, the MAMI dataset was released, consisting of 12000 memes annotated for misogyny and four sub-classes of misogyny - shame, objectification, violence and stereotype. While this balanced dataset is widely cited, we find that the task itself remains largely unsolved. Thus, in our work, we investigate the performance of multiple models in an effort to analyse whether domain specific pretraining helps model performance. We also investigate why even state of the art models find this task so challenging, and whether domain-specific pretraining can help. Our results show that pretraining BERT on hateful memes and leveraging an attention based approach with ViT outperforms state of the art models by more than 10%. Further, we provide insight into why these models may be struggling with this task with an extensive qualitative analysis of random samples from the test set.</abstract>
      <url hash="a94181f0">2023.woah-1.15</url>
      <bibkey>singh-etal-2023-female</bibkey>
      <doi>10.18653/v1/2023.woah-1.15</doi>
    </paper>
    <paper id="16">
      <title>Conversation Derailment Forecasting with Graph Convolutional Networks</title>
      <author><first>Enas</first><last>Altarawneh</last><affiliation>York University</affiliation></author>
      <author><first>Ameeta</first><last>Agrawal</last><affiliation>Portland State University</affiliation></author>
      <author><first>Michael</first><last>Jenkin</last><affiliation>York University</affiliation></author>
      <author><first>Manos</first><last>Papagelis</last><affiliation>York University</affiliation></author>
      <pages>160-169</pages>
      <url hash="4bc904cc">2023.woah-1.16</url>
      <bibkey>altarawneh-etal-2023-conversation</bibkey>
      <abstract>Online conversations are particularly susceptible to derailment, which can manifest itself in the form of toxic communication patterns like disrespectful comments or verbal abuse. Forecasting conversation derailment predicts signs of derailment in advance enabling proactive moderation of conversations. Current state-of-the-art approaches to address this problem rely on sequence models that treat dialogues as text streams. We propose a novel model based on a graph convolutional neural network that considers dialogue user dynamics and the influence of public perception on conversation utterances. Through empirical evaluation, we show that our model effectively captures conversation dynamics and outperforms the state-of-the-art models on the CGA and CMV benchmark datasets by 1.5\% and 1.7\%, respectively.</abstract>
      <doi>10.18653/v1/2023.woah-1.16</doi>
    </paper>
    <paper id="17">
      <title>Resources for Automated Identification of Online Gender-Based Violence: A Systematic Review</title>
      <author><first>Gavin</first><last>Abercrombie</last><affiliation>Heriot Watt University</affiliation></author>
      <author><first>Aiqi</first><last>Jiang</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Poppy</first><last>Gerrard-abbott</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Ioannis</first><last>Konstas</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Verena</first><last>Rieser</last><affiliation>Heriot-Watt University</affiliation></author>
      <pages>170-186</pages>
      <abstract>Online Gender-Based Violence (GBV), such as misogynistic abuse is an increasingly prevalent problem that technological approaches have struggled to address. Through the lens of the GBV framework, which is rooted in social science and policy, we systematically review 63 available resources for automated identification of such language. We find the datasets are limited in a number of important ways, such as their lack of theoretical grounding and stakeholder input, static nature, and focus on certain media platforms. Based on this review, we recommend development of future resources rooted in sociological expertise andcentering stakeholder voices, namely GBV experts and people with lived experience of GBV.</abstract>
      <url hash="93e624cf">2023.woah-1.17</url>
      <bibkey>abercrombie-etal-2023-resources</bibkey>
      <doi>10.18653/v1/2023.woah-1.17</doi>
    </paper>
    <paper id="19">
      <title>Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages with Limited Labeled Data</title>
      <author><first>Janis</first><last>Goldzycher</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Moritz</first><last>Preisig</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Chantal</first><last>Amrhein</last><affiliation>Department of Computational Linguistics, University of Zurich</affiliation></author>
      <author><first>Gerold</first><last>Schneider</last><affiliation>University of Zurich</affiliation></author>
      <pages>187-201</pages>
      <abstract>Most research on hate speech detection has focused on English where a sizeable amount of labeled training data is available. However, to expand hate speech detection into more languages, approaches that require minimal training data are needed. In this paper, we test whether natural language inference (NLI) models which perform well in zero- and few-shot settings can benefit hate speech detection performance in scenarios where only a limited amount of labeled data is available in the target language. Our evaluation on five languages demonstrates large performance improvements of NLI fine-tuning over direct fine-tuning in the target language. However, the effectiveness of previous work that proposed intermediate fine-tuning on English data is hard to match. Only in settings where the English training data does not match the test domain, can our customised NLI-formulation outperform intermediate fine-tuning on English. Based on our extensive experiments, we propose a set of recommendations for hate speech detection in languages where minimal labeled training data is available.</abstract>
      <url hash="7715c6ed">2023.woah-1.19</url>
      <bibkey>goldzycher-etal-2023-evaluating</bibkey>
      <doi>10.18653/v1/2023.woah-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>HOMO</fixed-case>-<fixed-case>MEX</fixed-case>: A <fixed-case>M</fixed-case>exican <fixed-case>S</fixed-case>panish Annotated Corpus for <fixed-case>LGBT</fixed-case>+phobia Detection on <fixed-case>T</fixed-case>witter</title>
      <author><first>Juan</first><last>Vásquez</last><affiliation>Posgrado en Ciencia e Ingeniería de la Computación. Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Scott</first><last>Andersen</last><affiliation>Posgrado en Ciencia e Ingeniería de la Computación. Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Gemma</first><last>Bel-enguix</last><affiliation>Instituto de Ingeniería. Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Helena</first><last>Gómez-adorno</last><affiliation>Instituto de Investigaciones en Matemáticas Aplicadas y en Sistemas. Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Sergio-luis</first><last>Ojeda-trueba</last><affiliation>Instituto de Ingeniería. Universidad Nacional Autónoma de México</affiliation></author>
      <pages>202-214</pages>
      <abstract>In the past few years, the NLP community has actively worked on detecting LGBT+Phobia in online spaces, using textual data publicly available Most of these are for the English language and its variants since it is the most studied language by the NLP community. Nevertheless, efforts towards creating corpora in other languages are active worldwide. Despite this, the Spanish language is an understudied language regarding digital LGBT+Phobia. The only corpus we found in the literature was for the Peninsular Spanish dialects, which use LGBT+phobic terms different than those in the Mexican dialect. For this reason, we present Homo-MEX, a novel corpus for detecting LGBT+Phobia in Mexican Spanish. In this paper, we describe our data-gathering and annotation process. Also, we present a classification benchmark using various traditional machine learning algorithms and two pre-trained deep learning models to showcase our corpus classification potential.</abstract>
      <url hash="ed3c6658">2023.woah-1.20</url>
      <bibkey>vasquez-etal-2023-homo</bibkey>
      <doi>10.18653/v1/2023.woah-1.20</doi>
    </paper>
    <paper id="21">
      <title>Factoring Hate Speech: A New Annotation Framework to Study Hate Speech in Social Media</title>
      <author><first>Gal</first><last>Ron</last><affiliation>Department of Political Science, The Hebrew University of Jerusalem</affiliation></author>
      <author><first>Effi</first><last>Levi</last><affiliation>The Hebrew University of Jerusalem</affiliation></author>
      <author><first>Odelia</first><last>Oshri</last><affiliation>Department of Political Science, The Hebrew University of Jerusalem</affiliation></author>
      <author><first>Shaul</first><last>Shenhav</last><affiliation>Department of Political Science, The Hebrew University of Jerusalem</affiliation></author>
      <pages>215-220</pages>
      <abstract>In this work we propose a novel annotation scheme which factors hate speech into five separate discursive categories. To evaluate our scheme, we construct a corpus of over 2.9M Twitter posts containing hateful expressions directed at Jews, and annotate a sample dataset of 1,050 tweets. We present a statistical analysis of the annotated dataset as well as discuss annotation examples, and conclude by discussing promising directions for future work.</abstract>
      <url hash="fa493a88">2023.woah-1.21</url>
      <bibkey>ron-etal-2023-factoring</bibkey>
      <doi>10.18653/v1/2023.woah-1.21</doi>
    </paper>
    <paper id="24">
      <title>Harmful Language Datasets: An Assessment of Robustness</title>
      <author><first>Katerina</first><last>Korre</last><affiliation>University of Bologna</affiliation></author>
      <author><first>John</first><last>Pavlopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Jeffrey</first><last>Sorensen</last><affiliation>Google Jigsaw</affiliation></author>
      <author><first>Léo</first><last>Laugier</last><affiliation>Swiss Federal Institute of Technology of Lausanne</affiliation></author>
      <author><first>Ion</first><last>Androutsopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Lucas</first><last>Dixon</last><affiliation>Google Research</affiliation></author>
      <author><first>Alberto</first><last>Barrón-cedeño</last><affiliation>Università di Bologna</affiliation></author>
      <pages>221-230</pages>
      <abstract>The automated detection of harmful language has been of great importance for the online world, especially with the growing importance of social media and, consequently, polarisation. There are many open challenges to high quality detection of harmful text, from dataset creation to generalisable application, thus calling for more systematic studies. In this paper, we explore re-annotation as a means of examining the robustness of already existing labelled datasets, showing that, despite using alternative definitions, the inter-annotator agreement remains very inconsistent, highlighting the intrinsically subjective and variable nature of the task. In addition, we build automatic toxicity detectors using the existing datasets, with their original labels, and we evaluate them on our multi-definition and multi-source datasets. Surprisingly, while other studies show that hate speech detection models perform better on data that are derived from the same distribution as the training set, our analysis demonstrates this is not necessarily true.</abstract>
      <url hash="1335fa92">2023.woah-1.24</url>
      <bibkey>korre-etal-2023-harmful</bibkey>
      <doi>10.18653/v1/2023.woah-1.24</doi>
    </paper>
    <paper id="25">
      <title>Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation</title>
      <author><first>Dimosthenis</first><last>Antypas</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jose</first><last>Camacho-Collados</last><affiliation>Cardiff University</affiliation></author>
      <pages>231-242</pages>
      <abstract>The automatic detection of hate speech online is an active research area in NLP. Most of the studies to date are based on social media datasets that contribute to the creation of hate speech detection models trained on them. However, data creation processes contain their own biases, and models inherently learn from these dataset-specific biases. In this paper, we perform a large-scale cross-dataset comparison where we fine-tune language models on different hate speech detection datasets. This analysis shows how some datasets are more generalizable than others when used as training data. Crucially, our experiments show how combining hate speech detection datasets can contribute to the development of robust hate speech detection models. This robustness holds even when controlling by data size and compared with the best individual datasets.</abstract>
      <url hash="9b8b4808">2023.woah-1.25</url>
      <bibkey>antypas-camacho-collados-2023-robust</bibkey>
      <doi>10.18653/v1/2023.woah-1.25</doi>
    </paper>
  </volume>
</collection>
