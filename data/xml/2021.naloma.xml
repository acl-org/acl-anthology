<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.naloma">
  <volume id="1" ingest-date="2021-10-27" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)</booktitle>
      <editor><first>Aikaterini-Lida</first><last>Kalouli</last></editor>
      <editor><first>Lawrence S.</first><last>Moss</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Groningen, the Netherlands (online)</address>
      <month>June</month>
      <year>2021</year>
      <url hash="68bf91c3">2021.naloma-1</url>
      <venue>naloma</venue>
    </meta>
    <frontmatter>
      <url hash="b8097cd2">2021.naloma-1.0</url>
      <bibkey>naloma-2021-workshops</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Learning General Event Schemas with Episodic Logic</title>
      <author><first>Lane</first><last>Lawley</last></author>
      <author><first>Benjamin</first><last>Kuehnert</last></author>
      <author><first>Lenhart</first><last>Schubert</last></author>
      <pages>1–6</pages>
      <abstract>We present a system for learning generalized, stereotypical patterns of events—or “schemas”—from natural language stories, and applying them to make predictions about other stories. Our schemas are represented with Episodic Logic, a logical form that closely mirrors natural language. By beginning with a “head start” set of protoschemas— schemas that a 1- or 2-year-old child would likely know—we can obtain useful, general world knowledge with very few story examples—often only one or two. Learned schemas can be combined into more complex, composite schemas, and used to make predictions in other stories where only partial information is available.</abstract>
      <url hash="6b3c13b6">2021.naloma-1.1</url>
      <bibkey>lawley-etal-2021-learning</bibkey>
    </paper>
    <paper id="2">
      <title>Applied Medical Code Mapping with Character-based Deep Learning Models and Word-based Logic</title>
      <author><first>John</first><last>Langton</last></author>
      <author><first>Krishna</first><last>Srihasam</last></author>
      <pages>7–11</pages>
      <abstract>Logical Observation Identifiers Names and Codes (LOINC) is a standard set of codes that enable clinicians to communicate about medical tests. Laboratories depend on LOINC to identify what tests a doctor orders for a patient. However, clinicians often use site specific, custom codes in their medical records systems that can include shorthand, spelling mistakes, and invented acronyms. Software solutions must map from these custom codes to the LOINC standard to support data interoperability. A key challenge is that LOINC is comprised of six elements. Mapping requires not only extracting those elements, but also combining them according to LOINC logic. We found that character-based deep learning excels at extracting LOINC elements while logic based methods are more effective for combining those elements into complete LOINC values. In this paper, we present an ensemble of machine learning and logic that is currently used in several medical facilities to map from</abstract>
      <url hash="a0ba1937">2021.naloma-1.2</url>
      <bibkey>langton-srihasam-2021-applied</bibkey>
    </paper>
    <paper id="3">
      <title>Attentive Tree-structured Network for Monotonicity Reasoning</title>
      <author><first>Zeming</first><last>Chen</last></author>
      <pages>12–21</pages>
      <abstract>Many state-of-art neural models designed for monotonicity reasoning perform poorly on downward inference. To address this shortcoming, we developed an attentive tree-structured neural network. It consists of a tree-based long-short-term-memory network (Tree-LSTM) with soft attention. It is designed to model the syntactic parse tree information from the sentence pair of a reasoning task. A self-attentive aggregator is used for aligning the representations of the premise and the hypothesis. We present our model and evaluate it using the Monotonicity Entailment Dataset (MED). We show and attempt to explain that our model outperforms existing models on MED.</abstract>
      <url hash="cfbaf3f9">2021.naloma-1.3</url>
      <bibkey>chen-2021-attentive</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/help">HELP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/med">MED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="4">
      <title>Transferring Representations of Logical Connectives</title>
      <author><first>Aaron</first><last>Traylor</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <author><first>Roman</first><last>Feiman</last></author>
      <pages>22–25</pages>
      <abstract>In modern natural language processing pipelines, it is common practice to “pretrain” a generative language model on a large corpus of text, and then to “finetune” the created representations by continuing to train them on a discriminative textual inference task. However, it is not immediately clear whether the logical meaning necessary to model logical entailment is captured by language models in this paradigm. We examine this pretrain-finetune recipe with language models trained on a synthetic propositional language entailment task, and present results on test sets probing models’ knowledge of axioms of first order logic.</abstract>
      <url hash="8ba0ccc2">2021.naloma-1.4</url>
      <bibkey>traylor-etal-2021-transferring</bibkey>
    </paper>
    <paper id="5">
      <title>Monotonic Inference for Underspecified Episodic Logic</title>
      <author><first>Gene</first><last>Kim</last></author>
      <author><first>Mandar</first><last>Juvekar</last></author>
      <author><first>Lenhart</first><last>Schubert</last></author>
      <pages>26–40</pages>
      <abstract>We present a method of making natural logic inferences from Unscoped Logical Form of Episodic Logic. We establish a correspondence between inference rules of scope resolved Episodic Logic and the natural logic treatment by Sánchez Valencia (1991a), and hence demonstrate the ability to handle foundational natural logic inferences from prior literature as well as more general nested monotonicity inferences.</abstract>
      <url hash="7d7fd64d">2021.naloma-1.5</url>
      <bibkey>kim-etal-2021-monotonic</bibkey>
    </paper>
    <paper id="6">
      <title>Supporting Context Monotonicity Abstractions in Neural <fixed-case>NLI</fixed-case> Models</title>
      <author><first>Julia</first><last>Rozanova</last></author>
      <author><first>Deborah</first><last>Ferreira</last></author>
      <author><first>Mokanarangan</first><last>Thayaparan</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <pages>41–50</pages>
      <abstract>Natural language contexts display logical regularities with respect to substitutions of related concepts: these are captured in a functional order-theoretic property called monotonicity. For a certain class of NLI problems where the resulting entailment label depends only on the context monotonicity and the relation between the substituted concepts, we build on previous techniques that aim to improve the performance of NLI models for these problems, as consistent performance across both upward and downward monotone contexts still seems difficult to attain even for state of the art models. To this end, we reframe the problem of context monotonicity classification to make it compatible with transformer-based pre-trained NLI models and add this task to the training pipeline. Furthermore, we introduce a sound and complete simplified monotonicity logic formalism which describes our treatment of contexts as abstract units. Using the notions in our formalism, we adapt targeted challenge sets to investigate whether an intermediate context monotonicity classification task can aid NLI models’ performance on examples exhibiting monotonicity reasoning.</abstract>
      <url hash="5fa4866f">2021.naloma-1.6</url>
      <bibkey>rozanova-etal-2021-supporting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/help">HELP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>B</fixed-case>ayesian Classification and Inference in a Probabilistic Type Theory with Records</title>
      <author><first>Staffan</first><last>Larsson</last></author>
      <author><first>Robin</first><last>Cooper</last></author>
      <pages>51–59</pages>
      <abstract>We propose a probabilistic account of semantic inference and classification formulated in terms of probabilistic type theory with records, building on Cooper et. al. (2014) and Cooper et. al. (2015). We suggest probabilistic type theoretic formulations of Naive Bayes Classifiers and Bayesian Networks. A central element of these constructions is a type-theoretic version of a random variable. We illustrate this account with a simple language game combining probabilistic classification of perceptual input with probabilistic (semantic) inference.</abstract>
      <url hash="6d329532">2021.naloma-1.7</url>
      <bibkey>larsson-cooper-2021-bayesian</bibkey>
    </paper>
    <paper id="8">
      <title>From compositional semantics to <fixed-case>B</fixed-case>ayesian pragmatics via logical inference</title>
      <author><first>Julian</first><last>Grove</last></author>
      <author><first>Jean-Philippe</first><last>Bernardy</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
      <pages>60–70</pages>
      <abstract>Formal semantics in the Montagovian tradition provides precise meaning characterisations, but usually without a formal theory of the pragmatics of contextual parameters and their sensitivity to background knowledge. Meanwhile, formal pragmatic theories make explicit predictions about meaning in context, but generally without a well-defined compositional semantics. We propose a combined framework for the semantic and pragmatic interpretation of sentences in the face of probabilistic knowledge. We do so by (1) extending a Montagovian interpretation scheme to generate a distribution over possible meanings, and (2) generating a posterior for this distribution using a variant of the Rational Speech Act (RSA) models, but generalised to arbitrary propositions. These aspects of our framework are tied together by evaluating entailment under probabilistic uncertainty. We apply our model to anaphora resolution and show that it provides expected biases under suitable assumptions about the distributions of lexical and world-knowledge. Further, we observe that the model’s output is robust to variations in its parameters within reasonable ranges.</abstract>
      <url hash="cfb614c9">2021.naloma-1.8</url>
      <bibkey>grove-etal-2021-compositional</bibkey>
    </paper>
    <paper id="9">
      <title>A (Mostly) Symbolic System for Monotonic Inference with Unscoped Episodic Logical Forms</title>
      <author><first>Gene</first><last>Kim</last></author>
      <author><first>Mandar</first><last>Juvekar</last></author>
      <author><first>Junis</first><last>Ekmekciu</last></author>
      <author><first>Viet</first><last>Duong</last></author>
      <author><first>Lenhart</first><last>Schubert</last></author>
      <pages>71–80</pages>
      <abstract>We implement the formalization of natural logic-like monotonic inference using Unscoped Episodic Logical Forms (ULFs) by Kim et al. (2020). We demonstrate this system’s capacity to handle a variety of challenging semantic phenomena using the FraCaS dataset (Cooper et al., 1996). These results give empirical evidence for prior claims that ULF is an appropriate representation to mediate natural logic-like inferences.</abstract>
      <url hash="a0604bc3">2021.naloma-1.9</url>
      <attachment type="Attachment" hash="079dbfba">2021.naloma-1.9.Attachment.zip</attachment>
      <bibkey>kim-etal-2021-mostly</bibkey>
    </paper>
  </volume>
</collection>
