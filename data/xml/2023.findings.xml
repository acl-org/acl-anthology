<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.findings">
  <volume id="eacl" ingest-date="2023-05-03">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: EACL 2023</booktitle>
      <editor><first>Andreas</first><last>Vlachos</last></editor>
      <editor><first>Isabelle</first><last>Augenstein</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dubrovnik, Croatia</address>
      <month>May</month>
      <year>2023</year>
      <url hash="f528210f">2023.findings-eacl</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="839d7a17">2023.findings-eacl.0</url>
      <bibkey>findings-2023-findings</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Using Punctuation as an Adversarial Attack on Deep Learning-Based <fixed-case>NLP</fixed-case> Systems: An Empirical Study</title>
      <author><first>Brian</first><last>Formento</last><affiliation>Nus</affiliation></author>
      <author><first>Chuan Sheng</first><last>Foo</last><affiliation>Institute for Infocomm Research</affiliation></author>
      <author><first>Luu Anh</first><last>Tuan</last><affiliation>Nanyang Technological University, Singapore</affiliation></author>
      <author><first>See Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>1-34</pages>
      <abstract>This work empirically investigates punctuation insertions as adversarial attacks on NLP systems. Data from experiments on three tasks, five datasets, and six models with four attacks show that punctuation insertions, when limited to a few symbols (apostrophes and hyphens), are a superior attack vector compared to character insertions due to 1) a lower after-attack accuracy ($A_{aft-atk}$) than alphabetical character insertions; 2) higher semantic similarity between the resulting and original texts; and 3) a resulting text that is easier and faster to read as assessed with the Test of Word Reading Efficiency (TOWRE)). The tests also indicate that 4) grammar checking does not mitigate punctuation insertions and 5) punctuation insertions outperform word-level attacks in settings with a limited number of word synonyms and queries to the victim’s model. Our findings indicate that inserting a few punctuation types that result in easy-to-read samples is a general attack mechanism. In light of this threat, we assess the impact of punctuation insertions, potential mitigations, the mitigation’s tradeoffs, punctuation insertion’s worst-case scenarios and summarize our findings in a qualitative casual map, so that developers can design safer, more secure systems.</abstract>
      <url hash="61152bda">2023.findings-eacl.1</url>
      <attachment type="software" hash="a8ea7241">2023.findings-eacl.1.software.zip</attachment>
      <bibkey>formento-etal-2023-using</bibkey>
    </paper>
    <paper id="2">
      <title>Self-Supervised Unimodal Label Generation Strategy Using Recalibrated Modality Representations for Multimodal Sentiment Analysis</title>
      <author><first>Yewon</first><last>Hwang</last><affiliation>The Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Jong-Hwan</first><last>Kim</last><affiliation>The Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>35-46</pages>
      <abstract>While multimodal sentiment analysis (MSA) has gained much attention over the last few years, the main focus of most work on MSA has been limited to constructing multimodal representations that capture interactions between different modalities in a single task. This was largely due to a lack of unimodal annotations in MSA benchmark datasets. However, training a model using only multimodal representations can lead to suboptimal performance due to insufficient learning of each uni-modal representation. In this work, to fully optimize learning representations from multimodal data, we propose SUGRM which jointly trains multimodal and unimodal tasks using recalibrated features. The features are recalibrated such that the model learns to weight the features differently based on the features of other modalities. Further, to leverage unimodal tasks, we auto-generate unimodal annotations via a unimodal label generation module (ULGM). The experiment results on two benchmark datasets demonstrate the efficacy of our framework.</abstract>
      <url hash="25b2427a">2023.findings-eacl.2</url>
      <bibkey>hwang-kim-2023-self</bibkey>
    </paper>
    <paper id="3">
      <title>Fighting <fixed-case>FIR</fixed-case>e with <fixed-case>FIRE</fixed-case>: Assessing the Validity of Text-to-Video Retrieval Benchmarks</title>
      <author><first>Pedro</first><last>Rodriguez</last><affiliation>Meta FAIR</affiliation></author>
      <author><first>Mahmoud</first><last>Azab</last><affiliation>Meta Reality Labs</affiliation></author>
      <author><first>Becka</first><last>Silvert</last><affiliation>Facebook</affiliation></author>
      <author><first>Renato</first><last>Sanchez</last><affiliation>Meta</affiliation></author>
      <author><first>Linzy</first><last>Labson</last><affiliation>Meta</affiliation></author>
      <author><first>Hardik</first><last>Shah</last><affiliation>Meta Inc</affiliation></author>
      <author><first>Seungwhan</first><last>Moon</last><affiliation>Meta Reality Labs</affiliation></author>
      <pages>47-68</pages>
      <abstract>Searching troves of videos with textual descriptions is a core multimodal retrieval task. Owing to the lack of a purpose-built dataset for text-to-video retrieval, video captioning datasets have been re-purposed to evaluate models by (1) treating captions as positive matches to their respective videos and (2) assuming all other videos to be negatives. However, this methodology leads to a fundamental flaw during evaluation: since captions are marked as relevant only to their original video, many alternate videos also match the caption, which introduces false-negative caption-video pairs. We show that when these false negatives are corrected, a recent state-of-the-art model gains 25% recall points—a difference that threatens the validity of the benchmark itself. To diagnose and mitigate this issue, we annotate and release 683K additional caption-video pairs. Using these, we recompute effectiveness scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find that (1) the recomputed metrics are up to 25% recall points higher for the best models, (2) these benchmarks are nearing saturation for Recall@10, (3) caption length (generality) is related to the number of positives, and (4) annotation costs can be mitigated through sampling. We recommend retiring these benchmarks in their current form, and we make recommendations for future text-to-video retrieval benchmarks.</abstract>
      <url hash="2ef1b775">2023.findings-eacl.3</url>
      <attachment type="dataset" hash="a5dfb508">2023.findings-eacl.3.dataset.zip</attachment>
      <bibkey>rodriguez-etal-2023-fighting</bibkey>
    </paper>
    <paper id="4">
      <title>Improving Numeracy by Input Reframing and Quantitative Pre-Finetuning Task</title>
      <author><first>Chung-Chi</first><last>Chen</last><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>The National Institute of Advanced Industrial Science and Technology (AIST)</affiliation></author>
      <author><first>Ichiro</first><last>Kobayashi</last><affiliation>Ochanomizu University</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>University of Tokyo</affiliation></author>
      <pages>69-77</pages>
      <abstract>Numbers have unique characteristics to words. Teaching models to understand numbers in text is an open-ended research question. Instead of discussing the required calculation skills, this paper focuses on a more fundamental topic: understanding numerals. We point out that innumeracy—the inability to handle basic numeral concepts—exists in most pretrained language models (LMs), and we propose a method to solve this issue by exploring the notation of numbers. Further, we discuss whether changing notation and pre-finetuning along with the comparing-number task can improve performance in three benchmark datasets containing quantitative-related tasks. The results of this study indicate that input reframing and the proposed pre-finetuning task is useful for RoBERTa.</abstract>
      <url hash="501dbfe7">2023.findings-eacl.4</url>
      <bibkey>chen-etal-2023-improving</bibkey>
    </paper>
    <paper id="5">
      <title>Visualize Before You Write: Imagination-Guided Open-Ended Text Generation</title>
      <author><first>Wanrong</first><last>Zhu</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>An</first><last>Yan</last><affiliation>University of California San Diego</affiliation></author>
      <author><first>Yujie</first><last>Lu</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Wenda</first><last>Xu</last><affiliation>University of California at Santa Barbara</affiliation></author>
      <author><first>Xin</first><last>Wang</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Miguel</first><last>Eckstein</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>Unversity of California, Santa Barbara</affiliation></author>
      <pages>78-92</pages>
      <abstract>Recent advances in text-to-image synthesis make it possible to visualize machine imaginations for a given context. On the other hand, when generating text, human writers are gifted at creative visualization, which enhances their writings by forming imaginations as blueprints before putting down the stories in words. Inspired by such a cognitive process, we ask the natural question of whether we can endow machines with the same ability to utilize visual information and construct a general picture of the context to guide text generation. In this work, we propose iNLG that uses machine-generated images to guide language models (LM) in open-ended text generation. The experiments and analyses demonstrate the effectiveness of iNLG on open-ended text generation tasks, including text completion, story generation, and concept-to-text generation in both few-shot and full-data scenarios. Both automatic metrics and human evaluations verify that the text snippets generated by our iNLG are coherent and informative while displaying minor degeneration.</abstract>
      <url hash="6a3ced3c">2023.findings-eacl.5</url>
      <bibkey>zhu-etal-2023-visualize</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>I</fixed-case>magin<fixed-case>E</fixed-case>: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation</title>
      <author><first>Wanrong</first><last>Zhu</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Xin</first><last>Wang</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>An</first><last>Yan</last><affiliation>University of California San Diego</affiliation></author>
      <author><first>Miguel</first><last>Eckstein</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>Unversity of California, Santa Barbara</affiliation></author>
      <pages>93-105</pages>
      <abstract>Automatic evaluations for natural language generation (NLG) conventionally rely on token-level or embedding-level comparisons with text references. This differs from human language processing, for which visual imagination often improves comprehension. In this work, we propose ImaginE, an imagination-based automatic evaluation metric for natural language generation. With the help of StableDiffusion, a state-of-the-art text-to-image generator, we automatically generate an image as the embodied imagination for the text snippet and compute the imagination similarity using contextual embeddings. Experiments spanning several text generation tasks demonstrate that adding machine-generated images with our ImaginE displays great potential in introducing multi-modal information into NLG evaluation, and improves existing automatic metrics’ correlations with human similarity judgments in both reference-based and reference-free evaluation scenarios.</abstract>
      <url hash="5d7a6296">2023.findings-eacl.6</url>
      <bibkey>zhu-etal-2023-imagine</bibkey>
    </paper>
    <paper id="7">
      <title>Entity-Aware Dual Co-Attention Network for Fake News Detection</title>
      <author><first>Sin-han</first><last>Yang</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chung-chi</first><last>Chen</last><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hen-Hsen</first><last>Huang</last><affiliation>Institute of Information Science, Academia Sinica</affiliation></author>
      <author><first>Hsin-Hsi</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>106-113</pages>
      <abstract>Fake news and misinformation spread rapidly on the Internet. How to identify it and how to interpret the identification results have become important issues. In this paper, we propose a Dual Co-Attention Network (Dual-CAN) for fake news detection, which takes news content, social media replies, and external knowledge into consideration. Our experimental results support that the proposed Dual-CAN outperforms current representative models in two benchmark datasets. We further make in-depth discussions by comparing how models work in both datasets with empirical analysis of attention weights.</abstract>
      <url hash="c5399170">2023.findings-eacl.7</url>
      <bibkey>yang-etal-2023-entity</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>CIKQA</fixed-case>: Learning Commonsense Inference with a Unified Knowledge-in-the-loop <fixed-case>QA</fixed-case> Paradigm</title>
      <author><first>Hongming</first><last>Zhang</last><affiliation>Tencent AI Lab, Bellevue</affiliation></author>
      <author><first>Yintong</first><last>Huo</last><affiliation>Chinese University of Hong Kong</affiliation></author>
      <author><first>Yanai</first><last>Elazar</last><affiliation>Ai2, Uw</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hkust</affiliation></author>
      <author><first>Yoav</first><last>Goldberg</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>114-124</pages>
      <abstract>We propose a new commonsense reasoning benchmark to motivate commonsense reasoning progress from two perspectives: (1) Evaluating whether models can distinguish knowledge quality by predicting if the knowledge is enough to answer the question; (2) Evaluating whether models can develop commonsense inference capabilities that generalize across tasks. We first extract supporting knowledge for each question and ask humans to annotate whether the auto-extracted knowledge is enough to answer the question or not. After that, we convert different tasks into a unified question-answering format to evaluate the models’ generalization capabilities. We name the benchmark Commonsense Inference with Knowledge-in-the-loop Question Answering ({name). Experiments show that with our learning paradigm, models demonstrate encouraging generalization capabilities. At the same time, we also notice that distinguishing knowledge quality remains challenging for current commonsense reasoning models.</abstract>
      <url hash="13d53fc8">2023.findings-eacl.8</url>
      <bibkey>zhang-etal-2023-cikqa</bibkey>
    </paper>
    <paper id="9">
      <title>Data-Efficient Methods For Improving Hate Speech Detection</title>
      <author><first>Sumegh</first><last>Roychowdhury</last><affiliation>Indian Institute of Technology , Kharagpur</affiliation></author>
      <author><first>Vikram</first><last>Gupta</last><affiliation>ShareChat, Bangalore</affiliation></author>
      <pages>125-132</pages>
      <abstract>Scarcity of large-scale datasets, especially for resource-impoverished languages motivates exploration of data-efficient methods for hate speech detection. Hateful intents are expressed explicitly (use of cuss, swear, abusive words) and implicitly (indirect and contextual). In this work, we progress implicit and explicit hate speech detection using an input-level data augmentation technique, task reformulation using entailment and cross-learning across five languages. Our proposed data augmentation technique EasyMix, improves the performance across all english datasets by ~1{% and across multilingual datasets by ~1-9{%. We also observe substantial gains of ~2-8% by reformulating hate speech detection as entail problem. We further probe the contextual models and observe that higher layers encode implicit hate while lower layers focus on explicit hate, highlighting the importance of token-level understanding for explicit and context-level for implicit hate speech detection. Code and Dataset splits - https://anonymous.4open.science/r/data_efficient_hatedetect/</abstract>
      <url hash="7158b4fa">2023.findings-eacl.9</url>
      <attachment type="dataset" hash="334bf119">2023.findings-eacl.9.dataset.zip</attachment>
      <bibkey>roychowdhury-gupta-2023-data</bibkey>
    </paper>
    <paper id="10">
      <title>Learning the Effects of Physical Actions in a Multi-modal Environment</title>
      <author><first>Gautier</first><last>Dagan</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Frank</first><last>Keller</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Alex</first><last>Lascarides</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>133-148</pages>
      <abstract>Large Language Models (LLMs) handle physical commonsense information inadequately. As a result of being trained in a disembodied setting, LLMs often fail to predict an action’s outcome in a given environment. However, predicting the effects of an action before it is executed is crucial in planning, where coherent sequences of actions are often needed to achieve a goal. Therefore, we introduce the multi-modal task of predicting the outcomes of actions solely from realistic sensory inputs (images and text). Next, we extend an LLM to model latent representations of objects to better predict action outcomes in an environment. We show that multi-modal models can capture physical commonsense when augmented with visual information. Finally, we evaluate our model’s performance on novel actions and objects and find that combining modalities help models to generalize and learn physical commonsense reasoning better.</abstract>
      <url hash="d5bfc213">2023.findings-eacl.10</url>
      <bibkey>dagan-etal-2023-learning</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>FVQA</fixed-case> 2.0: Introducing Adversarial Samples into Fact-based Visual Question Answering</title>
      <author><first>Weizhe</first><last>Lin</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Zhilin</first><last>Wang</last><affiliation>Nvidia</affiliation></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last><affiliation>University of Cambridge</affiliation></author>
      <pages>149-157</pages>
      <abstract>The widely used Fact-based Visual Question Answering (FVQA) dataset contains visually-grounded questions that require information retrieval using common sense knowledge graphs to answer. It has been observed that the original dataset is highly imbalanced and concentrated on a small portion of its associated knowledge graph. We introduce FVQA 2.0 which contains adversarial variants of test questions to address this imbalance. We show that systems trained with the original FVQA train sets can be vulnerable to adversarial samples and we demonstrate an augmentation scheme to reduce this vulnerability without human annotations.</abstract>
      <url hash="23285f32">2023.findings-eacl.11</url>
      <bibkey>lin-etal-2023-fvqa</bibkey>
    </paper>
    <paper id="12">
      <title>Revisiting Intermediate Layer Distillation for Compressing Language Models: An Overfitting Perspective</title>
      <author><first>Jongwoo</first><last>Ko</last><affiliation>Kaist</affiliation></author>
      <author><first>Seungjoon</first><last>Park</last><affiliation>Kaist</affiliation></author>
      <author><first>Minchan</first><last>Jeong</last><affiliation>Kaist</affiliation></author>
      <author><first>Sukjin</first><last>Hong</last><affiliation>Kt</affiliation></author>
      <author><first>Euijai</first><last>Ahn</last><affiliation>Kt</affiliation></author>
      <author><first>Du-Seong</first><last>Chang</last><affiliation>Kt</affiliation></author>
      <author><first>Se-Young</first><last>Yun</last><affiliation>Kaist</affiliation></author>
      <pages>158-175</pages>
      <abstract>Knowledge distillation (KD) is a highly promising method for mitigating the computational problems of pre-trained language models (PLMs). Among various KD approaches, Intermediate Layer Distillation (ILD) has been a de facto standard KD method with its performance efficacy in the NLP field. In this paper, we find that existing ILD methods are prone to overfitting to training datasets, although these methods transfer more information than the original KD. Next, we present the simple observations to mitigate the overfitting of ILD: distilling only the last Transformer layer and conducting ILD on supplementary tasks. Based on our two findings, we propose a simple yet effective consistency-regularized ILD (CR-ILD), which prevents the student model from overfitting the training dataset. Substantial experiments on distilling BERT on the GLUE benchmark and several synthetic datasets demonstrate that our proposed ILD method outperforms other KD techniques. Our code is available at https://github.com/jongwooko/CR-ILD.</abstract>
      <url hash="95695ae8">2023.findings-eacl.12</url>
      <bibkey>ko-etal-2023-revisiting</bibkey>
    </paper>
    <paper id="13">
      <title>Implicit Temporal Reasoning for Evidence-Based Fact-Checking</title>
      <author><first>Liesbeth</first><last>Allein</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Marlon</first><last>Saelens</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Ruben</first><last>Cartuyvels</last><affiliation>Catholic University of Leuven</affiliation></author>
      <author><first>Marie-Francine</first><last>Moens</last><affiliation>KU Leuven</affiliation></author>
      <pages>176-189</pages>
      <abstract>Leveraging contextual knowledge has become standard practice in automated claim verification, yet the impact of temporal reasoning has been largely overlooked. Our study demonstrates that time positively influences the claim verification process of evidence-based fact-checking. The temporal aspects and relations between claims and evidence are first established through grounding on shared timelines, which are constructed using publication dates and time expressions extracted from their text. Temporal information is then provided to RNN-based and Transformer-based classifiers before or after claim and evidence encoding. Our time-aware fact-checking models surpass base models by up to 9% Micro F1 (64.17%) and 15% Macro F1 (47.43%) on the MultiFC dataset. They also outperform prior methods that explicitly model temporal relations between evidence. Our findings show that the presence of temporal information and the manner in which timelines are constructed greatly influence how fact-checking models determine the relevance and supporting or refuting character of evidence documents.</abstract>
      <url hash="58d18fd4">2023.findings-eacl.13</url>
      <bibkey>allein-etal-2023-implicit</bibkey>
    </paper>
    <paper id="14">
      <title>Active <fixed-case>PET</fixed-case>s: Active Data Annotation Prioritisation for Few-Shot Claim Verification with Pattern Exploiting Training</title>
      <author><first>Xia</first><last>Zeng</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Arkaitz</first><last>Zubiaga</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>190-204</pages>
      <abstract>To mitigate the impact of the scarcity of labelled data on fact-checking systems, we focus on few-shot claim verification. Despite recent work on few-shot classification by proposing advanced language models, there is a dearth of research in data annotation prioritisation that improves the selection of the few shots to be labelled for optimal model performance. We propose Active PETs, a novel weighted approach that utilises an ensemble of Pattern Exploiting Training (PET) models based on various language models, to actively select unlabelled data as candidates for annotation. Using Active PETs for few-shot data selection shows consistent improvement over the baseline methods, on two technical fact-checking datasets and using six different pretrained language models. We show further improvement with Active PETs-o, which further integrates an oversampling strategy. Our approach enables effective selection of instances to be labelled where unlabelled data is abundant but resources for labelling are limited, leading to consistently improved few-shot claim verification performance. Our code is available.</abstract>
      <url hash="27abb948">2023.findings-eacl.14</url>
      <bibkey>zeng-zubiaga-2023-active</bibkey>
    </paper>
    <paper id="15">
      <title>Plan-then-Seam: Towards Efficient Table-to-Text Generation</title>
      <author><first>Liang</first><last>Li</last><affiliation>Institute of Information Engineering,Chinese Academy of Sciences</affiliation></author>
      <author><first>Ruiying</first><last>Geng</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chengyang</first><last>Fang</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Bing</first><last>Li</last><affiliation>Institute of Information Engineering,Chinese Academy of Sciences</affiliation></author>
      <author><first>Can</first><last>Ma</last><affiliation>Institute of Information Engineering, CAS</affiliation></author>
      <author><first>Binhua</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>205-219</pages>
      <abstract>Table-to-text generation aims at automatically generating text to help people conveniently obtain salient information in tables. Recent works explicitly decompose the generation process into content planning and surface generation stages, employing two autoregressive networks for them respectively.However, they are computationally expensive due to the non-parallelizable nature of autoregressive decoding and the redundant parameters of two networks.In this paper, we propose the first totally non-autoregressive table-to-text model (Plan-then-Seam, PTS) that produces its outputs in parallel with one single network.PTS firstly writes and calibrates one plan of the content to be generated with a novel rethinking pointer predictor, and then takes the plan as the context for seaming to decode the description.These two steps share parameters and perform iteratively to capture token inter-dependency while keeping parallel decoding.Experiments on two public benchmarks show that PTS achieves 3.0~5.6 times speedup for inference time, reducing 50% parameters, while maintaining as least comparable performance against strong two-stage table-to-text competitors.</abstract>
      <url hash="95e8369c">2023.findings-eacl.15</url>
      <bibkey>li-etal-2023-plan</bibkey>
    </paper>
    <paper id="16">
      <title>A corpus of metaphors as register markers</title>
      <author><first>Markus</first><last>Egg</last><affiliation>Humboldt-Universität zu Berlin</affiliation></author>
      <author><first>Valia</first><last>Kordoni</last><affiliation>Humboldt-Universität zu Berlin</affiliation></author>
      <pages>220-226</pages>
      <abstract>The paper presents our work on corpus annotationfor metaphor in German. Metaphors denoteentities that are similar to their literal referent,e.g., when *Licht* ‘light’ is used in the senseof ‘hope’. We are interested in the relation betweenmetaphor and register, hence, the corpusincludes material from different registers.We focussed on metaphors that can serve asregister markers and can also be reliably indentifiedfor annotation. Our results show hugedifferences between registers in metaphor usage,which we interpret in terms of specificproperties of the registers.</abstract>
      <url hash="c2f7b73f">2023.findings-eacl.16</url>
      <bibkey>egg-kordoni-2023-corpus</bibkey>
    </paper>
    <paper id="17">
      <title>Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing</title>
      <author><first>Francesco</first><last>Cazzaro</last><affiliation>Universitat Politecnica de Catalunya</affiliation></author>
      <author><first>Davide</first><last>Locatelli</last><affiliation>Technical University of Catalonia</affiliation></author>
      <author><first>Ariadna</first><last>Quattoni</last><affiliation>Upc, Dmetrics</affiliation></author>
      <author><first>Xavier</first><last>Carreras</last><affiliation>dMetrics</affiliation></author>
      <pages>227-238</pages>
      <abstract>Prior work in semantic parsing has shown that conventional seq2seq models fail at compositional generalization tasks. This limitation led to a resurgence of methods that model alignments between sentences and their corresponding meaning representations, either implicitly through latent variables or explicitly by taking advantage of alignment annotations. We take the second direction and propose TPol, a two-step approach that first translates input sentences monotonically and then reorders them to obtain the correct output. This is achieved with a modular framework comprising a Translator and a Reorderer component. We test our approach on two popular semantic parsing datasets. Our experiments show that by means of the monotonic translations, TPol can learn reliable lexico-logical patterns from aligned data, significantly improving compositional generalization both over conventional seq2seq models, as well as over other approaches that exploit gold alignments.</abstract>
      <url hash="0fbb59ff">2023.findings-eacl.17</url>
      <bibkey>cazzaro-etal-2023-translate</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>P</fixed-case>e<fixed-case>P</fixed-case>e: Personalized Post-editing Model utilizing User-generated Post-edits</title>
      <author><first>Jihyeon</first><last>Lee</last><affiliation>Kakaobrain</affiliation></author>
      <author><first>Taehee</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Yunwon</first><last>Tae</last><affiliation>Vuno</affiliation></author>
      <author><first>Cheonbok</first><last>Park</last><affiliation>NAVER Corp,</affiliation></author>
      <author><first>Jaegul</first><last>Choo</last><affiliation>Kaist</affiliation></author>
      <pages>239-253</pages>
      <abstract>Incorporating personal preference is crucial in advanced machine translation tasks. Despite the recent advancement of machine translation, it remains a demanding task to properly reflect personal style. In this paper, we introduce a personalized automatic post-editing framework to address this challenge, which effectively generates sentences considering distinct personal behaviors. To build this framework, we first collect post-editing data that connotes the user preference from a live machine translation system. Specifically, real-world users enter source sentences for translation and edit the machine-translated outputs according to the user’s preferred style. We then propose a model that combines a discriminator module and user-specific parameters on the APE framework. Experimental results show that the proposed method outperforms other baseline models on four different metrics (i.e., BLEU, TER, YiSi-1, and human evaluation).</abstract>
      <url hash="da05e58d">2023.findings-eacl.18</url>
      <bibkey>lee-etal-2023-pepe</bibkey>
    </paper>
    <paper id="19">
      <title>Infusing Context and Knowledge Awareness in Multi-turn Dialog Understanding</title>
      <author><first>Ting-Wei</first><last>Wu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Biing-Hwang</first><last>Juang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>254-264</pages>
      <abstract>In multi-turn dialog understanding, semantic frames are constructed by detecting intents and slots within each user utterance.However, recent works lack the capability of modeling multi-turn dynamics within a dialog in natural language understanding (NLU), instead leaving them for updating dialog states only.Moreover, humans usually associate relevant background knowledge with the current dialog contexts to better illustrate slot semantics revealed from word connotations, where previous works have explored such possibility mostly in knowledge-grounded response generation.In this paper, we propose to amend the research gap by equipping a BERT-based NLU framework with knowledge and context awareness.We first encode dialog contexts with a unidirectional context-aware transformer encoder and select relevant inter-word knowledge with the current word and previous history based on a knowledge attention mechanism. Experimental results in two complicated multi-turn dialog datasets have demonstrated significant improvements of our proposed framework. Attention visualization also demonstrates how our modules leverage knowledge across the utterance.</abstract>
      <url hash="72f8749b">2023.findings-eacl.19</url>
      <attachment type="software" hash="f4485d93">2023.findings-eacl.19.software.zip</attachment>
      <bibkey>wu-juang-2023-infusing</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>MC</fixed-case>o<fixed-case>N</fixed-case>a<fixed-case>L</fixed-case>a: A Benchmark for Code Generation from Multiple Natural Languages</title>
      <author><first>Zhiruo</first><last>Wang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Grace</first><last>Cuenca</last><affiliation>Princeton University</affiliation></author>
      <author><first>Shuyan</first><last>Zhou</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Frank F.</first><last>Xu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>265-273</pages>
      <abstract>While there has been a recent burgeoning of applications at the intersection of natural and programming languages, such as code generation and code summarization, these applications are usually English-centric. This creates a barrier for program developers who are not proficient in English. To mitigate this gap in technology development across languages, we propose a multilingual dataset, MCoNaLa, to benchmark code generation from natural language commands extending beyond English. Modeled off of the methodology from the English Code/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896 NL-Code pairs in three languages: Spanish, Japanese, and Russian. We present a systematic evaluation on MCoNaLa by testing state-of-the-art code generation systems. Although the difficulties vary across three languages, all systems lag significantly behind their English counterparts, revealing the challenges in adapting code generation to new languages.</abstract>
      <url hash="27226a1f">2023.findings-eacl.20</url>
      <bibkey>wang-etal-2023-mconala</bibkey>
    </paper>
    <paper id="21">
      <title>Augmenting pre-trained language models with audio feature embedding for argumentation mining in political debates</title>
      <author><first>Rafael</first><last>Mestre</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Stuart</first><last>Middleton</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Matt</first><last>Ryan</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Masood</first><last>Gheasi</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Timothy</first><last>Norman</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Jiatong</first><last>Zhu</last><affiliation>University of Southampton</affiliation></author>
      <pages>274-288</pages>
      <abstract>The integration of multimodality in natural language processing (NLP) tasks seeks to exploit the complementary information contained in two or more modalities, such as text, audio and video. This paper investigates the integration of often under-researched audio features with text, using the task of argumentation mining (AM) as a case study. We take a previously reported dataset and present an audio-enhanced version (the Multimodal USElecDeb60To16 dataset). We report the performance of two text models based on BERT and GloVe embeddings, one audio model (based on CNN and Bi-LSTM) and multimodal combinations, on a dataset of 28,850 utterances. The results show that multimodal models do not outperform text-based models when using the full dataset. However, we show that audio features add value in fully supervised scenarios with limited data. We find that when data is scarce (e.g. with 10% of the original dataset) multimodal models yield improved performance, whereas text models based on BERT considerably decrease performance. Finally, we conduct a study with artificially generated voices and an ablation study to investigate the importance of different audio features in the audio models.</abstract>
      <url hash="ef828bd0">2023.findings-eacl.21</url>
      <bibkey>mestre-etal-2023-augmenting</bibkey>
    </paper>
    <paper id="22">
      <title>Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions</title>
      <author><first>Cuong</first><last>Hoang</last><affiliation>KaiLua Labs</affiliation></author>
      <author><first>Devendra</first><last>Sachan</last><affiliation>McGill University, MILA - Quebec AI Institute</affiliation></author>
      <author><first>Prashant</first><last>Mathur</last><affiliation>Amazon</affiliation></author>
      <author><first>Brian</first><last>Thompson</last><affiliation>Amazon</affiliation></author>
      <author><first>Marcello</first><last>Federico</last><affiliation>AWS AI Labs</affiliation></author>
      <pages>289-295</pages>
      <abstract>We explore zero-shot adaptation, where a general-domain model has access to customer or domain specific parallel data at inference time, but not during training. We build on the idea of Retrieval Augmented Translation (RAT) where top-k in-domain fuzzy matches are found for the source sentence, and target-language translations of those fuzzy-matched sentences are provided to the translation model at inference time. We propose a novel architecture to control interactions between a source sentence and the top-k fuzzy target-language matches, and compare it to architectures from prior work. We conduct experiments in two language pairs (En-De and En-Fr) by training models on WMT data and testing them with five and seven multi-domain datasets, respectively. Our approach consistently outperforms the alternative architectures, improving BLEU across language pair, domain, and number k of fuzzy matches.</abstract>
      <url hash="ed68f132">2023.findings-eacl.22</url>
      <bibkey>hoang-etal-2023-improving</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>CALM</fixed-case>-Bench: A Multi-task Benchmark for Evaluating Causality-Aware Language Models</title>
      <author><first>Dhairya</first><last>Dalal</last><affiliation>University of Galway</affiliation></author>
      <author><first>Paul</first><last>Buitelaar</last><affiliation>University of Galway</affiliation></author>
      <author><first>Mihael</first><last>Arcan</last><affiliation>University of Galway</affiliation></author>
      <pages>296-311</pages>
      <abstract>Causal reasoning is a critical component of human cognition and is required across a range of question-answering (QA) tasks (such as abductive reasoning, commonsense QA, and procedural reasoning). Research on causal QA has been underdefined, task-specific, and limited in complexity. Recent advances in foundation language models (such as BERT, ERNIE, and T5) have shown the efficacy of pre-trained models across diverse QA tasks. However, there is limited research exploring the causal reasoning capabilities of those language models and no standard evaluation benchmark. To unify causal QA research, we propose CALM-Bench, a multi-task benchmark for evaluating causality-aware language models (CALM). We present a standardized definition of causal QA tasks and show empirically that causal reasoning can be generalized and transferred across different QA tasks. Additionally, we share a strong multi-task baseline model which outperforms single-task fine-tuned models on the CALM-Bench tasks.</abstract>
      <url hash="061c48e8">2023.findings-eacl.23</url>
      <bibkey>dalal-etal-2023-calm</bibkey>
    </paper>
    <paper id="24">
      <title>ez<fixed-case>C</fixed-case>oref: Towards Unifying Annotation Guidelines for Coreference Resolution</title>
      <author><first>Ankita</first><last>Gupta</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Marzena</first><last>Karpinska</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Wenlong</first><last>Zhao</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Kalpesh</first><last>Krishna</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Jack</first><last>Merullo</last><affiliation>Brown University</affiliation></author>
      <author><first>Luke</first><last>Yeh</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Brendan</first><last>O’Connor</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>312-330</pages>
      <abstract>Large-scale, high-quality corpora are critical for advancing research in coreference resolution. However, existing datasets vary in their definition of coreferences and have been collected via complex and lengthy guidelines that are curated for linguistic experts. These concerns have sparked a growing interest among researchers to curate a unified set of guidelines suitable for annotators with various backgrounds. In this work, we develop a crowdsourcing-friendly coreference annotation methodology, ezCoref, consisting of an annotation tool and an interactive tutorial. We use ezCoref to re-annotate 240 passages from seven existing English coreference datasets (spanning fiction, news, and multiple other domains) while teaching annotators only cases that are treated similarly across these datasets. Surprisingly, we find that reasonable quality annotations were already achievable (90% agreement between the crowd and expert annotations) even without extensive training. On carefully analyzing the remaining disagreements, we identify the presence of linguistic cases that our annotators unanimously agree upon but lack unified treatments (e.g., generic pronouns, appositives) in existing datasets. We propose the research community should revisit these phenomena when curating future unified annotation guidelines.</abstract>
      <url hash="7e5348af">2023.findings-eacl.24</url>
      <bibkey>gupta-etal-2023-ezcoref</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>PREME</fixed-case>: Preference-based Meeting Exploration through an Interactive Questionnaire</title>
      <author><first>Negar</first><last>Arabzadeh</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Ali</first><last>Ahmadvand</last><affiliation>Emory University</affiliation></author>
      <author><first>Julia</first><last>Kiseleva</last><affiliation>Microsoft Research</affiliation></author>
      <author id="yang-liu-microsoft"><first>Yang</first><last>Liu</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Ming</first><last>Zhong</last><affiliation>University of Illinois</affiliation></author>
      <author><first>Milad</first><last>Shokouhi</last><affiliation>Microsoft</affiliation></author>
      <pages>331-342</pages>
      <abstract>The recent increase in the volume of online meetings necessitates automated tools for organizing the material, especially when an attendee has missed the discussion and needs assistance in quickly exploring it. In this work, we propose a novel end-to-end framework for generating interactive questionnaires for preference-based meeting exploration. As a result, users are supplied with a list of suggested questions reflecting their preferences. Since the task is new, we introduce an automatic evaluation strategy by measuring how much the generated questions via questionnaire are answerable to ensure factual correctness and covers the source meeting for the depth of possible exploration.</abstract>
      <url hash="144979c6">2023.findings-eacl.25</url>
      <bibkey>arabzadeh-etal-2023-preme</bibkey>
    </paper>
    <paper id="26">
      <title>Sentence Identification with <fixed-case>BOS</fixed-case> and <fixed-case>EOS</fixed-case> Label Combinations</title>
      <author><first>Takuma</first><last>Udagawa</last><affiliation>IBM Research - Tokyo</affiliation></author>
      <author><first>Hiroshi</first><last>Kanayama</last><affiliation>IBM Research - Tokyo</affiliation></author>
      <author><first>Issei</first><last>Yoshida</last><affiliation>IBM Research - Tokyo</affiliation></author>
      <pages>343-358</pages>
      <abstract>The sentence is a fundamental unit in many NLP applications. Sentence segmentation is widely used as the first preprocessing task, where an input text is split into consecutive sentences considering the end of the sentence (EOS) as their boundaries. This task formulation relies on a strong assumption that the input text consists only of sentences, or what we call the sentential units (SUs). However, real-world texts often contain non-sentential units (NSUs) such as metadata, sentence fragments, nonlinguistic markers, etc. which are unreasonable or undesirable to be treated as a part of an SU. To tackle this issue, we formulate a novel task of sentence identification, where the goal is to identify SUs while excluding NSUs in a given text. To conduct sentence identification, we propose a simple yet effective method which combines the beginning of the sentence (BOS) and EOS labels to determine the most probable SUs and NSUs based on dynamic programming. To evaluate this task, we design an automatic, language-independent procedure to convert the Universal Dependencies corpora into sentence identification benchmarks. Finally, our experiments on the sentence identification task demonstrate that our proposed method generally outperforms sentence segmentation baselines which only utilize EOS labels.</abstract>
      <url hash="f8dfa617">2023.findings-eacl.26</url>
      <bibkey>udagawa-etal-2023-sentence</bibkey>
    </paper>
    <paper id="27">
      <title>Gauging the Gap Between Human and Machine Text Simplification Through Analytical Evaluation of Simplification Strategies and Errors</title>
      <author><first>Daichi</first><last>Yamaguchi</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Rei</first><last>Miyata</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Sayuka</first><last>Shimada</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Satoshi</first><last>Sato</last><affiliation>Nagoya University</affiliation></author>
      <pages>359-375</pages>
      <abstract>This study presents an analytical evaluation of neural text simplification (TS) systems. Because recent TS models are trained in an end-to-end fashion, it is difficult to grasp their abilities to perform particular simplification operations. For the advancement of TS research and development, we should understand in detail what current TS systems can and cannot perform in comparison with human performance. To that end, we first developed an analytical evaluation framework consisting of fine-grained taxonomies of simplification strategies (at both the surface and content levels) and errors. Using this framework, we annotated TS instances produced by professional human editors and multiple neural TS systems and compared the results. Our analyses concretely and quantitatively revealed a wide gap between humans and systems, specifically indicating that systems tend to perform deletions and local substitutions while excessively omitting important information, and that the systems can hardly perform information addition operations. Based on our analyses, we also provide detailed directions to address these limitations.</abstract>
      <url hash="4e918bac">2023.findings-eacl.27</url>
      <bibkey>yamaguchi-etal-2023-gauging</bibkey>
    </paper>
    <paper id="28">
      <title>Bridging the Gap between Pre-Training and Fine-Tuning for Commonsense Generation</title>
      <author><first>Haoran</first><last>Yang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yan</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Piji</first><last>Li</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Wei</first><last>Bi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Chen</first><last>Xu</last><affiliation>Beijing University of Technology</affiliation></author>
      <pages>376-383</pages>
      <abstract>Commonsense generation aims to generate a plausible sentence containing all given unordered concept words. Previous methods focusing on this task usually directly concatenate these words as the input of a pre-trained language model (PLM). However, in PLMs’ pre-training process, the inputs are often corrupted sentences with correct word order. This input distribution discrepancy between pre-training and fine-tuning makes the model difficult to fully utilize the knowledge of PLMs. In this paper, we propose a two-stage framework to alleviate this issue. Firstly, in pre-training stage, we design a new format of input to endow PLMs the ability to deal with masked sentences with incorrect word order. Secondly, during fine-tuning, we insert the special token [MASK] between two consecutive concept words to make the input distribution more similar to the input distribution in pre-training. We conduct extensive experiments and provide thorough analysis to demonstrate the effectiveness of our proposed method.</abstract>
      <url hash="2b0777bb">2023.findings-eacl.28</url>
      <bibkey>yang-etal-2023-bridging</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>LED</fixed-case>: A Dataset for Life Event Extraction from Dialogs</title>
      <author><first>Yi-Pei</first><last>Chen</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>An-Zi</first><last>Yen</last><affiliation>National Yang Ming Chiao Tung University</affiliation></author>
      <author><first>Hen-Hsen</first><last>Huang</last><affiliation>Institute of Information Science, Academia Sinica</affiliation></author>
      <author><first>Hideki</first><last>Nakayama</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Hsin-Hsi</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>384-398</pages>
      <abstract>Lifelogging has gained more attention due to its wide applications, such as personalized recommendations or memory assistance. The issues of collecting and extracting personal life events have emerged. People often share their life experiences with others through conversations. However, extracting life events from conversations is rarely explored. In this paper, we present Life Event Dialog, a dataset containing fine-grained life event annotations on conversational data. In addition, we initiate a novel Conversational Life Event Extraction task and differentiate the task from the public event extraction or the life event extraction from other sources like microblogs. We explore three information extraction (IE) frameworks to address the Conversational Life Event Extraction task: OpenIE, relation extraction, and event extraction. A comprehensive empirical analysis of the three baselines is established. The results suggest that the current event extraction model still struggles with extracting life events from human daily conversations. Our proposed Life Event Dialog dataset and in-depth analysis of IE frameworks will facilitate future research on life event extraction from conversations.</abstract>
      <url hash="07471ee1">2023.findings-eacl.29</url>
      <bibkey>chen-etal-2023-led</bibkey>
    </paper>
    <paper id="30">
      <title>Reading and Reasoning over Chart Images for Evidence-based Automated Fact-Checking</title>
      <author><first>Mubashara</first><last>Akhtar</last><affiliation>King’s College London</affiliation></author>
      <author><first>Oana</first><last>Cocarascu</last><affiliation>King’s College London</affiliation></author>
      <author><first>Elena</first><last>Simperl</last><affiliation>King’s College London</affiliation></author>
      <pages>399-414</pages>
      <abstract>Evidence data for automated fact-checking (AFC) can be in multiple modalities such as text, tables, images, audio, or video. While there is increasing interest in using images for AFC, previous works mostly focus on detecting manipulated or fake images. We propose a novel task, chart-based fact-checking, and introduce ChartBERT as the first model for AFC against chart evidence. ChartBERT leverages textual, structural and visual information of charts to determine the veracity of textual claims. For evaluation, we create ChartFC, a new dataset of 15,886 charts. We systematically evaluate 75 different vision-language (VL) baselines and show that ChartBERT outperforms VL models, achieving 63.8% accuracy. Our results suggest that the task is complex yet feasible, with many challenges ahead.</abstract>
      <url hash="780dd096">2023.findings-eacl.30</url>
      <bibkey>akhtar-etal-2023-reading</bibkey>
    </paper>
    <paper id="31">
      <title>Causal Reasoning of Entities and Events in Procedural Texts</title>
      <author><first>Li</first><last>Zhang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Hainiu</first><last>Xu</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Yue</first><last>Yang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Shuyan</first><last>Zhou</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Weiqiu</first><last>You</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Manni</first><last>Arora</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>415-431</pages>
      <abstract>Entities and events are crucial to natural language reasoning and common in procedural texts. Existing work has focused either exclusively on entity state tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one would burn themselves by touching the pan), while these two tasks are often causally related. We propose CREPE, the first benchmark on causal reasoning of event plausibility and entity states. We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1. We boost model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code. By injecting the causal relations between entities and events as intermediate reasoning steps in our representation, we further boost the performance to .67 F1. Our findings indicate not only the challenge that CREPE brings for language models, but also the efficacy of code-like prompting combined with chain-of-thought prompting for multihop event reasoning.</abstract>
      <url hash="694d4c8e">2023.findings-eacl.31</url>
      <attachment type="dataset" hash="8546f1bc">2023.findings-eacl.31.dataset.zip</attachment>
      <bibkey>zhang-etal-2023-causal</bibkey>
    </paper>
    <paper id="32">
      <title>Few-Shot Structured Policy Learning for Multi-Domain and Multi-Task Dialogues</title>
      <author><first>Thibault</first><last>Cordier</last><affiliation>University of Avignon</affiliation></author>
      <author><first>Tanguy</first><last>Urvoy</last><affiliation>Orange</affiliation></author>
      <author><first>Fabrice</first><last>Lefèvre</last><affiliation>Avignon Univ.</affiliation></author>
      <author><first>Lina M.</first><last>Rojas Barahona</last><affiliation>Orange Innovation Research</affiliation></author>
      <pages>432-441</pages>
      <abstract>Reinforcement learning has been widely adopted to model dialogue managers in task-oriented dialogues. However, the user simulator provided by state-of-the-art dialogue frameworks are only rough approximations of human behaviour. The ability to learn from a small number of human interactions is hence crucial, especially on multi-domain and multi-task environments where the action space is large. We therefore propose to use structured policies to improve sample efficiency when learning on these kinds of environments. We also evaluate the impact of learning from human vs simulated experts. Among the different levels of structure that we tested, the graph neural networks (GNNs) show a remarkable superiority by reaching a success rate above 80% with only 50 dialogues when learning from simulated experts. They also show superiority when learning from human experts, although a performance drop was observed. We therefore suggest to concentrate future research efforts on bridging the gap between human data, simulators and automatic evaluators in dialogue frameworks.</abstract>
      <url hash="24f3c5da">2023.findings-eacl.32</url>
      <bibkey>cordier-etal-2023-shot</bibkey>
    </paper>
    <paper id="33">
      <title>Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?</title>
      <author><first>Jielin</first><last>Qiu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>William</first><last>Han</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jiacheng</first><last>Zhu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Mengdi</first><last>Xu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Michael</first><last>Rosenberg</last><affiliation>University of Colorado School of Medicine</affiliation></author>
      <author><first>Emerson</first><last>Liu</last><affiliation>Allegheny Health Network</affiliation></author>
      <author><first>Douglas</first><last>Weber</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Ding</first><last>Zhao</last><affiliation>Cmu</affiliation></author>
      <pages>442-453</pages>
      <abstract>Recent advancements in Large Language Models (LLMs) have drawn increasing attention since the learned embeddings pretrained on large-scale datasets have shown powerful ability in various downstream applications. However, whether the learned knowledge by LLMs can be transferred to clinical cardiology remains unknown. In this work, we aim to bridge this gap by transferring the knowledge of LLMs to clinical Electrocardiography (ECG). We propose an approach for cardiovascular disease diagnosis and automatic ECG diagnosis report generation. We also introduce an additional loss function by Optimal Transport (OT) to align the distribution between ECG and language embedding. The learned embeddings are evaluated on two downstream tasks: (1) automatic ECG diagnosis report generation, and (2) zero-shot cardiovascular disease detection. Our approach is able to generate high-quality cardiac diagnosis reports and also achieves competitive zero-shot classification performance even compared with supervised baselines, which proves the feasibility of transferring knowledge from LLMs to the cardiac domain.</abstract>
      <url hash="ec188d17">2023.findings-eacl.33</url>
      <bibkey>qiu-etal-2023-transfer</bibkey>
    </paper>
    <paper id="34">
      <title>Practical Takes on Federated Learning with Pretrained Language Models</title>
      <author><first>Ankur</first><last>Agarwal</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last><affiliation>Noah’s Ark Lab Huawei</affiliation></author>
      <author><first>Prasanna</first><last>Parthasarathi</last><affiliation>Noah’s Ark Lab</affiliation></author>
      <pages>454-471</pages>
      <abstract>Real-world applications of language models entail data privacy constraints when learning from diverse data domains. Federated learning with pretrained language models for language tasks has been gaining attention lately but there are definite confounders that warrants a careful study. Specifically, understanding the limits of federated NLP applications through varying the effects of different aspects (such as data heterogeneity, the trade-off between training time and performance, the effect of different data, and client distributions and sensitivity of the shared model to learning local distributions) is necessary to evaluate whether language models indeed learn to generalize by adapting to the different domains. Towards that, we elaborate different hypotheses over the components in federated NLP architectures and study them in detail with relevant experiments over three tasks: Stanford Sentiment Treebank-2, OntoNotes-5.0 and GigaWord. The experiments with different Transformer inductive biases on the variety of tasks provide a glimpse at the understanding of federated learning at NLP tasks. Specifically, the analysis suggests that regularization due to the ensembling effect may be masquerading as domain adaptation of federated learning in NLP with pre-trained language models.</abstract>
      <url hash="fa46fe0a">2023.findings-eacl.34</url>
      <bibkey>agarwal-etal-2023-practical</bibkey>
    </paper>
    <paper id="35">
      <title>Paper Bullets: Modeling Propaganda with the Help of Metaphor</title>
      <author><first>Daniel</first><last>Baleato Rodríguez</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Verna</first><last>Dankers</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>472-489</pages>
      <abstract>Propaganda aims to persuade an audience by appealing to emotions and using faulty reasoning, with the purpose of promoting a particular point of view. Similarly, metaphor modifies the semantic frame, thus eliciting a response that can be used to tune up or down the emotional volume of the message. Given the close relationship between them, we hypothesize that, when modeling them computationally, it can be beneficial to do so jointly. In particular, we perform multi-task learning with propaganda identification as the main task and metaphor detection as an auxiliary task. To the best of our knowledge, this is the first work that models metaphor and propaganda together. We experiment with two datasets for identifying propaganda techniques in news articles and in memes shared on social media. We find that leveraging metaphor improves model performance, particularly for the two most common propaganda techniques: loaded language and name-calling.</abstract>
      <url hash="011b459d">2023.findings-eacl.35</url>
      <bibkey>baleato-rodriguez-etal-2023-paper</bibkey>
    </paper>
    <paper id="36">
      <title>Lexical Semantics with Large Language Models: A Case Study of <fixed-case>E</fixed-case>nglish “break”</title>
      <author><first>Erika</first><last>Petersen</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher</first><last>Potts</last><affiliation>Stanford University</affiliation></author>
      <pages>490-511</pages>
      <abstract>Large neural language models (LLMs) can be powerful tools for research in lexical semantics. We illustrate this potential using the English verb “break”, which has numerous senses and appears in a wide range of syntactic frames. We show that LLMs capture known sense distinctions and can be used to identify informative new sense combinations for further analysis. More generally, we argue that LLMs are aligned with lexical semantic theories in providing high-dimensional, contextually modulated representations, but LLMs’ lack of discrete features and dependence on usage-based data offer a genuinely new perspective on traditional problems in lexical semantics.</abstract>
      <url hash="aba3c28b">2023.findings-eacl.36</url>
      <bibkey>petersen-potts-2023-lexical</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>SWING</fixed-case>: Balancing Coverage and Faithfulness for Dialogue Summarization</title>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Siffi</first><last>Singh</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiaofei</first><last>Ma</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Wei</first><last>Xiao</last><affiliation>AWS AI, Amazon</affiliation></author>
      <author><first>Feng</first><last>Nan</last><affiliation>Aws Ai</affiliation></author>
      <author><first>Nicholas</first><last>Dingwall</last><affiliation>Amazon AI Labs</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>Amazon AWS AI Labs</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last><affiliation>Columbia University and Amazon (Amazon Scholar)</affiliation></author>
      <pages>512-525</pages>
      <abstract>Missing information is a common issue of dialogue summarization where some information in the reference summaries is not covered in the generated summaries. To address this issue, we propose to utilize natural language inference (NLI) models to improve coverage while avoiding introducing factual inconsistencies. Specifically, we use NLI to compute fine-grained training signals to encourage the model to generate content in the reference summaries that have not been covered, as well as to distinguish between factually consistent and inconsistent generated sentences. Experiments on the DialogSum and SAMSum datasets confirm the effectiveness of the proposed approach in balancing coverage and faithfulness, validated with automatic metrics and human evaluations. Additionally, we compute the correlation between commonly used automatic metrics with human judgments in terms of three different dimensions regarding coverage and factual consistency to provide insight into the most suitable metric for evaluating dialogue summaries.</abstract>
      <url hash="27c9d270">2023.findings-eacl.37</url>
      <bibkey>huang-etal-2023-swing</bibkey>
    </paper>
    <paper id="38">
      <title>Language-Aware Multilingual Machine Translation with Self-Supervised Learning</title>
      <author><first>Haoran</first><last>Xu</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Jean</first><last>Maillard</last><affiliation>Meta AI</affiliation></author>
      <author><first>Vedanuj</first><last>Goswami</last><affiliation>Meta AI</affiliation></author>
      <pages>526-539</pages>
      <abstract>Multilingual machine translation (MMT) benefits from cross-lingual transfer but is a challenging multitask optimization problem. This is partly because there is no clear framework to systematically learn language-specific parameters. Self-supervised learning (SSL) approaches that leverage large quantities of monolingual data (where parallel data is unavailable) have shown promise by improving translation performance as complementary tasks to the MMT task. However, jointly optimizing SSL and MMT tasks is even more challenging. In this work, we first investigate how to utilize **intra-distillation** to learn more *language-specific* parameters and then show the importance of these language-specific parameters. Next, we propose a novel but simple SSL task, **concurrent denoising**, that co-trains with the MMT task by concurrently denoising monolingual data on both the encoder and decoder. Finally, we apply **intra-distillation** to this co-training approach. Combining these two approaches significantly improves MMT performance, outperforming three state-of-the-art SSL methods by a large margin, e.g., 11.3{% and 3.7{% improvement on an 8-language and a 15-language benchmark compared with MASS, respectively.</abstract>
      <url hash="8e1b8d0f">2023.findings-eacl.38</url>
      <bibkey>xu-etal-2023-language</bibkey>
    </paper>
    <paper id="39">
      <title>Cloze Quality Estimation for Language Assessment</title>
      <author><first>Zizheng</first><last>Zhang</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Masato</first><last>Mita</last><affiliation>CyberAgent Inc.</affiliation></author>
      <author><first>Mamoru</first><last>Komachi</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <pages>540-550</pages>
      <abstract>Cloze tests play an essential role in language assessment and help language learners improve their skills.In this paper, we propose a novel task called Cloze Quality Estimation (CQE) — a zero-shot task of evaluating whether a cloze test is of sufficient “high-quality” for language assessment based on two important factors: reliability and validity.We have taken the first step by creating a new dataset named CELA for the CQE task, which includes English cloze tests and corresponding evaluations about their quality annotated by native English speakers, which includes 2,597 and 1,730 instances in aspects of reliability and validity, respectively.We have tested baseline evaluation methods on the dataset, showing that our method could contribute to the CQE task, but the task is still challenging.</abstract>
      <url hash="55cc3bc3">2023.findings-eacl.39</url>
      <bibkey>zhang-etal-2023-cloze</bibkey>
    </paper>
    <paper id="40">
      <title>Bag of Tricks for In-Distribution Calibration of Pretrained Transformers</title>
      <author><first>Jaeyoung</first><last>Kim</last><affiliation>VUNO, Inc.</affiliation></author>
      <author><first>Dongbin</first><last>Na</last><affiliation>VUNO Inc.</affiliation></author>
      <author><first>Sungchul</first><last>Choi</last><affiliation>Pukyong National University</affiliation></author>
      <author><first>Sungbin</first><last>Lim</last><affiliation>Unist</affiliation></author>
      <pages>551-563</pages>
      <abstract>While pre-trained language models (PLMs) have become a de-facto standard promoting the accuracy of text classification tasks, recent studies find that PLMs often predict over-confidently.Although calibration methods have been proposed, such as ensemble learning and data augmentation, most of the methods have been verified in computer vision benchmarks rather than in PLM-based text classification tasks. In this paper, we present an empirical study on confidence calibration for PLMs, addressing three categories, including confidence penalty losses, data augmentations, and ensemble methods. We find that the ensemble model overfitted to the training set shows sub-par calibration performance and also observe that PLMs trained with confidence penalty loss have a trade-off between calibration and accuracy. Building on these observations, we propose the Calibrated PLM (CALL), a combination of calibration techniques. The CALL complements shortcomings that may occur when utilizing a calibration method individually and boosts both classification and calibration accuracy. Design choices in CALL’s training procedures are extensively studied, and we provide a detailed analysis of how calibration techniques affect the calibration performance of PLMs.</abstract>
      <url hash="686e61c3">2023.findings-eacl.40</url>
      <bibkey>kim-etal-2023-bag</bibkey>
    </paper>
    <paper id="41">
      <title>Fine-Tuning Deteriorates General Textual Out-of-Distribution Detection by Distorting Task-Agnostic Features</title>
      <author><first>Sishuo</first><last>Chen</last><affiliation>Center for Data Science, Peking University</affiliation></author>
      <author><first>Wenkai</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiaohan</first><last>Bi</last><affiliation>Peking University</affiliation></author>
      <author><first>Xu</first><last>Sun</last><affiliation>Peking University</affiliation></author>
      <pages>564-579</pages>
      <abstract>Detecting out-of-distribution (OOD) inputs is crucial for the safe deployment of natural language processing (NLP) models. Though existing methods, especially those based on the statistics in the feature space of fine-tuned pre-trained language models (PLMs), are claimed to be effective, their effectiveness on different types of distribution shifts remains underexplored. In this work, we take the first step to comprehensively evaluate the mainstream textual OOD detection methods for detecting semantic and non-semantic shifts. We find that: (1) no existing method behaves well in both settings; (2) fine-tuning PLMs on in-distribution data benefits detecting semantic shifts but severely deteriorates detecting non-semantic shifts, which can be attributed to the distortion of task-agnostic features. To alleviate the issue, we present a simple yet effective general OOD score named GNOME that integrates the confidence scores derived from the task-agnostic and task-specific representations. Experiments show that GNOME works well in both semantic and non-semantic shift scenarios, and further brings significant improvement on two cross-task benchmarks where both kinds of shifts simultaneously take place. Our code is available at https://github.com/lancopku/GNOME.</abstract>
      <url hash="437ea09d">2023.findings-eacl.41</url>
      <bibkey>chen-etal-2023-fine</bibkey>
    </paper>
    <paper id="42">
      <title>A Question of Style: A Dataset for Analyzing Formality on Different Levels</title>
      <author><first>Elisabeth</first><last>Eder</last><affiliation>University of Klagenfurt</affiliation></author>
      <author><first>Ulrike</first><last>Krieg-Holz</last><affiliation>University of Klagenfurt</affiliation></author>
      <author><first>Michael</first><last>Wiegand</last><affiliation>Alpen-Adria-Universitaet Klagenfurt</affiliation></author>
      <pages>580-593</pages>
      <abstract>Accounting for different degrees of formality is crucial for producing contextually appropriate language. To assist NLP applications concerned with this problem and formality analysis in general, we present the first dataset of sentences from a wide range of genres assessed on a continuous informal-formal scale via comparative judgments. It is the first corpus with a comprehensive perspective on German sentence-level formality overall. We compare machine learning models for formality scoring, a task we treat as a regression problem, on our dataset. Finally, we investigate the relation between sentence- and document-level formality and evaluate leveraging sentence-based annotations for assessing formality on documents.</abstract>
      <url hash="bdf83b71">2023.findings-eacl.42</url>
      <bibkey>eder-etal-2023-question</bibkey>
    </paper>
    <paper id="43">
      <title>Task-specific Compression for Multi-task Language Models using Attribution-based Pruning</title>
      <author><first>Nakyeong</first><last>Yang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Yunah</first><last>Jang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seohyeong</first><last>Jeong</last><affiliation>42dot</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last><affiliation>Seoul National University</affiliation></author>
      <pages>594-604</pages>
      <abstract>Multi-task language models show outstanding performance for various natural language understanding tasks with only a single model.However, these language models inevitably utilize an unnecessarily large number of model parameters, even when used only for a specific task.In this paper, we propose a novel training-free compression method for multi-task language models using pruning method.Specifically, we use an attribution method to determine which neurons are essential for performing a specific task.We task-specifically prune unimportant neurons and leave only task-specific parameters.Furthermore, we extend our method to be applicable in both low-resource and unsupervised settings. Since our compression method is training-free, it uses little computing resources and does not update the pre-trained parameters of language models, reducing storage space usage.Experimental results on the six widely-used datasets show that our proposed pruning method significantly outperforms baseline pruning methods.In addition, we demonstrate that our method preserves performance even in an unseen domain setting.</abstract>
      <url hash="1c9df74e">2023.findings-eacl.43</url>
      <bibkey>yang-etal-2023-task</bibkey>
    </paper>
    <paper id="44">
      <title>Zero-shot Transfer of Article-aware Legal Outcome Classification for <fixed-case>E</fixed-case>uropean Court of Human Rights Cases</title>
      <author><first>Santosh</first><last>T.y.s.s</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Oana</first><last>Ichim</last><affiliation>Graduate Institute of International and Development Studies</affiliation></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>605-617</pages>
      <abstract>In this paper, we cast Legal Judgment Prediction on European Court of Human Rights cases into an article-aware classification task, where the case outcome is classified from a combined input of case facts and convention articles. This configuration facilitates the model learning some legal reasoning ability in mapping article text to specific case fact text. It also provides an opportunity to evaluate the model’s ability to generalize to zero-shot settings when asked to classify the case outcome with respect to articles not seen during training. We devise zero-shot experiments and apply domain adaptation methods based on domain discrimination and Wasserstein distance. Our results demonstrate that the article-aware architecture outperforms straightforward fact classification. We also find that domain adaptation methods improve zero-shot transfer performance, with article relatedness and encoder pre-training influencing the effect.</abstract>
      <url hash="0430d11f">2023.findings-eacl.44</url>
      <bibkey>t-y-s-s-etal-2023-zero</bibkey>
    </paper>
    <paper id="45">
      <title>Abstractive Document Summarization with Summary-length Prediction</title>
      <author><first>Jingun</first><last>Kwon</last><affiliation>Tokyo Institute of Technology, Naver Corporation</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <pages>618-624</pages>
      <abstract>Recently, we can obtain a practical abstractive document summarization model by fine-tuning a pre-trained language model (PLM). Since the pre-training for PLMs does not consider summarization-specific information such as the target summary length, there is a gap between the pre-training and fine-tuning for PLMs in summarization tasks.To fill the gap, we propose a method for enabling the model to understand the summarization-specific information by predicting the summary length in the encoder and generating a summary of the predicted length in the decoder in fine-tuning.Experimental results on the WikiHow, NYT, and CNN/DM datasets showed that our methods improve ROUGE scores from BART by generating summaries of appropriate lengths. Further, we observed about 3.0, 1,5, and 3.1 point improvements for ROUGE-1, -2, and -L, respectively, from GSum on the WikiHow dataset. Human evaluation results also showed that our methods improve the informativeness and conciseness of summaries.</abstract>
      <url hash="351039b8">2023.findings-eacl.45</url>
      <attachment type="software" hash="30cadd6d">2023.findings-eacl.45.software.zip</attachment>
      <bibkey>kwon-etal-2023-abstractive</bibkey>
    </paper>
    <paper id="46">
      <title>Hierarchical Label Generation for Text Classification</title>
      <author><first>Jingun</first><last>Kwon</last><affiliation>Tokyo Institute of Technology, Naver Corporation</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Young-In</first><last>Song</last><affiliation>Naver</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <pages>625-632</pages>
      <abstract/>
      <url hash="2b3aef86">2023.findings-eacl.46</url>
      <bibkey>kwon-etal-2023-hierarchical</bibkey>
    </paper>
    <paper id="47">
      <title>Active Learning for Multilingual Semantic Parser</title>
      <author><first>Zhuang</first><last>Li</last><affiliation>Monash University</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University</affiliation></author>
      <pages>633-639</pages>
      <abstract>Current multilingual semantic parsing (MSP) datasets are almost all collected by translating the utterances in the existing datasets from the resource-rich language to the target language. However, manual translation is costly. To reduce the translation effort, this paper proposes the first active learning procedure for MSP (AL-MSP). AL-MSP selects only a subset from the existing datasets to be translated. We also propose a novel selection method that prioritizes the examples diversifying the logical form structures with more lexical choices, and a novel hyperparameter tuning method that needs no extra annotation cost. Our experiments show that AL-MSP significantly reduces translation costs with ideal selection methods. Our selection method with proper hyperparameters yields better parsing performance than the other baselines on two multilingual datasets.</abstract>
      <url hash="fd2596ec">2023.findings-eacl.47</url>
      <bibkey>li-haffari-2023-active</bibkey>
    </paper>
    <paper id="48">
      <title>Joint Word and Morpheme Segmentation with <fixed-case>B</fixed-case>ayesian Non-Parametric Models</title>
      <author><first>Shu</first><last>Okabe</last><affiliation>LISN/CNRS, Université Paris-Saclay</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR CNRS &amp; Sorbonne Université</affiliation></author>
      <pages>640-654</pages>
      <abstract>Language documentation often requires segmenting transcriptions of utterances collected on the field into words and morphemes. While these two tasks are typically performed in succession, we study here Bayesian models for simultaneously segmenting utterances at these two levels. Our aim is twofold: (a) to study the effect of explicitly introducing a hierarchy of units in joint segmentation models; (b) to further assess whether these two levels can be better identified through weak supervision. For this, we first consider a deterministic coupling between independent models; then design and evaluate hierarchical Bayesian models. Experiments with two under-resourced languages (Japhug and Tsez) allow us to better understand the value of various types of weak supervision. In our analysis, we use these results to revisit the distributional hypotheses behind Bayesian segmentation models and evaluate their validity for language documentation data.</abstract>
      <url hash="23335297">2023.findings-eacl.48</url>
      <bibkey>okabe-yvon-2023-joint</bibkey>
    </paper>
    <paper id="49">
      <title>Cross-Lingual Transfer of Cognitive Processing Complexity</title>
      <author><first>Charlotte</first><last>Pouw</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Nora</first><last>Hollenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Lisa</first><last>Beinborn</last><affiliation>Vrije Universiteit Amsterdam</affiliation></author>
      <pages>655-669</pages>
      <abstract>When humans read a text, their eye movements are influenced by the structural complexity of the input sentences. This cognitive phenomenon holds across languages and recent studies indicate that multilingual language models utilize structural similarities between languages to facilitate cross-lingual transfer. We use sentence-level eye-tracking patterns as a cognitive indicator for structural complexity and show that the multilingual model XLM-RoBERTa can successfully predict varied patterns for 13 typologically diverse languages, despite being fine-tuned only on English data. We quantify the sensitivity of the model to structural complexity and distinguish a range of complexity characteristics. Our results indicate that the model develops a meaningful bias towards sentence length but also integrates cross-lingual differences. We conduct a control experiment with randomized word order and find that the model seems to additionally capture more complex structural information.</abstract>
      <url hash="b2c58d40">2023.findings-eacl.49</url>
      <attachment type="software" hash="dfff89be">2023.findings-eacl.49.software.zip</attachment>
      <bibkey>pouw-etal-2023-cross</bibkey>
    </paper>
    <paper id="50">
      <title>Does Transliteration Help Multilingual Language Modeling?</title>
      <author><first>Ibraheem Muhammad</first><last>Moosa</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Mahmud Elahi</first><last>Akhter</last><affiliation>North South University</affiliation></author>
      <author><first>Ashfia</first><last>Habib</last><affiliation>North South Univeristy</affiliation></author>
      <pages>670-685</pages>
      <abstract>Script diversity presents a challenge to Multilingual Language Models (MLLM) by reducing lexical overlap among closely related languages. Therefore, transliterating closely related languages that use different writing scripts to a common script may improve the downstream task performance of MLLMs. We empirically measure the effect of transliteration on MLLMs in this context. We specifically focus on the Indic languages, which have the highest script diversity in the world, and we evaluate our models on the IndicGLUE benchmark. We perform the Mann-Whitney U test to rigorously verify whether the effect of transliteration is significant or not. We find that transliteration benefits the low-resource languages without negatively affecting the comparatively high-resource languages. We also measure the cross-lingual representation similarity of the models using centered kernel alignment on parallel sentences from the FLORES-101 dataset. We find that for parallel sentences across different languages, the transliteration-based model learns sentence representations that are more similar.</abstract>
      <url hash="25b6b494">2023.findings-eacl.50</url>
      <bibkey>moosa-etal-2023-transliteration</bibkey>
    </paper>
    <paper id="51">
      <title>A Multilingual Dataset of Racial Stereotypes in Social Media Conversational Threads</title>
      <author><first>Tom</first><last>Bourgeade</last><affiliation>IRIT, University of Toulouse</affiliation></author>
      <author><first>Alessandra Teresa</first><last>Cignarella</last><affiliation>Computer Science Department - University of Turin</affiliation></author>
      <author><first>Simona</first><last>Frenda</last><affiliation>Università degli Studi di Torino</affiliation></author>
      <author><first>Mario</first><last>Laurent</last><affiliation>Université Paul Sabatier</affiliation></author>
      <author><first>Wolfgang</first><last>Schmeisser-Nieto</last><affiliation>Universitat de Barcelona</affiliation></author>
      <author><first>Farah</first><last>Benamara</last><affiliation>University of toulouse</affiliation></author>
      <author><first>Cristina</first><last>Bosco</last><affiliation>Dipartimento di Informatica - Università di Torino</affiliation></author>
      <author><first>Véronique</first><last>Moriceau</last><affiliation>IRIT, Université Toulouse 3</affiliation></author>
      <author><first>Viviana</first><last>Patti</last><affiliation>University of Turin, Dipartimento di Informatica</affiliation></author>
      <author><first>Mariona</first><last>Taulé</last><affiliation>University of Barcelona</affiliation></author>
      <pages>686-696</pages>
      <abstract>In this paper, we focus on the topics of misinformation and racial hoaxes from a perspective derived from both social psychology and computational linguistics. In particular, we consider the specific case of anti-immigrant feeling as a first case study for addressing racial stereotypes. We describe the first corpus-based study for multilingual racial stereotype identification in social media conversational threads. Our contributions are: (i) a multilingual corpus of racial hoaxes, (ii) a set of common guidelines for the annotation of racial stereotypes in social media texts, and a multi-layered, fine-grained scheme, psychologically grounded on the work by Fiske, including not only stereotype presence, but also contextuality, implicitness, and forms of discredit, (iii) a multilingual dataset in Italian, Spanish, and French annotated following the aforementioned guidelines, and cross-lingual comparative analyses taking into account racial hoaxes and stereotypes in online discussions. The analysis and results show the usefulness of our methodology and resources, shedding light on how racial hoaxes are spread, and enable the identification of negative stereotypes that reinforce them.</abstract>
      <url hash="7037ea87">2023.findings-eacl.51</url>
      <bibkey>bourgeade-etal-2023-multilingual</bibkey>
    </paper>
    <paper id="52">
      <title>Detecting Contextomized Quotes in News Headlines by Contrastive Learning</title>
      <author><first>Seonyeong</first><last>Song</last><affiliation>Soongsil University</affiliation></author>
      <author><first>Hyeonho</first><last>Song</last><affiliation>Kaist</affiliation></author>
      <author><first>Kunwoo</first><last>Park</last><affiliation>Soongsil University</affiliation></author>
      <author><first>Jiyoung</first><last>Han</last><affiliation>Korea Advanced Institute of Science and Technology (KAIST)</affiliation></author>
      <author><first>Meeyoung</first><last>Cha</last><affiliation>Ibs &amp; Kaist</affiliation></author>
      <pages>697-704</pages>
      <abstract>Quotes are critical for establishing credibility in news articles. A direct quote enclosed in quotation marks has a strong visual appeal and is a sign of a reliable citation. Unfortunately, this journalistic practice is not strictly followed, and a quote in the headline is often “contextomized.” Such a quote uses words out of context in a way that alters the speaker’s intention so that there is no semantically matching quote in the body text. We present QuoteCSE, a contrastive learning framework that represents the embedding of news quotes based on domain-driven positive and negative samples to identify such an editorial strategy. The dataset and code are available at https://github.com/ssu-humane/contextomized-quote-contrastive.</abstract>
      <url hash="f02da065">2023.findings-eacl.52</url>
      <bibkey>song-etal-2023-detecting</bibkey>
    </paper>
    <paper id="53">
      <title>Zero-Shot On-the-Fly Event Schema Induction</title>
      <author><first>Rotem</first><last>Dror</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Haoyu</first><last>Wang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>705-725</pages>
      <abstract>What are the events involved in a pandemic outbreak? What steps should be taken when planning a wedding? The answers to these questions can be found by collecting many documents on the complex event of interest, extracting relevant information, and analyzing it. We present a new approach in which large language models are utilized to generate source documents that allow predicting, given a high-level event definition, the specific events, arguments, and relations between them to construct a schema that describes the complex event in its entirety.Using our model, complete schemas on any topic can be generated on-the-fly without any manual data collection, i.e., in a zero-shot manner. Moreover, we develop efficient methods to extract pertinent information from texts and demonstrate in a series of experiments that these schemas are considered to be more complete than human-curated ones in the majority of examined scenarios. Finally, we show that this framework is comparable in performance with previous supervised schema induction methods that rely on collecting real texts and even reaching the best score in the prediction task.</abstract>
      <url hash="fa971b33">2023.findings-eacl.53</url>
      <bibkey>dror-etal-2023-zero</bibkey>
    </paper>
    <paper id="54">
      <title><fixed-case>B</fixed-case>angla<fixed-case>NLG</fixed-case> and <fixed-case>B</fixed-case>angla<fixed-case>T</fixed-case>5: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in <fixed-case>B</fixed-case>angla</title>
      <author><first>Abhik</first><last>Bhattacharjee</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Tahmid</first><last>Hasan</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Wasi Uddin</first><last>Ahmad</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Rifat</first><last>Shahriyar</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <pages>726-735</pages>
      <abstract>This work presents ‘BanglaNLG,’ a comprehensive benchmark for evaluating natural language generation (NLG) models in Bangla, a widely spoken yet low-resource language. We aggregate six challenging conditional text generation tasks under the BanglaNLG benchmark, introducing a new dataset on dialogue generation in the process. Furthermore, using a clean corpus of 27.5 GB of Bangla data, we pretrain ‘BanglaT5’, a sequence-to-sequence Transformer language model for Bangla. BanglaT5 achieves state-of-the-art performance in all of these tasks, outperforming several multilingual models by up to 9% absolute gain and 32% relative gain. We are making the new dialogue dataset and the BanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in the hope of advancing future research on Bangla NLG.</abstract>
      <url hash="446bdae0">2023.findings-eacl.54</url>
      <bibkey>bhattacharjee-etal-2023-banglanlg</bibkey>
    </paper>
    <paper id="55">
      <title>It’s about Time: Rethinking Evaluation on Rumor Detection Benchmarks using Chronological Splits</title>
      <author><first>Yida</first><last>Mu</last><affiliation>The University of Sheffield</affiliation></author>
      <author><first>Kalina</first><last>Bontcheva</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Nikolaos</first><last>Aletras</last><affiliation>University of Sheffield</affiliation></author>
      <pages>736-743</pages>
      <abstract>New events emerge over time influencing the topics of rumors in social media. Current rumor detection benchmarks use random splits as training, development and test sets which typically results in topical overlaps. Consequently, models trained on random splits may not perform well on rumor classification on previously unseen topics due to the temporal concept drift. In this paper, we provide a re-evaluation of classification models on four popular rumor detection benchmarks considering chronological instead of random splits. Our experimental results show that the use of random splits can significantly overestimate predictive performance across all datasets and models. Therefore, we suggest that rumor detection models should always be evaluated using chronological splits for minimizing topical overlaps.</abstract>
      <url hash="4cf0b62b">2023.findings-eacl.55</url>
      <bibkey>mu-etal-2023-time</bibkey>
    </paper>
    <paper id="56">
      <title><fixed-case>MUTANT</fixed-case>: A Multi-sentential Code-mixed <fixed-case>H</fixed-case>inglish Dataset</title>
      <author><first>Rahul</first><last>Gupta</last><affiliation>Indian Institute of Technology Gandhinagar</affiliation></author>
      <author><first>Vivek</first><last>Srivastava</last><affiliation>TCS Research</affiliation></author>
      <author><first>Mayank</first><last>Singh</last><affiliation>IIT Gandhinagar</affiliation></author>
      <pages>744-753</pages>
      <abstract>The multi-sentential long sequence textual data unfolds several interesting research directions pertaining to natural language processing and generation. Though we observe several high-quality long-sequence datasets for English and other monolingual languages, there is no significant effort in building such resources for code-mixed languages such as Hinglish (code-mixing of Hindi-English). In this paper, we propose a novel task of identifying multi-sentential code-mixed text (MCT) from multilingual articles. As a use case, we leverage multilingual articles from two different data sources and build a first-of-its-kind multi-sentential code-mixed Hinglish dataset i.e., MUTANT. We propose a token-level language-aware pipeline and extend the existing metrics measuring the degree of code-mixing to a multi-sentential framework and automatically identify MCT in the multilingual articles. The MUTANT dataset comprises 67k articles with 85k identified Hinglish MCTs. To facilitate future research directions, we will make the dataset and the code publicly available upon publication.</abstract>
      <url hash="aea13fc3">2023.findings-eacl.56</url>
      <bibkey>gupta-etal-2023-mutant</bibkey>
    </paper>
    <paper id="57">
      <title>Bridging the Gap between Native Text and Translated Text through Adversarial Learning: A Case Study on Cross-Lingual Event Extraction</title>
      <author><first>Pengfei</first><last>Yu</last><affiliation>Department of Computer Science, University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>USC Information Sciences Institute</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois at Urbana-Champaign and Amazon (Amazon Scholar)</affiliation></author>
      <pages>754-769</pages>
      <abstract>Recent research in cross-lingual learning has found that combining large-scale pretrained multilingual language models with machine translation can yield good performance. We explore this idea for cross-lingual event extraction with a new model architecture that jointly encodes a source language input sentence with its translation to the target language during training, and takes a target language sentence with its translation back to the source language as input during evaluation. However, we observe significant representational gap between the native source language texts during training and the texts translated into source language during evaluation, as well as the texts translated into target language during training and the native target language texts during evaluation. This representational gap undermines the effectiveness of cross-lingual transfer learning for event extraction with machine-translated data. In order to mitigate this problem, we propose an adversarial training framework that encourages the language model to produce more similar representations for the translated text and the native text. To be specific, we train the language model such that its hidden representations are able to fool a jointly trained discriminator that distinguishes translated texts’ representations from native texts’ representations. We conduct experiments on cross-lingual for event extraction across three languages. Results demonstrate that our proposed adversarial training can effectively incorporate machine translation to improve event extraction, while simply adding machine-translated data yields unstable performance due to the representational gap.</abstract>
      <url hash="3ee70b0f">2023.findings-eacl.57</url>
      <bibkey>yu-etal-2023-bridging</bibkey>
    </paper>
    <paper id="58">
      <title>Scalable Prompt Generation for Semi-supervised Learning with Language Models</title>
      <author><first>Yuhang</first><last>Zhou</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Suraj</first><last>Maharjan</last><affiliation>Amazon</affiliation></author>
      <author><first>Beiye</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <pages>770-781</pages>
      <abstract>Prompt-based learning methods in semi-supervised learning (SSL) settings have been shown to be effective on multiple natural language understanding (NLU) datasets and tasks in the literature. However, manually designing multiple prompts and verbalizers requires domain knowledge and human effort, making it difficult and expensive to scale across different datasets. In this paper, we propose two methods to automatically design multiple prompts and integrate automatic verbalizer in SSL settings without sacrificing performance. The first method uses various demonstration examples with learnable continuous prompt tokens to create diverse prompt models. The second method uses a varying number of soft prompt tokens to encourage language models to learn different prompts. For the verbalizer, we use the prototypical verbalizer to replace the manual one. In summary, we obtained the best average accuracy of 71.5% (a relative improvement of 0.99% over even the previous state-of-the-art SSL method with manual prompts and verbalizers) in different few-shot learning settings.</abstract>
      <url hash="d67240cb">2023.findings-eacl.58</url>
      <bibkey>zhou-etal-2023-scalable</bibkey>
    </paper>
    <paper id="59">
      <title>Novel Feature Discovery for Task-Oriented Dialog Systems</title>
      <author><first>Vinh Thinh</first><last>Ho</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Mohamed</first><last>Soliman</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Abdalghani</first><last>Abujabal</last><affiliation>Amazon Alexa AI</affiliation></author>
      <pages>782-792</pages>
      <abstract>A novel feature represents a cluster of semantically equivalent novel user requests e.g., requests to play a song on a service or read user’s messages.Detecting and supporting novel features is crucial towards wider adoption of dialog systems by end users.Intuitively, features are represented by a combination of intents, slot types and/or their values. For example, while playing a song is a feature represented by a single intent (PlayMusic) only, playing a song on a service is another feature represented by the combination of PlayMusic intent and ServiceName slot type.Prior work on novelty detection limits the scope of features to those represented by novel single intents, leading to (1) giant clusters spanning several user-perceived fine-grained features belonging to the same intent, (2) incoherent interpretation of clusters from users’ perspective (no direct connection to some user-perceived feature), and (3) missing those features spanning several intents.In this work, we introduce feature discovery as opposed to single intent discovery, which aims at discovering novel features spanning a combination of intents and slots, and present a technique for discovering novel features from user utterances. Experiments on two datasets demonstrate the effectiveness of our approach and consistently show its ability to detect novel features.</abstract>
      <url hash="29804f4b">2023.findings-eacl.59</url>
      <bibkey>ho-etal-2023-novel</bibkey>
    </paper>
    <paper id="60">
      <title>Context Generation Improves Open Domain Question Answering</title>
      <author><first>Dan</first><last>Su</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Mostofa</first><last>Patwary</last><affiliation>Nvidia</affiliation></author>
      <author><first>Shrimai</first><last>Prabhumoye</last><affiliation>Nvidia</affiliation></author>
      <author><first>Peng</first><last>Xu</last><affiliation>Nvidia</affiliation></author>
      <author><first>Ryan</first><last>Prenger</last><affiliation>Nvidia</affiliation></author>
      <author><first>Mohammad</first><last>Shoeybi</last><affiliation>Nvidia</affiliation></author>
      <author><first>Pascale</first><last>Fung</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Anima</first><last>Anandkumar</last><affiliation>California Institute of Technology</affiliation></author>
      <author><first>Bryan</first><last>Catanzaro</last><affiliation>Nvidia</affiliation></author>
      <pages>793-808</pages>
      <abstract>Closed-book question answering (QA) requires a model to directly answer an open-domain question without access to any external knowledge. Prior work on closed-book QA either directly finetunes or prompts a pretrained language model (LM) to leverage the stored knowledge. However, they do not fully exploit the parameterized knowledge. To address this inefficiency, we propose a two-stage, closed-book QA framework which employs a coarse-to-fine approach to extract the relevant knowledge and answer a question. We first generate a related context for a given question by prompting a pretrained LM. We then prompt the same LM to generate an answer using the generated context and the question. Additionally, we marginalize over the generated contexts to improve the accuracies and reduce context uncertainty. Experimental results on three QA benchmarks show that our method significantly outperforms previous closed-book QA methods. For example on TriviaQA, our method improves exact match accuracy from 55.3% to 68.6%, and is on par with open-book QA methods (68.6% vs. 68.0%). Our results show that our new methodology is able to better exploit the stored knowledge in pretrained LMs without adding extra learnable parameters or needing finetuning, and paves the way for hybrid models that integrate pretrained LMs with external knowledge.</abstract>
      <url hash="4feb32fd">2023.findings-eacl.60</url>
      <bibkey>su-etal-2023-context</bibkey>
    </paper>
    <paper id="61">
      <title><fixed-case>R</fixed-case>ed<fixed-case>HOT</fixed-case>: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media</title>
      <author><first>Somin</first><last>Wadhwa</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Vivek</first><last>Khetan</last><affiliation>Accenture Labs</affiliation></author>
      <author><first>Silvio</first><last>Amir</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Byron</first><last>Wallace</last><affiliation>Northeastern University</affiliation></author>
      <pages>809-827</pages>
      <abstract>We present Reddit Health Online Talk (RedHOT), a corpus of 22,000 richly annotated social media posts from Reddit spanning 24 health conditions. Annotations include demarcations of spans corresponding to medical claims, personal experiences, and questions.We collect additional granular annotations on identified claims.Specifically, we mark snippets that describe patient Populations, Interventions, and Outcomes (PIO elements) within these. Using this corpus, we introduce the task of retrieving trustworthy evidence relevant to a given claim made on social media. We propose a new method to automatically derive (noisy) supervision for this task which we use to train a dense retrieval model; this outperforms baseline models. Manual evaluation of retrieval results performed by medical doctors indicate that while our system performance is promising, there is considerable room for improvement.We release all annotations collected (and scripts to assemble the dataset), and all code necessary to reproduce the results in this paper at: https://sominw.com/redhot.</abstract>
      <url hash="642a6f2e">2023.findings-eacl.61</url>
      <bibkey>wadhwa-etal-2023-redhot</bibkey>
    </paper>
    <paper id="62">
      <title>Paparazzi: A Deep Dive into the Capabilities of Language and Vision Models for Grounding Viewpoint Descriptions</title>
      <author><first>Henrik</first><last>Voigt</last><affiliation>Friedrich-Schiller-University</affiliation></author>
      <author><first>Jan</first><last>Hombeck</last><affiliation>University of Jena</affiliation></author>
      <author><first>Monique</first><last>Meuschke</last><affiliation>University of Jena</affiliation></author>
      <author><first>Kai</first><last>Lawonn</last><affiliation>University of Jena</affiliation></author>
      <author><first>Sina</first><last>Zarrieß</last><affiliation>University of Bielefeld</affiliation></author>
      <pages>828-843</pages>
      <abstract>Existing language and vision models achieve impressive performance in image-text understanding. Yet, it is an open question to what extent they can be used for language understanding in 3D environments and whether they implicitly acquire 3D object knowledge, e.g. about different views of an object.In this paper, we investigate whether a state-of-the-art language and vision model, CLIP, is able to ground perspective descriptions of a 3D object and identify canonical views of common objects based on text queries.We present an evaluation framework that uses a circling camera around a 3D object to generate images from different viewpoints and evaluate them in terms of their similarity to natural language descriptions.We find that a pre-trained CLIP model performs poorly on most canonical views and that fine-tuning using hard negative sampling and random contrasting yields good results even under conditions with little available training data.</abstract>
      <url hash="a232eaf5">2023.findings-eacl.62</url>
      <bibkey>voigt-etal-2023-paparazzi</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>PLACES</fixed-case>: Prompting Language Models for Social Conversation Synthesis</title>
      <author><first>Maximillian</first><last>Chen</last><affiliation>Columbia University</affiliation></author>
      <author><first>Alexandros</first><last>Papangelis</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Chenyang</first><last>Tao</last><affiliation>Amazon</affiliation></author>
      <author><first>Seokhwan</first><last>Kim</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Andy</first><last>Rosenbaum</last><affiliation>Amazon</affiliation></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last><affiliation>Amazon Alexa AI</affiliation></author>
      <pages>844-868</pages>
      <abstract>Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.</abstract>
      <url hash="2de5ef99">2023.findings-eacl.63</url>
      <bibkey>chen-etal-2023-places</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>F</fixed-case>ed<fixed-case>P</fixed-case>er<fixed-case>C</fixed-case>: Federated Learning for Language Generation with Personal and Context Preference Embeddings</title>
      <author><first>Andrew</first><last>Silva</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Pradyumna</first><last>Tambwekar</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Matthew</first><last>Gombolay</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>869-882</pages>
      <abstract>Federated learning is a training paradigm that learns from multiple distributed users without aggregating data on a centralized server, promising the ability to deploy machine-learning to a diverse population of users without first collecting large, labeled datasets.As federated learning involves averaging gradient updates across a decentralized population, there is a growing need for personalization of federated learning systems (i.e. conversational agents must personalize to individual users and the context of an interaction).In this work, we propose a new direction for personalization research within federated learning, leveraging both personal embeddings and shared context embeddings.We also present an approach to predict these “preference” embeddings, enabling personalization without backpropagation. Compared to state-of-the-art personalization baselines, our approach achieves a 50% improvement in test-time perplexity using 0.001% of the memory required by baseline approaches, and achieving greater sample- and compute-efficiency.</abstract>
      <url hash="1e5283fc">2023.findings-eacl.64</url>
      <bibkey>silva-etal-2023-fedperc</bibkey>
    </paper>
    <paper id="65">
      <title>A Neural <fixed-case>CRF</fixed-case>-based Hierarchical Approach for Linear Text Segmentation</title>
      <author><first>Inderjeet</first><last>Nair</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Aparna</first><last>Garimella</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last><affiliation>Adobe Research, India</affiliation></author>
      <author><first>Natwar</first><last>Modani</last><affiliation>Adobe Research, India</affiliation></author>
      <author><first>Niyati</first><last>Chhaya</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Srikrishna</first><last>Karanam</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Sumit</first><last>Shekhar</last><affiliation>Adobe Systems</affiliation></author>
      <pages>883-893</pages>
      <abstract>We consider the problem of segmenting unformatted text and transcripts linearly based on their topical structure. While prior approaches explicitly train to predict segment boundaries, our proposed approach solves this task by inferring the hierarchical segmentation structure associated with the input text fragment. Given the lack of a large annotated dataset for this task, we propose a data curation strategy and create a corpus of over 700K Wikipedia articles with their hierarchical structures. We then propose the first supervised approach to generating hierarchical segmentation structures based on these annotations. Our method, in particular, is based on a neural conditional random field (CRF), which explicitly models the statistical dependency between a node and its constituent child nodes. We introduce a new data augmentation scheme as part of our model training strategy, which involves sampling a variety of node aggregations, permutations, and removals, all of which help capture fine-grained and coarse topical shifts in the data and improve model performance. Extensive experiments show that our model outperforms or achieves competitive performance when compared to previous state-of-the-art algorithms in the following settings: rich-resource, cross-domain transferability, few-shot supervision, and segmentation when topic label annotations are provided.</abstract>
      <url hash="34687b17">2023.findings-eacl.65</url>
      <bibkey>nair-etal-2023-neural</bibkey>
    </paper>
    <paper id="66">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>F</fixed-case>in: A Dataset for Multilingual Financial <fixed-case>NLP</fixed-case></title>
      <author><first>Rasmus</first><last>Jørgensen</last><affiliation>University of Copenhagen, UCPH</affiliation></author>
      <author><first>Oliver</first><last>Brandt</last><affiliation>Independent researcher</affiliation></author>
      <author><first>Mareike</first><last>Hartmann</last><affiliation>Saarland University</affiliation></author>
      <author><first>Xiang</first><last>Dai</last><affiliation>CSIRO Data61</affiliation></author>
      <author><first>Christian</first><last>Igel</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>894-909</pages>
      <abstract>Financial information is generated and distributed across the world, resulting in a vast amount of domain-specific multilingual data. Multilingual models adapted to the financial domain would ease deployment when an organization needs to work with multiple languages on a regular basis. For the development and evaluation of such models, there is a need for multilingual financial language processing datasets. We describe MultiFin – a publicly available financial dataset consisting of real-world article headlines covering 15 languages across different writing systems and language families. The dataset consists of hierarchical label structure providing two classification tasks: multi-label and multi-class. We develop our annotation schema based on a real-world application and annotate our dataset using both ‘label by native-speaker’ and ‘translate-then-label’ approaches. The evaluation of several popular multilingual models, e.g., mBERT, XLM-R, and mT5, show that although decent accuracy can be achieved in high-resource languages, there is substantial room for improvement in low-resource languages.</abstract>
      <url hash="a16a2890">2023.findings-eacl.66</url>
      <bibkey>jorgensen-etal-2023-multifin</bibkey>
    </paper>
    <paper id="67">
      <title><fixed-case>MLASK</fixed-case>: Multimodal Summarization of Video-based News Articles</title>
      <author><first>Mateusz</first><last>Krubiński</last><affiliation>Charles University</affiliation></author>
      <author><first>Pavel</first><last>Pecina</last><affiliation>Charles University</affiliation></author>
      <pages>910-924</pages>
      <abstract>In recent years, the pattern of news consumption has been changing. The most popular multimedia news formats are now multimodal - the reader is often presented not only with a textual article but also with a short, vivid video. To draw the attention of the reader, such video-based articles are usually presented as a short textual summary paired with an image thumbnail.In this paper, we introduce MLASK (MultimodaL Article Summarization Kit) - a new dataset of video-based news articles paired with a textual summary and a cover picture, all obtained by automatically crawling several news websites. We demonstrate how the proposed dataset can be used to model the task of multimodal summarization by training a Transformer-based neural model. We also examine the effects of pre-training when the usage of generative pre-trained language models helps to improve the model performance, but (additional) pre-training on the simpler task of text summarization yields even better results. Our experiments suggest that the benefits of pre-training and using additional modalities in the input are not orthogonal.</abstract>
      <url hash="7fb171fd">2023.findings-eacl.67</url>
      <bibkey>krubinski-pecina-2023-mlask</bibkey>
    </paper>
    <paper id="68">
      <title>Going beyond research datasets: Novel intent discovery in the industry setting</title>
      <author><first>Aleksandra</first><last>Chrabrowa</last><affiliation>Allegro Machine Learning Research</affiliation></author>
      <author><first>Tsimur</first><last>Hadeliya</last><affiliation>Allegro Sp. z o.o.</affiliation></author>
      <author><first>Dariusz</first><last>Kajtoch</last><affiliation>Allegro sp. z o.o.</affiliation></author>
      <author><first>Robert</first><last>Mroczkowski</last><affiliation>Allegro SP. Z O.O.</affiliation></author>
      <author><first>Piotr</first><last>Rybak</last><affiliation>Institute of Computer Science, Polish Academy of Sciences</affiliation></author>
      <pages>925-941</pages>
      <abstract>Novel intent discovery automates the process of grouping similar messages (questions) to identify previously unknown intents. However, current research focuses on publicly available datasets which have only the question field and significantly differ from real-life datasets. This paper proposes methods to improve the intent discovery pipeline deployed in a large e-commerce platform. We show the benefit of pre-training language models on in-domain data: both self-supervised and with weak supervision. We also devise the best method to utilize the conversational structure (i.e., question and answer) of real-life datasets during fine-tuning for clustering tasks, which we call Conv. All our methods combined to fully utilize real-life datasets give up to 33pp performance boost over state-of-the-art Constrained Deep Adaptive Clustering (CDAC) model for question only. By comparison CDAC model for the question data only gives only up to 13pp performance boost over the naive baseline.</abstract>
      <url hash="c1dcfdec">2023.findings-eacl.68</url>
      <bibkey>chrabrowa-etal-2023-going</bibkey>
    </paper>
    <paper id="69">
      <title><fixed-case>DATS</fixed-case>core: Evaluating Translation with Data Augmented Translations</title>
      <author><first>Moussa</first><last>Kamal Eddine</last><affiliation>École polytechnique</affiliation></author>
      <author><first>Guokan</first><last>Shang</last><affiliation>Linagora</affiliation></author>
      <author><first>Michalis</first><last>Vazirgiannis</last><affiliation>Ecole Polytechnique</affiliation></author>
      <pages>942-952</pages>
      <abstract>The rapid development of large pretrained language models has revolutionized not only the field of Natural Language Generation (NLG) but also its evaluation. Inspired by the recent work of BARTScore: a metric leveraging the BART language model to evaluate the quality of generated text from various aspects, we introduce DATScore. DATScore uses data augmentation techniques to improve the evaluation of machine translation. Our main finding is that introducing data augmented translations of the source and reference texts is greatly helpful in evaluating the quality of the generated translation. We also propose two novel score averaging and term weighting strategies to improve the original score computing process of BARTScore. Experimental results on WMT show that DATScore correlates better with human meta-evaluations than the other recent state-of-the-art metrics, especially for low-resource languages. Ablation studies demonstrate the value added by our new scoring strategies. Moreover, we report in our extended experiments the performance of DATScore on 3 NLG tasks other than translation.</abstract>
      <url hash="89833e58">2023.findings-eacl.69</url>
      <bibkey>kamal-eddine-etal-2023-datscore</bibkey>
    </paper>
    <paper id="70">
      <title>How do decoding algorithms distribute information in dialogue responses?</title>
      <author><first>Saranya</first><last>Venkatraman</last><affiliation>The Pennsylvania State University</affiliation></author>
      <author><first>He</first><last>He</last><affiliation>New York University</affiliation></author>
      <author><first>David</first><last>Reitter</last><affiliation>Google Research</affiliation></author>
      <pages>953-962</pages>
      <abstract>Humans tend to follow the Uniform Information Density (UID) principle by distributing information evenly in utterances. We study if decoding algorithms implicitly follow this UID principle, and under what conditions adherence to UID might be desirable for dialogue generation. We generate responses using different decoding algorithms with GPT-2 on the Persona-Chat dataset and collect human judgments on their quality using Amazon Mechanical Turk. We find that (i) surprisingly, model-generated responses follow the UID principle to a greater extent than human responses, and (ii) decoding algorithms that promote UID do not generate higher-quality responses. Instead, when we control for surprisal, non-uniformity of information density correlates with the quality of responses with very low/high surprisal. Our findings indicate that encouraging non-uniform responses is a potential solution to the “likelihood trap” problem (quality degradation in very high-likelihood text). Our dataset containing multiple candidate responses per dialog history along with human-annotated quality ratings is available at: https://huggingface.co/datasets/saranya132/dialog_uid_gpt2.</abstract>
      <url hash="495c58f4">2023.findings-eacl.70</url>
      <bibkey>venkatraman-etal-2023-decoding</bibkey>
    </paper>
    <paper id="71">
      <title>Benchmarking Long-tail Generalization with Likelihood Splits</title>
      <author><first>Ameya</first><last>Godbole</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Robin</first><last>Jia</last><affiliation>University of Southern California</affiliation></author>
      <pages>963-983</pages>
      <abstract>In order to reliably process natural language, NLP systems must generalize to the long tail of rare utterances. We propose a method to create challenging benchmarks that require generalizing to the tail of the distribution by re-splitting existing datasets. We create ‘Likelihood Splits’ where examples that are assigned lower likelihood by a pre-trained language model (LM) are placed in the test set, and more likely examples are in the training set. This simple approach can be customized to construct meaningful train-test splits for a wide range of tasks. Likelihood Splits surface more challenges than random splits: relative error rates of state-of-the-art models increase by 59% for semantic parsing on Spider, 93% for natural language inference on SNLI, and 33% for yes/no question answering on BoolQ, on our splits compared with the corresponding random splits. Moreover, Likelihood Splits create fairer benchmarks than adversarial filtering; when the LM used to create the splits is also employed as the task model, our splits do not unfairly penalize the LM.</abstract>
      <url hash="c3e991da">2023.findings-eacl.71</url>
      <bibkey>godbole-jia-2023-benchmarking</bibkey>
    </paper>
    <paper id="72">
      <title>Exploring Enhanced Code-Switched Noising for Pretraining in Neural Machine Translation</title>
      <author><first>Vivek</first><last>Iyer</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Arturo</first><last>Oncevay</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>984-998</pages>
      <abstract>Multilingual pretraining approaches in Neural Machine Translation (NMT) have shown that training models to denoise synthetic code-switched data can yield impressive performance gains — owing to better multilingual semantic representations and transfer learning. However, they generated the synthetic code-switched data using non-contextual, one-to-one word translations obtained from lexicons - which can lead to significant noise in a variety of cases, including the poor handling of polysemes and multi-word expressions, violation of linguistic agreement and inability to scale to agglutinative languages. To overcome these limitations, we propose an approach called Contextual Code-Switching (CCS), where contextual, many-to-many word translations are generated using a ‘base’ NMT model. We conduct experiments on 3 different language families - Romance, Uralic, and Indo-Aryan - and show significant improvements (by up to 5.5 spBLEU points) over the previous lexicon-based SOTA approaches. We also observe that small CCS models can perform comparably or better than massive models like mBART50 and mRASP2, depending on the size of data provided. We empirically analyse several key factors responsible for these - including context, many-to-many substitutions, code-switching language count etc. - and prove that they all contribute to enhanced pretraining of multilingual NMT models.</abstract>
      <url hash="f593d081">2023.findings-eacl.72</url>
      <attachment type="software" hash="d4ce2bc3">2023.findings-eacl.72.software.zip</attachment>
      <bibkey>iyer-etal-2023-exploring</bibkey>
    </paper>
    <paper id="73">
      <title><fixed-case>XQA</fixed-case>-<fixed-case>DST</fixed-case>: Multi-Domain and Multi-Lingual Dialogue State Tracking</title>
      <author><first>Han</first><last>Zhou</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Ignacio</first><last>Iacobacci</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>Ucl</affiliation></author>
      <pages>999-1009</pages>
      <abstract>Dialogue State Tracking (DST), a crucial component of task-oriented dialogue (ToD) systems, keeps track of all important information pertaining to dialogue history: filling slots with the most probable values throughout the conversation. Existing methods generally rely on a predefined set of values and struggle to generalise to previously unseen slots in new domains. To overcome these challenges, we propose a domain-agnostic extractive question answering (QA) approach with shared weights across domains. To disentangle the complex domain information in ToDs, we train our DST with a novel domain filtering strategy by excluding out-of-domain question samples. With an independent classifier that predicts the presence of multiple domains given the context, our model tackles DST by extracting spans in active domains. Empirical results demonstrate that our model can efficiently leverage domain-agnostic QA datasets by two-stage fine-tuning while being both domain-scalable and open vocabulary in DST. It shows strong transferability by achieving zero-shot domain-adaptation results on MultiWOZ 2.1 with an average JGA of 36.7%. It further achieves cross-lingual transfer with state-of-the-art zero-shot results, 66.2% JGA from English to German and 75.7% JGA from English to Italian on WOZ 2.0.</abstract>
      <url hash="8d094ece">2023.findings-eacl.73</url>
      <bibkey>zhou-etal-2023-xqa</bibkey>
    </paper>
    <paper id="74">
      <title>Improving Prediction Backward-Compatiblility in <fixed-case>NLP</fixed-case> Model Upgrade with Gated Fusion</title>
      <author><first>Yi-An</first><last>Lai</last><affiliation>Amazon</affiliation></author>
      <author><first>Elman</first><last>Mansimov</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Yuqing</first><last>Xie</last><affiliation>The University of Waterloo</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>Amazon AI</affiliation></author>
      <pages>1010-1022</pages>
      <abstract>When upgrading neural models to a newer version, new errors that were not encountered in the legacy version can be introduced, known as regression errors. This inconsistent behavior during model upgrade often outweighs the benefits of accuracy gain and hinders the adoption of new models. To mitigate regression errors from model upgrade, distillation and ensemble have proven to be viable solutions without significant compromise in performance. Despite the progress, these approaches attained an incremental reduction in regression which is still far from achieving backward-compatible model upgrade. In this work, we propose a novel method, Gated Fusion, that promotes backward compatibility via learning to mix predictions between old and new models. Empirical results on two distinct model upgrade scenarios show that our method reduces the number of regression errors by 62% on average, outperforming the strongest baseline by an average of 25%.</abstract>
      <url hash="70552620">2023.findings-eacl.74</url>
      <bibkey>lai-etal-2023-improving</bibkey>
    </paper>
    <paper id="75">
      <title><fixed-case>A</fixed-case>mbi<fixed-case>C</fixed-case>oref: Evaluating Human and Model Sensitivity to Ambiguous Coreference</title>
      <author><first>Yuewei</first><last>Yuan</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Chaitanya</first><last>Malaviya</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Mark</first><last>Yatskar</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>1023-1030</pages>
      <abstract>Given a sentence “Abby told Brittney that she upset Courtney”, one would struggle to understand who “she” refers to, and ask for clarification. However, if the word “upset” were replaced with “hugged”, “she” unambiguously refers to Abby. We study if modern coreference resolution models are sensitive to such pronominal ambiguity. To this end, we construct AmbiCoref, a diagnostic corpus of minimal sentence pairs with ambiguous and unambiguous referents. Our examples generalize psycholinguistic studies of human perception of ambiguity around particular arrangements of verbs and their arguments. Analysis shows that (1) humans are less sure of referents in ambiguous AmbiCoref examples than unambiguous ones, and (2) most coreference models show little difference in output between ambiguous and unambiguous pairs. We release AmbiCoref as a diagnostic corpus for testing whether models treat ambiguity similarly to humans.</abstract>
      <url hash="e613a4bd">2023.findings-eacl.75</url>
      <bibkey>yuan-etal-2023-ambicoref</bibkey>
    </paper>
    <paper id="76">
      <title>Improving Unsupervised Out-of-domain detection through Pseudo Labeling and Learning</title>
      <author><first>Byounghan</first><last>Lee</last><affiliation>Department of Artificial Intelligence, Ajou university</affiliation></author>
      <author><first>Jaesik</first><last>Kim</last><affiliation>Department of Bioengineering, University of Pennsylvania</affiliation></author>
      <author><first>Junekyu</first><last>Park</last><affiliation>Superb AI</affiliation></author>
      <author><first>Kyung-Ah</first><last>Sohn</last><affiliation>Ajou University</affiliation></author>
      <pages>1031-1041</pages>
      <abstract>Unsupervised out-of-domain (OOD) detection is a task aimed at discriminating whether given samples are from the in-domain or not, without the categorical labels of in-domain instances. Unlike supervised OOD, as there are no labels for training a classifier, previous works on unsupervised OOD detection adopted the one-class classification (OCC) approach, assuming that the training samples come from a single domain. However, in-domain instances in many real world applications can have a heterogeneous distribution (i.e., across multiple domains or multiple classes). In this case, OCC methods have difficulty in reflecting the categorical information of the domain properly. To tackle this issue, we propose a two-stage framework that leverages the latent categorical information to improve representation learning for textual OOD detection. In the first stage, we train a transformer-based sentence encoder for pseudo labeling by contrastive loss and cluster loss. The second stage is pseudo label learning in which the model is re-trained with pseudo-labels obtained in the first stage. The empirical results on the three datasets show that our two-stage framework significantly outperforms baseline models in more challenging scenarios.</abstract>
      <url hash="ac84f823">2023.findings-eacl.76</url>
      <bibkey>lee-etal-2023-improving</bibkey>
    </paper>
    <paper id="77">
      <title>How Many Data Samples is an Additional Instruction Worth?</title>
      <author><first>Ravsehaj Singh</first><last>Puri</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Swaroop</first><last>Mishra</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Mihir</first><last>Parmar</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>1042-1057</pages>
      <abstract>Recently introduced instruction-paradigm empowers non-expert users to leverage NLP resources by defining a new task in natural language. Instruction-tuned models have significantly outperformed multitask learning models (without instruction); however they are far from state-of-the-art task-specific models. Conventional approaches to improve model performance via creating datasets with large number of task instances or architectural changes in the model may not be feasible for non-expert users. However, they can write alternate instructions to represent an instruction task. Is Instruction-augmentation helpful? We augment a subset of tasks in the expanded version of NATURAL INSTRUCTIONS with additional instructions and find that it significantly improves model performance (up to 35%), especially in the low-data regime. Our results indicate that an additional instruction can be equivalent to ~200 data samples on average across tasks.</abstract>
      <url hash="c88278a7">2023.findings-eacl.77</url>
      <bibkey>puri-etal-2023-many</bibkey>
    </paper>
    <paper id="78">
      <title>[<fixed-case>MASK</fixed-case>] Insertion: a robust method for anti-adversarial attacks</title>
      <author><first>Xinrong</first><last>Hu</last><affiliation>Wuhan Textile University</affiliation></author>
      <author><first>Ce</first><last>Xu</last><affiliation>Wuhan Textile University</affiliation></author>
      <author><first>Junlong</first><last>Ma</last><affiliation>Wuhan Textile University</affiliation></author>
      <author><first>Zijian</first><last>Huang</last><affiliation>Wuhan Textile University</affiliation></author>
      <author><first>Jie</first><last>Yang</last><affiliation>University of Wollongong</affiliation></author>
      <author><first>Yi</first><last>Guo</last><affiliation>Western Sydney University</affiliation></author>
      <author><first>Johan</first><last>Barthelemy</last><affiliation>Nvidia</affiliation></author>
      <pages>1058-1070</pages>
      <abstract>Adversarial attack aims to perturb input sequences and mislead a trained model for false predictions. To enhance the model robustness, defensing methods are accordingly employed by either data augmentation (involving adversarial samples) or model enhancement (modifying the training loss and/or model architecture). In contrast to previous work, this paper revisits the masked language modeling (MLM) and presents a simple yet efficient algorithm against adversarial attacks, termed [MASK] insertion for defensing (MI4D). Specifically, MI4D simply inserts [MASK] tokens to input sequences during training and inference, maximizing the intersection of the new convex hull (MI4D creates) with the original one (the clean input forms). As neither additional adversarial samples nor the model modification is required, MI4D is as computationally efficient as traditional fine-tuning. Comprehensive experiments have been conducted using three benchmark datasets and four attacking methods. MI4D yields a significant improvement (on average) of the accuracy between 3.2 and 11.1 absolute points when compared with six state-of-the-art defensing baselines.</abstract>
      <url hash="b5372da2">2023.findings-eacl.78</url>
      <bibkey>hu-etal-2023-mask</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>V</fixed-case>i<fixed-case>D</fixed-case>e<fixed-case>BERT</fixed-case>a: A powerful pre-trained language model for <fixed-case>V</fixed-case>ietnamese</title>
      <author><first>Cong Dao</first><last>Tran</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Nhut Huy</first><last>Pham</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Anh Tuan</first><last>Nguyen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Truong Son</first><last>Hy</last><affiliation>University of California San Diego</affiliation></author>
      <author><first>Tu</first><last>Vu</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>1071-1078</pages>
      <abstract>This paper presents ViDeBERTa, a new pre-trained monolingual language model for Vietnamese, with three versions - ViDeBERTa_xsmall, ViDeBERTa_base, and ViDeBERTa_large, which are pre-trained on a large-scale corpus of high-quality and diverse Vietnamese texts using DeBERTa architecture. Although many successful pre-trained language models based on Transformer have been widely proposed for the English language, there are still few pre-trained models for Vietnamese, a low-resource language, that perform good results on downstream tasks, especially Question answering. We fine-tune and evaluate our model on three important natural language downstream tasks, Part-of-speech tagging, Named-entity recognition, and Question answering. The empirical results demonstrate that ViDeBERTa with far fewer parameters surpasses the previous state-of-the-art models on multiple Vietnamese-specific natural language understanding tasks. Notably, ViDeBERTa_base with 86M parameters, which is only about 23% of PhoBERT_large with 370M parameters, still performs the same or better results than the previous state-of-the-art model. Our ViDeBERTa models are available at: https://github.com/HySonLab/ViDeBERTa.</abstract>
      <url hash="8cf0f7f1">2023.findings-eacl.79</url>
      <bibkey>tran-etal-2023-videberta</bibkey>
    </paper>
    <paper id="80">
      <title><fixed-case>N</fixed-case>ap<fixed-case>SS</fixed-case>: Paragraph-level Medical Text Simplification via Narrative Prompting and Sentence-matching Summarization</title>
      <author><first>Junru</first><last>Lu</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Jiazheng</first><last>Li</last><affiliation>King’s College London</affiliation></author>
      <author><first>Byron</first><last>Wallace</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London</affiliation></author>
      <author><first>Gabriele</first><last>Pergola</last><affiliation>University of Warwick</affiliation></author>
      <pages>1079-1091</pages>
      <abstract>Accessing medical literature is difficult for laypeople as the content is written for specialists and contains medical jargon. Automated text simplification methods offer a potential means to address this issue. In this work, we propose a summarize-then-simplify two-stage strategy, which we call NapSS, identifying the relevant content to simplify while ensuring that the original narrative flow is preserved. In this approach, we first generate reference summaries via sentence matching between the original and the simplified abstracts. These summaries are then used to train an extractive summarizer, learning the most relevant content to be simplified. Then, to ensure the narrative consistency of the simplified text, we synthesize auxiliary narrative prompts combining key phrases derived from the syntactical analyses of the original text. Our model achieves results significantly better than the seq2seq baseline on an English medical corpus, yielding 3%~4% absolute improvements in terms of lexical similarity, and providing a further 1.1% improvement of SARI score when combined with the baseline. We also highlight shortcomings of existing evaluation methods, and introduce new metrics that take into account both lexical and high-level semantic similarity. A human evaluation conducted on a random sample of the test set further establishes the effectiveness of the proposed approach. Codes and models are released here: https://github.com/LuJunru/NapSS.</abstract>
      <url hash="8f0ddbee">2023.findings-eacl.80</url>
      <bibkey>lu-etal-2023-napss</bibkey>
    </paper>
    <paper id="81">
      <title>Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions</title>
      <author><first>Ruohong</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yau-Shian</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yiming</first><last>Yang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Donghan</first><last>Yu</last><affiliation>Carnegine Mellon University</affiliation></author>
      <author><first>Tom</first><last>Vu</last><affiliation>Flexport</affiliation></author>
      <author><first>Likun</first><last>Lei</last><affiliation>Flexport</affiliation></author>
      <pages>1092-1106</pages>
      <abstract>Extreme Multi-label Text Classification (XMTC) has been a tough challenge in machine learning research and applications due to the sheer sizes of the label spaces and the severe data scarcity problem associated with the long tail of rare labels in highly skewed distributions. This paper addresses the challenge of tail label prediction by leveraging the power of dense neural retrieval model in mapping input documents (as queries) to relevant label descriptions. To further enhance the quality of label descriptions, we propose to generate pseudo label descriptions from a trained bag-of-words (BoW) classifier, which demonstrates better classification performance under severe scarce data conditions.The proposed approach achieves the state-of-the-art (SOTA) performance of overall label prediction on XMTC benchmark datasets and especially outperforms the SOTA models in the tail label prediction. We also provide a theoretical analysis for relating the BoW and neural models w.r.t. performance lower bound.</abstract>
      <url hash="e13dccf9">2023.findings-eacl.81</url>
      <bibkey>zhang-etal-2023-long</bibkey>
    </paper>
    <paper id="82">
      <title>Unsupervised Keyphrase Extraction via Interpretable Neural Networks</title>
      <author><first>Rishabh</first><last>Joshi</last><affiliation>Google</affiliation></author>
      <author><first>Vidhisha</first><last>Balachandran</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Emily</first><last>Saldanha</last><affiliation>Pacific Northwest National Laboratory</affiliation></author>
      <author><first>Maria</first><last>Glenski</last><affiliation>Pacific Northwest National Laboratory</affiliation></author>
      <author><first>Svitlana</first><last>Volkova</last><affiliation>Aptima Inc.</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>University of Washington</affiliation></author>
      <pages>1107-1119</pages>
      <abstract>Keyphrase extraction aims at automatically extracting a list of “important” phrases representing the key concepts in a document. Prior approaches for unsupervised keyphrase extraction resorted to heuristic notions of phrase importance via embedding clustering or graph centrality, requiring extensive domain expertise. Our work presents a simple alternative approach which defines keyphrases as document phrases that are salient for predicting the topic of the document. To this end, we propose INSPECT—an approach that uses self-explaining models for identifying influential keyphrases in a document by measuring the predictive impact of input phrases on the downstream task of the document topic classification. We show that this novel method not only alleviates the need for ad-hoc heuristics but also achieves state-of-the-art results in unsupervised keyphrase extraction in four datasets across two domains: scientific publications and news articles.</abstract>
      <url hash="2ec19953">2023.findings-eacl.82</url>
      <bibkey>joshi-etal-2023-unsupervised</bibkey>
    </paper>
    <paper id="83">
      <title>Large Language Models are few(1)-shot Table Reasoners</title>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo &amp; Google Research</affiliation></author>
      <pages>1120-1130</pages>
      <abstract>Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus. When combined with ‘chain of thoughts’ prompting, LLMs can achieve very strong performance with only a 1-shot demonstration, even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in {url{https://github.com/wenhuchen/TableCoT}.</abstract>
      <url hash="e86974fb">2023.findings-eacl.83</url>
      <bibkey>chen-2023-large</bibkey>
    </paper>
    <paper id="84">
      <title>Realistic Citation Count Prediction Task for Newly Published Papers</title>
      <author><first>Jun</first><last>Hirako</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Koichi</first><last>Takeda</last><affiliation>Nagoya University</affiliation></author>
      <pages>1131-1141</pages>
      <abstract>Citation count prediction is the task of predicting the future citation counts of academic papers, which is particularly useful for estimating the future impacts of an ever-growing number of academic papers.Although there have been many studies on citation count prediction, they are not applicable to predicting the citation counts of newly published papers, because they assume the availability of future citation counts for papers that have not had enough time pass since publication.In this paper, we first identify problems in the settings of existing studies and introduce a realistic citation count prediction task that strictly uses information available at the time of a target paper’s publication.For realistic citation count prediction, we then propose two methods to leverage the citation counts of papers shortly after publication.Through experiments using papers collected from arXiv and bioRxiv, we demonstrate that our methods considerably improve the performance of citation count prediction for newly published papers in a realistic setting.</abstract>
      <url hash="7c867f32">2023.findings-eacl.84</url>
      <bibkey>hirako-etal-2023-realistic</bibkey>
    </paper>
    <paper id="85">
      <title>“Why do <fixed-case>I</fixed-case> feel offended?” - <fixed-case>K</fixed-case>orean Dataset for Offensive Language Identification</title>
      <author><first>San-Hee</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Kang-Min</first><last>Kim</last><affiliation>The Catholic University of Korea</affiliation></author>
      <author><first>O-Joun</first><last>Lee</last><affiliation>The Catholic University of Korea</affiliation></author>
      <author><first>Youjin</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaewon</first><last>Lee</last><affiliation>Department of Transdisciplinary Studies, Seoul National University</affiliation></author>
      <author><first>Su-Min</first><last>Lee</last><affiliation>The Catholic University of Korea</affiliation></author>
      <author><first>SangKeun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>1142-1153</pages>
      <abstract>Warning: This paper contains some offensive expressions.Offensive content is an unavoidable issue on social media. Most existing offensive language identification methods rely on the compilation of labeled datasets. However, existing methods rarely consider low-resource languages that have relatively less data available for training (e.g., Korean). To address these issues, we construct a novel KOrean Dataset for Offensive Language Identification (KODOLI). KODOLI comprises more fine-grained offensiveness categories (i.e., not offensive, likely offensive, and offensive) than existing ones. A likely offensive language refers to texts with implicit offensiveness or abusive language without offensive intentions. In addition, we propose two auxiliary tasks to help identify offensive languages: abusive language detection and sentiment analysis. We provide experimental results for baselines on KODOLI and observe that language models suffer from identifying “LIKELY” offensive statements. Quantitative results and qualitative analysis demonstrate that jointly learning offensive language, abusive language and sentiment information improves the performance of offensive language identification.</abstract>
      <url hash="19d300ea">2023.findings-eacl.85</url>
      <attachment type="dataset" hash="cc484a68">2023.findings-eacl.85.dataset.zip</attachment>
      <bibkey>park-etal-2023-feel</bibkey>
    </paper>
    <paper id="86">
      <title>Empirical Investigation of Neural Symbolic Reasoning Strategies</title>
      <author><first>Yoichi</first><last>Aoki</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Keito</first><last>Kudo</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Tatsuki</first><last>Kuribayashi</last><affiliation>Tohoku University / Langsmith Inc.</affiliation></author>
      <author><first>Ana</first><last>Brassard</last><affiliation>RIKEN AIP / Tohoku University</affiliation></author>
      <author><first>Masashi</first><last>Yoshikawa</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Keisuke</first><last>Sakaguchi</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Tohoku University / Riken</affiliation></author>
      <pages>1154-1162</pages>
      <abstract>Neural reasoning accuracy improves when generating intermediate reasoning steps. However, the source of this improvement is yet unclear.Here, we investigate and factorize the benefit of generating intermediate steps for symbolic reasoning.Specifically, we decompose the reasoning strategy w.r.t. step granularity and chaining strategy. With a purely symbolic numerical reasoning dataset (e.g., A=1, B=3, C=A+3, C?), we found that the choice of reasoning strategies significantly affects the performance, with the gap becoming even larger as the extrapolation length becomes longer.Surprisingly, we also found that certain configurations lead to nearly perfect performance, even in the case of length extrapolation.Our results indicate the importance of further exploring effective strategies for neural reasoning models.</abstract>
      <url hash="89f5ff67">2023.findings-eacl.86</url>
      <attachment type="software" hash="f143974b">2023.findings-eacl.86.software.zip</attachment>
      <bibkey>aoki-etal-2023-empirical</bibkey>
    </paper>
    <paper id="87">
      <title>Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering</title>
      <author><first>Xanh</first><last>Ho</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Anh-Khoa</first><last>Duong Nguyen</last><affiliation>N/a</affiliation></author>
      <author><first>Saku</first><last>Sugawara</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Akiko</first><last>Aizawa</last><affiliation>National Institute of Informatics</affiliation></author>
      <pages>1163-1180</pages>
      <abstract>To explain the predicted answers and evaluate the reasoning abilities of models, several studies have utilized underlying reasoning (UR) tasks in multi-hop question answering (QA) datasets. However, it remains an open question as to how effective UR tasks are for the QA task when training models on both tasks in an end-to-end manner. In this study, we address this question by analyzing the effectiveness of UR tasks (including both sentence-level and entity-level tasks) in three aspects: (1) QA performance, (2) reasoning shortcuts, and (3) robustness. While the previous models have not been explicitly trained on an entity-level reasoning prediction task, we build a multi-task model that performs three tasks together: sentence-level supporting facts prediction, entity-level reasoning prediction, and answer prediction. Experimental results on 2WikiMultiHopQA and HotpotQA-small datasets reveal that (1) UR tasks can improve QA performance. Using four debiased datasets that are newly created, we demonstrate that (2) UR tasks are helpful in preventing reasoning shortcuts in the multi-hop QA task. However, we find that (3) UR tasks do not contribute to improving the robustness of the model on adversarial questions, such as sub-questions and inverted questions. We encourage future studies to investigate the effectiveness of entity-level reasoning in the form of natural language questions (e.g., sub-question forms).</abstract>
      <url hash="df42e10e">2023.findings-eacl.87</url>
      <bibkey>ho-etal-2023-analyzing</bibkey>
    </paper>
    <paper id="88">
      <title><fixed-case>P</fixed-case>ub<fixed-case>M</fixed-case>ed<fixed-case>CLIP</fixed-case>: How Much Does <fixed-case>CLIP</fixed-case> Benefit Visual Question Answering in the Medical Domain?</title>
      <author><first>Sedigheh</first><last>Eslami</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Christoph</first><last>Meinel</last><affiliation>Hasso Plattner Institute</affiliation></author>
      <author><first>Gerard</first><last>de Melo</last><affiliation>Hasso Plattner Institute, University of Potsdam</affiliation></author>
      <pages>1181-1193</pages>
      <abstract>Contrastive Language–Image Pre-training (CLIP) has shown remarkable success in learning with cross-modal supervision from extensive amounts of image–text pairs collected online. Thus far, the effectiveness of CLIP has been investigated primarily in general-domain multimodal problems. In this work, we evaluate the effectiveness of CLIP for the task of Medical Visual Question Answering (MedVQA). We present PubMedCLIP, a fine-tuned version of CLIP for the medical domain based on PubMed articles. Our experiments conducted on two MedVQA benchmark datasets illustrate that PubMedCLIP achieves superior results improving the overall accuracy up to 3% in comparison to the state-of-the-art Model-Agnostic Meta-Learning (MAML) networks pre-trained only on visual data. The PubMedCLIP model with different back-ends, the source code for pre-training them and reproducing our MedVQA pipeline is publicly available at https://github.com/sarahESL/PubMedCLIP.</abstract>
      <url hash="f7e9525d">2023.findings-eacl.88</url>
      <bibkey>eslami-etal-2023-pubmedclip</bibkey>
    </paper>
    <paper id="89">
      <title>Multilingual <fixed-case>BERT</fixed-case> has an accent: Evaluating <fixed-case>E</fixed-case>nglish influences on fluency in multilingual models</title>
      <author><first>Isabel</first><last>Papadimitriou</last><affiliation>Stanford University</affiliation></author>
      <author><first>Kezia</first><last>Lopez</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>1194-1200</pages>
      <abstract>While multilingual language models can improve NLP performance on low-resource languages by leveraging higher-resource languages, they also reduce average performance on all languages (the ‘curse of multilinguality’). Here we show another problem with multilingual models: grammatical structures in higher-resource languages bleed into lower-resource languages, a phenomenon we call grammatical structure bias. We show this bias via a novel method for comparing the fluency of multilingual models to the fluency of monolingual Spanish and Greek models: testing their preference for two carefully-chosen variable grammatical structures (optional pronoun-drop in Spanish and optional Subject-Verb ordering in Greek). We find that multilingual BERT is biased toward the English-like setting (explicit pronouns and Subject-Verb-Object ordering) as compared to our monolingual control language model. With our case studies, we hope to bring to light the fine-grained ways in which multilingual models can be biased, and encourage more linguistically-aware fluency evaluation.</abstract>
      <url hash="4ce513fd">2023.findings-eacl.89</url>
      <bibkey>papadimitriou-etal-2023-multilingual</bibkey>
    </paper>
    <paper id="90">
      <title>Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization</title>
      <author><first>Aishwarya</first><last>Agrawal</last><affiliation>University of Montreal, Mila, DeepMind</affiliation></author>
      <author><first>Ivana</first><last>Kajic</last><affiliation>DeepMind</affiliation></author>
      <author><first>Emanuele</first><last>Bugliarello</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Elnaz</first><last>Davoodi</last><affiliation>DeepMind</affiliation></author>
      <author><first>Anita</first><last>Gergely</last><affiliation>DeepMind</affiliation></author>
      <author><first>Phil</first><last>Blunsom</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Aida</first><last>Nematzadeh</last><affiliation>DeepMind</affiliation></author>
      <pages>1201-1226</pages>
      <abstract>Vision-and-language (V&amp;L) models pretrained on large-scale multimodal data have demonstrated strong performance on various tasks such as image captioning and visual question answering (VQA). The quality of such models is commonly assessed by measuring their performance on unseen data that typically comes from the same distribution as the training data. However, when evaluated under out-of-distribution (out-of-dataset) settings for VQA, we observe that these models exhibit poor generalization. We comprehensively evaluate two pretrained V&amp;L models under different settings (i.e. classification and open-ended text generation) by conducting cross-dataset evaluations. We find that these models tend to learn to solve the benchmark, rather than learning the high-level skills required by the VQA task. We also find that in most cases generative models are less susceptible to shifts in data distribution compared to discriminative ones, and that multimodal pretraining is generally helpful for OOD generalization. Finally, we revisit assumptions underlying the use of automatic VQA evaluation metrics, and empirically show that their stringent nature repeatedly penalizes models for correct responses.</abstract>
      <url hash="77d51b13">2023.findings-eacl.90</url>
      <bibkey>agrawal-etal-2023-reassessing</bibkey>
    </paper>
    <paper id="91">
      <title>Our kind of people? Detecting populist references in political debates</title>
      <author><first>Christopher</first><last>Klamm</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Ines</first><last>Rehbein</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last><affiliation>University of Mannheim</affiliation></author>
      <pages>1227-1243</pages>
      <abstract>This paper investigates the identification of populist rhetoric in text and presents a novel cross-lingual dataset for this task. Our work is based on the definition of populism as a “communication style of political actors that refers to the people” but also includes anti-elitism as another core feature of populism. Accordingly, we annotate references to The People and The Elite in German and English parliamentary debates with a hierarchical scheme. The paper describes our dataset and annotation procedure and reports inter-annotator agreement for this task. Next, we compare and evaluate different transformer-based model architectures on a German dataset and report results for zero-shot learning on a smaller English dataset. We then show that semi-supervised tri-training can improve results in the cross-lingual setting. Our dataset can be used to investigate how political actors talk about The Elite and The People and to study how populist rhetoric is used as a strategic device.</abstract>
      <url hash="6abc8df2">2023.findings-eacl.91</url>
      <attachment type="dataset" hash="938ba72f">2023.findings-eacl.91.dataset.zip</attachment>
      <bibkey>klamm-etal-2023-kind</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>S</fixed-case>har<fixed-case>PT</fixed-case>: Shared Latent Space Prompt Tuning</title>
      <author><first>Bo</first><last>Pang</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Semih</first><last>Yavuz</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce</affiliation></author>
      <author><first>Yingbo</first><last>Zhou</last><affiliation>Salesforce Research</affiliation></author>
      <pages>1244-1250</pages>
      <abstract>Prompt tuning is an efficient method for adapting large language models, and Soft Prompt Transfer (SPoT) further narrows the gap between prompt tuning and full model tuning by transferring prompts learned from source tasks to target tasks. It is nevertheless difficult and expensive to identify the source task that provides optimal prompts. In this work, we propose to learn a shared latent space which captures a set of basis skills from a mixture of source tasks. Given an instance, its embedding queries the latent space, yielding a basis skill vector. This vector generates soft prompts, via a lightweight prompt generator, which modulates a frozen model. The latent space and prompt transformation are learned end-to-end by training on source tasks. Transfer learning from source tasks to a target task simply amounts to finetuning the prompt generator, accounting for roughly 0.3% parameters of the frozen backbone model, while the shared latent space is also frozen in finetuning. Our approach outperforms prior soft prompt methods by a significant margin on a variety of tasks such as NLI, sentence completion, QA, conference resolution, word sense disambiguation. We also find, on various model scales, our method achieves competitive performance compared to finetuning the full model.</abstract>
      <url hash="e601eb41">2023.findings-eacl.92</url>
      <bibkey>pang-etal-2023-sharpt</bibkey>
    </paper>
    <paper id="93">
      <title>Mini But Mighty: Efficient Multilingual Pretraining with Linguistically-Informed Data Selection</title>
      <author><first>Tolulope</first><last>Ogunremi</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher</first><last>Manning</last><affiliation>Stanford University</affiliation></author>
      <pages>1251-1266</pages>
      <abstract>With the prominence of large pretrained language models, low-resource languages are rarely modelled monolingually and become victims of the “curse of multilinguality” in massively multilingual models. Recently, AfriBERTa showed that training transformer models from scratch on 1GB of data from many unrelated African languages outperforms massively multilingual models on downstream NLP tasks. Here we extend this direction, focusing on the use of related languages. We propose that training on smaller amounts of data but from related languages could match the performance of models trained on large, unrelated data. We test our hypothesis on the Niger-Congo family and its Bantu and Volta-Niger sub-families, pretraining models with data solely from Niger-Congo languages and finetuning on 4 downstream tasks: NER, part-of-speech tagging, sentiment analysis and text classification. We find that models trained on genetically related languages achieve equal performance on downstream tasks in low-resource languages despite using less training data. We recommend selecting training data based on language-relatedness when pretraining language models for low-resource languages.</abstract>
      <url hash="1229afc0">2023.findings-eacl.93</url>
      <bibkey>ogunremi-etal-2023-mini</bibkey>
    </paper>
    <paper id="94">
      <title>Long Document Summarization with Top-down and Bottom-up Inference</title>
      <author><first>Bo</first><last>Pang</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Erik</first><last>Nijkamp</last><affiliation>Ucla</affiliation></author>
      <author><first>Wojciech</first><last>Kryscinski</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Yingbo</first><last>Zhou</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce</affiliation></author>
      <pages>1267-1284</pages>
      <abstract>Text summarization aims to condense long documents and retain key information. Critical to the success of a summarization model is the faithful inference of latent representations of words or tokens in the source documents. Most recent models infer the latent representations with a transformer encoder, which is purely bottom-up and thus does not capture long-distance context well. Also, self-attention-based models face the challenge of quadratic complexity with respect to sequence length. We propose a method to improve summarization models on these two aspects. Our method assumes a hierarchical latent structure of a document where the top-level captures the long range dependency at a coarser time scale and the bottom token level preserves the details. Critically, our method enables token representations to be updated in both a bottom-up and top-down manner. In the bottom-up pass, token representations are inferred with local self-attention to leverage its efficiency. Top-down correction is then applied to allow tokens to capture global context. We demonstrate the effectiveness on a diverse set of summarization datasets, including narrative, conversational, scientific documents and news. Our model achieves state-of-the-art performance on a wide range of long document summarization benchmarks, compared to recent efficient transformers. We show that our model can summarize an entire book and achieve competitive performance using 0.27% parameters and much less training data, compared to a recent GPT-3-based model. These results indicate the general applicability and benefits of the framework.</abstract>
      <url hash="a103e5fa">2023.findings-eacl.94</url>
      <bibkey>pang-etal-2023-long</bibkey>
    </paper>
    <paper id="95">
      <title>Open Information Extraction with Entity Focused Constraints</title>
      <author><first>Prajna</first><last>Upadhyay</last><affiliation>Inria Saclay</affiliation></author>
      <author><first>Oana</first><last>Balalau</last><affiliation>Inria and Ecole Polytechnique</affiliation></author>
      <author><first>Ioana</first><last>Manolescu</last><affiliation>Inria and Ecole Polytechnique</affiliation></author>
      <pages>1285-1296</pages>
      <abstract>Open Information Extraction (OIE) is the task of extracting tuples of the form (subject, predicate, object), without any knowledge of the type and lexical form of the predicate, the subject, or the object. In this work, we focus on improving OIE quality by exploiting domain knowledge about the subject and object. More precisely, knowing that the subjects and objects in sentences are oftentimes named entities, we explore how to inject constraints in the extraction through constrained inference and constraint-aware training. Our work leverages the state-of-the-art OpenIE6 platform, which we adapt to our setting. Through a carefully constructed training dataset and constrained training, we obtain a 29.17% F1-score improvement in the CaRB metric and a 24.37% F1-score improvement in the WIRe57 metric. Our technique has important applications – one of them is investigative journalism, where automatically extracting conflict-of-interest between scientists and funding organizations helps understand the type of relations companies engage with the scientists.</abstract>
      <url hash="785c5846">2023.findings-eacl.95</url>
      <bibkey>upadhyay-etal-2023-open</bibkey>
    </paper>
    <paper id="96">
      <title><fixed-case>H</fixed-case>ierarchical3<fixed-case>D</fixed-case> Adapters for Long Video-to-text Summarization</title>
      <author><first>Pinelopi</first><last>Papalampidi</last><affiliation>DeepMind</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>School of Informatics, University of Edinburgh</affiliation></author>
      <pages>1297-1320</pages>
      <abstract>In this paper, we focus on video-to-text summarization and investigate how to best utilize multimodal information for summarizing long inputs (e.g., an hour-long TV show) into long outputs (e.g., a multi-sentence summary). We extend SummScreen (Chen et al., 2022), a dialogue summarization dataset consisting of transcripts of TV episodes with reference summaries, and create a multimodal variant by collecting corresponding full-length videos. We incorporate multimodal information into a pre-trained textual summarizer efficiently using adapter modules augmented with a hierarchical structure while tuning only 3.8% of model parameters. Our experiments demonstrate that multimodal information offers superior performance over more memory-heavy and fully fine-tuned textual summarization methods.</abstract>
      <url hash="9c7bdf10">2023.findings-eacl.96</url>
      <bibkey>papalampidi-lapata-2023-hierarchical3d</bibkey>
    </paper>
    <paper id="97">
      <title>An Intra-Class Relation Guided Approach for Code Comment Generation</title>
      <author><first>Zhenni</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiaohan</first><last>Yu</last><affiliation>Peking University</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Pku.edu.cn</affiliation></author>
      <pages>1321-1333</pages>
      <abstract>Code comments are critical for maintaining and comprehending software programs, but they are often missing, mismatched, or outdated in practice. Code comment generation task aims to automatically produce descriptive comments for code snippets. Recently, methods based on the neural encoder-decoder architecture have achieved impressive performance. These methods assume that all the information required to generate comments is encoded in the target function itself, yet in most realistic situations, it is hard to understand a function in isolation from the surrounding context. Furthermore, the global context may contain redundant information that should not be introduced. To address the above issues, we present a novel graph-based learning framework to capture various relations among functions in a class file. Our approach is based on a common real-world scenario in which only a few functions in the source file have human-written comments. Guided by intra-class function relations, our model incorporates contextual information extracted from both the source code and available comments to generate missing comments. We conduct experiments on a Java dataset collected from real-world projects. Experimental results show that the proposed method outperforms competitive baseline models on all automatic and human evaluation metrics.</abstract>
      <url hash="06c6c90d">2023.findings-eacl.97</url>
      <bibkey>wang-etal-2023-intra</bibkey>
    </paper>
    <paper id="98">
      <title>Spelling convention sensitivity in neural language models</title>
      <author><first>Elizabeth</first><last>Nielsen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Christo</first><last>Kirov</last><affiliation>Google</affiliation></author>
      <author><first>Brian</first><last>Roark</last><affiliation>Google Inc.</affiliation></author>
      <pages>1334-1346</pages>
      <abstract>We examine whether large neural language models, trained on very large collections of varied English text, learn the potentially long-distance dependency of British versus American spelling conventions, i.e., whether spelling is consistently one or the other within model-generated strings. In contrast to long-distance dependencies in non-surface underlying structure (e.g., syntax), spelling consistency is easier to measure both in LMs and the text corpora used to train them, which can provide additional insight into certain observed model behaviors. Using a set of probe words unique to either British or American English, we first establish that training corpora exhibit substantial (though not total) consistency. A large T5 language model does appear to internalize this consistency, though only with respect to observed lexical items (not nonce words with British/American spelling patterns). We further experiment with correcting for biases in the training data by fine-tuning T5 on synthetic data that has been debiased, and find that finetuned T5 remains only somewhat sensitive to spelling consistency. Further experiments show GPT2 to be similarly limited.</abstract>
      <url hash="3310ebab">2023.findings-eacl.98</url>
      <bibkey>nielsen-etal-2023-spelling</bibkey>
    </paper>
    <paper id="99">
      <title>Modelling Language Acquisition through Syntactico-Semantic Pattern Finding</title>
      <author><first>Jonas</first><last>Doumen</last><affiliation>KU Leuven, imec research group itec</affiliation></author>
      <author><first>Katrien</first><last>Beuls</last><affiliation>Faculté d’informatique, University of Namur</affiliation></author>
      <author><first>Paul</first><last>Van Eecke</last><affiliation>Artificial Intelligence Lab - Vrije Universiteit Brussel</affiliation></author>
      <pages>1347-1357</pages>
      <abstract>Usage-based theories of language acquisition have extensively documented the processes by which children acquire language through communicative interaction. Notably, Tomasello (2003) distinguishes two main cognitive capacities that underlie human language acquisition: intention reading and pattern finding. Intention reading is the process by which children try to continuously reconstruct the intended meaning of their interlocutors. Pattern finding refers to the process that allows them to distil linguistic schemata from multiple communicative interactions. Even though the fields of cognitive science and psycholinguistics have studied these processes in depth, no faithful computational operationalisations of these mechanisms through which children learn language exist to date. The research on which we report in this paper aims to fill part of this void by introducing a computational operationalisation of syntactico-semantic pattern finding. Concretely, we present a methodology for learning grammars based on similarities and differences in the form and meaning of linguistic observations alone. Our methodology is able to learn compositional lexical and item-based constructions of variable extent and degree of abstraction, along with a network of emergent syntactic categories. We evaluate our methodology on the CLEVR benchmark dataset and show that the methodology allows for fast, incremental and effective learning. The constructions and categorial network that result from the learning process are fully transparent and bidirectional, facilitating both language comprehension and production. Theoretically, our model provides computational evidence for the learnability of usage-based constructionist theories of language acquisition. Practically, the techniques that we present facilitate the learning of computationally tractable, usage-based construction grammars, which are applicable for natural language understanding and production tasks.</abstract>
      <url hash="21563627">2023.findings-eacl.99</url>
      <attachment type="dataset" hash="ad38bf7a">2023.findings-eacl.99.dataset.zip</attachment>
      <bibkey>doumen-etal-2023-modelling</bibkey>
    </paper>
    <paper id="100">
      <title>Benchmark Data and Evaluation Framework for Intent Discovery Around <fixed-case>COVID</fixed-case>-19 Vaccine Hesitancy</title>
      <author><first>Shai</first><last>Gretz</last><affiliation>IBM Research</affiliation></author>
      <author><first>Assaf</first><last>Toledo</last><affiliation>IBM Research</affiliation></author>
      <author><first>Roni</first><last>Friedman</last><affiliation>IBM Research</affiliation></author>
      <author><first>Dan</first><last>Lahav</last><affiliation>IBM Research</affiliation></author>
      <author><first>Rose</first><last>Weeks</last><affiliation>Johns Hopkins Bloomberg School of Public Health</affiliation></author>
      <author><first>Naor</first><last>Bar-Zeev</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>João</first><last>Sedoc</last><affiliation>New York University</affiliation></author>
      <author><first>Pooja</first><last>Sangha</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Yoav</first><last>Katz</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Noam</first><last>Slonim</last><affiliation>IBM Research</affiliation></author>
      <pages>1358-1370</pages>
      <abstract>The COVID-19 pandemic has made a huge global impact and cost millions of lives. As COVID-19 vaccines were rolled out, they were quickly met with widespread hesitancy. To address the concerns of hesitant people, we launched VIRA, a public dialogue system aimed at addressing questions and concerns surrounding the COVID-19 vaccines. Here, we release VIRADialogs, a dataset of over 8k dialogues conducted by actual users with VIRA, providing a unique real-world conversational dataset. In light of rapid changes in users’ intents, due to updates in guidelines or in response to new information, we highlight the important task of intent discovery in this use-case. We introduce a novel automatic evaluation framework for intent discovery, leveraging the existing intent classifier of VIRA. We use this framework to report baseline intent discovery results over VIRADialogs, that highlight the difficulty of this task.</abstract>
      <url hash="c7ed9889">2023.findings-eacl.100</url>
      <bibkey>gretz-etal-2023-benchmark</bibkey>
    </paper>
    <paper id="101">
      <title>Learning Disentangled Representations for Natural Language Definitions</title>
      <author><first>Danilo</first><last>Silva De Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Giangiacomo</first><last>Mercatali</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Yingji</first><last>Zhang</last><affiliation>The University of Manchester</affiliation></author>
      <author><first>André</first><last>Freitas</last><affiliation>University of Manchester</affiliation></author>
      <pages>1371-1384</pages>
      <abstract>Disentangling the encodings of neural models is a fundamental aspect for improving interpretability, semantic control and downstream task performance in Natural Language Processing. Currently, most disentanglement methods are unsupervised or rely on synthetic datasets with known generative factors. We argue that recurrent syntactic and semantic regularities in textual data can be used to provide the models with both structural biases and generative factors. We leverage the semantic structures present in a representative and semantically dense category of sentence types, definitional sentences, for training a Variational Autoencoder to learn disentangled representations. Our experimental results show that the proposed model outperforms unsupervised baselines on several qualitative and quantitative benchmarks for disentanglement, and it also improves the results in the downstream task of definition modeling.</abstract>
      <url hash="04379dee">2023.findings-eacl.101</url>
      <bibkey>silva-de-carvalho-etal-2023-learning</bibkey>
    </paper>
    <paper id="102">
      <title>Distinguishability Calibration to In-Context Learning</title>
      <author><first>Hongjing</first><last>Li</last><affiliation>Computer Science Department, the University of Warwick</affiliation></author>
      <author><first>Hanqi</first><last>Yan</last><affiliation>Computer Science Department, University of Warwick</affiliation></author>
      <author><first>Yanran</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Li</first><last>Qian</last><affiliation>XiaomiAI</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London</affiliation></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London</affiliation></author>
      <pages>1385-1397</pages>
      <abstract>Recent years have witnessed increasing interests in prompt-based learning in which models can be trained on only a few annotated instances, making them suitable in low-resource settings. It is even challenging in fine-grained classification as the pre-trained language models tend to generate similar output embedding which makes it difficult to discriminate for the prompt-based classifier. In this work, we alleviate this information diffusion issue by proposing a calibration method based on a transformation which rotates the embedding feature into a new metric space where we adapt the ratio of each dimension to a uniform distribution to guarantee the distinguishability of learned embeddings. Furthermore, we take the advantage of hyperbolic embedding to capture the relation between dimensions by a coarse-fine metric learning strategy to enhance interpretability. Extensive experiments on the three datasets under various settings demonstrate the effectiveness of our approach.</abstract>
      <url hash="f217d318">2023.findings-eacl.102</url>
      <bibkey>li-etal-2023-distinguishability</bibkey>
    </paper>
    <paper id="103">
      <title>Investigating anatomical bias in clinical machine learning algorithms</title>
      <author><first>Jannik</first><last>Pedersen</last><affiliation>University of Southern Denmark</affiliation></author>
      <author><first>Martin</first><last>Laursen</last><affiliation>University of Southern Denmark</affiliation></author>
      <author><first>Pernille</first><last>Vinholt</last><affiliation>Odense University Hospital</affiliation></author>
      <author><first>Anne</first><last>Alnor</last><affiliation>Odense University Hospital</affiliation></author>
      <author><first>Thiusius</first><last>Savarimuthu</last><affiliation>University of Southern Denmark</affiliation></author>
      <pages>1398-1410</pages>
      <abstract>Clinical machine learning algorithms have shown promising results and could potentially be implemented in clinical practice to provide diagnosis support and improve patient treatment. Barriers for realisation of the algorithms’ full potential include bias which is systematic and unfair discrimination against certain individuals in favor of others.The objective of this work is to measure anatomical bias in clinical text algorithms. We define anatomical bias as unfair algorithmic outcomes against patients with medical conditions in specific anatomical locations. We measure the degree of anatomical bias across two machine learning models and two Danish clinical text classification tasks, and find that clinical text algorithms are highly prone to anatomical bias. We argue that datasets for creating clinical text algorithms should be curated carefully to isolate the effect of anatomical location in order to avoid bias against patient subgroups.</abstract>
      <url hash="ae014a97">2023.findings-eacl.103</url>
      <attachment type="dataset" hash="f4aaa562">2023.findings-eacl.103.dataset.zip</attachment>
      <bibkey>pedersen-etal-2023-investigating</bibkey>
    </paper>
    <paper id="104">
      <title>Topic Ontologies for Arguments</title>
      <author><first>Yamen</first><last>Ajjour</last><affiliation>Leipzig University, Bauhaus-Universität Weimar</affiliation></author>
      <author><first>Johannes</first><last>Kiesel</last><affiliation>Bauhaus-Universität Weimar</affiliation></author>
      <author><first>Benno</first><last>Stein</last><affiliation>Bauhaus-Universität Weimar</affiliation></author>
      <author><first>Martin</first><last>Potthast</last><affiliation>Leipzig University</affiliation></author>
      <pages>1411-1427</pages>
      <abstract>Many computational argumentation tasks, such as stance classification, are topic-dependent: The effectiveness of approaches to these tasks depends largely on whether they are trained with arguments on the same topics as those on which they are tested. The key question is: What are these training topics? To answer this question, we take the first step of mapping the argumentation landscape with The Argument Ontology (TAO). TAO draws on three authoritative sources for argument topics: the World Economic Forum, Wikipedia’s list of controversial topics, and Debatepedia. By comparing the topics in our ontology with those in 59 argument corpora, we perform the first comprehensive assessment of their topic coverage. While TAO already covers most of the corpus topics, the corpus topics barely cover all the topics in TAO. This points to a new goal for corpus construction to achieve a broad topic coverage and thus better generalizability of computational argumentation approaches.</abstract>
      <url hash="f71acfa9">2023.findings-eacl.104</url>
      <bibkey>ajjour-etal-2023-topic</bibkey>
    </paper>
    <paper id="105">
      <title>Longtonotes: <fixed-case>O</fixed-case>nto<fixed-case>N</fixed-case>otes with Longer Coreference Chains</title>
      <author><first>Kumar</first><last>Shridhar</last><affiliation>ETH Zürich</affiliation></author>
      <author><first>Nicholas</first><last>Monath</last><affiliation>Google</affiliation></author>
      <author><first>Raghuveer</first><last>Thirukovalluru</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Alessandro</first><last>Stolfo</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Manzil</first><last>Zaheer</last><affiliation>Google Inc</affiliation></author>
      <author><first>Andrew</first><last>McCallum</last><affiliation>UMass Amherst</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>ETH Zurich</affiliation></author>
      <pages>1428-1442</pages>
      <abstract>Ontonotes has served as the most important benchmark for coreference resolution. However, for ease of annotation, several long documents in Ontonotes were split into smaller parts.In this work, we build a corpus of coreference-annotated documents of significantly longer length than what is currently available.We do so by providing an accurate, manually-curated, merging of annotations from documents that were split into multiple parts in the original Ontonotes annotation process.The resulting corpus, which we call LongtoNotes contains documents in multiple genres of the English language with varying lengths, the longest of which are up to 8x the length of documents in Ontonotes, and 2x those in Litbank.We evaluate state-of-the-art neural coreference systems on this new corpus, analyze the relationships between model architectures/hyperparameters and document length on performance and efficiency of the models, and demonstrate areas of improvement in long-document coreference modelling revealed by our new corpus.</abstract>
      <url hash="5a212498">2023.findings-eacl.105</url>
      <bibkey>shridhar-etal-2023-longtonotes</bibkey>
    </paper>
    <paper id="106">
      <title>More Robust Schema-Guided Dialogue State Tracking via Tree-Based Paraphrase Ranking</title>
      <author><first>Alexandru</first><last>Coca</last><affiliation>University Cambridge</affiliation></author>
      <author><first>Bo-Hsiang</first><last>Tseng</last><affiliation>Apple</affiliation></author>
      <author><first>Weizhe</first><last>Lin</last><affiliation>University of Cambridge</affiliation></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last><affiliation>University of Cambridge</affiliation></author>
      <pages>1443-1454</pages>
      <abstract>The schema-guided paradigm overcomes scalability issues inherent in building task-oriented dialogue (TOD) agents with static ontologies. Rather than operating on dialogue context alone, agents have access to hierarchical schemas containing task-relevant natural language descriptions. Fine-tuned language models excel at schema-guided dialogue state tracking (DST) but are sensitive to the writing style of the schemas. We explore methods for improving the robustness of DST models. We propose a framework for generating synthetic schemas which uses tree-based ranking to jointly optimise lexical diversity and semantic faithfulness. The robust generalisation of strong baselines is improved when augmenting their training data with prompts generated by our framework, as demonstrated by marked improvements in average Joint Goal Accuracy (JGA) and schema sensitivity (SS) on the SGD-X benchmark.</abstract>
      <url hash="0103498c">2023.findings-eacl.106</url>
      <bibkey>coca-etal-2023-robust</bibkey>
    </paper>
    <paper id="107">
      <title>Language Model Decoding as Likelihood–Utility Alignment</title>
      <author><first>Martin</first><last>Josifoski</last><affiliation>Ecole Polytechnique Federale de Lausane</affiliation></author>
      <author><first>Maxime</first><last>Peyrard</last><affiliation>Epfl</affiliation></author>
      <author><first>Frano</first><last>Rajič</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Jiheng</first><last>Wei</last><affiliation>PSL University</affiliation></author>
      <author><first>Debjit</first><last>Paul</last><affiliation>Epfl</affiliation></author>
      <author><first>Valentin</first><last>Hartmann</last><affiliation>Epfl</affiliation></author>
      <author><first>Barun</first><last>Patra</last><affiliation>Microsoft</affiliation></author>
      <author><first>Vishrav</first><last>Chaudhary</last><affiliation>Microsoft</affiliation></author>
      <author><first>Emre</first><last>Kiciman</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Boi</first><last>Faltings</last><affiliation>Epfl</affiliation></author>
      <pages>1455-1470</pages>
      <abstract>A critical component of a successful language generation pipeline is the decoding algorithm. However, the general principles that should guide the choice of a decoding algorithm remain unclear. Previous works only compare decoding algorithms in narrow scenarios, and their findings do not generalize across tasks. We argue that the misalignment between the model’s likelihood and the task-specific notion of utility is the key factor in understanding the effectiveness of decoding algorithms. To structure the discussion, we introduce a taxonomy of misalignment mitigation strategies (MMSs), providing a unifying view of decoding as a tool for alignment. The MMS taxonomy groups decoding algorithms based on their implicit assumptions about likelihood–utility misalignment, yielding general statements about their applicability across tasks. Specifically, by analyzing the correlation between the likelihood and the utility of predictions across a diverse set of tasks, we provide empirical evidence supporting the proposed taxonomy and a set of principles to structure reasoning when choosing a decoding algorithm. Crucially, our analysis is the first to relate likelihood-based decoding algorithms with algorithms that rely on external information, such as value-guided methods and prompting, and covers the most diverse set of tasks to date. Code, data, and models are available at https://github.com/epfl-dlab/understanding-decoding.</abstract>
      <url hash="95e9a5ad">2023.findings-eacl.107</url>
      <bibkey>josifoski-etal-2023-language</bibkey>
    </paper>
    <paper id="108">
      <title>Lightweight Spatial Modeling for Combinatorial Information Extraction From Documents</title>
      <author><first>Yanfei</first><last>Dong</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Lambert</first><last>Deng</last><affiliation>PayPal</affiliation></author>
      <author><first>Jiazheng</first><last>Zhang</last><affiliation>PayPal</affiliation></author>
      <author><first>Xiaodong</first><last>Yu</last><affiliation>PayPal</affiliation></author>
      <author><first>Ting</first><last>Lin</last><affiliation>Nanyang Technology University</affiliation></author>
      <author><first>Francesco</first><last>Gelli</last><affiliation>PayPal</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Wee Sun</first><last>Lee</last><affiliation>National University of Singapore</affiliation></author>
      <pages>1471-1484</pages>
      <abstract>Documents that consist of diverse templates and exhibit complex spatial structures pose a challenge for document entity classification. We propose KNN-Former, which incorporates a new kind of spatial bias in attention calculation based on the K-nearest-neighbor (KNN) graph of document entities. We limit entities’ attention only to their local radius defined by the KNN graph. We also use combinatorial matching to address the one-to-one mapping property that exists in many documents, where one field has only one corresponding entity. Moreover, our method is highly parameter-efficient compared to existing approaches in terms of the number of trainable parameters. Despite this, experiments across various datasets show our method outperforms baselines in most entity types. Many real-world documents exhibit combinatorial properties which can be leveraged as inductive biases to improve extraction accuracy, but existing datasets do not cover these documents.To facilitate future research into these types of documents, we release a new ID document dataset that covers diverse templates and languages. We also release enhanced annotations for an existing dataset.</abstract>
      <url hash="82906c0d">2023.findings-eacl.108</url>
      <bibkey>dong-etal-2023-lightweight</bibkey>
    </paper>
    <paper id="109">
      <title>On the Generalization Ability of Retrieval-Enhanced Transformers</title>
      <author><first>Tobias</first><last>Norlund</last><affiliation>Chalmers University of Technology / Recorded Future</affiliation></author>
      <author><first>Ehsan</first><last>Doostmohammadi</last><affiliation>Linköping University</affiliation></author>
      <author><first>Richard</first><last>Johansson</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Marco</first><last>Kuhlmann</last><affiliation>Linköping University</affiliation></author>
      <pages>1485-1493</pages>
      <abstract>Recent work on the Retrieval-Enhanced Transformer (RETRO) model has shown impressive results: off-loading memory from trainable weights to a retrieval database can significantly improve language modeling and match the performance of non-retrieval models that are an order of magnitude larger in size. It has been suggested that at least some of this performance gain is due to non-trivial generalization based on both model weights and retrieval. In this paper, we try to better understand the relative contributions of these two components. We find that the performance gains from retrieval to a very large extent originate from overlapping tokens between the database and the test data, suggesting less of non-trivial generalization than previously assumed. More generally, our results point to the challenges of evaluating the generalization of retrieval-augmented language models such as RETRO, as even limited token overlap may significantly decrease test-time loss. We release our code and model at https://github.com/TobiasNorlund/retro</abstract>
      <url hash="e06e0ed2">2023.findings-eacl.109</url>
      <bibkey>norlund-etal-2023-generalization</bibkey>
    </paper>
    <paper id="110">
      <title>Assessing Monotonicity Reasoning in <fixed-case>D</fixed-case>utch through Natural Language Inference</title>
      <author><first>Gijs</first><last>Wijnholds</last><affiliation>Utrecht University</affiliation></author>
      <pages>1494-1500</pages>
      <abstract>In this paper we investigate monotonicity reasoning in Dutch, through a novel Natural Language Inference dataset. Monotonicity reasoning shows to be highly challenging for Transformer-based language models in English and here, we corroborate those findings using a parallel Dutch dataset, obtained by translating the Monotonicity Entailment Dataset of Yanaka et al. (2019). After fine-tuning two Dutch language models BERTje and RobBERT on the Dutch NLI dataset SICK-NL, we find that performance severely drops on the monotonicity reasoning dataset, indicating poor generalization capacity of the models. We provide a detailed analysis of the test results by means of the linguistic annotations in the dataset. We find that models struggle with downward entailing contexts, and argue that this is due to a poor understanding of negation. Additionally, we find that the choice of monotonicity context affects model performance on conjunction and disjunction. We hope that this new resource paves the way for further research in generalization of neural reasoning models in Dutch, and contributes to the development of better language technology for Natural Language Inference, specifically for Dutch.</abstract>
      <url hash="2c136d22">2023.findings-eacl.110</url>
      <attachment type="dataset" hash="73d0c51b">2023.findings-eacl.110.dataset.zip</attachment>
      <bibkey>wijnholds-2023-assessing</bibkey>
    </paper>
    <paper id="111">
      <title>Noisy Parallel Data Alignment</title>
      <author><first>Ruoyu</first><last>Xie</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>George Mason University</affiliation></author>
      <pages>1501-1513</pages>
      <abstract>An ongoing challenge in current natural language processing is how its major advancements tend to disproportionately favor resource-rich languages, leaving a significant number of under-resourced languages behind. Due to the lack of resources required to train and evaluate models, most modern language technologies are either nonexistent or unreliable to process endangered, local, and non-standardized languages. Optical character recognition (OCR) is often used to convert endangered language documents into machine-readable data. However, such OCR output is typically noisy, and most word alignment models are not built to work under such noisy conditions. In this work, we study the existing word-level alignment models under noisy settings and aim to make them more robust to noisy data. Our noise simulation and structural biasing method, tested on multiple language pairs, manages to reduce the alignment error rate on a state-of-the-art neural-based alignment model up to 59.6%.</abstract>
      <url hash="a63ceef5">2023.findings-eacl.111</url>
      <bibkey>xie-anastasopoulos-2023-noisy</bibkey>
    </paper>
    <paper id="112">
      <title>Enhancing Dialogue Generation with Conversational Concept Flows</title>
      <author><first>Siheng</first><last>Li</last><affiliation>Tsinghua Shenzhen International Graduate School, Tsinghua University</affiliation></author>
      <author><first>Wangjie</first><last>Jiang</last><affiliation>Tsinghua Shenzhen International Graduate School</affiliation></author>
      <author><first>Pengda</first><last>Si</last><affiliation>Tsinghua Shenzhen International Graduate School</affiliation></author>
      <author><first>Cheng</first><last>Yang</last><affiliation>Tsinghua Shenzhen International Graduate School</affiliation></author>
      <author><first>Qiu</first><last>Yao</last><affiliation>Tencent Inc, Beijing, China</affiliation></author>
      <author><first>Jinchao</first><last>Zhang</last><affiliation>Tencent Inc, Beijing, China</affiliation></author>
      <author><first>Jie</first><last>Zhou</last><affiliation>Tencent Inc, Beijing, China</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Tsinghua.edu.cn</affiliation></author>
      <pages>1514-1525</pages>
      <abstract>Human conversations contain natural and reasonable topic shifts, reflected as the concept flows across utterances.Previous researches prove that explicitly modeling concept flows with a large commonsense knowledge graph effectively improves response quality.However, we argue that there exists a gap between the knowledge graph and the conversation.The knowledge graph has limited commonsense knowledge and ignores the characteristics of natural conversations.Thus, many concepts and relations in conversations are not included.To bridge this gap, we propose to enhance dialogue generation with conversational concept flows.Specifically, we extract abundant concepts and relations from natural conversations and build a new conversation-aware knowledge graph.In addition, we design a novel relation-aware graph encoder to capture the concept flows guided by the knowledge graph.Experimental results on the large-scale Reddit conversation dataset indicate that our method performs better than strong baselines, andfurther analysis verifies the effectiveness of each component.All our code and data will be publicly available after acceptance.</abstract>
      <url hash="ba1de98b">2023.findings-eacl.112</url>
      <attachment type="software" hash="78809694">2023.findings-eacl.112.software.zip</attachment>
      <bibkey>li-etal-2023-enhancing</bibkey>
    </paper>
    <paper id="113">
      <title><fixed-case>SMHD</fixed-case>-<fixed-case>GER</fixed-case>: A Large-Scale Benchmark Dataset for Automatic Mental Health Detection from Social Media in <fixed-case>G</fixed-case>erman</title>
      <author><first>Sourabh</first><last>Zanwar</last><affiliation>RWTH Aachen University</affiliation></author>
      <author><first>Daniel</first><last>Wiechmann</last><affiliation>Institute for Logic Language and Computation</affiliation></author>
      <author><first>Yu</first><last>Qiao</last><affiliation>RWTH-Achen</affiliation></author>
      <author><first>Elma</first><last>Kerz</last><affiliation>RWTH Aachen University</affiliation></author>
      <pages>1526-1541</pages>
      <abstract>Mental health problems are a challenge to our modern society, and their prevalence is predicted to increase worldwide. Recently, a surge of research has demonstrated the potential of automated detection of mental health conditions (MHC) through social media posts, with the ultimate goal of enabling early intervention and monitoring population-level health outcomes in real-time. Progress in this area of research is highly dependent on the availability of high-quality datasets and benchmark corpora. However, the publicly available datasets for understanding and modelling MHC are largely confined to the English language. In this paper, we introduce SMHD-GER (Self-Reported Mental Health Diagnoses for German), a large-scale, carefully constructed dataset for MHC detection built on high-precision patterns and the approach proposed for English. We provide benchmark models for this dataset to facilitate further research and conduct extensive experiments. These models leverage engineered (psycho-)linguistic features as well as BERT-German. We also examine nuanced patterns of linguistic markers characteristics of specific MHC.</abstract>
      <url hash="9f1a2a2c">2023.findings-eacl.113</url>
      <bibkey>zanwar-etal-2023-smhd</bibkey>
    </paper>
    <paper id="114">
      <title>Exploring Data Augmentation for Code Generation Tasks</title>
      <author><first>Pinzhen</first><last>Chen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Gerasimos</first><last>Lampouras</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <pages>1542-1550</pages>
      <abstract>Advances in natural language processing, such as transfer learning from pre-trained language models, have impacted how models are trained for programming language tasks too. Previous research primarily explored code pre-training and expanded it through multi-modality and multi-tasking, yet the data for downstream tasks remain modest in size. Focusing on data utilization for downstream tasks, we propose and adapt augmentation methods that yield consistent improvements in code translation and summarization by up to 6.9% and 7.5% respectively. Further analysis suggests that our methods work orthogonally and show benefits in output code style and numeric consistency. We also discuss test data imperfections.</abstract>
      <url hash="1fae719e">2023.findings-eacl.114</url>
      <bibkey>chen-lampouras-2023-exploring</bibkey>
    </paper>
    <paper id="115">
      <title>Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking</title>
      <author><first>Derek</first><last>Chen</last><affiliation>Columbia University</affiliation></author>
      <author><first>Kun</first><last>Qian</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>1551-1564</pages>
      <abstract>Prompt-based methods with large pre-trained language models (PLMs) have shown impressive unaided performance across many NLP tasks. These models improve even further with the addition of a few labeled in-context exemplars to guide output generation. However, for more complex tasks such as dialogue state tracking (DST), designing prompts that reliably convey the desired intent is nontrivial, leading to unstable results. Furthermore, building in-context exemplars for dialogue tasks is difficult because conversational contexts are long while model input lengths are relatively short.To overcome these issues we first adapt a meta-learning scheme to the dialogue domain which stabilizes the ability of the model to perform well under various prompts. We additionally design a novel training method to improve upon vanilla retrieval mechanisms to find ideal in-context examples. Finally, we introduce a saliency model to limit dialogue text length, allowing us to include more exemplars per query. In effect, we are able to achieve highly competitive results for few-shot DST on MultiWOZ.</abstract>
      <url hash="f2a86463">2023.findings-eacl.115</url>
      <bibkey>chen-etal-2023-stabilized</bibkey>
    </paper>
    <paper id="116">
      <title>Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers</title>
      <author><first>Chia-Chien</first><last>Hung</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>University of Hamburg</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>University of Würzburg</affiliation></author>
      <pages>1565-1580</pages>
      <abstract>Demographic factors (e.g., gender or age) shape our language. Previous work showed that incorporating demographic factors can consistently improve performance for various NLP tasks with traditional NLP models. In this work, we investigate whether these previous findings still hold with state-of-the-art pretrained Transformer-based language models (PLMs). We use three common specialization methods proven effective for incorporating external knowledge into pretrained Transformers (e.g., domain-specific or geographic knowledge). We adapt the language representations for the demographic dimensions of gender and age, using continuous language modeling and dynamic multi-task learning for adaptation, where we couple language modeling objectives with the prediction of demographic classes. Our results, when employing a multilingual PLM, show substantial gains in task performance across four languages (English, German, French, and Danish), which is consistent with the results of previous work. However, controlling for confounding factors – primarily domain and language proficiency of Transformer-based PLMs – shows that downstream performance gains from our demographic adaptation do not actually stem from demographic knowledge. Our results indicate that demographic specialization of PLMs, while holding promise for positive societal impact, still represents an unsolved problem for (modern) NLP.</abstract>
      <url hash="4516c4d1">2023.findings-eacl.116</url>
      <bibkey>hung-etal-2023-demographic</bibkey>
    </paper>
    <paper id="117">
      <title><fixed-case>JBL</fixed-case>i<fixed-case>MP</fixed-case>: <fixed-case>J</fixed-case>apanese Benchmark of Linguistic Minimal Pairs</title>
      <author><first>Taiga</first><last>Someya</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>1581-1594</pages>
      <abstract>In this paper, we introduce JBLiMP (Japanese Benchmark of Linguistic Minimal Pairs), a novel dataset for targeted syntactic evaluations of language models in Japanese. JBLiMP consists of 331 minimal pairs, which are created based on acceptability judgments extracted from journal articles in theoretical linguistics. These minimal pairs are grouped into 11 categories, each covering a different linguistic phenomenon. JBLiMP is unique in that it successfully combines two important features independently observed in existing datasets: (i) coverage of complex linguistic phenomena (cf. CoLA) and (ii) presentation of sentences as minimal pairs (cf. BLiMP). In addition, JBLiMP is the first dataset for targeted syntactic evaluations of language models in Japanese, thus allowing the comparison of syntactic knowledge of language models across different languages. We then evaluate the syntactic knowledge of several language models on JBLiMP: GPT-2, LSTM, and n-gram language models. The results demonstrated that all the architectures achieved comparable overall accuracies around 75%. Error analyses by linguistic phenomenon further revealed that these language models successfully captured local dependencies like nominal structures, but not long-distance dependencies such as verbal agreement and binding.</abstract>
      <url hash="f2dd9340">2023.findings-eacl.117</url>
      <bibkey>someya-oseki-2023-jblimp</bibkey>
    </paper>
    <paper id="118">
      <title><fixed-case>SMATCH</fixed-case>++: Standardized and Extended Evaluation of Semantic Graphs</title>
      <author><first>Juri</first><last>Opitz</last><affiliation>Heidelberg University</affiliation></author>
      <pages>1595-1607</pages>
      <abstract>The Smatch metric is a popular method for evaluating graph distances, as is necessary, for instance, to assess the performance of semantic graph parsing systems. However, we observe some issues in the metric that jeopardize meaningful evaluation. E.g., opaque pre-processing choices can affect results, and current graph-alignment solvers do not provide us with upper-bounds. Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore, adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity) are spread out, and lack a unifying framework. For better inspection, we divide the metric into three modules: pre-processing, alignment, and scoring. Examining each module, we specify its goals and diagnose potential issues, for which we discuss and test mitigation strategies. For pre-processing, we show how to fully conform to annotation guidelines that allow structurally deviating but valid graphs. For safer and enhanced alignment, we show the feasibility of optimal alignment in a standard evaluation setup, and develop a lossless graph compression method that shrinks the search space and significantly increases efficiency. For improved scoring, we propose standardized and extended metric calculation of fine-grained sub-graph meaning aspects. Our code is available at https://github.com/flipz357/smatchpp</abstract>
      <url hash="250c3657">2023.findings-eacl.118</url>
      <bibkey>opitz-2023-smatch</bibkey>
    </paper>
    <paper id="119">
      <title>An Extended Sequence Tagging Vocabulary for Grammatical Error Correction</title>
      <author><first>Stuart</first><last>Mesham</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Christopher</first><last>Bryant</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Marek</first><last>Rei</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>King’s College London</affiliation></author>
      <pages>1608-1619</pages>
      <abstract>We extend a current sequence-tagging approach to Grammatical Error Correction (GEC) by introducing specialised tags for spelling correction and morphological inflection using the SymSpell and LemmInflect algorithms. Our approach improves generalisation: the proposed new tagset allows a smaller number of tags to correct a larger range of errors. Our results show a performance improvement both overall and in the targeted error categories. We further show that ensembles trained with our new tagset outperform those trained with the baseline tagset on the public BEA benchmark.</abstract>
      <url hash="5bba87a0">2023.findings-eacl.119</url>
      <attachment type="software" hash="6046b7c7">2023.findings-eacl.119.software.zip</attachment>
      <bibkey>mesham-etal-2023-extended</bibkey>
    </paper>
    <paper id="120">
      <title>Cheating to Identify Hard Problems for Neural Machine Translation</title>
      <author><first>Proyag</first><last>Pal</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Kenneth</first><last>Heafield</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>1620-1631</pages>
      <abstract>We identify hard problems for neural machine translation models by analyzing progressively higher-scoring translations generated by letting models cheat to various degrees. If a system cheats and still gets something wrong, that suggests it is a hard problem. We experiment with two forms of cheating: providing the model a compressed representation of the target as an additional input, and fine-tuning on the test set. Contrary to popular belief, we find that the most frequent tokens are not necessarily the most accurately translated due to these often being function words and punctuation that can be used more flexibly in translation, or content words which can easily be paraphrased. We systematically analyze system outputs to identify categories of tokens which are particularly hard for the model to translate, and find that this includes certain types of named entities, subordinating conjunctions, and unknown and foreign words. We also encounter a phenomenon where words, often names, which were not infrequent in the training data are still repeatedly mistranslated by the models — we dub this the Fleetwood Mac problem.</abstract>
      <url hash="7c468a9e">2023.findings-eacl.120</url>
      <bibkey>pal-heafield-2023-cheating</bibkey>
    </paper>
    <paper id="121">
      <title>Model-Agnostic Bias Measurement in Link Prediction</title>
      <author><first>Lena</first><last>Schwertmann</last><affiliation>Hasso Plattner Institute, University of Potsdam</affiliation></author>
      <author><first>Manoj Prabhakar</first><last>Kannan Ravi</last><affiliation>LexisNexis</affiliation></author>
      <author><first>Gerard</first><last>de Melo</last><affiliation>Hasso Plattner Institute, University of Potsdam</affiliation></author>
      <pages>1632-1648</pages>
      <abstract>Link prediction models based on factual knowledge graphs are commonly used in applications such as search and question answering. However, work investigating social bias in these models has been limited. Previous work focused on knowledge graph embeddings, so more recent classes of models achieving superior results by fine-tuning Transformers have not yet been investigated. We therefore present a model-agnostic approach for bias measurement leveraging fairness metrics to compare bias in knowledge graph embedding-based predictions (KG only) with models that use pre-trained, Transformer-based language models (KG+LM). We further create a dataset to measure gender bias in occupation predictions and assess whether the KG+LM models are more or less biased than KG only models. We find that gender bias tends to be higher for the KG+LM models and analyze potential connections to the accuracy of the models and the data bias inherent in our dataset.Finally, we discuss the limitations and ethical considerations of our work. The repository containing the source code and the data set is publicly available at https://github.com/lena-schwert/comparing-bias-in-KG-models.</abstract>
      <url hash="ff8d262c">2023.findings-eacl.121</url>
      <bibkey>schwertmann-etal-2023-model</bibkey>
    </paper>
    <paper id="122">
      <title>Divergence-Based Domain Transferability for Zero-Shot Classification</title>
      <author><first>Alexander</first><last>Pugantsov</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Richard</first><last>McCreadie</last><affiliation>University of Glasgow</affiliation></author>
      <pages>1649-1654</pages>
      <abstract>Transferring learned patterns from pretrained neural language models has been shown to significantly improve effectiveness across a variety of language-based tasks, meanwhile further tuning on intermediate tasks has been demonstrated to provide additional performance benefits, provided the intermediate task is sufficiently related to the target task. However, how to identify related tasks is an open problem, and brute-force searching effective task combinations is prohibitively expensive. Hence, the question arises, are we able to improve the effectiveness and efficiency of tasks with no training examples through selective fine-tuning? In this paper, we explore statistical measures that approximate the divergence between domain representations as a means to estimate whether tuning using one task pair will exhibit performance benefits over tuning another. This estimation can then be used to reduce the number of task pairs that need to be tested by eliminating pairs that are unlikely to provide benefits. Through experimentation over 58 tasks and over 6,600 task pair combinations, we demonstrate that statistical measures can distinguish effective task pairs, and the resulting estimates can reduce end-to-end runtime by up to 40%.</abstract>
      <url hash="ed1f36bf">2023.findings-eacl.122</url>
      <bibkey>pugantsov-mccreadie-2023-divergence</bibkey>
    </paper>
    <paper id="123">
      <title><fixed-case>EDU</fixed-case>-level Extractive Summarization with Varying Summary Lengths</title>
      <author><first>Yuping</first><last>Wu</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Ching-Hsun</first><last>Tseng</last><affiliation>the University of Manchester</affiliation></author>
      <author><first>Jiayu</first><last>Shang</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Shengzhong</first><last>Mao</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Xiao-Jun</first><last>Zeng</last><affiliation>University of Manchester</affiliation></author>
      <pages>1655-1667</pages>
      <abstract>Extractive models usually formulate text summarization as extracting fixed top-k salient sentences from the document as a summary. Few works exploited extracting finer-grained Elementary Discourse Unit (EDU) with little analysis and justification for the extractive unit selection. Further, the selection strategy of the fixed top-k salient sentences fits the summarization need poorly, as the number of salient sentences in different documents varies and therefore a common or best k does not exist in reality. To fill these gaps, this paper first conducts the comparison analysis of oracle summaries based on EDUs and sentences, which provides evidence from both theoretical and experimental perspectives to justify and quantify that EDUs make summaries with higher automatic evaluation scores than sentences. Then, considering this merit of EDUs, this paper further proposes an EDU-level extractive model with Varying summary Lengths (EDU-VL) and develops the corresponding learning algorithm. EDU-VL learns to encode and predict probabilities of EDUs in the document, generate multiple candidate summaries with varying lengths based on various k values, and encode and score candidate summaries, in an end-to-end training manner. Finally, EDU-VL is experimented on single and multi-document benchmark datasets and shows improved performances on ROUGE scores in comparison with state-of-the-art extractive models, and further human evaluation suggests that EDU-constituent summaries maintain good grammaticality and readability.</abstract>
      <url hash="ed168e9f">2023.findings-eacl.123</url>
      <attachment type="software" hash="182f1ce9">2023.findings-eacl.123.software.zip</attachment>
      <bibkey>wu-etal-2023-edu</bibkey>
    </paper>
    <paper id="124">
      <title>“Chère maison” or “maison chère”? Transformer-based prediction of adjective placement in <fixed-case>F</fixed-case>rench</title>
      <author><first>Eleni</first><last>Metheniti</last><affiliation>CLLE-CNRS and IRIT-CNRS</affiliation></author>
      <author><first>Tim</first><last>Van de Cruys</last><affiliation>University of Leuven</affiliation></author>
      <author><first>Wissam</first><last>Kerkri</last><affiliation>Clle</affiliation></author>
      <author><first>Juliette</first><last>Thuilier</last><affiliation>Université Toulouse Jean Jaurès &amp; CLLE-ERSS</affiliation></author>
      <author><first>Nabil</first><last>Hathout</last><affiliation>CLLE, CNRS &amp; Universite de Toulouse</affiliation></author>
      <pages>1668-1683</pages>
      <abstract>In French, the placement of the adjective within a noun phrase is subject to variation: it can appear either before or after the noun. We conduct experiments to assess whether transformer-based language models are able to learn the adjective position in noun phrases in French –a position which depends on several linguistic factors. Prior findings have shown that transformer models are insensitive to permutated word order, but in this work, we show that finetuned models are successful at learning and selecting the correct position of the adjective. However, this success can be attributed to the process of finetuning rather than the linguistic knowledge acquired during pretraining, as evidenced by the low accuracy of experiments of classification that make use of pretrained embeddings. Comparing the finetuned models to the choices of native speakers (with a questionnaire), we notice that the models favor context and global syntactic roles, and are weaker with complex structures and fixed expressions.</abstract>
      <url hash="56bcd1df">2023.findings-eacl.124</url>
      <bibkey>metheniti-etal-2023-chere</bibkey>
    </paper>
    <paper id="125">
      <title>On the Role of Reviewer Expertise in Temporal Review Helpfulness Prediction</title>
      <author><first>Mir Tafseer</first><last>Nayeem</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Davood</first><last>Rafiei</last><affiliation>University of Alberta</affiliation></author>
      <pages>1684-1692</pages>
      <abstract>Helpful reviews have been essential for the success of e-commerce services, as they help customers make quick purchase decisions and benefit the merchants in their sales. While many reviews are informative, others provide little value and may contain spam, excessive appraisal, or unexpected biases. With the large volume of reviews and their uneven quality, the problem of detecting helpful reviews has drawn much attention lately. Existing methods for identifying helpful reviews primarily focus on review text and ignore the two key factors of (1) who post the reviews and (2) when the reviews are posted. Moreover, the helpfulness votes suffer from scarcity for less popular products and recently submitted (a.k.a., cold-start) reviews. To address these challenges, we introduce a dataset and develop a model that integrates the reviewer’s expertise, derived from the past review history of the reviewers, and the temporal dynamics of the reviews to automatically assess review helpfulness. We conduct experiments on our dataset to demonstrate the effectiveness of incorporating these factors and report improved results compared to several well-established baselines.</abstract>
      <url hash="06bfaff6">2023.findings-eacl.125</url>
      <bibkey>nayeem-rafiei-2023-role</bibkey>
    </paper>
    <paper id="126">
      <title>Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering</title>
      <author><first>Chenxi</first><last>Whitehouse</last><affiliation>City, University of London</affiliation></author>
      <author><first>Tillman</first><last>Weyde</last><affiliation>City, University of London</affiliation></author>
      <author><first>Pranava</first><last>Madhyastha</last><affiliation>City, University of London</affiliation></author>
      <pages>1693-1705</pages>
      <abstract>The field of visual question answering (VQA) has recently seen a surge in research focused on providing explanations for predicted answers. However, current systems mostly rely on separate models to predict answers and generate explanations, leading to less grounded and frequently inconsistent results. To address this, we propose a multitask learning approach towards a Unified Model for Answer and Explanation generation (UMAE). Our approach involves the addition of artificial prompt tokens to training data and fine-tuning a multimodal encoder-decoder model on a variety of VQA-related tasks. In our experiments, UMAE models surpass the prior state-of-the-art answer accuracy on A-OKVQA by 10~15%, show competitive results on OK-VQA, achieve new state-of-the-art explanation scores on A-OKVQA and VCR, and demonstrate promising out-of-domain performance on VQA-X.</abstract>
      <url hash="72fc7a80">2023.findings-eacl.126</url>
      <bibkey>whitehouse-etal-2023-towards</bibkey>
    </paper>
    <paper id="127">
      <title>Machine Translation between Spoken Languages and Signed Languages Represented in <fixed-case>S</fixed-case>ign<fixed-case>W</fixed-case>riting</title>
      <author><first>Zifan</first><last>Jiang</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Amit</first><last>Moryossef</last><affiliation>Bar-Ilan university, University of Zurich</affiliation></author>
      <author><first>Mathias</first><last>Müller</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Sarah</first><last>Ebling</last><affiliation>University of Zurich</affiliation></author>
      <pages>1706-1724</pages>
      <abstract>This paper presents work on novel machine translation (MT) systems between spoken and signed languages, where signed languages are represented in SignWriting, a sign language writing system. Our work seeks to address the lack of out-of-the-box support for signed languages in current MT systems and is based on the SignBank dataset, which contains pairs of spoken language text and SignWriting content. We introduce novel methods to parse, factorize, decode, and evaluate SignWriting, leveraging ideas from neural factored MT. In a bilingual setup—translating from American Sign Language to (American) English—our method achieves over 30 BLEU, while in two multilingual setups—translating in both directions between spoken languages and signed languages—we achieve over 20 BLEU. We find that common MT techniques used to improve spoken language translation similarly affect the performance of sign language translation. These findings validate our use of an intermediate text representation for signed languages to include them in natural language processing research.</abstract>
      <url hash="44d816fe">2023.findings-eacl.127</url>
      <bibkey>jiang-etal-2023-machine</bibkey>
    </paper>
    <paper id="128">
      <title>A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models</title>
      <author><first>Jimin</first><last>Sun</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Patrick</first><last>Fernandes</last><affiliation>Carnegie Mellon University, Instituto de Telecomunicações</affiliation></author>
      <author><first>Xinyi</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1725-1735</pages>
      <abstract>Recent works on tokenizer-free multilingual pretrained models show promising results in improving cross-lingual transfer and reducing engineering overhead compared to subword-based alternatives.However, previous work mainly focuses on reporting accuracy on a limited set of tasks and data settings, placing less emphasis on other important factors when tuning and deploying the models in practice, such as memory usage, inference speed, and finetuning data efficiency. We attempt to fill this gap by performing a comprehensive empirical comparison of multilingual tokenizer-free and subword-based models considering the various dimensions. Surprisingly, we find that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage. Based on these results, we encourage future work in tokenizer-free methods to consider these factors when designing and evaluating new models.</abstract>
      <url hash="fa25f392">2023.findings-eacl.128</url>
      <bibkey>sun-etal-2023-multi</bibkey>
    </paper>
    <paper id="129">
      <title>Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey</title>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <author><first>Svitlana</first><last>Vakulenko</last><affiliation>Amazon</affiliation></author>
      <author><first>Marco</first><last>del Tredici</last><affiliation>Amazon</affiliation></author>
      <author><first>Gianni</first><last>Barlacchi</last><affiliation>Amazon Alexa</affiliation></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Adria</first><last>de Gispert</last><affiliation>Amazon</affiliation></author>
      <pages>1736-1750</pages>
      <abstract>Neural ranking (NR) has become a key component for open-domain question-answering in order to access external knowledge. However, training a good NR model requires substantial amounts of relevance annotations, which is very costly to scale. To address this, a growing body of research works have been proposed to reduce the annotation cost by training the NR model with weak supervision (WS) instead. These works differ in what resources they require and employ a diverse set of WS signals to train the model. Understanding such differences is crucial for choosing the right WS technique. To facilitate this understanding, we provide a structured overview of standard WS signals used for training a NR model. Based on their required resources, we divide them into three main categories: (1) only documents are needed; (2) documents and questions are needed; and (3) documents and question-answer pairs are needed. For every WS signal, we review its general idea and choices. Promising directions are outlined for future research.</abstract>
      <url hash="6740f9e9">2023.findings-eacl.129</url>
      <bibkey>shen-etal-2023-neural</bibkey>
    </paper>
    <paper id="130">
      <title>Double Retrieval and Ranking for Accurate Question Answering</title>
      <author><first>Zeyu</first><last>Zhang</last><affiliation>School of Information, the University of Arizona</affiliation></author>
      <author><first>Thuy</first><last>Vu</last><affiliation>Amazon</affiliation></author>
      <author><first>Alessandro</first><last>Moschitti</last><affiliation>Amazon</affiliation></author>
      <pages>1751-1762</pages>
      <abstract>Recent work has shown that an answer verification step introduced in Transformer-based answer selection models can significantly improve the state of the art in Question Answering. This step is performed by aggregating the embeddings of top $k$ answer candidates to support the verification of a target answer. Although the approach is intuitive and sound, it still shows two limitations: (i) the supporting candidates are ranked only according to the relevancy with the question and not with the answer, and (ii) the support provided by the other answer candidates is suboptimal as these are retrieved independently of the target answer. In this paper, we address both drawbacks by proposing (i) a double reranking model, which, for each target answer, selects the best support; and (ii) a second neural retrieval stage designed to encode question and answer pair as the query, which finds more specific verification information. The results on well-known datasets for Answer Sentence Selection show significant improvement over the state of the art.</abstract>
      <url hash="53c44d09">2023.findings-eacl.130</url>
      <bibkey>zhang-etal-2023-double</bibkey>
    </paper>
    <paper id="131">
      <title>Evaluating the Diversity, Equity, and Inclusion of <fixed-case>NLP</fixed-case> Technology: A Case Study for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Simran</first><last>Khanuja</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Sebastian</first><last>Ruder</last><affiliation>Google</affiliation></author>
      <author><first>Partha</first><last>Talukdar</last><affiliation>Google Research and IISc</affiliation></author>
      <pages>1763-1777</pages>
      <abstract>In order for NLP technology to be widely applicable, fair, and useful, it needs to serve a diverse set of speakers across the world’s languages, be equitable, i.e., not unduly biased towards any particular language, and be inclusive of all users, particularly in low-resource settings where compute constraints are common. In this paper, we propose an evaluation paradigm that assesses NLP technologies across all three dimensions. While diversity and inclusion have received attention in recent literature, equity is currently unexplored. We propose to address this gap using the Gini coefficient, a well-established metric used for estimating societal wealth inequality. Using our paradigm, we highlight the distressed state of current technologies for Indian (IN) languages (a linguistically large and diverse set, with a varied speaker population), across all three dimensions. To improve upon these metrics, we demonstrate the importance of region-specific choices in model building and dataset creation, and more importantly, propose a novel, generalisable approach to optimal resource allocation during fine-tuning. Finally, we discuss steps to mitigate these biases and encourage the community to employ multi-faceted evaluation when building linguistically diverse and equitable technologies.</abstract>
      <url hash="5b4910ba">2023.findings-eacl.131</url>
      <bibkey>khanuja-etal-2023-evaluating</bibkey>
    </paper>
    <paper id="132">
      <title>Joint Reasoning on Hybrid-knowledge sources for Task-Oriented Dialog</title>
      <author><first>Mayank</first><last>Mishra</last><affiliation>IBM Research</affiliation></author>
      <author><first>Danish</first><last>Contractor</last><affiliation>IBM Research IBM Research</affiliation></author>
      <author><first>Dinesh</first><last>Raghu</last><affiliation>IBM Research</affiliation></author>
      <pages>1778-1787</pages>
      <abstract>Traditional systems designed for task oriented dialog utilize knowledge present only in structured knowledge sources to generate responses. However, relevant information required to generate responses may also reside in unstructured sources, such as documents. Recent state of the art models such as HyKnow (Gao et al., 2021b) and SEKNOW (Gao et al., 2021a) aimed at overcoming these challenges make limiting assumptions about the knowledge sources. For instance, these systems assume that certain types of information, such as a phone number, is always present in a structured knowledge base (KB) while information about aspects such as entrance ticket prices, would always be available in documents.In this paper, we create a modified version of the MutliWOZ-based dataset prepared by (Gao et al., 2021a) to demonstrate how current methods have significant degradation in performance when strict assumptions about the source of information are removed. Then, in line with recent work exploiting pre-trained language models, we fine-tune a BART (Lewiset al., 2020) based model using prompts (Brown et al., 2020; Sun et al., 2021) for the tasks of querying knowledge sources, as well as, for response generation, without makingassumptions about the information present in each knowledge source. Through a series of experiments, we demonstrate that our model is robust to perturbations to knowledge modality (source of information), and that it can fuse information from structured as well as unstructured knowledge to generate responses.</abstract>
      <url hash="527851d8">2023.findings-eacl.132</url>
      <bibkey>mishra-etal-2023-joint</bibkey>
    </paper>
    <paper id="133">
      <title>Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models</title>
      <author><first>Mohammadreza</first><last>Banaei</last><affiliation>Epfl</affiliation></author>
      <author><first>Klaudia</first><last>Bałazy</last><affiliation>Jagiellonian University</affiliation></author>
      <author><first>Artur</first><last>Kasymov</last><affiliation>Jagiellonian University</affiliation></author>
      <author><first>Rémi</first><last>Lebret</last><affiliation>Epfl</affiliation></author>
      <author><first>Jacek</first><last>Tabor</last><affiliation>Jagiellonian University</affiliation></author>
      <author><first>Karl</first><last>Aberer</last><affiliation>Epfl</affiliation></author>
      <pages>1788-1805</pages>
      <abstract>Recent transformer language models achieve outstanding results in many natural language processing (NLP) tasks. However, their enormous size often makes them impractical on memory-constrained devices, requiring practitioners to compress them to smaller networks. In this paper, we explore offline compression methods, meaning computationally-cheap approaches that do not require further fine-tuning of the compressed model. We challenge the classical matrix factorization methods by proposing a novel, better-performing autoencoder-based framework. We perform a comprehensive ablation study of our approach, examining its different aspects over a diverse set of evaluation settings. Moreover, we show that enabling collaboration between modules across layers by compressing certain modules together positively impacts the final model performance. Experiments on various NLP tasks demonstrate that our approach significantly outperforms commonly used factorization-based offline compression methods.</abstract>
      <url hash="1d926a91">2023.findings-eacl.133</url>
      <bibkey>banaei-etal-2023-revisiting</bibkey>
    </paper>
    <paper id="134">
      <title><fixed-case>P</fixed-case>ri<fixed-case>M</fixed-case>e<fixed-case>SRL</fixed-case>-Eval: A Practical Quality Metric for Semantic Role Labeling Systems Evaluation</title>
      <author><first>Ishan</first><last>Jindal</last><affiliation>IBM Research</affiliation></author>
      <author><first>Alexandre</first><last>Rademaker</last><affiliation>IBM Research and EMAp/FGV</affiliation></author>
      <author><first>Khoi-Nguyen</first><last>Tran</last><affiliation>Ibm</affiliation></author>
      <author><first>Huaiyu</first><last>Zhu</last><affiliation>IBM Research - Almaden</affiliation></author>
      <author><first>Hiroshi</first><last>Kanayama</last><affiliation>IBM Research - Tokyo</affiliation></author>
      <author><first>Marina</first><last>Danilevsky</last><affiliation>IBM Research</affiliation></author>
      <author><first>Yunyao</first><last>Li</last><affiliation>Apple</affiliation></author>
      <pages>1806-1818</pages>
      <abstract>Semantic role labeling (SRL) identifies the predicate-argument structure in a sentence. This task is usually accomplished in four steps: predicate identification, predicate sense disambiguation, argument identification, and argument classification. Errors introduced at one step propagate to later steps. Unfortunately, the existing SRL evaluation scripts do not consider the full effect of this error propagation aspect. They either evaluate arguments independent of predicate sense (CoNLL09) or do not evaluate predicate sense at all (CoNLL05), yielding an inaccurate SRL model performance on the argument classification task. In this paper, we address key practical issues with existing evaluation scripts and propose a more strict SRL evaluation metric PriMeSRL. We observe that by employing PriMeSRL, the quality evaluation of all SoTA SRL models drops significantly, and their relative rankings also change. We also show that PriMeSRLsuccessfully penalizes actual failures in SoTA SRL models.</abstract>
      <url hash="ea374953">2023.findings-eacl.134</url>
      <attachment type="software" hash="e7bf8e40">2023.findings-eacl.134.software.zip</attachment>
      <bibkey>jindal-etal-2023-primesrl</bibkey>
    </paper>
    <paper id="135">
      <title>Prompt-based Learning for Text Readability Assessment</title>
      <author><first>Bruce W.</first><last>Lee</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Jason</first><last>Lee</last><affiliation>Lxper Ai</affiliation></author>
      <pages>1819-1824</pages>
      <abstract>We propose the novel adaptation of a pre-trained seq2seq model for readability assessment. We prove that a seq2seq model - T5 or BART - can be adapted to discern which text is more difficult from two given texts (pairwise). As an exploratory study to prompt-learn a neural network for text readability in a text-to-text manner, we report useful tips for future work in seq2seq training and ranking-based approach to readability assessment. Specifically, we test nine input-output formats/prefixes and show that they can significantly influence the final model performance.Also, we argue that the combination of text-to-text training and pairwise ranking setup 1) enables leveraging multiple parallel text simplification data for teaching readability and 2) trains a neural model for the general concept of readability (therefore, better cross-domain generalization). At last, we report a 99.6% pairwise classification accuracy on Newsela and a 98.7% for OneStopEnglish, through a joint training approach. Our code is available at github.com/brucewlee/prompt-learning-readability.</abstract>
      <url hash="50323060">2023.findings-eacl.135</url>
      <bibkey>lee-lee-2023-prompt</bibkey>
    </paper>
    <paper id="136">
      <title>Best Practices in the Creation and Use of Emotion Lexicons</title>
      <author><first>Saif</first><last>Mohammad</last><affiliation>Nrc</affiliation></author>
      <pages>1825-1836</pages>
      <abstract>Words play a central role in how we express ourselves. Lexicons of word–emotion associations are widely used in research and real-world applications for sentiment analysis, tracking emotions associated with products and policies, studying health disorders, tracking emotional arcs of stories, and so on. However, inappropriate and incorrect use of these lexicons can lead to not just sub-optimal results, but also inferences that are directly harmful to people. This paper brings together ideas from Affective Computing and AI Ethics to present, some of the practical and ethical considerations involved in the creation and use of emotion lexicons – best practices. The goal is to provide a comprehensive set of relevant considerations, so that readers (especially those new to work with emotions) can find relevant information in one place. We hope this work will facilitate more thoughtfulness when one is deciding on what emotions to work on, how to create an emotion lexicon, how to use an emotion lexicon, how to draw meaningful inferences, and how to judge success.</abstract>
      <url hash="d5a03e51">2023.findings-eacl.136</url>
      <bibkey>mohammad-2023-best</bibkey>
    </paper>
    <paper id="137">
      <title>The Role of Semantic Parsing in Understanding Procedural Text</title>
      <author><first>Hossein</first><last>Rajaby Faghihi</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Choh Man</first><last>Teng</last><affiliation>Institute for Human and Machine Cognition</affiliation></author>
      <author><first>James</first><last>Allen</last><affiliation>University of Rochester</affiliation></author>
      <pages>1837-1849</pages>
      <abstract>In this paper, we investigate whether symbolic semantic representations, extracted from deep semantic parsers, can help reasoning over the states of involved entities in a procedural text. We consider a deep semantic parser~(TRIPS) and semantic role labeling as two sources of semantic parsing knowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural reasoning framework.Second, we integrate semantic parsing information into state-of-the-art neural models to conduct procedural reasoning.Our experiments indicate that explicitly incorporating such semantic knowledge improves procedural understanding. This paper presents new metrics for evaluating procedural reasoning tasks that clarify the challenges and identify differences among neural, symbolic, and integrated models.</abstract>
      <url hash="c5d442f0">2023.findings-eacl.137</url>
      <bibkey>rajaby-faghihi-etal-2023-role</bibkey>
    </paper>
    <paper id="138">
      <title>Named Entity Recognition in a Very Homogenous Domain</title>
      <author><first>Oshin</first><last>Agarwal</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Ani</first><last>Nenkova</last><affiliation>Adobe Research</affiliation></author>
      <pages>1850-1855</pages>
      <abstract>Machine Learning models have lower accuracy when tested on out-of-domain data. Developing models that perform well on several domains or can be quickly adapted to a new domain is an important research area. Domain, however, is a vague term, that can refer to any aspect of data such as language, genre, source and structure. We consider a very homogeneous source of data, specifically sentences from news articles from the same newspaper in English, and collect a dataset of such “in-domain” sentences annotated with named entities. We find that even in such a homogeneous domain, the performance of named entity recognition models varies significantly across news topics. Selection of diverse data, as we demonstrate, is crucial even in a seemingly homogeneous domain.</abstract>
      <url hash="ed8f4b60">2023.findings-eacl.138</url>
      <bibkey>agarwal-nenkova-2023-named</bibkey>
    </paper>
    <paper id="139">
      <title>Crawling The Internal Knowledge-Base of Language Models</title>
      <author><first>Roi</first><last>Cohen</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Mor</first><last>Geva</last><affiliation>Google</affiliation></author>
      <author><first>Jonathan</first><last>Berant</last><affiliation>Tel Aviv University and AI2</affiliation></author>
      <author><first>Amir</first><last>Globerson</last><affiliation>Tel Aviv University, Google</affiliation></author>
      <pages>1856-1869</pages>
      <abstract>Language models are trained on large volumes of text, and as a result their parameters might contain a significant body of factual knowledge. Any downstream task performed by these models implicitly builds on these facts, and thus it is highly desirable to have means for representing this body of knowledge in an interpretable way. However, there is currently no mechanism for such a representation.Here, we propose to address this goal by extracting a knowledge-graph of facts from a given language model. We describe a procedure for “crawling” the internal knowledge-base of a language model. Specifically, given a seed entity, we expand a knowledge-graph around it. The crawling procedure is decomposed into sub-tasks, realized through specially designed prompts that control for both precision (i.e., that no wrong facts are generated) and recall (i.e., the number of facts generated). We evaluate our approach on graphs crawled starting from dozens of seed entities, and show it yields high precision graphs (82-92%), while emitting a reasonable number of facts per entity.</abstract>
      <url hash="1f54aae5">2023.findings-eacl.139</url>
      <bibkey>cohen-etal-2023-crawling</bibkey>
    </paper>
    <paper id="140">
      <title>Intent Identification and Entity Extraction for Healthcare Queries in <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Ankan</first><last>Mullick</last><affiliation>Indian Institute of Technology, Kharagpur</affiliation></author>
      <author><first>Ishani</first><last>Mondal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sourjyadip</first><last>Ray</last><affiliation>Indian Institute of Technology, Kharagpur</affiliation></author>
      <author><first>Raghav</first><last>R</last><affiliation>Cmu</affiliation></author>
      <author><first>G</first><last>Chaitanya</last><affiliation>IIT Kharagpur</affiliation></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <pages>1870-1881</pages>
      <abstract>Scarcity of data and technological limitations for resource-poor languages in developing countries like India poses a threat to the development of sophisticated NLU systems for healthcare. To assess the current status of various state-of-the-art language models in healthcare, this paper studies the problem by initially proposing two different Healthcare datasets, Indian Healthcare Query Intent-WebMD and 1mg (IHQID-WebMD and IHQID-1mg) and one real world Indian hospital query data in English and multiple Indic languages (Hindi, Bengali, Tamil, Telugu, Marathi and Gujarati) which are annotated with the query intents as well as entities. Our aim is to detect query intents and corresponding entities. We perform extensive experiments on a set of models which in various realistic settings and explore two scenarios based on the access to English data only (less costly) and access to target language data (more expensive). We analyze context specific practical relevancy through empirical analysis. The results, expressed in terms of overall F-score show that our approach is practically useful to identify intents and entities.</abstract>
      <url hash="d72f9391">2023.findings-eacl.140</url>
      <bibkey>mullick-etal-2023-intent</bibkey>
    </paper>
    <paper id="141">
      <title>Text-Derived Knowledge Helps Vision: A Simple Cross-modal Distillation for Video-based Action Anticipation</title>
      <author><first>Sayontan</first><last>Ghosh</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Tanvi</first><last>Aggarwal</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Minh</first><last>Hoai</last><affiliation>Stony Brook University</affiliation></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>Stony Brook University</affiliation></author>
      <pages>1882-1897</pages>
      <abstract>Anticipating future actions in a video is useful for many autonomous and assistive technologies. Prior action anticipation work mostly treat this as a vision modality problem, where the models learn the task information primarily from the video features in the action anticipation datasets. However, knowledge about action sequences can also be obtained from external textual data. In this work, we show how knowledge in pretrained language models can be adapted and distilled into vision based action anticipation models. We show that a simple distillation technique can achieve effective knowledge transfer and provide consistent gains on a strong vision model (Anticipative Vision Transformer) for two action anticipation datasets (3.5% relative gain on EGTEA-GAZE+ and 7.2% relative gain on EPIC-KITCHEN 55), giving a new state-of-the-art result.</abstract>
      <url hash="93239279">2023.findings-eacl.141</url>
      <bibkey>ghosh-etal-2023-text</bibkey>
    </paper>
    <paper id="142">
      <title>Simple Yet Effective Synthetic Dataset Construction for Unsupervised Opinion Summarization</title>
      <author><first>Ming</first><last>Shen</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Jie</first><last>Ma</last><affiliation>AWS AI Lab</affiliation></author>
      <author><first>Shuai</first><last>Wang</last><affiliation>Amazon AI</affiliation></author>
      <author><first>Yogarshi</first><last>Vyas</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Kalpit</first><last>Dixit</last><affiliation>Amazon</affiliation></author>
      <author><first>Miguel</first><last>Ballesteros</last><affiliation>Amazon</affiliation></author>
      <author><first>Yassine</first><last>Benajiba</last><affiliation>AWS AI Labs</affiliation></author>
      <pages>1898-1911</pages>
      <abstract>Opinion summarization provides an important solution for summarizing opinions expressed among a large number of reviews. However, generating aspect-specific and general summaries is challenging due to the lack of annotated data. In this work, we propose two simple yet effective unsupervised approaches to generate both aspect-specific and general opinion summaries by training on synthetic datasets constructed with aspect-related review contents. Our first approach, Seed Words Based Leave-One-Out (SW-LOO), identifies aspect-related portions of reviews simply by exact-matching aspect seed words and outperforms existing methods by 3.4 ROUGE-L points on Space and 0.5 ROUGE-1 point on Oposum+ for aspect-specific opinion summarization.Our second approach, Natural Language Inference Based Leave-One-Out (NLI-LOO) identifies aspect-related sentences utilizing an NLI model in a more general setting without using seed words and outperforms existing approaches by 1.2 ROUGE-L points on Space for aspect-specific opinion summarization and remains competitive on other metrics.</abstract>
      <url hash="710841d3">2023.findings-eacl.142</url>
      <bibkey>shen-etal-2023-simple</bibkey>
    </paper>
    <paper id="143">
      <title>Towards Fine-tuning Pre-trained Language Models with Integer Forward and Backward Propagation</title>
      <author><first>Mohammadreza</first><last>Tayaranian Hosseini</last><affiliation>Huawei Technologies Canada</affiliation></author>
      <author><first>Alireza</first><last>Ghaffari</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Marzieh S.</first><last>Tahaei</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last><affiliation>Noah’s Ark Lab Huawei</affiliation></author>
      <author><first>Masoud</first><last>Asgharian</last><affiliation>McGill Universirty</affiliation></author>
      <author><first>Vahid</first><last>Partovi Nia</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <pages>1912-1921</pages>
      <abstract>The large number of parameters of some prominent language models, such as BERT, makes their fine-tuning on downstream tasks computationally intensive and energy hungry.Previously researchers were focused on lower bit-width integer data types for the forward propagation of language models to save memory and computation.As for the backward propagation, however, only 16-bit floating-point data type has been used for the fine-tuning of BERT.In this work, we use integer arithmetic for both forward and back propagation in the fine-tuning of BERT.We study the effects of varying the integer bit-width on the model’s metric performance.Our integer fine-tuning uses integer arithmetic to perform forward propagation and gradient computation of linear, layer-norm, and embedding layers of BERT.We fine-tune BERT using our integer training method on SQuAD v1.1 and SQuAD v2., and GLUE benchmark.We demonstrate that metric performance of fine-tuning 16-bit integer BERT matches both 16-bit and 32-bit floating-point baselines.Furthermore, using the faster and more memory efficient 8-bit integer data type, integer fine-tuning of BERT loses an average of 3.1 points compared to the FP32 baseline.</abstract>
      <url hash="72105025">2023.findings-eacl.143</url>
      <bibkey>tayaranian-hosseini-etal-2023-towards</bibkey>
    </paper>
    <paper id="144">
      <title>Data Augmentation for Radiology Report Simplification</title>
      <author><first>Ziyu</first><last>Yang</last><affiliation>Temple University</affiliation></author>
      <author><first>Santhosh</first><last>Cherian</last><affiliation>Temple University Hospital</affiliation></author>
      <author><first>Slobodan</first><last>Vucetic</last><affiliation>Temple University</affiliation></author>
      <pages>1922-1932</pages>
      <abstract>This work considers the development of a text simplification model to help patients better understand their radiology reports. This paper proposes a data augmentation approach to address the data scarcity issue caused by the high cost of manual simplification. It prompts a large foundational pre-trained language model to generate simplifications of unlabeled radiology sentences. In addition, it uses paraphrasing of labeled radiology sentences. Experimental results show that the proposed data augmentation approach enables the training of a significantly more accurate simplification model than the baselines.</abstract>
      <url hash="1f70841a">2023.findings-eacl.144</url>
      <attachment type="dataset" hash="70c758d7">2023.findings-eacl.144.dataset.zip</attachment>
      <bibkey>yang-etal-2023-data</bibkey>
    </paper>
    <paper id="145">
      <title>Embedding Recycling for Language Models</title>
      <author><first>Jon</first><last>Saad-Falcon</last><affiliation>Stanford University</affiliation></author>
      <author><first>Amanpreet</first><last>Singh</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Mike</first><last>D’Arcy</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Doug</first><last>Downey</last><affiliation>Allen Institute for AI, Northwestern University</affiliation></author>
      <pages>1933-1953</pages>
      <abstract>Real-world applications of neural language models often involve running many different models over the same corpus. The high computational cost of these runs has led to interest in techniques that can reuse the contextualized embeddings produced in previous runs to speed training and inference of future ones. We refer to this approach as embedding recycling (ER). While multiple ER techniques have been proposed, their practical effectiveness is still unknown because existing evaluations consider very few models and do not adequately account for overhead costs. We perform an extensive evaluation of ER across eight different models (17 to 900 million parameters) and fourteen tasks in English. We show how a simple ER technique that caches activations from an intermediate layer of a pretrained model, and learns task-specific adapters on the later layers, is broadly effective. For the best-performing baseline in our experiments (DeBERTa-v2 XL), adding a precomputed cache results in a 90% speedup during training and 87-91% speedup for inference, with negligible impact on accuracy. Our analysis reveals important areas of future work.</abstract>
      <url hash="5ca919c9">2023.findings-eacl.145</url>
      <bibkey>saad-falcon-etal-2023-embedding</bibkey>
    </paper>
    <paper id="146">
      <title>Trained on 100 million words and still in shape: <fixed-case>BERT</fixed-case> meets <fixed-case>B</fixed-case>ritish <fixed-case>N</fixed-case>ational <fixed-case>C</fixed-case>orpus</title>
      <author><first>David</first><last>Samuel</last><affiliation>University of Oslo, Language Technology Group</affiliation></author>
      <author><first>Andrey</first><last>Kutuzov</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Lilja</first><last>Øvrelid</last><affiliation>Dept of Informatics, University of Oslo</affiliation></author>
      <author><first>Erik</first><last>Velldal</last><affiliation>University of Oslo</affiliation></author>
      <pages>1954-1974</pages>
      <abstract>While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source – the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.</abstract>
      <url hash="6ce1ab43">2023.findings-eacl.146</url>
      <bibkey>samuel-etal-2023-trained</bibkey>
    </paper>
    <paper id="147">
      <title>Generating Synthetic Speech from <fixed-case>S</fixed-case>poken<fixed-case>V</fixed-case>ocab for Speech Translation</title>
      <author><first>Jinming</first><last>Zhao</last><affiliation>Dept of Data Science and AI, Faculty of IT, Monash University</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University</affiliation></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University</affiliation></author>
      <pages>1975-1981</pages>
      <abstract>Training end-to-end speech translation (ST) systems requires sufficiently large-scale data, which is unavailable for most language pairs and domains. One practical solution to the data scarcity issue is to convert text-based machine translation (MT) data to ST data via text-to-speech (TTS) systems.Yet, using TTS systems can be tedious and slow. In this work, we propose SpokenVocab, a simple, scalable and effective data augmentation technique to convert MT data to ST data on-the-fly. The idea is to retrieve and stitch audio snippets, corresponding to words in an MT sentence, from a spoken vocabulary bank. Our experiments on multiple language pairs show that stitched speech helps to improve translation quality by an average of 1.83 BLEU score, while performing equally well as TTS-generated speech in improving translation quality. We also showcase how SpokenVocab can be applied in code-switching ST for which often no TTS systems exit.</abstract>
      <url hash="f319dd4c">2023.findings-eacl.147</url>
      <bibkey>zhao-etal-2023-generating</bibkey>
    </paper>
    <paper id="148">
      <title>Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</title>
      <author><first>Albert</first><last>Lu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Hongxin</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yanzhe</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Xuezhi</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>1982-2008</pages>
      <abstract>The limits of open-ended generative models are unclear, yet increasingly important. What causes them to succeed and what causes them to fail? In this paper, we take a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models. We present a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic. These constraint types are categorized into a set of well-defined constraints that are analyzable by a single prompt. We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model’s generative failures. We also show the generalizability of our proposed method on other large models like BLOOM and OPT. Our results and our in-context mitigation strategies reveal open challenges for future research.</abstract>
      <url hash="6f809fd3">2023.findings-eacl.148</url>
      <bibkey>lu-etal-2023-bounding</bibkey>
    </paper>
    <paper id="149">
      <title>Learning to Retrieve Engaging Follow-Up Queries</title>
      <author><first>Christopher</first><last>Richardson</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Sudipta</first><last>Kar</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Anjishnu</first><last>Kumar</last><affiliation>Amazon Alexa</affiliation></author>
      <author><first>Anand</first><last>Ramachandran</last><affiliation>Amazon</affiliation></author>
      <author><first>Zeynab</first><last>Raeesy</last><affiliation>Amazon</affiliation></author>
      <author><first>Omar</first><last>Khan</last><affiliation>Amazon</affiliation></author>
      <author><first>Abhinav</first><last>Sethy</last><affiliation>Amazon</affiliation></author>
      <pages>2009-2016</pages>
      <abstract>Open domain conversational agents can answer a broad range of targeted queries. However, the sequential nature of interaction with these systems makes knowledge exploration a lengthy task which burdens the user with asking a chain of well phrased questions. In this paper, we present a retrieval based system and associated dataset for predicting the next questions that the user might have. Such a system can proactively assist users in knowledge exploration leading to a more engaging dialog. The retrieval system is trained on a dataset called the Follow-up Query Bank (FQ-Bank). FQ-Bank contains ~14K multi-turn information-seeking conversations with a valid follow-up question and a set of invalid candidates. The invalid candidates are generated to simulate various syntactic and semantic confounders such as paraphrases, partial entity match, irrelevant entity, and ASR errors. We use confounder specific techniques to simulate these negative examples on the OR-QuAC dataset. Then, we train ranking models on FQ-Bank and present results comparing supervised and unsupervised approaches. The results suggest that we can retrieve the valid follow-ups by ranking them in higher positions compared to confounders, but further knowledge grounding can improve ranking performance.FQ-Bank is publicly available at https://github.com/amazon-science/fq-bank.</abstract>
      <url hash="45df0353">2023.findings-eacl.149</url>
      <bibkey>richardson-etal-2023-learning</bibkey>
    </paper>
    <paper id="150">
      <title>Selective-<fixed-case>LAMA</fixed-case>: Selective Prediction for Confidence-Aware Evaluation of Language Models</title>
      <author><first>Hiyori</first><last>Yoshikawa</last><affiliation>Fujitsu Limited</affiliation></author>
      <author><first>Naoaki</first><last>Okazaki</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <pages>2017-2028</pages>
      <abstract>Recent studies have suggested that neural language models learn and store a large amount of facts and commonsense knowledge from training data. The ability of language models to restore such knowledge is often evaluated via zero-shot cloze-style QA tasks. However, such evaluations rely only on prediction accuracy without punishing the systems for their mistakes, e.g., simply guessing or hallucinating likely answers. Selective prediction is a more informative evaluation framework that takes the confidence of predictions into account. Under the selective prediction setting, a model is evaluated not only by the number of correct predictions, but also by the ability to filter out dubious predictions by estimating the confidence of individual predictions. Such confidence-aware evaluation is crucial for determining whether to trust zero-shot predictions of language models. In this paper, we apply the selective prediction setting to an existing benchmark, LAMA probe, and conduct extensive experiments with recent neural language models and different confidence functions. We empirically show that our Selective-LAMA evaluation is more robust to the effect of simple guesses than the conventional accuracy-based evaluation.Our evaluation reveals the importance of the choice of confidence functions by showing that simply relying on token probabilities is not always the best choice.Further analysis shows that various confidence functions exhibit different preferences over predicted tokens for a given context.</abstract>
      <url hash="270b8411">2023.findings-eacl.150</url>
      <attachment type="software" hash="ef302ab8">2023.findings-eacl.150.software.zip</attachment>
      <bibkey>yoshikawa-okazaki-2023-selective</bibkey>
    </paper>
    <paper id="151">
      <title>Multi-View Source Ablation for Faithful Summarization</title>
      <author><first>Shuyang</first><last>Cao</last><affiliation>Univerisity of Michigan</affiliation></author>
      <author><first>Liang</first><last>Ma</last><affiliation>Dataminr</affiliation></author>
      <author><first>Di</first><last>Lu</last><affiliation>Dataminr</affiliation></author>
      <author><first>Robert L</first><last>Logan IV</last><affiliation>Dataminr</affiliation></author>
      <author><first>Joel</first><last>Tetreault</last><affiliation>Dataminr</affiliation></author>
      <author><first>Alejandro</first><last>Jaimes</last><affiliation>Dataminr</affiliation></author>
      <pages>2029-2047</pages>
      <abstract>In this paper, we present MuFaSSa (Multi-view Faithfulness Scoring via Source Ablation), a metric for evaluating faithfulness of abstractive summaries, and for guiding training of more faithful summarizers. For evaluation, MuFaSSa employs different strategies (e.g., masking entity mentions) to first remove information from the source document to form multiple ablated views. Then, the faithfulness level of each token in a generated summary is measured by the difference between the token generation probabilities when given the original document and the ablated document as inputs to trained summarizers. For training, MuFaSSa uses a novel word truncation objective that drops unfaithful tokens located by MuFaSSa in both the decoder input and output. Alignments with human-annotated faithfulness labels on AggreFact show that MuFaSSa is comparable to or better than existing metrics built on classifiers or QA models pre-trained on other tasks. In experiments on summarization with XSum and CNN/DailyMail, models trained with word truncation using MuFaSSa outperform competitive methods according to both automatic faithfulness metrics and human assessments.</abstract>
      <url hash="77e28880">2023.findings-eacl.151</url>
      <bibkey>cao-etal-2023-multi</bibkey>
    </paper>
    <paper id="152">
      <title>Mining Effective Features Using Quantum Entropy for Humor Recognition</title>
      <author id="yang-liu-tianjin"><first>Yang</first><last>Liu</last><affiliation>College of Intelligence and Computing, Tianjin University, Tianjin, China</affiliation></author>
      <author><first>Yuexian</first><last>Hou</last><affiliation>College of Intelligence and Computing, Tianjin University, Tianjin, China</affiliation></author>
      <pages>2048-2053</pages>
      <abstract>Humor recognition has been extensively studied with different methods in the past years. However, existing studies on humor recognition do not understand the mechanisms that generate humor. In this paper, inspired by the incongruity theory, any joke can be divided into two components (the setup and the punchline). Both components have multiple possible semantics, and there is an incongruous relationship between them. We use density matrices to represent the semantic uncertainty of the setup and the punchline, respectively, and design QE-Uncertainty and QE-Incongruity with the help of quantum entropy as features for humor recognition. The experimental results on the SemEval2021 Task 7 dataset show that the proposed features are more effective than the baselines for recognizing humorous and non-humorous texts.</abstract>
      <url hash="40e55e09">2023.findings-eacl.152</url>
      <attachment type="dataset" hash="e117da59">2023.findings-eacl.152.dataset.rar</attachment>
      <bibkey>liu-hou-2023-mining</bibkey>
    </paper>
    <paper id="153">
      <title><fixed-case>A</fixed-case>dapter<fixed-case>S</fixed-case>oup: Weight Averaging to Improve Generalization of Pretrained Language Models</title>
      <author><first>Alexandra</first><last>Chronopoulou</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Matthew</first><last>Peters</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Jesse</first><last>Dodge</last><affiliation>Allen Institute for AI</affiliation></author>
      <pages>2054-2063</pages>
      <abstract>Pretrained language models (PLMs) are trained on massive corpora, but often need to specialize to specific domains. A parameter-efficient adaptation method suggests training an adapter for each domain on the task of language modeling. This leads to good in-domain scores but can be impractical for domain- or resource-restricted settings. A solution is to use a related-domain adapter for the novel domain at test time. In this paper, we introduce AdapterSoup, an approach that performs weight-space averaging of adapters trained on different domains. Our approach is embarrassingly parallel: first, we train a set of domain-specific adapters; then, for each novel domain, we determine which adapters should be averaged at test time. We present extensive experiments showing that AdapterSoup consistently improves performance to new domains without extra training. We also explore weight averaging of adapters trained on the same domain with different hyper-parameters, and show that it preserves the performance of a PLM on new domains while obtaining strong in-domain results. We explore various approaches for choosing which adapters to combine, such as text clustering and semantic similarity. We find that using clustering leads to the most competitive results on novel domains.</abstract>
      <url hash="4e92467b">2023.findings-eacl.153</url>
      <bibkey>chronopoulou-etal-2023-adaptersoup</bibkey>
    </paper>
    <paper id="154">
      <title>Towards End-to-End Open Conversational Machine Reading</title>
      <author><first>Sizhe</first><last>Zhou</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Siru</first><last>Ouyang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>2064-2076</pages>
      <abstract>In open-retrieval conversational machine reading (OR-CMR) task, machines are required to do multi-turn question answering given dialogue history and a textual knowledge base. Existing works generally utilize two independent modules to approach this problem’s two successive sub-tasks: first with a hard-label decision making and second with a question generation aided by various entailment reasoning methods. Such usual cascaded modeling is vulnerable to error propagation and prevents the two sub-tasks from being consistently optimized. In this work, we instead model OR-CMR as a unified text-to-text task in a fully end-to-end style. Experiments on the ShARC and OR-ShARC dataset show the effectiveness of our proposed end-to-end framework on both sub-tasks by a large margin, achieving new state-of-the-art results. Further ablation studies support that our framework can generalize to different backbone models.</abstract>
      <url hash="74f2a5b6">2023.findings-eacl.154</url>
      <bibkey>zhou-etal-2023-towards</bibkey>
    </paper>
    <paper id="155">
      <title>Generative Knowledge Selection for Knowledge-Grounded Dialogues</title>
      <author><first>Weiwei</first><last>Sun</last><affiliation>Shandong University</affiliation></author>
      <author><first>Pengjie</first><last>Ren</last><affiliation>Shandong University</affiliation></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Shandong University</affiliation></author>
      <pages>2077-2088</pages>
      <abstract>Knowledge selection is the key in knowledge-grounded dialogues (KGD), which aims to select an appropriate knowledge snippet to be used in the utterance based on dialogue history. Previous studies mainly employ the classification approach to classify each candidate snippet as “relevant” or “irrelevant” independently. However, such approaches neglect the interactions between snippets, leading to difficulties in inferring the meaning of snippets. Moreover, they lack modeling of the discourse structure of dialogue-knowledge interactions. We propose a simple yet effective generative approach for knowledge selection, called GenKS. GenKS learns to select snippets by generating their identifiers with a sequence-to-sequence model. GenKS therefore captures intra-knowledge interaction inherently through attention mechanisms. Meanwhile, we devise a hyperlink mechanism to model the dialogue-knowledge interactions explicitly. We conduct experiments on three benchmark datasets, and verify GenKS achieves the best results on both knowledge selection and response generation.</abstract>
      <url hash="93a1f155">2023.findings-eacl.155</url>
      <bibkey>sun-etal-2023-generative</bibkey>
    </paper>
    <paper id="156">
      <title>Evaluating the Tradeoff Between Abstractiveness and Factuality in Abstractive Summarization</title>
      <author><first>Markus</first><last>Dreyer</last><affiliation>Amazon.com</affiliation></author>
      <author><first>Mengwen</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Feng</first><last>Nan</last><affiliation>Aws Ai</affiliation></author>
      <author><first>Sandeep</first><last>Atluri</last><affiliation>Amazon</affiliation></author>
      <author><first>Sujith</first><last>Ravi</last><affiliation>SliceX AI</affiliation></author>
      <pages>2089-2105</pages>
      <abstract>Neural models for abstractive summarization tend to generate output that is fluent and well-formed but lacks semantic faithfulness, or factuality, with respect to the input documents. In this paper, we analyze the tradeoff between abstractiveness and factuality of generated summaries across multiple datasets and models, using extensive human evaluations of factuality. In our analysis, we visualize the rates of change in factuality as we gradually increase abstractiveness using a decoding constraint, and we observe that, while increased abstractiveness generally leads to a drop in factuality, the rate of factuality decay depends on factors such as the data that the system was trained on. We introduce two datasets with human factuality judgements; one containing 10.2k generated summaries with systematically varied degrees of abstractiveness; the other containing 4.2k summaries from five different summarization models. We propose new factuality metrics that adjust for the degree of abstractiveness, and we use them to compare the abstractiveness-adjusted factuality of previous summarization works, providing baselines for future work.</abstract>
      <url hash="be27149d">2023.findings-eacl.156</url>
      <bibkey>dreyer-etal-2023-evaluating</bibkey>
    </paper>
    <paper id="157">
      <title>Fairness in Language Models Beyond <fixed-case>E</fixed-case>nglish: Gaps and Challenges</title>
      <author><first>Krithika</first><last>Ramesh</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Microsoft</affiliation></author>
      <pages>2106-2119</pages>
      <abstract>With language models becoming increasingly ubiquitous, it has become essential to address their inequitable treatment of diverse demographic groups and factors. Most research on evaluating and mitigating fairness harms has been concentrated on English, while multilingual models and non-English languages have received comparatively little attention. In this paper, we survey different aspects of fairness in languages beyond English and multilingual contexts. This paper presents a survey of fairness in multilingual and non-English contexts, highlighting the shortcomings of current research and the difficulties faced by methods designed for English. We contend that the multitude of diverse cultures and languages across the world makes it infeasible to achieve comprehensive coverage in terms of constructing fairness datasets. Thus, the measurement and mitigation of biases must evolve beyond the current dataset-driven practices that are narrowly focused on specific dimensions and types of biases and, therefore, impossible to scale across languages and cultures.</abstract>
      <url hash="2844454f">2023.findings-eacl.157</url>
      <bibkey>ramesh-etal-2023-fairness</bibkey>
    </paper>
    <paper id="158">
      <title>Global-Local Modeling with Prompt-Based Knowledge Enhancement for Emotion Inference in Conversation</title>
      <author><first>Renxi</first><last>Wang</last><affiliation>Northeastern University, China</affiliation></author>
      <author><first>Shi</first><last>Feng</last><affiliation>Northeastern University, China</affiliation></author>
      <pages>2120-2127</pages>
      <abstract>The ability to recognize emotions in conversations is necessary and important for the online chatbot to do tasks such as empathetic response generation and emotional support. Present researches mainly focus on recognizing emotions through a speaker’s utterance, while research on emotion inference predicts emotions of addressees through previous utterances. Because of the lack of the addressee’s utterance, emotion inference is more challenging than emotion recognition. In this paper, we propose a global-local modeling method based on recurrent neural networks (RNN) and pre-trained language models (PLM) to do emotion inference, which utilizes the sequence modeling ability of RNNs and abundant knowledge from PLMs. Moreover, we take the whole dialogue history as input of PLM to generate knowledge by in-context learning. Experimental results show that our model with knoledge enhancement achieves state-of-the-art performance on all three datasets.</abstract>
      <url hash="13b20ab3">2023.findings-eacl.158</url>
      <attachment type="dataset" hash="b7ae075f">2023.findings-eacl.158.dataset.zip</attachment>
      <bibkey>wang-feng-2023-global</bibkey>
    </paper>
    <paper id="159">
      <title>Headline Token-based Discriminative Learning for Subheading Generation in News Article</title>
      <author><first>Joonwon</first><last>Jang</last><affiliation>Sejong University</affiliation></author>
      <author><first>Misuk</first><last>Kim</last><affiliation>Sejong University</affiliation></author>
      <pages>2128-2135</pages>
      <abstract>The news subheading summarizes an article’s contents in several sentences to support the headline limited to solely conveying the main contents. So, it is necessary to generate compelling news subheadings in consideration of the structural characteristics of the news. In this paper, we propose a subheading generation model using topical headline information. We introduce a discriminative learning method that utilizes the prediction result of masked headline tokens. Experiments show that the proposed model is effective and outperforms the comparative models on three news datasets written in two languages. We also show that our model performs robustly on a small dataset and various masking ratios. Qualitative analysis and human evaluations also show that the overall quality of generated subheadings improved over the comparative models.</abstract>
      <url hash="1e09fa7d">2023.findings-eacl.159</url>
      <attachment type="dataset" hash="ef34f709">2023.findings-eacl.159.dataset.zip</attachment>
      <bibkey>jang-kim-2023-headline</bibkey>
    </paper>
    <paper id="160">
      <title>Decipherment as Regression: Solving Historical Substitution Ciphers by Learning Symbol Recurrence Relations</title>
      <author><first>Nishant</first><last>Kambhatla</last><affiliation>Simon Fraser University</affiliation></author>
      <author><first>Logan</first><last>Born</last><affiliation>Simon Fraser University</affiliation></author>
      <author><first>Anoop</first><last>Sarkar</last><affiliation>Simon Fraser University</affiliation></author>
      <pages>2136-2152</pages>
      <abstract>Solving substitution ciphers involves mapping sequences of cipher symbols to fluent text in a target language. This has conventionally been formulated as a search problem, to find the decipherment key using a character-level language model to constrain the search space. This work instead frames decipherment as a sequence prediction task, using a Transformer-based causal language model to learn recurrences between characters in a ciphertext. We introduce a novel technique for transcribing arbitrary substitution ciphers into a common recurrence encoding. By leveraging this technique, we (i) create a large synthetic dataset of homophonic ciphers using random keys, and (ii) train a decipherment model that predicts the plaintext sequence given a recurrence-encoded ciphertext. Our method achieves strong results on synthetic 1:1 and homophonic ciphers, and cracks several real historic homophonic ciphers. Our analysis shows that the model learns recurrence relations between cipher symbols and recovers decipherment keys in its self-attention.</abstract>
      <url hash="74cb7b46">2023.findings-eacl.160</url>
      <bibkey>kambhatla-etal-2023-decipherment</bibkey>
    </paper>
    <paper id="161">
      <title>A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models</title>
      <author><first>Mingyang</first><last>Song</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yi</first><last>Feng</last><affiliation>BeiJing JiaoTong University</affiliation></author>
      <author><first>Liping</first><last>Jing</last><affiliation>Beijing Jiaotong Univesity</affiliation></author>
      <pages>2153-2164</pages>
      <abstract>Keyphrase Extraction (KE) is a critical component in Natural Language Processing (NLP) systems for selecting a set of phrases from the document that could summarize the important information discussed in the document. Typically, a keyphrase extraction system can significantly accelerate the speed of information retrieval and help people get first-hand information from a long document quickly and accurately. Specifically, keyphrases are capable of providing semantic metadata characterizing documents and producing an overview of the content of a document. In this paper, we introduce keyphrase extraction, present a review of the recent studies based on pre-trained language models, offer interesting insights on the different approaches, highlight open issues, and give a comparative experimental study of popular supervised as well as unsupervised techniques on several datasets. To encourage more instantiations, we release the related files mentioned in this paper.</abstract>
      <url hash="79dec4d8">2023.findings-eacl.161</url>
      <bibkey>song-etal-2023-survey</bibkey>
    </paper>
    <paper id="162">
      <title>Prompting for explanations improves Adversarial <fixed-case>NLI</fixed-case>. Is this true? {Yes} it is {true} because {it weakens superficial cues}</title>
      <author><first>Pride</first><last>Kavumba</last><affiliation>Tohoku University / RIKEN AIP</affiliation></author>
      <author><first>Ana</first><last>Brassard</last><affiliation>RIKEN AIP / Tohoku University</affiliation></author>
      <author><first>Benjamin</first><last>Heinzerling</last><affiliation>RIKEN AIP &amp; Tohoku University</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Tohoku University / Riken</affiliation></author>
      <pages>2165-2180</pages>
      <abstract>Explanation prompts ask language models to not only assign a particular label to a giveninput, such as true, entailment, or contradiction in the case of natural language inference but also to generate a free-text explanation that supports this label. For example: “This is {label} because {explanation}.” While this type of prompt was originally introduced with the aim of improving model interpretability, we showhere that explanation prompts also improve robustness to adversarial perturbations in naturallanguage inference benchmarks. Compared to prompting for labels only, explanation prompting consistently yields stronger performance on adversarial benchmarks, outperforming the state of the art on Adversarial Natural Language Inference, Counterfactually-Augmented Natural Language Inference, and SNLI-Hard datasets. We argue that the increase in robustness is due to the fact that prompting for explanations weakens superficial cues. Specifically, single tokens that are highly predictive of the correct answer in the label-only setting become uninformative when the model also has to generate explanations.</abstract>
      <url hash="aa869a9e">2023.findings-eacl.162</url>
      <bibkey>kavumba-etal-2023-prompting</bibkey>
    </paper>
    <paper id="163">
      <title><fixed-case>J</fixed-case>ob<fixed-case>XMLC</fixed-case>: <fixed-case>EX</fixed-case>treme Multi-Label Classification of Job Skills with Graph Neural Networks</title>
      <author><first>Nidhi</first><last>Goyal</last><affiliation>Iiit</affiliation></author>
      <author><first>Jushaan</first><last>Kalra</last><affiliation>PreCog</affiliation></author>
      <author><first>Charu</first><last>Sharma</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Raghava</first><last>Mutharaju</last><affiliation>IIIT-Delhi</affiliation></author>
      <author><first>Niharika</first><last>Sachdeva</last><affiliation>Infoedge</affiliation></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last><affiliation>IIIT Hyderabad</affiliation></author>
      <pages>2181-2191</pages>
      <abstract>Writing a good job description is an important step in the online recruitment process to hire the best candidates. Most recruiters forget to include some relevant skills in the job description. These missing skills affect the performance of recruitment tasks such as job suggestions, job search, candidate recommendations, etc. Existing approaches are limited to contextual modelling, do not exploit inter-relational structures like job-job and job-skill relationships, and are not scalable. In this paper, we exploit these structural relationships using a graph-based approach. We propose a novel skill prediction framework called JobXMLC, which uses graph neural networks with skill attention to predict missing skills using job descriptions. JobXMLC enables joint learning over a job-skill graph consisting of 22.8K entities (jobs and skills) and 650K relationships. We experiment with real-world recruitment datasets to evaluate our proposed approach. We train JobXMLC on 20,298 job descriptions and 2,548 skills within 30 minutes on a single GPU machine. JobXMLC outperforms the state-of-the-art approaches by 6% in precision and 3% in recall. JobXMLC is 18X faster for training task and up to 634X faster in skill prediction on benchmark datasets enabling JobXMLC to scale up on larger datasets.</abstract>
      <url hash="67881392">2023.findings-eacl.163</url>
      <bibkey>goyal-etal-2023-jobxmlc</bibkey>
    </paper>
    <paper id="164">
      <title><fixed-case>V</fixed-case>i<fixed-case>LPA</fixed-case>ct: A Benchmark for Compositional Generalization on Multimodal Human Activities</title>
      <author><first>Terry Yue</first><last>Zhuo</last><affiliation>CSIRO’s Data61 and Monash University</affiliation></author>
      <author><first>Yaqing</first><last>Liao</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Yuecheng</first><last>Lei</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Gerard</first><last>de Melo</last><affiliation>HPI/University of Potsdam</affiliation></author>
      <author><first>Xiaojun</first><last>Chang</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Yazhou</first><last>Ren</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Zenglin</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>2192-2207</pages>
      <abstract>We introduce {dataset, a novel vision-language benchmark for human activity planning. It is designed for a task where embodied AI agents can reason and forecast future actions of humans based on video clips about their initial activities and intents in text. The dataset consists of 2.9k videos from {charades extended with intents via crowdsourcing, a multi-choice question test set, and four strong baselines. One of the baselines implements a neurosymbolic approach based on a multi-modal knowledge base (MKB), while the other ones are deep generative models adapted from recent state-of-the-art (SOTA) methods. According to our extensive experiments, the key challenges are compositional generalization and effective use of information from both modalities.</abstract>
      <url hash="b613eda2">2023.findings-eacl.164</url>
      <bibkey>zhuo-etal-2023-vilpact</bibkey>
    </paper>
    <paper id="165">
      <title>Grammatical Error Correction through Round-Trip Machine Translation</title>
      <author><first>Yova</first><last>Kementchedjhieva</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Anders</first><last>Søgaard</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>2208-2215</pages>
      <abstract>Machine translation (MT) operates on the premise of an interlingua which abstracts away from surface form while preserving meaning. A decade ago the idea of using round-trip MT to guide grammatical error correction was proposed as a way to abstract away from potential errors in surface forms (Madnani _et_ _al_., 2012). At the time, it did not pan out due to the low quality of MT systems of the day. Today much stronger MT systems are available so we re-evaluate this idea across five languages and models of various sizes. We find that for extra large models input augmentation through round-trip MT has little to no effect. For more ‘workable’ model sizes, however, it yields consistent improvements, sometimes bringing the performance of a _base_ or _large_ model up to that of a _large_ or _xl_ model, respectively. The round-trip translation comes at a computational cost though, so one would have to determine whether to opt for a larger model or for input augmentation on a case-by-case basis.</abstract>
      <url hash="2d237fc0">2023.findings-eacl.165</url>
      <bibkey>kementchedjhieva-sogaard-2023-grammatical</bibkey>
    </paper>
    <paper id="166">
      <title>Does Masked Language Model Pre-training with Artificial Data Improve Low-resource Neural Machine Translation?</title>
      <author><first>Hiroto</first><last>Tamura</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Tosho</first><last>Hirasawa</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Hwichan</first><last>Kim</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Mamoru</first><last>Komachi</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <pages>2216-2225</pages>
      <abstract>Pre-training masked language models (MLMs) with artificial data has been proven beneficial for several natural language processing tasks such as natural language understanding and summarization; however, it has been less explored for neural machine translation (NMT).A previous study revealed the benefit of transfer learning for NMT in a limited setup, which differs from MLM.In this study, we prepared two kinds of artificial data and compared the translation performance of NMT when pre-trained with MLM.In addition to the random sequences, we created artificial data mimicking token frequency information from the real world.Our results showed that pre-training the models with artificial data by MLM improves translation performance in low-resource situations.Additionally, we found that pre-training on artificial data created considering token frequency information facilitates improved performance.</abstract>
      <url hash="43771cb6">2023.findings-eacl.166</url>
      <bibkey>tamura-etal-2023-masked</bibkey>
    </paper>
    <paper id="167">
      <title>Performance and Risk Trade-offs for Multi-word Text Prediction at Scale</title>
      <author><first>Aniket</first><last>Vashishtha</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>S Sai</first><last>Prasad</last><affiliation>Microsoft</affiliation></author>
      <author><first>Payal</first><last>Bajaj</last><affiliation>Microsoft</affiliation></author>
      <author><first>Vishrav</first><last>Chaudhary</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kate</first><last>Cook</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sandipan</first><last>Dandapat</last><affiliation>Microsoft India</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Microsoft</affiliation></author>
      <pages>2226-2242</pages>
      <abstract>Large Language Models such as GPT-3 are well-suited for text prediction tasks, which can help and delight users during text composition. LLMs are known to generate ethically inappropriate predictions even for seemingly innocuous contexts. Toxicity detection followed by filtering is a common strategy for mitigating the harm from such predictions. However, as we shall argue in this paper, in the context of text prediction, it is not sufficient to detect and filter toxic content. One also needs to ensure factual correctness and group-level fairness of the predictions; failing to do so can make the system ineffective and nonsensical at best, and unfair and detrimental to the users at worst. We discuss the gaps and challenges of toxicity detection approaches - from blocklist-based approaches to sophisticated state-of-the-art neural classifiers - by evaluating them on the text prediction task for English against a manually crafted CheckList of harms targeted at different groups and different levels of severity.</abstract>
      <url hash="f7d08d89">2023.findings-eacl.167</url>
      <bibkey>vashishtha-etal-2023-performance</bibkey>
    </paper>
    <paper id="168">
      <title>Searching for Better Database Queries in the Outputs of Semantic Parsers</title>
      <author><first>Anton</first><last>Osokin</last><affiliation>HSE University, Yandex</affiliation></author>
      <author><first>Irina</first><last>Saparina</last><affiliation>HSE University, Yandex</affiliation></author>
      <author><first>Ramil</first><last>Yarullin</last><affiliation>HSE University, Yandex</affiliation></author>
      <pages>2243-2256</pages>
      <abstract>The task of generating a database query from a question in natural language suffers from ambiguity and insufficiently precise description of the goal. The problem is amplified when the system needs to generalize to databases unseen at training. In this paper, we consider the case when, at the test time, the system has access to an external criterion that evaluates the generated queries. The criterion can vary from checking that a query executes without errors to verifying the query on a set of tests. In this setting, we augment neural autoregressive models with a search algorithm that looks for a query satisfying the criterion. We apply our approach to the state-of-the-art semantic parsers and report that it allows us to find many queries passing all the tests on different datasets.</abstract>
      <url hash="cb389ec2">2023.findings-eacl.168</url>
      <bibkey>osokin-etal-2023-searching</bibkey>
    </paper>
    <paper id="169">
      <title>Style-Aware Contrastive Learning for Multi-Style Image Captioning</title>
      <author><first>Yucheng</first><last>Zhou</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Guodong</first><last>Long</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>2257-2267</pages>
      <abstract>Existing multi-style image captioning methods show promising results in generating a caption with accurate visual content and desired linguistic style. However, existing methods overlook the relationship between linguistic style and visual content. To overcome this drawback, we propose style-aware contrastive learning for multi-style image captioning. First, we present a style-aware visual encoder with contrastive learning to mine potential visual content relevant to style. Moreover, we propose a style-aware triplet contrast objective to distinguish whether the image, style and caption matched. To provide positive and negative samples for contrastive learning, we present three retrieval schemes: object-based retrieval, RoI-based retrieval and triplet-based retrieval, and design a dynamic trade-off function to calculate retrieval scores. Experimental results demonstrate that our approach achieves state-of-the-art performance. In addition, we conduct an extensive analysis to verify the effectiveness of our method.</abstract>
      <url hash="388e6ec8">2023.findings-eacl.169</url>
      <bibkey>zhou-long-2023-style</bibkey>
    </paper>
    <paper id="170">
      <title>Strategize Before Teaching: A Conversational Tutoring System with Pedagogy Self-Distillation</title>
      <author><first>Lingzhi</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>ETH Zurich</affiliation></author>
      <author><first>Xingshan</first><last>Zeng</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong</affiliation></author>
      <pages>2268-2274</pages>
      <abstract>Conversational tutoring systems (CTSs) aim to help students master educational material with natural language interaction in the form of a dialog. CTSs have become a key pillar in educational data mining research. A key challenge in CTSs is to engage the student in the conversation while exposing them to a diverse set of teaching strategies, akin to a human teacher, thereby, helping them learn in the process. Different from previous work that generates responses given the strategies as input, we propose to jointly predict teaching strategies and generate tutor responses accordingly, which fits a more realistic application scenario. We benchmark several competitive models on three dialog tutoring datasets and propose a unified framework that combines teaching response generation and pedagogical strategy prediction, where a self-distillation mechanism is adopted to guide the teaching strategy learning and facilitate tutor response generation. Our experiments and analyses shed light on how teaching strategies affect dialog tutoring.</abstract>
      <url hash="3f366ebe">2023.findings-eacl.170</url>
      <bibkey>wang-etal-2023-strategize</bibkey>
    </paper>
    <paper id="171">
      <title><fixed-case>ICA</fixed-case>-Proto: Iterative Cross Alignment Prototypical Network for Incremental Few-Shot Relation Classification</title>
      <author><first>Wangjie</first><last>Jiang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhihao</first><last>Ye</last><affiliation>Tencent Jarvis</affiliation></author>
      <author><first>Bang</first><last>Liu</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Ruihui</first><last>Zhao</last><affiliation>Bytedance</affiliation></author>
      <author><first>Jianguang</first><last>Zheng</last><affiliation>Tencent Jarvis</affiliation></author>
      <author><first>Mengyao</first><last>Li</last><affiliation>Hunan University</affiliation></author>
      <author><first>Zhiyong</first><last>Li</last><affiliation>Hunan University</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Tsinghua.edu.cn</affiliation></author>
      <author><first>Yefeng</first><last>Zheng</last><affiliation>Tencent</affiliation></author>
      <pages>2275-2284</pages>
      <abstract>In the task of incremental few-shot relation classification, model performance is always limited by the incompatibility between the base feature embedding space and the novel feature embedding space. To tackle the issue, we propose a novel model named ICA-Proto: Iterative Cross Alignment prototypical network. Specifically, we incorporate the query representation into the encoding of novel prototypes and utilize the query-aware prototypes to update the query representation at the same time. Further, we implement the above process iteratively to achieve more interaction. In addition, a novel prototype quadruplet loss is designed to regulate the spatial distributions of embedding space, so as to make it easier for the relation classification. Experimental results on two benchmark datasets demonstrate that ICA-Proto significantly outperforms the state-of-the-art baseline model.</abstract>
      <url hash="c05e2333">2023.findings-eacl.171</url>
      <bibkey>jiang-etal-2023-ica</bibkey>
    </paper>
    <paper id="172">
      <title>A Large-Scale Multilingual Study of Visual Constraints on Linguistic Selection of Descriptions</title>
      <author><first>Uri</first><last>Berger</last><affiliation>The Hebrew University of Jerusalem, University of Melbourne</affiliation></author>
      <author><first>Lea</first><last>Frermann</last><affiliation>Melbourne University</affiliation></author>
      <author><first>Gabriel</first><last>Stanovsky</last><affiliation>The Hebrew University of Jerusalem</affiliation></author>
      <author><first>Omri</first><last>Abend</last><affiliation>The Hebrew University of Jerusalem</affiliation></author>
      <pages>2285-2299</pages>
      <abstract>We present a large, multilingual study into how vision constrains linguistic choice, covering four languages and five linguistic properties, such as verb transitivity or use of numerals. We propose a novel method that leverages existing corpora of images with captions written by native speakers, and apply it to nine corpora, comprising 600k images and 3M captions. We study the relation between visual input and linguistic choices by training classifiers to predict the probability of expressing a property from raw images, and find evidence supporting the claim that linguistic properties are constrained by visual context across languages. We complement this investigation with a corpus study, taking the test case of numerals. Specifically, we use existing annotations (number or type of objects) to investigate the effect of different visual conditions on the use of numeral expressions in captions, and show that similar patterns emerge across languages.Our methods and findings both confirm and extend existing research in the cognitive literature. We additionally discuss possible applications for language generation.</abstract>
      <url hash="a44d9367">2023.findings-eacl.172</url>
      <bibkey>berger-etal-2023-large</bibkey>
    </paper>
    <paper id="173">
      <title>How Much Syntactic Supervision is “Good Enough”?</title>
      <author><first>Hiroshi</first><last>Noji</last><affiliation>LeapMind Inc.</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>2300-2305</pages>
      <abstract>In this paper, we explore how much syntactic supervision is “good enough” to make language models (LMs) more human-like. Specifically, we propose the new method called syntactic ablation, where syntactic LMs, namely Recurrent Neural Network Grammars (RNNGs), are gradually ablated from full syntactic supervision to zero syntactic supervision (≈ unidirectional LSTM) by preserving NP, VP, PP, SBAR nonterminal symbols and the combinations thereof. The 17 ablated grammars are then evaluated via targeted syntactic evaluation on the SyntaxGym benchmark. The results of our syntactic ablation demonstrated that (i) the RNNG with zero syntactic supervision underperformed the RNNGs with some syntactic supervision, (ii) the RNNG with full syntactic supervision underperformed the RNNGs with less syntactic supervision, and (iii) the RNNG with mild syntactic supervision achieved the best performance comparable to the state-of-the-art GPT-2-XL. Those results may suggest that the “good enough” approach to language processing seems to make LMs more human-like.</abstract>
      <url hash="619e82af">2023.findings-eacl.173</url>
      <bibkey>noji-oseki-2023-much</bibkey>
    </paper>
    <paper id="174">
      <title>Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?</title>
      <author><first>Sonal</first><last>Sannigrahi</last><affiliation>Saarland University</affiliation></author>
      <author><first>Josef</first><last>van Genabith</last><affiliation>Dfki</affiliation></author>
      <author><first>Cristina</first><last>España-Bonet</last><affiliation>DFKI GmbH</affiliation></author>
      <pages>2306-2316</pages>
      <abstract>Dense vector representations for textual data are crucial in modern NLP. Word embeddings and sentence embeddings estimated from raw texts are key in achieving state-of-the-art resultsin various tasks requiring semantic understanding. However, obtaining embeddings at the document level is challenging due to computational requirements and lack of appropriate data. Instead, most approaches fall back on computing document embeddings based on sentence representations. Although there exist architectures and models to encode documents fully, they are in general limited to English and few other high-resourced languages. In this work, we provide a systematic comparison of methods to produce document-level representations from sentences based on LASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare input token number truncation, sentence averaging as well as some simple windowing and in some cases new augmented and learnable approaches, on 3 multi- and cross-lingual tasks in 8 languages belonging to 3 different language families. Our task-based extrinsic evaluations show that, independently of the language, a clever combination of sentence embeddings is usually better than encoding the full document as a single unit, even when this is possible. We demonstrate that while a simple sentence average results in a strong baseline for classification tasks, more complex combinations are necessary for semantic tasks</abstract>
      <url hash="2557a6b9">2023.findings-eacl.174</url>
      <bibkey>sannigrahi-etal-2023-best</bibkey>
    </paper>
    <paper id="175">
      <title>Improving User Controlled Table-To-Text Generation Robustness</title>
      <author><first>Hanxu</first><last>Hu</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Yunqing</first><last>Liu</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhongyi</first><last>Yu</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Laura</first><last>Perez-Beltrachini</last><affiliation>School of Informatics, University of Edinburgh</affiliation></author>
      <pages>2317-2324</pages>
      <abstract>In this work we study user controlled table-to-text generation where users explore the content in a table by selecting cells and reading a natural language description thereof automatically produce by a natural language generator. Such generation models usually learn from carefully selected cell combinations (clean cell selections); however, in practice users may select unexpected, redundant, or incoherent cell combinations (noisy cell selections). In experiments, we find that models perform well on test sets coming from the same distribution as the train data but their performance drops when evaluated on realistic noisy user inputs. We propose a fine-tuning regime with additional user-simulated noisy cell selections. Models fine-tuned with the proposed regime gain 4.85 BLEU points on user noisy test cases and 1.4 on clean test cases; and achieve comparable state-of-the-art performance on the ToTTo dataset.</abstract>
      <url hash="da9235d1">2023.findings-eacl.175</url>
      <bibkey>hu-etal-2023-improving</bibkey>
    </paper>
    <paper id="176">
      <title>Better Pre-Training by Reducing Representation Confusion</title>
      <author><first>Haojie</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Mingfei</first><last>Liang</last><affiliation>WeChat Search Application Department, Tencent, China</affiliation></author>
      <author><first>Ruobing</first><last>Xie</last><affiliation>WeChat, Tencent</affiliation></author>
      <author><first>Zhenlong</first><last>Sun</last><affiliation>WeChat Search Application Department, Tencent, China</affiliation></author>
      <author><first>Bo</first><last>Zhang</last><affiliation>WeChat Search Application Department, Tencent, China</affiliation></author>
      <author><first>Leyu</first><last>Lin</last><affiliation>WeChat Search Application Department, Tencent, China</affiliation></author>
      <pages>2325-2336</pages>
      <abstract>In this work, we revisit the Transformer-based pre-trained language models and identify two different types of information confusion in position encoding and model representations, respectively. Firstly, we show that in the relative position encoding, the joint modeling about relative distances and directions brings confusion between two heterogeneous information. It may make the model unable to capture the associative semantics of the same distance and the opposite directions, which in turn affects the performance of downstream tasks. Secondly, we notice the BERT with Mask Language Modeling (MLM) pre-training objective outputs similar token representations (last hidden states of different tokens) and head representations (attention weights{footnote{”attention weights” mainly refer to the dot product between Key and Query in the self-attention module.}of different heads), which may make the diversity of information expressed by different tokens and heads limited.Motivated by the above investigation, we propose two novel techniques to improve pre-trained language models: Decoupled Directional Relative Position (DDRP) encoding and MTH{footnote{MTH is the abbreviation of our proposed MLM with Token Cosine Differentiation (TCD) and Head Cosine Differentiation (HCD) pre-training task. TCD and HCD are described in detail in sec. {ref{sec.introduction}(2) and sec.{ref{sec.CD}.} pre-training objective. DDRP decouples the relative distance features and the directional features in classical relative position encoding. MTH applies two novel auxiliary regularizers besides MLM to enlarge the dissimilarities between (a) last hidden states of different tokens, and (b) attention weights of different heads. These designs allow the model to capture different categories of information more clearly, as a way to alleviate information confusion in representation learning for better optimization.Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of our proposed methods.</abstract>
      <url hash="26b7cc31">2023.findings-eacl.176</url>
      <bibkey>zhang-etal-2023-better</bibkey>
    </paper>
    <paper id="177">
      <title><fixed-case>MAF</fixed-case>i<fixed-case>D</fixed-case>: Moving Average Equipped Fusion-in-Decoder for Question Answering over Tabular and Textual Data</title>
      <author><first>Sung-Min</first><last>Lee</last><affiliation>Jeonbuk National University</affiliation></author>
      <author><first>Eunhwan</first><last>Park</last><affiliation>Jeonbuk National University</affiliation></author>
      <author><first>Daeryong</first><last>Seo</last><affiliation>Naver</affiliation></author>
      <author><first>Donghyeon</first><last>Jeon</last><affiliation>NAVER Search</affiliation></author>
      <author><first>Inho</first><last>Kang</last><affiliation>Naver Search</affiliation></author>
      <author><first>Seung-Hoon</first><last>Na</last><affiliation>Jeonbuk National University</affiliation></author>
      <pages>2337-2344</pages>
      <abstract>Transformer-based models for question answering (QA) over tables and texts confront a “long” hybrid sequence over tabular and textual elements, causing long-range reasoning problems. To handle long-range reasoning, we extensively employ a fusion-in-decoder (FiD) and exponential moving average (EMA), proposing a {underline{M}oving {underline{A}verage Equipped {underline{F}usion-{underline{i}n-{underline{D}ecoder ({textbf{MAFiD}). With FiD as the backbone architecture, MAFiD combines various levels of reasoning: {textit{independent encoding} of homogeneous data and {textit{single-row} and {textit{multi-row heterogeneous reasoning}, using a {textit{gated cross attention layer} to effectively aggregate the three types of representations resulting from various reasonings. Experimental results on HybridQA indicate that MAFiD achieves state-of-the-art performance by increasing exact matching (EM) and F1 by $1.1$ and $1.7$, respectively, on the blind test set.</abstract>
      <url hash="e2a9eb0d">2023.findings-eacl.177</url>
      <bibkey>lee-etal-2023-mafid</bibkey>
    </paper>
    <paper id="178">
      <title>Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis</title>
      <author><first>Akshita</first><last>Jha</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Adithya</first><last>Samavedhi</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Vineeth</first><last>Rakesh</last><affiliation>InterDigital</affiliation></author>
      <author><first>Jaideep</first><last>Chandrashekar</last><affiliation>InterDigital</affiliation></author>
      <author><first>Chandan</first><last>Reddy</last><affiliation>Virginia Tech</affiliation></author>
      <pages>2345-2355</pages>
      <abstract>Recent advances in the area of long document matching have primarily focused on using transformer-based models for long document encoding and matching. There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost – both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time. In this work, we empirically demonstrate the effectiveness of simple neural models (such as feed-forward networks, and CNNs) and simple embeddings (like GloVe, and Paragraph Vector) over transformer-based models on the task of document matching. We show that simple models outperform the more complex BERT-based models while taking significantly less training time, energy, and memory. The simple models are also more robust to variations in document length and text perturbations.</abstract>
      <url hash="69add9e2">2023.findings-eacl.178</url>
      <bibkey>jha-etal-2023-transformer</bibkey>
    </paper>
    <paper id="179">
      <title>Simple and Effective Multi-Token Completion from Masked Language Models</title>
      <author><first>Oren</first><last>Kalinsky</last><affiliation>Amazon</affiliation></author>
      <author><first>Guy</first><last>Kushilevitz</last><affiliation>Amazon</affiliation></author>
      <author><first>Alexander</first><last>Libov</last><affiliation>Amazon</affiliation></author>
      <author><first>Yoav</first><last>Goldberg</last><affiliation>Bar Ilan University</affiliation></author>
      <pages>2356-2369</pages>
      <abstract>Pre-trained neural masked language models are often used for predicting a replacement token for a given sequence position, in a cloze-like task. However, this usage is restricted to predicting a single token, from a relatively small pre-trained vocabulary. Recent Sequence2Sequence pre-trained LMs like T5 do allow predicting multi-token completions, but are more expensive to train and run. We show that pre-trained masked language models can be adapted to produce multi-token completions, with only a modest addition to their parameter count. We propose two simple adaptation approaches, trading parameter counts for accuracy. The first method generates multi-token completions from a conditioned RNN. It has a very low parameter count and achieves competitive results. The second method is even simpler: it adds items corresponding to multi-token units to the output prediction matrix. While being higher in parameter count than the RNN method, it also surpasses current state-of-the-art multi-token completion models, including T5-3B, while being significantly more parameter efficient. We demonstrate that our approach is flexible to different vocabularies and domains and can effectively leverage existing pre-trained models available in different domains. Finally, a human evaluation further validates our results and shows that our solution regularly provides valid completions, as well as reasonable correctness for factual-sentence completions.</abstract>
      <url hash="3a938f13">2023.findings-eacl.179</url>
      <bibkey>kalinsky-etal-2023-simple</bibkey>
    </paper>
    <paper id="180">
      <title>A Survey on Dynamic Neural Networks for Natural Language Processing</title>
      <author><first>Canwen</first><last>Xu</last><affiliation>UC San Diego</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>Ucsd</affiliation></author>
      <pages>2370-2381</pages>
      <abstract>Effectively scaling large Transformer models is a main driver of recent advances in natural language processing. Dynamic neural networks, as an emerging research direction, are capable of scaling up neural networks with sub-linear increases in computation and time by dynamically adjusting their computational path based on the input. Dynamic neural networks could be a promising solution to the growing parameter numbers of pretrained language models, allowing both model pretraining with trillions of parameters and faster inference on mobile devices. In this survey, we summarize the progress of three types of dynamic neural networks in NLP: skimming, mixture of experts, and early exit. We also highlight current challenges in dynamic neural networks and directions for future research.</abstract>
      <url hash="6dfdc695">2023.findings-eacl.180</url>
      <bibkey>xu-mcauley-2023-survey</bibkey>
    </paper>
    <paper id="181">
      <title>Transformers with Learnable Activation Functions</title>
      <author><first>Haishuo</first><last>Fang</last><affiliation>UKP Lab, TU Darmstadt</affiliation></author>
      <author><first>Ji-Ung</first><last>Lee</last><affiliation>UKP, TU Darmstadt</affiliation></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last><affiliation>Department of Computer Science, The University of Sheffield</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <pages>2382-2398</pages>
      <abstract>Activation functions can have a significant impact on reducing the topological complexity of input data and therefore, improving a model’s performance. However, the choice of activation functions is seldom discussed or explored in Transformer-based language models. As a common practice, commonly used activation functions like Gaussian Error Linear Unit (GELU) are chosen beforehand and then remain fixed from pre-training to fine-tuning. In this paper, we investigate the impact of activation functions on Transformer-based models by utilizing rational activation functions (RAFs). In contrast to fixed activation functions (FAF), RAFs are capable of learning the optimal activation functions from data. Our experiments show that the RAF-based Transformer model (RAFT) achieves a better performance than its FAF-based counterpart ({baseline). For instance, we find that RAFT outperforms {baseline on the GLUE benchmark by 5.71 points when using only 100 training examples and by 2.05 points on SQuAD with all available data. Analyzing the shapes of the learned RAFs further unveils that they vary across different layers and different tasks; opening a promising way to better analyze and understand large, pre-trained language models.</abstract>
      <url hash="f7420647">2023.findings-eacl.181</url>
      <bibkey>fang-etal-2023-transformers</bibkey>
    </paper>
    <paper id="182">
      <title>The Solvability of Interpretability Evaluation Metrics</title>
      <author><first>Yilun</first><last>Zhou</last><affiliation>Mit</affiliation></author>
      <author><first>Julie</first><last>Shah</last><affiliation>Mit</affiliation></author>
      <pages>2399-2415</pages>
      <abstract>Feature attribution methods are popular for explaining neural network predictions, and they are often evaluated on metrics such as comprehensiveness and sufficiency. In this paper, we highlight an intriguing property of these metrics: their solvability. Concretely, we can define the problem of optimizing an explanation for a metric, which can be solved by beam search. This observation leads to the obvious yet unaddressed question: why do we use explainers (e.g., LIME) not based on solving the target metric, if the metric value represents explanation quality? We present a series of investigations showing strong performance of this beam search explainer and discuss its broader implication: a definition-evaluation duality of interpretability concepts. We implement the explainer and release the Python solvex package for models of text, image and tabular domains.</abstract>
      <url hash="87aa8ccd">2023.findings-eacl.182</url>
      <bibkey>zhou-shah-2023-solvability</bibkey>
    </paper>
    <paper id="183">
      <title>Reliable Gradient-free and Likelihood-free Prompt Tuning</title>
      <author><first>Maohao</first><last>Shen</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Soumya</first><last>Ghosh</last><affiliation>IBM Research</affiliation></author>
      <author><first>Prasanna</first><last>Sattigeri</last><affiliation>IBM Research</affiliation></author>
      <author><first>Subhro</first><last>Das</last><affiliation>MIT-IBM Watson AI Lab</affiliation></author>
      <author><first>Yuheng</first><last>Bu</last><affiliation>University of Florida</affiliation></author>
      <author><first>Gregory</first><last>Wornell</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>2416-2429</pages>
      <abstract>Due to privacy or commercial constraints, large pre-trained language models (PLMs) are often offered as black-box APIs. Fine-tuning such models to downstream tasks is challenging because one can neither access the model’s internal representations nor propagate gradients through it. This paper addresses these challenges by developing techniques for adapting PLMs with only API access. Building on recent work on soft prompt tuning, we develop methods to tune the soft prompts without requiring gradient computation. Further, we develop extensions that in addition to not requiring gradients also do not need to access any internal representation of the PLM beyond the input embeddings. Moreover, instead of learning a single prompt, our methods learn a distribution over prompts allowing us to quantify predictive uncertainty. Ours is the first work to consider uncertainty in prompts when only having API access to the PLM. Finally, through extensive experiments, we carefully vet the proposed methods and find them competitive with (and sometimes even improving on) gradient-based approaches with full access to the PLM.</abstract>
      <url hash="0bfa1f77">2023.findings-eacl.183</url>
      <bibkey>shen-etal-2023-reliable</bibkey>
    </paper>
    <paper id="184">
      <title>Combining Psychological Theory with Language Models for Suicide Risk Detection</title>
      <author><first>Daniel</first><last>Izmaylov</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <author><first>Avi</first><last>Segal</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <author><first>Kobi</first><last>Gal</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <author><first>Meytal</first><last>Grimland</last><affiliation>Ruppin Academic Center</affiliation></author>
      <author><first>Yossi</first><last>Levi-Belz</last><affiliation>Ruppin Academic Center</affiliation></author>
      <pages>2430-2438</pages>
      <abstract>With the increased awareness of situations of mental crisis and their societal impact, online services providing emergency support are becoming commonplace in many countries. Computational models, trained on discussions between help-seekers and providers, can support suicide prevention by identifying at-risk individuals. However, the lack of domain-specific models, especially in low-resource languages, poses a significant challenge for the automatic detection of suicide risk. We propose a model that combines pre-trained language models (PLM) with a fixed set of manually crafted (and clinically approved) set of suicidal cues, followed by a two-stage fine-tuning process.Our model achieves 0.91 ROC-AUC and an F2-score of 0.55, significantly outperforming an array of strong baselines even early on in the conversation, which is critical for real-time detection in the field. Moreover, the model performs well across genders and age groups.</abstract>
      <url hash="dc766402">2023.findings-eacl.184</url>
      <bibkey>izmaylov-etal-2023-combining</bibkey>
    </paper>
    <paper id="185">
      <title>Cross-Lingual Question Answering over Knowledge Base as Reading Comprehension</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yuxuan</first><last>Lai</last><affiliation>Peking University</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <author><first>Xingyu</first><last>Shen</last><affiliation>Peking University</affiliation></author>
      <author><first>Haowei</first><last>Du</last><affiliation>Peking University</affiliation></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Pku.edu.cn</affiliation></author>
      <pages>2439-2452</pages>
      <abstract>Although many large-scale knowledge bases (KBs) claim to contain multilingual information, their support for many non-English languages is often incomplete. This incompleteness gives birth to the task of cross-lingual question answering over knowledge base (xKBQA), which aims to answer questions in languages different from that of the provided KB. One of the major challenges facing xKBQA is the high cost of data annotation, leading to limited resources available for further exploration. Another challenge is mapping KB schemas and natural language expressions in the questions under cross-lingual settings. In this paper, we propose a novel approach for xKBQA in a reading comprehension paradigm. We convert KB subgraphs into passages to narrow the gap between KB schemas and questions, which enables our model to benefit from recent advances in multilingual pre-trained language models (MPLMs) and cross-lingual machine reading comprehension (xMRC). Specifically, we use MPLMs, with considerable knowledge of cross-lingual mappings, for cross-lingual reading comprehension. Existing high-quality xMRC datasets can be further utilized to finetune our model, greatly alleviating the data scarcity issue in xKBQA. Extensive experiments on two xKBQA datasets in 12 languages show that our approach outperforms various baselines and achieves strong few-shot and zero-shot performance. Our dataset and code are released for further research.</abstract>
      <url hash="bb2bf885">2023.findings-eacl.185</url>
      <bibkey>zhang-etal-2023-cross</bibkey>
    </paper>
    <paper id="186">
      <title>Delving Deeper into Cross-lingual Visual Question Answering</title>
      <author><first>Chen</first><last>Liu</last><affiliation>Technische Universitat Darmstadt</affiliation></author>
      <author><first>Jonas</first><last>Pfeiffer</last><affiliation>Google</affiliation></author>
      <author><first>Anna</first><last>Korhonen</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Ivan</first><last>Vulić</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>UKP Lab, Technische Universität Darmstadt</affiliation></author>
      <pages>2453-2468</pages>
      <abstract>Visual question answering (VQA) is one of the crucial vision-and-language tasks. Yet, existing VQA research has mostly focused on the English language, due to a lack of suitable evaluation resources. Previous work on cross-lingual VQA has reported poor zero-shot transfer performance of current multilingual multimodal Transformers with large gaps to monolingual performance, without any deeper analysis. In this work, we delve deeper into the different aspects of cross-lingual VQA, aiming to understand the impact of 1) modeling methods and choices, including architecture, inductive bias, fine-tuning; 2) learning biases: including question types and modality biases in cross-lingual setups. The key results of our analysis are: 1. We show that simple modifications to the standard training setup can substantially reduce the transfer gap to monolingual English performance, yielding +10 accuracy points over existing methods. 2. We analyze cross-lingual VQA across different question types of varying complexity for different multilingual multimodal Transformers, and identify question types that are the most difficult to improve on. 3. We provide an analysis of modality biases present in training data and models, revealing why zero-shot performance gaps remain for certain question types and languages.</abstract>
      <url hash="1b3f39b1">2023.findings-eacl.186</url>
      <bibkey>liu-etal-2023-delving</bibkey>
    </paper>
    <paper id="187">
      <title>Bridging Argument Quality and Deliberative Quality Annotations with Adapters</title>
      <author><first>Neele</first><last>Falk</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Gabriella</first><last>Lapesa</last><affiliation>Universität Stuttgart, Institut für Maschinelle Sprachverarbeitung</affiliation></author>
      <pages>2469-2488</pages>
      <abstract>Assessing the quality of an argument is a complex, highly subjective task, influenced by heterogeneous factors (e.g., prior beliefs of the annotators, topic, domain, and application), and crucial for its impact in downstream tasks (e.g., argument retrieval or generation). Both the Argument Mining and the Social Science community have devoted plenty of attention to it, resulting in a wide variety of argument quality dimensions and a large number of annotated resources.This work aims at a better understanding of how the different aspects of argument quality relate to each other from a practical point of view. We employ adapter-fusion (Pfeiffer et al., 2021) as a multi-task learning framework which a) can improve the prediction of individual quality dimensions by injecting knowledge about related dimensions b) is efficient and modular and c) can serve as an analysis tool to investigate relations between different dimensions. We conduct experiments on 6 datasets and 20 quality dimensions. We find that the majority of the dimensions can be learned as a weighted combination of other quality aspects, and that for 8 dimensions adapter fusion improves quality prediction. Last, we show the benefits of this approach by improving the performance in an extrinsic, out-of-domain task: prediction of moderator interventions in a deliberative forum.</abstract>
      <url hash="11a1075a">2023.findings-eacl.187</url>
      <bibkey>falk-lapesa-2023-bridging</bibkey>
    </paper>
    <paper id="188">
      <title>Interventional Probing in High Dimensions: An <fixed-case>NLI</fixed-case> Case Study</title>
      <author><first>Julia</first><last>Rozanova</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Marco</first><last>Valentino</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Lucas</first><last>Cordeiro</last><affiliation>University of Manchester</affiliation></author>
      <author><first>André</first><last>Freitas</last><affiliation>University of Manchester</affiliation></author>
      <pages>2489-2500</pages>
      <abstract>Probing strategies have been shown to detectthe presence of various linguistic features inlarge language models; in particular, seman-tic features intermediate to the “natural logic”fragment of the Natural Language Inferencetask (NLI). In the case of natural logic, the rela-tion between the intermediate features and theentailment label is explicitly known: as such,this provides a ripe setting for interventionalstudies on the NLI models’ representations, al-lowing for stronger causal conjectures and adeeper critical analysis of interventional prob-ing methods. In this work, we carry out newand existing representation-level interventionsto investigate the effect of these semantic fea-tures on NLI classification: we perform am-nesic probing (which removes features as di-rected by learned linear probes) and introducethe mnestic probing variation (which forgetsall dimensions except the probe-selected ones).Furthermore, we delve into the limitations ofthese methods and outline some pitfalls havebeen obscuring the effectivity of interventionalprobing studies.</abstract>
      <url hash="9ee97fe8">2023.findings-eacl.188</url>
      <bibkey>rozanova-etal-2023-interventional</bibkey>
    </paper>
    <paper id="189">
      <title>Program Synthesis for Complex <fixed-case>QA</fixed-case> on Charts via Probabilistic Grammar Based Filtered Iterative Back-Translation</title>
      <author><first>Shabbirhussain</first><last>Bhaisaheb</last><affiliation>TCS Research</affiliation></author>
      <author><first>Shubham</first><last>Paliwal</last><affiliation>TCS Research</affiliation></author>
      <author><first>Rajaswa</first><last>Patil</last><affiliation>TCS Research</affiliation></author>
      <author><first>Manasi</first><last>Patwardhan</last><affiliation>TCS Research</affiliation></author>
      <author><first>Lovekesh</first><last>Vig</last><affiliation>TCS Research</affiliation></author>
      <author><first>Gautam</first><last>Shroff</last><affiliation>TCS Research</affiliation></author>
      <pages>2501-2515</pages>
      <abstract>Answering complex reasoning questions from chart images is a challenging problem requiring a combination of natural language understanding, fine-grained perception, and analytical reasoning. Current chart-based Question Answering (QA) approaches largely address structural, visual or simple data retrieval-type questions with fixed-vocabulary answers and perform poorly on reasoning queries. We focus on answering realistic, complex, reasoning-based questions where the answer needs to be computed and not selected from a fixed set of choices. Our approach employs a neural semantic parser to transform Natural Language (NL) questions into SQL programs and execute them on a standardized schema populated from the extracted chart contents. In the absence of program annotations, i.e., in a weak supervision setting, we obtain initial SQL predictions from a pre-trained CodeT5 semantic parser and employ Filtered Iterative Back-Translation (FIBT) for iteratively augmenting our NL-SQL training set. The forward (neural semantic parser) and backward (language model) models are initially trained with an external NL-SQL dataset. We iteratively move towards the NL query distribution by generating NL questions from the synthesized SQL programs using a Probabilistic Context-Free Grammar (PCFG) where the production rule probabilities are induced to be inversely proportional to the probabilities in the training data. We filter out the generated NL queries with mismatched structures and compositions. Our FIBT approach achieves State-of-the-Art (SOTA) results on reasoning-based queries in the PlotQA dataset yielding a test accuracy of 60.44%, superseding the previous baselines by a large margin.</abstract>
      <url hash="8b35f87d">2023.findings-eacl.189</url>
      <bibkey>bhaisaheb-etal-2023-program</bibkey>
    </paper>
    <paper id="190">
      <title>Exploiting Language Characteristics for Legal Domain-Specific Language Model Pretraining</title>
      <author><first>Inderjeet</first><last>Nair</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Natwar</first><last>Modani</last><affiliation>Adobe Research, India</affiliation></author>
      <pages>2516-2526</pages>
      <abstract>Pretraining large language models has resulted in tremendous performance improvement for many natural language processing (NLP) tasks. While for non-domain specific tasks, such models can be used directly, a common strategy to achieve better performance for specific domains involves pretraining these language models over domain specific data using objectives like Masked Language Modelling (MLM), Autoregressive Language Modelling, etc. While such pretraining addresses the change in vocabulary and style of language for the domain, it is otherwise a domain agnostic approach. In this work, we investigate the effect of incorporating pretraining objectives that explicitly tries to exploit the domain specific language characteristics in addition to such MLM based pretraining. Particularly, we examine two distinct characteristics associated with the legal domain and propose pretraining objectives modelling these characteristics. The proposed objectives target improvement of token-level feature representation, as well as aim to incorporate sentence level semantics. We demonstrate superiority in the performance of the models pretrained using our objectives against those trained using domain-agnostic objectives over several legal downstream tasks.</abstract>
      <url hash="4d266a5d">2023.findings-eacl.190</url>
      <bibkey>nair-modani-2023-exploiting</bibkey>
    </paper>
    <paper id="191">
      <title>Global Constraints with Prompting for Zero-Shot Event Argument Classification</title>
      <author><first>Zizheng</first><last>Lin</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last><affiliation>Tencent AI Lab, Bellevue</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>Hkust</affiliation></author>
      <pages>2527-2538</pages>
      <abstract>Determining the role of event arguments is a crucial subtask of event extraction. Most previous supervised models leverage costly annotations, which is not practical for open-domain applications. In this work, we propose to use global constraints with prompting to effectively tackles event argument classification without any annotation and task-specific training. Specifically, given an event and its associated passage, the model first creates several new passages by prefix prompts and cloze prompts, where prefix prompts indicate event type and trigger span, and cloze prompts connect each candidate role with the target argument span. Then, a pre-trained language model scores the new passages, making the initial prediction. Our novel prompt templates can easily adapt to all events and argument types without manual effort. Next, the model regularizes the prediction by global constraints exploiting cross-task, cross-argument, and cross-event relations. Extensive experiments demonstrate our model’s effectiveness: it outperforms the best zero-shot baselines by 12.5% and 10.9% F1 on ACE and ERE with given argument spans and by 4.3% and 3.3% F1, respectively, without given argument spans. We have made our code publicly available.</abstract>
      <url hash="7bde8964">2023.findings-eacl.191</url>
      <bibkey>lin-etal-2023-global</bibkey>
    </paper>
    <paper id="192">
      <title>Distillation of encoder-decoder transformers for sequence labelling</title>
      <author><first>Marco</first><last>Farina</last><affiliation>Bloomberg L.P.</affiliation></author>
      <author><first>Duccio</first><last>Pappadopulo</last><affiliation>Bloomberg LP</affiliation></author>
      <author><first>Anant</first><last>Gupta</last><affiliation>Bloomberg.net</affiliation></author>
      <author><first>Leslie</first><last>Huang</last><affiliation>Bloomberg LP</affiliation></author>
      <author><first>Ozan</first><last>Irsoy</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Thamar</first><last>Solorio</last><affiliation>University of Houston</affiliation></author>
      <pages>2539-2549</pages>
      <abstract>Driven by encouraging results on a wide range of tasks, the field of NLP is experiencing an accelerated race to develop bigger language models. This race for bigger models has also underscored the need to continue the pursuit of practical distillation approaches that can leverage the knowledge acquired by these big models in a compute-efficient manner. Having this goal in mind, we build on recent work to propose a hallucination-free framework for sequence tagging that is especially suited for distillation. We show empirical results of new state-of-the-art performance across multiple sequence labelling datasets and validate the usefulness of this framework for distilling a large model in a few-shot learning scenario.</abstract>
      <url hash="5e93decc">2023.findings-eacl.192</url>
      <bibkey>farina-etal-2023-distillation</bibkey>
    </paper>
    <paper id="193">
      <title>Predicting Desirable Revisions of Evidence and Reasoning in Argumentative Writing</title>
      <author><first>Tazin</first><last>Afrin</last><affiliation>Educational Testing Service</affiliation></author>
      <author><first>Diane</first><last>Litman</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>2550-2561</pages>
      <abstract>We develop models to classify desirable evidence and desirable reasoning revisions in student argumentative writing. We explore two ways to improve classifier performance – using the essay context of the revision, and using the feedback students received before the revision. We perform both intrinsic and extrinsic evaluation for each of our models and report a qualitative analysis. Our results show that while a model using feedback information improves over a baseline model, models utilizing context - either alone or with feedback - are the most successful in identifying desirable revisions.</abstract>
      <url hash="7662042c">2023.findings-eacl.193</url>
      <bibkey>afrin-litman-2023-predicting</bibkey>
    </paper>
    <paper id="194">
      <title>Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues</title>
      <author><first>Chuyuan</first><last>Li</last><affiliation>Loria</affiliation></author>
      <author><first>Patrick</first><last>Huber</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Wen</first><last>Xiao</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Maxime</first><last>Amblard</last><affiliation>Université de Lorraine</affiliation></author>
      <author><first>Chloe</first><last>Braud</last><affiliation>Irit - Cnrs - Aniti</affiliation></author>
      <author><first>Giuseppe</first><last>Carenini</last><affiliation>University Of British Columbia</affiliation></author>
      <pages>2562-2579</pages>
      <abstract>Discourse processing suffers from data sparsity, especially for dialogues. As a result, we explore approaches to infer latent discourse structures for dialogues, based on attention matrices from Pre-trained Language Models (PLMs). We investigate multiple auxiliary tasks for fine-tuning and show that the dialogue-tailored Sentence Ordering task performs best. To locate and exploit discourse information in PLMs, we propose an unsupervised and a semi-supervised method. Our proposals thereby achieve encouraging results on the STAC corpus, with F1 scores of 57.2 and 59.3 for the unsupervised and semi-supervised methods, respectively. When restricted to projective trees, our scores improved to 63.3 and 68.1.</abstract>
      <url hash="389d2b00">2023.findings-eacl.194</url>
      <bibkey>li-etal-2023-discourse</bibkey>
    </paper>
    <paper id="195">
      <title>Relation Extraction with Weighted Contrastive Pre-training on Distant Supervision</title>
      <author><first>Zhen</first><last>Wan</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Fei</first><last>Cheng</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Qianying</first><last>Liu</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Zhuoyuan</first><last>Mao</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Haiyue</first><last>Song</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>Kyoto University</affiliation></author>
      <pages>2580-2585</pages>
      <abstract>Contrastive pre-training on distant supervision has shown remarkable effectiveness in improving supervised relation extraction tasks. However, the existing methods ignore the intrinsic noise of distant supervision during the pre-training stage. In this paper, we propose a weighted contrastive learning method by leveraging the supervised data to estimate the reliability of pre-training instances and explicitly reduce the effect of noise.Experimental results on three supervised datasets demonstrate the advantages of our proposed weighted contrastive learning approach compared to two state-of-the-art non-weighted baselines.Our code and models are available at: https://github.com/YukinoWan/WCL.</abstract>
      <url hash="c5645035">2023.findings-eacl.195</url>
      <bibkey>wan-etal-2023-relation</bibkey>
    </paper>
    <paper id="196">
      <title><fixed-case>CK</fixed-case>-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension</title>
      <author><first>Zhi</first><last>Zhang</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Helen</first><last>Yannakoudakis</last><affiliation>King’s College London, University of Cambridge</affiliation></author>
      <author><first>Xiantong</first><last>Zhen</last><affiliation>United Imaging Healthcare</affiliation></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>2586-2596</pages>
      <abstract>The task of multimodal referring expression comprehension (REC), aiming at localizing an image region described by a natural language expression, has recently received increasing attention within the research comminity. In this paper, we specifically focus on referring expression comprehension with commonsense knowledge (KB-Ref), a task which typically requires reasoning beyond spatial, visual or semantic information. We propose a novel framework for Commonsense Knowledge Enhanced Transformers (CK-Transformer) which effectively integrates commonsense knowledge into the representations of objects in an image, facilitating identification of the target objects referred to by the expressions. We conduct extensive experiments on several benchmarks for the task of KB-Ref. Our results show that the proposed CK-Transformer achieves a new state of the art, with an absolute improvement of 3.14% accuracy over the existing state of the art.</abstract>
      <url hash="fa01abcc">2023.findings-eacl.196</url>
      <bibkey>zhang-etal-2023-ck</bibkey>
    </paper>
    <paper id="197">
      <title>Curricular Next Conversation Prediction Pretraining for Transcript Segmentation</title>
      <author><first>Anvesh Rao</first><last>Vijjini</last><affiliation>UNC Chapel Hill</affiliation></author>
      <author><first>Hanieh</first><last>Deilamsalehy</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Snigdha</first><last>Chaturvedi</last><affiliation>University of North Carolina, Chapel Hill</affiliation></author>
      <pages>2597-2607</pages>
      <abstract>Transcript segmentation is the task of dividing a single continuous transcript into multiple segments. While document segmentation is a popular task, transcript segmentation has significant challenges due to the relatively noisy and sporadic nature of data. We propose pretraining strategies to address these challenges. The strategies are based on “Next Conversation Prediction” (NCP) with the underlying idea of pretraining a model to identify consecutive conversations. We further introduce “Advanced NCP” to make the pretraining task more relevant to the downstream task of segmentation break prediction while being significantly easier. Finally we introduce a curriculum to Advanced NCP (Curricular NCP) based on the similarity between pretraining and downstream task samples. Curricular NCP applied to a state-of-the-art model for text segmentation outperforms prior results. We also show that our pretraining strategies make the model robust to speech recognition errors commonly found in automatically generated transcripts.</abstract>
      <url hash="8166576d">2023.findings-eacl.197</url>
      <bibkey>vijjini-etal-2023-curricular</bibkey>
    </paper>
  </volume>
</collection>
