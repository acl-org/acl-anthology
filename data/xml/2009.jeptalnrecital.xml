<?xml version='1.0' encoding='UTF-8'?>
<collection id="2009.jeptalnrecital">
  <volume id="long" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Adeline</first><last>Nazarenko</last></editor>
      <editor><first>Thierry</first><last>Poibeau</last></editor>
      <publisher>ATALA</publisher>
      <address>Senlis, France</address>
      <month>June</month>
      <year>2009</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="df4f8409">2009.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2009-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Acquisition morphologique à partir d’un dictionnaire informatisé</title>
      <author><first>Nabil</first><last>Hathout</last></author>
      <pages>1–10</pages>
      <abstract>L’article propose un modèle linguistique et informatique permettant de faire émerger la structure morphologique dérivationnelle du lexique à partir des régularités sémantiques et formelles des mots qu’il contient. Ce modèle est radicalement lexématique. La structure morphologique est constituée par les relations que chaque mot entretient avec les autres unités du lexique et notamment avec les mots de sa famille morphologique et de sa série dérivationnelle. Ces relations forment des paradigmes analogiques. La modélisation a été testée sur le lexique du français en utilisant le dictionnaire informatisé TLFi.</abstract>
      <url hash="87f83f51">2009.jeptalnrecital-long.1</url>
      <language>fra</language>
      <bibkey>hathout-2009-acquisition</bibkey>
    </paper>
    <paper id="2">
      <title>Analyse déductive pour les grammaires d’interaction</title>
      <author><first>Joseph</first><last>Le Roux</last></author>
      <pages>11–20</pages>
      <abstract>Nous proposons un algorithme d’analyse pour les grammaires d’interaction qui utilise le cadre formel de l’analyse déductive. Cette approche donne un point de vue nouveau sur ce problème puisque les méthodes précédentes réduisaient ce dernier à la réécriture de graphes et utilisaient des techniques de résolution de contraintes. D’autre part, cette présentation permet de décrire le processus de manière standard et d’exhiber les sources d’indéterminisme qui rendent ce problème difficile.</abstract>
      <url hash="fdea8922">2009.jeptalnrecital-long.2</url>
      <language>fra</language>
      <bibkey>le-roux-2009-analyse</bibkey>
    </paper>
    <paper id="3">
      <title>Analyse syntaxique en dépendances de l’oral spontané</title>
      <author><first>Alexis</first><last>Nasr</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <pages>21–30</pages>
      <abstract>Cet article décrit un modèle d’analyse syntaxique de l’oral spontané axé sur la reconnaissance de cadres valenciels verbaux. Le modèle d’analyse se décompose en deux étapes : une étape générique, basée sur des ressources génériques du français et une étape de réordonnancement des solutions de l’analyseur réalisé par un modèle spécifique à une application. Le modèle est évalué sur le corpus MEDIA.</abstract>
      <url hash="0bf1512c">2009.jeptalnrecital-long.3</url>
      <language>fra</language>
      <bibkey>nasr-bechet-2009-analyse</bibkey>
    </paper>
    <paper id="4">
      <title>Analyse syntaxique du français : des constituants aux dépendances</title>
      <author><first>Marie</first><last>Candito</last></author>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <author><first>Pascal</first><last>Denis</last></author>
      <author><first>François</first><last>Guérin</last></author>
      <pages>31–40</pages>
      <abstract>Cet article présente une technique d’analyse syntaxique statistique à la fois en constituants et en dépendances. L’analyse procède en ajoutant des étiquettes fonctionnelles aux sorties d’un analyseur en constituants, entraîné sur le French Treebank, pour permettre l’extraction de dépendances typées. D’une part, nous spécifions d’un point de vue formel et linguistique les structures de dépendances à produire, ainsi que la procédure de conversion du corpus en constituants (le French Treebank) vers un corpus cible annoté en dépendances, et partiellement validé. D’autre part, nous décrivons l’approche algorithmique qui permet de réaliser automatiquement le typage des dépendances. En particulier, nous nous focalisons sur les méthodes d’apprentissage discriminantes d’étiquetage en fonctions grammaticales.</abstract>
      <url hash="cf4d00e3">2009.jeptalnrecital-long.4</url>
      <language>fra</language>
      <bibkey>candito-etal-2009-analyse</bibkey>
    </paper>
    <paper id="5">
      <title>Annotation fonctionnelle de corpus arborés avec des Champs Aléatoires Conditionnels</title>
      <author><first>Erwan</first><last>Moreau</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <author><first>Antonio</first><last>Balvet</last></author>
      <author><first>Grégoire</first><last>Laurence</last></author>
      <author><first>Antoine</first><last>Rozenknop</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>41–50</pages>
      <abstract>L’objectif de cet article est d’évaluer dans quelle mesure les “fonctions syntaxiques” qui figurent dans une partie du corpus arboré de Paris 7 sont apprenables à partir d’exemples. La technique d’apprentissage automatique employée pour cela fait appel aux “Champs Aléatoires Conditionnels” (Conditional Random Fields ou CRF), dans une variante adaptée à l’annotation d’arbres. Les expériences menées sont décrites en détail et analysées. Moyennant un bon paramétrage, elles atteignent une F1-mesure de plus de 80%.</abstract>
      <url hash="dae101bc">2009.jeptalnrecital-long.5</url>
      <language>fra</language>
      <bibkey>moreau-etal-2009-annotation</bibkey>
    </paper>
    <paper id="6">
      <title>Apport d’un corpus comparable déséquilibré à l’extraction de lexiques bilingues</title>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <pages>51–60</pages>
      <abstract>Les principaux travaux en extraction de lexiques bilingues à partir de corpus comparables reposent sur l’hypothèse implicite que ces corpus sont équilibrés. Cependant, les différentes méthodes computationnelles associées sont relativement insensibles à la taille de chaque partie du corpus. Dans ce contexte, nous étudions l’influence que peut avoir un corpus comparable déséquilibré sur la qualité des terminologies bilingues extraites à travers différentes expériences. Nos résultats montrent que sous certaines conditions l’utilisation d’un corpus comparable déséquilibré peut engendrer un gain significatif dans la qualité des lexiques extraits.</abstract>
      <url hash="c87c5b2f">2009.jeptalnrecital-long.6</url>
      <language>fra</language>
      <bibkey>morin-2009-apport</bibkey>
    </paper>
    <paper id="7">
      <title>Classification d’un contenu encyclopédique en vue d’un étiquetage par entités nommées</title>
      <author><first>Eric</first><last>Charton</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <pages>61–70</pages>
      <abstract>On utilise souvent des ressources lexicales externes pour améliorer les performances des systèmes d’étiquetage d’entités nommées. Les contenus de ces ressources lexicales peuvent être variés : liste de noms propres, de lieux, de marques. On note cependant que la disponibilité de corpus encyclopédiques exhaustifs et ouverts de grande taille tels que Worldnet ou Wikipedia, a fait émerger de nombreuses propositions spécifiques d’exploitation de ces contenus par des systèmes d’étiquetage. Un problème demeure néanmoins ouvert avec ces ressources : celui de l’adaptation de leur taxonomie interne, complexe et composée de dizaines de milliers catégories, aux exigences particulières de l’étiquetage des entités nommées. Pour ces dernières, au plus de quelques centaines de classes sémantiques sont requises. Dans cet article nous explorons cette difficulté et proposons un système complet de transformation d’un arbre taxonomique encyclopédique en une système à classe sémantiques adapté à l’étiquetage d’entités nommées.</abstract>
      <url hash="3b9e9aa2">2009.jeptalnrecital-long.7</url>
      <language>fra</language>
      <bibkey>charton-torres-moreno-2009-classification</bibkey>
    </paper>
    <paper id="8">
      <title>Étude quantitative de liens entre l’analogie formelle et la morphologie constructionnelle</title>
      <author><first>Philippe</first><last>Langlais</last></author>
      <pages>71–80</pages>
      <abstract>Plusieurs travaux ont récemment étudié l’apport de l’apprentissage analogique dans des applications du traitement automatique des langues comme la traduction automatique, ou la recherche d’information. Il est souvent admis que les relations analogiques de forme entre les mots capturent des informations de nature morphologique. Le but de cette étude est de présenter une analyse des points de rencontre entre l’analyse morphologique et les analogies de forme. C’est à notre connaissance la première étude de ce type portant sur des corpus de grande taille et sur plusieurs langues. Bien que notre étude ne soit pas dédiée à une tâche particulière du traitement des langues, nous montrons cependant que le principe d’analogie permet de segmenter des mots en morphèmes avec une bonne précision.</abstract>
      <url hash="c5809ce2">2009.jeptalnrecital-long.8</url>
      <language>fra</language>
      <bibkey>langlais-2009-etude</bibkey>
    </paper>
    <paper id="9">
      <title>Exploitation d’un corpus bilingue pour la création d’un système de traduction probabiliste Vietnamien - Français</title>
      <author><first>Thi-Ngoc-Diep</first><last>Do</last></author>
      <author><first>Viet-Bac</first><last>Le</last></author>
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Eric</first><last>Castelli</last></author>
      <pages>81–90</pages>
      <abstract>Cet article présente nos premiers travaux en vue de la construction d’un système de traduction probabiliste pour le couple de langue vietnamien-français. La langue vietnamienne étant considérée comme une langue peu dotée, une des difficultés réside dans la constitution des corpus parallèles, indispensable à l’apprentissage des modèles. Nous nous concentrons sur la constitution d’un grand corpus parallèle vietnamien-français. La méthode d’identification automatique des paires de documents parallèles fondée sur la date de publication, les mots spéciaux et les scores d’alignements des phrases est appliquée. Cet article présente également la construction d’un premier système de traduction automatique probabiliste vietnamienfrançais et français-vietnamien à partir de ce corpus et discute l’opportunité d’utiliser des unités lexicales ou sous-lexicales pour le vietnamien (syllabes, mots, ou leurs combinaisons). Les performances du système sont encourageantes et se comparent avantageusement à celles du système de Google.</abstract>
      <url hash="7b7b1c8e">2009.jeptalnrecital-long.9</url>
      <language>fra</language>
      <bibkey>do-etal-2009-exploitation</bibkey>
    </paper>
    <paper id="10">
      <title>Influence des points d’ancrage pour l’extraction lexicale bilingue à partir de corpus comparables spécialisés</title>
      <author><first>Emmanuel</first><last>Prochasson</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <pages>91–100</pages>
      <abstract>L’extraction de lexiques bilingues à partir de corpus comparables affiche de bonnes performances pour des corpus volumineux mais chute fortement pour des corpus d’une taille plus modeste. Pour pallier cette faiblesse, nous proposons une nouvelle contribution au processus d’alignement lexical à partir de corpus comparables spécialisés qui vise à renforcer la significativité des contextes lexicaux en s’appuyant sur le vocabulaire spécialisé du domaine étudié. Les expériences que nous avons réalisées en ce sens montrent qu’une meilleure prise en compte du vocabulaire spécialisé permet d’améliorer la qualité des lexiques extraits.</abstract>
      <url hash="e3d44002">2009.jeptalnrecital-long.10</url>
      <language>fra</language>
      <bibkey>prochasson-morin-2009-influence</bibkey>
    </paper>
    <paper id="11">
      <title>Intégration de l’alignement de mots dans le concordancier bilingue <fixed-case>T</fixed-case>rans<fixed-case>S</fixed-case>earch</title>
      <author><first>Stéphane</first><last>Huet</last></author>
      <author><first>Julien</first><last>Bourdaillet</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <pages>101–110</pages>
      <abstract>Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Dans cet article, nous présentons nos travaux menés dans le but d’améliorer le concordancier bilingue TransSearch. Ce service, accessible sur le Web, repose principalement sur un alignement au niveau des phrases. Dans cette étude, nous discutons et évaluons l’intégration d’un alignement statistique au niveau des mots. Nous présentons deux nouvelles problématiques essentielles au succès de notre nouveau prototype : la détection des traductions erronées et le regroupement des variantes de traduction similaires.</abstract>
      <url hash="20861895">2009.jeptalnrecital-long.11</url>
      <language>fra</language>
      <bibkey>huet-etal-2009-integration</bibkey>
    </paper>
    <paper id="12">
      <title>Jugements d’évaluation et constituants périphériques</title>
      <author><first>Agata</first><last>Jackiewicz</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <author><first>Stéphane</first><last>Ferrari</last></author>
      <pages>111–120</pages>
      <abstract>L’article présente une étude portant sur des constituants détachés à valeur axiologique. Dans un premier temps, une analyse linguistique sur corpus met en évidence un ensemble de patrons caractéristiques du phénomène. Ensuite, une expérimentation informatique est proposée sur un corpus de plus grande taille afin de permettre l’observation des patrons en vue d’un retour sur le modèle linguistique. Ce travail s’inscrit dans un projet mené à l’interface de la linguistique et du TAL, qui se donne pour but d’enrichir, d’adapter au français et de formaliser le modèle général Appraisal de l’évaluation dans la langue.</abstract>
      <url hash="45289161">2009.jeptalnrecital-long.12</url>
      <language>fra</language>
      <bibkey>jackiewicz-etal-2009-jugements</bibkey>
    </paper>
    <paper id="13">
      <title>Le projet <fixed-case>B</fixed-case>aby<fixed-case>T</fixed-case>alk : génération de texte à partir de données hétérogènes pour la prise de décision en unité néonatale</title>
      <author><first>François</first><last>Portet</last></author>
      <author><first>Albert</first><last>Gatt</last></author>
      <author><first>Jim</first><last>Hunter</last></author>
      <author><first>Ehud</first><last>Reiter</last></author>
      <author><first>Somayajulu</first><last>Sripada</last></author>
      <pages>121–130</pages>
      <abstract>Notre société génère une masse d’information toujours croissante, que ce soit en médecine, en météorologie, etc. La méthode la plus employée pour analyser ces données est de les résumer sous forme graphique. Cependant, il a été démontré qu’un résumé textuel est aussi un mode de présentation efficace. L’objectif du prototype BT-45, développé dans le cadre du projet Babytalk, est de générer des résumés de 45 minutes de signaux physiologiques continus et d’événements temporels discrets en unité néonatale de soins intensifs (NICU). L’article présente l’aspect génération de texte de ce prototype. Une expérimentation clinique a montré que les résumés humains améliorent la prise de décision par rapport à l’approche graphique, tandis que les textes de BT-45 donnent des résultats similaires à l’approche graphique. Une analyse a identifié certaines des limitations de BT-45 mais en dépit de cellesci, notre travail montre qu’il est possible de produire automatiquement des résumés textuels efficaces de données complexes.</abstract>
      <url hash="88671fd4">2009.jeptalnrecital-long.13</url>
      <language>fra</language>
      <bibkey>portet-etal-2009-le</bibkey>
    </paper>
    <paper id="14">
      <title>Les adjectifs relationnels dans les lexiques informatisés : formalisation et exploitation dans un contexte multilingue</title>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <pages>131–140</pages>
      <abstract>Dans cet article, nous nous intéressons aux adjectifs dits relationnels et à leur statut en traitement automatique des langues naturelles (TALN). Nous montrons qu’ils constituent une « sous-classe » d’adjectifs rarement explicitée et donc rarement représentée dans les lexiques sur lesquels reposent les applications du TALN, alors qu’ils jouent un rôle important dans de nombreuses applications. Leur formation morphologique est source d’importantes divergences entre différentes langues, et c’est pourquoi ces adjectifs sont un véritable défi pour les applications informatiques multilingues. Dans une partie plus pratique, nous proposons une formalisation de ces adjectifs permettant de rendre compte de leurs liens avec leur base nominale. Nous tentons d’extraire ces informations dans les lexiques informatisés existants, puis nous les exploitons pour traduire les adjectifs relationnels préfixés de l’italien en français.</abstract>
      <url hash="d3c283a4">2009.jeptalnrecital-long.14</url>
      <language>fra</language>
      <bibkey>cartoni-2009-les</bibkey>
    </paper>
    <paper id="15">
      <title>Motifs séquentiels pour l’extraction d’information : illustration sur le problème de la détection d’interactions entre gènes</title>
      <author><first>Marc</first><last>Plantevit</last></author>
      <author><first>Thierry</first><last>Charnois</last></author>
      <pages>141–150</pages>
      <abstract>Face à la prolifération des publications en biologie et médecine (plus de 18 millions de publications actuellement recensées dans PubMed), l’extraction d’information automatique est devenue un enjeu crucial. Il existe de nombreux travaux dans le domaine du traitement de la langue appliquée à la biomédecine (“BioNLP”). Ces travaux se distribuent en deux grandes tendances. La première est fondée sur les méthodes d’apprentissage automatique de type numérique qui donnent de bons résultats mais ont un fonctionnement de type “boite noire”. La deuxième tendance est celle du TALN à base d’analyses (lexicales, syntaxiques, voire sémantiques ou discursives) coûteuses en temps de développement des ressources nécessaires (lexiques, grammaires, etc.). Nous proposons dans cet article une approche basée sur la découverte de motifs séquentiels pour apprendre automatiquement les ressources linguistiques, en l’occurrence les patrons linguistiques qui permettent l’extraction de l’information dans les textes. Plusieurs aspects méritent d’être soulignés : cette approche permet de s’affranchir de l’analyse syntaxique de la phrase, elle ne nécessite pas de ressources en dehors du corpus d’apprentissage et elle ne demande que très peu d’intervention manuelle. Nous illustrons l’approche sur le problème de la détection d’interactions entre gènes et donnons les résultats obtenus sur des corpus biologiques qui montrent l’intérêt de ce type d’approche.</abstract>
      <url hash="20c03cb5">2009.jeptalnrecital-long.15</url>
      <language>fra</language>
      <bibkey>plantevit-charnois-2009-motifs</bibkey>
    </paper>
    <paper id="16">
      <title>Prise en compte de dépendances syntaxiques pour la traduction contextuelle de segments</title>
      <author><first>Aurélien</first><last>Max</last></author>
      <author><first>Rafik</first><last>Maklhoufi</last></author>
      <author><first>Philippe</first><last>Langlais</last></author>
      <pages>151–160</pages>
      <abstract>Dans un système standard de traduction statistique basé sur les segments, le score attribué aux différentes traductions d’un segment ne dépend pas du contexte dans lequel il apparaît. Plusieurs travaux récents tendent à montrer l’intérêt de prendre en compte le contexte source lors de la traduction, mais ces études portent sur des systèmes traduisant vers l’anglais, une langue faiblement fléchie. Dans cet article, nous décrivons nos expériences sur la prise en compte du contexte source dans un système statistique traduisant de l’anglais vers le français, basé sur l’approche proposée par Stroppa et al. (2007). Nous étudions l’impact de différents types d’indices capturant l’information contextuelle, dont des dépendances syntaxiques typées. Si les mesures automatiques d’évaluation de la qualité d’une traduction ne révèlent pas de gains significatifs de notre système par rapport à un système à l’état de l’art ne faisant pas usage du contexte, une évaluation manuelle conduite sur 100 phrases choisies aléatoirement est en faveur de notre système. Cette évaluation fait également ressortir que la prise en compte de certaines dépendances syntaxiques est bénéfique à notre système.</abstract>
      <url hash="eb1df976">2009.jeptalnrecital-long.16</url>
      <language>fra</language>
      <bibkey>max-etal-2009-prise</bibkey>
    </paper>
    <paper id="17">
      <title>Proposition de caractérisation et de typage des expressions temporelles en contexte</title>
      <author><first>Maud</first><last>Ehrmann</last></author>
      <author><first>Caroline</first><last>Hagège</last></author>
      <pages>161–170</pages>
      <abstract>Nous assistons actuellement en TAL à un regain d’intérêt pour le traitement de la temporalité véhiculée par les textes. Dans cet article, nous présentons une proposition de caractérisation et de typage des expressions temporelles tenant compte des travaux effectués dans ce domaine tout en cherchant à pallier les manques et incomplétudes de certains de ces travaux. Nous explicitons comment nous nous situons par rapport à l’existant et les raisons pour lesquelles parfois nous nous en démarquons. Le typage que nous définissons met en évidence de réelles différences dans l’interprétation et le mode de résolution référentielle d’expressions qui, en surface, paraissent similaires ou identiques. Nous proposons un ensemble des critères objectifs et linguistiquement motivés permettant de reconnaître, de segmenter et de typer ces expressions. Nous verrons que cela ne peut se réaliser sans considérer les procès auxquels ces expressions sont associées et un contexte parfois éloigné.</abstract>
      <url hash="b15a0fc3">2009.jeptalnrecital-long.17</url>
      <language>fra</language>
      <bibkey>ehrmann-hagege-2009-proposition</bibkey>
    </paper>
    <paper id="18">
      <title>Quel indice pour mesurer l’efficacité en segmentation de textes?</title>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>171–180</pages>
      <abstract>L’évaluation de l’efficacité d’algorithmes de segmentation thématique est généralement effectuée en quantifiant le degré d’accord entre une segmentation hypothétique et une segmentation de référence. Les indices classiques de précision et de rappel étant peu adaptés à ce domaine, WindowDiff (Pevzner, Hearst, 2002) s’est imposé comme l’indice de référence. Une analyse de cet indice montre toutefois qu’il présente plusieurs limitations. L’objectif de ce rapport est d’évaluer un indice proposé par Bookstein, Kulyukin et Raita (2002), la distance de Hamming généralisée, qui est susceptible de remédier à celles-ci. Les analyses montrent que celui-ci conserve tous les avantages de WindowDiff sans les limitations. De plus, contrairement à WindowDiff, il présente une interprétation simple puisqu’il correspond à une vraie distance entre les deux segmentations à comparer.</abstract>
      <url hash="4cdaae39">2009.jeptalnrecital-long.18</url>
      <language>fra</language>
      <bibkey>bestgen-2009-quel</bibkey>
    </paper>
    <paper id="19">
      <title>Repérer automatiquement les segments obsolescents à l’aide d’indices sémantiques et discursifs</title>
      <author><first>Marion</first><last>Laignelet</last></author>
      <author><first>François</first><last>Rioult</last></author>
      <pages>181–190</pages>
      <abstract>Cet article vise la description et le repérage automatique des segments d’obsolescence dans les documents de type encyclopédique. Nous supposons que des indices sémantiques et discursifs peuvent permettre le repérage de tels segments. Pour ce faire, nous travaillons sur un corpus annoté manuellement par des experts sur lequel nous projetons des indices repérés automatiquement. Les techniques statistiques de base ne permettent pas d’expliquer ce phénomène complexe. Nous proposons l’utilisation de techniques de fouille de données pour le caractériser et nous évaluons le pouvoir prédictif de nos indices. Nous montrons, à l’aide de techniques de classification supervisée et de calcul de l’aire sous la courbe ROC, que nos hypothèses sont pertinentes.</abstract>
      <url hash="c04bdbc2">2009.jeptalnrecital-long.19</url>
      <language>fra</language>
      <bibkey>laignelet-rioult-2009-reperer</bibkey>
    </paper>
    <paper id="20">
      <title>Résumé automatique de textes d’opinions</title>
      <author><first>Michel</first><last>Généreux</last></author>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <pages>191–200</pages>
      <abstract>Le traitement des langues fait face à une demande croissante en matière d’analyse de textes véhiculant des critiques ou des opinions. Nous présentons ici un système de résumé automatique tourné vers l’analyse d’articles postés sur des blogues, où sont exprimées à la fois des informations factuelles et des prises de position sur les faits considérés. Nous montrons qu’une approche classique à base de traits de surface est tout à fait efficace dans ce cadre. Le système est évalué à travers une participation à la campagne d’évaluation internationale TAC (Text Analysis Conference) où notre système a réalisé des performances satisfaisantes.</abstract>
      <url hash="17abb98c">2009.jeptalnrecital-long.20</url>
      <language>fra</language>
      <bibkey>genereux-bossard-2009-resume</bibkey>
    </paper>
    <paper id="21">
      <title>Sens, synonymes et définitions</title>
      <author><first>Ingrid</first><last>Falk</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <author><first>Évelyne</first><last>Jacquey</last></author>
      <author><first>Fabienne</first><last>Venant</last></author>
      <pages>201–210</pages>
      <abstract>Cet article décrit une méthodologie visant la réalisation d’une ressource sémantique en français centrée sur la synonymie. De manière complémentaire aux travaux existants, la méthode proposée n’a pas seulement pour objectif d’établir des liens de synonymie entre lexèmes, mais également d’apparier les sens possibles d’un lexème avec les ensembles de synonymes appropriés. En pratique, les sens possibles des lexèmes proviennent des définitions du TLFi et les synonymes de cinq dictionnaires accessibles à l’ATILF. Pour évaluer la méthode d’appariement entre sens d’un lexème et ensemble de synonymes, une ressource de référence a été réalisée pour 27 verbes du français par quatre lexicographes qui ont spécifié manuellement l’association entre verbe, sens (définition TLFi) et ensemble de synonymes. Relativement à ce standard étalon, la méthode d’appariement affiche une F-mesure de 0.706 lorsque l’ensemble des paramètres est pris en compte, notamment la distinction pronominal / non-pronominal pour les verbes du français et de 0.602 sans cette distinction.</abstract>
      <url hash="11188613">2009.jeptalnrecital-long.21</url>
      <language>fra</language>
      <bibkey>falk-etal-2009-sens</bibkey>
    </paper>
    <paper id="22">
      <title>Vers des contraintes plus linguistiques en résolution de coréférences</title>
      <author><first>Étienne</first><last>Ailloud</last></author>
      <author><first>Manfred</first><last>Klenner</last></author>
      <pages>211–220</pages>
      <abstract>Nous proposons un modèle filtrant de résolution de coréférences basé sur les notions de transitivité et d’exclusivité linguistique. À partir de l’hypothèse générale que les chaînes de coréférence demeurent cohérentes tout au long d’un texte, notre modèle assure le respect de certaines contraintes linguistiques (via des filtres) quant à la coréférence, ce qui améliore la résolution globale. Le filtrage a lieu à différentes étapes de l’approche standard (c-à-d. par apprentissage automatique), y compris avant l’apprentissage et avant la classification, accélérant et améliorant ce processus.</abstract>
      <url hash="e3f21373">2009.jeptalnrecital-long.22</url>
      <language>fra</language>
      <bibkey>ailloud-klenner-2009-vers</bibkey>
    </paper>
    <paper id="23">
      <title>Trouver et confondre les coupables : un processus sophistiqué de correction de lexique</title>
      <author><first>Lionel</first><last>Nicolas</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Miguel</first><last>A. Molinero</last></author>
      <author><first>Jacques</first><last>Farré</last></author>
      <author><first>Éric</first><last>Villemonte De La Clergerie</last></author>
      <pages>221–230</pages>
      <abstract>La couverture d’un analyseur syntaxique dépend avant tout de la grammaire et du lexique sur lequel il repose. Le développement d’un lexique complet et précis est une tâche ardue et de longue haleine, surtout lorsque le lexique atteint un certain niveau de qualité et de couverture. Dans cet article, nous présentons un processus capable de détecter automatiquement les entrées manquantes ou incomplètes d’un lexique, et de suggérer des corrections pour ces entrées. La détection se réalise au moyen de deux techniques reposant soit sur un modèle statistique, soit sur les informations fournies par un étiqueteur syntaxique. Les hypothèses de corrections pour les entrées lexicales détectées sont générées en étudiant les modifications qui permettent d’améliorer le taux d’analyse des phrases dans lesquelles ces entrées apparaissent. Le processus global met en oeuvre plusieurs techniques utilisant divers outils tels que des étiqueteurs et des analyseurs syntaxiques ou des classifieurs d’entropie. Son application au Lefff , un lexique morphologique et syntaxique à large couverture du français, nous a déjà permis de réaliser des améliorations notables.</abstract>
      <url hash="6afd0f64">2009.jeptalnrecital-long.23</url>
      <language>fra</language>
      <bibkey>nicolas-etal-2009-trouver</bibkey>
    </paper>
    <paper id="24">
      <title>Un analyseur de surface non déterministe pour le français</title>
      <author><first>François</first><last>Trouilleux</last></author>
      <pages>231–240</pages>
      <abstract>Les analyseurs syntaxiques de surface à base de règles se caractérisent par un processus en deux temps : désambiguïsation lexicale, puis reconnaissance de patrons. Considérant que ces deux étapes introduisent une certaine redondance dans la description linguistique et une dilution des heuristiques dans les différents processus, nous proposons de définir un analyseur de surface qui fonctionne sur une entrée non désambiguïsée et produise l’ensemble des analyses possibles en termes de syntagmes noyau (chunks). L’analyseur, implanté avec NooJ, repose sur la définition de patrons étendus qui annotent des séquences de syntagmes noyau. Les résultats obtenus sur un corpus de développement d’environ 22 500 mots, avec un rappel proche de 100 %, montrent la faisabilité de l’approche et signalent quelques points d’ambiguïté à étudier plus particulièrement pour améliorer la précision.</abstract>
      <url hash="fb4ed76c">2009.jeptalnrecital-long.24</url>
      <language>fra</language>
      <bibkey>trouilleux-2009-un</bibkey>
    </paper>
    <paper id="25">
      <title>Une approche mixte-statistique et structurelle - pour le résumé automatique de dépêches</title>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <pages>241–250</pages>
      <abstract>Les techniques de résumé automatique multi-documents par extraction ont récemment évolué vers des méthodes statistiques pour la sélection des phrases à extraire. Dans cet article, nous présentons un système conforme à l’« état de l’art » — CBSEAS — que nous avons développé pour les tâches Opinion (résumés d’opinions issues de blogs) et Update (résumés de dépêches et mise à jour du résumé à partir de nouvelles dépêches sur le même événement) de la campagne d’évaluation TAC 2008, et montrons l’intérêt d’analyses structurelles et linguistiques des documents à résumer. Nous présentons également notre étude sur la structure des dépêches et l’impact de son intégration à CBSEAS.</abstract>
      <url hash="c8bc416e">2009.jeptalnrecital-long.25</url>
      <language>fra</language>
      <bibkey>bossard-2009-une</bibkey>
    </paper>
    <paper id="26">
      <title>Une expérience de fusion pour l’annotation d’entités nommées</title>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Nicolas</first><last>Dessaigne</last></author>
      <author><first>Maud</first><last>Ehrmann</last></author>
      <author><first>Baptiste</first><last>Gaillard</last></author>
      <author><first>Sylvie</first><last>Guillemin-Lanne</last></author>
      <author><first>Guillaume</first><last>Jacquet</last></author>
      <author><first>Aaron</first><last>Kaplan</last></author>
      <author><first>Marianna</first><last>Kucharski</last></author>
      <author><first>Claude</first><last>Martineau</last></author>
      <author><first>Aurélie</first><last>Migeotte</last></author>
      <author><first>Takuya</first><last>Nakamura</last></author>
      <author><first>Stavroula</first><last>Voyatzi</last></author>
      <pages>251–260</pages>
      <abstract>Nous présentons une expérience de fusion d’annotations d’entités nommées provenant de différents annotateurs. Ce travail a été réalisé dans le cadre du projet Infom@gic, projet visant à l’intégration et à la validation d’applications opérationnelles autour de l’ingénierie des connaissances et de l’analyse de l’information, et soutenu par le pôle de compétitivité Cap Digital « Image, MultiMédia et Vie Numérique ». Nous décrivons tout d’abord les quatre annotateurs d’entités nommées à l’origine de cette expérience. Chacun d’entre eux fournit des annotations d’entités conformes à une norme développée dans le cadre du projet Infom@gic. L’algorithme de fusion des annotations est ensuite présenté ; il permet de gérer la compatibilité entre annotations et de mettre en évidence les conflits, et ainsi de fournir des informations plus fiables. Nous concluons en présentant et interprétant les résultats de la fusion, obtenus sur un corpus de référence annoté manuellement.</abstract>
      <url hash="fccefcd2">2009.jeptalnrecital-long.26</url>
      <language>fra</language>
      <bibkey>brun-etal-2009-une</bibkey>
    </paper>
    <paper id="27">
      <title>Un système modulaire d’acquisition automatique de traductions à partir du Web</title>
      <author><first>Stéphanie</first><last>Léon</last></author>
      <pages>261–270</pages>
      <abstract>Nous présentons une méthode de Traduction Automatique d’Unités Lexicales Complexes (ULC) pour la construction de ressources bilingues français/anglais, basée sur un système modulaire qui prend en compte les propriétés linguistiques des unités sources (compositionnalité, polysémie, etc.). Notre système exploite les différentes « facettes » du Web multilingue pour valider des traductions candidates ou acquérir de nouvelles traductions. Après avoir collecté une base d’ULC en français à partir d’un corpus de pages Web, nous passons par trois phases de traduction qui s’appliquent à un cas linguistique, avec une méthode adaptée : les traductions compositionnelles non polysémiques, les traductions compositionnelles polysémiques et les traductions non compositionnelles et/ou inconnues. Notre évaluation sur un vaste échantillon d’ULC montre que l’exploitation du Web pour la traduction et la prise en compte des propriétés linguistiques au sein d’un système modulaire permet une acquisition automatique de traductions avec une excellente précision.</abstract>
      <url hash="0a6f3d09">2009.jeptalnrecital-long.27</url>
      <language>fra</language>
      <bibkey>leon-2009-un</bibkey>
    </paper>
    <paper id="28">
      <title>Des relations d’alignement pour décrire l’interaction des domaines linguistiques : vers des Grammaires Multimodales</title>
      <author><first>Philippe</first><last>Blache</last></author>
      <pages>271–280</pages>
      <abstract>Un des problèmes majeurs de la linguistique aujourd’hui réside dans la prise en compte de phénomènes relevant de domaines et de modalités différentes. Dans la littérature, la réponse consiste à représenter les relations pouvant exister entre ces domaines de façon externe, en termes de relation de structure à structure, s’appuyant donc sur une description distincte de chaque domaine ou chaque modalité. Nous proposons dans cet article une approche différente permettant représenter ces phénomènes dans un cadre formel unique, permettant de rendre compte au sein d’une même grammaire tous les phénomènes concernés. Cette représentation précise de l’interaction entre domaines et modalités s’appuie sur la définition de relations d’alignement.</abstract>
      <url hash="0a0ca915">2009.jeptalnrecital-long.28</url>
      <language>fra</language>
      <bibkey>blache-2009-des</bibkey>
    </paper>
    <paper id="29">
      <title>Vers une méthodologie d’annotation des entités nommées en corpus ?</title>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Maud</first><last>Ehrmann</last></author>
      <author><first>Adeline</first><last>Nazarenko</last></author>
      <pages>281–290</pages>
      <abstract>La tâche, aujourd’hui considérée comme fondamentale, de reconnaissance d’entités nommées, présente des difficultés spécifiques en matière d’annotation. Nous les précisons ici, en les illustrant par des expériences d’annotation manuelle dans le domaine de la microbiologie. Ces problèmes nous amènent à reposer la question fondamentale de ce que les annotateurs doivent annoter et surtout, pour quoi faire. Nous identifions pour cela les applications nécessitant l’extraction d’entités nommées et, en fonction des besoins de ces applications, nous proposons de définir sémantiquement les éléments à annoter. Nous présentons ensuite un certain nombre de recommandations méthodologiques permettant d’assurer un cadre d’annotation cohérent et évaluable.</abstract>
      <url hash="8bc1dba4">2009.jeptalnrecital-long.29</url>
      <language>fra</language>
      <bibkey>fort-etal-2009-vers</bibkey>
    </paper>
  </volume>
  <volume id="position" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles. Prise de position</booktitle>
      <editor><first>Adeline</first><last>Nazarenko</last></editor>
      <editor><first>Thierry</first><last>Poibeau</last></editor>
      <publisher>ATALA</publisher>
      <address>Senlis, France</address>
      <month>June</month>
      <year>2009</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="65611a06">2009.jeptalnrecital-position.0</url>
      <bibkey>jep-taln-recital-2009-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Données bilingues pour la <fixed-case>TAS</fixed-case> français-anglais : impact de la langue source et direction de traduction originales sur la qualité de la traduction</title>
      <author><first>Sylwia</first><last>Ozdowska</last></author>
      <pages>1–6</pages>
      <abstract>Dans cet article, nous prenons position par rapport à la question de la qualité des données bilingues destinées à la traduction automatique statistique en terme de langue source et direction de traduction originales à l’égard d’une tâche de traduction français-anglais. Nous montrons que l’entraînement sur un corpus contenant des textes qui ont été à l’origine traduits du français vers l’anglais améliore la qualité de la traduction. Inversement, l’entraînement sur un corpus contenant exclusivement des textes dont la langue source originale n’est ni le français ni l’anglais dégrade la traduction.</abstract>
      <url hash="6e065cdb">2009.jeptalnrecital-position.1</url>
      <language>fra</language>
      <bibkey>ozdowska-2009-donnees</bibkey>
    </paper>
    <paper id="2">
      <title>La place de la désambiguïsation lexicale dans la Traduction Automatique Statistique</title>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <pages>7–12</pages>
      <abstract>L’étape de la désambiguïsation lexicale est souvent esquivée dans les systèmes de Traduction Automatique Statistique (Statistical Machine Translation (SMT)) car considérée comme non nécessaire à la sélection de traductions correctes. Le débat autour de cette nécessité est actuellement assez vif. Dans cet article, nous présentons les principales positions sur le sujet. Nous analysons les avantages et les inconvénients de la conception actuelle de la désambiguïsation dans le cadre de la SMT, d’après laquelle les sens des mots correspondent à leurs traductions dans des corpus parallèles. Ensuite, nous présentons des arguments en faveur d’une analyse plus poussée des informations sémantiques induites à partir de corpus parallèles et nous expliquons comment les résultats d’une telle analyse pourraient être exploités pour une évaluation plus flexible et concluante de l’impact de la désambiguïsation dans la SMT.</abstract>
      <url hash="57fc0bba">2009.jeptalnrecital-position.2</url>
      <language>fra</language>
      <bibkey>apidianaki-2009-la</bibkey>
    </paper>
    <paper id="3">
      <title>Nouveau paradigme d’évaluation des systèmes de dialogue homme-machine</title>
      <author><first>Marianne</first><last>Laurent</last></author>
      <author><first>Ghislain</first><last>Putois</last></author>
      <author><first>Philippe</first><last>Bretier</last></author>
      <author><first>Thierry</first><last>Moudenc</last></author>
      <pages>13–18</pages>
      <abstract>L’évaluation des systèmes de dialogue homme-machine est un problème difficile et pour lequel ni les objectifs ni les solutions proposées ne font aujourd’hui l’unanimité. Les approches ergonomiques traditionnelles soumettent le système de dialogue au regard critique de l’utilisateur et tente d’en capter l’expression, mais l’absence d’un cadre objectivable des usages de ces utilisateurs empêche une comparaison entre systèmes différents, ou entre évolutions d’un même système. Nous proposons d’inverser cette vision et de mesurer le comportement de l’utilisateur au regard du système de dialogue. Aussi, au lieu d’évaluer l’adéquation du système à ses utilisateurs, nous mesurons l’adéquation des utilisateurs au système. Ce changement de paradigme permet un changement de référentiel qui n’est plus les usages des utilisateurs mais le cadre du système. Puisque le système est complètement défini, ce paradigme permet des approches quantitatives et donc des évaluations comparatives de systèmes.</abstract>
      <url hash="24a427bb">2009.jeptalnrecital-position.3</url>
      <language>fra</language>
      <bibkey>laurent-etal-2009-nouveau</bibkey>
    </paper>
  </volume>
  <volume id="court" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles. Articles courts</booktitle>
      <editor><first>Adeline</first><last>Nazarenko</last></editor>
      <editor><first>Thierry</first><last>Poibeau</last></editor>
      <publisher>ATALA</publisher>
      <address>Senlis, France</address>
      <month>June</month>
      <year>2009</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="367d001f">2009.jeptalnrecital-court.0</url>
      <bibkey>jep-taln-recital-2009-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Adaptation de parsers statistiques lexicalisés pour le français : Une évaluation complète sur corpus arborés</title>
      <author><first>Djamé</first><last>Seddah</last></author>
      <author><first>Marie</first><last>Candito</last></author>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <pages>1–10</pages>
      <abstract>Cet article présente les résultats d’une évaluation exhaustive des principaux analyseurs syntaxiques probabilistes dit “lexicalisés” initialement conçus pour l’anglais, adaptés pour le français et évalués sur le CORPUS ARBORÉ DU FRANÇAIS (Abeillé et al., 2003) et le MODIFIED FRENCH TREEBANK (Schluter &amp; van Genabith, 2007). Confirmant les résultats de (Crabbé &amp; Candito, 2008), nous montrons que les modèles lexicalisés, à travers les modèles de Charniak (Charniak, 2000), ceux de Collins (Collins, 1999) et le modèle des TIG Stochastiques (Chiang, 2000), présentent des performances moindres face à un analyseur PCFG à Annotation Latente (Petrov et al., 2006). De plus, nous montrons que le choix d’un jeu d’annotations issus de tel ou tel treebank oriente fortement les résultats d’évaluations tant en constituance qu’en dépendance non typée. Comparés à (Schluter &amp; van Genabith, 2008; Arun &amp; Keller, 2005), tous nos résultats sont state-of-the-art et infirment l’hypothèse d’une difficulté particulière qu’aurait le français en terme d’analyse syntaxique probabiliste et de sources de données.</abstract>
      <url hash="d900dfd5">2009.jeptalnrecital-court.1</url>
      <language>fra</language>
      <bibkey>seddah-etal-2009-adaptation</bibkey>
    </paper>
    <paper id="2">
      <title>Analyse automatique des noms déverbaux composés : pourquoi et comment faire interagir analogie et système de règles</title>
      <author><first>Fiammetta</first><last>Namer</last></author>
      <pages>11–20</pages>
      <abstract>Cet article aborde deux problèmes d’analyse morpho-sémantique du lexique : (1) attribuer automatiquement une définition à des noms et verbes morphologiquement construits inconnus des dictionnaires mais présents dans les textes ; (2) proposer une analyse combinant règles et analogie, deux techniques généralement contradictoires. Les noms analysés sont apparemment suffixés et composés (HYDROMASSAGE). La plupart d’entre eux, massivement attestés dans les documents (journaux, Internet) sont absents des dictionnaires. Ils sont souvent reliés à des verbes (HYDROMASSER) également néologiques. Le nombre de ces noms et verbes est estimé à 5.400. L’analyse proposée leur attribue une définition par rapport à leur base, et enrichit un lexique de référence pour le TALN au moyen de cette base, si elle est néologique. L’implémentation des contraintes linguistiques qui régissent ces formations est reproductible dans d’autres langues européennes où sont rencontrés les mêmes types de données dont l’analyse reflète le même raisonnement que pour le français.</abstract>
      <url hash="524a187f">2009.jeptalnrecital-court.2</url>
      <language>fra</language>
      <bibkey>namer-2009-analyse</bibkey>
    </paper>
    <paper id="3">
      <title>Analyse en dépendances à l’aide des grammaires d’interaction</title>
      <author><first>Jonathan</first><last>Marchand</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Guy</first><last>Perrier</last></author>
      <pages>21–30</pages>
      <abstract>Cet article propose une méthode pour extraire une analyse en dépendances d’un énoncé à partir de son analyse en constituants avec les grammaires d’interaction. Les grammaires d’interaction sont un formalisme grammatical qui exprime l’interaction entre les mots à l’aide d’un système de polarités. Le mécanisme de composition syntaxique est régi par la saturation des polarités. Les interactions s’effectuent entre les constituants, mais les grammaires étant lexicalisées, ces interactions peuvent se traduire sur les mots. La saturation des polarités lors de l’analyse syntaxique d’un énoncé permet d’extraire des relations de dépendances entre les mots, chaque dépendance étant réalisée par une saturation. Les structures de dépendances ainsi obtenues peuvent être vues comme un raffinement de l’analyse habituellement effectuée sous forme d’arbre de dépendance. Plus généralement, ce travail apporte un éclairage nouveau sur les liens entre analyse en constituants et analyse en dépendances.</abstract>
      <url hash="9e1e144b">2009.jeptalnrecital-court.3</url>
      <language>fra</language>
      <bibkey>marchand-etal-2009-analyse</bibkey>
    </paper>
    <paper id="4">
      <title>Analyse relâchée à base de contraintes</title>
      <author><first>Jean-Philippe</first><last>Prost</last></author>
      <pages>31–40</pages>
      <abstract>La question de la grammaticalité, et celle duale de l’agrammaticalité, sont des sujets délicats à aborder, dès lors que l’on souhaite intégrer différents degrés, tant de grammaticalité que d’agrammaticalité. En termes d’analyse automatique, les problèmes posés sont de l’ordre de la représentation des connaissances, du traitement, et bien évidement de l’évaluation. Dans cet article, nous nous concentrons sur l’aspect traitement, et nous nous penchons sur la question de l’analyse d’énoncés agrammaticaux. Nous explorons la possibilité de fournir une analyse la plus complète possible pour un énoncé agrammatical, sans l’apport d’information complémentaire telle que par le biais de mal-règles ou autre grammaire d’erreurs. Nous proposons une solution algorithmique qui permet l’analyse automatique d’un énoncé agrammatical, sur la seule base d’une grammaire modèle-théorique de bonne formation. Cet analyseur est prouvé générer une solution optimale, selon un critère numérique maximisé.</abstract>
      <url hash="8843e368">2009.jeptalnrecital-court.4</url>
      <language>fra</language>
      <bibkey>prost-2009-analyse</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>ANNODIS</fixed-case>: une approche outillée de l’annotation de structures discursives</title>
      <author><first>Marie-Paule</first><last>Péry-Woodley</last></author>
      <author><first>Nicholas</first><last>Asher</last></author>
      <author><first>Patrice</first><last>Enjalbert</last></author>
      <author><first>Farah</first><last>Benamara</last></author>
      <author><first>Myriam</first><last>Bras</last></author>
      <author><first>Cécile</first><last>Fabre</last></author>
      <author><first>Stéphane</first><last>Ferrari</last></author>
      <author><first>Lydia-Mai</first><last>Ho-Dac</last></author>
      <author><first>Anne</first><last>Le Draoulec</last></author>
      <author><first>Yann</first><last>Mathet</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Laurent</first><last>Prévot</last></author>
      <author><first>Josette</first><last>Rebeyrolle</last></author>
      <author><first>Ludovic</first><last>Tanguy</last></author>
      <author><first>Marianne</first><last>Vergez-Couret</last></author>
      <author><first>Laure</first><last>Vieu</last></author>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <pages>41–46</pages>
      <abstract>Le projet ANNODIS vise la construction d’un corpus de textes annotés au niveau discursif ainsi que le développement d’outils pour l’annotation et l’exploitation de corpus. Les annotations adoptent deux points de vue complémentaires : une perspective ascendante part d’unités de discours minimales pour construire des structures complexes via un jeu de relations de discours ; une perspective descendante aborde le texte dans son entier et se base sur des indices pré-identifiés pour détecter des structures discursives de haut niveau. La construction du corpus est associée à la création de deux interfaces : la première assiste l’annotation manuelle des relations et structures discursives en permettant une visualisation du marquage issu des prétraitements ; une seconde sera destinée à l’exploitation des annotations. Nous présentons les modèles et protocoles d’annotation élaborés pour mettre en oeuvre, au travers de l’interface dédiée, la campagne d’annotation.</abstract>
      <url hash="53042ffb">2009.jeptalnrecital-court.5</url>
      <language>fra</language>
      <bibkey>pery-woodley-etal-2009-annodis</bibkey>
    </paper>
    <paper id="6">
      <title>Apport de la syntaxe dans un système de question-réponse : étude du système <fixed-case>FIDJI</fixed-case>.</title>
      <author><first>Véronique</first><last>Moriceau</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <pages>47–56</pages>
      <abstract>Cet article présente une série d’évaluations visant à étudier l’apport d’une analyse syntaxique robuste des questions et des documents dans un système de questions-réponses. Ces évaluations ont été effectuées sur le système FIDJI, qui utilise à la fois des informations syntaxiques et des techniques plus “traditionnelles”. La sélection des documents, l’extraction de la réponse ainsi que le comportement selon les différents types de questions ont été étudiés.</abstract>
      <url hash="3eb36d14">2009.jeptalnrecital-court.6</url>
      <language>fra</language>
      <bibkey>moriceau-tannier-2009-apport</bibkey>
    </paper>
    <paper id="7">
      <title>Apport des cooccurrences à la correction et à l’analyse syntaxique</title>
      <author><first>Dominique</first><last>Laurent</last></author>
      <author><first>Sophie</first><last>Nègre</last></author>
      <author><first>Patrick</first><last>Séguéla</last></author>
      <pages>57–66</pages>
      <abstract>Le correcteur grammatical Cordial utilise depuis de nombreuses années les cooccurrences pour la désambiguïsation sémantique. Un dictionnaire de cooccurrences ayant été constitué pour les utilisateurs du logiciel de correction et d’aides à la rédaction, la grande richesse de ce dictionnaire a incité à l’utiliser intensivement pour la correction, spécialement des homonymes et paronymes. Les résultats obtenus sont spectaculaires sur ces types d’erreurs mais la prise en compte des cooccurrences a également été utilisée avec profit pour la pure correction orthographique et pour le rattachement des groupes en analyse syntaxique.</abstract>
      <url hash="aa4657cd">2009.jeptalnrecital-court.7</url>
      <language>fra</language>
      <bibkey>laurent-etal-2009-apport</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>A</fixed-case>rabic Disambiguation Using Dependency Grammar</title>
      <author><first>Daoud</first><last>Daoud</last></author>
      <author><first>Mohammad</first><last>Daoud</last></author>
      <pages>67–76</pages>
      <abstract>In this paper, we present a new approach to disambiguation Arabic using a joint rule-based model which is conceptualized using Dependency Grammar. This approach helps in highly accurate analysis of sentences. The analysis produces a semantic net like structure expressed by means of Universal Networking Language (UNL) - a recently proposed interlingua. Extremely varied and complex phenomena of Arabic language have been addressed.</abstract>
      <url hash="7007fa3f">2009.jeptalnrecital-court.8</url>
      <bibkey>daoud-daoud-2009-arabic</bibkey>
    </paper>
    <paper id="9">
      <title>Association automatique de lemmes et de paradigmes de flexion à un mot inconnu</title>
      <author><first>Claude</first><last>De Loupy</last></author>
      <author><first>Michaël</first><last>Bagur</last></author>
      <author><first>Helena</first><last>Blancafort</last></author>
      <pages>77–86</pages>
      <abstract>La maintenance et l’enrichissement des lexiques morphosyntaxiques sont souvent des tâches fastidieuses. Dans cet article nous présentons la mise en place d’une procédure de guessing de flexion afin d’aider les linguistes dans leur travail de lexicographes. Le guesser développé ne fait pas qu’évaluer l’étiquette morphosyntaxique comme c’est généralement le cas. Il propose pour un mot français inconnu, un ou plusieurs candidats-lemmes, ainsi que les paradigmes de flexion associés (formes fléchies et étiquettes morphosyntaxiques). Dans cet article, nous décrivons le modèle probabiliste utilisé ainsi que les résultats obtenus. La méthode utilisée permet de réduire considérablement le nombre de règles à valider, permettant ainsi un gain de temps important.</abstract>
      <url hash="647348f8">2009.jeptalnrecital-court.9</url>
      <language>fra</language>
      <bibkey>de-loupy-etal-2009-association</bibkey>
    </paper>
    <paper id="10">
      <title>Catégorisation sémantico-discursive des évaluations exprimées dans la blogosphère</title>
      <author><first>Matthieu</first><last>Vernier</last></author>
      <author><first>Laura</first><last>Monceaux</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <author><first>Estelle</first><last>Dubreil</last></author>
      <pages>87–96</pages>
      <abstract>Les blogs constituent un support d’observations idéal pour des applications liées à la fouille d’opinion. Toutefois, ils imposent de nouvelles problématiques et de nouveaux défis au regard des méthodes traditionnelles du domaine. De ce fait, nous proposons une méthode automatique pour la détection et la catégorisation des évaluations localement exprimées dans un corpus de blogs multi-domaine. Celle-ci rend compte des spécificités du langage évaluatif décrites dans deux théories linguistiques. L’outil développé au sein de la plateforme UIMA vise d’une part à construire automatiquement une grammaire du langage évaluatif, et d’autre part à utiliser cette grammaire pour la détection et la catégorisation des passages évaluatifs d’un texte. La catégorisation traite en particulier l’aspect axiologique de l’évaluation, sa configuration d’énonciation et sa modalité dans le discours.</abstract>
      <url hash="513e8f38">2009.jeptalnrecital-court.10</url>
      <language>fra</language>
      <bibkey>vernier-etal-2009-categorisation</bibkey>
    </paper>
    <paper id="11">
      <title>Chaîne de traitement linguistique : du repérage d’expressions temporelles au peuplement d’une ontologie de tourisme</title>
      <author><first>Stéphanie</first><last>Weiser</last></author>
      <author><first>Martin</first><last>Coste</last></author>
      <author><first>Florence</first><last>Amardeilh</last></author>
      <pages>97–106</pages>
      <abstract>Cet article présente la chaîne de traitement linguistique réalisée pour la mise en place d’une plateforme touristique sur Internet. Les premières étapes de cette chaîne sont le repérage et l’annotation des expressions temporelles présentes dans des pages Web. Ces deux tâches sont effectuées à l’aide de patrons linguistiques. Elles soulèvent de nombreux questionnements auxquels nous tentons de répondre, notamment au sujet de la définition des informations à extraire, du format d’annotation et des contraintes. L’étape suivante consiste en l’exploitation des données annotées pour le peuplement d’une ontologie du tourisme. Nous présentons les règles d’acquisition nécessaires pour alimenter la base de connaissance du projet. Enfin, nous exposons une évaluation du système d’annotation. Cette évaluation permet de juger aussi bien le repérage des expressions temporelles que leur annotation.</abstract>
      <url hash="0296a87b">2009.jeptalnrecital-court.11</url>
      <language>fra</language>
      <bibkey>weiser-etal-2009-chaine</bibkey>
    </paper>
    <paper id="12">
      <title>Détection des contradictions dans les annotations sémantiques</title>
      <author><first>Yue</first><last>Ma</last></author>
      <author><first>Laurent</first><last>Audibert</last></author>
      <pages>107–112</pages>
      <abstract>L’annotation sémantique a pour objectif d’apporter au texte une représentation explicite de son interprétation sémantique. Dans un précédent article, nous avons proposé d’étendre les ontologies par des règles d’annotation sémantique. Ces règles sont utilisées pour l’annotation sémantique d’un texte au regard d’une ontologie dans le cadre d’une plate-forme d’annotation linguistique automatique. Nous présentons dans cet article une mesure, basée sur la valeur de Shapley, permettant d’identifier les règles qui sont sources de contradiction dans l’annotation sémantique. Par rapport aux classiques mesures de précision et de rappel, l’intérêt de cette mesure est de ne pas nécessiter de corpus manuellement annoté, d’être entièrement automatisable et de permettre l’identification des règles qui posent problème.</abstract>
      <url hash="760b824b">2009.jeptalnrecital-court.12</url>
      <language>fra</language>
      <bibkey>ma-audibert-2009-detection</bibkey>
    </paper>
    <paper id="13">
      <title>Détection des émotions à partir du contenu linguistique d’énoncés oraux : application à un robot compagnon pour enfants fragilisés</title>
      <author><first>Marc</first><last>Le Tallec</last></author>
      <author><first>Jeanne</first><last>Villaneau</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Agata</first><last>Savary</last></author>
      <author><first>Arielle</first><last>Syssau-Vaccarella</last></author>
      <pages>113–119</pages>
      <abstract>Le projet ANR Emotirob aborde la question de la détection des émotions sous un cadre original : concevoir un robot compagnon émotionnel pour enfants fragilisés. Notre approche consiste à combiner détection linguistique et prosodie. Nos expériences montrent qu’un sujet humain peut estimer de manière fiable la valence émotionnelle d’un énoncé à partir de son contenu propositionnel. Nous avons donc développé un premier modèle de détection linguistique qui repose sur le principe de compositionnalité des émotions : les mots simples ont une valence émotionnelle donnée et les prédicats modifient la valence de leurs arguments. Après une description succincte du système logique de compréhension dont les sorties sont utilisées pour le calcul global de l’émotion, cet article présente la construction d’une norme émotionnelle lexicale de référence, ainsi que d’une ontologie de classes émotionnelles de prédicats, pour des enfants de 5 et 7 ans.</abstract>
      <url hash="203b07aa">2009.jeptalnrecital-court.13</url>
      <language>fra</language>
      <bibkey>le-tallec-etal-2009-detection</bibkey>
    </paper>
    <paper id="14">
      <title>Dispersion sémantique dans des familles morpho-phonologiques : éléments théoriques et empiriques</title>
      <author><first>Nuria</first><last>Gala</last></author>
      <author><first>Véronique</first><last>Rey</last></author>
      <author><first>Laurent</first><last>Tichit</last></author>
      <pages>120–127</pages>
      <abstract>Traditionnellement, la morphologie lexicale a été diachronique et a permis de proposer le concept de famille de mots. Ce dernier est repris dans les études en synchronie et repose sur une forte cohérence sémantique entre les mots d’une même famille. Dans cet article, nous proposons une approche en synchronie fondée sur la notion de continuité à la fois phonologique et sémantique. Nous nous intéressons, d’une part, à la morpho-phonologie et, d’autre part, à la dispersion sémantique des mots dans les familles. Une première étude (Gala &amp; Rey, 2008) montrait que les familles de mots obtenues présentaient des espaces sémantiques soit de grande cohésion soit de grande dispersion. Afin de valider ces observations, nous présentons ici une méthode empirique qui permet de pondérer automatiquement les unités de sens d’un mot et d’une famille. Une expérience menée auprès de 30 locuteurs natifs valide notre approche et ouvre la voie pour une étude approfondie du lexique sur ces bases phonologiques et sémantiques.</abstract>
      <url hash="f51a7f6f">2009.jeptalnrecital-court.14</url>
      <language>fra</language>
      <bibkey>gala-etal-2009-dispersion</bibkey>
    </paper>
    <paper id="15">
      <title>Exploitation d’une structure pour les questions enchaînées</title>
      <author><first>Kévin</first><last>Séjourné</last></author>
      <pages>128–137</pages>
      <abstract>Nous présentons des travaux réalisés dans le domaine des systèmes de questions réponses (SQR) utilisant des questions enchaînées. La recherche des documents dans un SQR est perturbée par l’absence des éléments utiles à la recherche dans les questions liées, éléments figurant dans les échanges précédents. Les récentes campagnes d’évaluation montrent que ce problème est sous-estimé, et n’a pas fait l’objet de technique dédiée. Afin d’améliorer la recherche des documents dans un SQR nous utilisons une méthode récente d’organisation des informations liées aux interactions entre questions. Celle-ci se base sur l’exploitation d’une structure de données adaptée à la transmission des informations des questions liées jusqu’au moteur d’interrogation. Le moteur d’interrogation doit alors être adapté afin de tirer partie de cette structure de données.</abstract>
      <url hash="d0973f82">2009.jeptalnrecital-court.15</url>
      <language>fra</language>
      <bibkey>sejourne-2009-exploitation</bibkey>
    </paper>
    <paper id="16">
      <title>Exploitation du terrain commun pour la production d’expressions référentielles dans les systèmes de dialogue</title>
      <author><first>Alexandre</first><last>Denis</last></author>
      <author><first>Matthieu</first><last>Quignard</last></author>
      <pages>138–147</pages>
      <abstract>Cet article présente un moyen de contraindre la production d’expressions référentielles par un système de dialogue en fonction du terrain commun. Cette capacité, fondamentale pour atteindre la compréhension mutuelle, est trop souvent oubliée dans les systèmes de dialogue. Le modèle que nous proposons s’appuie sur une modélisation du processus d’ancrage (grounding process) en proposant un raffinement du statut d’ancrage appliqué à la description des référents. Il décrit quand et comment ce statut doit être révisé en fonction des jugements de compréhension des deux participants ainsi que son influence dans le choix d’une description partagée destinée à la génération d’une expression référentielle.</abstract>
      <url hash="474cc819">2009.jeptalnrecital-court.16</url>
      <language>fra</language>
      <bibkey>denis-quignard-2009-exploitation</bibkey>
    </paper>
    <paper id="17">
      <title>Gestion de dialogue oral Homme-machine en arabe</title>
      <author><first>Younès</first><last>Bahou</last></author>
      <author><first>Amine</first><last>Bayoudhi</last></author>
      <author><first>Lamia</first><last>Hadrich Belguith</last></author>
      <pages>148–157</pages>
      <abstract>Dans le présent papier, nous présentons nos travaux sur la gestion du dialogue oral arabe Homme-machine. Ces travaux entrent dans le cadre de la réalisation du serveur vocal interactif SARF (Bahou et al., 2008) offrant des renseignements sur le transport ferroviaire tunisien en langue arabe standard moderne. Le gestionnaire de dialogue que nous proposons est basé sur une approche structurelle et est composé de deux modèles à savoir, le modèle de tâche et le modèle de dialogue. Le premier modèle permet de i) compléter et vérifier l’incohérence des structures sémantiques représentant les sens utiles des énoncés, ii) générer une requête vers l’application et iii) récupérer le résultat et de formuler une réponse à l’utilisateur en langage naturel. Quant au modèle de dialogue, il assure l’avancement du dialogue avec l’utilisateur et l’identification de ses intentions. L’interaction entre ces deux modèles est assurée grâce à un contexte du dialogue permettant le suivi et la mise à jour de l’historique du dialogue.</abstract>
      <url hash="299608cd">2009.jeptalnrecital-court.17</url>
      <language>fra</language>
      <bibkey>bahou-etal-2009-gestion</bibkey>
    </paper>
    <paper id="18">
      <title>Grammaires d’erreur – correction grammaticale avec analyse profonde et proposition de corrections minimales</title>
      <author><first>Lionel</first><last>Clément</last></author>
      <author><first>Kim</first><last>Gerdes</last></author>
      <author><first>Renaud</first><last>Marlet</last></author>
      <pages>158–167</pages>
      <abstract>Nous présentons un système de correction grammatical ouvert, basé sur des analyses syntaxiques profondes. La spécification grammaticale est une grammaire hors-contexte équipée de structures de traits plates. Après une analyse en forêt partagée où les contraintes d’accord de traits sont relâchées, la détection d’erreur minimise globalement les corrections à effectuer et des phrases alternatives correctes sont automatiquement proposées.</abstract>
      <url hash="c38e436b">2009.jeptalnrecital-court.18</url>
      <language>fra</language>
      <bibkey>clement-etal-2009-grammaires</bibkey>
    </paper>
    <paper id="19">
      <title>Intégration des constructions à verbe support dans <fixed-case>T</fixed-case>ime<fixed-case>ML</fixed-case></title>
      <author><first>André</first><last>Bittar</last></author>
      <author><first>Laurence</first><last>Danlos</last></author>
      <pages>168–176</pages>
      <abstract>Le langage TimeML a été conçu pour l’annotation des informations temporelles dans les textes, notamment les événements, les expressions de temps et les relations entre les deux. Des consignes d’annotation générales ont été élaborées afin de guider l’annotateur dans cette tâche, mais certains phénomènes linguistiques restent à traiter en détail. Un problème commun dans les tâches de TAL, que ce soit en traduction, en génération ou en compréhension, est celui de l’encodage des constructions à verbe support. Relativement peu d’attention a été portée, jusqu’à maintenant, sur ce problème dans le cadre du langage TimeML. Dans cet article, nous proposons des consignes d’annotation pour les constructions à verbe support.</abstract>
      <url hash="365686a9">2009.jeptalnrecital-court.19</url>
      <language>fra</language>
      <bibkey>bittar-danlos-2009-integration</bibkey>
    </paper>
    <paper id="20">
      <title>Intégrer les tables du Lexique-Grammaire à un analyseur syntaxique robuste à grande échelle</title>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Elsa</first><last>Tolone</last></author>
      <pages>177–186</pages>
      <abstract>Dans cet article, nous montrons comment nous avons converti les tables du Lexique-Grammaire en un format TAL, celui du lexique Lefff, permettant ainsi son intégration dans l’analyseur syntaxique FRMG. Nous présentons les fondements linguistiques de ce processus de conversion et le lexique obtenu. Nous validons le lexique obtenu en évaluant l’analyseur syntaxique FRMG sur le corpus de référence de la campagne EASy selon qu’il utilise les entrées verbales du Lefff ou celles des tables des verbes du Lexique-Grammaire ainsi converties.</abstract>
      <url hash="9cf2d63f">2009.jeptalnrecital-court.20</url>
      <language>fra</language>
      <bibkey>sagot-tolone-2009-integrer</bibkey>
    </paper>
    <paper id="21">
      <title>La complémentarité des approches manuelle et automatique en acquisition lexicale</title>
      <author><first>Cédric</first><last>Messiant</last></author>
      <author><first>Takuya</first><last>Nakamura</last></author>
      <author><first>Stavroula</first><last>Voyatzi</last></author>
      <pages>187–196</pages>
      <abstract>Les ressources lexicales sont essentielles pour obtenir des systèmes de traitement des langues performants. Ces ressources peuvent être soit construites à la main, soit acquises automatiquement à partir de gros corpus. Dans cet article, nous montrons la complémentarité de ces deux approches. Pour ce faire, nous utilisons l’exemple de la sous-catégorisation verbale en comparant un lexique acquis par des méthodes automatiques (LexSchem) avec un lexique construit manuellement (Le Lexique-Grammaire). Nous montrons que les informations acquises par ces deux méthodes sont bien distinctes et qu’elles peuvent s’enrichir mutuellement.</abstract>
      <url hash="c3f4b082">2009.jeptalnrecital-court.21</url>
      <language>fra</language>
      <bibkey>messiant-etal-2009-la</bibkey>
    </paper>
    <paper id="22">
      <title>L’analyseur syntaxique Cordial dans Passage</title>
      <author><first>Dominique</first><last>Laurent</last></author>
      <author><first>Sophie</first><last>Nègre</last></author>
      <author><first>Patrick</first><last>Séguéla</last></author>
      <pages>197–206</pages>
      <abstract>Cordial est un analyseur syntaxique et sémantique développé par la société Synapse Développement. Largement utilisé par les laboratoires de TALN depuis plus de dix ans, cet analyseur participe à la campagne Passage (“Produire des Annotations Syntaxiques à Grande Échelle”). Comment fonctionne cet analyseur ? Quels résultats a-t-il obtenu lors de la première phase d’évaluation de cette campagne ? Au-delà de ces questions, cet article montre en quoi les contraintes industrielles façonnent les outils d’analyse automatique du langage naturel.</abstract>
      <url hash="fb4ae0cd">2009.jeptalnrecital-court.22</url>
      <language>fra</language>
      <bibkey>laurent-etal-2009-lanalyseur</bibkey>
    </paper>
    <paper id="23">
      <title>La plate-forme Glozz : environnement d’annotation et d’exploration de corpus</title>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <author><first>Yann</first><last>Mathet</last></author>
      <pages>207–216</pages>
      <abstract>La nécessité d’une interaction systématique entre modèles, traitements et corpus impose la disponibilité d’annotations de référence auxquelles modèles et traitements pourront être confrontés. Or l’établissement de telles annotations requiert un cadre formel permettant la représentation d’objets linguistiques variés, et des applications permettant à l’annotateur de localiser sur corpus et de caractériser les occurrences des phénomènes observés. Si différents outils d’annotation ont vu le jour, ils demeurent souvent fortement liés à un modèle théorique et à des objets linguistiques particuliers, et ne permettent que marginalement d’explorer certaines structures plus récemment appréhendées expérimentalement, notamment à granularité élevée et en matière d’analyse du discours. La plate-forme Glozz répond à ces différentes contraintes et propose un environnement d’exploration de corpus et d’annotation fortement configurable et non limité a priori au contexte discursif dans lequel elle a initialement vu le jour.</abstract>
      <url hash="3b7ff73c">2009.jeptalnrecital-court.23</url>
      <language>fra</language>
      <bibkey>widlocher-mathet-2009-la</bibkey>
    </paper>
    <paper id="24">
      <title>Morfetik, ressource lexicale pour le <fixed-case>TAL</fixed-case></title>
      <author><first>Pierre-André</first><last>Buvet</last></author>
      <author><first>Emmanuel</first><last>Cartier</last></author>
      <author><first>Fabrice</first><last>Issac</last></author>
      <author><first>Yassine</first><last>Madiouni</last></author>
      <author><first>Michel</first><last>Mathieu-Colas</last></author>
      <author><first>Salah</first><last>Mejri</last></author>
      <pages>217–226</pages>
      <abstract>Le traitement automatique des langues exige un recensement lexical aussi rigoureux que possible. Dans ce but, nous avons développé un dictionnaire morphologique du français, conçu comme le point de départ d’un système modulaire (Morfetik) incluant un moteur de flexion, des interfaces de consultation et d’interrogation et des outils d’exploitation. Nous présentons dans cet article, après une brève description du dictionnaire de base (lexique des mots simples), quelques-uns des outils informatiques liés à cette ressource : un moteur de recherche des lemmes et des formes fléchies ; un moteur de flexion XML et MySQL ; des outils NLP permettant d’exploiter le dictionnaire ainsi généré ; nous présentons notamment un analyseur linguistique développé dans notre laboratoire. Nous comparons dans une dernière partie Morfetik avec d’autres ressources analogues du français : Morphalou, Lexique3 et le DELAF.</abstract>
      <url hash="fb744eba">2009.jeptalnrecital-court.24</url>
      <language>fra</language>
      <bibkey>buvet-etal-2009-morfetik</bibkey>
    </paper>
    <paper id="25">
      <title>Nouvelles considérations pour la détection de réutilisation de texte</title>
      <author><first>Fabien</first><last>Poulard</last></author>
      <author><first>Stergos</first><last>Afantenos</last></author>
      <author><first>Nicolas</first><last>Hernandez</last></author>
      <pages>227–236</pages>
      <abstract>Dans cet article nous nous intéressons au problème de la détection de réutilisation de texte. Plus particulièrement, étant donné un document original et un ensemble de documents candidats — thématiquement similaires au premier — nous cherchons à classer ceux qui sont dérivés du document original et ceux qui ne le sont pas. Nous abordons le problème selon deux approches : dans la première, nous nous intéressons aux similarités discursives entre les documents, dans la seconde au recouvrement de n-grams hapax. Nous présentons le résultat d’expérimentations menées sur un corpus de presse francophone construit dans le cadre du projet ANR PIITHIE.</abstract>
      <url hash="f6795888">2009.jeptalnrecital-court.25</url>
      <language>fra</language>
      <bibkey>poulard-etal-2009-nouvelles</bibkey>
    </paper>
    <paper id="26">
      <title>Collecte et analyses de réponses naturelles pour les systèmes de questions-réponses</title>
      <author><first>Anne</first><last>Garcia-Fernandez</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>237–246</pages>
      <abstract>Notre travail se situe dans le cadre des systèmes de réponse a une question et à pour but de fournir une réponse en langue naturelle aux questions posées en langue naturelle. Cet article présente une expérience permettant d’analyser les réponses de locuteurs du français à des questions que nous leur posons. L’expérience se déroule à l’écrit comme à l’oral et propose à des locuteurs français des questions relevant de différents types sémantiques et syntaxiques. Nous mettons en valeur une large variabilité dans les formes de réponses possibles en langue française. D’autre part nous établissons un certain nombre de liens entre formulation de question et formulation de réponse. Nous proposons d’autre part une comparaison des réponses selon la modalité oral / écrit. Ces résultats peuvent être intégrés à des systèmes existants pour produire une réponse en langue naturelle de façon dynamique.</abstract>
      <url hash="38c5709c">2009.jeptalnrecital-court.26</url>
      <language>fra</language>
      <bibkey>garcia-fernandez-etal-2009-collecte</bibkey>
    </paper>
    <paper id="27">
      <title>La /f<fixed-case>O</fixed-case>netizasjc/ comme un problème de translittération</title>
      <author><first>Vincent</first><last>Claveau</last></author>
      <pages>247–252</pages>
      <abstract>La phonétisation est une étape essentielle pour le traitement de l’oral. Dans cet article, nous décrivons un système automatique de phonétisation de mots isolés qui est simple, portable et performant. Il repose sur une approche par apprentissage ; le système est donc construit à partir d’exemples de mots et de leur représentation phonétique. Nous utilisons pour cela une technique d’inférence de règles de réécriture initialement développée pour la translittération et la traduction. Pour évaluer les performances de notre approche, nous avons utilisé plusieurs jeux de données couvrant différentes langues et divers alphabets phonétiques, tirés du challenge Pascal Pronalsyl. Les très bons résultats obtenus égalent ou dépassent ceux des meilleurs systèmes de l’état de l’art.</abstract>
      <url hash="fbb969eb">2009.jeptalnrecital-court.27</url>
      <language>fra</language>
      <bibkey>claveau-2009-la</bibkey>
    </paper>
    <paper id="28">
      <title>Plusieurs langues (bien choisies) valent mieux qu’une : traduction statistique multi-source par renforcement lexical</title>
      <author><first>Josep Maria</first><last>Crego</last></author>
      <author><first>Aurélien</first><last>Max</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>253–262</pages>
      <abstract>Les systèmes de traduction statistiques intègrent différents types de modèles dont les prédictions sont combinées, lors du décodage, afin de produire les meilleures traductions possibles. Traduire correctement des mots polysémiques, comme, par exemple, le mot avocat du français vers l’anglais (lawyer ou avocado), requiert l’utilisation de modèles supplémentaires, dont l’estimation et l’intégration s’avèrent complexes. Une alternative consiste à tirer parti de l’observation selon laquelle les ambiguïtés liées à la polysémie ne sont pas les mêmes selon les langues source considérées. Si l’on dispose, par exemple, d’une traduction vers l’espagnol dans laquelle avocat a été traduit par aguacate, alors la traduction de ce mot vers l’anglais n’est plus ambiguë. Ainsi, la connaissance d’une traduction français!espagnol permet de renforcer la sélection de la traduction avocado pour le système français!anglais. Dans cet article, nous proposons d’utiliser des documents en plusieurs langues pour renforcer les choix lexicaux effectués par un système de traduction automatique. En particulier, nous montrons une amélioration des performances sur plusieurs métriques lorsque les traductions auxiliaires utilisées sont obtenues manuellement.</abstract>
      <url hash="d35dad18">2009.jeptalnrecital-court.28</url>
      <language>fra</language>
      <bibkey>maria-crego-etal-2009-plusieurs</bibkey>
    </paper>
    <paper id="29">
      <title>Problématique d’analyse et de modélisation des erreurs en production écrite. Approche interdisciplinaire</title>
      <author><first>Jean-Léon</first><last>Bouraoui</last></author>
      <author><first>Philippe</first><last>Boissière</last></author>
      <author><first>Mustapha</first><last>Mojahid</last></author>
      <author><first>Nadine</first><last>Vigouroux</last></author>
      <author><first>Aurélie</first><last>Lagarrigue</last></author>
      <author><first>Frédéric</first><last>Vella</last></author>
      <author><first>Jean-Luc</first><last>Nespoulous</last></author>
      <pages>263–272</pages>
      <abstract>L’objectif du travail présenté ici est la modélisation de la détection et la correction des erreurs orthographiques et dactylographiques, plus particulièrement dans le contexte des handicaps langagiers. Le travail est fondé sur une analyse fine des erreurs d’écriture commises. La première partie de cet article est consacrée à une description précise de la faute. Dans la seconde partie, nous analysons l’erreur (1) en déterminant la nature de la faute (typographique, orthographique, ou grammaticale) et (2) en explicitant sa conséquence sur le niveau de perturbation linguistique (phonologique, orthographique, morphologique ou syntaxique). Il résulte de ce travail un modèle général des erreurs (une grille) que nous présenterons, ainsi que les résultats statistiques correspondants. Enfin, nous montrerons sur des exemples, l’utilité de l’apport de cette grille, en soumettant ces types de fautes à quelques correcteurs. Nous envisageons également les implications informatiques de ce travail.</abstract>
      <url hash="bf39623a">2009.jeptalnrecital-court.29</url>
      <language>fra</language>
      <bibkey>bouraoui-etal-2009-problematique</bibkey>
    </paper>
    <paper id="30">
      <title>Profilage de candidatures assisté par Relevance Feedback</title>
      <author><first>Rémy</first><last>Kessler</last></author>
      <author><first>Nicolas</first><last>Béchet</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <author><first>Mathieu</first><last>Roche</last></author>
      <author><first>Marc</first><last>El-Bèze</last></author>
      <pages>273–282</pages>
      <abstract>Le marché d’offres d’emploi et des candidatures sur Internet connaît une croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme de texte libre) qu’il n’est plus possible de traiter manuellement. Une analyse et catégorisation assistées nous semble pertinente en réponse à cette problématique. Nous proposons E-Gen, système qui a pour but l’analyse et catégorisation assistés d’offres d’emploi et des réponses des candidats. Dans cet article nous présentons plusieurs stratégies, reposant sur les modèles vectoriel et probabiliste, afin de résoudre la problématique du profilage des candidatures en fonction d’une offre précise. Nous avons évalué une palette de mesures de similarité afin d’effectuer un classement pertinent des candidatures au moyen des courbes ROC. L’utilisation d’une forme de relevance feedback a permis de surpasser nos résultats sur ce problème difficile et sujet à une grande subjectivité.</abstract>
      <url hash="f345da57">2009.jeptalnrecital-court.30</url>
      <language>fra</language>
      <bibkey>kessler-etal-2009-profilage</bibkey>
    </paper>
    <paper id="31">
      <title>Profilage sémantique endogène des relations de synonymie au sein de Gene Ontology</title>
      <author><first>Thierry</first><last>Hamon</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <pages>283–292</pages>
      <abstract>Le calcul de la similarité sémantique entre les termes repose sur l’existence et l’utilisation de ressources sémantiques. Cependant de telles ressources, qui proposent des équivalences entre entités, souvent des relations de synonymie, doivent elles-mêmes être d’abord analysées afin de définir des zones de fiabilité où la similarité sémantique est plus forte. Nous proposons une méthode d’acquisition de synonymes élémentaires grâce à l’exploitation des terminologies structurées au travers l’analyse de la structure syntaxique des termes complexes et de leur compositionnalité. Les synonymes acquis sont ensuite profilés grâce aux indicateurs endogènes inférés automatiquement à partir de ces mêmes terminologies (d’autres types de relations, inclusions lexicales, productivité, forme des composantes connexes). Dans le domaine biomédical, il existe de nombreuses terminologies structurées qui peuvent être exploitées pour la constitution de ressources sémantiques. Le travail présenté ici exploite une de ces terminologies, Gene Ontology.</abstract>
      <url hash="c4b288af">2009.jeptalnrecital-court.31</url>
      <language>fra</language>
      <bibkey>hamon-grabar-2009-profilage</bibkey>
    </paper>
    <paper id="32">
      <title>Quels attributs discriminants pour une analyse syntaxique par classification de textes en langue arabe ?</title>
      <author><first>Fériel</first><last>Ben Fraj</last></author>
      <author><first>Chiraz</first><last>Ben Othmane Zribi</last></author>
      <author><first>Mohamed</first><last>Ben Ahmed</last></author>
      <pages>293–300</pages>
      <abstract>Dans le cadre dune approche déterministe et incrémentale danalyse syntaxique par classification de textes en langue arabe, nous avons prévu de prendre en considération un ensemble varié dattributs discriminants afin de mieux assister la procédure de classification dans ses prises de décisions à travers les différentes étapes danalyse. Ainsi, en plus des attributs morpho-syntaxiques du mot en cours danalyse et des informations contextuelles des mots lavoisinant, nous avons ajouté des informations compositionnelles extraites du fragment de larbre syntaxique déjà construit lors de létape précédente de lanalyse en cours. Ce papier présente notre approche danalyse syntaxique par classification et vise lexposition dune justification expérimentale de lapport de chaque type dattributs discriminants et spécialement ceux compositionnels dans ladite analyse syntaxique.</abstract>
      <url hash="226fc240">2009.jeptalnrecital-court.32</url>
      <language>fra</language>
      <bibkey>ben-fraj-etal-2009-quels</bibkey>
    </paper>
    <paper id="33">
      <title>Recto /Verso Un système de conversion automatique ancienne / nouvelle orthographe à visée linguistique et didactique</title>
      <author><first>Richard</first><last>Beaufort</last></author>
      <author><first>Anne</first><last>Dister</last></author>
      <author><first>Hubert</first><last>Naets</last></author>
      <author><first>Kévin</first><last>Macé</last></author>
      <author><first>Cédrick</first><last>Fairon</last></author>
      <pages>301–310</pages>
      <abstract>Cet article présente Recto /Verso, un système de traitement automatique du langage dédié à l’application des rectifications orthographiques de 1990. Ce système a été développé dans le cadre de la campagne de sensibilisation réalisée en mars dernier par le Service et le Conseil de la langue française et de la politique linguistique de la Communauté française de Belgique. Nous commençons par rappeler les motivations et le contenu de la réforme proposée, et faisons le point sur les principes didactiques retenus dans le cadre de la campagne. La plus grande partie de l’article est ensuite consacrée à l’implémentation du système. Nous terminons enfin par une première analyse de l’impact de la campagne sur les utilisateurs.</abstract>
      <url hash="15d3ebe3">2009.jeptalnrecital-court.33</url>
      <language>fra</language>
      <bibkey>beaufort-etal-2009-recto</bibkey>
    </paper>
    <paper id="34">
      <title>Relevance of <fixed-case>ASR</fixed-case> for the Automatic Generation of Keywords Suggestions for <fixed-case>TV</fixed-case> programs</title>
      <author><first>Véronique</first><last>Malaisé</last></author>
      <author><first>Luit</first><last>Gazendam</last></author>
      <author><first>Willemijn</first><last>Heeren</last></author>
      <author><first>Roeland</first><last>Ordelman</last></author>
      <author><first>Hennie</first><last>Brugman</last></author>
      <pages>311–320</pages>
      <abstract>Semantic access to multimedia content in audiovisual archives is to a large extent dependent on quantity and quality of the metadata, and particularly the content descriptions that are attached to the individual items. However, the manual annotation of collections puts heavy demands on resources. A large number of archives are introducing (semi) automatic annotation techniques for generating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the extraction of keywords from textual resources related to TV programs to be archived (context documents), in collaboration with the Dutch audiovisual archives, Sound and Vision. This paper investigates the suitability of Automatic Speech Recognition transcripts produced in the CATCH-CHoral project for generating such keywords, which we evaluate against manual annotations of the documents, and against keywords automatically generated from context documents describing the TV programs’ content.</abstract>
      <url hash="8dc4b14f">2009.jeptalnrecital-court.34</url>
      <bibkey>malaise-etal-2009-relevance</bibkey>
    </paper>
    <paper id="35">
      <title>Résumé automatique multi-document et indépendance de la langue : une première évaluation en français</title>
      <author><first>Florian</first><last>Boudin</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <pages>321–330</pages>
      <abstract>Le résumé automatique de texte est une problématique difficile, fortement dépendante de la langue et qui peut nécessiter un ensemble de données d’apprentissage conséquent. L’approche par extraction peut aider à surmonter ces difficultés. (Mihalcea, 2004) a démontré l’intérêt des approches à base de graphes pour l’extraction de segments de texte importants. Dans cette étude, nous décrivons une approche indépendante de la langue pour la problématique du résumé automatique multi-documents. L’originalité de notre méthode repose sur l’utilisation d’une mesure de similarité permettant le rapprochement de segments morphologiquement proches. De plus, c’est à notre connaissance la première fois que l’évaluation d’une approche de résumé automatique multi-document est conduite sur des textes en français.</abstract>
      <url hash="b5e613e2">2009.jeptalnrecital-court.35</url>
      <language>fra</language>
      <bibkey>boudin-torres-moreno-2009-resume</bibkey>
    </paper>
    <paper id="36">
      <title>Segmentation et classification non supervisée de conversations téléphoniques automatiquement retranscrites</title>
      <author><first>Laurent</first><last>Bozzi</last></author>
      <author><first>Philippe</first><last>Suignard</last></author>
      <author><first>Claire</first><last>Waast-Richard</last></author>
      <pages>331–336</pages>
      <abstract>Cette étude porte sur l’analyse de conversations entre des clients et des téléconseillers d’EDF. Elle propose une chaîne de traitements permettant d’automatiser la détection des sujets abordés dans chaque conversation. L’aspect multi-thématique des conversations nous incite à trouver une unité de documents entre le simple tour de parole et la conversation entière. Cette démarche enchaîne une étape de segmentation de la conversation en thèmes homogènes basée sur la notion de cohésion lexicale, puis une étape de text-mining comportant une analyse linguistique enrichie d’un vocabulaire métier spécifique à EDF, et enfin une classification non supervisée des segments obtenus. Plusieurs algorithmes de segmentation ont été évalués sur un corpus de test, segmenté et annoté manuellement : le plus « proche » de la segmentation de référence est C99. Cette démarche, appliquée à la fois sur un corpus de conversations transcrites à la main, et sur les mêmes conversations décodées par un moteur de reconnaissance vocale, aboutit quasiment à l’obtention des 20 mêmes classes thématiques.</abstract>
      <url hash="7fc995c7">2009.jeptalnrecital-court.36</url>
      <language>fra</language>
      <bibkey>bozzi-etal-2009-segmentation</bibkey>
    </paper>
    <paper id="37">
      <title>Segmentation multiple d’un flux de données textuelles pour la modélisation statistique du langage</title>
      <author><first>Sopheap</first><last>Seng</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Brigitte</first><last>Bigi</last></author>
      <author><first>Eric</first><last>Castelli</last></author>
      <pages>337–346</pages>
      <abstract>Dans cet article, nous traitons du problème de la modélisation statistique du langage pour les langues peu dotées et sans segmentation entre les mots. Tandis que le manque de données textuelles a un impact sur la performance des modèles, les erreurs introduites par la segmentation automatique peuvent rendre ces données encore moins exploitables. Pour exploiter au mieux les données textuelles, nous proposons une méthode qui effectue des segmentations multiples sur le corpus d’apprentissage au lieu d’une segmentation unique. Cette méthode basée sur les automates d’état finis permet de retrouver les n-grammes non trouvés par la segmentation unique et de générer des nouveaux n-grammes pour l’apprentissage de modèle du langage. L’application de cette approche pour l’apprentissage des modèles de langage pour les systèmes de reconnaissance automatique de la parole en langue khmère et vietnamienne s’est montrée plus performante que la méthode par segmentation unique, à base de règles.</abstract>
      <url hash="ff770488">2009.jeptalnrecital-court.37</url>
      <language>fra</language>
      <bibkey>seng-etal-2009-segmentation</bibkey>
    </paper>
    <paper id="38">
      <title>Sens et usages d’un terme dans un réseau lexical évolutif</title>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Alain</first><last>Joubert</last></author>
      <author><first>Stéphane</first><last>Riou</last></author>
      <pages>347–356</pages>
      <abstract>L’obtention d’informations lexicales fiables est un enjeu primordial en TALN, mais cette collecte peut s’avérer difficile. L’approche présentée ici vise à pallier les écueils de cette difficulté en faisant participer un grand nombre de personnes à un projet contributif via des jeux accessibles sur le web. Ainsi, les joueurs vont construire le réseau lexical, en fournissant de plusieurs manières possibles des associations de termes à partir d’un terme cible et d’une consigne correspondant à une relation typée. Le réseau lexical ainsi produit est de grande taille et comporte une trentaine de types de relations. A partir de cette ressource, nous abordons la question de la détermination des différents sens et usages d’un terme. Ceci est réalisé en analysant les relations entre ce terme et ses voisins immédiats dans le réseau et en calculant des cliques ou des quasi-cliques. Ceci nous amène naturellement à introduire la notion de similarité entre cliques, que nous interprétons comme une mesure de similarité entre ces différents sens et usages. Nous pouvons ainsi construire pour un terme son arbre des usages, qui est une structure de données exploitable en désambiguïsation de sens. Nous présentons quelques résultats obtenus en soulignant leur caractère évolutif.</abstract>
      <url hash="dff0c615">2009.jeptalnrecital-court.38</url>
      <language>fra</language>
      <bibkey>lafourcade-etal-2009-sens</bibkey>
    </paper>
    <paper id="39">
      <title>Traitement automatique de disfluences dans un corpus linguistiquement contraint</title>
      <author><first>Jean-Léon</first><last>Bouraoui</last></author>
      <author><first>Nadine</first><last>Vigouroux</last></author>
      <pages>357–366</pages>
      <abstract>Cet article présente un travail de modélisation et de détection des phénomènes de disfluence. Une des spécificité de ce travail est le cadre dans lequel il se situe: le contrôle de la navigation aérienne. Nous montrons ce que ce cadre particulier implique certains choix concernant la modélisation et l’implémentation. Ainsi, nous constatons que la modélisation fondée sur la syntaxe, souvent utilisée dans le traitement des langues naturelles, n’est pas la plus appropriée ici. Nous expliquons la façon dont l’implémentation a été réalisée. Dans une dernière partie, nous présentons la validation de ce dispositif, effectuée sur 400 énoncés.</abstract>
      <url hash="700ef8d9">2009.jeptalnrecital-court.39</url>
      <language>fra</language>
      <bibkey>bouraoui-vigouroux-2009-traitement</bibkey>
    </paper>
    <paper id="40">
      <title>Un Algorithme d’Analyse de Type <fixed-case>E</fixed-case>arley pour Grammaires à Concaténation d’Intervalles</title>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Wolfgang</first><last>Maier</last></author>
      <author><first>Yannick</first><last>Parmentier</last></author>
      <pages>367–376</pages>
      <abstract>Nous présentons ici différents algorithmes d’analyse pour grammaires à concaténation d’intervalles (Range Concatenation Grammar, RCG), dont un nouvel algorithme de type Earley, dans le paradigme de l’analyse déductive. Notre travail est motivé par l’intérêt porté récemment à ce type de grammaire, et comble un manque dans la littérature existante.</abstract>
      <url hash="9b197b4d">2009.jeptalnrecital-court.40</url>
      <language>fra</language>
      <bibkey>kallmeyer-etal-2009-un</bibkey>
    </paper>
    <paper id="41">
      <title>Un Analyseur Sémantique pour le <fixed-case>DHM</fixed-case> Modélisation – Réalisation – Évaluation</title>
      <author><first>Jérôme</first><last>Lehuen</last></author>
      <author><first>Thierry</first><last>Lemeunier</last></author>
      <pages>377–386</pages>
      <abstract>Cet article décrit un modèle de langage dédié au dialogue homme-machine, son implémentation en CLIPS, ainsi qu’une évaluation comparative. Notre problématique n’est ni d’analyser des grands corpus, ni de proposer une grammaire à grande couverture. Notre objectif est de produire des représentations sémantiques utilisables par un module de dialogue à partir d’énoncés oraux courts, le plus souvent agrammaticaux. Une démarche pragmatique nous a conduit à fonder l’analyse sur des principes simples mais efficaces dans le cadre que nous nous sommes fixé. L’algorithme retenu s’inspire de l’analyse tabulaire. L’évaluation que nous présentons repose sur le corpus MEDIA qui a fait l’objet d’une annotation sémantique manuelle de référence pour une campagne d’évaluation d’analyseurs sémantiques pour le dialogue. Les résultats que nous obtenons place notre analyseur dans le trio de tête des systèmes évalués lors de la campagne de juin 2005, et nous confortent dans nos choix d’algorithme et de représentation des connaissances.</abstract>
      <url hash="d64a7404">2009.jeptalnrecital-court.41</url>
      <language>fra</language>
      <bibkey>lehuen-lemeunier-2009-un</bibkey>
    </paper>
    <paper id="42">
      <title>Une approche exploratoire de compression automatique de phrases basée sur des critères thermodynamiques</title>
      <author><first>Silvia</first><last>Fernández Sabido</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <pages>387–393</pages>
      <abstract>Nous présentons une approche exploratoire basée sur des notions thermodynamiques de la Physique statistique pour la compression de phrases. Nous décrivons le modèle magnétique des verres de spins, adapté à notre conception de la problématique. Des simulations Métropolis Monte-Carlo permettent d’introduire des fluctuations thermiques pour piloter la compression. Des comparaisons intéressantes de notre méthode ont été réalisées sur un corpus en français.</abstract>
      <url hash="41d8dbc6">2009.jeptalnrecital-court.42</url>
      <language>fra</language>
      <bibkey>fernandez-sabido-torres-moreno-2009-une</bibkey>
    </paper>
    <paper id="43">
      <title>Un nouveau schéma de pondération pour la catégorisation de documents manuscrits</title>
      <author><first>Sebastián</first><last>Peña Saldarriaga</last></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Christian</first><last>Viard-Gaudin</last></author>
      <pages>394–403</pages>
      <abstract>Les schémas de pondération utilisés habituellement en catégorisation de textes, et plus généralement en recherche d’information (RI), ne sont pas adaptés à l’utilisation de données liées à des textes issus d’un processus de reconnaissance de l’écriture. En particulier, les candidats-mot à la reconnaissance ne pourraient être exploités sans introduire de fausses occurrences de termes dans le document. Dans cet article nous présentons un nouveau schéma de pondération permettant d’exploiter les listes de candidats-mot. Il permet d’estimer le pouvoir discriminant d’un terme en fonction de la probabilité a posteriori d’un candidat-mot dans une liste de candidats. Les résultats montrent que le taux de classification de documents fortement dégradés peut être amélioré en utilisant le schéma proposé.</abstract>
      <url hash="15ecc4b0">2009.jeptalnrecital-court.43</url>
      <language>fra</language>
      <bibkey>pena-saldarriaga-etal-2009-un</bibkey>
    </paper>
    <paper id="44">
      <title>Un système de traduction automatique paramétré par des atlas dialectologiques</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <pages>404–413</pages>
      <abstract>Contrairement à la plupart des systèmes de traitement du langage, qui s’appliquent à des langues écrites et standardisées, nous présentons ici un système de traduction automatique qui prend en compte les spécificités des dialectes. En général, les dialectes se caractérisent par une variation continue et un manque de données textuelles en qualité et quantité suffisantes. En même temps, du moins en Europe, les dialectologues ont étudié en détail les caractéristiques linguistiques des dialectes. Nous soutenons que des données provenant d’atlas dialectologiques peuvent être utilisées pour paramétrer un système de traduction automatique. Nous illustrons cette idée avec le prototype d’un système de traduction basé sur des règles, qui traduit de l’allemand standard vers les différents dialectes de Suisse allemande. Quelques exemples linguistiquement motivés serviront à exposer l’architecture de ce système.</abstract>
      <url hash="9b95fcca">2009.jeptalnrecital-court.44</url>
      <language>fra</language>
      <bibkey>scherrer-2009-un</bibkey>
    </paper>
    <paper id="45">
      <title>Utilisation de <fixed-case>PLSI</fixed-case> en recherche d’information Représentation des requêtes</title>
      <author><first>Jean-Cédric</first><last>Chappelier</last></author>
      <author><first>Emmanuel</first><last>Eckard</last></author>
      <pages>414–422</pages>
      <abstract>Le modèle PLSI (« Probabilistic Latent Semantic Indexing ») offre une approche de l’indexation de documents fondée sur des modèles probabilistes de catégories sémantiques latentes et a conduit à des applications dans différents domaines. Toutefois, ce modèle rend impossible le traitement de documents inconnus au moment de l’apprentissage, problème particulièrement sensible pour la représentation des requêtes dans le cadre de la recherche d’information. Une méthode, dite de « folding-in », permet dans une certaine mesure de contourner ce problème, mais présente des faiblesses. Cet article introduit nouvelle une mesure de similarité document-requête pour PLSI, fondée sur lesmodèles de langue, où le problème du « folding-in » ne se pose pas. Nous comparons cette nouvelle similarité aux noyaux de Fisher, l’état de l’art en la matière. Nous présentons aussi une évaluation de PLSI sur un corpus de recherche d’information de près de 7500 documents et de plus d’un million d’occurrences de termes provenant de la collection TREC–AP, une taille considérable dans le cadre de PLSI.</abstract>
      <url hash="72009325">2009.jeptalnrecital-court.45</url>
      <language>fra</language>
      <bibkey>chappelier-eckard-2009-utilisation</bibkey>
    </paper>
    <paper id="46">
      <title>Utiliser des sens de mots pour la segmentation thématique ?</title>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>423–432</pages>
      <abstract>La segmentation thématique est un domaine de l’analyse discursive ayant donné lieu à de nombreux travaux s’appuyant sur la notion de cohésion lexicale. La plupart d’entre eux n’exploitent que la simple récurrence lexicale mais quelques uns ont néanmoins exploré l’usage de connaissances rendant compte de cette cohésion lexicale. Celles-ci prennent généralement la forme de réseaux lexicaux, soit construits automatiquement à partir de corpus, soit issus de dictionnaires élaborés manuellement. Dans cet article, nous examinons dans quelle mesure une ressource d’une nature un peu différente peut être utilisée pour caractériser la cohésion lexicale des textes. Il s’agit en l’occurrence de sens de mots induits automatiquement à partir de corpus, à l’instar de ceux produits par la tâche «Word Sense Induction and Discrimination » de l’évaluation SemEval 2007. Ce type de ressources apporte une structuration des réseaux lexicaux au niveau sémantique dont nous évaluons l’apport pour la segmentation thématique.</abstract>
      <url hash="41af5d66">2009.jeptalnrecital-court.46</url>
      <language>fra</language>
      <bibkey>ferret-2009-utiliser</bibkey>
    </paper>
  </volume>
  <volume id="demonstration" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles. Démonstrations</booktitle>
      <editor><first>Adeline</first><last>Nazarenko</last></editor>
      <editor><first>Thierry</first><last>Poibeau</last></editor>
      <publisher>ATALA</publisher>
      <address>Senlis, France</address>
      <month>June</month>
      <year>2009</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="28def127">2009.jeptalnrecital-demonstration.0</url>
      <bibkey>jep-taln-recital-2009-actes-de-la-16eme</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>ACOLAD</fixed-case> un environnement pour l’édition de corpus de dépendances</title>
      <author><first>Francis</first><last>Brunet-Manquat</last></author>
      <author><first>Jérôme</first><last>Goulian</last></author>
      <pages>1–3</pages>
      <abstract>Dans cette démonstration, nous présentons le prototype d’un environnement open-source pour l’édition de corpus de dépendances. Cet environnement, nommé ACOLAD (Annotation de COrpus Linguistique pour l’Analyse de dépendances), propose des services manuels de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des chunks et annotation syntaxique des dépendances entre mots ou entre chunks).</abstract>
      <url hash="b23dfa69">2009.jeptalnrecital-demonstration.1</url>
      <language>fra</language>
      <bibkey>brunet-manquat-goulian-2009-acolad</bibkey>
    </paper>
    <paper id="2">
      <title>Amener des utilisateurs à créer et évaluer des paraphrases par le jeu</title>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Aurélien</first><last>Max</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>4–6</pages>
      <abstract>Dans cet article, nous présentons une application sur le web pour l’acquisition de paraphrases phrastiques et sous-phrastiques sous forme de jeu. L’application permet l’acquisition à la fois de paraphrases et de jugements humains multiples sur ces paraphrases, ce qui constitue des données particulièrement utiles pour les applications du TAL basées sur les phénomènes paraphrastiques.</abstract>
      <url hash="7284c333">2009.jeptalnrecital-demonstration.2</url>
      <language>fra</language>
      <bibkey>bouamor-etal-2009-amener</bibkey>
    </paper>
    <paper id="3">
      <title>anymalign : un outil d’alignement sous-phrastique libre pour les êtres humains</title>
      <author><first>Adrien</first><last>Lardilleux</last></author>
      <author><first>Yves</first><last>Lepage</last></author>
      <pages>7–9</pages>
      <abstract>Nous présentons anymalign, un aligneur sous-phrastique grand public. Ses résultats ont une qualité qui rivalise avec le meilleur outil du domaine, GIZA++. Il est rapide et simple d’utilisation, et permet de produire dictionnaires et autres tables de traduction en une seule commande. À notre connaissance, c’est le seul outil au monde permettant d’aligner un nombre quelconque de langues simultanément. Il s’agit donc du premier aligneur sousphrastique réellement multilingue.</abstract>
      <url hash="069d35e2">2009.jeptalnrecital-demonstration.3</url>
      <language>fra</language>
      <bibkey>lardilleux-lepage-2009-anymalign</bibkey>
    </paper>
    <paper id="4">
      <title>Apport des outils de <fixed-case>TAL</fixed-case> à la construction d’ontologies : propositions au sein de la plateforme <fixed-case>D</fixed-case>a<fixed-case>FOE</fixed-case></title>
      <author><first>Jean</first><last>Charlet</last></author>
      <author><first>Sylvie</first><last>Szulman</last></author>
      <author><first>Nathalie</first><last>Aussenac-Gilles</last></author>
      <author><first>Adeline</first><last>Nazarenko</last></author>
      <author><first>Nathalie</first><last>Hernandez</last></author>
      <author><first>Nadia</first><last>Nadah</last></author>
      <author><first>Éric</first><last>Sardet</last></author>
      <author><first>Jean</first><last>Delahousse</last></author>
      <author><first>Guy</first><last>Pierra</last></author>
      <pages>10–12</pages>
      <abstract>La construction d’ontologie à partir de textes fait l’objet d’études depuis plusieurs années dans le domaine de l’ingénierie des ontologies. Un cadre méthodologique en quatre étapes (constitution d’un corpus de documents, analyse linguistique du corpus, conceptualisation, opérationnalisation de l’ontologie) est commun à la plupart des méthodes de construction d’ontologies à partir de textes. S’il existe plusieurs plateformes de traitement automatique de la langue (TAL) permettant d’analyser automatiquement les corpus et de les annoter tant du point de vue syntaxique que statistique, il n’existe actuellement aucune procédure généralement acceptée, ni a fortiori aucun ensemble cohérent d’outils supports, permettant de concevoir de façon progressive, explicite et traçable une ontologie de domaine à partir d’un ensemble de ressources informationnelles relevant de ce domaine. Le but de ce court article est de présenter les propositions développées, au sein du projet ANR DaFOE 4app, pour favoriser l’émergence d’un tel ensemble d’outils.</abstract>
      <url hash="89f14729">2009.jeptalnrecital-demonstration.4</url>
      <language>fra</language>
      <bibkey>charlet-etal-2009-apport</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>ASSIST</fixed-case> : un moteur de recherche spécialisé pour l’analyse des cadres d’expériences</title>
      <author><first>Davy</first><last>Weissenbacher</last></author>
      <author><first>Elisa</first><last>Pieri</last></author>
      <author><first>Sophia</first><last>Ananiadou</last></author>
      <author><first>Brian</first><last>Rea</last></author>
      <author><first>Farida</first><last>Vis</last></author>
      <author><first>Yuwei</first><last>Lin</last></author>
      <author><first>Rob</first><last>Procter</last></author>
      <author><first>Peter</first><last>Halfpenny</last></author>
      <pages>13–15</pages>
      <abstract>L’analyse qualitative des données demande au sociologue un important travail de sélection et d’interprétation des documents. Afin de faciliter ce travail, cette communauté c’est dotée d’outils informatique mais leur fonctionnalités sont encore limitées. Le projet ASSIST est une étude exploratoire pour préciser les modules de traitement automatique des langues (TAL) permettant d’assister le sociologue dans son travail d’analyse. Nous présentons le moteur de recherche réalisé et nous justifions le choix des composants de TAL intégrés au prototype.</abstract>
      <url hash="a2ac2391">2009.jeptalnrecital-demonstration.5</url>
      <language>fra</language>
      <bibkey>weissenbacher-etal-2009-assist</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>CETLEF</fixed-case>.fr - diagnostic automatique des erreurs de déclinaison tchèque dans un outil <fixed-case>ELAO</fixed-case></title>
      <author><first>Ivan</first><last>Šmilauer</last></author>
      <pages>16–18</pages>
      <abstract>CETLEF.fr – une application Web dynamique – propose des exercices de déclinaison tchèque avec un diagnostic automatique des erreurs. Le diagnostic a nécessité l’élaboration d’un modèle formel spécifique de la déclinaison contenant un classement des types paradigmatiques et des règles pour la réalisation des alternances morphématiques. Ce modèle est employé pour l’annotation des formes requises, nécessaire pour le diagnostic, mais également pour une présentation didactique sur la plateforme apprenant. Le diagnostic est effectué par comparaison d’une production erronée avec des formes hypothétiques générées à partir du radical de la forme requise et des différentes désinences casuelles. S’il existe une correspondance, l’erreur est interprétée d’après les différences dans les traits morphologiques de la forme requise et de la forme hypothétique. La majorité des erreurs commises peut être interprétée à l’aide de cette technique.</abstract>
      <url hash="ab108ef4">2009.jeptalnrecital-demonstration.6</url>
      <language>fra</language>
      <bibkey>smilauer-2009-cetlef</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>CIFLI</fixed-case>-<fixed-case>S</fixed-case>urvi<fixed-case>T</fixed-case>ra, deux facettes : démonstrateur de composants de <fixed-case>TA</fixed-case> fondée sur <fixed-case>UNL</fixed-case>, et phrasebook multilingue</title>
      <author><first>Georges</first><last>Fafiotte</last></author>
      <author><first>Achille</first><last>Falaise</last></author>
      <author><first>Jérôme</first><last>Goulian</last></author>
      <pages>19–21</pages>
      <abstract>CIFLI-SurviTra (“Survival Translation” assistant) est une plate-forme destinée à favoriser l’ingénierie et la mise au point de composants UNL de TA, à partir d’une mémoire de traduction formée de livres de phrases multilingues avec variables lexicales. SurviTra est aussi un phrasebook digital multilingue, assistant linguistique pour voyageurs monolingues (français, hindi, tamoul, anglais) en situation de “survie linguistique”. Le corpus d’un domaine-pilote (“Restaurant”) a été structuré et construit : sous-domaines de phrases alignées et classes lexicales de locutions quadrilingues, graphes UNL, dictionnaires UW++/français et UW++/hindi par domaines. L’approche, générique, est applicable à d’autres langues. Le prototype d’assistant linguistique (application Web, à interface textuelle) peut évoluer vers une application UNL embarquée sur SmartPhone, avec Traitement de Parole et multimodalité.</abstract>
      <url hash="eb3c34eb">2009.jeptalnrecital-demonstration.7</url>
      <language>fra</language>
      <bibkey>fafiotte-etal-2009-cifli</bibkey>
    </paper>
    <paper id="8">
      <title>Composition multilingue de sentiments</title>
      <author><first>Stefanos</first><last>Petrakis</last></author>
      <author><first>Manfred</first><last>Klenner</last></author>
      <author><first>Étienne</first><last>Ailloud</last></author>
      <author><first>Angela</first><last>Fahrni</last></author>
      <pages>22–24</pages>
      <abstract>Nous présentons ici PolArt, un outil multilingue pour l’analyse de sentiments qui aborde la composition des sentiments en appliquant des transducteurs en cascade. La compositionnalité est assurée au moyen de polarités préalables extraites d’un lexique et des règles de composition appliquées de manière incrémentielle.</abstract>
      <url hash="fdfa5884">2009.jeptalnrecital-demonstration.8</url>
      <language>fra</language>
      <bibkey>petrakis-etal-2009-composition</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>EXCOM</fixed-case> : Plate-forme d’annotation sémantique de textes multilingues</title>
      <author><first>Motasem</first><last>Alrahabi</last></author>
      <author><first>Jean-Pierre</first><last>Desclés</last></author>
      <pages>25–27</pages>
      <abstract>Nous proposons une plateforme d‟annotation sémantique, appelée « EXCOM ». Basée sur la méthode de l‟ « Exploration Contextuelle », elle permet, à travers une diversité de langues, de procéder à des annotations automatiques de segments textuels par l’analyse des formes de surface dans leur contexte. Les textes sont traités selon des « points de vue » discursifs dont les valeurs sont organisées dans une « carte sémantique ». L‟annotation se base sur un ensemble de règles linguistiques, écrites par un analyste, qui permettent d‟identifier les représentations textuelles sous-jacentes aux différentes catégories de la carte. Le système offre, à travers deux types d‟interfaces (développeur ou utilisateur), une chaîne de traitements automatiques de textes qui comprend la segmentation, l‟annotation et d‟autres fonctionnalités de post-traitement. Les documents annotés peuvent être utilisés, par exemple, pour des systèmes de recherche d‟information, de veille, de classification ou de résumé automatique.</abstract>
      <url hash="707ff646">2009.jeptalnrecital-demonstration.9</url>
      <language>fra</language>
      <bibkey>alrahabi-descles-2009-excom</bibkey>
    </paper>
    <paper id="10">
      <title>La plate-forme d’annotation Glozz</title>
      <author><first>Antoine</first><last>Widlöcher</last></author>
      <author><first>Yann</first><last>Mathet</last></author>
      <pages>28–30</pages>
      <abstract/>
      <url hash="3abfd3ee">2009.jeptalnrecital-demonstration.10</url>
      <language>fra</language>
      <bibkey>widlocher-mathet-2009-la-plate</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>SAGACE</fixed-case>-v3.3 ; Analyseur de corpus pour langues non flexionnelles</title>
      <author><first>Blin</first><last>Raoul</last></author>
      <pages>31–33</pages>
      <abstract>Nous présentons la dernière version du logiciel SAGACE, analyseur de corpus pour langues faiblement flexionnelles (par exemple japonais ou chinois). Ce logiciel est distribué avec un lexique où les catégories sont exprimées à l’aide de systèmes de traits.</abstract>
      <url hash="9c342c82">2009.jeptalnrecital-demonstration.11</url>
      <language>fra</language>
      <bibkey>raoul-2009-sagace</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>A</fixed-case>pache <fixed-case>UIMA</fixed-case> pour le Traitement Automatique des Langues</title>
      <author><first>Nicolas</first><last>Hernandez</last></author>
      <author><first>Fabien</first><last>Poulard</last></author>
      <author><first>Stergos</first><last>Afantenos</last></author>
      <author><first>Matthieu</first><last>Vernier</last></author>
      <author><first>Jérôme</first><last>Rocheteau</last></author>
      <pages>34–36</pages>
      <abstract>L’objectif de la démonstration est d’une part de faire un retour d’expérience sur la solution logicielle Apache UIMA comme infrastructure de développement d’applications distribuées de TAL, et d’autre part de présenter les développements réalisés par l’équipe TALN du LINA pour permettre à la communauté de s’approprier ce « framework ».</abstract>
      <url hash="ddf8fcc7">2009.jeptalnrecital-demonstration.12</url>
      <language>fra</language>
      <bibkey>hernandez-etal-2009-apache</bibkey>
    </paper>
    <paper id="13">
      <title>Un Analyseur Sémantique pour le <fixed-case>DHM</fixed-case></title>
      <author><first>Jérôme</first><last>Lehuen</last></author>
      <author><first>Thierry</first><last>Lemeunier</last></author>
      <pages>37–39</pages>
      <abstract/>
      <url hash="c5487a9f">2009.jeptalnrecital-demonstration.13</url>
      <language>fra</language>
      <bibkey>lehuen-lemeunier-2009-un-analyseur</bibkey>
    </paper>
    <paper id="14">
      <title>Un chunker multilingue endogène</title>
      <author><first>Jacques</first><last>Vergne</last></author>
      <pages>40–42</pages>
      <abstract>Le chunking consiste à segmenter un texte en chunks, segments sous-phrastiques qu’Abney a défini approximativement comme des groupes accentuels. Traditionnellement, le chunking utilise des ressources monolingues, le plus souvent exhaustives, quelquefois partielles : des mots grammaticaux et des ponctuations, qui marquent souvent des débuts et fins de chunk. Mais cette méthode, si l’on veut l’étendre à de nombreuses langues, nécessite de multiplier les ressources monolingues. Nous présentons une nouvelle méthode : le chunking endogène, qui n’utilise aucune ressource hormis le texte analysé lui-même. Cette méthode prolonge les travaux de Zipf : la minimisation de l’effort de communication conduit les locuteurs à raccourcir les mots fréquents. On peut alors caractériser un chunk comme étant la période des fonctions périodiques correllées longueur et effectif des mots sur l’axe syntagmatique. Cette méthode originale présente l’avantage de s’appliquer à un grand nombre de langues d’écriture alphabétique, avec le même algorithme, sans aucune ressource.</abstract>
      <url hash="541f7215">2009.jeptalnrecital-demonstration.14</url>
      <language>fra</language>
      <bibkey>vergne-2009-un</bibkey>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2021-02-05">
    <meta>
      <booktitle>Actes de la 16ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Thibault</first><last>Mondary</last></editor>
      <editor><first>Aurélien</first><last>Bossard</last></editor>
      <editor><first>Thierry</first><last>Hamon</last></editor>
      <publisher>ATALA</publisher>
      <address>Senlis, France</address>
      <month>June</month>
      <year>2009</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="c659f01f">2009.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2009-actes-de-la-16eme-sur</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Apprentissage automatique et Co-training</title>
      <author><first>Pierre</first><last>Gotab</last></author>
      <pages>1–10</pages>
      <abstract>Dans le domaine de la classification supervisée et semi-supervisée, cet article présente un contexte favorable à l’application de méthodes statistiques de classification. Il montre l’application d’une stratégie alternative dans le cas où les données d’apprentissage sont insuffisantes, mais où de nombreuses données non étiquetées sont à notre disposition : le cotraining multi-classifieurs. Les deux vues indépendantes habituelles du co-training sont remplacées par deux classifieurs basés sur des techniques de classification différentes : icsiboost sur le boosting et LIBLINEAR sur de la régression logistique.</abstract>
      <url hash="95c0386b">2009.jeptalnrecital-recital.1</url>
      <language>fra</language>
      <bibkey>gotab-2009-apprentissage</bibkey>
    </paper>
    <paper id="2">
      <title>Comparing Speech Recognizers Derived from Mono- and Multilingual Grammars</title>
      <author><first>Marianne</first><last>Santaholma</last></author>
      <pages>11–20</pages>
      <abstract>This paper examines the performance of multilingual parameterized grammar rules on speech recognition. We present a performance comparison of two different types of Japanese and English grammar-based speech recognizers. One system is derived from monolingual grammar rules and the other from multilingual parameterized grammar rules. The latter one uses hence the same grammar rules for creation of the language models for these two different languages. We carried out experiments on speech recognition of limited domain dialog application. These experiments show that the language models derived from multilingual parameterized grammar rules (1) perform equally well on both tested languages, on English and Japanese, and (2) that the performance is comparable with the recognizers derived from monolingual grammars that were explicitly developed for these languages. This suggests that the sharing grammar resources between different languages could be one solution for more efficient development of rule-based speech recognizers.</abstract>
      <url hash="9c2cf6de">2009.jeptalnrecital-recital.2</url>
      <bibkey>santaholma-2009-comparing</bibkey>
    </paper>
    <paper id="3">
      <title>Détection de la cohésion lexicale par voisinage distributionnel : application à la segmentation thématique</title>
      <author><first>Clémentine</first><last>Adam</last></author>
      <author><first>François</first><last>Morlane-Hondère</last></author>
      <pages>21–30</pages>
      <abstract>Cette étude s’insère dans le projet VOILADIS (VOIsinage Lexical pour l’Analyse du DIScours), qui a pour objectif d’exploiter des marques de cohésion lexicale pour mettre au jour des phénomènes discursifs. Notre propos est de montrer la pertinence d’une ressource, construite par l’analyse distributionnelle automatique d’un corpus, pour repérer les liens lexicaux dans les textes. Nous désignons par voisins les mots rapprochés par l’analyse distributionnelle sur la base des contextes syntaxiques qu’ils partagent au sein du corpus. Pour évaluer la pertinence de la ressource ainsi créée, nous abordons le problème du repérage des liens lexicaux à travers une application de TAL, la segmentation thématique. Nous discutons l’importance, pour cette tâche, de la ressource lexicale mobilixsée ; puis nous présentons la base de voisins distributionnels que nous utilisons ; enfin, nous montrons qu’elle permet, dans un système de segmentation thématique inspiré de (Hearst, 1997), des performances supérieures à celles obtenues avec une ressource traditionnelle.</abstract>
      <url hash="6c3179a5">2009.jeptalnrecital-recital.3</url>
      <language>fra</language>
      <bibkey>adam-morlane-hondere-2009-detection</bibkey>
    </paper>
    <paper id="4">
      <title>Extraction de lexique dans un corpus spécialisé en chinois contemporain</title>
      <author><first>Gaël</first><last>Patin</last></author>
      <pages>31–40</pages>
      <abstract>La constitution de ressources lexicales est une tâche cruciale pour l’amélioration des performances des systèmes de recherche d’information. Cet article présente une méthode d’extraction d’unités lexicales en chinois contemporain dans un corpus spécialisé non-annoté et non-segmenté. Cette méthode se base sur une construction incrémentale de l’unité lexicale orientée par une mesure d’association. Elle se distingue des travaux précédents par une approche linguistique non-supervisée assistée par les statistiques. Les résultats de l’extraction, évalués sur un échantillon aléatoire du corpus de travail, sont honorables avec des scores de précision et de rappel respectivement de 52,6 % et 53,7 %.</abstract>
      <url hash="63861163">2009.jeptalnrecital-recital.4</url>
      <language>fra</language>
      <bibkey>patin-2009-extraction</bibkey>
    </paper>
    <paper id="5">
      <title>Induction de sens de mots à partir de multiples espaces sémantiques</title>
      <author><first>Claire</first><last>Mouton</last></author>
      <pages>41–50</pages>
      <abstract>Les mots sont souvent porteurs de plusieurs sens. Pour traiter l’information correctement, un ordinateur doit être capable de décider quel sens d’un mot est employé à chacune de ses occurrences. Ce problème non parfaitement résolu a généré beaucoup de travaux sur la désambiguïsation du sens des mots (Word Sense Disambiguation) et dans la génération d’espaces sémantiques dont un des buts est de distinguer ces différents sens. Nous nous inspirons ici de deux méthodes existantes de détection automatique des différents usages et/ou sens des mots, pour les appliquer à des espaces sémantiques issus d’une analyse syntaxique effectuée sur un très grand nombre de pages web. Les adaptations et résultats présentés dans cet article se distinguent par le fait d’utiliser non plus une seule représentation mais une combinaison de multiples espaces de forte dimensionnalité. Ces multiples représentations étant en compétition entre elles, elles participent chacune par vote à l’induction des sens lors de la phase de clustering.</abstract>
      <url hash="2df738ae">2009.jeptalnrecital-recital.5</url>
      <language>fra</language>
      <bibkey>mouton-2009-induction</bibkey>
    </paper>
    <paper id="6">
      <title>Méta-moteur de traduction automatique : proposition d’une métrique pour le classement de traductions</title>
      <author><first>Marion</first><last>Potet</last></author>
      <pages>51–60</pages>
      <abstract>Compte tenu de l’essor du Web et du développement des documents multilingues, le besoin de traductions “à la volée” est devenu une évidence. Cet article présente un système qui propose, pour une phrase donnée, non pas une unique traduction, mais une liste de N hypothèses de traductions en faisant appel à plusieurs moteurs de traduction pré-existants. Neufs moteurs de traduction automatique gratuits et disponibles sur leWeb ont été sélectionnés pour soumettre un texte à traduire et réceptionner sa traduction. Les traductions obtenues sont classées selon une métrique reposant sur l’utilisation d’un modèle de langage. Les expériences conduites ont montré que ce méta-moteur de traduction se révèle plus pertinent que l’utilisation d’un seul système de traduction.</abstract>
      <url hash="2ee3b4b7">2009.jeptalnrecital-recital.6</url>
      <language>fra</language>
      <bibkey>potet-2009-meta</bibkey>
    </paper>
    <paper id="7">
      <title>Modèles statistiques pour l’estimation automatique de la difficulté de textes de <fixed-case>FLE</fixed-case></title>
      <author><first>Thomas</first><last>François</last></author>
      <pages>61–70</pages>
      <abstract>La lecture constitue l’une des tâches essentielles dans l’apprentissage d’une langue étrangère. Toutefois, la découverte d’un texte portant sur un sujet précis et qui soit adapté au niveau de chaque apprenant est consommatrice de temps et pourrait être automatisée. Des expériences montrent que, pour l’anglais, l’utilisation de classifieurs statistiques permet d’estimer automatiquement la difficulté d’un texte. Dans cet article, nous proposons une méthodologie originale comparant, pour le français langue étrangère (FLE), diverses techniques de classification (la régression logistique, le bagging et le boosting) sur deux corpus d’entraînement. Il ressort de cette analyse comparative une légère supériorité de la régression logistique multinomiale.</abstract>
      <url hash="0906d7b1">2009.jeptalnrecital-recital.7</url>
      <language>fra</language>
      <bibkey>francois-2009-modeles</bibkey>
    </paper>
    <paper id="8">
      <title>Modélisation des mouvements explicites dans les <fixed-case>ACG</fixed-case> avec le produit dépendant</title>
      <author><first>Florent</first><last>Pompigne</last></author>
      <pages>71–80</pages>
      <abstract/>
      <url hash="5b75895e">2009.jeptalnrecital-recital.8</url>
      <language>fra</language>
      <bibkey>pompigne-2009-modelisation</bibkey>
    </paper>
    <paper id="9">
      <title>Normalisation des entités nommées : pour une approche mixte et orientée utilisateurs</title>
      <author><first>Vanessa</first><last>Andréani</last></author>
      <pages>81–90</pages>
      <abstract>La normalisation intervient dans de nombreux champs du traitement de l’information. Elle permet d’optimiser les performances des applications, telles que la recherche ou l’extraction d’information, et de rendre plus fiable la constitution de ressources langagières. La normalisation consiste à ramener toutes les variantes d’un même terme ou d’une entité nommée à une forme standard, et permet de limiter l’impact de la variation linguistique. Notre travail porte sur la normalisation des entités nommées, pour laquelle nous avons mis en place un système complexe mêlant plusieurs approches. Nous en présentons ici une des composantes : une méthode endogène de délimitation et de validation de l’entité nommée normée, adaptée à des données multilingues. De plus, nous plaçons l’utilisateur au centre du processus de normalisation, dans l’objectif d’obtenir des données parfaitement fiables et adaptées à ses besoins.</abstract>
      <url hash="a406e9f6">2009.jeptalnrecital-recital.9</url>
      <language>fra</language>
      <bibkey>andreani-2009-normalisation</bibkey>
    </paper>
    <paper id="10">
      <title>Combinaison de contenus encyclopédiques multilingues pour une reconnaissance d’entités nommées en contexte</title>
      <author><first>Eric</first><last>Charton</last></author>
      <pages>91–100</pages>
      <abstract>Dans cet article, nous présentons une méthode de transformation de Wikipédia en ressource d’information externe pour détecter et désambiguïser des entités nommées, en milieu ouvert et sans apprentissage spécifique. Nous expliquons comment nous construisons notre système, puis nous utilisons cinq éditions linguistiques de Wikipédia afin d’enrichir son lexique. Pour finir nous réalisons une évaluation et comparons les performances du système avec et sans compléments lexicaux issus des informations inter-linguistiques, sur une tâche d’extraction d’entités nommées appliquée à un corpus d’articles journalistiques.</abstract>
      <url hash="4213f155">2009.jeptalnrecital-recital.10</url>
      <language>fra</language>
      <bibkey>charton-2009-combinaison</bibkey>
    </paper>
    <paper id="11">
      <title>La distance intertextuelle pour la classification de textes en langue arabe</title>
      <author><first>Rami</first><last>Ayadi</last></author>
      <author><first>Walid</first><last>Jaoudi</last></author>
      <pages>101–110</pages>
      <abstract>Nos travaux de recherche s’intéressent à l’application de la théorie de la distance intertextuelle sur la langue arabe en tant qu’outil pour la classification de textes. Cette théorie traite de la classification de textes selon des critères de statistique lexicale, se basant sur la notion de connexion lexicale. Notre objectif est d’intégrer cette théorie en tant qu’outil de classification de textes en langue arabe. Ceci nécessite l’intégration d’une métrique pour la classification de textes au niveau d’une base de corpus lemmatisés étiquetés et identifiés comme étant des références d’époques, de genre, de thèmes littéraires et d’auteurs et ceci afin de permettre la classification de textes anonymes.</abstract>
      <url hash="9865ea45">2009.jeptalnrecital-recital.11</url>
      <language>fra</language>
      <bibkey>ayadi-jaoudi-2009-la</bibkey>
    </paper>
    <paper id="12">
      <title>Techniques argumentatives pour aider à générer des descriptions orientées d’un événement</title>
      <author><first>Sara</first><last>Boutouhami</last></author>
      <pages>111–120</pages>
      <abstract>Les moyens et les formes stratégiques permettant la génération de descriptions textuelles argumentées d’une même réalité effective sont nombreux. La plupart des définitions proposées de l’argumentation partagent l’idée qu’argumenter c’est fournir les éléments en faveur d’une conclusion donnée. Or dans notre tâche qui consiste à générer des descriptions argumentées pour des accidents de la route, nous ne disposons pas uniquement d’éléments en faveur de la conclusion souhaitée mais aussi d’éléments qui vont à l’encontre de cette dernière et dont la présence est parfois obligatoire pour la compréhension de ces descriptions. Afin de remédier à ce problème, nous proposons des techniques de génération de descriptions argumentées qui présentent au mieux les éléments indésirables à l’aide de stratégies argumentatives.</abstract>
      <url hash="1c656b71">2009.jeptalnrecital-recital.12</url>
      <language>fra</language>
      <bibkey>boutouhami-2009-techniques</bibkey>
    </paper>
  </volume>
</collection>
