<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.trustnlp">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the First Workshop on Trustworthy Natural Language Processing</booktitle>
      <editor><first>Yada</first><last>Pruksachatkun</last></editor>
      <editor><first>Anil</first><last>Ramakrishna</last></editor>
      <editor><first>Kai-Wei</first><last>Chang</last></editor>
      <editor><first>Satyapriya</first><last>Krishna</last></editor>
      <editor><first>Jwala</first><last>Dhamala</last></editor>
      <editor><first>Tanaya</first><last>Guha</last></editor>
      <editor><first>Xiang</first><last>Ren</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.trustnlp-1</url>
      <venue>trustnlp</venue>
    </meta>
    <frontmatter>
      <url hash="c36171ab">2021.trustnlp-1.0</url>
      <bibkey>trustnlp-2021-trustworthy</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Interpretability Rules: Jointly Bootstrapping a Neural Relation Extractorwith an Explanation Decoder</title>
      <author><first>Zheng</first><last>Tang</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>1–7</pages>
      <abstract>We introduce a method that transforms a rule-based relation extraction (RE) classifier into a neural one such that both interpretability and performance are achieved. Our approach jointly trains a RE classifier with a decoder that generates explanations for these extractions, using as sole supervision a set of rules that match these relations. Our evaluation on the TACRED dataset shows that our neural RE classifier outperforms the rule-based one we started from by 9 F1 points; our decoder generates explanations with a high BLEU score of over 90%; and, the joint learning improves the performance of both the classifier and decoder.</abstract>
      <url hash="06b035ec">2021.trustnlp-1.1</url>
      <attachment type="OptionalSupplementaryData" hash="99d332b6">2021.trustnlp-1.1.OptionalSupplementaryData.tgz</attachment>
      <doi>10.18653/v1/2021.trustnlp-1.1</doi>
      <bibkey>tang-surdeanu-2021-interpretability</bibkey>
    </paper>
    <paper id="2">
      <title>Measuring Biases of Word Embeddings: What Similarity Measures and Descriptive Statistics to Use?</title>
      <author><first>Hossein</first><last>Azarpanah</last></author>
      <author><first>Mohsen</first><last>Farhadloo</last></author>
      <pages>8–14</pages>
      <abstract>Word embeddings are widely used in Natural Language Processing (NLP) for a vast range of applications. However, it has been consistently proven that these embeddings reflect the same human biases that exist in the data used to train them. Most of the introduced bias indicators to reveal word embeddings’ bias are average-based indicators based on the cosine similarity measure. In this study, we examine the impacts of different similarity measures as well as other descriptive techniques than averaging in measuring the biases of contextual and non-contextual word embeddings. We show that the extent of revealed biases in word embeddings depends on the descriptive statistics and similarity measures used to measure the bias. We found that over the ten categories of word embedding association tests, Mahalanobis distance reveals the smallest bias, and Euclidean distance reveals the largest bias in word embeddings. In addition, the contextual models reveal less severe biases than the non-contextual word embedding models.</abstract>
      <url hash="f76a7135">2021.trustnlp-1.2</url>
      <doi>10.18653/v1/2021.trustnlp-1.2</doi>
      <bibkey>azarpanah-farhadloo-2021-measuring</bibkey>
    </paper>
    <paper id="3">
      <title>Private Release of Text Embedding Vectors</title>
      <author><first>Oluwaseyi</first><last>Feyisetan</last></author>
      <author><first>Shiva</first><last>Kasiviswanathan</last></author>
      <pages>15–27</pages>
      <abstract>Ensuring strong theoretical privacy guarantees on text data is a challenging problem which is usually attained at the expense of utility. However, to improve the practicality of privacy preserving text analyses, it is essential to design algorithms that better optimize this tradeoff. To address this challenge, we propose a release mechanism that takes any (text) embedding vector as input and releases a corresponding private vector. The mechanism satisfies an extension of differential privacy to metric spaces. Our idea based on first randomly projecting the vectors to a lower-dimensional space and then adding noise in this projected space generates private vectors that achieve strong theoretical guarantees on its utility. We support our theoretical proofs with empirical experiments on multiple word embedding models and NLP datasets, achieving in some cases more than 10% gains over the existing state-of-the-art privatization techniques.</abstract>
      <url hash="8e14c23f">2021.trustnlp-1.3</url>
      <doi>10.18653/v1/2021.trustnlp-1.3</doi>
      <bibkey>feyisetan-kasiviswanathan-2021-private</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="4">
      <title>Accountable Error Characterization</title>
      <author><first>Amita</first><last>Misra</last></author>
      <author><first>Zhe</first><last>Liu</last></author>
      <author><first>Jalal</first><last>Mahmud</last></author>
      <pages>28–33</pages>
      <abstract>Customers of machine learning systems demand accountability from the companies employing these algorithms for various prediction tasks. Accountability requires understanding of system limit and condition of erroneous predictions, as customers are often interested in understanding the incorrect predictions, and model developers are absorbed in finding methods that can be used to get incremental improvements to an existing system. Therefore, we propose an accountable error characterization method, AEC, to understand when and where errors occur within the existing black-box models. AEC, as constructed with human-understandable linguistic features, allows the model developers to automatically identify the main sources of errors for a given classification system. It can also be used to sample for the set of most informative input points for a next round of training. We perform error detection for a sentiment analysis task using AEC as a case study. Our results on the sample sentiment task show that AEC is able to characterize erroneous predictions into human understandable categories and also achieves promising results on selecting erroneous samples when compared with the uncertainty-based sampling.</abstract>
      <url hash="cfdfe3f4">2021.trustnlp-1.4</url>
      <doi>10.18653/v1/2021.trustnlp-1.4</doi>
      <bibkey>misra-etal-2021-accountable</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="5">
      <title>x<fixed-case>ER</fixed-case>: An Explainable Model for Entity Resolution using an Efficient Solution for the Clique Partitioning Problem</title>
      <author><first>Samhita</first><last>Vadrevu</last></author>
      <author><first>Rakesh</first><last>Nagi</last></author>
      <author><first>JinJun</first><last>Xiong</last></author>
      <author><first>Wen-mei</first><last>Hwu</last></author>
      <pages>34–44</pages>
      <abstract>In this paper, we propose a global, self- explainable solution to solve a prominent NLP problem: Entity Resolution (ER). We formu- late ER as a graph partitioning problem. Every mention of a real-world entity is represented by a node in the graph, and the pairwise sim- ilarity scores between the mentions are used to associate these nodes to exactly one clique, which represents a real-world entity in the ER domain. In this paper, we use Clique Partition- ing Problem (CPP), which is an Integer Pro- gram (IP) to formulate ER as a graph partition- ing problem and then highlight the explainable nature of this method. Since CPP is NP-Hard, we introduce an efficient solution procedure, the xER algorithm, to solve CPP as a combi- nation of finding maximal cliques in the graph and then performing generalized set packing using a novel formulation. We discuss the advantages of using xER over the traditional methods and provide the computational exper- iments and results of applying this method to ER data sets.</abstract>
      <url hash="2c21d97b">2021.trustnlp-1.5</url>
      <doi>10.18653/v1/2021.trustnlp-1.5</doi>
      <bibkey>vadrevu-etal-2021-xer</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="6">
      <title>Gender Bias in Natural Language Processing Across Human Languages</title>
      <author><first>Abigail</first><last>Matthews</last></author>
      <author><first>Isabella</first><last>Grasso</last></author>
      <author><first>Christopher</first><last>Mahoney</last></author>
      <author><first>Yan</first><last>Chen</last></author>
      <author><first>Esma</first><last>Wali</last></author>
      <author><first>Thomas</first><last>Middleton</last></author>
      <author><first>Mariama</first><last>Njie</last></author>
      <author><first>Jeanna</first><last>Matthews</last></author>
      <pages>45–54</pages>
      <abstract>Natural Language Processing (NLP) systems are at the heart of many critical automated decision-making systems making crucial recommendations about our future world. Gender bias in NLP has been well studied in English, but has been less studied in other languages. In this paper, a team including speakers of 9 languages - Chinese, Spanish, English, Arabic, German, French, Farsi, Urdu, and Wolof - reports and analyzes measurements of gender bias in the Wikipedia corpora for these 9 languages. We develop extensions to profession-level and corpus-level gender bias metric calculations originally designed for English and apply them to 8 other languages, including languages that have grammatically gendered nouns including different feminine, masculine, and neuter profession words. We discuss future work that would benefit immensely from a computational linguistics perspective.</abstract>
      <url hash="8adea273">2021.trustnlp-1.6</url>
      <doi>10.18653/v1/2021.trustnlp-1.6</doi>
      <bibkey>matthews-etal-2021-gender</bibkey>
    </paper>
    <paper id="7">
      <title>Interpreting Text Classifiers by Learning Context-sensitive Influence of Words</title>
      <author><first>Sawan</first><last>Kumar</last></author>
      <author><first>Kalpit</first><last>Dixit</last></author>
      <author><first>Kashif</first><last>Shah</last></author>
      <pages>55–67</pages>
      <abstract>Many existing approaches for interpreting text classification models focus on providing importance scores for parts of the input text, such as words, but without a way to test or improve the interpretation method itself. This has the effect of compounding the problem of understanding or building trust in the model, with the interpretation method itself adding to the opacity of the model. Further, importance scores on individual examples are usually not enough to provide a sufficient picture of model behavior. To address these concerns, we propose MOXIE (MOdeling conteXt-sensitive InfluencE of words) with an aim to enable a richer interface for a user to interact with the model being interpreted and to produce testable predictions. In particular, we aim to make predictions for importance scores, counterfactuals and learned biases with MOXIE. In addition, with a global learning objective, MOXIE provides a clear path for testing and improving itself. We evaluate the reliability and efficiency of MOXIE on the task of sentiment analysis.</abstract>
      <url hash="47b284a8">2021.trustnlp-1.7</url>
      <doi>10.18653/v1/2021.trustnlp-1.7</doi>
      <bibkey>kumar-etal-2021-interpreting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="8">
      <title>Towards Benchmarking the Utility of Explanations for Model Debugging</title>
      <author><first>Maximilian</first><last>Idahl</last></author>
      <author><first>Lijun</first><last>Lyu</last></author>
      <author><first>Ujwal</first><last>Gadiraju</last></author>
      <author><first>Avishek</first><last>Anand</last></author>
      <pages>68–73</pages>
      <abstract>Post-hoc explanation methods are an important class of approaches that help understand the rationale underlying a trained model’s decision. But how useful are they for an end-user towards accomplishing a given task? In this vision paper, we argue the need for a benchmark to facilitate evaluations of the utility of post-hoc explanation methods. As a first step to this end, we enumerate desirable properties that such a benchmark should possess for the task of debugging text classifiers. Additionally, we highlight that such a benchmark facilitates not only assessing the effectiveness of explanations but also their efficiency.</abstract>
      <url hash="dfa79dcd">2021.trustnlp-1.8</url>
      <doi>10.18653/v1/2021.trustnlp-1.8</doi>
      <bibkey>idahl-etal-2021-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
  </volume>
</collection>
