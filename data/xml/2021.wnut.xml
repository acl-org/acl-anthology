<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.wnut">
  <volume id="1" ingest-date="2021-11-12">
    <meta>
      <booktitle>Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</booktitle>
      <editor><first>Wei</first><last>Xu</last></editor>
      <editor><first>Alan</first><last>Ritter</last></editor>
      <editor><first>Tim</first><last>Baldwin</last></editor>
      <editor><first>Afshin</first><last>Rahimi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2021</year>
      <venue>wnut</venue>
    </meta>
    <frontmatter>
      <url hash="375a0bef">2021.wnut-1.0</url>
      <bibkey>wnut-2021-noisy</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Text Simplification for Comprehension-based Question-Answering</title>
      <author><first>Tanvi</first><last>Dadu</last></author>
      <author><first>Kartikey</first><last>Pant</last></author>
      <author><first>Seema</first><last>Nagar</last></author>
      <author><first>Ferdous</first><last>Barbhuiya</last></author>
      <author><first>Kuntal</first><last>Dey</last></author>
      <pages>1–10</pages>
      <abstract>Text simplification is the process of splitting and rephrasing a sentence to a sequence of sentences making it easier to read and understand while preserving the content and approximating the original meaning. Text simplification has been exploited in NLP applications like machine translation, summarization, semantic role labeling, and information extraction, opening a broad avenue for its exploitation in comprehension-based question-answering downstream tasks. In this work, we investigate the effect of text simplification in the task of question-answering using a comprehension context. We release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset. Firstly, we outline each step in the dataset creation pipeline, including style transfer, thresholding of sentences showing correct transfer, and offset finding for each answer. Secondly, we verify the quality of the transferred sentences through various methodologies involving both automated and human evaluation. Thirdly, we benchmark the newly created corpus and perform an ablation study for examining the effect of the simplification process in the SQuAD-based question answering task. Our experiments show that simplification leads to up to 2.04% and 1.74% increase in Exact Match and F1, respectively. Finally, we conclude with an analysis of the transfer process, investigating the types of edits made by the model, and the effect of sentence length on the transfer model.</abstract>
      <url hash="7f5d9cfb">2021.wnut-1.1</url>
      <bibkey>dadu-etal-2021-text</bibkey>
      <doi>10.18653/v1/2021.wnut-1.1</doi>
      <pwccode url="https://github.com/kartikeypant/text-simplification-qa-www2021" additional="false">kartikeypant/text-simplification-qa-www2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisplit">WikiSplit</pwcdataset>
    </paper>
    <paper id="2">
      <title>Finding the needle in a haystack: Extraction of Informative <fixed-case>COVID</fixed-case>-19 <fixed-case>D</fixed-case>anish Tweets</title>
      <author><first>Benjamin</first><last>Olsen</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>11–19</pages>
      <abstract>Finding informative COVID-19 posts in a stream of tweets is very useful to monitor health-related updates. Prior work focused on a balanced data setup and on English, but informative tweets are rare, and English is only one of the many languages spoken in the world. In this work, we introduce a new dataset of 5,000 tweets for finding informative COVID-19 tweets for Danish. In contrast to prior work, which balances the label distribution, we model the problem by keeping its natural distribution. We examine how well a simple probabilistic model and a convolutional neural network (CNN) perform on this task. We find a weighted CNN to work well but it is sensitive to embedding and hyperparameter choices. We hope the contributed dataset is a starting point for further work in this direction.</abstract>
      <url hash="589a2c87">2021.wnut-1.2</url>
      <bibkey>olsen-plank-2021-finding</bibkey>
      <doi>10.18653/v1/2021.wnut-1.2</doi>
    </paper>
    <paper id="3">
      <title>Detecting Depression in <fixed-case>T</fixed-case>hai Blog Posts: a Dataset and a Baseline</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Pattama</first><last>Patpong</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <pages>20–25</pages>
      <abstract>We present the first openly available corpus for detecting depression in Thai. Our corpus is compiled by expert verified cases of depression in several online blogs. We experiment with two different LSTM based models and two different BERT based models. We achieve a 77.53% accuracy with a Thai BERT model in detecting depression. This establishes a good baseline for future researcher on the same corpus. Furthermore, we identify a need for Thai embeddings that have been trained on a more varied corpus than Wikipedia. Our corpus, code and trained models have been released openly on Zenodo.</abstract>
      <url hash="ab75172a">2021.wnut-1.3</url>
      <bibkey>hamalainen-etal-2021-detecting</bibkey>
      <doi>10.18653/v1/2021.wnut-1.3</doi>
    </paper>
    <paper id="4">
      <title>Keyphrase Extraction with Incomplete Annotated Training Data</title>
      <author><first>Yanfei</first><last>Lei</last></author>
      <author><first>Chunming</first><last>Hu</last></author>
      <author><first>Guanghui</first><last>Ma</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <pages>26–34</pages>
      <abstract>Extracting keyphrases that summarize the main points of a document is a fundamental task in natural language processing. Supervised approaches to keyphrase extraction(KPE) are largely developed based on the assumption that the training data is fully annotated. However, due to the difficulty of keyphrase annotating, KPE models severely suffer from incomplete annotated problem in many scenarios. To this end, we propose a more robust training method that learns to mitigate the misguidance brought by unlabeled keyphrases. We introduce negative sampling to adjust training loss, and conduct experiments under different scenarios. Empirical studies on synthetic datasets and open domain dataset show that our model is robust to incomplete annotated problem and surpasses prior baselines. Extensive experiments on five scientific domain datasets of different scales demonstrate that our model is competitive with the state-of-the-art method.</abstract>
      <url hash="f5db7d3a">2021.wnut-1.4</url>
      <attachment type="Software" hash="429484a1">2021.wnut-1.4.Software.zip</attachment>
      <bibkey>lei-etal-2021-keyphrase</bibkey>
      <doi>10.18653/v1/2021.wnut-1.4</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="5">
      <title>Fine-grained Temporal Relation Extraction with Ordered-Neuron <fixed-case>LSTM</fixed-case> and Graph Convolutional Networks</title>
      <author><first>Minh</first><last>Tran Phu</last></author>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>35–45</pages>
      <abstract>Fine-grained temporal relation extraction (FineTempRel) aims to recognize the durations and timeline of event mentions in text. A missing part in the current deep learning models for FineTempRel is their failure to exploit the syntactic structures of the input sentences to enrich the representation vectors. In this work, we propose to fill this gap by introducing novel methods to integrate the syntactic structures into the deep learning models for FineTempRel. The proposed model focuses on two types of syntactic information from the dependency trees, i.e., the syntax-based importance scores for representation learning of the words and the syntactic connections to identify important context words for the event mentions. We also present two novel techniques to facilitate the knowledge transfer between the subtasks of FineTempRel, leading to a novel model with the state-of-the-art performance for this task.</abstract>
      <url hash="81e1333c">2021.wnut-1.5</url>
      <bibkey>tran-phu-etal-2021-fine</bibkey>
      <doi>10.18653/v1/2021.wnut-1.5</doi>
      <video href="2021.wnut-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Does It Happen? Multi-hop Path Structures for Event Factuality Prediction with Graph Transformer Networks</title>
      <author><first>Duong</first><last>Le</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>46–55</pages>
      <abstract>The goal of Event Factuality Prediction (EFP) is to determine the factual degree of an event mention, representing how likely the event mention has happened in text. Current deep learning models has demonstrated the importance of syntactic and semantic structures of the sentences to identify important context words for EFP. However, the major problem with these EFP models is that they only encode the one-hop paths between the words (i.e., the direct connections) to form the sentence structures. In this work, we show that the multi-hop paths between the words are also necessary to compute the sentence structures for EFP. To this end, we introduce a novel deep learning model for EFP that explicitly considers multi-hop paths with both syntax-based and semantic-based edges between the words to obtain sentence structures for representation learning in EFP. We demonstrate the effectiveness of the proposed model via the extensive experiments in this work.</abstract>
      <url hash="abea7dbc">2021.wnut-1.6</url>
      <bibkey>le-nguyen-2021-happen</bibkey>
      <doi>10.18653/v1/2021.wnut-1.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>G</fixed-case>oogle-trickers, Yaminjeongeum, and Leetspeak: An Empirical Taxonomy for Intentionally Noisy User-Generated Text</title>
      <author><first>Won Ik</first><last>Cho</last></author>
      <author><first>Soomin</first><last>Kim</last></author>
      <pages>56–61</pages>
      <abstract>WARNING: This article contains contents that may offend the readers. Strategies that insert intentional noise into text when posting it are commonly observed in the online space, and sometimes they aim to let only certain community users understand the genuine semantics. In this paper, we explore the purpose of such actions by categorizing them into tricks, memes, fillers, and codes, and organize the linguistic strategies that are used for each purpose. Through this, we identify that such strategies can be conducted by authors for multiple purposes, regarding the presence of stakeholders such as ‘Peers’ and ‘Others’. We finally analyze how these strategies appear differently in each circumstance, along with the unified taxonomy accompanying examples.</abstract>
      <url hash="c3c18619">2021.wnut-1.7</url>
      <bibkey>cho-kim-2021-google</bibkey>
      <doi>10.18653/v1/2021.wnut-1.7</doi>
    </paper>
    <paper id="8">
      <title>Description-based Label Attention Classifier for Explainable <fixed-case>ICD</fixed-case>-9 Classification</title>
      <author><first>Malte</first><last>Feucht</last></author>
      <author><first>Zhiliang</first><last>Wu</last></author>
      <author><first>Sophia</first><last>Althammer</last></author>
      <author><first>Volker</first><last>Tresp</last></author>
      <pages>62–66</pages>
      <abstract>ICD-9 coding is a relevant clinical billing task, where unstructured texts with information about a patient’s diagnosis and treatments are annotated with multiple ICD-9 codes. Automated ICD-9 coding is an active research field, where CNN- and RNN-based model architectures represent the state-of-the-art approaches. In this work, we propose a description-based label attention classifier to improve the model explainability when dealing with noisy texts like clinical notes.</abstract>
      <url hash="5fff6075">2021.wnut-1.8</url>
      <bibkey>feucht-etal-2021-description</bibkey>
      <doi>10.18653/v1/2021.wnut-1.8</doi>
    </paper>
    <paper id="9">
      <title>A Text Editing Approach to Joint <fixed-case>J</fixed-case>apanese Word Segmentation, <fixed-case>POS</fixed-case> Tagging, and Lexical Normalization</title>
      <author><first>Shohei</first><last>Higashiyama</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>67–80</pages>
      <abstract>Lexical normalization, in addition to word segmentation and part-of-speech tagging, is a fundamental task for Japanese user-generated text processing. In this paper, we propose a text editing model to solve the three task jointly and methods of pseudo-labeled data generation to overcome the problem of data deficiency. Our experiments showed that the proposed model achieved better normalization performance when trained on more diverse pseudo-labeled data.</abstract>
      <url hash="16c82883">2021.wnut-1.9</url>
      <bibkey>higashiyama-etal-2021-text</bibkey>
      <doi>10.18653/v1/2021.wnut-1.9</doi>
    </paper>
    <paper id="10">
      <title>Intrinsic evaluation of language models for code-switching</title>
      <author><first>Sik Feng</first><last>Cheong</last></author>
      <author><first>Hai Leong</first><last>Chieu</last></author>
      <author><first>Jing</first><last>Lim</last></author>
      <pages>81–86</pages>
      <abstract>Language models used in speech recognition are often either evaluated intrinsically using perplexity on test data, or extrinsically with an automatic speech recognition (ASR) system. The former evaluation does not always correlate well with ASR performance, while the latter could be specific to particular ASR systems. Recent work proposed to evaluate language models by using them to classify ground truth sentences among alternative phonetically similar sentences generated by a fine state transducer. Underlying such an evaluation is the assumption that the generated sentences are linguistically incorrect. In this paper, we first put this assumption into question, and observe that alternatively generated sentences could often be linguistically correct when they differ from the ground truth by only one edit. Secondly, we showed that by using multi-lingual BERT, we can achieve better performance than previous work on two code-switching data sets. Our implementation is publicly available on Github at https://github.com/sikfeng/language-modelling-for-code-switching.</abstract>
      <url hash="dab0802c">2021.wnut-1.10</url>
      <bibkey>cheong-etal-2021-intrinsic</bibkey>
      <doi>10.18653/v1/2021.wnut-1.10</doi>
      <pwccode url="https://github.com/sikfeng/language-modelling-for-code-switching" additional="false">sikfeng/language-modelling-for-code-switching</pwccode>
    </paper>
    <paper id="11">
      <title>Can images help recognize entities? A study of the role of images for Multimodal <fixed-case>NER</fixed-case></title>
      <author><first>Shuguang</first><last>Chen</last></author>
      <author><first>Gustavo</first><last>Aguilar</last></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>87–96</pages>
      <abstract>Multimodal named entity recognition (MNER) requires to bridge the gap between language understanding and visual context. While many multimodal neural techniques have been proposed to incorporate images into the MNER task, the model’s ability to leverage multimodal interactions remains poorly understood. In this work, we conduct in-depth analyses of existing multimodal fusion techniques from different perspectives and describe the scenarios where adding information from the image does not always boost performance. We also study the use of captions as a way to enrich the context for MNER. Experiments on three datasets from popular social platforms expose the bottleneck of existing multimodal models and the situations where using captions is beneficial.</abstract>
      <url hash="af58aa30">2021.wnut-1.11</url>
      <bibkey>chen-etal-2021-images</bibkey>
      <doi>10.18653/v1/2021.wnut-1.11</doi>
      <pwccode url="https://github.com/RiTUAL-UH/multimodal_NER" additional="false">RiTUAL-UH/multimodal_NER</pwccode>
    </paper>
    <paper id="12">
      <title>Perceived and Intended Sarcasm Detection with Graph Attention Networks</title>
      <author><first>Joan</first><last>Plepi</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>97–105</pages>
      <abstract>Existing sarcasm detection systems focus on exploiting linguistic markers, context, or user-level priors. However, social studies suggest that the relationship between the author and the audience can be equally relevant for the sarcasm usage and interpretation. In this work, we propose a framework jointly leveraging (1) a user context from their historical tweets together with (2) the social information from a user’s conversational neighborhood in an interaction graph, to contextualize the interpretation of the post. We use graph attention networks (GAT) over users and tweets in a conversation thread, combined with dense user history representations. Apart from achieving state-of-the-art results on the recently published dataset of 19k Twitter users with 30K labeled tweets, adding 10M unlabeled tweets as context, our results indicate that the model contributes to interpreting the sarcastic intentions of an author more than to predicting the sarcasm perception by others.</abstract>
      <url hash="fc846cee">2021.wnut-1.12</url>
      <bibkey>plepi-flek-2021-perceived</bibkey>
      <doi>10.18653/v1/2021.wnut-1.12</doi>
      <video href="2021.wnut-1.12.mp4"/>
      <pwccode url="https://github.com/caisa-lab/sarcasm_detection" additional="false">caisa-lab/sarcasm_detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/spirs">SPIRS</pwcdataset>
    </paper>
    <paper id="13">
      <title>Hierarchical Character Tagger for Short Text Spelling Error Correction</title>
      <author><first>Mengyi</first><last>Gao</last></author>
      <author><first>Canran</first><last>Xu</last></author>
      <author><first>Peng</first><last>Shi</last></author>
      <pages>106–113</pages>
      <abstract>State-of-the-art approaches to spelling error correction problem include Transformer-based Seq2Seq models, which require large training sets and suffer from slow inference time; and sequence labeling models based on Transformer encoders like BERT, which involve token-level label space and therefore a large pre-defined vocabulary dictionary. In this paper we present a Hierarchical Character Tagger model, or HCTagger, for short text spelling error correction. We use a pre-trained language model at the character level as a text encoder, and then predict character-level edits to transform the original text into its error-free form with a much smaller label space. For decoding, we propose a hierarchical multi-task approach to alleviate the issue of long-tail label distribution without introducing extra model parameters. Experiments on two public misspelling correction datasets demonstrate that HCTagger is an accurate and much faster approach than many existing models.</abstract>
      <url hash="72936ccf">2021.wnut-1.13</url>
      <bibkey>gao-etal-2021-hierarchical</bibkey>
      <doi>10.18653/v1/2021.wnut-1.13</doi>
    </paper>
    <paper id="14">
      <title>Common Sense Bias in Semantic Role Labeling</title>
      <author><first>Heather</first><last>Lent</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>114–119</pages>
      <abstract>Large-scale language models such as ELMo and BERT have pushed the horizon of what is possible in semantic role labeling (SRL), solving the out-of-vocabulary problem and enabling end-to-end systems, but they have also introduced significant biases. We evaluate three SRL parsers on very simple transitive sentences with verbs usually associated with animate subjects and objects, such as, “Mary babysat Tom”: a state-of-the-art parser based on BERT, an older parser based on GloVe, and an even older parser from before the days of word embeddings. When arguments are word forms predominantly used as person names, aligning with common sense expectations of animacy, the BERT-based parser is unsurprisingly superior; yet, with abstract or random nouns, the opposite picture emerges. We refer to this as “common sense bias” and present a challenge dataset for evaluating the extent to which parsers are sensitive to such a bias. Our code and challenge dataset are available here: github.com/coastalcph/comte</abstract>
      <url hash="ba322713">2021.wnut-1.14</url>
      <bibkey>lent-sogaard-2021-common</bibkey>
      <doi>10.18653/v1/2021.wnut-1.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>P</fixed-case>oli<fixed-case>WAM</fixed-case>: An Exploration of a Large Scale Corpus of Political Discussions on <fixed-case>W</fixed-case>hats<fixed-case>A</fixed-case>pp Messenger</title>
      <author><first>Vivek</first><last>Srivastava</last></author>
      <author><first>Mayank</first><last>Singh</last></author>
      <pages>120–130</pages>
      <abstract>WhatsApp Messenger is one of the most popular channels for spreading information with a current reach of more than 180 countries and 2 billion people. Its widespread usage has made it one of the most popular media for information propagation among the masses during any socially engaging event. In the recent past, several countries have witnessed its effectiveness and influence in political and social campaigns. We observe a high surge in information and propaganda flow during election campaigning. In this paper, we explore a high-quality large-scale user-generated dataset curated from WhatsApp comprising of 281 groups, 31,078 unique users, and 223,404 messages shared before, during, and after the Indian General Elections 2019, encompassing all major Indian political parties and leaders. In addition to the raw noisy user-generated data, we present a fine-grained annotated dataset of 3,848 messages that will be useful to understand the various dimensions of WhatsApp political campaigning. We present several complementary insights into the investigative and sensational news stories from the same period. Exploratory data analysis and experiments showcase several exciting results and future research opportunities. To facilitate reproducible research, we make the anonymized datasets available in the public domain.</abstract>
      <url hash="7c1b283e">2021.wnut-1.15</url>
      <bibkey>srivastava-singh-2021-poliwam</bibkey>
      <doi>10.18653/v1/2021.wnut-1.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>P</fixed-case>ars<fixed-case>T</fixed-case>wi<fixed-case>NER</fixed-case>: A Corpus for Named Entity Recognition at Informal <fixed-case>P</fixed-case>ersian</title>
      <author><first>MohammadMahdi</first><last>Aghajani</last></author>
      <author><first>AliAkbar</first><last>Badri</last></author>
      <author><first>Hamid</first><last>Beigy</last></author>
      <pages>131–136</pages>
      <abstract>As a result of unstructured sentences and some misspellings and errors, finding named entities in a noisy environment such as social media takes much more effort. ParsTwiNER contains about 250k tokens, based on standard instructions like MUC-6 or CoNLL 2003, gathered from Persian Twitter. Using Cohen’s Kappa coefficient, the consistency of annotators is 0.95, a high score. In this study, we demonstrate that some state-of-the-art models degrade on these corpora, and trained a new model using parallel transfer learning based on the BERT architecture. Experimental results show that the model works well in informal Persian as well as in formal Persian.</abstract>
      <url hash="fbdd3493">2021.wnut-1.16</url>
      <bibkey>aghajani-etal-2021-parstwiner</bibkey>
      <doi>10.18653/v1/2021.wnut-1.16</doi>
      <pwccode url="https://github.com/overfit-ir/parstwiner" additional="false">overfit-ir/parstwiner</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/parstwiner">ParsTwiner</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/peyma">PEYMA</pwcdataset>
    </paper>
    <paper id="17">
      <title><fixed-case>D</fixed-case>ream<fixed-case>D</fixed-case>rug - A crowdsourced <fixed-case>NER</fixed-case> dataset for detecting drugs in darknet markets</title>
      <author><first>Johannes</first><last>Bogensperger</last></author>
      <author><first>Sven</first><last>Schlarb</last></author>
      <author><first>Allan</first><last>Hanbury</last></author>
      <author><first>Gábor</first><last>Recski</last></author>
      <pages>137–157</pages>
      <abstract>We present DreamDrug, a crowdsourced dataset for detecting mentions of drugs in noisy user-generated item listings from darknet markets. Our dataset contains nearly 15,000 manually annotated drug entities in over 3,500 item listings scraped from the darknet market platform “DreamMarket” in 2017. We also train and evaluate baseline models for detecting these entities, using contextual language models fine-tuned in a few-shot setting and on the full dataset, and examine the effect of pretraining on in-domain unannotated corpora.</abstract>
      <url hash="835df13e">2021.wnut-1.17</url>
      <bibkey>bogensperger-etal-2021-dreamdrug</bibkey>
      <doi>10.18653/v1/2021.wnut-1.17</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
    </paper>
    <paper id="18">
      <title>Comparing Grammatical Theories of Code-Mixing</title>
      <author><first>Adithya</first><last>Pratapa</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>158–167</pages>
      <abstract>Code-mixed text generation systems have found applications in many downstream tasks, including speech recognition, translation and dialogue. A paradigm of these generation systems relies on well-defined grammatical theories of code-mixing, and there is a lack of comparison of these theories. We present a large-scale human evaluation of two popular grammatical theories, Matrix-Embedded Language (ML) and Equivalence Constraint (EC). We compare them against three heuristic-based models and quantitatively demonstrate the effectiveness of the two grammatical theories.</abstract>
      <url hash="251fcc03">2021.wnut-1.18</url>
      <bibkey>pratapa-choudhury-2021-comparing</bibkey>
      <doi>10.18653/v1/2021.wnut-1.18</doi>
    </paper>
    <paper id="19">
      <title>Improving Punctuation Restoration for Speech Transcripts via External Data</title>
      <author><first>Xue-Yong</first><last>Fu</last></author>
      <author><first>Cheng</first><last>Chen</last></author>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last></author>
      <author><first>Shashi</first><last>Bhushan</last></author>
      <author><first>Simon</first><last>Corston-Oliver</last></author>
      <pages>168–174</pages>
      <abstract>Automatic Speech Recognition (ASR) systems generally do not produce punctuated transcripts. To make transcripts more readable and follow the expected input format for downstream language models, it is necessary to add punctuation marks. In this paper, we tackle the punctuation restoration problem specifically for the noisy text (e.g., phone conversation scenarios). To leverage the available written text datasets, we introduce a data sampling technique based on an n-gram language model to sample more training data that are similar to our in-domain data. Moreover, we propose a two-stage fine-tuning approach that utilizes the sampled external data as well as our in-domain dataset for models based on BERT. Extensive experiments show that the proposed approach outperforms the baseline with an improvement of 1.12% F1 score.</abstract>
      <url hash="f811d030">2021.wnut-1.19</url>
      <bibkey>fu-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.wnut-1.19</doi>
    </paper>
    <paper id="20">
      <title>Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation</title>
      <author><first>Yang</first><last>Deng</last></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>175–181</pages>
      <abstract>In this work, we propose a novel and easy-to-apply data augmentation strategy, namely <b>Bi</b>lateral <b>G</b>eneration (<b>BiG</b>), with a contrastive training objective for improving the performance of ranking question answer pairs with existing labeled data. In specific, we synthesize pseudo-positive QA pairs in contrast to the original negative QA pairs with two pre-trained generation models, one for question generation, the other for answer generation, which are fine-tuned on the limited positive QA pairs from the original dataset. With the augmented dataset, we design a contrastive training objective for learning to rank question answer pairs. Experimental results on three benchmark datasets show that our method significantly improves the performance of ranking models by making full use of existing labeled data and can be easily applied to different ranking models.</abstract>
      <url hash="4dac4f84">2021.wnut-1.20</url>
      <bibkey>deng-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.wnut-1.20</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="21">
      <title>Mitigation of Diachronic Bias in Fake News Detection Dataset</title>
      <author><first>Taichi</first><last>Murayama</last></author>
      <author><first>Shoko</first><last>Wakamiya</last></author>
      <author><first>Eiji</first><last>Aramaki</last></author>
      <pages>182–188</pages>
      <abstract>Fake news causes significant damage to society. To deal with these fake news, several studies on building detection models and arranging datasets have been conducted. Most of the fake news datasets depend on a specific time period. Consequently, the detection models trained on such a dataset have difficulty detecting novel fake news generated by political changes and social changes; they may possibly result in biased output from the input, including specific person names and organizational names. We refer to this problem as Diachronic Bias because it is caused by the creation date of news in each dataset. In this study, we confirm the bias, especially proper nouns including person names, from the deviation of phrase appearances in each dataset. Based on these findings, we propose masking methods using Wikidata to mitigate the influence of person names and validate whether they make fake news detection models robust through experiments with in-domain and out-of-domain data.</abstract>
      <url hash="6b3d5998">2021.wnut-1.21</url>
      <bibkey>murayama-etal-2021-mitigation</bibkey>
      <doi>10.18653/v1/2021.wnut-1.21</doi>
    </paper>
    <paper id="22">
      <title>Understanding the Impact of <fixed-case>UGC</fixed-case> Specificities on Translation Quality</title>
      <author><first>José Carlos</first><last>Rosales Núñez</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <pages>189–198</pages>
      <abstract>This work takes a critical look at the evaluation of user-generated content automatic translation, the well-known specificities of which raise many challenges for MT. Our analyses show that measuring the average-case performance using a standard metric on a UGC test set falls far short of giving a reliable image of the UGC translation quality. That is why we introduce a new data set for the evaluation of UGC translation in which UGC specificities have been manually annotated using a fine-grained typology. Using this data set, we conduct several experiments to measure the impact of different kinds of UGC specificities on translation quality, more precisely than previously possible.</abstract>
      <url hash="ed1094f8">2021.wnut-1.22</url>
      <bibkey>rosales-nunez-etal-2021-understanding</bibkey>
      <doi>10.18653/v1/2021.wnut-1.22</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="23">
      <title>Noisy <fixed-case>UGC</fixed-case> Translation at the Character Level: Revisiting Open-Vocabulary Capabilities and Robustness of Char-Based Models</title>
      <author><first>José Carlos</first><last>Rosales Núñez</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>199–211</pages>
      <abstract>This work explores the capacities of character-based Neural Machine Translation to translate noisy User-Generated Content (UGC) with a strong focus on exploring the limits of such approaches to handle productive UGC phenomena, which almost by definition, cannot be seen at training time. Within a strict zero-shot scenario, we first study the detrimental impact on translation performance of various user-generated content phenomena on a small annotated dataset we developed and then show that such models are indeed incapable of handling unknown letters, which leads to catastrophic translation failure once such characters are encountered. We further confirm this behavior with a simple, yet insightful, copy task experiment and highlight the importance of reducing the vocabulary size hyper-parameter to increase the robustness of character-based models for machine translation.</abstract>
      <url hash="ce2752ee">2021.wnut-1.23</url>
      <bibkey>rosales-nunez-etal-2021-noisy</bibkey>
      <doi>10.18653/v1/2021.wnut-1.23</doi>
      <pwccode url="https://github.com/josecar25/char_based_nmt-noisy_ugc" additional="false">josecar25/char_based_nmt-noisy_ugc</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="24">
      <title>Changes in <fixed-case>T</fixed-case>witter geolocations: Insights and suggestions for future usage</title>
      <author><first>Anna</first><last>Kruspe</last></author>
      <author><first>Matthias</first><last>Häberle</last></author>
      <author><first>Eike J.</first><last>Hoffmann</last></author>
      <author><first>Samyo</first><last>Rode-Hasinger</last></author>
      <author><first>Karam</first><last>Abdulahhad</last></author>
      <author><first>Xiao Xiang</first><last>Zhu</last></author>
      <pages>212–221</pages>
      <abstract>Twitter data has become established as a valuable source of data for various application scenarios in the past years. For many such applications, it is necessary to know where Twitter posts (tweets) were sent from or what location they refer to. Researchers have frequently used exact coordinates provided in a small percentage of tweets, but Twitter removed the option to share these coordinates in mid-2019. Moreover, there is reason to suspect that a large share of the provided coordinates did not correspond to GPS coordinates of the user even before that. In this paper, we explain the situation and the 2019 policy change and shed light on the various options of still obtaining location information from tweets. We provide usage statistics including changes over time, and analyze what the removal of exact coordinates means for various common research tasks performed with Twitter data. Finally, we make suggestions for future research requiring geolocated tweets.</abstract>
      <url hash="80627891">2021.wnut-1.24</url>
      <bibkey>kruspe-etal-2021-changes</bibkey>
      <doi>10.18653/v1/2021.wnut-1.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>C</fixed-case>on<fixed-case>Q</fixed-case>uest: Contextual Question Paraphrasing through Answer-Aware Synthetic Question Generation</title>
      <author><first>Mostafa</first><last>Mirshekari</last></author>
      <author><first>Jing</first><last>Gu</last></author>
      <author><first>Aaron</first><last>Sisto</last></author>
      <pages>222–229</pages>
      <abstract>Despite excellent performance on tasks such as question answering, Transformer-based architectures remain sensitive to syntactic and contextual ambiguities. Question Paraphrasing (QP) offers a promising solution as a means to augment existing datasets. The main challenges of current QP models include lack of training data and difficulty in generating diverse and natural questions. In this paper, we present Conquest, a framework for generating synthetic datasets for contextual question paraphrasing. To this end, Conquest first employs an answer-aware question generation (QG) model to create a question-pair dataset and then uses this data to train a contextualized question paraphrasing model. We extensively evaluate Conquest and show its ability to produce more diverse and fluent question pairs than existing approaches. Our contextual paraphrase model also establishes a strong baseline for end-to-end contextual paraphrasing. Further, We find that context can improve BLEU-1 score on contextual compression and expansion by 4.3 and 11.2 respectively, compared to a non-contextual model.</abstract>
      <url hash="9421b5d1">2021.wnut-1.25</url>
      <bibkey>mirshekari-etal-2021-conquest</bibkey>
      <doi>10.18653/v1/2021.wnut-1.25</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="26">
      <title><fixed-case>NADE</fixed-case>: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations</title>
      <author><first>Simone</first><last>Scaboro</last></author>
      <author><first>Beatrice</first><last>Portelli</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <author><first>Giuseppe</first><last>Serra</last></author>
      <pages>230–237</pages>
      <abstract>Adverse Drug Event (ADE) extraction models can rapidly examine large collections of social media texts, detecting mentions of drug-related adverse reactions and trigger medical investigations. However, despite the recent advances in NLP, it is currently unknown if such models are robust in face of negation, which is pervasive across language varieties. In this paper we evaluate three state-of-the-art systems, showing their fragility against negation, and then we introduce two possible strategies to increase the robustness of these models: a pipeline approach, relying on a specific component for negation detection; an augmentation of an ADE extraction dataset to artificially create negated samples and further train the models. We show that both strategies bring significant increases in performance, lowering the number of spurious entities predicted by the models. Our dataset and code will be publicly released to encourage research on the topic.</abstract>
      <url hash="d12b38ef">2021.wnut-1.26</url>
      <attachment type="Software" hash="e195929d">2021.wnut-1.26.Software.zip</attachment>
      <bibkey>scaboro-etal-2021-nade</bibkey>
      <doi>10.18653/v1/2021.wnut-1.26</doi>
      <pwccode url="https://github.com/ailabudinegit/nade-dataset" additional="false">ailabudinegit/nade-dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/smm4h">SMM4H</pwcdataset>
    </paper>
    <paper id="27">
      <title><fixed-case>S</fixed-case>pan<fixed-case>A</fixed-case>lign: Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining</title>
      <author><first>Léo</first><last>Jacqmin</last></author>
      <author><first>Gabriel</first><last>Marzinotto</last></author>
      <author><first>Justyna</first><last>Gromada</last></author>
      <author><first>Ewelina</first><last>Szczekocka</last></author>
      <author><first>Robert</first><last>Kołodyński</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <pages>238–248</pages>
      <abstract>Following the increasing performance of neural machine translation systems, the paradigm of using automatically translated data for cross-lingual adaptation is now studied in several applicative domains. The capacity to accurately project annotations remains however an issue for sequence tagging tasks where annotation must be projected with correct spans. Additionally, when the task implies noisy user-generated text, the quality of translation and annotation projection can be affected. In this paper we propose to tackle multilingual sequence tagging with a new span alignment method and apply it to opinion target extraction from customer reviews. We show that provided suitable heuristics, translated data with automatic span-level annotation projection can yield improvements both for cross-lingual adaptation compared to zero-shot transfer, and data augmentation compared to a multilingual baseline.</abstract>
      <url hash="0c6832bf">2021.wnut-1.27</url>
      <bibkey>jacqmin-etal-2021-spanalign</bibkey>
      <doi>10.18653/v1/2021.wnut-1.27</doi>
    </paper>
    <paper id="28">
      <title>A Novel Framework for Detecting Important Subevents from Crisis Events via Dynamic Semantic Graphs</title>
      <author><first>Evangelia</first><last>Spiliopoulou</last></author>
      <author><first>Tanay Kumar</first><last>Saha</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes</last></author>
      <pages>249–259</pages>
      <abstract>Social media is an essential tool to share information about crisis events, such as natural disasters. Event Detection aims at extracting information in the form of an event, but considers each event in isolation, without combining information across sentences or events. Many posts in Crisis NLP contain repetitive or complementary information which needs to be aggregated (e.g., the number of trapped people and their location) for disaster response. Although previous approaches in Crisis NLP aggregate information across posts, they only use shallow representations of the content (e.g., keywords), which cannot adequately represent the semantics of a crisis event and its sub-events. In this work, we propose a novel framework to extract critical sub-events from a large-scale crisis event by combining important information across relevant tweets. Our framework first converts all the tweets from a crisis event into a temporally-ordered set of graphs. Then it extracts sub-graphs that represent semantic relationships connecting verbs and nouns in 3 to 6 node sub-graphs. It does this by learning edge weights via Dynamic Graph Convolutional Networks (DGCNs) and extracting smaller, relevant sub-graphs. Our experiments show that our extracted structures (1) are semantically meaningful sub-events and (2) contain information important for the large crisis-event. Furthermore, we show that our approach significantly outperforms event detection baselines, highlighting the importance of aggregating information across tweets for our task.</abstract>
      <url hash="17858d59">2021.wnut-1.28</url>
      <bibkey>spiliopoulou-etal-2021-novel</bibkey>
      <doi>10.18653/v1/2021.wnut-1.28</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="29">
      <title>Synthetic Data Generation and Multi-Task Learning for Extracting Temporal Information from Health-Related Narrative Text</title>
      <author><first>Heereen</first><last>Shim</last></author>
      <author><first>Dietwig</first><last>Lowet</last></author>
      <author><first>Stijn</first><last>Luca</last></author>
      <author><first>Bart</first><last>Vanrumste</last></author>
      <pages>260–273</pages>
      <abstract>Extracting temporal information is critical to process health-related text. Temporal information extraction is a challenging task for language models because it requires processing both texts and numbers. Moreover, the fundamental challenge is how to obtain a large-scale training dataset. To address this, we propose a synthetic data generation algorithm. Also, we propose a novel multi-task temporal information extraction model and investigate whether multi-task learning can contribute to performance improvement by exploiting additional training signals with the existing training data. For experiments, we collected a custom dataset containing unstructured texts with temporal information of sleep-related activities. Experimental results show that utilising synthetic data can improve the performance when the augmentation factor is 3. The results also show that when multi-task learning is used with an appropriate amount of synthetic data, the performance can significantly improve from 82. to 88.6 and from 83.9 to 91.9 regarding micro-and macro-average exact match scores of normalised time prediction, respectively.</abstract>
      <url hash="df876777">2021.wnut-1.29</url>
      <bibkey>shim-etal-2021-synthetic</bibkey>
      <doi>10.18653/v1/2021.wnut-1.29</doi>
    </paper>
    <paper id="30">
      <title>Neural-based <fixed-case>RST</fixed-case> Parsing And Analysis In Persuasive Discourse</title>
      <author><first>Jinfen</first><last>Li</last></author>
      <author><first>Lu</first><last>Xiao</last></author>
      <pages>274–283</pages>
      <abstract>Most of the existing studies of language use in social media content have focused on the surface-level linguistic features (e.g., function words and punctuation marks) and the semantic level aspects (e.g., the topics, sentiment, and emotions) of the comments. The writer’s strategies of constructing and connecting text segments have not been widely explored even though this knowledge is expected to shed light on how people reason in online environments. Contributing to this analysis direction for social media studies, we build an openly accessible neural RST parsing system that analyzes discourse relations in an online comment. Our experiments demonstrate that this system achieves comparable performance among all the neural RST parsing systems. To demonstrate the use of this tool in social media analysis, we apply it to identify the discourse relations in persuasive and non-persuasive comments and examine the relationships among the binary discourse tree depth, discourse relations, and the perceived persuasiveness of online comments. Our work demonstrates the potential of analyzing discourse structures of online comments with our system and the implications of these structures for understanding online communications.</abstract>
      <url hash="2696cac7">2021.wnut-1.30</url>
      <bibkey>li-xiao-2021-neural</bibkey>
      <doi>10.18653/v1/2021.wnut-1.30</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>BART</fixed-case> for Post-Correction of <fixed-case>OCR</fixed-case> Newspaper Text</title>
      <author><first>Elizabeth</first><last>Soper</last></author>
      <author><first>Stanley</first><last>Fujimoto</last></author>
      <author><first>Yen-Yun</first><last>Yu</last></author>
      <pages>284–290</pages>
      <abstract>Optical character recognition (OCR) from newspaper page images is susceptible to noise due to degradation of old documents and variation in typesetting. In this report, we present a novel approach to OCR post-correction. We cast error correction as a translation task, and fine-tune BART, a transformer-based sequence-to-sequence language model pretrained to denoise corrupted text. We are the first to use sentence-level transformer models for OCR post-correction, and our best model achieves a 29.4% improvement in character accuracy over the original noisy OCR text. Our results demonstrate the utility of pretrained language models for dealing with noisy text.</abstract>
      <url hash="d4001fb1">2021.wnut-1.31</url>
      <bibkey>soper-etal-2021-bart</bibkey>
      <doi>10.18653/v1/2021.wnut-1.31</doi>
    </paper>
    <paper id="32">
      <title>Coping with Noisy Training Data Labels in Paraphrase Detection</title>
      <author><first>Teemu</first><last>Vahtola</last></author>
      <author><first>Mathias</first><last>Creutz</last></author>
      <author><first>Eetu</first><last>Sjöblom</last></author>
      <author><first>Sami</first><last>Itkonen</last></author>
      <pages>291–296</pages>
      <abstract>We present new state-of-the-art benchmarks for paraphrase detection on all six languages in the Opusparcus sentential paraphrase corpus: English, Finnish, French, German, Russian, and Swedish. We reach these baselines by fine-tuning BERT. The best results are achieved on smaller and cleaner subsets of the training sets than was observed in previous research. Additionally, we study a translation-based approach that is competitive for the languages with more limited and noisier training data.</abstract>
      <url hash="d302747c">2021.wnut-1.32</url>
      <bibkey>vahtola-etal-2021-coping</bibkey>
      <doi>10.18653/v1/2021.wnut-1.32</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opusparcus">Opusparcus</pwcdataset>
    </paper>
    <paper id="33">
      <title>Knowledge Distillation with Noisy Labels for Natural Language Understanding</title>
      <author><first>Shivendra</first><last>Bhardwaj</last></author>
      <author><first>Abbas</first><last>Ghaddar</last></author>
      <author><first>Ahmad</first><last>Rashid</last></author>
      <author><first>Khalil</first><last>Bibi</last></author>
      <author><first>Chengyang</first><last>Li</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>297–303</pages>
      <abstract>Knowledge Distillation (KD) is extensively used to compress and deploy large pre-trained language models on edge devices for real-world applications. However, one neglected area of research is the impact of noisy (corrupted) labels on KD. We present, to the best of our knowledge, the first study on KD with noisy labels in Natural Language Understanding (NLU). We document the scope of the problem and present two methods to mitigate the impact of label noise. Experiments on the GLUE benchmark show that our methods are effective even under high noise levels. Nevertheless, our results indicate that more research is necessary to cope with label noise under the KD.</abstract>
      <url hash="ec11cdb7">2021.wnut-1.33</url>
      <bibkey>bhardwaj-etal-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.wnut-1.33</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="34">
      <title>Integrating Transformers and Knowledge Graphs for <fixed-case>T</fixed-case>witter Stance Detection</title>
      <author><first>Thomas</first><last>Clark</last></author>
      <author><first>Costanza</first><last>Conforti</last></author>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Zaiqiao</first><last>Meng</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>304–312</pages>
      <abstract>Stance detection (SD) entails classifying the sentiment of a text towards a given target, and is a relevant sub-task for opinion mining and social media analysis. Recent works have explored knowledge infusion supplementing the linguistic competence and latent knowledge of large pre-trained language models with structured knowledge graphs (KGs), yet few works have applied such methods to the SD task. In this work, we first perform stance-relevant knowledge probing on Transformers-based pre-trained models in a zero-shot setting, showing these models’ latent real-world knowledge about SD targets and their sensitivity to context. We then train and evaluate new knowledge-enriched stance detection models on two Twitter stance datasets, achieving state-of-the-art performance on both.</abstract>
      <url hash="13e8059c">2021.wnut-1.34</url>
      <bibkey>clark-etal-2021-integrating</bibkey>
      <doi>10.18653/v1/2021.wnut-1.34</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="35">
      <title>Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media</title>
      <author><first>Sayan</first><last>Ghosh</last></author>
      <author><first>Dylan</first><last>Baker</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <pages>313–328</pages>
      <abstract>Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to marginalized groups, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations/lexicons available. Consequently, biases concerning non-Western contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geo-cultural contexts. Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts. We also conduct analysis of a model trained on a dataset with ground truth labels to better understand these biases, and present preliminary mitigation experiments.</abstract>
      <url hash="66cc0450">2021.wnut-1.35</url>
      <bibkey>ghosh-etal-2021-detecting</bibkey>
      <doi>10.18653/v1/2021.wnut-1.35</doi>
    </paper>
    <paper id="36">
      <title>Detection of Puffery on the <fixed-case>E</fixed-case>nglish <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Amanda</first><last>Bertsch</last></author>
      <author><first>Steven</first><last>Bethard</last></author>
      <pages>329–333</pages>
      <abstract>On Wikipedia, an online crowdsourced encyclopedia, volunteers enforce the encyclopedia’s editorial policies. Wikipedia’s policy on maintaining a neutral point of view has inspired recent research on bias detection, including “weasel words” and “hedges”. Yet to date, little work has been done on identifying “puffery,” phrases that are overly positive without a verifiable source. We demonstrate that collecting training data for this task requires some care, and construct a dataset by combining Wikipedia editorial annotations and information retrieval techniques. We compare several approaches to predicting puffery, and achieve 0.963 f1 score by incorporating citation features into a RoBERTa model. Finally, we demonstrate how to integrate our model with Wikipedia’s public infrastructure to give back to the Wikipedia editor community.</abstract>
      <url hash="17a682c2">2021.wnut-1.36</url>
      <bibkey>bertsch-bethard-2021-detection</bibkey>
      <doi>10.18653/v1/2021.wnut-1.36</doi>
      <pwccode url="https://github.com/abertsch72/wikipedia-puffery-detection" additional="false">abertsch72/wikipedia-puffery-detection</pwccode>
    </paper>
    <paper id="37">
      <title>Robustness and Sensitivity of <fixed-case>BERT</fixed-case> Models Predicting <fixed-case>A</fixed-case>lzheimer’s Disease from Text</title>
      <author><first>Jekaterina</first><last>Novikova</last></author>
      <pages>334–339</pages>
      <abstract>Understanding robustness and sensitivity of BERT models predicting Alzheimer’s disease from text is important for both developing better classification models and for understanding their capabilities and limitations. In this paper, we analyze how a controlled amount of desired and undesired text alterations impacts performance of BERT. We show that BERT is robust to natural linguistic variations in text. On the other hand, we show that BERT is not sensitive to removing clinically important information from text.</abstract>
      <url hash="02c68688">2021.wnut-1.37</url>
      <bibkey>novikova-2021-robustness</bibkey>
      <doi>10.18653/v1/2021.wnut-1.37</doi>
    </paper>
    <paper id="38">
      <title>Understanding Model Robustness to User-generated Noisy Texts</title>
      <author><first>Jakub</first><last>Náplava</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Jana</first><last>Straková</last></author>
      <pages>340–350</pages>
      <abstract>Sensitivity of deep-neural models to input noise is known to be a challenging problem. In NLP, model performance often deteriorates with naturally occurring noise, such as spelling errors. To mitigate this issue, models may leverage artificially noised data. However, the amount and type of generated noise has so far been determined arbitrarily. We therefore propose to model the errors statistically from grammatical-error-correction corpora. We present a thorough evaluation of several state-of-the-art NLP systems’ robustness in multiple languages, with tasks including morpho-syntactic analysis, named entity recognition, neural machine translation, a subset of the GLUE benchmark and reading comprehension. We also compare two approaches to address the performance drop: a) training the NLP models with noised data generated by our framework; and b) reducing the input noise with external system for natural language correction. The code is released at https://github.com/ufal/kazitext.</abstract>
      <url hash="e8d84f14">2021.wnut-1.38</url>
      <bibkey>naplava-etal-2021-understanding</bibkey>
      <doi>10.18653/v1/2021.wnut-1.38</doi>
      <pwccode url="https://github.com/ufal/kazitext" additional="false">ufal/kazitext</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/akces-gec">AKCES-GEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="39">
      <title><fixed-case>CIDE</fixed-case>r-<fixed-case>R</fixed-case>: Robust Consensus-based Image Description Evaluation</title>
      <author><first>Gabriel</first><last>Oliveira dos Santos</last></author>
      <author><first>Esther Luna</first><last>Colombini</last></author>
      <author><first>Sandra</first><last>Avila</last></author>
      <pages>351–360</pages>
      <abstract>This paper shows that CIDEr-D, a traditional evaluation metric for image description, does not work properly on datasets where the number of words in the sentence is significantly greater than those in the MS COCO Captions dataset. We also show that CIDEr-D has performance hampered by the lack of multiple reference sentences and high variance of sentence length. To bypass this problem, we introduce CIDEr-R, which improves CIDEr-D, making it more flexible in dealing with datasets with high sentence length variance. We demonstrate that CIDEr-R is more accurate and closer to human judgment than CIDEr-D; CIDEr-R is more robust regarding the number of available references. Our results reveal that using Self-Critical Sequence Training to optimize CIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized, the generated captions’ length tends to be similar to the reference length. However, the models also repeat several times the same word to increase the sentence length.</abstract>
      <url hash="1e750212">2021.wnut-1.39</url>
      <bibkey>oliveira-dos-santos-etal-2021-cider</bibkey>
      <doi>10.18653/v1/2021.wnut-1.39</doi>
      <pwccode url="https://github.com/ruotianluo/coco-caption" additional="false">ruotianluo/coco-caption</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco-captions">COCO Captions</pwcdataset>
    </paper>
    <paper id="40">
      <title>Improved Named Entity Recognition for Noisy Call Center Transcripts</title>
      <author><first>Sam</first><last>Davidson</last></author>
      <author><first>Jordan</first><last>Hosier</last></author>
      <author><first>Yu</first><last>Zhou</last></author>
      <author><first>Vijay</first><last>Gurbani</last></author>
      <pages>361–370</pages>
      <abstract>We explore the application of state-of-the-art NER algorithms to ASR-generated call center transcripts. Previous work in this domain focused on the use of a BiLSTM-CRF model which relied on Flair embeddings; however, such a model is unwieldy in terms of latency and memory consumption. In a production environment, end users require low-latency models which can be readily integrated into existing pipelines. To that end, we present two different models which can be utilized based on the latency and accuracy requirements of the user. First, we propose a set of models which utilize state-of-the-art Transformer language models (RoBERTa) to develop a high-accuracy NER system trained on custom annotated set of call center transcripts. We then use our best-performing Transformer-based model to label a large number of transcripts, which we use to pretrain a BiLSTM-CRF model and further fine-tune on our annotated dataset. We show that this model, while not as accurate as its Transformer-based counterpart, is highly effective in identifying items which require redaction for privacy law compliance. Further, we propose a new general annotation scheme for NER in the call-center environment.</abstract>
      <url hash="c84c0ef8">2021.wnut-1.40</url>
      <bibkey>davidson-etal-2021-improved</bibkey>
      <doi>10.18653/v1/2021.wnut-1.40</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="41">
      <title>Contrapositive Local Class Inference</title>
      <author><first>Omid</first><last>Kashefi</last></author>
      <author><first>Rebecca</first><last>Hwa</last></author>
      <pages>371–380</pages>
      <abstract>Certain types of classification problems may be performed at multiple levels of granularity; for example, we might want to know the sentiment polarity of a document or a sentence, or a phrase. Often, the prediction at a greater-context (e.g., sentences or paragraphs) may be informative for a more localized prediction at a smaller semantic unit (e.g., words or phrases). However, directly inferring the most salient local features from the global prediction may overlook the semantics of this relationship. This work argues that inference along the contraposition relationship of the local prediction and the corresponding global prediction makes an inference framework that is more accurate and robust to noise. We show how this contraposition framework can be implemented as a transfer function that rewrites a greater-context from one class to another and demonstrate how an appropriate transfer function can be trained from a noisy user-generated corpus. The experimental results validate our insight that the proposed contrapositive framework outperforms the alternative approaches on resource-constrained problem domains.</abstract>
      <url hash="2a07fa07">2021.wnut-1.41</url>
      <bibkey>kashefi-hwa-2021-contrapositive</bibkey>
      <doi>10.18653/v1/2021.wnut-1.41</doi>
      <pwccode url="https://github.com/omidkashefi/contrapositive-inference" additional="false">omidkashefi/contrapositive-inference</pwccode>
    </paper>
    <paper id="42">
      <title>Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction</title>
      <author><first>Shubhanshu</first><last>Mishra</last></author>
      <author><first>Aria</first><last>Haghighi</last></author>
      <pages>381–388</pages>
      <abstract>We evaluate a simple approach to improving zero-shot multilingual transfer of mBERT on social media corpus by adding a pretraining task called translation pair prediction (TPP), which predicts whether a pair of cross-lingual texts are a valid translation. Our approach assumes access to translations (exact or approximate) between source-target language pairs, where we fine-tune a model on source language task data and evaluate the model in the target language. In particular, we focus on language pairs where transfer learning is difficult for mBERT: those where source and target languages are different in script, vocabulary, and linguistic typology. We show improvements from TPP pretraining over mBERT alone in zero-shot transfer from English to Hindi, Arabic, and Japanese on two social media tasks: NER (a 37% average relative improvement in F1 across target languages) and sentiment classification (12% relative improvement in F1) on social media text, while also benchmarking on a non-social media task of Universal Dependency POS tagging (6.7% relative improvement in accuracy). Our results are promising given the lack of social media bitext corpus. Our code can be found at: https://github.com/twitter-research/multilingual-alignment-tpp.</abstract>
      <url hash="d9d16d6b">2021.wnut-1.42</url>
      <bibkey>mishra-haghighi-2021-improved</bibkey>
      <doi>10.18653/v1/2021.wnut-1.42</doi>
      <video href="2021.wnut-1.42.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="43">
      <title>Co-training for Commit Classification</title>
      <author><first>Jian Yi David</first><last>Lee</last></author>
      <author><first>Hai Leong</first><last>Chieu</last></author>
      <pages>389–395</pages>
      <abstract>Commits in version control systems (e.g. Git) track changes in a software project. Commits comprise noisy user-generated natural language and code patches. Automatic commit classification (CC) has been used to determine the type of code maintenance activities performed, as well as to detect bug fixes in code repositories. Much prior work occurs in the fully-supervised setting – a setting that can be a stretch in resource-scarce situations presenting difficulties in labeling commits. In this paper, we apply co-training, a semi-supervised learning method, to take advantage of the two views available – the commit message (natural language) and the code changes (programming language) – to improve commit classification.</abstract>
      <url hash="921bda46">2021.wnut-1.43</url>
      <bibkey>lee-chieu-2021-co</bibkey>
      <doi>10.18653/v1/2021.wnut-1.43</doi>
      <pwccode url="https://github.com/davidleejy/wnut21-cotrain" additional="false">davidleejy/wnut21-cotrain</pwccode>
    </paper>
    <paper id="44">
      <title>Study of Manifestation of Civil Unrest on <fixed-case>T</fixed-case>witter</title>
      <author><first>Abhinav</first><last>Chinta</last></author>
      <author><first>Jingyu</first><last>Zhang</last></author>
      <author><first>Alexandra</first><last>DeLucia</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <author><first>Anna L.</first><last>Buczak</last></author>
      <pages>396–409</pages>
      <abstract>Twitter is commonly used for civil unrest detection and forecasting tasks, but there is a lack of work in evaluating <i>how</i> civil unrest manifests on Twitter across countries and events. We present two in-depth case studies for two specific large-scale events, one in a country with high (English) Twitter usage (Johannesburg riots in South Africa) and one in a country with low Twitter usage (Burayu massacre protests in Ethiopia). We show that while there is event signal during the events, there is little signal leading up to the events. In addition to the case studies, we train Ngram-based models on a larger set of Twitter civil unrest data across time, events, and countries and use machine learning explainability tools (SHAP) to identify important features. The models were able to find words indicative of civil unrest that generalized across countries. The 42 countries span Africa, Middle East, and Southeast Asia and the events range occur between 2014 and 2019.</abstract>
      <url hash="9f7726a5">2021.wnut-1.44</url>
      <bibkey>chinta-etal-2021-study</bibkey>
      <doi>10.18653/v1/2021.wnut-1.44</doi>
      <pwccode url="https://github.com/aadelucia/civil-unrest-case-study" additional="false">aadelucia/civil-unrest-case-study</pwccode>
    </paper>
    <paper id="45">
      <title>The <fixed-case>K</fixed-case>orean Morphologically Tight-Fitting Tokenizer for Noisy User-Generated Texts</title>
      <author><first>Sangah</first><last>Lee</last></author>
      <author><first>Hyopil</first><last>Shin</last></author>
      <pages>410–416</pages>
      <abstract>User-generated texts include various types of stylistic properties, or noises. Such texts are not properly processed by existing morpheme analyzers or language models based on formal texts such as encyclopedias or news articles. In this paper, we propose a simple morphologically tight-fitting tokenizer (K-MT) that can better process proper nouns, coinages, and internet slang among other types of noise in Korean user-generated texts. We tested our tokenizer by performing classification tasks on Korean user-generated movie reviews and hate speech datasets, and the Korean Named Entity Recognition dataset. Through our tests, we found that K-MT is better fit to process internet slangs, proper nouns, and coinages, compared to a morpheme analyzer and a character-level WordPiece tokenizer.</abstract>
      <url hash="338f2ac3">2021.wnut-1.45</url>
      <bibkey>lee-shin-2021-korean</bibkey>
      <doi>10.18653/v1/2021.wnut-1.45</doi>
    </paper>
    <paper id="46">
      <title>Character Transformations for Non-Autoregressive <fixed-case>GEC</fixed-case> Tagging</title>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Jakub</first><last>Náplava</last></author>
      <author><first>Jana</first><last>Straková</last></author>
      <pages>417–422</pages>
      <abstract>We propose a character-based non-autoregressive GEC approach, with automatically generated character transformations. Recently, per-word classification of correction edits has proven an efficient, parallelizable alternative to current encoder-decoder GEC systems. We show that word replacement edits may be suboptimal and lead to explosion of rules for spelling, diacritization and errors in morphologically rich languages, and propose a method for generating character transformations from GEC corpus. Finally, we train character transformation models for Czech, German and Russian, reaching solid results and dramatic speedup compared to autoregressive systems. The source code is released at https://github.com/ufal/wnut2021_character_transformations_gec.</abstract>
      <url hash="23583507">2021.wnut-1.46</url>
      <bibkey>straka-etal-2021-character</bibkey>
      <doi>10.18653/v1/2021.wnut-1.46</doi>
      <pwccode url="https://github.com/ufal/wnut2021_character_transformations_gec" additional="false">ufal/wnut2021_character_transformations_gec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/akces-gec">AKCES-GEC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
    </paper>
    <paper id="47">
      <title>Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?</title>
      <author><first>Arij</first><last>Riabi</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <pages>423–436</pages>
      <abstract>Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability set- tings.</abstract>
      <url hash="0dee8f63">2021.wnut-1.47</url>
      <bibkey>riabi-etal-2021-character</bibkey>
      <doi>10.18653/v1/2021.wnut-1.47</doi>
    </paper>
    <paper id="48">
      <title>“Something Something Hota Hai!” An Explainable Approach towards Sentiment Analysis on <fixed-case>I</fixed-case>ndian Code-Mixed Data</title>
      <author><first>Aman</first><last>Priyanshu</last></author>
      <author><first>Aleti</first><last>Vardhan</last></author>
      <author><first>Sudarshan</first><last>Sivakumar</last></author>
      <author><first>Supriti</first><last>Vijay</last></author>
      <author><first>Nipuna</first><last>Chhabra</last></author>
      <pages>437–444</pages>
      <abstract>The increasing use of social media sites in countries like India has given rise to large volumes of code-mixed data. Sentiment analysis of this data can provide integral insights into people’s perspectives and opinions. Code-mixed data is often noisy in nature due to multiple spellings for the same word, lack of definite order of words in a sentence, and random abbreviations. Thus, working with code-mixed data is more challenging than monolingual data. Interpreting a model’s predictions allows us to determine the robustness of the model against different forms of noise. In this paper, we propose a methodology to integrate explainable approaches into code-mixed sentiment analysis. By interpreting the predictions of sentiment analysis models we evaluate how well the model is able to adapt to the implicit noises present in code-mixed data.</abstract>
      <url hash="8350c695">2021.wnut-1.48</url>
      <bibkey>priyanshu-etal-2021-something</bibkey>
      <doi>10.18653/v1/2021.wnut-1.48</doi>
    </paper>
    <paper id="49">
      <title><fixed-case>BERT</fixed-case>weet<fixed-case>FR</fixed-case> : Domain Adaptation of Pre-Trained Language Models for <fixed-case>F</fixed-case>rench Tweets</title>
      <author><first>Yanzhu</first><last>Guo</last></author>
      <author><first>Virgile</first><last>Rennard</last></author>
      <author><first>Christos</first><last>Xypolopoulos</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <pages>445–450</pages>
      <abstract>We introduce BERTweetFR, the first large-scale pre-trained language model for French tweets. Our model is initialised using a general-domain French language model CamemBERT which follows the base architecture of BERT. Experiments show that BERTweetFR outperforms all previous general-domain French language models on two downstream Twitter NLP tasks of offensiveness identification and named entity recognition. The dataset used in the offensiveness detection task is first created and annotated by our team, filling in the gap of such analytic datasets in French. We make our model publicly available in the transformers library with the aim of promoting future research in analytic tasks for French tweets.</abstract>
      <url hash="594c8dad">2021.wnut-1.49</url>
      <bibkey>guo-etal-2021-bertweetfr</bibkey>
      <doi>10.18653/v1/2021.wnut-1.49</doi>
      <pwccode url="" additional="true"/>
    </paper>
    <paper id="50">
      <title>To What Extent Does Lexical Normalization Help <fixed-case>E</fixed-case>nglish-as-a-Second Language Learners to Read Noisy <fixed-case>E</fixed-case>nglish Texts?</title>
      <author><first>Yo</first><last>Ehara</last></author>
      <pages>451–456</pages>
      <abstract>How difficult is it for English-as-a-second language (ESL) learners to read noisy English texts? Do ESL learners need lexical normalization to read noisy English texts? These questions may also affect community formation on social networking sites where differences can be attributed to ESL learners and native English speakers. However, few studies have addressed these questions. To this end, we built highly accurate readability assessors to evaluate the readability of texts for ESL learners. We then applied these assessors to noisy English texts to further assess the readability of the texts. The experimental results showed that although intermediate-level ESL learners can read most noisy English texts in the first place, lexical normalization significantly improves the readability of noisy English texts for ESL learners.</abstract>
      <url hash="4c6e5af1">2021.wnut-1.50</url>
      <bibkey>ehara-2021-extent-lexical</bibkey>
      <doi>10.18653/v1/2021.wnut-1.50</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    <paper id="51">
      <title>Multilingual Sequence Labeling Approach to solve Lexical Normalization</title>
      <author><first>Divesh</first><last>Kubal</last></author>
      <author><first>Apurva</first><last>Nagvenkar</last></author>
      <pages>457–464</pages>
      <abstract>The task of converting a nonstandard text to a standard and readable text is known as lexical normalization. Almost all the Natural Language Processing (NLP) applications require the text data in normalized form to build quality task-specific models. Hence, lexical normalization has been proven to improve the performance of numerous natural language processing tasks on social media. This study aims to solve the problem of Lexical Normalization by formulating the Lexical Normalization task as a Sequence Labeling problem. This paper proposes a sequence labeling approach to solve the problem of Lexical Normalization in combination with the word-alignment technique. The goal is to use a single model to normalize text in various languages namely Croatian, Danish, Dutch, English, Indonesian-English, German, Italian, Serbian, Slovenian, Spanish, Turkish, and Turkish-German. This is a shared task in “2021 The 7th Workshop on Noisy User-generated Text (W-NUT)” in which the participants are expected to create a system/model that performs lexical normalization, which is the translation of non-canonical texts into their canonical equivalents, comprising data from over 12 languages. The proposed single multilingual model achieves an overall ERR score of 43.75 on intrinsic evaluation and an overall Labeled Attachment Score (LAS) score of 63.12 on extrinsic evaluation. Further, the proposed method achieves the highest Error Reduction Rate (ERR) score of 61.33 among the participants in the shared task. This study highlights the effects of using additional training data to get better results as well as using a pre-trained Language model trained on multiple languages rather than only on one language.</abstract>
      <url hash="8be67a39">2021.wnut-1.51</url>
      <bibkey>kubal-nagvenkar-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.wnut-1.51</doi>
    </paper>
    <paper id="52">
      <title>Sesame Street to Mount Sinai: <fixed-case>BERT</fixed-case>-constrained character-level <fixed-case>M</fixed-case>oses models for multilingual lexical normalization</title>
      <author><first>Yves</first><last>Scherrer</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <pages>465–472</pages>
      <abstract>This paper describes the HEL-LJU submissions to the MultiLexNorm shared task on multilingual lexical normalization. Our system is based on a BERT token classification preprocessing step, where for each token the type of the necessary transformation is predicted (none, uppercase, lowercase, capitalize, modify), and a character-level SMT step where the text is translated from original to normalized given the BERT-predicted transformation constraints. For some languages, depending on the results on development data, the training data was extended by back-translating OpenSubtitles data. In the final ordering of the ten participating teams, the HEL-LJU team has taken the second place, scoring better than the previous state-of-the-art.</abstract>
      <url hash="255f5f11">2021.wnut-1.52</url>
      <bibkey>scherrer-ljubesic-2021-sesame</bibkey>
      <doi>10.18653/v1/2021.wnut-1.52</doi>
    </paper>
    <paper id="53">
      <title>Sequence-to-Sequence Lexical Normalization with Multilingual Transformers</title>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Adrian</first><last>Cosma</last></author>
      <author><first>Liviu P.</first><last>Dinu</last></author>
      <pages>473–482</pages>
      <abstract>Current benchmark tasks for natural language processing contain text that is qualitatively different from the text used in informal day to day digital communication. This discrepancy has led to severe performance degradation of state-of-the-art NLP models when fine-tuned on real-world data. One way to resolve this issue is through lexical normalization, which is the process of transforming non-standard text, usually from social media, into a more standardized form. In this work, we propose a sentence-level sequence-to-sequence model based on mBART, which frames the problem as a machine translation problem. As the noisy text is a pervasive problem across languages, not just English, we leverage the multi-lingual pre-training of mBART to fine-tune it to our data. While current approaches mainly operate at the word or subword level, we argue that this approach is straightforward from a technical standpoint and builds upon existing pre-trained transformer networks. Our results show that while word-level, intrinsic, performance evaluation is behind other methods, our model improves performance on extrinsic, downstream tasks through normalization compared to models operating on raw, unprocessed, social media text.</abstract>
      <url hash="1fbf78c4">2021.wnut-1.53</url>
      <bibkey>bucur-etal-2021-sequence</bibkey>
      <doi>10.18653/v1/2021.wnut-1.53</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="54">
      <title><fixed-case>ÚFAL</fixed-case> at <fixed-case>M</fixed-case>ulti<fixed-case>L</fixed-case>ex<fixed-case>N</fixed-case>orm 2021: Improving Multilingual Lexical Normalization by Fine-tuning <fixed-case>B</fixed-case>y<fixed-case>T</fixed-case>5</title>
      <author><first>David</first><last>Samuel</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <pages>483–492</pages>
      <abstract>We present the winning entry to the Multilingual Lexical Normalization (MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which evaluates lexical-normalization systems on 12 social media datasets in 11 languages. We base our solution on a pre-trained byte-level language model, ByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then fine-tune on authentic normalization data. Our system achieves the best performance by a wide margin in intrinsic evaluation, and also the best performance in extrinsic evaluation through dependency parsing. The source code is released at https://github.com/ufal/multilexnorm2021 and the fine-tuned models at https://huggingface.co/ufal.</abstract>
      <url hash="ba0bc551">2021.wnut-1.54</url>
      <bibkey>samuel-straka-2021-ufal</bibkey>
      <doi>10.18653/v1/2021.wnut-1.54</doi>
      <pwccode url="https://github.com/ufal/multilexnorm2021" additional="false">ufal/multilexnorm2021</pwccode>
    </paper>
    <paper id="55">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>L</fixed-case>ex<fixed-case>N</fixed-case>orm: A Shared Task on Multilingual Lexical Normalization</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Alan</first><last>Ramponi</last></author>
      <author><first>Arkaitz</first><last>Zubiaga</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Benjamin</first><last>Muller</last></author>
      <author><first>Iñaki</first><last>San Vicente Roncal</last></author>
      <author><first>Nikola</first><last>Ljubešić</last></author>
      <author><first>Özlem</first><last>Çetinoğlu</last></author>
      <author><first>Rahmad</first><last>Mahendra</last></author>
      <author><first>Talha</first><last>Çolakoğlu</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Tommaso</first><last>Caselli</last></author>
      <author><first>Wladimir</first><last>Sidorenko</last></author>
      <pages>493–509</pages>
      <abstract>Lexical normalization is the task of transforming an utterance into its standardized form. This task is beneficial for downstream analysis, as it provides a way to harmonize (often spontaneous) linguistic variation. Such variation is typical for social media on which information is shared in a multitude of ways, including diverse languages and code-switching. Since the seminal work of Han and Baldwin (2011) a decade ago, lexical normalization has attracted attention in English and multiple other languages. However, there exists a lack of a common benchmark for comparison of systems across languages with a homogeneous data and evaluation setup. The MultiLexNorm shared task sets out to fill this gap. We provide the largest publicly available multilingual lexical normalization benchmark including 13 language variants. We propose a homogenized evaluation setup with both intrinsic and extrinsic evaluation. As extrinsic evaluation, we use dependency parsing and part-of-speech tagging with adapted evaluation metrics (a-LAS, a-UAS, and a-POS) to account for alignment discrepancies. The shared task hosted at W-NUT 2021 attracted 9 participants and 18 submissions. The results show that neural normalization systems outperform the previous state-of-the-art system by a large margin. Downstream parsing and part-of-speech tagging performance is positively affected but to varying degrees, with improvements of up to 1.72 a-LAS, 0.85 a-UAS, and 1.54 a-POS for the winning system.</abstract>
      <url hash="ab90c282">2021.wnut-1.55</url>
      <bibkey>van-der-goot-etal-2021-multilexnorm</bibkey>
      <doi>10.18653/v1/2021.wnut-1.55</doi>
      <pwccode url="https://bitbucket.org/robvanderg/multilexnorm" additional="false">robvanderg/multilexnorm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tweebank">Tweebank</pwcdataset>
    </paper>
    <paper id="56">
      <title><fixed-case>CL</fixed-case>-<fixed-case>M</fixed-case>o<fixed-case>N</fixed-case>oise: Cross-lingual Lexical Normalization</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>510–514</pages>
      <abstract>Social media is notoriously difficult to process for existing natural language processing tools, because of spelling errors, non-standard words, shortenings, non-standard capitalization and punctuation. One method to circumvent these issues is to normalize input data before processing. Most previous work has focused on only one language, which is mostly English. In this paper, we are the first to propose a model for cross-lingual normalization, with which we participate in the WNUT 2021 shared task. To this end, we use MoNoise as a starting point, and make a simple adaptation for cross-lingual application. Our proposed model outperforms the leave-as-is baseline provided by the organizers which copies the input. Furthermore, we explore a completely different model which converts the task to a sequence labeling task. Performance of this second system is low, as it does not take capitalization into account in our implementation.</abstract>
      <url hash="0f7c3298">2021.wnut-1.56</url>
      <bibkey>van-der-goot-2021-cl</bibkey>
      <doi>10.18653/v1/2021.wnut-1.56</doi>
    </paper>
  </volume>
</collection>
