<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.intexsempar">
  <volume id="1" ingest-date="2020-11-06">
    <meta>
      <booktitle>Proceedings of the First Workshop on Interactive and Executable Semantic Parsing</booktitle>
      <editor><first>Ben</first><last>Bogin</last></editor>
      <editor><first>Srinivasan</first><last>Iyer</last></editor>
      <editor><first>Victoria</first><last>Lin</last></editor>
      <editor><first>Dragomir</first><last>Radev</last></editor>
      <editor><first>Alane</first><last>Suhr</last></editor>
      <editor><first/><last>Panupong</last></editor>
      <editor><first>Caiming</first><last>Xiong</last></editor>
      <editor><first>Pengcheng</first><last>Yin</last></editor>
      <editor><first>Tao</first><last>Yu</last></editor>
      <editor><first>Rui</first><last>Zhang</last></editor>
      <editor><first>Victor</first><last>Zhong</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2020</year>
    </meta>
    <frontmatter>
      <url hash="677e9d09">2020.intexsempar-1.0</url>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>QA</fixed-case>2<fixed-case>E</fixed-case>xplanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph</title>
      <author><first>Saeedeh</first><last>Shekarpour</last></author>
      <author><first>Abhishek</first><last>Nadgeri</last></author>
      <author><first>Kuldeep</first><last>Singh</last></author>
      <pages>1–11</pages>
      <abstract>In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide acceptance due to their “black box” nature, which hinders transparency, fairness, and accountability of QA systems. Therefore, users are unable to understand how and why particular questions have been answered, whereas some others fail. To address this challenge, in this paper, we develop an automatic approach for generating explanations during various stages of a pipeline-based QA system. Our approach is a supervised and automatic approach which considers three classes (i.e., success, no answer, and wrong answer) for annotating the output of involved QA components. Upon our prediction, a template explanation is chosen and integrated into the output of the corresponding component. To measure the effectiveness of the approach, we conducted a user survey as to how non-expert users perceive our generated explanations. The results of our study show a significant increase in the four dimensions of the human factor from the Human-computer interaction community.</abstract>
      <url hash="1b670206">2020.intexsempar-1.1</url>
      <doi>10.18653/v1/2020.intexsempar-1.1</doi>
    </paper>
    <paper id="2">
      <title>Uncertainty and Traffic-Aware Active Learning for Semantic Parsing</title>
      <author><first>Priyanka</first><last>Sen</last></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>12–17</pages>
      <abstract>Collecting training data for semantic parsing is a time-consuming and expensive task. As a result, there is growing interest in industry to reduce the number of annotations required to train a semantic parser, both to cut down on costs and to limit customer data handled by annotators. In this paper, we propose uncertainty and traffic-aware active learning, a novel active learning method that uses model confidence and utterance frequencies from customer traffic to select utterances for annotation. We show that our method significantly outperforms baselines on an internal customer dataset and the Facebook Task Oriented Parsing (TOP) dataset. On our internal dataset, our method achieves the same accuracy as random sampling with 2,000 fewer annotations.</abstract>
      <url hash="94923eba">2020.intexsempar-1.2</url>
      <doi>10.18653/v1/2020.intexsempar-1.2</doi>
    </paper>
    <paper id="3">
      <title>Improving Sequence-to-Sequence Semantic Parser for Task Oriented Dialog</title>
      <author><first>Chaoting</first><last>Xuan</last></author>
      <pages>18–22</pages>
      <abstract>Task Oriented Parsing (TOP) attempts to map utterances to compositional requests, including multiple intents and their slots. Previous work focus on a tree-based hierarchical meaning representation, and applying constituency parsing techniques to address TOP. In this paper, we propose a new format of meaning representation that is more compact and amenable to sequence-to-sequence (seq-to-seq) models. A simple copy-augmented seq-to-seq parser is built and evaluated over a public TOP dataset, resulting in 3.44% improvement over prior best seq-to-seq parser (exact match accuracy), which is also comparable to constituency parsers’ performance.</abstract>
      <url hash="190ad7b5">2020.intexsempar-1.3</url>
      <doi>10.18653/v1/2020.intexsempar-1.3</doi>
    </paper>
    <paper id="4">
      <title>Learning Adaptive Language Interfaces through Decomposition</title>
      <author><first>Siddharth</first><last>Karamcheti</last></author>
      <author><first>Dorsa</first><last>Sadigh</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <pages>23–33</pages>
      <abstract>Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions through decomposition: users interactively teach the system by breaking down high-level utterances describing novel behavior into low-level steps that it can understand. Unfortunately, existing methods either rely on grammars which parse sentences with limited flexibility, or neural sequence-to-sequence models that do not learn efficiently or reliably from individual examples. Our approach bridges this gap, demonstrating the flexibility of modern neural systems, as well as the one-shot reliable generalization of grammar-based methods. Our crowdsourced interactive experiments suggest that over time, users complete complex tasks more efficiently while using our system by leveraging what they just taught. At the same time, getting users to trust the system enough to be incentivized to teach high-level utterances is still an ongoing challenge. We end with a discussion of some of the obstacles we need to overcome to fully realize the potential of the interactive paradigm.</abstract>
      <url hash="72e5b6eb">2020.intexsempar-1.4</url>
      <doi>10.18653/v1/2020.intexsempar-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>C</fixed-case>ollo<fixed-case>QL</fixed-case>: Robust Text-to-<fixed-case>SQL</fixed-case> Over Search Queries</title>
      <author><first>Karthik</first><last>Radhakrishnan</last></author>
      <author><first>Arvind</first><last>Srikantan</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <pages>34–45</pages>
      <abstract>Translating natural language utterances to executable queries is a helpful technique in making the vast amount of data stored in relational databases accessible to a wider range of non-tech-savvy end users. Prior work in this area has largely focused on textual input that is linguistically correct and semantically unambiguous. However, real-world user queries are often succinct, colloquial, and noisy, resembling the input of a search engine. In this work, we introduce data augmentation techniques and a sampling-based content-aware BERT model (ColloQL) to achieve robust text-to-SQL modeling over natural language search (NLS) questions. Due to the lack of evaluation data, we curate a new dataset of NLS questions and demonstrate the efficacy of our approach. ColloQL’s superior performance extends to well-formed text, achieving an 84.9% (logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to the best of our knowledge, the highest performing model that does not use execution guided decoding.</abstract>
      <url hash="a7835a2c">2020.intexsempar-1.5</url>
      <doi>10.18653/v1/2020.intexsempar-1.5</doi>
    </paper>
    <paper id="6">
      <title>Natural Language Response Generation from <fixed-case>SQL</fixed-case> with Generalization and Back-translation</title>
      <author><first>Saptarashmi</first><last>Bandyopadhyay</last></author>
      <author><first>Tianyang</first><last>Zhao</last></author>
      <pages>46–49</pages>
      <abstract>Generation of natural language responses to the queries of structured language like SQL is very challenging as it requires generalization to new domains and the ability to answer ambiguous queries among other issues. We have participated in the CoSQL shared task organized in the IntEx-SemPar workshop at EMNLP 2020. We have trained a number of Neural Machine Translation (NMT) models to efficiently generate the natural language responses from SQL. Our shuffled back-translation model has led to a BLEU score of 7.47 on the unknown test dataset. In this paper, we will discuss our methodologies to approach the problem and future directions to improve the quality of the generated natural language responses.</abstract>
      <url hash="20075c43">2020.intexsempar-1.6</url>
      <doi>10.18653/v1/2020.intexsempar-1.6</doi>
    </paper>
  </volume>
</collection>
