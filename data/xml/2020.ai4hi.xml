<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.ai4hi">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 1st International Workshop on Artificial Intelligence for Historical Image Enrichment and Access</booktitle>
      <editor><first>Yalemisew</first><last>Abgaz</last></editor>
      <editor><first>Amelie</first><last>Dorn</last></editor>
      <editor><first>Jose Luis Preza</first><last>Diaz</last></editor>
      <editor><first>Gerda</first><last>Koch</last></editor>
      <publisher>European Language Resources Association (ELRA)</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-63-4</isbn>
    </meta>
    <frontmatter>
      <url hash="16f73dc5">2020.ai4hi-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Enriching Historic Photography with Structured Data using Image Region Segmentation</title>
      <author><first>Taylor</first><last>Arnold</last></author>
      <author><first>Lauren</first><last>Tilton</last></author>
      <pages>1–10</pages>
      <abstract>Cultural institutions such as galleries, libraries, archives and museums continue to make commitments to large scale digitization of collections. An ongoing challenge is how to increase discovery and access through structured data and the semantic web. In this paper we describe a method for using computer vision algorithms that automatically detect regions of “stuff” — such as the sky, water, and roads — to produce rich and accurate structured data triples for describing the content of historic photography. We apply our method to a collection of 1610 documentary photographs produced in the 1930s and 1940 by the FSA-OWI division of the U.S. federal government. Manual verification of the extracted annotations yields an accuracy rate of 97.5%, compared to 70.7% for relations extracted from object detection and 31.5% for automatically generated captions. Our method also produces a rich set of features, providing more unique labels (1170) than either the captions (1040) or object detection (178) methods. We conclude by describing directions for a linguistically-focused ontology of region categories that can better enrich historical image data. Open source code and the extracted metadata from our corpus are made available as external resources.</abstract>
      <url hash="69630606">2020.ai4hi-1.1</url>
      <language>eng</language>
    </paper>
    <paper id="2">
      <title>Interlinking Iconclass Data with Concepts of Art &amp; Architecture Thesaurus</title>
      <author><first>Anna</first><last>Breit</last></author>
      <pages>11–15</pages>
      <abstract>Iconclass, being a a well established classification system, could benefit from interconnections with other ontologies in order to semantically enrich its content. This work presents a disambiguating and interlinking approach which is used to map Iconclass Subjects to concepts of the Art and Architecture Thesaurus. In a preliminary evaluation, the system is able to produce promising predictions, though the task is highly challenging due to conceptual and schema heterogeneity. Several algorithmic improvements for this specific interlinking task, as well as and future research directions are suggestions. The produced mappings, as well as the source code and additional information can be found at https://github.com/annabreit/taxonomy-interlinking.</abstract>
      <url hash="cf1a0e8a">2020.ai4hi-1.2</url>
      <language>eng</language>
    </paper>
    <paper id="3">
      <title>Toward the Automatic Retrieval and Annotation of Outsider Art images: A Preliminary Statement</title>
      <author><first>John</first><last>Roberto</last></author>
      <author><first>Diego</first><last>Ortego</last></author>
      <author><first>Brian</first><last>Davis</last></author>
      <pages>16–22</pages>
      <abstract>The aim of this position paper is to establish an initial approach to the automatic classification of digital images about the Outsider Art style of painting. Specifically, we explore whether is it possible to classify non-traditional artistic styles by using the same features that are used for classifying traditional styles? Our research question is motivated by two facts. First, art historians state that non-traditional styles are influenced by factors “outside” of the world of art. Second, some studies have shown that several artistic styles confound certain classification techniques. Following current approaches to style prediction, this paper utilises Deep Learning methods to encode image features. Our preliminary experiments have provided motivation to think that, as is the case with traditional styles, Outsider Art can be computationally modelled with objective means by using training datasets and CNN models. Nevertheless, our results are not conclusive due to the lack of a large available dataset on Outsider Art. Therefore, at the end of the paper, we have mapped future lines of action, which include the compilation of a large dataset of Outsider Art images and the creation of an ontology of Outsider Art.</abstract>
      <url hash="df26060c">2020.ai4hi-1.3</url>
      <language>eng</language>
    </paper>
    <paper id="4">
      <title>Automatic Matching of Paintings and Descriptions in Art-Historic Archives using Multimodal Analysis</title>
      <author><first>Christian</first><last>Bartz</last></author>
      <author><first>Nitisha</first><last>Jain</last></author>
      <author><first>Ralf</first><last>Krestel</last></author>
      <pages>23–28</pages>
      <abstract>Cultural heritage data plays a pivotal role in the understanding of human history and culture. A wealth of information is buried in art-historic archives which can be extracted via digitization and analysis. This information can facilitate search and browsing, help art historians to track the provenance of artworks and enable wider semantic text exploration for digital cultural resources. However, this information is contained in images of artworks, as well as textual descriptions or annotations accompanied with the images. During the digitization of such resources, the valuable associations between the images and texts are frequently lost. In this project description, we propose an approach to retrieve the associations between images and texts for artworks from art-historic archives. To this end, we use machine learning to generate text descriptions for the extracted images on the one hand, and to detect descriptive phrases and titles of images from the text on the other hand. Finally, we use embeddings to align both, the descriptions and the images.</abstract>
      <url hash="69c62853">2020.ai4hi-1.4</url>
      <language>eng</language>
    </paper>
    <paper id="5">
      <title>Towards a Comprehensive Assessment of the Quality and Richness of the Europeana Metadata of food-related Images</title>
      <author><first>Yalemisew</first><last>Abgaz</last></author>
      <author><first>Amelie</first><last>Dorn</last></author>
      <author><first>Jose Luis</first><last>Preza Diaz</last></author>
      <author><first>Gerda</first><last>Koch</last></author>
      <pages>29–33</pages>
      <abstract>Semantic enrichment of historical images to build interactive AI systems for the Digital Humanities domain has recently gained significant attention. However, before implementing any semantic enrichment tool for building AI systems, it is also crucial to analyse the quality and richness of the existing datasets and understand the areas where semantic enrichment is most required. Here, we propose an approach to conducting a preliminary analysis of selected historical images from the Europeana platform using existing linked data quality assessment tools. The analysis targets food images by collecting metadata provided from curators such as Galleries, Libraries, Archives and Museums (GLAMs) and cultural aggregators such as Europeana. We identified metrics to evaluate the quality of the metadata associated with food-related images which are harvested from the Europeana platform. In this paper, we present the food-image dataset, the associated metadata and our proposed method for the assessment. The results of our assessment will be used to guide the current effort to semantically enrich the images and build high-quality metadata using Computer Vision.</abstract>
      <url hash="fd01dfe5">2020.ai4hi-1.5</url>
      <language>eng</language>
    </paper>
  </volume>
</collection>
