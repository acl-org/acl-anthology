<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.nlrse">
  <volume id="1" ingest-date="2024-07-25" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)</booktitle>
      <editor><first>Bhavana</first><last>Dalvi Mishra</last></editor>
      <editor><first>Greg</first><last>Durrett</last></editor>
      <editor><first>Peter</first><last>Jansen</last></editor>
      <editor><first>Ben</first><last>Lipkin</last></editor>
      <editor><first>Danilo</first><last>Neves Ribeiro</last></editor>
      <editor><first>Lionel</first><last>Wong</last></editor>
      <editor><first>Xi</first><last>Ye</last></editor>
      <editor><first>Wenting</first><last>Zhao</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand</address>
      <month>August</month>
      <year>2024</year>
      <url hash="a777fb7f">2024.nlrse-1</url>
      <venue>nlrse</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="2a382cfa">2024.nlrse-1.0</url>
      <bibkey>nlrse-2024-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>G</fixed-case>raph<fixed-case>R</fixed-case>eason: Enhancing Reasoning Capabilities of Large Language Models through A Graph-Based Verification Approach</title>
      <author><first>Lang</first><last>Cao</last></author>
      <pages>1-12</pages>
      <abstract>Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph Verifier (GraphReason) to analyze and verify the solutions generated by LLMs. By evaluating these graphs, models can yield more accurate and reliable results.Our experimental results show that our graph-based verification method not only significantly enhances the reasoning abilities of LLMs but also outperforms existing verifier methods in terms of improving these models’ reasoning performance.</abstract>
      <url hash="55794b9c">2024.nlrse-1.1</url>
      <bibkey>cao-2024-graphreason</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>PROC</fixed-case>2<fixed-case>PDDL</fixed-case>: Open-Domain Planning Representations from Texts</title>
      <author><first>Tianyi</first><last>Zhang</last></author>
      <author id="li-zhang-upenn"><first>Li</first><last>Zhang</last></author>
      <author><first>Zhaoyi</first><last>Hou</last></author>
      <author><first>Ziyu</first><last>Wang</last></author>
      <author><first>Yuling</first><last>Gu</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>Allen Institute for Artificial Intelligence and University of Pennsylvania</affiliation></author>
      <author><first>Niket</first><last>Tandon</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>13-24</pages>
      <abstract>Planning in a text-based environment continues to be a significant challenge for AI systems. Recent approaches have utilized language models to predict planning domain definitions (e.g., PDDL) but have only been evaluated in closed-domain simulated environments. To address this, we present Proc2PDDL, the first dataset containing open-domain procedural texts paired with expert-annotated PDDL representations. Using this dataset, we evaluate the task of predicting domain actions (parameters, preconditions, and effects). We experiment with various large language models (LLMs) and prompting mechanisms, including a novel instruction inspired by the zone of proximal development (ZPD), which reconstructs the task as incremental basic skills. Our results demonstrate that Proc2PDDL is highly challenging for end-to-end LLMs, with GPT-3.5’s success rate close to 0% and GPT-4o’s 38%. With ZPD instructions, GPT-4o’s success rate increases to 45%, outperforming regular chain-of-thought prompting’s 34%. Our analysis systematically examines both syntactic and semantic errors, providing insights into the strengths and weaknesses of language models in generating domain-specific programs.</abstract>
      <url hash="a0238243">2024.nlrse-1.2</url>
      <bibkey>zhang-etal-2024-proc2pddl</bibkey>
    </paper>
    <paper id="3">
      <title>Towards A Unified View of Answer Calibration for Multi-Step Reasoning</title>
      <author><first>Shumin</first><last>Deng</last></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Nay</first><last>Oo</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Bryan</first><last>Hooi</last><affiliation>National University of Singapore</affiliation></author>
      <pages>25-38</pages>
      <abstract>Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: *path generation* to generate the reasoning path(s); and *answer calibration* post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.</abstract>
      <url hash="31fe4da4">2024.nlrse-1.3</url>
      <bibkey>deng-etal-2024-towards</bibkey>
    </paper>
    <paper id="4">
      <title>Applying <fixed-case>RLAIF</fixed-case> for Code Generation with <fixed-case>API</fixed-case>-usage in Lightweight <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sujan</first><last>Dutta</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author><first>Sayantan</first><last>Mahinder</last><affiliation>Apple</affiliation></author>
      <author><first>Raviteja</first><last>Anantha</last><affiliation>Apple</affiliation></author>
      <author><first>Bortik</first><last>Bandyopadhyay</last><affiliation>Apple</affiliation></author>
      <pages>39-45</pages>
      <abstract>Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant potential across various domains, including mitigating harm in LLM outputs, enhancing text summarization, and mathematical reasoning. This paper introduces an RLAIF framework for improving the code generation abilities of lightweight (&lt;1B parameters) LLMs. We specifically focus on code generation tasks that require writing appropriate API calls, which is challenging due to the well-known issue of hallucination in LLMs. Our framework extracts AI feedback from a larger LLM (e.g., GPT-3.5) through a specialized prompting strategy and uses this data to train a reward model towards better alignment from smaller LLMs. We run our experiments on the Gorilla dataset and meticulously assess the quality of the model-generated code across various metrics, including AST, ROUGE, and Code-BLEU, and develop a pipeline to compute its executability rate accurately. Our approach significantly enhances the fine-tuned LLM baseline’s performance, achieving a 4.5% improvement in executability rate. Notably, a smaller LLM model (780M parameters) trained with RLAIF surpasses a much larger fine-tuned baseline with 7B parameters, achieving a 1.0% higher code executability rate.</abstract>
      <url hash="de68de79">2024.nlrse-1.4</url>
      <bibkey>dutta-etal-2024-applying</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>S</fixed-case>umm<fixed-case>EQ</fixed-case>u<fixed-case>AL</fixed-case>: Summarization Evaluation via Question Answering using Large Language Models</title>
      <author><first>Junyuan</first><last>Liu</last></author>
      <author><first>Zhengyan</first><last>Shi</last></author>
      <author><first>Aldo</first><last>Lipani</last><affiliation>University College London, University of London</affiliation></author>
      <pages>46-55</pages>
      <abstract>Summarization is hard to evaluate due to its diverse and abstract nature. Although N-gram-based metrics like BLEU and ROUGE are prevalent, they often do not align well with human evaluations. While model-based alternatives such as BERTScore improve, they typically require extensive labelled data. The advent of Large Language Models (LLMs) presents a promising avenue for evaluation. To this end, we introduce SummEQuAL, a novel content-based framework using LLMs for unified, reproducible summarization evaluation. SummEQuAL evaluates summaries by comparing their content with the source document, employing a question-answering approach to gauge both recall and precision. To validate SummEQuAL’s effectiveness, we develop a dataset based on MultiWOZ. We conduct experiments on SummEval and our MultiWOZ-based dataset, showing that SummEQuAL largely improves the quality of summarization evaluation. Notably, SummEQuAL demonstrates a 19.7% improvement over QuestEval in terms of sample-level Pearson correlation with human assessments of consistency on the SummEval dataset. Furthermore, it exceeds the performance of the BERTScore baseline by achieving a 17.3% increase in Spearman correlation on our MultiWOZ-based dataset. Our study illuminates the potential of LLMs for a unified evaluation framework, setting a new paradigm for future summarization evaluation.</abstract>
      <url hash="e3379147">2024.nlrse-1.5</url>
      <bibkey>liu-etal-2024-summequal</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>LOGIC</fixed-case>-<fixed-case>LM</fixed-case>++: Multi-Step Refinement for Symbolic Formulations</title>
      <author><first>Shashank</first><last>Kirtania</last><affiliation>Microsoft</affiliation></author>
      <author><first>Priyanshu</first><last>Gupta</last><affiliation>Microsoft</affiliation></author>
      <author><first>Arjun</first><last>Radhakrishna</last><affiliation>Microsoft</affiliation></author>
      <pages>56-63</pages>
      <abstract>In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. While current approaches leverage formal languages as intermediate representation for these reasoning problems, they still struggle with generating intermediate for-mal specifications with great correctness and in refining these representations. To address these issues, this paper proposes Logic-LM++, an improvement on Logic-LM (Pan et al., 2023). It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM. The paper demonstrates that Logic-LM++ outperforms Logic-LM and LLM based techniques on natural language reasoning tasks on two datasets, FOLIO, ProofWriter and AR-LSAT. Logic-LM++ show an average improvement of 18.5% on standard prompting, 12.3% on chain of thought prompting and 5% on Logic-LM.</abstract>
      <url hash="568291d5">2024.nlrse-1.6</url>
      <bibkey>kirtania-etal-2024-logic</bibkey>
    </paper>
    <paper id="7">
      <title>From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting</title>
      <author><first>Nuo</first><last>Chen</last></author>
      <author><first>Hongguang</first><last>Li</last><affiliation>xiaobing.ai</affiliation></author>
      <author><first>Baoyuan</first><last>Wang</last><affiliation>Xiaobing.ai</affiliation></author>
      <author><first>Jia</first><last>Li</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>64-79</pages>
      <abstract>This paper investigates the performance of Large Language Models (LLMs) and Tool-augmented LLMs in tackling complex mathematical reasoning tasks. We introduce IMR-TIP: Improving Math Reasoning with Tool-augmented Interleaf Prompting, a framework that combines the strengths of both LLMs and Tool-augmented LLMs. IMR-TIP follows the “From Good to Great” concept, collecting multiple potential solutions from both LLMs and their Tool-Augmented counterparts for the same math problem, and then selecting or re-generating the most accurate answer after cross-checking these solutions via tool-augmented interleaf prompting. The framework incorporates two key aspects: self-prompt and tool-augmented interleaf prompting (TIP). The former allows LLMs to autonomously refine and improve an initial prompt related to tool usage, while the latter enables LLMs to derive the final answer by dynamically analyzing the problem, cross-checking potential solutions, and revising previous reasoning hints in an interleaved manner. Experimental analysis shows that IMR-TIP achieves enhanced mathematical capabilities and outperforms traditional LLMs and tool-augmented LLMs in accuracy and reasoning diversity on math reasoning tasks. For instance, IMR-TIP can improve Tool-augmented ChatGPT on GSM8K-Hard from 56.0% to 65.2 %.</abstract>
      <url hash="9d795281">2024.nlrse-1.7</url>
      <bibkey>chen-etal-2024-good</bibkey>
    </paper>
  </volume>
</collection>
