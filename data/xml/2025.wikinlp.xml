<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.wikinlp">
  <volume id="1" ingest-date="2025-07-22" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2nd Workshop on Advancing Natural Language Processing for Wikipedia (WikiNLP 2025)</booktitle>
      <editor><first>Akhil</first><last>Arora</last></editor>
      <editor><first>Isaac</first><last>Johnson</last></editor>
      <editor><first>Lucie-Aimée</first><last>Kaffee</last></editor>
      <editor><first>Tzu-Sheng</first><last>Kuo</last></editor>
      <editor><first>Tiziano</first><last>Piccardi</last></editor>
      <editor><first>Indira</first><last>Sen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="10c39182">2025.wikinlp-1</url>
      <venue>wikinlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-284-8</isbn>
      <doi>10.18653/v1/2025.wikinlp-1</doi>
    </meta>
    <frontmatter>
      <url hash="08e8498a">2025.wikinlp-1.0</url>
      <bibkey>wikinlp-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.wikinlp-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Wikivecs: A Fully Reproducible Vectorization of Multilingual <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Brandon</first><last>Duderstadt</last><affiliation>Nomic AI</affiliation></author>
      <pages>1-9</pages>
      <abstract>Dense vector representations have become foundational to modern natural language processing (NLP), powering diverse workflows from semantic search and retrieval augmented generation to content comparison across languages. Although Wikipedia is one of the most comprehensive and widely used datasets in modern NLP research, it lacks a fully reproducible and permissively licensed dense vectorization.In this paper, we present Wikivecs, a fully reproducible, permissively licensed dataset containing dense vector embeddings for every article in Multilingual Wikipedia. Our pipeline leverages a fully reproducible and permissively licensed multilingual text encoder to embed Wikipedia articles into a unified vector space, making it easy to compare and analyze content across languages.Alongside these vectors, we release a two-dimensional data map derived from the vectors, enabling visualization and exploration of Multilingual Wikipedia’s content landscape.We demonstrate the utility of our dataset by identifying several content gaps between English and Russian Wikipedia.</abstract>
      <url hash="3ab7dc7b">2025.wikinlp-1.1</url>
      <bibkey>duderstadt-2025-wikivecs</bibkey>
      <doi>10.18653/v1/2025.wikinlp-1.1</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>WETB</fixed-case>ench: A Benchmark for Detecting Task-Specific Machine-Generated Text on <fixed-case>W</fixed-case>ikipedia</title>
      <author id="gerrit-quaremba" orcid="0009-0007-8410-2846"><first>Gerrit</first><last>Quaremba</last></author>
      <author><first>Elizabeth</first><last>Black</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Denny</first><last>Vrandecic</last></author>
      <author><first>Elena</first><last>Simperl</last><affiliation>King’s College London</affiliation></author>
      <pages>10-30</pages>
      <abstract>Given Wikipedia’s role as a trusted source of high-quality, reliable content, there are growing concerns about the proliferation of low-quality machine-generated text (MGT) produced by large language models (LLMs) on its platform. Reliable detection of MGT is therefore essential, yet existing work primarily evaluates MGT detectors on generic generation tasks, rather than on tasks more commonly performed by Wikipedia editors. This misalignment can lead to poor generalisability when applied to real-world Wikipedia contexts.We introduce WETBench, a multilingual, multi-generator, and task-specific benchmark for MGT detection. We define three editing tasks empirically grounded in Wikipedia editors’ perceived use cases for LLM-assisted editing: Paragraph Writing, Summarisation, and Text Style Transfer, which we implement using two new datasets across three languages. For each writing task, we evaluate three prompts, produce MGT across multiple generators using the best-performing prompt, and benchmark diverse detectors.We find that, across settings, training-based detectors achieve an average accuracy of 78%, while zero-shot detectors average 58%. These results demonstrate that detectors struggle with MGT in realistic generation scenarios and underscore the importance of evaluating such models on diverse, task-specific data to assess their reliability in editor-driven contexts.</abstract>
      <url hash="12554b92">2025.wikinlp-1.6</url>
      <bibkey>quaremba-etal-2025-wetbench</bibkey>
      <doi>10.18653/v1/2025.wikinlp-1.6</doi>
    </paper>
    <paper id="8">
      <title>Proper Noun Diacritization for <fixed-case>A</fixed-case>rabic <fixed-case>W</fixed-case>ikipedia: A Benchmark Dataset</title>
      <author><first>Rawan</first><last>Bondok</last><affiliation>New York University, Abu Dhabi</affiliation></author>
      <author><first>Mayar</first><last>Nassar</last><affiliation>Ain Shams University</affiliation></author>
      <author id="salam-khalifa" orcid="0000-0003-0049-3637"><first>Salam</first><last>Khalifa</last><affiliation>New York University and State University of New York, Stony Brook</affiliation></author>
      <author id="kurt-micallef" orcid="0000-0002-6778-5733"><first>Kurt</first><last>Micallef</last><affiliation>University of Malta</affiliation></author>
      <author id="nizar-habash" orcid="0000-0002-1831-3457"><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>31-44</pages>
      <abstract>Proper nouns in Arabic Wikipedia are frequently undiacritized, creating ambiguity in pronunciation and interpretation, especially for transliterated named entities of foreign origin. While transliteration and diacritization have been well-studied separately in Arabic NLP, their intersection remains underexplored. In this paper, we introduce a new manually diacritized dataset of Arabic proper nouns of various origins with their English Wikipedia equivalent glosses, and present the challenges and guidelines we followed to create it. We benchmark GPT-4o on the task of recovering full diacritization given the undiacritized Arabic and English forms, and analyze its performance. Achieving 73% accuracy, our results underscore both the difficulty of the task and the need for improved models and resources. We release our dataset to facilitate further research on Arabic Wikipedia proper noun diacritization.</abstract>
      <url hash="c2d8a26f">2025.wikinlp-1.8</url>
      <bibkey>bondok-etal-2025-proper</bibkey>
      <doi>10.18653/v1/2025.wikinlp-1.8</doi>
    </paper>
  </volume>
</collection>
