<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.knowllm">
  <volume id="1" ingest-date="2025-07-24" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 3rd Workshop on Towards Knowledgeable Foundation Models (KnowFM)</booktitle>
      <editor><first>Yuji</first><last>Zhang</last></editor>
      <editor><first>Canyu</first><last>Chen</last></editor>
      <editor><first>Sha</first><last>Li</last></editor>
      <editor><first>Mor</first><last>Geva</last></editor>
      <editor><first>Chi</first><last>Han</last></editor>
      <editor><first>Xiaozhi</first><last>Wang</last></editor>
      <editor><first>Shangbin</first><last>Feng</last></editor>
      <editor><first>Silin</first><last>Gao</last></editor>
      <editor><first>Isabelle</first><last>Augenstein</last></editor>
      <editor><first>Mohit</first><last>Bansal</last></editor>
      <editor><first>Manling</first><last>Li</last></editor>
      <editor><first>Heng</first><last>Ji</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="72fb90fc">2025.knowllm-1</url>
      <venue>knowllm</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-283-1</isbn>
      <doi>10.18653/v1/2025.knowllm-1</doi>
    </meta>
    <frontmatter>
      <url hash="d56a2f05">2025.knowllm-1.0</url>
      <bibkey>knowllm-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Temporal Information Retrieval via Time-Specifier Model Merging</title>
      <author><first>SeungYoon</first><last>Han</last></author>
      <author><first>Taeho</first><last>Hwang</last><affiliation>Korea Telecom Research</affiliation></author>
      <author><first>Sukmin</first><last>Cho</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Soyeong</first><last>Jeong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hoyun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Huije</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jong C.</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>1-13</pages>
      <abstract>The rapid expansion of digital information and knowledge across structured and unstructured sources has heightened the importance of Information Retrieval (IR). While dense retrieval methods have substantially improved semantic matching for general queries, they consistently underperform on queries with explicit temporal constraints–often those containing numerical expressions and time specifiers such as “in 2015.” Existing approaches to Temporal Information Retrieval (TIR) improve temporal reasoning but often suffer from catastrophic forgetting, leading to reduced performance on non-temporal queries. To address this, we propose Time-Specifier Model Merging (TSM), a novel method that enhances temporal retrieval while preserving accuracy on non-temporal queries. TSM trains specialized retrievers for individual time specifiers and merges them into a unified model, enabling precise handling of temporal constraints without compromising non-temporal retrieval. Extensive experiments on both temporal and non-temporal datasets demonstrate that TSM significantly improves performance on temporally constrained queries while maintaining strong results on non-temporal queries, consistently outperforming other training methods. Our code is available at https://github.com/seungyoonee/TSM.</abstract>
      <url hash="fb89c9d4">2025.knowllm-1.1</url>
      <bibkey>han-etal-2025-temporal</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>E</fixed-case>d<fixed-case>T</fixed-case>ec-<fixed-case>I</fixed-case>tem<fixed-case>G</fixed-case>en: Enhancing Retrieval-Augmented Item Generation Through Key Point Extraction</title>
      <author><first>Alonso</first><last>Palomino</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>David</first><last>Buschhüter</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Roland</first><last>Roller</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Niels</first><last>Pinkwart</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Benjamin</first><last>Paassen</last></author>
      <pages>14-25</pages>
      <abstract>A major bottleneck in exam construction involves designing test items (i.e., questions) that accurately reflect key content from domain-aligned curricular materials. For instance, during formative assessments in vocational education and training (VET), exam designers must generate updated test items that assess student learning progress while covering the full breadth of topics in the curriculum. Large language models (LLMs) can partially support this process, but effective use requires careful prompting and task-specific understanding. We propose a new key point extraction method for retrieval-augmented item generation that enhances the process of generating test items with LLMs. We exhaustively evaluated our method using a TREC-RAG approach, finding that prompting LLMs with key content rather than directly using full curricular text passages significantly improves item quality regarding key information coverage by 8%. To demonstrate these findings, we release EdTec-ItemGen, a retrieval-augmented item generation demo tool to support item generation in education.</abstract>
      <url hash="9841e24d">2025.knowllm-1.2</url>
      <bibkey>palomino-etal-2025-edtec</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.2</doi>
    </paper>
    <paper id="3">
      <title>Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals</title>
      <author><first>Lida</first><last>Chen</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zujie</first><last>Liang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xintao</first><last>Wang</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Feng</first><last>Wei</last></author>
      <author><first>Jinglei</first><last>Chen</last><affiliation>ANT GROUP</affiliation></author>
      <author><first>Zhenghong</first><last>Hao</last></author>
      <author><first>Bing</first><last>Han</last><affiliation>mybank, antgroup</affiliation></author>
      <author><first>Wei</first><last>Wang</last></author>
      <pages>26-39</pages>
      <abstract>Large language models (LLMs) have achieved great success, but their occasional content fabrication, or hallucination, limits their practical application. Hallucination arises because LLMs struggle to admit ignorance due to inadequate training on knowledge boundaries. We call it a limitation of LLMs that they can not accurately express their knowledge boundary, answering questions they know while admitting ignorance to questions they do not know. In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know. We propose CoKE, which first probes LLMs’ knowledge boundary via internal confidence given a set of questions, and then leverages the probing results to elicit the expression of the knowledge boundary. Extensive experiments show CoKE helps LLMs express knowledge boundaries, answering known questions while declining unknown ones, significantly improving in-domain and out-of-domain performance.</abstract>
      <url hash="9b8c28f6">2025.knowllm-1.3</url>
      <bibkey>chen-etal-2025-teaching</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.3</doi>
    </paper>
    <paper id="4">
      <title>Knowledge-Grounded Detection of Cryptocurrency Scams with Retrieval-Augmented <fixed-case>LM</fixed-case>s</title>
      <author><first>Zichao</first><last>Li</last><affiliation>University of Waterloo</affiliation></author>
      <pages>40-48</pages>
      <abstract>This paper presents a knowledge-grounded framework for cryptocurrency scam detection using retrieval-augmented language models. We address three key limitations of existing approaches: static knowledge bases, unreliable LM outputs, and fixed classification thresholds. Our method combines (1) temporally-weighted retrieval from scam databases, (2) confidence-aware fusion of parametric and external knowledge, and (3) adaptive threshold optimization via gradient ascent. Experiments on CryptoScams and Twitter Financial Scams datasets demonstrate state-of-the-art performance, with 22% higher recall at equivalent precision compared to fixed thresholds, 4.3× lower hallucination rates than pure LMs, and 89% temporal performance retention on emerging scam types. The system achieves real-time operation (45ms/query) while maintaining interpretability through evidence grounding. Ablation studies confirm each component’s necessity, with confidence fusion proving most critical (12.1% performance drop when removed). These advances enable more robust monitoring of evolving cryptocurrency threats while addressing fundamental challenges in knowledgeable foundation models.</abstract>
      <url hash="91315a5f">2025.knowllm-1.4</url>
      <bibkey>li-2025-knowledge</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.4</doi>
    </paper>
    <paper id="5">
      <title>Stress-Testing Multimodal Foundation Models for Crystallographic Reasoning</title>
      <author><first>Can</first><last>Polat</last></author>
      <author><first>Hasan</first><last>Kurban</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author><first>Erchin</first><last>Serpedin</last></author>
      <author><first>Mustafa</first><last>Kurban</last><affiliation>Ankara University</affiliation></author>
      <pages>49-58</pages>
      <abstract>Evaluating foundation models for crystallographic reasoning requires benchmarks that isolate generalization behavior while enforcing physical constraints. This work introduces, xCrysAlloys, a multiscale multicrystal dataset with two physically grounded evaluation protocols to stress-test multimodal generative models. The Spatial-Exclusion benchmark withholds all supercells of a given radius from a diverse dataset, enabling controlled assessments of spatial interpolation and extrapolation. The Compositional-Exclusion benchmark omits all samples of a specific chemical composition, probing generalization across stoichiometries. Nine vision–language foundation models are prompted with crystallographic images and textual context to generate structural annotations. Responses are evaluated via (i) relative errors in lattice parameters and density, (ii) a physics-consistency index penalizing volumetric violations, and (iii) a hallucination score capturing geometric outliers and invalid space-group predictions. These benchmarks establish a reproducible, physically informed framework for assessing generalization, consistency, and reliability in large-scale multimodal models. Dataset and implementation are available at https://github.com/KurbanIntelligenceLab/StressTestingMMFMinCR.</abstract>
      <url hash="ec341cc7">2025.knowllm-1.5</url>
      <bibkey>polat-etal-2025-stress</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>MLAN</fixed-case>: Language-Based Instruction Tuning Preserves and Transfers Knowledge in Multimodal Language Models</title>
      <author><first>Jianhong</first><last>Tu</last></author>
      <author><first>Zhuohao</first><last>Ni</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Nicholas</first><last>Crispino</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <author><first>Zihao</first><last>Yu</last></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <author><first>Beliz</first><last>Gunel</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Ruoxi</first><last>Jia</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Lingjuan</first><last>Lyu</last><affiliation>Sony</affiliation></author>
      <author><first>Dawn</first><last>Song</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Chenguang</first><last>Wang</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <pages>59-74</pages>
      <abstract>We present a novel visual instruction tuning strategy to improve the zero-shot task generalization of multimodal large language models by building a firm text-only knowledge base. Existing work lacks sufficient experimentation on the importance of each modality in the instruction tuning stage, often using a majority of vision-language data while keeping text-only data limited and fixing mixtures of modalities. By incorporating diverse text-only data in the visual instruction tuning stage, we vary vision-language data in various controlled experiments to investigate the importance of modality in visual instruction tuning. Our comprehensive evaluation shows that the text-heavy instruction tuning approach is able to perform on par with traditional vision-heavy mixtures on both modalities across 12 general datasets while using as low as half the total training tokens. We find that simply increasing sufficiently diverse text-only data enables transfer of instruction following ability and domain knowledge across modalities while being more efficient than the vision-language approach.</abstract>
      <url hash="0a8d44ff">2025.knowllm-1.6</url>
      <bibkey>tu-etal-2025-mlan</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>T</fixed-case>ool<fixed-case>R</fixed-case>e<fixed-case>AG</fixed-case>t: Tool Retrieval for <fixed-case>LLM</fixed-case>-based Complex Task Solution via Retrieval Augmented Generation</title>
      <author><first>Norbert</first><last>Braunschweiler</last><affiliation>Toshiba Corporation</affiliation></author>
      <author><first>Rama</first><last>Doddipatla</last><affiliation>Toshiba Europe LTD</affiliation></author>
      <author><first>Tudor-catalin</first><last>Zorila</last><affiliation>Toshiba Cambridge Research Laboratory</affiliation></author>
      <pages>75-83</pages>
      <abstract>Artificial intelligence agents when deployed to solve complex problems, need to first decompose the task into smaller manageable sub-tasks, and further associate tools if one is required to solve the sub-task. If the size of the set of tools to chose from is large, a retrieval system is usually employed to narrow down the tool choices before the LLM can proceed with associating tools to the sub-tasks. This paper focuses on the retrieval problem to identify the set of relevant tools to solve a complex task given a large pool of tools to chose from using retrieval augmented generation (RAG) and we refer to it as ToolReAGT. The proposed approach employs ReAct prompting to perform the retrieval in an iterative fashion to first identify if a tool is required and then associate one or more tools for each sub-task. This deviates from conventional RAG where an n-best list of tools are identified given the complex task directly. Experiments are presented on the UltraTool benchmark corpus with 1000 complex tasks and over 2000 tools to select from. A conventional RAG-system is established as baseline and compared to the ToolReAGt approach, resulting in an 8.9% improved retrieval accuracy score recall@5.</abstract>
      <url hash="79ab1428">2025.knowllm-1.7</url>
      <bibkey>braunschweiler-etal-2025-toolreagt</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.7</doi>
    </paper>
    <paper id="8">
      <title>Can <fixed-case>LLM</fixed-case>s Recognize Their Own Analogical Hallucinations? Evaluating Uncertainty Estimation for Analogical Reasoning</title>
      <author><first>Zheng</first><last>Chen</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Zhaoxin</first><last>Feng</last></author>
      <author><first>Jianfei</first><last>Ma</last></author>
      <author><first>Jiexi</first><last>Xu</last></author>
      <author id="bo-li"><first>Bo</first><last>Li</last></author>
      <pages>84-93</pages>
      <abstract>Large language models (LLMs) often demonstrate strong performance by leveraging implicit knowledge acquired during pretraining. Analogical reasoning, which solves new problems by referencing similar known examples, offers a structured way to utilize this knowledge, but can also lead to subtle factual errors and hallucinations. In this work, we investigate whether LLMs can recognize the reliability of their own analogical outputs using black-box uncertainty estimation (UE). We evaluate six UE metrics across two reasoning-intensive tasks: mathematical problem solving (GSM8K) and code generation (Codeforces). Our results show that Kernel Language Entropy (KLE) and Lexical Similarity (LexSim) are the most robust indicators of correctness. Moreover, while analogical prompting increases model confidence over direct prompting, most uncertainty arises during the analogy transfer step. These findings highlight the limitations of analogical knowledge transfer in LLMs and demonstrate the potential of UE methods for detecting hallucinated reasoning in black-box settings.</abstract>
      <url hash="3ce1cd1e">2025.knowllm-1.8</url>
      <bibkey>chen-etal-2025-llms-recognize</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.8</doi>
    </paper>
    <paper id="9">
      <title>Meetalk: Retrieval-Augmented and Adaptively Personalized Meeting Summarization with Knowledge Learning from User Corrections</title>
      <author><first>Zheng</first><last>Chen</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiang</first><last>Futian</last></author>
      <author id="yue-deng"><first>Yue</first><last>Deng</last></author>
      <author><first>Changyang</first><last>He</last></author>
      <author id="bo-li"><first>Bo</first><last>Li</last></author>
      <pages>94-110</pages>
      <abstract>We present Meetalk, a retrieval-augmented and knowledge-adaptive system for generating personalized meeting minutes. Although large language models (LLMs) excel at summarizing, their output often lacks faithfulness and does not reflect user-specific structure and style. Meetalk addresses these issues by integrating ASR-based transcription with LLM generation guided by user-derived knowledge. Specifically, Meetalk maintains and updates three structured databases, Table of Contents, Chapter Allocation, and Writing Style, based on user-uploaded samples and editing feedback. These serve as a dynamic memory that is retrieved during generation to ground the model’s outputs. To further enhance reliability, Meetalk introduces hallucination-aware uncertainty markers that highlight low-confidence segments for user review. In a user study in five real-world meeting scenarios, Meetalk significantly outperforms a strong baseline (iFLYTEK ASR + ChatGPT-4o) in completeness, contextual relevance, and user trust. Our findings underscore the importance of knowledge foundation and feedback-driven adaptation in building trustworthy, personalized LLM systems for high-stakes summarization tasks.</abstract>
      <url hash="fdf22c95">2025.knowllm-1.9</url>
      <bibkey>chen-etal-2025-meetalk</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.9</doi>
    </paper>
    <paper id="10">
      <title>Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models</title>
      <author><first>Samir</first><last>Abdaljalil</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Hasan</first><last>Kurban</last><affiliation>Hamad Bin Khalifa University</affiliation></author>
      <author><first>Khalid</first><last>Qaraqe</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Erchin</first><last>Serpedin</last></author>
      <pages>111-119</pages>
      <abstract>Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.</abstract>
      <url hash="351e88e7">2025.knowllm-1.10</url>
      <bibkey>abdaljalil-etal-2025-theorem</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.10</doi>
    </paper>
    <paper id="11">
      <title>Reasoning or Memorization? Investigating <fixed-case>LLM</fixed-case>s’ Capability in Restoring <fixed-case>C</fixed-case>hinese <fixed-case>I</fixed-case>nternet Homophones</title>
      <author><first>Jianfei</first><last>Ma</last></author>
      <author><first>Zhaoxin</first><last>Feng</last></author>
      <author><first>Huacheng</first><last>Song</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zheng</first><last>Chen</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>120-139</pages>
      <abstract>Chinese homophones, prevalent in Internet culture, bring rich linguistic twists that are challenging for language models. While native speakers disambiguate them through phonological reasoning and contextual understanding, it remains untested how well LLMs perform on this task and whether LLMs also achieve this via similar reasoning processes or merely through memorization of homophone-original word pairs during training.In this paper, we present HomoP-CN, the first Chinese Internet homophones dataset with systematic perturbations for evaluating LLMs’ homophone restoration capabilities. Using this benchmark, we investigated the influence of semantic, phonological, and graphemic features on LLMs’ restoration accuracy, measured the reliance levels of each model on memorization during restoration through consistency ratios under controlled perturbations, and assessed the effectiveness of various prompting strategies, including contextual cues, pinyin augmentation, few-shot learning, and thought-chain approaches.</abstract>
      <url hash="b9c7f15e">2025.knowllm-1.11</url>
      <bibkey>ma-etal-2025-reasoning</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.11</doi>
      <revision id="1" href="2025.knowllm-1.11v1" hash="924da789"/>
      <revision id="2" href="2025.knowllm-1.11v2" hash="b9c7f15e" date="2025-09-02">Correct author list.</revision>
    </paper>
    <paper id="12">
      <title>Superfluous Instruction: Vulnerabilities Stemming from Task-Specific Superficial Expressions in Instruction Templates</title>
      <author><first>Toma</first><last>Suzuki</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Justin</first><last>Vasselli</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>140-152</pages>
      <abstract>Large language models (LLMs) achieve high performance through instruction-tuning, which involves learning various tasks using instruction templates. However, these templates often contain task-specific expressions, which are words that frequently appear in certain contexts but do not always convey the actual meaning of that context, even if they seem closely related to the target task. Biases inherent in such instruction templates may be learned by LLMs during training, potentially degrading performance when the models encounter superficial expressions. In this study, we propose a method that incorporates additional instructions to FLAN templates, without altering the base instruction to produce “superfluous instructions”. This allows us to investigate the vulnerabilities of LLMs caused by overfitting to task-specific expressions embedded in instruction templates. The experimental results revealed that the inclusion of superficial words strongly related to each task in the instruction text can alter the output, regardless of the intended meaning.</abstract>
      <url hash="4d8d4825">2025.knowllm-1.12</url>
      <bibkey>suzuki-etal-2025-superfluous</bibkey>
      <doi>10.18653/v1/2025.knowllm-1.12</doi>
    </paper>
  </volume>
</collection>
