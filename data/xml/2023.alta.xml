<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.alta">
  <volume id="1" ingest-date="2024-03-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 21st Annual Workshop of the Australasian Language Technology Association</booktitle>
      <editor><first>Smaranda</first><last>Muresan</last></editor>
      <editor><first>Vivian</first><last>Chen</last></editor>
      <editor><first>Kennington</first><last>Casey</last></editor>
      <editor><first>Vandyke</first><last>David</last></editor>
      <editor><first>Dethlefs</first><last>Nina</last></editor>
      <editor><first>Inoue</first><last>Koji</last></editor>
      <editor><first>Ekstedt</first><last>Erik</last></editor>
      <editor><first>Ultes</first><last>Stefan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Melbourne, Australia</address>
      <month>November</month>
      <year>2023</year>
      <url hash="62df9d1c">2023.alta-1</url>
      <venue>alta</venue>
    </meta>
    <frontmatter>
      <url hash="62df9d1c">2023.alta-1.0</url>
      <bibkey>alta-2023-australasian</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>B</fixed-case>angla<fixed-case>C</fixed-case>lick<fixed-case>BERT</fixed-case>: <fixed-case>B</fixed-case>angla Clickbait Detection from News Headlines using Domain Adaptive <fixed-case>B</fixed-case>angla<fixed-case>BERT</fixed-case> and <fixed-case>MLP</fixed-case> Techniques</title>
      <author><first>Saman Sarker</first><last>Joy</last></author>
      <author><first>Tanusree Das</first><last>Aishi</last></author>
      <author><first>Naima Tahsin</first><last>Nodi</last></author>
      <author><first>Annajiat Alim</first><last>Rasel</last></author>
      <pages>1-10</pages>
      <abstract>News headlines or titles that deliberately persuade readers to view a particular online content are referred to as clickbait. There have been numerous studies focused on clickbait detection in English language, compared to that, there have been very few researches carried out that address clickbait detection in Bangla news headlines. In this study, we have experimented with several distinctive transformers models, namely BanglaBERT and XLM-RoBERTa. Additionally, we introduced a domain-adaptive pretrained model, BanglaClickBERT. We conducted a series of experiments to identify the most effective model. The dataset we used for this study contained 15,056 labeled and 65,406 unlabeled news headlines; in addition to that, we have collected more unlabeled Bangla news headlines by scraping clickbait-dense websites making a total of 1 million unlabeled news headlines in order to make our BanglaClickBERT. Our approach has successfully surpassed the performance of existing state-of-the-art technologies providing a more accurate and efficient solution for detecting clickbait in Bangla news headlines, with potential implications for improving online content quality and user experience.</abstract>
      <url hash="d6356aab">2023.alta-1.1</url>
      <bibkey>joy-etal-2023-banglaclickbert</bibkey>
    </paper>
    <paper id="2">
      <title>Story Co-telling Dialogue Generation based on Multi-Agent Reinforcement Learning and Story Highlights</title>
      <author><first>Yu-Kai</first><last>Lee</last></author>
      <author><first>Chia-Hui</first><last>Chang</last></author>
      <pages>11-19</pages>
      <abstract>Retelling a story is one way to develop narrative skills in students, but it may present some challenges for English as Second Language (ESL) students who are learning new stories and vocabularies at the same time. The goal of this research is to develop a dialogue module for story co-telling for ESL students in order to help students to co-narrate an English story and enhance their narrative skills. However, story co-telling is a relatively underexplored and novel task. In order to understand the story content and select the right plot to continue the story co-telling based on the current dialogue, we utilize open domain information extraction techniques to construct a knowledge graph, and adopt multi-agent reinforcement learning methods to train two agents to select relevant facts from the knowledge graph and generate responses, jointly accomplishing the task of story co-telling. Compared to models that reply on chronological order, our model improves the performance from 67.0% to 70.8% through self-training with reward evaluation, achieving an increase of approximately 3.8%.</abstract>
      <url hash="8779ba8d">2023.alta-1.2</url>
      <bibkey>lee-chang-2023-story</bibkey>
    </paper>
    <paper id="3">
      <title>Using <fixed-case>C</fixed-case>-<fixed-case>LARA</fixed-case> to evaluate <fixed-case>GPT</fixed-case>-4’s multilingual processing</title>
      <author><first>ChatGPT</first><last>C-LARA-Instance</last></author>
      <author><first>Belinda</first><last>Chiera</last></author>
      <author><first>Cathy</first><last>Chua</last></author>
      <author><first>Chadi</first><last>Raheb</last></author>
      <author><first>Manny</first><last>Rayner</last></author>
      <author><first>Annika</first><last>Simonsen</last></author>
      <author><first>Zhengkang</first><last>Xiang</last></author>
      <author><first>Rina</first><last>Zviel-Girshin</last></author>
      <pages>20-29</pages>
      <abstract>We present a cross-linguistic study in which the open source C-LARA platform was used to evaluate GPT-4’s ability to perform several key tasks relevant to Computer Assisted Language Learning. For each of the languages English, Farsi, Faroese, Mandarin and Russian, we instructed GPT-4, through C-LARA, to write six different texts, using prompts chosen to obtain texts of widely differing character. We then further instructed GPT-4 to annotate each text with segmentation markup, glosses and lemma/part-of-speech information; native speakers hand-corrected the texts and annotations to obtain error rates on the different component tasks. The C-LARA platform makes it easy to combine the results into a single multimodal document, further facilitating checking of their correctness. GPT-4’s performance varied widely across languages and processing tasks, but performance on different text genres was roughly comparable. In some cases, most notably glossing of English text, we found that GPT-4 was consistently able to revise its annotations to improve them.</abstract>
      <url hash="07b090b6">2023.alta-1.3</url>
      <bibkey>c-lara-instance-etal-2023-using</bibkey>
    </paper>
    <paper id="4">
      <title>Exploring Causal Directions through Word Occurrences: Semi-supervised <fixed-case>B</fixed-case>ayesian Classification Framework</title>
      <author><first>King Tao Jason</first><last>Ng</last></author>
      <author><first>Diego</first><last>Molla</last></author>
      <pages>30-39</pages>
      <abstract>Determining causal directions in sentences plays a critical role into understanding a cause-and-effect relationship between entities. In this paper, we show empirically that word occurrences from several Internet domains resemble the characteristics of causal directions. Our research contributes to the knowledge of the underlying data generation process behind causal directions. We propose a two-phase method: 1. Bayesian framework, which generates synthetic data from posteriors by incorporating word occurrences from the Internet domains. 2. Pre-trained BERT, which utilises semantics of words based on the context to perform classification. The proposed method achieves an improvement in performance for the Cause-Effect relations of the SemEval-2010 dataset, when compared with random guessing.</abstract>
      <url hash="5b775845">2023.alta-1.4</url>
      <bibkey>ng-molla-2023-exploring</bibkey>
    </paper>
    <paper id="5">
      <title>The sub-band cepstrum as a tool for local spectral analysis in forensic voice comparison</title>
      <author><first>Shunichi</first><last>Ishihara</last></author>
      <author><first>Frantz</first><last>Clermont</last></author>
      <pages>40-50</pages>
      <abstract>This paper exploits band-limited cepstral coefficients (BLCCs) in forensic voice comparison (FVC), with the primary aim of locating speaker-sensitive spectral regions. BLCCs are sub-band cepstral coefficients (CCs) which are easily obtained by a linear transformation of full-band CCs. The transformation gives the flexibility of selecting any sub-band region without the recurrent cost of spectral analyses. Using multi-band BLCCs obtained by sliding a 600-Hz sub-band every 400 Hz across the full [0-5kHz] range, FVC experiments were attempted using citation recordings of the 5 Japanese vowels from 297 adult-male, native speakers. The FVC results give locations and ranges for the most speaker-sensitive sub-bands, and show that combining 3-4 of these yields comparable FVC performance with full-band CCs. Owing to their ability to easily extract locally-encoded speaker information from full-band CCs, it can be conjectured that BLCCs have a significant role to play in the search for meaningful interpretations of the numerical outcome of forensic analyses.</abstract>
      <url hash="c7770261">2023.alta-1.5</url>
      <bibkey>ishihara-clermont-2023-sub</bibkey>
    </paper>
    <paper id="6">
      <title>Right the docs: Characterising voice dataset documentation practices used in machine learning</title>
      <author><first>Kathy</first><last>Reid</last></author>
      <author><first>Elizabeth T.</first><last>Williams</last></author>
      <pages>51-66</pages>
      <abstract>Voice-enabled technologies such as virtual assistants are quickly becoming ubiquitous. Their functionality relies on machine learning (ML) models that perform tasks such as automatic speech recognition (ASR). These models, in general, currently perform less accurately for some cohorts of speakers, across axes such as age, gender and accent; they are biased. ML models are trained from large datasets. ML Practitioners (MLPs) are interested in addressing bias across the ML lifecycle, and they often use dataset documentation here to understand dataset characteristics. However, there is a lack of research centred on voice dataset documentation. Our work makes an empirical contribution to this gap, identifying shortcomings in voice dataset documents (VDD), and arguing for actions to improve them. First, we undertake 13 interviews with MLPs who work with voice data, exploring how they use VDDs. We focus here on MLP roles and trade-offs made when working with VDDs. Drawing from the literature and from interview data, we create a rubric through which to analyse VDDs for nine voice datasets. Triangulating the two methods in our findings, we show that VDDs are inadequate for the needs of MLPs on several fronts. VDDs currently codify voice data characteristics in fragmented ways that make it difficult to compare and combine datasets, presenting a barrier to MLPs’ bias reduction efforts. We then seek to address these shortcomings and “right the docs” by proposing improvement actions aligned to our findings.</abstract>
      <url hash="7694c5b9">2023.alta-1.6</url>
      <bibkey>reid-williams-2023-right</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>MCASP</fixed-case>: Multi-Modal Cross Attention Network for Stock Market Prediction</title>
      <author><first>Kamaladdin</first><last>Fataliyev</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <pages>67-77</pages>
      <abstract>Stock market prediction is considered a complex task due to the non-stationary and volatile nature of the stock markets. With the increasing amount of online data, various information sources have been analyzed to understand the underlying patterns of the price movements. However, most existing works in the literature mostly focus on either the intra-modality information within each input data type, or the inter-modal relationships among the input modalities. Different from these, in this research, we propose a novel Multi-Modal Cross Attention Network for Stock Market Prediction (MCASP) by capturing both modality-specific features and the joint influence of each modality in a unified framework. We utilize financial news, historical market data and technical indicators to predict the movement direction of the market prices. After processing the input modalities with three separate deep networks, we first construct a self-attention network that utilizes multiple Transformer models to capture the intra-modal information. Then we design a novel cross-attention network that processes the inputs in pairs to exploit the cross-modal and joint information of the modalities. Experiments with real world datasets for S&amp;P500 index forecast and the prediction of five individual stocks, demonstrate the effectiveness of the proposed multi-modal design over several state-of-the-art baseline models.</abstract>
      <url hash="c2d5576b">2023.alta-1.7</url>
      <bibkey>fataliyev-liu-2023-mcasp</bibkey>
    </paper>
    <paper id="8">
      <title>Catching Misdiagnosed Limb Fractures in the Emergency Department Using Cross-institution Transfer Learning</title>
      <author><first>Filip</first><last>Rusak</last></author>
      <author><first>Bevan</first><last>Koopman</last></author>
      <author><first>Nathan J.</first><last>Brown</last></author>
      <author><first>Kevin</first><last>Chu</last></author>
      <author><first>Jinghui</first><last>Liu</last></author>
      <author><first>Anthony</first><last>Nguyen</last></author>
      <pages>78-87</pages>
      <abstract>We investigated the development of a Machine Learning (ML)-based classifier to identify abnormalities in radiology reports from Emergency Departments (EDs) that can help automate the radiology report reconciliation process. Often, radiology reports become available to the ED only after the patient has been treated and discharged, following ED clinician interpretation of the X-ray. However, occasionally ED clinicians misdiagnose or fail to detect subtle abnormalities on X-rays, so they conduct a manual radiology report reconciliation process as a safety net. Previous studies addressed this problem of automated reconciliation using ML-based classification solutions that require data samples from the target institution that is heavily based on feature engineering, implying lower transferability between hospitals. In this paper, we investigated the benefits of using pre-trained BERT models for abnormality classification in a cross-institutional setting where data for fine-tuning was unavailable from the target institution. We also examined how the inclusion of synthetically generated radiology reports from ChatGPT affected the performance of the BERT models. Our findings suggest that BERT-like models outperform previously proposed ML-based methods in cross-institutional scenarios, and that adding ChatGPT-generated labelled radiology reports can improve the classifier’s performance by reducing the number of misdiagnosed discharged patients.</abstract>
      <url hash="765bf7b7">2023.alta-1.8</url>
      <bibkey>rusak-etal-2023-catching</bibkey>
    </paper>
    <paper id="9">
      <title>Turning Flowchart into Dialog: Augmenting Flowchart-grounded Troubleshooting Dialogs via Synthetic Data Generation</title>
      <author><first>Haolan</first><last>Zhan</last></author>
      <author><first>Sameen</first><last>Maruf</last></author>
      <author><first>Lizhen</first><last>Qu</last></author>
      <author><first>Yufei</first><last>Wang</last></author>
      <author><first>Ingrid</first><last>Zukerman</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>88-99</pages>
      <abstract>Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the instructions of a flowchart to diagnose users’ problems in specific domains (e.g., vehicle, laptop), have been gaining research interest in recent years. However, collecting sufficient dialogues that are naturally grounded on flowcharts is costly, thus FTD systems are impeded by scarce training data. To mitigate the data sparsity issue, we propose a plan-based synthetic data generation (PlanSDG) approach that generates diverse synthetic dialog data at scale by transforming concise flowchart into dialogues. Specifically, its generative model employs a variational-base framework with a hierarchical planning strategy that includes global and local latent planning variables. Experiments on the FloDial dataset show that synthetic dialogue produced by PlanSDG improves the performance of downstream tasks, including flowchart path retrieval and response generation, in particular on the Out-of-Flowchart settings. In addition, further analysis demonstrate the quality of synthetic data generated by PlanSDG in paths that are covered by current sample dialogues and paths that are not covered.</abstract>
      <url hash="2f37d5fb">2023.alta-1.9</url>
      <bibkey>zhan-etal-2023-turning</bibkey>
    </paper>
    <paper id="10">
      <title>Encoding Prefixation in <fixed-case>S</fixed-case>outhern <fixed-case>M</fixed-case>in</title>
      <author><first>Yishan</first><last>Huang</last></author>
      <pages>100-106</pages>
      <abstract>This study adopts an inter-disciplinary approach to explore how the prefixation is encoded and contributes to the word formation in Zhangzhou Southern Min, an under-described Sinitic dialect spoken in the South Fujian of mainland China. It addresses five specific aspects, comprising semantic function, morpho-syntactic characteristics, prosodic effect, pragmatic significance, along with their occurrence constraints. The exploration directly fills in the research gap in the study of Zhangzhou, and substantially advance our knowledge of the encoding of prefixation in Southern Chinese dialects. It contributes vital linguistic data to the typology of prefixation as an important phenomenon in world’s natural languages, while enlightening the theoretical discussion on how Sinitic languages should be better defined from the morpho-syntactic perspective.</abstract>
      <url hash="80015e4e">2023.alta-1.10</url>
      <bibkey>huang-2023-encoding-prefixation</bibkey>
    </paper>
    <paper id="11">
      <title>An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text</title>
      <author><first>Vijini</first><last>Liyanage</last></author>
      <author><first>Davide</first><last>Buscaldi</last></author>
      <pages>107-111</pages>
      <abstract>Thanks to the state-of-the-art Large Language Models (LLMs), language generation has reached outstanding levels. These models are capable of generating high quality content, thus making it a challenging task to detect generated text from human-written content. Despite the advantages provided by Natural Language Generation, the inability to distinguish automatically generated text can raise ethical concerns in terms of authenticity. Consequently, it is important to design and develop methodologies to detect artificial content. In our work, we present some classification models constructed by ensembling transformer models such as Sci-BERT, DeBERTa and XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate that the considered ensemble architectures surpass the performance of the individual transformer models for classification. Furthermore, the proposed SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared task 2023 data.</abstract>
      <url hash="463e6bce">2023.alta-1.11</url>
      <bibkey>liyanage-buscaldi-2023-ensemble</bibkey>
    </paper>
    <paper id="12">
      <title>Chat Disentanglement: Data for New Domains and Methods for More Accurate Annotation</title>
      <author><first>Sai R.</first><last>Gouravajhala</last></author>
      <author><first>Andrew M.</first><last>Vernier</last></author>
      <author><first>Yiming</first><last>Shi</last></author>
      <author><first>Zihan</first><last>Li</last></author>
      <author><first>Mark S.</first><last>Ackerman</last></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last></author>
      <pages>112-117</pages>
      <abstract>Conversation disentanglement is the task of taking a log of intertwined conversations from a shared channel and breaking the log into individual conversations. The standard datasets for disentanglement are in a single domain and were annotated by linguistics experts with careful training for the task. In this paper, we introduce the first multi-domain dataset and a study of annotation by people without linguistics expertise or extensive training. We experiment with several variations in interfaces, conducting user studies with domain experts and crowd workers. We also test a hypothesis from prior work that link-based annotation is more accurate, finding that it actually has comparable accuracy to set-based annotation. Our new dataset will support the development of more useful systems for this task, and our experimental findings suggest that users are capable of improving the usefulness of these systems by accurately annotating their own data.</abstract>
      <url hash="cbf39639">2023.alta-1.12</url>
      <bibkey>gouravajhala-etal-2023-chat</bibkey>
    </paper>
    <paper id="13">
      <title>Enhancing Bacterial Infection Prediction in Critically Ill Patients by Integrating Clinical Text</title>
      <author><first>Jinghui</first><last>Liu</last></author>
      <author><first>Anthony</first><last>Nguyen</last></author>
      <pages>118-124</pages>
      <abstract>Bacterial infection (BI) is an important clinical condition and is related to many diseases that are difficult to treat. Early prediction of BI can lead to better treatment and appropriate use of antimicrobial medications. In this paper, we study a variety of NLP models to predict BI for critically ill patients and compare them with a strong baseline based on clinical measurements. We find that choosing the proper text-based model to combine with measurements can lead to substantial improvements. Our results show the value of clinical text in predicting and managing BI. We also find that the NLP model developed using patients with BI can be transferred to the more general patient cohort for patient risk prediction.</abstract>
      <url hash="b2e978b0">2023.alta-1.13</url>
      <bibkey>liu-nguyen-2023-enhancing</bibkey>
    </paper>
    <paper id="14">
      <title>Predicting Empathic Accuracy from User-Designer Interviews</title>
      <author><first>Steven Nguyen</first><last>Fataliyev</last></author>
      <author><first>Daniel</first><last>Beck</last></author>
      <author><first>Katja</first><last>Holtta-Otto</last></author>
      <pages>125-129</pages>
      <abstract>Measuring empathy as a natural language processing task has often been limited to a subjective measure of how well individuals respond to each other in emotive situations. Cognitive empathy, or an individual’s ability to accurately assess another individual’s thoughts, remains a more novel task. In this paper, we explore natural language processing techniques to measure cognitive empathy using paired sentence data from design interviews. Our findings show that an unsupervised approach based on similarity of vectors from a Large Language Model is surprisingly promising, while adding supervision does not necessarily improve the performance. An analysis of the results highlights potential reasons for this behaviour and gives directions for future work in this space.</abstract>
      <url hash="6fd81ab7">2023.alta-1.14</url>
      <bibkey>fataliyev-etal-2023-predicting</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>CRF</fixed-case>-based recognition of invasive fungal infection concepts in <fixed-case>CHIFIR</fixed-case> clinical reports</title>
      <author><first>Yang</first><last>Meng</last></author>
      <author><first>Vlada</first><last>Rozova</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <pages>130-135</pages>
      <abstract>Named entity recognition (NER) in clinical documentation is often hindered by the use of highly specialised terminology, variation in language used to express medical findings and general scarcity of high-quality data available for training. This short paper compares a Conditional Random Fields model to the previously established dictionary-based approach and evaluates its ability to extract information from a small corpus of annotated pathology reports. The results suggest that including token descriptors as well as contextual features significantly improves precision on several concept categories while maintaining the same level of recall.</abstract>
      <url hash="e7e97213">2023.alta-1.15</url>
      <bibkey>meng-etal-2023-crf</bibkey>
    </paper>
    <paper id="16">
      <title>The uncivil empathy: Investigating the relation between empathy and toxicity in online mental health support forums</title>
      <author><first>Ming-Bin</first><last>Chen</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>136-147</pages>
      <abstract>We explore the relationship between empathy and toxicity in the context of online mental health forums. Despite the common assumption of a negative correlation between these concepts, it has not been empirically examined. We augment the EPITOME mental health empathy dataset with toxicity labels using two widely employed toxic/harmful content detection APIs: Perspective API and OpenAI moderation API. We find a notable presence of toxic/harmful content (17.77%) within empathetic responses, and only a very weak negative correlation between the two variables. Qualitative analysis revealed contributions labeled as empathetic often contain harmful content such as promotion of suicidal ideas. Our results highlight the need for reevaluating empathy independently from toxicity in future research and encourage a reconsideration of empathy’s role in natural language generation and evaluation.</abstract>
      <url hash="39d931bb">2023.alta-1.16</url>
      <bibkey>chen-etal-2023-uncivil</bibkey>
    </paper>
    <paper id="17">
      <title>Overview of the 2023 <fixed-case>ALTA</fixed-case> Shared Task: Discriminate between Human-Written and Machine-Generated Text</title>
      <author><first>Diego</first><last>Molla</last></author>
      <author><first>Haolan</first><last>Zhan</last></author>
      <author><first>Xuanli</first><last>He</last></author>
      <author><first>Qiongkai</first><last>Xu</last></author>
      <pages>148-152</pages>
      <abstract>The ALTA shared tasks have been running annually since 2010. In 2023, the purpose of the task is to build automatic detection systems that can discriminate between human-written and synthetic text generated by Large Language Models (LLM). In this paper we present the task, the evaluation criteria, and the results of the systems participating in the shared task.</abstract>
      <url hash="f06cd89d">2023.alta-1.17</url>
      <bibkey>molla-etal-2023-overview</bibkey>
    </paper>
    <paper id="18">
      <title>A Prompt in the Right Direction: Prompt Based Classification of Machine-Generated Text Detection</title>
      <author><first>Rinaldo</first><last>Gagiano</last></author>
      <author><first>Lin</first><last>Tian</last></author>
      <pages>153-158</pages>
      <abstract>The goal of ALTA 2023 Shared Task is to distinguish between human-authored text and synthetic text generated by Large Language Models (LLMs). Given the growing societal concerns surrounding LLMs, this task addresses the urgent need for robust text verification strategies. In this paper, we describe our method, a fine-tuned Falcon-7B model with incorporated label smoothing into the training process. We applied model prompting to samples with lower confidence scores to enhance prediction accuracy. Our model achieved a statistically significant accuracy of 0.991.</abstract>
      <url hash="b4e34a92">2023.alta-1.18</url>
      <bibkey>gagiano-tian-2023-prompt</bibkey>
    </paper>
    <paper id="19">
      <title>Automatic Detection of Machine-Generated Text Using Pre-Trained Language Models</title>
      <author><first>Yunhao</first><last>Fang</last></author>
      <pages>159-163</pages>
      <abstract>In this paper, I provide a detailed description of my approach to tackling the ALTA 2023 shared task whose objective is to build an automatic detection system to distinguish between humanauthored text and text generated from Large Language Models. By leveraging several pretrained language models through model finetuning as well as the multi-model ensemble, the system managed to achieve second place on the test set leaderboard in the competition.</abstract>
      <url hash="7c9cc66b">2023.alta-1.19</url>
      <bibkey>fang-2023-automatic</bibkey>
    </paper>
    <paper id="20">
      <title>An Ensemble Based Approach To Detecting <fixed-case>LLM</fixed-case>-Generated Texts</title>
      <author><first>Ahmed</first><last>El-Sayed</last></author>
      <author><first>Omar</first><last>Nasr</last></author>
      <pages>164-168</pages>
      <abstract>Recent advancements in Large Language models (LLMs) have empowered them to achieve text generation capabilities on par with those of humans. These recent advances paired with the wide availability of those models have made Large Language models adaptable in many domains, from scientific writing to story generation along with many others. This recent rise has made it crucial to develop systems to discriminate between human-authored and synthetic text generated by Large Language models (LLMs). Our proposed system for the ALTA shared task, based on ensembling a number of language models, claimed first place on the development set with an accuracy of 99.35% and third place on the test set with an accuracy of 98.35%.</abstract>
      <url hash="ae2f99af">2023.alta-1.20</url>
      <bibkey>el-sayed-nasr-2023-ensemble</bibkey>
    </paper>
    <paper id="21">
      <title>Feature-Level Ensemble Learning for Robust Synthetic Text Detection with <fixed-case>D</fixed-case>e<fixed-case>BERT</fixed-case>a<fixed-case>V</fixed-case>3 and <fixed-case>XLM</fixed-case>-<fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a</title>
      <author><first>Saman Sarker</first><last>Joy</last></author>
      <author><first>Tanusree Das</first><last>Aishi</last></author>
      <pages>169-172</pages>
      <abstract>As large language models, or LLMs, continue to advance in recent years, they require the development of a potent system to detect whether a text was created by a human or an LLM in order to prevent the unethical use of LLMs. To address this challenge, ALTA Shared Task 2023 introduced a task to build an automatic detection system that can discriminate between human-authored and synthetic text generated by LLMs. In this paper, we present our participation in this task where we proposed a feature-level ensemble of two transformer models namely DeBERTaV3 and XLM-RoBERTa to come up with a robust system. The given dataset consisted of textual data with two labels where the task was binary classification. Experimental results show that our proposed method achieved competitive performance among the participants. We believe this solution would make an impact and provide a feasible solution for detection of synthetic text detection.</abstract>
      <url hash="c70ac9ce">2023.alta-1.21</url>
      <bibkey>joy-aishi-2023-feature</bibkey>
    </paper>
    <paper id="22">
      <title>Stacking the Odds: Transformer-Based Ensemble for <fixed-case>AI</fixed-case>-Generated Text Detection</title>
      <author><first>Duke</first><last>Nguyen</last></author>
      <author><first>Khaing Myat Noe</first><last>Naing</last></author>
      <author><first>Aditya</first><last>Joshi</last></author>
      <pages>173-178</pages>
      <abstract>This paper reports our submission under the team name ‘SynthDetectives’ to the ALTA 2023 Shared Task. We use a stacking ensemble of Transformers for the task of AI-generated text detection. Our approach is novel in terms of its choice of models in that we use accessible and lightweight models in the ensemble. We show that ensembling the models results in an improved accuracy in comparison with using them individually. Our approach achieves an accuracy score of 0.9555 on the official test data provided by the shared task organisers.</abstract>
      <url hash="79becaa1">2023.alta-1.22</url>
      <bibkey>nguyen-etal-2023-stacking</bibkey>
    </paper>
    <paper id="23">
      <title>Natural Language Processing for Clinical Text</title>
      <author><first>Vlada</first><last>Rozova</last></author>
      <author><first>Jinghui</first><last>Liu</last></author>
      <author><first>Mike</first><last>Conway</last></author>
      <pages>179-182</pages>
      <abstract>Learning from real-world clinical data has potential to promote the quality of care, improve the efficiency of healthcare systems, and support clinical research. As a large proportion of clinical information is recorded only in unstructured free-text format, applying NLP to process and understand the vast amount of clinical text generated in clinical encounters is essential. However, clinical text is known to be highly ambiguous, it contains complex professional terms requiring clinical expertise to understand and annotate, and it is written in different clinical contexts with distinct purposes. All these factors together make clinical NLP research both rewarding and challenging. In this tutorial, we will discuss the characteristics of clinical text and provide an overview of some of the tools and methods used to process it. We will also present a real-world example to show the effectiveness of different NLP methods in processing and understanding clinical text. Finally, we will discuss the strengths and limitations of large language models and their applications, evaluations, and extensions in clinical NLP.</abstract>
      <url hash="16dcc2da">2023.alta-1.23</url>
      <bibkey>rozova-etal-2023-natural</bibkey>
    </paper>
  </volume>
</collection>
