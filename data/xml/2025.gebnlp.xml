<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.gebnlp">
  <volume id="1" ingest-date="2025-07-21" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 6th Workshop on Gender Bias in Natural Language Processing (GeBNLP)</booktitle>
      <editor id="agnieszka-falenska"><first>Agnieszka</first><last>Faleńska</last></editor>
      <editor><first>Christine</first><last>Basta</last></editor>
      <editor><first>Marta</first><last>Costa-jussà</last></editor>
      <editor><first>Karolina</first><last>Stańczak</last></editor>
      <editor><first>Debora</first><last>Nozza</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="739d3ce4">2025.gebnlp-1</url>
      <venue>gebnlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-277-0</isbn>
      <doi>10.18653/v1/2025.gebnlp-1</doi>
    </meta>
    <frontmatter>
      <url hash="80c03cd2">2025.gebnlp-1.0</url>
      <bibkey>gebnlp-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>JBBQ</fixed-case>: <fixed-case>J</fixed-case>apanese Bias Benchmark for Analyzing Social Biases in Large Language Models</title>
      <author id="hitomi-yanaka" orcid="0000-0003-0354-6116"><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <author><first>Namgi</first><last>Han</last><affiliation>Tokyo University</affiliation></author>
      <author><first>Ryoma</first><last>Kumon</last></author>
      <author><first>Lu</first><last>Jie</last></author>
      <author id="masashi-takeshita" orcid="0000-0001-5853-3262"><first>Masashi</first><last>Takeshita</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Ryo</first><last>Sekizawa</last></author>
      <author><first>Taisei</first><last>Katô</last></author>
      <author><first>Hiromi</first><last>Arai</last><affiliation>RIKEN</affiliation></author>
      <pages>1-17</pages>
      <abstract>With the development of large language models (LLMs), social biases in these LLMs have become a pressing issue.Although there are various benchmarks for social biases across languages, the extent to which Japanese LLMs exhibit social biases has not been fully investigated.In this study, we construct the Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the English bias benchmark BBQ, with analysis of social biases in Japanese LLMs.The results show that while current open Japanese LLMs with more parameters show improved accuracies on JBBQ, their bias scores increase.In addition, prompts with a warning about social biases and chain-of-thought prompting reduce the effect of biases in model outputs, but there is room for improvement in extracting the correct evidence from contexts in Japanese. Our dataset is available at https://github.com/ynklab/JBBQ_data.</abstract>
      <url hash="a6adba33">2025.gebnlp-1.1</url>
      <bibkey>yanaka-etal-2025-jbbq</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Intersectional Bias in <fixed-case>J</fixed-case>apanese Large Language Models from a Contextualized Perspective</title>
      <author id="hitomi-yanaka" orcid="0000-0003-0354-6116"><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <author id="xinqi-he" orcid="0009-0001-0926-944X"><first>Xinqi</first><last>He</last><affiliation>Rikkyo University (St. Paul’s University)</affiliation></author>
      <author><first>Lu</first><last>Jie</last></author>
      <author><first>Namgi</first><last>Han</last><affiliation>Tokyo University</affiliation></author>
      <author><first>Sunjin</first><last>Oh</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Ryoma</first><last>Kumon</last></author>
      <author><first>Yuma</first><last>Matsuoka</last></author>
      <author><first>Kazuhiko</first><last>Watabe</last></author>
      <author id="yuko-itatsu" orcid="0009-0006-5342-3084"><first>Yuko</first><last>Itatsu</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>18-32</pages>
      <abstract>An growing number of studies have examined the social bias of rapidly developed large language models (LLMs). Although most of these studies have focused on bias occurring in a single social attribute, research in social science has shown that social bias often occurs in the form of intersectionality—the constitutive and contextualized perspective on bias aroused by social attributes. In this study, we construct the Japanese benchmark inter-JBBQ, designed to evaluate the intersectional bias in LLMs on the question-answering setting. Using inter-JBBQ to analyze GPT-4o and Swallow, we find that biased output varies according to its contexts even with the equal combination of social attributes.</abstract>
      <url hash="43beed68">2025.gebnlp-1.2</url>
      <bibkey>yanaka-etal-2025-intersectional</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.2</doi>
      <revision id="1" href="2025.gebnlp-1.2v1" hash="2f422fe8"/>
      <revision id="2" href="2025.gebnlp-1.2v2" hash="43beed68" date="2025-07-29">Typos.</revision>
    </paper>
    <paper id="3">
      <title>Detecting Bias and Intersectional Bias in <fixed-case>I</fixed-case>talian Word Embeddings and Language Models</title>
      <author><first>Alexandre</first><last>Puttick</last><affiliation>BFH - Bern University of Applied Sciences</affiliation></author>
      <author id="mascha-kurpicz-briki" orcid="0000-0001-5539-6370"><first>Mascha</first><last>Kurpicz-Briki</last><affiliation>BFH - Bern University of Applied Sciences</affiliation></author>
      <pages>33-51</pages>
      <abstract>Bias in Natural Language Processing (NLP) applications has become a critical issue, with many methods developed to measure and mitigate bias in word embeddings and language models. However, most approaches focus on single categories such as gender or ethnicity, neglecting the intersectionality of biases, particularly in non-English languages. This paper addresses these gaps by studying both single-category and intersectional biases in Italian word embeddings and language models. We extend existing bias metrics to Italian, introducing GG-FISE, a novel method for detecting intersectional bias while accounting for grammatical gender. We also adapt the CrowS-Pairs dataset and bias metric to Italian. Through a series of experiments using WEAT, SEAT, and LPBS tests, we identify significant biases along gender and ethnic lines, with particular attention to biases against Romanian and South Asian populations. Our results highlight the need for culturally adapted methods to detect and address biases in multilingual and intersectional contexts.</abstract>
      <url hash="d662c25a">2025.gebnlp-1.3</url>
      <bibkey>puttick-kurpicz-briki-2025-detecting</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.3</doi>
      <revision id="1" href="2025.gebnlp-1.3v1" hash="7a1f68a4"/>
      <revision id="2" href="2025.gebnlp-1.3v2" hash="d662c25a" date="2025-09-06">Minor updates.</revision>
    </paper>
    <paper id="4">
      <title>Power(ful) Associations: Rethinking “Stereotype” for <fixed-case>NLP</fixed-case></title>
      <author id="hannah-devinney" orcid="0000-0003-0278-9757"><first>Hannah</first><last>Devinney</last><affiliation>Linköping University</affiliation></author>
      <pages>52-58</pages>
      <abstract>The tendency for Natural Language Processing (NLP) technologies to reproduce stereotypical associations, such as associating Black people with criminality or women with care professions, is a site of major concern and, therefore, much study. Stereotyping is a powerful tool of oppression, but the social and linguistic mechanisms behind it are largely ignored in the NLP field. Thus, we fail to effectively challenge stereotypes and the power asymmetries they reinforce. This opinion paper problematizes several common aspects of current work addressing stereotyping in NLP, and offers practicable suggestions for potential forward directions.</abstract>
      <url hash="aba0aa5c">2025.gebnlp-1.4</url>
      <bibkey>devinney-2025-power</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Introducing <fixed-case>MARB</fixed-case> — A Dataset for Studying the Social Dimensions of Reporting Bias in Language Models</title>
      <author><first>Tom</first><last>Södahl Bladsjö</last></author>
      <author id="ricardo-munoz-sanchez" orcid="0000-0002-9902-2925"><first>Ricardo</first><last>Muñoz Sánchez</last></author>
      <pages>59-74</pages>
      <abstract>Reporting bias is the tendency for speakers to omit unnecessary or obvious information while mentioning things they consider relevant or surprising. In descriptions of people, reporting bias can manifest as a tendency to over report on attributes that deviate from the norm. While social bias in language models has garnered a lot of attention in recent years, a majority of the existing work equates “bias” with “stereotypes”. We suggest reporting bias as an alternative lens through which to study how social attitudes manifest in language models. We present the MARB dataset, a diagnostic dataset for studying the interaction between social bias and reporting bias in language models. We use MARB to evaluate the off-the-shelf behavior of both masked and autoregressive language models and find signs of reporting bias with regards to marginalized identities, mirroring that which can be found in human text. This effect is particularly pronounced when taking gender into account, demonstrating the importance of considering intersectionality when studying social phenomena like biases.</abstract>
      <url hash="65eb9dec">2025.gebnlp-1.5</url>
      <bibkey>bladsjo-munoz-sanchez-2025-introducing</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Gender Bias in <fixed-case>N</fixed-case>epali-<fixed-case>E</fixed-case>nglish Machine Translation: A Comparison of <fixed-case>LLM</fixed-case>s and Existing <fixed-case>MT</fixed-case> Systems</title>
      <author><first>Supriya</first><last>Khadka</last></author>
      <author><first>Bijayan</first><last>Bhattarai</last></author>
      <pages>75-82</pages>
      <abstract>Bias in Nepali NLP is rarely addressed, as the language is classified as low-resource, which leads to the perpetuation of biases in downstream systems. Our research focuses on gender bias in Nepali-English machine translation, an area that has seen little exploration. With the emergence of Large Language Models(LLM), there is a unique opportunity to mitigate these biases. In this study, we quantify and evaluate gender bias by constructing an occupation corpus and adapting three gender-bias challenge sets for Nepali. Our findings reveal that gender bias is prevalent in existing translation systems, with translations often reinforcing stereotypes and misrepresenting gender-specific roles. However, LLMs perform significantly better in both gender-neutral and gender-specific contexts, demonstrating less bias compared to traditional machine translation systems. Despite some quirks, LLMs offer a promising alternative for culture-rich, low-resource languages like Nepali. We also explore how LLMs can improve gender accuracy and mitigate biases in occupational terms, providing a more equitable translation experience. Our work contributes to the growing effort to reduce biases in machine translation and highlights the potential of LLMs to address bias in low-resource languages, paving the way for more inclusive and accurate translation systems.</abstract>
      <url hash="d6312810">2025.gebnlp-1.6</url>
      <bibkey>khadka-bhattarai-2025-gender</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Mind the Gap: Gender-based Differences in Occupational Embeddings</title>
      <author id="olga-kononykhina" orcid="0009-0002-9414-2685"><first>Olga</first><last>Kononykhina</last><affiliation>MCML - Munich Center for Machine Learning and Ludwig-Maximilians-Universität München</affiliation></author>
      <author id="anna-carolina-haensch" orcid="0000-0001-6772-0393"><first>Anna-Carolina</first><last>Haensch</last><affiliation>University of Maryland, College Park and Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Frauke</first><last>Kreuter</last><affiliation>University of Maryland</affiliation></author>
      <pages>83-91</pages>
      <abstract>Large Language Models (LLMs) offer promising alternatives to traditional occupational coding approaches in survey research. Using a German dataset, we examine the extent to which LLM-based occupational coding differs by gender. Our findings reveal systematic disparities: gendered job titles (e.g., “Autor” vs. “Autorin”, meaning “male author” vs. “female author”) frequently result in diverging occupation codes, even when semantically identical. Across all models, 54%–82% of gendered inputs obtain different Top-5 suggestions. The practical impact, however, depends on the model. GPT includes the correct code most often (62%) but demonstrates female bias (up to +18 pp). IBM is less accurate (51%) but largely balanced. Alibaba, Gemini, and MiniLM achieve about 50% correct-code inclusion, and their small (&lt; 10 pp) and direction-flipping gaps could indicate a sampling noise rather than gender bias. We discuss these findings in the context of fairness and reproducibility in NLP applications for social data.</abstract>
      <url hash="5946c56c">2025.gebnlp-1.7</url>
      <bibkey>kononykhina-etal-2025-mind</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.7</doi>
      <revision id="1" href="2025.gebnlp-1.7v1" hash="37b146d6"/>
      <revision id="2" href="2025.gebnlp-1.7v2" hash="5946c56c" date="2026-02-15">Correct citation errors.</revision>
    </paper>
    <paper id="9">
      <title>Assessing the Reliability of <fixed-case>LLM</fixed-case>s Annotations in the Context of Demographic Bias and Model Explanation</title>
      <author id="hadi-mohammadi" orcid="0000-0003-0860-9200"><first>Hadi</first><last>Mohammadi</last></author>
      <author><first>Tina</first><last>Shahedi</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author id="pablo-mosteiro" orcid="0000-0001-7231-2773"><first>Pablo</first><last>Mosteiro</last><affiliation>Utrecht University</affiliation></author>
      <author id="massimo-poesio" orcid="0000-0001-8469-2072"><first>Massimo</first><last>Poesio</last><affiliation>Utrecht University and Queen Mary, University of London</affiliation></author>
      <author id="ayoub-bagheri" orcid="0000-0001-6366-2173"><first>Ayoub</first><last>Bagheri</last><affiliation>Utrecht University</affiliation></author>
      <author id="anastasia-giachanou" orcid="0000-0002-7601-8667"><first>Anastasia</first><last>Giachanou</last><affiliation>Utrecht University</affiliation></author>
      <pages>92-104</pages>
      <abstract>Understanding the sources of variability in annotations is crucial for developing fair NLP systems, especially for tasks like sexism detection where demographic bias is a concern. This study investigates the extent to which annotator demographic features influence labeling decisions compared to text content. Using a Generalized Linear Mixed Model, we quantify this influence, finding that while statistically present, demographic factors account for a minor fraction (~8%) of the observed variance, with tweet content being the dominant factor. We then assess the reliability of Generative AI (GenAI) models as annotators, specifically evaluating if guiding them with demographic personas improves alignment with human judgments. Our results indicate that simplistic persona prompting often fails to enhance, and sometimes degrades, performance compared to baseline models. Furthermore, explainable AI (XAI) techniques reveal that model predictions rely heavily on content-specific tokens related to sexism, rather than correlates of demographic characteristics. We argue that focusing on content-driven explanations and robust annotation protocols offers a more reliable path towards fairness than potentially persona simulation.</abstract>
      <url hash="9c4a07c1">2025.gebnlp-1.9</url>
      <bibkey>mohammadi-etal-2025-assessing</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>W</fixed-case>o<fixed-case>NB</fixed-case>ias: A Dataset for Classifying Bias &amp; Prejudice Against Women in <fixed-case>B</fixed-case>engali Text</title>
      <author><first>Md. Raisul Islam</first><last>Aupi</last></author>
      <author><first>Nishat</first><last>Tafannum</last></author>
      <author><first>Md. Shahidur</first><last>Rahman</last><affiliation>Shahjalal University of Science and Technology</affiliation></author>
      <author><first>Kh Mahmudul</first><last>Hassan</last><affiliation>Shahjalal University of Science and Technology</affiliation></author>
      <author><first>Naimur</first><last>Rahman</last></author>
      <pages>105-110</pages>
      <abstract>This paper presents WoNBias, a curated Bengali dataset to identify gender-based biases, stereotypes, and harmful language directed at women. It merges digital sources- social media, blogs, news- with offline tactics comprising surveys and focus groups, alongside some existing corpora to compile a total of 31,484 entries (10,656 negative; 10,170 positive; 10,658 neutral). WoNBias reflects the sociocultural subtleties of bias in both Bengali digital and offline conversations. By bridging online and offline biased contexts, the dataset supports content moderation, policy interventions, and equitable NLP research for Bengali, a low-resource language critically underserved by existing tools. WoNBias aims to combat systemic gender discrimination against women on digital platforms, empowering researchers and practitioners to combat harmful narratives in Bengali-speaking communities.</abstract>
      <url hash="4949bfde">2025.gebnlp-1.10</url>
      <bibkey>aupi-etal-2025-wonbias</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.10</doi>
    </paper>
    <paper id="12">
      <title>Strengths and Limitations of Word-Based Task Explainability in Vision Language Models: a Case Study on Biological Sex Biases in the Medical Domain</title>
      <author id="lorenzo-bertolini" orcid="0000-0002-1709-9372"><first>Lorenzo</first><last>Bertolini</last><affiliation>European Commission Joint Research Centre</affiliation></author>
      <author><first>Valentin</first><last>Comte</last></author>
      <author id="victoria-ruiz-serra" orcid="0000-0003-3991-0514"><first>Victoria</first><last>Ruiz-Serra</last><affiliation>European Commission</affiliation></author>
      <author><first>Lia</first><last>Orfei</last><affiliation>Joint Research Centre of the European Commission</affiliation></author>
      <author id="mario-ceresa" orcid="0000-0002-2410-0212"><first>Mario</first><last>Ceresa</last><affiliation>Joint Research Center</affiliation></author>
      <pages>111-123</pages>
      <abstract>Vision-language models (VLMs) can achieve high accuracy in medical applications but can retain demographic biases from training data. While multiple works have identified the presence of these biases in many VLMs, it remains unclear how strong their impact at the inference level is. In this work, we study how well a task-level explainability method based on linear combinations of words can detect multiple types of biases, with a focus on medical image classification. By manipulating the training datasets with demographic and non-demographic biases, we show how the adopted approach can detect explicitly encoded biases but fails with implicitly encoded ones, particularly biological sex. Our results suggest that such a failure likely stems from misalignment between sex-describing features in image versus text modalities. Our findings highlight limitations in the evaluated explainability method for detecting implicit biases in medical VLMs.</abstract>
      <url hash="9d102690">2025.gebnlp-1.12</url>
      <bibkey>bertolini-etal-2025-strengths</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Wanted: Personalised Bias Warnings for Gender Bias in Language Models</title>
      <author><first>Chiara</first><last>Di Bonaventura</last></author>
      <author id="michelle-nwachukwu" orcid="0009-0007-7546-4888"><first>Michelle</first><last>Nwachukwu</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Maria</first><last>Stoica</last></author>
      <pages>124-136</pages>
      <abstract>The widespread use of language models, especially Large Language Models, paired with their inherent biases can propagate and amplify societal inequalities. While research has extensively explored methods for bias mitigation and measurement, limited attention has been paid to how such biases are communicated to users, which instead can have a positive impact on increasing user trust and understanding of these models. Our study addresses this gap by investigating user preferences for gender bias mitigation, measurement and communication in language models. To this end, we conducted a user study targeting female AI practitioners with eighteen female and one male participant. Our findings reveal that user preferences for bias mitigation and measurement show strong consensus, whereas they vary widely for bias communication, underscoring the importance of tailoring warnings to individual needs.Building on these findings, we propose a framework for user-centred bias reporting, which leverages runtime monitoring techniques to assess and visualise bias in real time and in a customizable fashion.</abstract>
      <url hash="a16ea3da">2025.gebnlp-1.13</url>
      <bibkey>di-bonaventura-etal-2025-wanted</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>GG</fixed-case>-<fixed-case>BBQ</fixed-case>: <fixed-case>G</fixed-case>erman Gender Bias Benchmark for Question Answering</title>
      <author><first>Shalaka</first><last>Satheesh</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Katrin</first><last>Klug</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author id="katharina-beckh" orcid="0000-0002-7824-6647"><first>Katharina</first><last>Beckh</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author id="hector-allende-cid" orcid="0000-0003-3047-8817"><first>Héctor</first><last>Allende-Cid</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Sebastian</first><last>Houben</last><affiliation>Hochschule Bonn-Rhein-Sieg</affiliation></author>
      <author id="teena-hassan" orcid="0000-0002-6105-0752"><first>Teena</first><last>Hassan</last><affiliation>Hochschule Bonn-Rhein-Sieg</affiliation></author>
      <pages>137-148</pages>
      <abstract>Within the context of Natural Language Processing (NLP), fairness evaluation is often associated with the assessment of bias and reduction of associated harm. In this regard, the evaluation is usually carried out by using a benchmark dataset, for a task such as Question Answering, created for the measurement of bias in the model’s predictions along various dimensions, including gender identity. In our work, we evaluate gender bias in German Large Language Models (LLMs) using the Bias Benchmark for Question Answering by Parrish et al. (2022) as a reference. Specifically, the templates in the gender identity subset of this English dataset were machine translated into German. The errors in the machine translated templates were then manually reviewed and corrected with the help of a language expert. We find that manual revision of the translation is crucial when creating datasets for gender bias evaluation because of the limitations of machine translation from English to a language such as German with grammatical gender. Our final dataset is comprised of two subsets: Subset-I, which consists of group terms related to gender identity, and Subset-II, where group terms are replaced with proper names. We evaluate several LLMs used for German NLP on this newly created dataset and report the accuracy and bias scores. The results show that all models exhibit bias, both along and against existing social stereotypes.</abstract>
      <url hash="7dcb97ae">2025.gebnlp-1.14</url>
      <bibkey>satheesh-etal-2025-gg</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Tag-First: Mitigating Distributional Bias in Synthetic User Profiles through Controlled Attribute Generation</title>
      <author id="ismael-garrido-munoz" orcid="0000-0001-6656-9679"><first>Ismael</first><last>Garrido-Muñoz</last><affiliation>Universidad de Jaén</affiliation></author>
      <author id="arturo-montejo-raez" orcid="0000-0002-8643-2714"><first>Arturo</first><last>Montejo-Ráez</last><affiliation>Universidad de Jaén</affiliation></author>
      <author id="fernando-martinez-santiago-1752" orcid="0000-0002-1480-1752"><first>Fernando Martínez</first><last>Santiago</last></author>
      <pages>149-159</pages>
      <abstract>Addressing the critical need for robust bias testing in AI systems, current methods often rely on overly simplistic or rigid persona templates, limiting the depth and realism of fairness evaluations. We introduce a novel framework and an associated tool designed to generate high-quality, diverse, and configurable personas specifically for nuanced bias assessment. Our core innovation lies in a two-stage process: first, generating structured persona tags based solely on user-defined configurations (specified manually or via an included agent tool), ensuring attribute distributions are controlled and crucially, are not skewed by an LLM’s inherent biases regarding attribute correlations during the selection phase. Second, transforming these controlled tags into various realistic outputs—including natural language descriptions, CVs, or profiles—suitable for diverse bias testing scenarios. This tag-centric approach preserves ground-truth attributes for analyzing correlations and biases within the generated population and downstream AI applications. We demonstrate the system’s efficacy by generating and validating 1,000 personas, analyzing both the adherence of natural language descriptions to the source tags and the potential biases introduced by the LLM during the transformation step. The provided dataset, including both generated personas and their source tags, enables detailed analysis. This work offers a significant step towards more reliable, controllable, and representative fairness testing in AI development.</abstract>
      <url hash="f7bf2d6f">2025.gebnlp-1.15</url>
      <bibkey>garrido-munoz-etal-2025-tag</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Characterizing non-binary <fixed-case>F</fixed-case>rench: A first step towards debiasing gender inference</title>
      <author id="marie-flesch" orcid="0000-0003-2139-1517"><first>Marie</first><last>Flesch</last></author>
      <author><first>Heather</first><last>Burnett</last><affiliation>CNRS</affiliation></author>
      <pages>160-170</pages>
      <abstract>This paper addresses a bias of gender inference systems: their binary nature. Based on the observation that, for French, systems based on pattern-matching of grammatical gender markers in “I am” expressions perform better than machine-learning approaches (Ciot et al. 2013), we examine the use of grammatical gender by non-binary individuals. We describe the construction of a corpus of texts produced by non-binary authors on Reddit, (formely) Twitter and three forums. Our linguistic analysis shows three main patterns of use: authors who use non-binary markers, authors who consistently use one grammatical gender, and authors who use both feminine and masculine markers. Using this knowledge, we make proposals for the improvements of existing gender inference systems based on grammatical gender.</abstract>
      <url hash="a9751889">2025.gebnlp-1.16</url>
      <bibkey>flesch-burnett-2025-characterizing</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Can Explicit Gender Information Improve Zero-Shot Machine Translation?</title>
      <author><first>Van-Hien</first><last>Tran</last><affiliation>NICT</affiliation></author>
      <author id="huy-hien-vu" orcid="0000-0002-5957-0345"><first>Huy Hien</first><last>Vu</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hideki</first><last>Tanaka</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>171-181</pages>
      <abstract>Large language models (LLMs) have demonstrated strong zero-shot machine translation (MT) performance but often exhibit gender bias that is present in their training data, especially when translating into grammatically gendered languages. In this paper, we investigate whether explicitly providing gender information can mitigate this issue and improve translation quality. We propose a two-step approach: (1) inferring entity gender from context, and (2) incorporating this information into prompts using either <b>Structured Tagging</b> or <b>Natural Language</b>. Experiments with five LLMs across four language pairs show that explicit gender cues consistently reduce gender errors, with structured tagging yielding the largest gains. Our results highlight prompt-level gender disambiguation as a simple yet effective strategy for more accurate and fair zero-shot MT.</abstract>
      <url hash="ce859aff">2025.gebnlp-1.17</url>
      <bibkey>tran-etal-2025-explicit</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from <fixed-case>LLM</fixed-case>s</title>
      <author><first>Elisa Forcada</first><last>Rodríguez</last></author>
      <author id="olatz-perez-de-vinaspre" orcid="0000-0002-0933-2461"><first>Olatz</first><last>Perez-de-Vinaspre</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Jon Ander</first><last>Campos</last><affiliation>Cohere</affiliation></author>
      <author id="dietrich-klakow" orcid="0000-0002-4147-9690"><first>Dietrich</first><last>Klakow</last></author>
      <author id="vagrant-gautam" orcid="0000-0002-7263-8578"><first>Vagrant</first><last>Gautam</last></author>
      <pages>182-194</pages>
      <abstract>One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work.</abstract>
      <url hash="f660f197">2025.gebnlp-1.18</url>
      <bibkey>rodriguez-etal-2025-colombian</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Bias Attribution in <fixed-case>F</fixed-case>ilipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages</title>
      <author id="lance-calvin-lim-gamboa" orcid="0000-0003-0095-1335"><first>Lance Calvin Lim</first><last>Gamboa</last><affiliation>University of Birmingham and Ateneo de Manila University</affiliation></author>
      <author><first>Yue</first><last>Feng</last><affiliation>University of Birmingham</affiliation></author>
      <author id="mark-lee"><first>Mark G.</first><last>Lee</last></author>
      <pages>195-205</pages>
      <abstract>Emerging research on bias attribution and interpretability have revealed how tokens contribute to biased behavior in language models processing English texts. We build on this line of inquiry by adapting the information-theoretic bias attribution score metric for implementation on models handling agglutinative languages—particularly Filipino. We then demonstrate the effectiveness of our adapted method by using it on a purely Filipino model and on three multilingual models—one trained on languages worldwide and two on Southeast Asian data. Our results show that Filipino models are driven towards bias by words pertaining to <tex-math>\textit{people}</tex-math>, <tex-math>\textit{objects}</tex-math>, and <tex-math>\textit{relationships}</tex-math>—entity-based themes that stand in contrast to the action-heavy nature of bias-contributing themes in English (i.e., <tex-math>\textit{criminal}</tex-math>, <tex-math>\textit{sexual}</tex-math>, and <tex-math>\textit{prosocial}</tex-math> behaviors). These findings point to differences in how English and non-English models process inputs linked to sociodemographic groups and bias.</abstract>
      <url hash="79b1ac24">2025.gebnlp-1.19</url>
      <bibkey>gamboa-etal-2025-bias</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models</title>
      <author><first>Aleksandra</first><last>Sorokovikova</last><affiliation>JetBrains</affiliation></author>
      <author><first>Pavel</first><last>Chizhov</last><affiliation>Technical University of Applied Sciences Würzburg-Schweinfurt and PleIAs</affiliation></author>
      <author><first>Iuliia</first><last>Eremenko</last><affiliation>Universität Kassel</affiliation></author>
      <author id="ivan-p-yamshchikov" orcid="0000-0003-3784-0671"><first>Ivan P.</first><last>Yamshchikov</last><affiliation>Technical University of Applied Sciences Würzburg-Schweinfurt</affiliation></author>
      <pages>206-227</pages>
      <abstract>Modern language models are trained on large amounts of data. These data inevitably include controversial and stereotypical content, which contains all sorts of biases related to gender, origin, age, etc. As a result, the models express biased points of view or produce different results based on the assigned personality or the personality of the user. In this paper, we investigate various proxy measures of bias in large language models (LLMs). We find that evaluating models with pre-prompted personae on a multi-subject benchmark (MMLU) leads to negligible and mostly random differences in scores. However, if we reformulate the task and ask a model to grade the user’s answer, this shows more significant signs of bias. Finally, if we ask the model for salary negotiation advice, we see pronounced bias in the answers. With the recent trend for LLM assistant memory and personalization, these problems open up from a different angle: modern LLM users do not need to pre-prompt the description of their persona since the model already knows their socio-demographics.</abstract>
      <url hash="805f9b92">2025.gebnlp-1.20</url>
      <bibkey>sorokovikova-etal-2025-surface</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.20</doi>
    </paper>
    <paper id="21">
      <title>Measuring Gender Bias in Language Models in <fixed-case>F</fixed-case>arsi</title>
      <author id="hamidreza-saffari" orcid="0009-0004-4252-5662"><first>Hamidreza</first><last>Saffari</last><affiliation>Polytechnic Institute of Milan</affiliation></author>
      <author><first>Mohammadamin</first><last>Shafiei</last><affiliation>University of Milan</affiliation></author>
      <author id="donya-rooein" orcid="0000-0002-0368-3084"><first>Donya</first><last>Rooein</last><affiliation>Bocconi University</affiliation></author>
      <author id="debora-nozza" orcid="0000-0002-7998-2267"><first>Debora</first><last>Nozza</last><affiliation>Bocconi University</affiliation></author>
      <pages>228-241</pages>
      <abstract>As Natural Language Processing models become increasingly embedded in everyday life, ensuring that these systems can measure and mitigate bias is critical. While substantial work has been done to identify and mitigate gender bias in English, Farsi remains largely underexplored. This paper presents the first comprehensive study of gender bias in language models in Farsi across three tasks: emotion analysis, question answering, and hurtful sentence completion. We assess a range of language models across all the tasks in zero-shot settings. By adapting established evaluation frameworks for Farsi, we uncover patterns of gender bias that differ from those observed in English, highlighting the urgent need for culturally and linguistically inclusive approaches to bias mitigation in NLP.</abstract>
      <url hash="af45dfe3">2025.gebnlp-1.21</url>
      <bibkey>saffari-etal-2025-measuring</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.21</doi>
    </paper>
    <paper id="22">
      <title>A Diachronic Analysis of Human and Model Predictions on Audience Gender in How-to Guides</title>
      <author id="margherita-fanton" orcid="0000-0002-8953-6148"><first>Nicola</first><last>Fanton</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author id="sidharth-ranjan" orcid="0000-0002-8032-5504"><first>Sidharth</first><last>Ranjan</last><affiliation>Universität Stuttgart</affiliation></author>
      <author id="titus-von-der-malsburg" orcid="0000-0001-5925-5145"><first>Titus Von Der</first><last>Malsburg</last><affiliation>Universität Stuttgart</affiliation></author>
      <author><first>Michael</first><last>Roth</last><affiliation>University of Technology Nuremberg</affiliation></author>
      <pages>242-255</pages>
      <abstract>We examine audience-specific how-to guides on wikiHow, in English, diachronically by comparing predictions from fine-tuned language models and human judgments. Using both early and revised versions, we quantitatively and qualitatively study how gender-specific features are identified over time. While language model performance remains relatively stable in terms of macro F<tex-math>_1</tex-math>-scores, we observe an increased reliance on stereotypical tokens. Notably, both models and human raters tend to overpredict women as an audience, raising questions about bias in the evaluation of educational systems and resources.</abstract>
      <url hash="e7bcd08a">2025.gebnlp-1.22</url>
      <bibkey>fanton-etal-2025-diachronic</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>A</fixed-case>r<fixed-case>GAN</fixed-case>: <fixed-case>A</fixed-case>rabic Gender, Ability, and Nationality Dataset for Evaluating Biases in Large Language Models</title>
      <author><first>Ranwa</first><last>Aly</last></author>
      <author><first>Yara</first><last>Allam</last></author>
      <author><first>Rana</first><last>Gaber</last></author>
      <author><first>Christine</first><last>Basta</last><affiliation>Alexandria University</affiliation></author>
      <pages>256-267</pages>
      <abstract>Large language models (LLMs) are pretrained on substantial, unfiltered corpora, assembled from a variety of sources. This risks inheriting the deep-rooted biases that exist within them, both implicit and explicit. This is even more apparent in low-resource languages, where corpora may be prioritized by quantity over quality, potentially leading to more unchecked biases. More specifically, we address the biases present in the Arabic language in both general-purpose and Arabic-specialized architectures in three dimensions of demographics: gender, ability, and nationality. To properly assess the fairness of these models, we experiment with bias-revealing prompts and estimate the performance using existing evaluation metrics, and propose adaptations to others.</abstract>
      <url hash="94a48574">2025.gebnlp-1.23</url>
      <bibkey>aly-etal-2025-argan</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Assessing Gender Bias of Pretrained <fixed-case>B</fixed-case>angla Language Models in <fixed-case>STEM</fixed-case> and <fixed-case>SHAPE</fixed-case> Fields</title>
      <author id="noor-mairukh-khan-arnob" orcid="0009-0008-6278-4134"><first>Noor Mairukh Khan</first><last>Arnob</last><affiliation>University of Asia Pacific</affiliation></author>
      <author id="saiyara-mahmud" orcid="0009-0000-6017-6831"><first>Saiyara</first><last>Mahmud</last></author>
      <author id="azmine-toushik-wasi" orcid="0000-0001-9509-5804"><first>Azmine Toushik</first><last>Wasi</last></author>
      <pages>268-281</pages>
      <abstract>Gender bias continues to shape societal perceptions across both STEM (Science, Technology, Engineering, and Mathematics) and SHAPE (Social Sciences, Humanities, and the Arts for People and the Economy) domains. While existing studies have explored such biases in English language models, similar analyses in Bangla—spoken by over 240 million people—remain scarce. In this work, we investigate gender-profession associations in Bangla language models. We introduce <i>Pokkhopat</i>, a curated dataset of gendered terms and profession-related words across STEM and SHAPE disciplines. Using a suite of embedding-based bias detection methods—including WEAT, ECT, RND, RIPA, and cosine similarity visualizations—we evaluate 11 Bangla language models. Our findings show that several widely-used open-source Bangla NLP models (e.g., sagorsarker/bangla-bert-base) exhibit significant gender bias, underscoring the need for more inclusive and bias-aware development in low-resource languages like Bangla. We also find that many STEM and SHAPE-related words are absent from these models’ vocabularies, complicating bias detection and possibly amplifying existing biases. This emphasizes the importance of incorporating more diverse and comprehensive training data to mitigate such biases moving forward. Code available at <url>https://github.com/HerWILL-Inc/ACL-2025/</url>.</abstract>
      <url hash="829ce23c">2025.gebnlp-1.24</url>
      <bibkey>arnob-etal-2025-assessing</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.24</doi>
    </paper>
    <paper id="25">
      <title>One Size Fits None: Rethinking Fairness in Medical <fixed-case>AI</fixed-case></title>
      <author><first>Roland</first><last>Roller</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Michael</first><last>Hahn</last><affiliation>Friedrich-Alexander-Universität Erlangen-Nürnberg</affiliation></author>
      <author><first>Ajay Madhavan</first><last>Ravichandran</last><affiliation>Technische Universität Berlin</affiliation></author>
      <author><first>Bilgin</first><last>Osmanodja</last><affiliation>Charité - Universitätsmedizin Berlin</affiliation></author>
      <author><first>Florian</first><last>Oetke</last><affiliation>DNC Information Management GmbH</affiliation></author>
      <author><first>Zeineb</first><last>Sassi</last><affiliation>University of Regensburg</affiliation></author>
      <author id="aljoscha-burchardt" orcid="0000-0002-5117-5965"><first>Aljoscha</first><last>Burchardt</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Klaus</first><last>Netter</last><affiliation>University of Regensburg</affiliation></author>
      <author><first>Klemens</first><last>Budde</last><affiliation>Charité - Universitätsmedizin Berlin</affiliation></author>
      <author><first>Anne</first><last>Herrmann</last><affiliation>University of Regensburg, University Hospital Regensburg</affiliation></author>
      <author><first>Tobias</first><last>Strapatsas</last><affiliation>Asklepios Klinikum Harburg</affiliation></author>
      <author><first>Peter</first><last>Dabrock</last><affiliation>Friedrich-Alexander-Universität Erlangen-Nürnberg</affiliation></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>282-289</pages>
      <abstract>Machine learning (ML) models are increasingly used to support clinical decision-making. However, real-world medical datasets are often noisy, incomplete, and imbalanced, leading to performance disparities across patient subgroups. These differences raise fairness concerns, particularly when they reinforce existing disadvantages for marginalized groups. In this work, we analyze several medical prediction tasks and demonstrate how model performance varies with patient characteristics. While ML models may demonstrate good overall performance, we argue that subgroup-level evaluation is essential before integrating them into clinical workflows. By conducting a performance analysis at the subgroup level, differences can be clearly identified—allowing, on the one hand, for performance disparities to be considered in clinical practice, and on the other hand, for these insights to inform the responsible development of more effective models. Thereby, our work contributes to a practical discussion around the subgroup-sensitive development and deployment of medical ML models and the interconnectedness of fairness and transparency.</abstract>
      <url hash="db260c7d">2025.gebnlp-1.25</url>
      <bibkey>roller-etal-2025-one</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.25</doi>
    </paper>
    <paper id="26">
      <title>From Measurement to Mitigation: Exploring the Transferability of Debiasing Approaches to Gender Bias in <fixed-case>M</fixed-case>altese Language Models</title>
      <author><first>Melanie</first><last>Galea</last></author>
      <author id="claudia-borg" orcid="0000-0001-8704-6893"><first>Claudia</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <pages>290-301</pages>
      <abstract>The advancement of Large Language Models (LLMs) has transformed Natural Language Processing (NLP), enabling performance across diverse tasks with little task-specific training. However, LLMs remain susceptible to social biases, particularly reflecting harmful stereotypes from training data, which can disproportionately affect marginalised communities.We measure gender bias in Maltese LMs, arguing that such bias is harmful as it reinforces societal stereotypes and fails to account for gender diversity, which is especially problematic in gendered, low-resource languages.While bias evaluation and mitigation efforts have progressed for English-centric models, research on low-resourced and morphologically rich languages remains limited. This research investigates the transferability of debiasing methods to Maltese language models, focusing on BERTu and mBERTu, BERT-based monolingual and multilingual models respectively. Bias measurement and mitigation techniques from English are adapted to Maltese, using benchmarks such as CrowS-Pairs and SEAT, alongside debiasing methods Counterfactual Data Augmentation, Dropout Regularization, Auto-Debias, and GuiDebias. We also contribute to future work in the study of gender bias in Maltese by creating evaluation datasets.Our findings highlight the challenges of applying existing bias mitigation methods to linguistically complex languages, underscoring the need for more inclusive approaches in the development of multilingual NLP.</abstract>
      <url hash="dd1cd532">2025.gebnlp-1.26</url>
      <bibkey>galea-borg-2025-measurement</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.26</doi>
    </paper>
    <paper id="27">
      <title><fixed-case>GENDEROUS</fixed-case>: Machine Translation and Cross-Linguistic Evaluation of a Gender-Ambiguous Dataset</title>
      <author><first>Janiça</first><last>Hackenbuchner</last></author>
      <author><first>Eleni</first><last>Gkovedarou</last></author>
      <author><first>Joke</first><last>Daems</last></author>
      <pages>302-319</pages>
      <abstract>Contributing to research on gender beyond the binary, this work introduces GENDEROUS, a dataset of gender-ambiguous sentences containing gender-marked occupations and adjectives, and sentences with the ambiguous or non-binary pronoun their. We cross-linguistically evaluate how machine translation (MT) systems and large language models (LLMs) translate these sentences from English into four grammatical gender languages: Greek, German, Spanish and Dutch. We show the systems’ continued default to male-gendered translations, with exceptions (particularly for Dutch). Prompting for alternatives, however, shows potential in attaining more diverse and neutral translations across all languages. An LLM-as-a-judge approach was implemented, where benchmarking against gold standards emphasises the continued need for human annotations.</abstract>
      <url hash="6026079e">2025.gebnlp-1.27</url>
      <bibkey>hackenbuchner-etal-2025-genderous</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.27</doi>
    </paper>
    <paper id="28">
      <title>Fine-Tuning vs Prompting Techniques for Gender-Fair Rewriting of Machine Translations</title>
      <author><first>Paolo</first><last>Mainardi</last><affiliation>University of Bologna</affiliation></author>
      <author id="federico-garcea" orcid="0000-0002-1071-5056"><first>Federico</first><last>Garcea</last><affiliation>University of Bologna</affiliation></author>
      <author id="alberto-barron-cedeno" orcid="0000-0003-4719-3420"><first>Alberto</first><last>Barrón-Cedeño</last><affiliation>Università di Bologna</affiliation></author>
      <pages>320-337</pages>
      <abstract>Increasing attention is being dedicated by the NLP community to gender-fair practices, including emerging forms of non-binary language. Given the shift to the prompting paradigm for multiple tasks, direct comparisons between prompted and fine-tuned models in this context are lacking. We aim to fill this gap by comparing prompt engineering and fine-tuning techniques for gender-fair rewriting in Italian. We do so by framing a rewriting task where Italian gender-marked translations from English gender-ambiguous sentences are adapted into a gender-neutral alternative using direct non-binary language. We augment existing datasets with gender-neutral translations and conduct experiments to determine the best architecture and approach to complete such task, by fine-tuning and prompting seq2seq encoder-decoder and autoregressive decoder-only models. We show that smaller seq2seq models can reach good performance when fine-tuned, even with relatively little data; when it comes to prompts, including task demonstrations is crucial, and we find that chat-tuned models reach the best results in a few-shot setting. We achieve promising results, especially in contexts of limited data and resources.</abstract>
      <url hash="d14ccbb8">2025.gebnlp-1.28</url>
      <bibkey>mainardi-etal-2025-fine</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.28</doi>
    </paper>
    <paper id="29">
      <title>Some Myths About Bias: A Queer Studies Reading Of Gender Bias In <fixed-case>NLP</fixed-case></title>
      <author id="filipa-calado" orcid="0000-0002-3512-6717"><first>Filipa</first><last>Calado</last><affiliation>Pratt Institute</affiliation></author>
      <pages>338-346</pages>
      <abstract>This paper critiques common assumptions about gender bias in NLP, focusing primarily on word vector-based methods for detecting and mitigating bias. It argues that these methods assume a kind of “binary thinking” that goes beyond the gender binary toward a conceptual model that structures and limits the effectiveness of these techniques. Drawing its critique from the Humanities field of Queer Studies, this paper demonstrates that binary thinking drives two “myths” in gender bias research: first, that bias is categorical, measuring bias in terms of presence/absence, and second, that it is zero-sum, where the relations between genders are idealized as symmetrical. Due to their use of binary thinking, each of these myths flattens bias into a measure that cannot distinguish between the types of bias and their effects in language. The paper concludes by briefly pointing to methods that resist binary thinking, such as those that diversify and amplify gender expressions.</abstract>
      <url hash="09a1dd08">2025.gebnlp-1.29</url>
      <bibkey>calado-2025-myths</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.29</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>G</fixed-case>en<fixed-case>W</fixed-case>riter: Reducing Gender Cues in Biographies through Text Rewriting</title>
      <author id="shweta-soundararajan" orcid="0009-0005-2171-5501"><first>Shweta</first><last>Soundararajan</last><affiliation>Technological University Dublin</affiliation></author>
      <author id="sarah-jane-delany" orcid="0000-0002-2062-7439"><first>Sarah Jane</first><last>Delany</last><affiliation>Technological University Dublin</affiliation></author>
      <pages>347-357</pages>
      <abstract>Gendered language is the use of words that indicate an individual’s gender. Though useful in certain context, it can reinforce gender stereotypes and introduce bias, particularly in machine learning models used for tasks like occupation classification. When textual content such as biographies contains gender cues, it can influence model predictions, leading to unfair outcomes such as reduced hiring opportunities for women. To address this issue, we propose GenWriter, an approach that integrates Case-Based Reasoning (CBR) with Large Language Models (LLMs) to rewrite biographies in a way that obfuscates gender while preserving semantic content. We evaluate GenWriter by measuring gender bias in occupation classification before and after rewriting the biographies used for training the occupation classification model. Our results show that GenWriter significantly reduces gender bias by 89% in nurse biographies and 62% in surgeon biographies, while maintaining classification accuracy. In comparison, an LLM-only rewriting approach achieves smaller bias reductions (by 44% and 12% in nurse and surgeon biographies, respectively) and leads to some classification performance degradation.</abstract>
      <url hash="3612c57f">2025.gebnlp-1.30</url>
      <bibkey>soundararajan-delany-2025-genwriter</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.30</doi>
    </paper>
    <paper id="31">
      <title>Examining the Cultural Encoding of Gender Bias in <fixed-case>LLM</fixed-case>s for Low-Resourced <fixed-case>A</fixed-case>frican Languages</title>
      <author id="abigail-oppong" orcid="0000-0002-6468-1787"><first>Abigail</first><last>Oppong</last></author>
      <author><first>Hellina Hailu</first><last>Nigatu</last></author>
      <author id="chinasa-t-okolo" orcid="0000-0002-6474-3378"><first>Chinasa T.</first><last>Okolo</last><affiliation>The Brookings Institution</affiliation></author>
      <pages>358-378</pages>
      <abstract>Large Language Models (LLMs) are deployed in several aspects of everyday life. While the technology could have several benefits, like many socio-technical systems, it also encodes several biases. Trained on large, crawled datasets from the web, these models perpetuate stereotypes and regurgitate representational bias that is rampant in their training data. Languages encode gender in varying ways; some languages are grammatically gendered, while others do not. Bias in the languages themselves may also vary based on cultural, social, and religious contexts. In this paper, we investigate gender bias in LLMs by selecting two languages, Twi and Amharic. Twi is a non-gendered African language spoken in Ghana, while Amharic is a gendered language spoken in Ethiopia. Using these two languages on the two ends of the continent and their opposing grammatical gender system, we evaluate LLMs in three tasks: Machine Translation, Image Generation, and Sentence Completion. Our results give insights into the gender bias encoded in LLMs using two low-resourced languages and broaden the conversation on how culture and social structures play a role in disparate system performances.</abstract>
      <url hash="2f19d93b">2025.gebnlp-1.31</url>
      <bibkey>oppong-etal-2025-examining</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.31</doi>
    </paper>
    <paper id="32">
      <title>Ableism, Ageism, Gender, and Nationality bias in <fixed-case>N</fixed-case>orwegian and Multilingual Language Models</title>
      <author><first>Martin</first><last>Sjåvik</last></author>
      <author><first>Samia</first><last>Touileb</last><affiliation>University of Bergen</affiliation></author>
      <pages>379-392</pages>
      <abstract>We investigate biases related to ageism, ableism, nationality, and gender in four Norwegian and two multilingual language models. Our methodology involves using a set of templates. constructed around stimuli and attributes relevant to these categories. We use statistical and predictive evaluation methods, including Kendall’s Tau correlation and dependent variable prediction rates, to assess model behaviour and output bias. Our findings indicate that models frequently associate older individuals, people with disabilities, and poorer countries with negative attributes, potentially reinforcing harmful stereotypes. However, most tested models appear to handle gender-related biases more effectively. Our findings indicate a correlation between the sentiment of the input and that of the output.</abstract>
      <url hash="5a72ac21">2025.gebnlp-1.32</url>
      <bibkey>sjavik-touileb-2025-ableism</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.32</doi>
    </paper>
    <paper id="33">
      <title>Disentangling Biased Representations: A Causal Intervention Framework for Fairer <fixed-case>NLP</fixed-case> Models</title>
      <author id="yangge-qian" orcid="0009-0003-1787-7108"><first>Yangge</first><last>Qian</last></author>
      <author><first>Yilong</first><last>Hu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author id="siqi-zhang" orcid="0000-0002-8562-8125"><first>Siqi</first><last>Zhang</last></author>
      <author><first>Xu</first><last>Gu</last></author>
      <author id="xiaolin-qin" orcid="0000-0001-5087-8178"><first>Xiaolin</first><last>Qin</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <pages>393-402</pages>
      <abstract>Natural language processing (NLP) systems often inadvertently encode and amplify social biases through entangled representations of demographic attributes and task-related attributes. To mitigate this, we propose a novel framework that combines causal analysis with practical intervention strategies. The method leverages attribute-specific prompting to isolate sensitive attributes while applying information-theoretic constraints to minimize spurious correlations. Experiments across six language models and two classification tasks demonstrate its effectiveness. We hope this work will provide the NLP community with a causal disentanglement perspective for achieving fairness in NLP systems.</abstract>
      <url hash="c6b0f413">2025.gebnlp-1.33</url>
      <bibkey>qian-etal-2025-disentangling</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.33</doi>
    </paper>
    <paper id="35">
      <title>Towards Massive Multilingual Holistic Bias</title>
      <author><first>Xiaoqing</first><last>Tan</last><affiliation>Meta AI</affiliation></author>
      <author><first>Prangthip</first><last>Hansanti</last></author>
      <author><first>Arina</first><last>Turkatenko</last><affiliation>Facebook</affiliation></author>
      <author><first>Joe</first><last>Chuang</last><affiliation>FAIR</affiliation></author>
      <author><first>Carleigh</first><last>Wood</last></author>
      <author><first>Bokai</first><last>Yu</last><affiliation>Meta AI</affiliation></author>
      <author><first>Christophe</first><last>Ropers</last><affiliation>Meta</affiliation></author>
      <author id="marta-r-costa-jussa"><first>Marta R.</first><last>Costa-jussà</last><affiliation>Meta</affiliation></author>
      <pages>403-426</pages>
      <abstract>In the current landscape of automatic language generation, there is a need to understand, evaluate, and mitigate demographic biases, as existing models are becoming increasingly multilingual. To address this, we present the initial eight languages from the Massive Multilingual Holistic Bias (MMHB) dataset and benchmark consisting of approximately 6 million sentences. The sentences are designed to induce biases towards different groups of people which can yield significant results when using them as a benchmark to test different text generation models. To further scale up in terms of both language coverage and size and to leverage limited human translation, we use systematic approach to independently translate sentence parts. This technique carefully designs a structure to dynamically generate multiple sentence variations and significantly reduces the human translation workload. The translation process has been meticulously conducted to avoid an English-centric perspective and include all necessary morphological variations for languages that require them, improving from the original English HOLISTICBIAS. Finally, we utilize MMHB to report results on gender bias and added toxicity in MT tasks.</abstract>
      <url hash="7a110326">2025.gebnlp-1.35</url>
      <bibkey>tan-etal-2025-towards</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.35</doi>
    </paper>
    <paper id="37">
      <title>Exploring Gender Bias in Large Language Models: An In-depth Dive into the <fixed-case>G</fixed-case>erman Language</title>
      <author><first>Kristin</first><last>Gnadt</last></author>
      <author id="david-thulke" orcid="0000-0002-9808-7073"><first>David</first><last>Thulke</last><affiliation>RWTH Aachen University and AppTek</affiliation></author>
      <author><first>Simone</first><last>Kopeinik</last><affiliation>Know Center GmbH</affiliation></author>
      <author id="ralf-schlueter" orcid="0000-0003-2839-9247"><first>Ralf</first><last>Schlüter</last><affiliation>AppTek GmbH and Rheinisch Westfälische Technische Hochschule Aachen</affiliation></author>
      <pages>427-450</pages>
      <abstract>In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs). A key challenge lies in the transferability of bias measurement methods initially developed for the English language when applied to other languages. This work aims to contribute to this research strand by presenting five German datasets for gender bias evaluation in LLMs. The datasets are grounded in well-established concepts of gender bias and are accessible through multiple methodologies. Our findings, reported for eight multilingual LLM models, reveal unique challenges associated with gender bias in German, including the ambiguous interpretation of male occupational terms and the influence of seemingly neutral nouns on gender perception. This work contributes to the understanding of gender bias in LLMs across languages and underscores the necessity for tailored evaluation frameworks.</abstract>
      <url hash="e953dc90">2025.gebnlp-1.37</url>
      <bibkey>gnadt-etal-2025-exploring</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.37</doi>
    </paper>
    <paper id="38">
      <title>Adapting Psycholinguistic Research for <fixed-case>LLM</fixed-case>s: Gender-inclusive Language in a Coreference Context</title>
      <author id="marion-bartl" orcid="0000-0002-8893-4961"><first>Marion</first><last>Bartl</last></author>
      <author id="thomas-brendan-murphy" orcid="0000-0002-5668-7046"><first>Thomas Brendan</first><last>Murphy</last><affiliation>University College Dublin</affiliation></author>
      <author><first>Susan</first><last>Leavy</last><affiliation>University College Dublin</affiliation></author>
      <pages>451-467</pages>
      <abstract>Gender-inclusive language is often used with the aim of ensuring that all individuals, regardless of gender, can be associated with certain concepts. While psycholinguistic studies have examined its effects in relation to human cognition, it remains unclear how Large Language Models (LLMs) process gender-inclusive language. Given that commercial LLMs are gaining an increasingly strong foothold in everyday applications, it is crucial to examine whether LLMs in fact interpret gender-inclusive language neutrally, because the language they generate has the potential to influence the language of their users. This study examines whether LLM-generated coreferent terms align with a given gender expression or reflect model biases. Adapting psycholinguistic methods from French to English and German, we find that in English, LLMs generally maintain the antecedent’s gender but exhibit underlying masculine bias. In German, this bias is much stronger, overriding all tested gender-neutralization strategies.</abstract>
      <url hash="11af5bb8">2025.gebnlp-1.38</url>
      <bibkey>bartl-etal-2025-adapting</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.38</doi>
    </paper>
    <paper id="39">
      <title>Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora</title>
      <author id="erik-derner" orcid="0000-0002-7588-7668"><first>Erik</first><last>Derner</last><affiliation>ELLIS Alicante</affiliation></author>
      <author><first>Sara Sansalvador De La</first><last>Fuente</last></author>
      <author id="yoan-gutierrez-7427" orcid="0000-0002-4052-7427"><first>Yoan</first><last>Gutierrez</last><affiliation>University of Alicante</affiliation></author>
      <author id="paloma-moreda-pozo-1561" orcid="0000-0002-7193-1561"><first>Paloma Moreda</first><last>Pozo</last><affiliation>Universidad de Alicante</affiliation></author>
      <author id="nuria-m-oliver" orcid="0000-0001-5985-691X"><first>Nuria M</first><last>Oliver</last><affiliation>ELLIS unit Alicante Foundation and Chief Data Scientist</affiliation></author>
      <pages>468-483</pages>
      <abstract>Large language models (LLMs) often inherit and amplify social biases embedded in their training data. A prominent social bias is gender bias. In this regard, prior work has mainly focused on gender stereotyping bias – the association of specific roles or traits with a particular gender – in English and on evaluating gender bias in model embeddings or generated outputs. In contrast, gender representation bias – the unequal frequency of references to individuals of different genders – in the training corpora has received less attention. Yet such imbalances in the training data constitute an upstream source of bias that can propagate and intensify throughout the entire model lifecycle. To fill this gap, we propose a novel LLM-based method to detect and quantify gender representation bias in LLM training data in gendered languages, where grammatical gender challenges the applicability of methods developed for English. By leveraging the LLMs’ contextual understanding, our approach automatically identifies and classifies person-referencing words in gendered language corpora. Applied to four Spanish-English benchmarks and five Valencian corpora, our method reveals substantial male-dominant imbalances. We show that such biases in training data affect model outputs, but can surprisingly be mitigated leveraging small-scale training on datasets that are biased towards the opposite gender. Our findings highlight the need for corpus-level gender bias analysis in multilingual NLP. We make our code and data publicly available.</abstract>
      <url hash="111d1f84">2025.gebnlp-1.39</url>
      <bibkey>derner-etal-2025-leveraging</bibkey>
      <doi>10.18653/v1/2025.gebnlp-1.39</doi>
    </paper>
  </volume>
</collection>
