<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.clib">
  <volume id="1" ingest-date="2024-10-11" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Sixth International Conference on Computational Linguistics in Bulgaria (CLIB 2024)</booktitle>
      <publisher>Department of Computational Linguistics, Institute for Bulgarian Language, Bulgarian Academy of Sciences</publisher>
      <address>Sofia, Bulgaria</address>
      <month>September</month>
      <year>2024</year>
      <url hash="ccd00621">2024.clib-1</url>
      <venue>clib</venue>
    </meta>
    <frontmatter>
      <pages>344</pages>
      <url hash="606e7bf6">2024.clib-1.0</url>
      <bibkey>clib-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>A Cross-model Study on Learning <fixed-case>R</fixed-case>omanian Parts of Speech with Transformer Models</title>
      <author><first>Radu</first><last>Ion</last></author>
      <author><first>Verginica</first><last>Barbu Mititelu</last></author>
      <author><first>Vasile</first><last>Păiş</last></author>
      <author><first>Elena</first><last>Irimia</last></author>
      <author><first>Valentin</first><last>Badea</last></author>
      <pages>6–13</pages>
      <abstract>This paper will attempt to determine experimentally if POS tagging of unseen words produces comparable performance, in terms of accuracy, as for words that were rarely seen in the training set (i.e. frequency less than 5), or more frequently seen (i.e. frequency greater than 10). To compare accuracies objectively, we will use the odds ratio statistic and its confidence interval testing to show that odds of being correct on unseen words are close to odds of being correct on rarely seen words. For the training of the POS taggers, we use different Romanian BERT models that are freely available on HuggingFace.</abstract>
      <url hash="d1021f75">2024.clib-1.1</url>
      <bibkey>ion-etal-2024-cross</bibkey>
    </paper>
    <paper id="2">
      <title>What do <fixed-case>BERT</fixed-case> Word Embeddings Learn about the <fixed-case>F</fixed-case>rench Language?</title>
      <author><first>Ekaterina</first><last>Goliakova</last></author>
      <author><first>David</first><last>Langlois</last></author>
      <pages>14–32</pages>
      <abstract>Pre-trained word embeddings (for example, BERT-like) have been successfully used in a variety of downstream tasks. However, do all embeddings, obtained from the models of the same architecture, encode information in the same way? Does the size of the model correlate to the quality of the information encoding? In this paper, we will attempt to dissect the dimensions of several BERT-like models that were trained on the French language to find where grammatical information (gender, plurality, part of speech) and semantic features might be encoded. In addition to this, we propose a framework for comparing the quality of encoding in different models.</abstract>
      <url hash="87d0ee41">2024.clib-1.2</url>
      <bibkey>goliakova-langlois-2024-bert</bibkey>
    </paper>
    <paper id="3">
      <title>Whisper–<fixed-case>TAD</fixed-case>: A General Model for Transcription, Alignment and Diarization of Speech</title>
      <author><first>Camille</first><last>Lavigne</last></author>
      <author><first>Alex</first><last>Stasica</last></author>
      <pages>33–38</pages>
      <abstract>Currently, there is a lack of a straightforward implementation of diarization-augmented speech transcription (DAST), ie. implementation of transcription, diarization and alignment to the audio within one model. These tasks typically require distinct models, necessitating to stack them together for complete processing. In this study, we advocate for leveraging the advanced capabilities of the Whisper models, which already excels in automatic transcription and partial alignment. Our approach involves fine-tuning the model’s parameters on both transcription and diarization tasks in a SOT-FIFO (Serialized Output Training-First In First Out) manner. This comprehensive framework facilitates the creation of orthographic transcriptions, identification of speakers, and precise alignment, thus enhancing the efficiency of audio processing workflows. While our work represents an initial step towards a unified transcription and diarization framework, the development of such a model demands substantial high-quality data augmentation and computational resources beyond our current scope. Consequently, our focus is narrowed to the English language. Despite these limitations, our method demonstrates promising performance in both transcription and diarization tasks. Comparative analysis between pre-trained models and fine-tuned TAD (Transcription, Alignment, Diarization) versions suggests that incorporating diarization into a Whisper model doesn’t compromise transcription accuracy. Our findings hint that deploying our TAD framework on the largest Whisper model could potentially yield state-of-the-art performance across all mentioned tasks.</abstract>
      <url hash="8f0aab30">2024.clib-1.3</url>
      <bibkey>lavigne-stasica-2024-whisper</bibkey>
    </paper>
    <paper id="4">
      <title>Contemporary <fixed-case>LLM</fixed-case>s and Literary Abridgement: An Analytical Inquiry</title>
      <author><first>Iglika</first><last>Nikolova-Stoupak</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <author><first>Eva</first><last>Schaeffer-Lacroix</last></author>
      <pages>39–57</pages>
      <abstract>Within the framework of this study, several contemporary Large Language Models (ChatGPT, Gemini Pro, Mistral-Instruct and BgGPT) are evaluated in relation to their ability to generate abridged versions of literary texts. The analysis is based on ’The Ugly Duckling’ by H. C. Andersen as translated into English, French and Bulgarian. The different scenarios of abridgement experimented with include zero-shot, one-shot, division into chunks and crosslingual (including chain-of-thought) abridgement. The resulting texts are evaluated both automatically and via human evaluation. The automatic analysis includes ROUGE and BERTScore as well as the ratios of a selection of readability-related textual features (e.g. number of words, type-to-token ratio) as pertaining to the original versus automatically abridged texts. Professionally composed abridged versions are regarded as gold standard. Following the automatic analysis, six selected best candidate texts per language are then evaluated by volunteers with university education in terms of textual characteristics of a more qualitative nature, such as coherence, consistency and aesthetic appeal.</abstract>
      <url hash="9428cc4c">2024.clib-1.4</url>
      <bibkey>nikolova-stoupak-etal-2024-contemporary</bibkey>
    </paper>
    <paper id="5">
      <title>Advancing Sentiment Analysis in <fixed-case>S</fixed-case>erbian Literature: A Zero and Few–Shot Learning Approach Using the Mistral Model</title>
      <author><first>Milica Ikonić</first><last>Nešić</last></author>
      <author><first>Saša</first><last>Petalinkar</last></author>
      <author><first>Mihailo</first><last>Škorić</last></author>
      <author><first>Ranka</first><last>Stanković</last></author>
      <author><first>Biljana</first><last>Rujević</last></author>
      <pages>58–70</pages>
      <abstract>This study presents the Sentiment Analysis of the Serbian old novels from the 1840-1920 period, employing the Mistral Large Language Model (LLM) to pioneer zero and few-shot learning techniques. The main approach innovates by devising research prompts that include guidance text for zero-shot classification and examples for few-shot learning, enabling the LLM to classify sentiments into positive, negative, or objective categories. This methodology aims to streamline sentiment analysis by limiting responses, thereby enhancing classification precision. Python, along with the Hugging Face Transformers and LangChain libraries, serves as our technological backbone, facilitating the creation and refinement of research prompts tailored for sentence-level sentiment analysis. The results of sentiment analysis in both scenarios, zero-shot and few-shot, have indicated that the zero-shot approach outperforms, achieving an accuracy of 68.2%.</abstract>
      <url hash="ddf0b36d">2024.clib-1.5</url>
      <bibkey>nesic-etal-2024-advancing</bibkey>
    </paper>
    <paper id="6">
      <title>Generating Phonetic Embeddings for <fixed-case>B</fixed-case>ulgarian Words with Neural Networks</title>
      <author><first>Lyuboslav</first><last>Karev</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <pages>71–79</pages>
      <abstract>Word embeddings can be considered the cornerstone of modern natural language processing. They are used in many NLP tasks and allow us to create models that can understand the meaning of words. Most word embeddings model the semantics of the words. In this paper, we create phoneme-based word embeddings, which model how a word sounds. This is accomplished by training a neural network that can automatically generate transcriptions of Bulgarian words. We used the Jaccard index and direct comparison metrics to measure the performance of neural networks. The models perform nearly perfectly with the task of generating transcriptions. The model’s word embeddings offer versatility across various applications, with its application in automatic paronym detection being particularly notable, as well as the task of detecting the language of origin of a Bulgarian word. The performance of this paronym detection is measured with the standard classifier metrics - accuracy, precision, recall, and F1.</abstract>
      <url hash="683dae07">2024.clib-1.6</url>
      <bibkey>karev-koychev-2024-generating</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies Treebank for Standard <fixed-case>A</fixed-case>lbanian: A New Approach</title>
      <author><first>Nelda</first><last>Kote</last></author>
      <author><first>Rozana</first><last>Rushiti</last></author>
      <author><first>Anila</first><last>Çepani</last></author>
      <author><first>Alba</first><last>Haveriku</last></author>
      <author><first>Evis</first><last>Trandafili</last></author>
      <author><first>Elinda Kajo</first><last>Meçe</last></author>
      <author><first>Elsa Skënderi</first><last>Rakipllari</last></author>
      <author><first>Lindita</first><last>Xhanari</last></author>
      <author><first>Albana</first><last>Deda</last></author>
      <pages>80–89</pages>
      <abstract>In this paper, we present a Universal Dependencies (UD) treebank for the Standard Albanian Language (SAL), annotated by expert linguistics supported by information technology professionals. The annotated treebank consists of 24,537 tokens (1,400 sentences) and includes annotation for syntactic dependencies, part-of-speech tags, morphological features, and lemmas. This treebank represents the largest UD treebank available for SAL. In order to overcome annotation challenges in SAL within the UD framework, we delicately balanced the preservation of the richness of SAL grammar while adapting the UD tagset and addressing unique language-specific features for a unified annotation. We discuss the criteria followed to select the sentences included in the treebank and address the most significant linguistic considerations when adapting the UD framework conform to the grammar of the SAL. Our efforts contribute to the advancement of linguistic analyses and Natural Language Processing (NLP) in the SAL. The treebank will be made available online under an open license so that to provide the possibility for further developments of NLP tools based on the Artificial Intelligence (AI) models for the Albanian language.</abstract>
      <url hash="b239ad86">2024.clib-1.7</url>
      <bibkey>kote-etal-2024-universal</bibkey>
    </paper>
    <paper id="8">
      <title>Function Multiword Expressions Annotated with Discourse Relations in the <fixed-case>R</fixed-case>omanian Reference Treebank</title>
      <author><first>Verginica</first><last>Barbu Mititelu</last></author>
      <author><first>Tudor</first><last>Voicu</last></author>
      <pages>90–97</pages>
      <abstract>For the Romanian Reference Treebank, a general language corpus, covering several genres and annotated according to the principles of Universal Dependencies, we present here the annotation of some function words, namely multiword conjunctions, with discourse relations from the Penn Discourse Treebank version 3.0 inventory of such relations. The annotation process was manual, with two annotators for each occurrence of the conjunctions. Lexical-semantic relations of the types synonymy, polysemy can be established between the senses of such conjunctions. The discourse relations are added to the CoNLL-U file in which the treebank is represented.</abstract>
      <url hash="65014829">2024.clib-1.8</url>
      <bibkey>barbu-mititelu-voicu-2024-function</bibkey>
    </paper>
    <paper id="9">
      <title>Dependency Parser for <fixed-case>B</fixed-case>ulgarian</title>
      <author><first>Atanas</first><last>Atanasov</last></author>
      <pages>98–105</pages>
      <abstract>This paper delves into the implementation of a Biaffine Attention Model, a sophisticated neural network architecture employed for dependency parsing tasks. Proposed by Dozat and Manning, this model is applied to Bulgarian language processing. The model’s training and evaluation are conducted using the Bulgarian Universal Dependencies dataset. The paper offers a comprehensive explanation of the model’s architecture and the data preparation process, aiming to demonstrate that for highly inflected languages, the inclusion of two additional input layers - lemmas and language-specific morphological information - is beneficial. The results of the experiments are subsequently presented and discussed. The paper concludes with a reflection on the model’s performance and suggestions for potential future work.</abstract>
      <url hash="9b82abd4">2024.clib-1.9</url>
      <bibkey>atanasov-2024-dependency</bibkey>
    </paper>
    <paper id="10">
      <title>Towards a <fixed-case>R</fixed-case>omanian Phrasal Academic Lexicon</title>
      <author><first>Madalina</first><last>Chitez</last></author>
      <author><first>Ana-Maria</first><last>Bucur</last></author>
      <author><first>Andreea</first><last>Dinca</last></author>
      <author><first>Roxana</first><last>Rogobete</last></author>
      <pages>106–112</pages>
      <abstract>The lack of NLP based research studies on academic writing in Romania results in an unbalanced development of automatic support tools in Romanian compared to other languages, such as English. For this study, we use Romanian subsets of two bilingual academic writing corpora: the ROGER corpus, consisting of university student papers, and the EXPRES corpus, composed of expert research articles. Working with the Romanian Academic Word List / RoAWL, we present two phrase extraction phases: (i) use Ro-AWL words as node words to extract collocations according to the thresholds of statistical measures and (ii) classify extracted phrases into general versus domain-specific multi-word units. We show how manual rhetorical function annotation of resulting phrases can be combined with automatic function detection. The comparison between academic phrases in ROGER and EXPRES validates the final phrase list. The Romanian phrasal academic lexicon (ROPAL), similar to the Oxford Phrasal Academic Lexicon (OPAL), is a written academic phrase lexicon for Romanian language made available for academic use and further research or applications.</abstract>
      <url hash="6eb8cb88">2024.clib-1.10</url>
      <bibkey>chitez-etal-2024-towards-romanian</bibkey>
    </paper>
    <paper id="11">
      <title>Classifying Multi–Word Expressions in the <fixed-case>L</fixed-case>atvian Monolingual Electronic Dictionary Tēzaurs.lv</title>
      <author><first>Laura</first><last>Rituma</last></author>
      <author><first>Gunta</first><last>Nešpore-Bērzkalne</last></author>
      <author><first>Agute</first><last>Klints</last></author>
      <author><first>Ilze</first><last>Lokmane</last></author>
      <author><first>Madara</first><last>Stāde</last></author>
      <author><first>Pēteris</first><last>Paikens</last></author>
      <pages>113–118</pages>
      <abstract>The electronic dictionary Tēzaurs.lv contains more than 400,000 entries from which 73,000 entries are multi-word expressions (MWEs). Over the past two years, there has been an ongoing division of these MWEs into subgroups (proper names, multi-word terms, taxa, phraseological units, collocations). The article describes the classification of MWEs, focusing on phraseological units (approximately 7,250 entries), as well as on borderline cases of phraseological unit types (phrasemes and idioms) and different MWE groups in general. The division of phraseological units depends on semantic divisibility and figurativeness. In a phraseme, at least one of the constituents retains its literal sense, whereas the meaning of an idiom is not dependent on the literal sense of any of its constituents. As a result, 65919 entries of MWE have been manually classified, and now this information of MWE type is available for the users of the electronic dictionary Tēzaurs.lv.</abstract>
      <url hash="95b30fd2">2024.clib-1.11</url>
      <bibkey>rituma-etal-2024-classifying</bibkey>
    </paper>
    <paper id="12">
      <title>Complex Word Identification for <fixed-case>I</fixed-case>talian Language: A Dictionary–based Approach</title>
      <author><first>Laura</first><last>Occhipinti</last></author>
      <pages>119–129</pages>
      <abstract>Assessing word complexity in Italian poses significant challenges, particularly due to the absence of a standardized dataset. This study introduces the first automatic model designed to identify word complexity for native Italian speakers. A dictionary of simple and complex words was constructed, and various configurations of linguistic features were explored to find the best statistical classifier based on Random Forest algorithm. Considering the probabilities of a word to belong to a class, a comparison between the models’ predictions and human assessments derived from a dataset annotated for complexity perception was made. Finally, the degree of accord between the model predictions and the human inter-annotator agreement was analyzed using Spearman correlation. Our findings indicate that a model incorporating both linguistic features and word embeddings performed better than other simpler models, also showing a value of correlation with the human judgements similar to the inter-annotator agreement. This study demonstrates the feasibility of an automatic system for detecting complexity in the Italian language with good performances and comparable effectiveness to humans in this subjective task.</abstract>
      <url hash="2a2b9b73">2024.clib-1.12</url>
      <bibkey>occhipinti-2024-complex</bibkey>
    </paper>
    <paper id="13">
      <title>Verbal Multiword Expressions in the <fixed-case>C</fixed-case>roatian Verb Lexicon</title>
      <author><first>Ivana</first><last>Brač</last></author>
      <author><first>Matea</first><last>Birtić</last></author>
      <pages>130–139</pages>
      <abstract>The paper examines the complexities of encoding verbal multiword expressions in the Croatian verb lexicon. The lexicon incorporates a verb’s description at the syntactic, morphological, and semantic levels. This study explores the treatment of reflexive verbs, light verb constructions, and verbal idioms across several Croatian and Slavic language resources to find the best solution for the verb lexicon. It addresses the following research questions: 1. How should reflexive verbs, i.e., verbs with the reflexive marker se, be treated? Should they be considered as separate lemmas, sublemmas of non-reflexive counterparts, or as one of their senses? 2. What syntactic label and semantic role should be assigned to a predicative noun in light verb constructions? 3. Should verbal idioms be included, and, if so, at which level of a description? Our conclusion is that all reflexive verbs should be treated as separate lemmas since they are distinct lexemes that have undergone semantic and syntactic change. To differentiate between a semantically full verb and a light verb, we have introduced the label LV and decided not to assign a semantic role to a predicative noun. By including verbal idioms and their translation into English, non-native users can benefit from the lexicon. The aim is to enhance the verb lexicon for the more effective description and recognition of verbal multiword expressions.</abstract>
      <url hash="10c88989">2024.clib-1.13</url>
      <bibkey>brac-birtic-2024-verbal</bibkey>
    </paper>
    <paper id="14">
      <title>Assessing Reading Literacy of <fixed-case>B</fixed-case>ulgarian Pupils with Finger–tracking</title>
      <author><first>Alessandro</first><last>Lento</last></author>
      <author><first>Andrea</first><last>Nadalini</last></author>
      <author><first>Marcello</first><last>Ferro</last></author>
      <author><first>Claudia</first><last>Marzi</last></author>
      <author><first>Vito</first><last>Pirrelli</last></author>
      <author><first>Tsvetana</first><last>Dimitrova</last></author>
      <author><first>Hristina</first><last>Kukova</last></author>
      <author><first>Valentina</first><last>Stefanova</last></author>
      <author><first>Maria</first><last>Todorova</last></author>
      <author><first>Svetla</first><last>Koeva</last></author>
      <pages>140–149</pages>
      <abstract>The paper reports on the first steps in developing a time-stamped multimodal dataset of reading data by Bulgarian children. Data are being collected, structured and analysed by means of ReadLet, an innovative infrastructure for multimodal language data collection that uses a tablet as a reader’s front-end. The overall goal of the project is to quantitatively analyse the reading skills of a sample of early Bulgarian readers collected over a two-year period, and compare them with the reading data of early readers of Italian, collected using the same protocol. We illustrate design issues of the experimental protocol, as well as the data acquisition process and the post-processing phase of data annotation/augmentation. To evaluate the potential and usefulness of the Bulgarian dataset for reading research, we present some preliminary statistical analyses of our recently collected data. They show robust convergence trends between Bulgarian and Italian early reading development stages.</abstract>
      <url hash="74b459c3">2024.clib-1.14</url>
      <bibkey>lento-etal-2024-assessing</bibkey>
    </paper>
    <paper id="15">
      <title>Educational Horizons: Mapping the Terrain of Artificial Intelligence Integration in <fixed-case>B</fixed-case>ulgarian Educational Settings</title>
      <author><first>Denitza</first><last>Kurshumova</last></author>
      <pages>150–156</pages>
      <abstract>The role of artificial intelligence in education (AIEd) has recently become a major topic of discussion and future planning. This article presents data from a large-scale survey involving 1463 Bulgarian educators in primary, secondary, and high schools. The results revealed that 70.30% of the teachers were familiar with or somewhat familiar with the existence of AI applications. Chatbots were the most popular among the surveyed teachers, with ChatGPT ranking as the most familiar. The teachers were almost equally split between those who reported use and those who declared nonuse of AI technology for instructional purposes. A significant association was found between the teachers’ familiarity with and use of AI technology and their age-related generational traits. The younger educators (up to 40 years of age) were associated with higher use of AI technology as a support tool for creating lesson plans, lesson content, tests, and exams. The outlined tendencies can be used to inform policy, professional development, and future research in the realm of AI-driven education.</abstract>
      <url hash="57329730">2024.clib-1.15</url>
      <bibkey>kurshumova-2024-educational</bibkey>
    </paper>
    <paper id="16">
      <title>Evidential Auxiliaries as Non–reliability Markers in <fixed-case>B</fixed-case>ulgarian Parliamentary Speech</title>
      <author><first>Ekaterina</first><last>Tarpomanova</last></author>
      <pages>157–165</pages>
      <abstract>In the evidentiality system of Bulgarian, there are three evidential auxiliaries that form complex verbal forms. The paper analyzes their potential to mark non-reliability in political discourse by using the ParlaMint-BG corpus of parliamentary debates. The method of the study includes detection, categorisation and context analysis of the evidentials formed with auxiliaries. The results prove that the evidential auxiliaries function as markers of non-reliability, especially in argumentative text type such as political discourse.</abstract>
      <url hash="345d9d4b">2024.clib-1.16</url>
      <bibkey>tarpomanova-2024-evidential</bibkey>
    </paper>
    <paper id="17">
      <title>Extended Context at the Introduction of Complex Vocabulary in Abridged Literary Texts</title>
      <author><first>Iglika</first><last>Nikolova-Stoupak</last></author>
      <author><first>Eva</first><last>Schaeffer-Lacroix</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>166–177</pages>
      <abstract>Psycholinguistics speaks of a fine-tuning process used by parents as they address children, in which complex vocabulary is introduced with additional context (Leung et al., 2021). This somewhat counterintuitive lengthening of text in order to aid one’s interlocutor in the process of language acquisition also comes in accord with Harris (1988)’s notion that for every complex sentence, there is an equivalent longer (non-contracted) yet simpler one that contains the same amount of information. Within the proposed work, a corpus of eight renowned literary works (e.g. Alice’s Adventures in Wonderland, The Adventures of Tom Sawyer, Les Misérables) in four distinct languages (English, French, Russian and Spanish) is gathered: both the original (or translated) versions and up to four abridged versions for various audiences (e.g. children of a defined age or foreign language learners of a defined level) are present. The contexts of the first appearance of complex words (as determined based on word frequency) in pairs of original and abridged works are compared, and the cases in which the abridged texts offer longer context are investigated. The discovered transformations are consequently classified into three separate categories: addition of vocabulary items from the same lexical field as the complex word, simplification of grammar and insertion of a definition. Context extensions are then statistically analysed as associated with different languages and reader audiences.</abstract>
      <url hash="5436ec95">2024.clib-1.17</url>
      <bibkey>nikolova-stoupak-etal-2024-extended</bibkey>
    </paper>
    <paper id="18">
      <title>Corpus–based Research into Derivational Morphology: A Comparative Study of <fixed-case>J</fixed-case>apanese and <fixed-case>E</fixed-case>nglish Verbalization</title>
      <author><first>Junya</first><last>Morita</last></author>
      <pages>178–186</pages>
      <abstract>As part of elucidating the syntax-morphology interaction, this study investigates where and how complex verbs are formed in Japanese and English. Focusing on the Japanese verb-forming suffix -ka-suru (e.g. toshi-o gendai-ka-suru ‘modernize city’), relevant verbs are extracted from a large-scale corpus and they receive an in-depth analysis from semantic, morphosyntactic, and functional viewpoints. The properties of -ka-suru and those of its English counterpart are then compared and contrasted. The result reveals three main points: (i) -ka-suru verbs are constantly created in syntactic settings to fulfill the functions of brevity and conceptualization, (ii) while denominal -ize derivatives have several submeanings such as ‘result,’ ‘ornative,’ and ‘agentive,’ -ka-suru equivalents retain the meaning ‘result,’ and (iii) -ka-suru can be combined with compound nouns, but -ize cannot. We will demonstrate that the above features originate in the underlying syntactic structure related to each suffix and their difference, thus supporting the thesis of syntactic word formation. (1) ji-kokumin-o moomai-ka-suru one’s-people-ACC ignorant-change-do ‘make one’s people ignorant’ (2) shinikaketa momiji-o bonsai-ka-suru dying maple-ACC bonsai-change-do ‘turn a dying maple into a bonsai’</abstract>
      <url hash="d7bcebdd">2024.clib-1.18</url>
      <bibkey>morita-2024-corpus</bibkey>
    </paper>
    <paper id="19">
      <title>The Verbal Category of Conditionality in <fixed-case>B</fixed-case>ulgarian and its <fixed-case>U</fixed-case>krainian Correspondences</title>
      <author><first>Ivan</first><last>Derzhanski</last></author>
      <author><first>Olena</first><last>Siruk</last></author>
      <pages>187–195</pages>
      <abstract>Modern Bulgarian shares a conditional mood with the other Slavic languages, but it also has developed a future-in-the-past tense which is structurally analogous to many Western European languages’ category traditionally called a conditional mood in their grammars. The distinction between these two forms is sometimes elusive and can be difficult for native speakers of Slavic languages who are learning Bulgarian. In this paper we consider the uses of the Bulgarian conditional mood and future-in-the-past tense in a parallel corpus of Bulgarian and Ukrainian text, examining the corresponding wording in Ukrainian, where the conditional mood is supplemented by modal verbs, and discuss the breadth of choices open to translators when working in each direction.</abstract>
      <url hash="d4a3ea0a">2024.clib-1.19</url>
      <bibkey>derzhanski-siruk-2024-verbal</bibkey>
    </paper>
    <paper id="20">
      <title>Lexical Richness of <fixed-case>F</fixed-case>rench and <fixed-case>Q</fixed-case>uebec Journalistic Texts</title>
      <author><first>Natalia</first><last>Dankova</last></author>
      <pages>196–200</pages>
      <abstract>This paper presents some results of a quantitative study that focuses on the variety and word frequency in texts from a comparative perspective. The study aims to analyze and compare French and Quebec journalistic texts on political and cultural topics written in French and recently published in major newspapers such as Le Monde, le Figaro, Le Devoir, etc. The statistical analysis concerns the number of different words in the text, the number of different adjectives, the number of different verbs (and also passive structures, participles and gerunds which contribute to syntactic and stylistic sophistication), and the number of hapaxes. French texts from France exhibit greater lexical richness and sophistication: they contain more adjectives, a greater variety of adjectives, as well as more participles and gerunds compared to French texts from Quebec. The originality of the study lies in the fact that it analyzes variation in French using a lexicometric approach.</abstract>
      <url hash="95c6a709">2024.clib-1.20</url>
      <bibkey>dankova-2024-lexical</bibkey>
    </paper>
    <paper id="21">
      <title>A Corpus of Liturgical Texts in <fixed-case>G</fixed-case>erman: Towards Multilevel Text Annotation</title>
      <author><first>Maria</first><last>Khokhlova</last></author>
      <author><first>Mikhail</first><last>Koryshev</last></author>
      <pages>201–205</pages>
      <abstract>The aim of the study is to create a “documented” literary and theological history of German Catholic hymnography. The paper focuses on the creation of a corpus of liturgical texts in German and describes the first stage of annotation dealing with the metatextual markup of Catholic hymns. The authors dwell in detail on the parameters of the multi-level classification of hymn texts they developed, which allows them to differentiate hymns on different grounds. The parameters include not only characteristics that represent hymns (the period and the source of their origin, rubrics, musical accompaniment), but also ones that are inherent for strophes. Based on the created markup, it is possible to trace general trends in texts divided according to certain meta-features. The developed scheme of annotation is given on the example of the hymnbook Gotteslob (1975). The results present statistics on different parameters used for hymn description.</abstract>
      <url hash="a14799f9">2024.clib-1.21</url>
      <bibkey>khokhlova-koryshev-2024-corpus</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>E</fixed-case>ur<fixed-case>L</fixed-case>ex<fixed-case>S</fixed-case>ummarization – A New Text Summarization Dataset on <fixed-case>EU</fixed-case> Legislation in 24 Languages with <fixed-case>GPT</fixed-case> Evaluation</title>
      <author><first>Valentin</first><last>Zmiycharov</last></author>
      <author><first>Todor</first><last>Tsonkov</last></author>
      <author><first>Ivan</first><last>Koychev</last></author>
      <pages>206–213</pages>
      <abstract>Legal documents are notorious for their length and complexity, making it challenging to extract crucial information efficiently. In this paper, we introduce a new dataset for legal text summarization, covering 24 languages. We not only present and analyze the dataset but also conduct experiments using various extractive techniques. We provide a comparison between these techniques and summaries generated by the state-of-the-art GPT models. The abstractive GPT approach outperforms the extractive TextRank approach in 8 languages, but produces slightly lower results in the remaining 16 languages. This research aims to advance the field of legal document summarization by addressing the need for accessible and comprehensive information retrieval from lengthy legal texts.</abstract>
      <url hash="6679d62b">2024.clib-1.22</url>
      <bibkey>zmiycharov-etal-2024-eurlexsummarization</bibkey>
    </paper>
    <paper id="23">
      <title>On a Hurtlex Resource for <fixed-case>B</fixed-case>ulgarian</title>
      <author><first>Petya</first><last>Osenova</last></author>
      <pages>214–219</pages>
      <abstract>The paper reports on the cleaning of the Hurtlex lexicon for Bulgarian as part of the multilingual Hurtlex resource. All the challenges during the cleaning process are presented, such as: deleting strings or lexica that are clear errors from the automatic translation, establishing criteria for keeping or discarding a lexeme based on its meaning and potential usages, contextualizing the lexeme with the meaning through an example, etc. In addition, the paper discusses the mapping of the offensive lexica to the BTB-Wordnet as well as the system that has been used.</abstract>
      <url hash="22a94939">2024.clib-1.23</url>
      <bibkey>osenova-2024-hurtlex</bibkey>
    </paper>
    <paper id="24">
      <title>Unified Annotation of the Stages of the <fixed-case>B</fixed-case>ulgarian Language. First Steps</title>
      <author><first>Fabio</first><last>Maion</last></author>
      <author><first>Tsvetana</first><last>Dimitrova</last></author>
      <author><first>Andrej</first><last>Bojadziev</last></author>
      <pages>220–226</pages>
      <abstract>The paper reports on an ongoing work on a proposal of guidelines for unified annotation of the stages in the development of the Bulgarian language from the Middle Ages to the early modern period. It discusses the criteria for the selection of texts and their representation, along with some results of the trial tagging with an existing tagger which was already trained on other texts.</abstract>
      <url hash="a53b582c">2024.clib-1.24</url>
      <bibkey>maion-etal-2024-unified</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>: Detection of <fixed-case>S</fixed-case>panish Terms Based on False <fixed-case>F</fixed-case>riends</title>
      <author><first>Amal</first><last>Haddad Haddad</last></author>
      <author><first>Damith</first><last>Premasiri</last></author>
      <pages>227–240</pages>
      <abstract>One of the common errors which translators commit when transferring terms from one lan- guage into another is erroneously coining terms which are based on a false friend mistake due to the similarity between lexical units forming part of terms. In this case-study, we use Chat- GPT to automatically detect terms in Spanish which may be coined based on a false friend relation. To carry out this study, we imple- mented two experiments with GPT and com- pared the results. In the first, we prompted GPT to produce a list of twenty terms in Span- ish extracted from the UN discourse, which are possibly based on false friend relation, and its English equivalents and analysed the veracity of the results. In the second experiment, we used an aligned corpus to further study the ca- pabilities of the Language Model on detecting false friends in English and Spanish Text. Some results were significant for future terminologi- cal studies.</abstract>
      <url hash="49a17dcd">2024.clib-1.25</url>
      <bibkey>haddad-haddad-premasiri-2024-chatgpt</bibkey>
    </paper>
    <paper id="26">
      <title>Deep Learning Framework for Identifying Future Market Opportunities from Textual User Reviews</title>
      <author><first>Jordan</first><last>Kralev</last></author>
      <pages>241–248</pages>
      <abstract>The paper develops an application of design gap theory for identification of future market segment growth and capitalization from a set of customer reviews for bought products from the market in a given past period. To build a consumer feature space, an encoded-decoder network with attention is trained over the textual reviews after they are pre-processed through tokenization and embedding layers. The encodings for product reviews are used to train a variational auto encoder network for representation of a product feature space. The sampling capabilities of this network are extended with a function to look for innovative designs with high consumer preferences, characterizing future opportunities in a given market segment. The framework is demonstrated for processing of Amazon reviews in consumer electronics segment.</abstract>
      <url hash="b41cd769">2024.clib-1.26</url>
      <bibkey>kralev-2024-deep</bibkey>
    </paper>
    <paper id="27">
      <title>Look Who’s Talking: The Most Frequently Used Words in the <fixed-case>B</fixed-case>ulgarian Parliament 1990-2024</title>
      <author><first>Ruslana</first><last>Margova</last></author>
      <author><first>Bastiaan</first><last>Bruinsma</last></author>
      <pages>249–256</pages>
      <abstract>In this study we identify the most frequently used words and some multi-word expressions in the Bulgarian Parliament. We do this by using the transcripts of all plenary sessions between 1990 and 2024 - 3,936 in total. This allows us both to study an interesting period known in the Bulgarian linguistic space as the years of “transition and democracy”, and to provide scholars of Bulgarian politics with a purposefully generated list of additional stop words that they can use for future analysis. Because our list of words was generated from the data, there is no preconceived theory, and because we include all interactions during all sessions, our analysis goes beyond traditional party lines. We provide details of how we selected, retrieved, and cleaned our data, and discuss our findings.</abstract>
      <url hash="2e704205">2024.clib-1.27</url>
      <bibkey>margova-bruinsma-2024-look</bibkey>
    </paper>
    <paper id="28">
      <title>Estimating Commonsense Knowledge from a Linguistic Analysis on Information Distribution</title>
      <author><first>Sabrina</first><last>Mennella</last></author>
      <author><first>Maria</first><last>Di Maro</last></author>
      <author><first>Martina</first><last>Di Bratto</last></author>
      <pages>257–263</pages>
      <abstract>Commonsense Knowledge (CSK) is defined as a complex and multifaceted structure, encompassing a wide range of knowledge and reasoning generally acquired through everyday experiences. As CSK is often implicit in communication, it poses a challenge for AI systems to simulate human-like interaction. This work aims to deepen the CSK information structure from a linguistic perspective, starting from its organisation in conversations. To achieve this goal, we developed a three-level analysis model to extract more insights about this knowledge, focusing our attention on the second level. In particular, we aimed to extract the distribution of explicit actions and their execution order in the communicative flow. We built an annotation scheme based on FrameNet and applied it to a dialogical corpus on the culinary domain. Preliminary results indicate that certain frames occur earlier in the dialogues, while others occur towards the process’s end. These findings contribute to the systematic nature of actions by establishing clear patterns and relationships between frames.</abstract>
      <url hash="e672d2e1">2024.clib-1.28</url>
      <bibkey>mennella-etal-2024-estimating</bibkey>
    </paper>
    <paper id="29">
      <title>Pondera: A Personalized <fixed-case>AI</fixed-case>–Driven Weight Loss Mobile Companion with Multidimensional Goal Fulfillment Analytics</title>
      <author><first>Georgi</first><last>Pashev</last></author>
      <author><first>Silvia</first><last>Gaftandzhieva</last></author>
      <pages>264–271</pages>
      <abstract>The global obesity epidemic is a significant challenge to public health, necessitating innovative and personalized solutions. This paper presents Pondera, an innovative mobile app revolutionizing weight management by integrating Artificial Intelligence (AI) and multidimensional goal fulfilment analytics. Pondera distinguishes itself by supplying a tailored approach to weight loss, combining individual user data, including dietary preferences, fitness levels, and specific weight loss objectives, with advanced AI algorithms to generate personalized weight loss plans. Future development directions include refining AI algorithms, enhancing user experience, and validating effectiveness through comprehensive studies, ensuring Pondera becomes a pivotal tool in achieving sustainable weight loss and health improvement.</abstract>
      <url hash="adc09cfa">2024.clib-1.29</url>
      <bibkey>pashev-gaftandzhieva-2024-pondera</bibkey>
    </paper>
    <paper id="30">
      <title>Mitigating Hallucinations in Large Language Models via Semantic Enrichment of Prompts: Insights from <fixed-case>B</fixed-case>io<fixed-case>BERT</fixed-case> and Ontological Integration</title>
      <author><first>Stanislav</first><last>Penkov</last></author>
      <pages>272–276</pages>
      <abstract>The advent of Large Language Models (LLMs) has been transformative for natural language processing, yet their tendency to produce “hallucinations”—outputs that are factually incorrect or entirely fabricated— remains a significant hurdle. This paper introduces a proactive methodology for reducing hallucinations by strategically enriching LLM prompts. This involves identifying key entities and contextual cues from varied domains and integrating this information into the LLM prompts to guide the model towards more accurate and relevant responses. Leveraging examples from BioBERT for biomedical entity recognition and ChEBI for chemical ontology, we illustrate a broader approach that encompasses semantic prompt enrichment as a versatile tool for enhancing LLM output accuracy. By examining the potential of semantic and ontological enrichment in diverse contexts, we aim to present a scalable strategy for improving the reliability of AI-generated content, thereby contributing to the ongoing efforts to refine LLMs for a wide range of applications.</abstract>
      <url hash="d3b79674">2024.clib-1.30</url>
      <bibkey>penkov-2024-mitigating</bibkey>
    </paper>
    <paper id="31">
      <title>Commercially Minor Languages and Localization</title>
      <author><first>Maria</first><last>Todorova</last></author>
      <pages>277–285</pages>
      <abstract>This paper offers a perspective of languages with a less significant volume of digital usership as minor in the context of globalization and localization. With this premise, the risks this status poses to the quality of localized texts, the substantiality of genre conventions, the public image of professional translators, and the users’ linguistic competence in these languages is explored. Furthermore, the common lack of established or clear conventions in the localization of digital products into commercially minor languages (and in the digital product genres) is highlighted as one of the factors amplifying these risks. These perspectives are contextualized with the Bulgarian language with examples of errors encountered in Bulgarian digital content localized from English and more specifically – errors and problems related to gender neutrality and register.</abstract>
      <url hash="6ab633cb">2024.clib-1.31</url>
      <bibkey>todorova-2024-commercially</bibkey>
    </paper>
    <paper id="32">
      <title>Semantic features in the automatic analysis of verbs of creation in <fixed-case>B</fixed-case>ulgarian and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Ivelina</first><last>Stoyanova</last></author>
      <pages>286–295</pages>
      <abstract>The paper focuses on the semantic class of verbs of creation as a subclass of dynamic verbs. The objective is to present the description of creation verbs in terms of their corresponding semantic frames and to outline the semantic features of the frame elements with a view to their automatic identification and analysis in text. The observations are performed on Bulgarian and English data with the aim to establish the language-independent and language-specific features in the semantic description of the analysed class of verbs.</abstract>
      <url hash="8d2bb6fa">2024.clib-1.32</url>
      <bibkey>stoyanova-2024-semantic</bibkey>
    </paper>
    <paper id="33">
      <title>A ‘Dipdive’ into Motion: Exploring Lexical Resources towards a Comprehensive Semantic and Syntactic Description</title>
      <author><first>Svetlozara</first><last>Leseva</last></author>
      <pages>296–308</pages>
      <abstract>In this paper I illustrate the semantic description of verbs provided in three semantic resources (FrameNet, VerbNet and VerbAtlas) in comparative terms with a view to identifying common and distinct components in their representation and obtaining a preliminary idea of the resources’ interoperability. To this end, I provide a comparison of a small sample of motion verbs aligned with semantic frames and classes in the three resources. I also describe the semantic annotation of Bulgarian motion verbs using the framework defined in the Berkeley FrameNet project and its enrichment with information from the other two resources, which has been enabled by the mapping between: (i) their major semantic units – FrameNet frames, VerbNet classes and VerbAtlas frames, and (ii) their ’building blocks’ – frame elements (FrameNet )and semantic roles (VerbNet, VerbAtlas).</abstract>
      <url hash="57aa1c61">2024.clib-1.33</url>
      <bibkey>leseva-2024-dipdive</bibkey>
    </paper>
    <paper id="34">
      <title>Multilingual Corpus of Illustrative Examples on Activity Predicates</title>
      <author><first>Ivelina</first><last>Stoyanova</last></author>
      <author><first>Hristina</first><last>Kukova</last></author>
      <author><first>Maria</first><last>Todorova</last></author>
      <author><first>Tsvetana</first><last>Dimitrova</last></author>
      <pages>309–318</pages>
      <abstract>The paper presents the ongoing process of compilation of a multilingual corpus of illustrative examples to supplement our work on the syntactic and semantic analysis of predicates representing activities in Bulgarian and other languages. The corpus aims to include over 1,000 illustrative examples on verbs from six semantic classes of predicates (verbs of motion, contact, consumption, creation, competition and bodily functions) which provide a basis for observations on the specificity of their realisation. The corpus of illustrative examples will be used for contrastive studies and further elaboration on the scope and behaviour of activity verbs in general, as well as its semantic subclasses.</abstract>
      <url hash="5dbd4dc7">2024.clib-1.34</url>
      <bibkey>stoyanova-etal-2024-multilingual</bibkey>
    </paper>
    <paper id="35">
      <title>Large Language Models in Linguistic Research: the Pilot and the Copilot</title>
      <author><first>Svetla</first><last>Koeva</last></author>
      <pages>319–328</pages>
      <abstract>In this paper, we present two experiments focussing on linguistic classification and annotation of examples, using zero-shot prompting. The aim is to show how large language models can confirm or reject the linguistic judgements of experts in order to increase the productivity of their work. In the first experiment, new lexical units evoking a particular FrameNet semantic frame are selected simultaneously with the annotation of examples with the core frame elements. The second experiment attempts to categorise verbs into the aspectual classes, assuming that only certain combinations of verbs belonging to different aspectual classes evoke a semantic frame. The linguistic theories underlying the two experiments, the development of the prompts and the results of the experiments are presented.</abstract>
      <url hash="6993e6ee">2024.clib-1.35</url>
      <bibkey>koeva-2024-large</bibkey>
    </paper>
  </volume>
</collection>
