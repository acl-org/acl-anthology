<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.wmt">
  <volume id="1" ingest-date="2022-01-11">
    <meta>
      <booktitle>Proceedings of the Sixth Conference on Machine Translation</booktitle>
      <editor><first>Loic</first><last>Barrault</last></editor>
      <editor><first>Ondrej</first><last>Bojar</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Rajen</first><last>Chatterjee</last></editor>
      <editor><first>Marta R.</first><last>Costa-jussa</last></editor>
      <editor><first>Christian</first><last>Federmann</last></editor>
      <editor><first>Mark</first><last>Fishel</last></editor>
      <editor><first>Alexander</first><last>Fraser</last></editor>
      <editor><first>Markus</first><last>Freitag</last></editor>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Roman</first><last>Grundkiewicz</last></editor>
      <editor><first>Paco</first><last>Guzman</last></editor>
      <editor><first>Barry</first><last>Haddow</last></editor>
      <editor><first>Matthias</first><last>Huck</last></editor>
      <editor><first>Antonio Jimeno</first><last>Yepes</last></editor>
      <editor><first>Philipp</first><last>Koehn</last></editor>
      <editor><first>Tom</first><last>Kocmi</last></editor>
      <editor><first>Andre</first><last>Martins</last></editor>
      <editor><first>Makoto</first><last>Morishita</last></editor>
      <editor><first>Christof</first><last>Monz</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>November</month>
      <year>2021</year>
      <url hash="74ed0994">2021.wmt-1</url>
      <venue>wmt</venue>
    </meta>
    <frontmatter>
      <url hash="bbbc26a2">2021.wmt-1.0</url>
      <bibkey>wmt-2021-machine</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Findings of the 2021 Conference on Machine Translation (<fixed-case>WMT</fixed-case>21)</title>
      <author><first>Farhad</first><last>Akhbardeh</last></author>
      <author><first>Arkady</first><last>Arkhangorodsky</last></author>
      <author><first>Magdalena</first><last>Biesialska</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Rajen</first><last>Chatterjee</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Marta R.</first><last>Costa-jussa</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Leonie</first><last>Harter</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <author><first>Christopher</first><last>Homan</last></author>
      <author><first>Matthias</first><last>Huck</last></author>
      <author><first>Kwabena</first><last>Amponsah-Kaakyire</last></author>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Kevin</first><last>Knight</last></author>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Nicholas</first><last>Lourie</last></author>
      <author><first>Christof</first><last>Monz</last></author>
      <author><first>Makoto</first><last>Morishita</last></author>
      <author><first>Masaaki</first><last>Nagata</last></author>
      <author><first>Ajay</first><last>Nagesh</last></author>
      <author><first>Toshiaki</first><last>Nakazawa</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Santanu</first><last>Pal</last></author>
      <author><first>Allahsera Auguste</first><last>Tapo</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <author><first>Valentin</first><last>Vydrin</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>1–88</pages>
      <abstract>This paper presents the results of the newstranslation task, the multilingual low-resourcetranslation for Indo-European languages, thetriangular translation task, and the automaticpost-editing task organised as part of the Con-ference on Machine Translation (WMT) 2021.In the news task, participants were asked tobuild machine translation systems for any of10 language pairs, to be evaluated on test setsconsisting mainly of news stories. The taskwas also opened up to additional test suites toprobe specific aspects of translation.</abstract>
      <url hash="f2acaffe">2021.wmt-1.1</url>
      <bibkey>akhbardeh-etal-2021-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccaligned">CCAligned</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dogc">DOGC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cawac">caWaC</pwcdataset>
    </paper>
    <paper id="2">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2021 Shared Task on Large-Scale Multilingual Machine Translation</title>
      <author><first>Guillaume</first><last>Wenzek</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <author><first>Sahir</first><last>Gomez</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Somya</first><last>Jain</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <pages>89–99</pages>
      <abstract>We present the results of the first task on Large-Scale Multilingual Machine Translation. The task consists on the many-to-many evaluation of a single model across a variety of source and target languages. This year, the task consisted on three different settings: (i) SMALL-TASK1 (Central/South-Eastern European Languages), (ii) the SMALL-TASK2 (South-East Asian Languages), and (iii) FULL-TASK (all 101 x 100 language pairs). All the tasks used the FLORES-101 dataset as the evaluation benchmark. To ensure the longevity of the dataset, the test sets were not publicly released and the models were evaluated in a controlled environment on Dynabench. There were a total of 10 participating teams for the tasks, with a total of 151 intermediate model submissions and 13 final models. This year’s result show a significant improvement over the known base-lines with +17.8 BLEU for SMALL-TASK2, +10.6 for FULL-TASK and +3.6 for SMALL-TASK1.</abstract>
      <url hash="23ef8cf5">2021.wmt-1.2</url>
      <bibkey>wenzek-etal-2021-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
    </paper>
    <paper id="3">
      <title><fixed-case>GTCOM</fixed-case> Neural Machine Translation Systems for <fixed-case>WMT</fixed-case>21</title>
      <author><first>Chao</first><last>Bei</last></author>
      <author><first>Hao</first><last>Zong</last></author>
      <pages>100–103</pages>
      <abstract>This paper describes the Global Tone Communication Co., Ltd.’s submission of the WMT21 shared news translation task. We participate in six directions: English to/from Hausa, Hindi to/from Bengali and Zulu to/from Xhosa. Our submitted systems are unconstrained and focus on multilingual translation odel, backtranslation and forward-translation. We also apply rules and language model to filter monolingual, parallel sentences and synthetic sentences.</abstract>
      <url hash="8ac14359">2021.wmt-1.3</url>
      <bibkey>bei-zong-2021-gtcom</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
    </paper>
    <paper id="4">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s <fixed-case>E</fixed-case>nglish-<fixed-case>G</fixed-case>erman and <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>ausa Submissions to the <fixed-case>WMT</fixed-case>21 News Translation Task</title>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <author><first>Jindřich</first><last>Helcl</last></author>
      <author><first>Ulrich</first><last>Germann</last></author>
      <author><first>Laurie</first><last>Burchell</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Antonio Valerio</first><last>Miceli Barone</last></author>
      <author><first>Jonas</first><last>Waldendorf</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>104–109</pages>
      <abstract>This paper presents the University of Edinburgh’s constrained submissions of English-German and English-Hausa systems to the WMT 2021 shared task on news translation. We build En-De systems in three stages: corpus filtering, back-translation, and fine-tuning. For En-Ha we use an iterative back-translation approach on top of pre-trained En-De models and investigate vocabulary embedding mapping.</abstract>
      <url hash="52042e1c">2021.wmt-1.4</url>
      <bibkey>chen-etal-2021-university</bibkey>
      <pwccode url="https://github.com/marian-nmt/marian" additional="false">marian-nmt/marian</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="5">
      <title>Tune in: The <fixed-case>AFRL</fixed-case> <fixed-case>WMT</fixed-case>21 News-Translation Systems</title>
      <author><first>Grant</first><last>Erdmann</last></author>
      <author><first>Jeremy</first><last>Gwinnup</last></author>
      <author><first>Tim</first><last>Anderson</last></author>
      <pages>110–116</pages>
      <abstract>This paper describes the Air Force Research Laboratory (AFRL) machine translation sys- tems and the improvements that were developed during the WMT21 evaluation campaign. This year, we explore various methods of adapting our baseline models from WMT20 and again measure improvements in performance on the Russian–English language pair.</abstract>
      <url hash="ac7ec2b4">2021.wmt-1.5</url>
      <bibkey>erdmann-etal-2021-tune</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="6">
      <title>The <fixed-case>TALP</fixed-case>-<fixed-case>UPC</fixed-case> Participation in <fixed-case>WMT</fixed-case>21 News Translation Task: an m<fixed-case>BART</fixed-case>-based <fixed-case>NMT</fixed-case> Approach</title>
      <author><first>Carlos</first><last>Escolano</last></author>
      <author><first>Ioannis</first><last>Tsiamas</last></author>
      <author><first>Christine</first><last>Basta</last></author>
      <author><first>Javier</first><last>Ferrando</last></author>
      <author><first>Marta R.</first><last>Costa-jussa</last></author>
      <author><first>José A. R.</first><last>Fonollosa</last></author>
      <pages>117–122</pages>
      <abstract>This paper describes the submission to the WMT 2021 news translation shared task by the UPC Machine Translation group. The goal of the task is to translate German to French (De-Fr) and French to German (Fr-De). Our submission focuses on fine-tuning a pre-trained model to take advantage of monolingual data. We fine-tune mBART50 using the filtered data, and additionally, we train a Transformer model on the same data from scratch. In the experiments, we show that fine-tuning mBART50 results in 31.69 BLEU for De-Fr and 23.63 BLEU for Fr-De, which increases 2.71 and 1.90 BLEU accordingly, as compared to the model we train from scratch. Our final submission is an ensemble of these two models, further increasing 0.3 BLEU for Fr-De.</abstract>
      <url hash="ec7e9bd5">2021.wmt-1.6</url>
      <bibkey>escolano-etal-2021-talp</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>CUNI</fixed-case> Systems in <fixed-case>WMT</fixed-case>21: Revisiting Backtranslation Techniques for <fixed-case>E</fixed-case>nglish-<fixed-case>C</fixed-case>zech <fixed-case>NMT</fixed-case></title>
      <author><first>Petr</first><last>Gebauer</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Vojtěch</first><last>Švandelík</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <pages>123–129</pages>
      <abstract>We describe our two NMT systems submitted to the WMT2021 shared task in English-Czech news translation: CUNI-DocTransformer (document-level CUBBITT) and CUNI-Marian-Baselines. We improve the former with a better sentence-segmentation pre-processing and a post-processing for fixing errors in numbers and units. We use the latter for experiments with various backtranslation techniques.</abstract>
      <url hash="fab99610">2021.wmt-1.7</url>
      <bibkey>gebauer-etal-2021-cuni</bibkey>
    </paper>
    <paper id="8">
      <title>Ensembling of Distilled Models from Multi-task Teachers for Constrained Resource Language Pairs</title>
      <author><first>Amr</first><last>Hendy</last></author>
      <author><first>Esraa A.</first><last>Gad</last></author>
      <author><first>Mohamed</first><last>Abdelghaffar</last></author>
      <author><first>Jailan S.</first><last>ElMosalami</last></author>
      <author><first>Mohamed</first><last>Afify</last></author>
      <author><first>Ahmed Y.</first><last>Tawfik</last></author>
      <author><first>Hany</first><last>Hassan Awadalla</last></author>
      <pages>130–135</pages>
      <abstract>This paper describes the Microsoft Egypt Development Center (EgDC) submission to the constrained track of WMT21 shared news translation task. We focus on the three relatively low resource language pairs Bengali ↔ Hindi, English ↔ Hausa and Xhosa ↔ Zulu. To overcome the limitation of relatively low parallel data we train a multilingual model using a multitask objective employing both parallel and monolingual data. In addition, we augment the data using back translation. We also train a bilingual model incorporating back translation and knowledge distillation then combine the two models using sequence-to-sequence mapping. We see around 70% relative gain in BLEU point for En ↔ Ha and around 25% relative improvements for Bn ↔ Hi and Xh ↔ Zu compared to bilingual baselines.</abstract>
      <url hash="f235080b">2021.wmt-1.8</url>
      <bibkey>hendy-etal-2021-ensembling</bibkey>
    </paper>
    <paper id="9">
      <title>Miðeind’s <fixed-case>WMT</fixed-case> 2021 Submission</title>
      <author><first>Haukur Barri</first><last>Símonarson</last></author>
      <author><first>Vésteinn</first><last>Snæbjarnarson</last></author>
      <author><first>Pétur Orri</first><last>Ragnarson</last></author>
      <author><first>Haukur</first><last>Jónsson</last></author>
      <author><first>Vilhjalmur</first><last>Thorsteinsson</last></author>
      <pages>136–139</pages>
      <abstract>We present Miðeind’s submission for the English→Icelandic and Icelandic→English subsets of the 2021 WMT news translation task. Transformer-base models are trained for translation on parallel data to generate backtranslations teratively. A pretrained mBART-25 model is then adapted for translation using parallel data as well as the last backtranslation iteration. This adapted pretrained model is then used to re-generate backtranslations, and the training of the adapted model is continued.</abstract>
      <url hash="1b5aa8e5">2021.wmt-1.9</url>
      <bibkey>simonarson-etal-2021-mideinds</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccmatrix">CCMatrix</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ipac">IPAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="10">
      <title>Allegro.eu Submission to <fixed-case>WMT</fixed-case>21 News Translation Task</title>
      <author><first>Mikołaj</first><last>Koszowski</last></author>
      <author><first>Karol</first><last>Grzegorczyk</last></author>
      <author><first>Tsimur</first><last>Hadeliya</last></author>
      <pages>140–143</pages>
      <abstract>We submitted two uni-directional models, one for English→Icelandic direction and other for Icelandic→English direction. Our news translation system is based on the transformer-big architecture, it makes use of corpora filtering, back-translation and forward translation applied to parallel and monolingual data alike</abstract>
      <url hash="2d5b1551">2021.wmt-1.10</url>
      <bibkey>koszowski-etal-2021-allegro</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>I</fixed-case>llinois <fixed-case>J</fixed-case>apanese <tex-math>\leftrightarrow</tex-math> <fixed-case>E</fixed-case>nglish <fixed-case>N</fixed-case>ews <fixed-case>T</fixed-case>ranslation for <fixed-case>WMT</fixed-case> 2021</title>
      <author><first>Giang</first><last>Le</last></author>
      <author><first>Shinka</first><last>Mori</last></author>
      <author><first>Lane</first><last>Schwartz</last></author>
      <pages>144–153</pages>
      <abstract>This system paper describes an end-to-end NMT pipeline for the Japanese <tex-math>\leftrightarrow</tex-math> English news translation task as submitted to WMT 2021, where we explore the efficacy of techniques such as tokenizing with language-independent and language-dependent tokenizers, normalizing by orthographic conversion, creating a politeness-and-formality-aware model by implementing a tagger, back-translation, model ensembling, and n-best reranking. We use parallel corpora provided by WMT 2021 organizers for training, and development and test data from WMT 2020 for evaluation of different experiment models. The preprocessed corpora are trained with a Transformer neural network model. We found that combining various techniques described herein, such as language-independent BPE tokenization, incorporating politeness and formality tags, model ensembling, n-best reranking, and back-translation produced the best translation models relative to other experiment systems.</abstract>
      <url hash="c97ab624">2021.wmt-1.11</url>
      <bibkey>le-etal-2021-illinois</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>M</fixed-case>i<fixed-case>SS</fixed-case>@<fixed-case>WMT</fixed-case>21: Contrastive Learning-reinforced Domain Adaptation in Neural Machine Translation</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>154–161</pages>
      <abstract>In this paper, we describe our MiSS system that participated in the WMT21 news translation task. We mainly participated in the evaluation of the three translation directions of English-Chinese and Japanese-English translation tasks. In the systems submitted, we primarily considered wider networks, deeper networks, relative positional encoding, and dynamic convolutional networks in terms of model structure, while in terms of training, we investigated contrastive learning-reinforced domain adaptation, self-supervised training, and optimization objective switching training methods. According to the final evaluation results, a deeper, wider, and stronger network can improve translation performance in general, yet our data domain adaption method can improve performance even more. In addition, we found that switching to the use of our proposed objective during the finetune phase using relatively small domain-related data can effectively improve the stability of the model’s convergence and achieve better optimal performance.</abstract>
      <url hash="20f578db">2021.wmt-1.12</url>
      <bibkey>li-etal-2021-miss-wmt21</bibkey>
    </paper>
    <paper id="13">
      <title>The Fujitsu <fixed-case>DMATH</fixed-case> Submissions for <fixed-case>WMT</fixed-case>21 News Translation and Biomedical Translation Tasks</title>
      <author><first>Ander</first><last>Martinez</last></author>
      <pages>162–166</pages>
      <abstract>This paper describes the Fujitsu DMATH systems used for WMT 2021 News Translation and Biomedical Translation tasks. We focused on low-resource pairs, using a simple system. We conducted experiments on English-Hausa, Xhosa-Zulu and English-Basque, and submitted the results for Xhosa→Zulu in the News Translation Task, and English→Basque in the Biomedical Translation Task, abstract and terminology translation subtasks. Our system combines BPE dropout, sub-subword features and back-translation with a Transformer (base) model, achieving good results on the evaluation sets.</abstract>
      <url hash="641d5825">2021.wmt-1.13</url>
      <bibkey>martinez-2021-fujitsu</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>A</fixed-case>dam <fixed-case>M</fixed-case>ickiewicz <fixed-case>U</fixed-case>niversity’s <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>ausa Submissions to the <fixed-case>WMT</fixed-case> 2021 News Translation Task</title>
      <author><first>Artur</first><last>Nowakowski</last></author>
      <author><first>Tomasz</first><last>Dwojak</last></author>
      <pages>167–171</pages>
      <abstract>This paper presents the Adam Mickiewicz University’s (AMU) submissions to the WMT 2021 News Translation Task. The submissions focus on the English↔Hausa translation directions, which is a low-resource translation scenario between distant languages. Our approach involves thorough data cleaning, transfer learning using a high-resource language pair, iterative training, and utilization of monolingual data via back-translation. We experiment with NMT and PB-SMT approaches alike, using the base Transformer architecture for all of the NMT models while utilizing PB-SMT systems as comparable baseline solutions.</abstract>
      <url hash="5ac0f362">2021.wmt-1.14</url>
      <bibkey>nowakowski-dwojak-2021-adam</bibkey>
    </paper>
    <paper id="15">
      <title>e<fixed-case>T</fixed-case>ranslation’s Submissions to the <fixed-case>WMT</fixed-case> 2021 News Translation Task</title>
      <author><first>Csaba</first><last>Oravecz</last></author>
      <author><first>Katina</first><last>Bontcheva</last></author>
      <author><first>David</first><last>Kolovratník</last></author>
      <author><first>Bhavani</first><last>Bhaskar</last></author>
      <author><first>Michael</first><last>Jellinghaus</last></author>
      <author><first>Andreas</first><last>Eisele</last></author>
      <pages>172–179</pages>
      <abstract>The paper describes the 3 NMT models submitted by the eTranslation team to the WMT 2021 news translation shared task. We developed systems in language pairs that are actively used in the European Commission’s eTranslation service. In the WMT news task, recent years have seen a steady increase in the need for computational resources to train deep and complex architectures to produce competitive systems. We took a different approach and explored alternative strategies focusing on data selection and filtering to improve the performance of baseline systems. In the domain constrained task for the French–German language pair our approach resulted in the best system by a significant margin in BLEU. For the other two systems (English–German and English-Czech) we tried to build competitive models using standard best practices.</abstract>
      <url hash="9070a114">2021.wmt-1.15</url>
      <bibkey>oravecz-etal-2021-etranslations</bibkey>
    </paper>
    <paper id="16">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s <fixed-case>B</fixed-case>engali-<fixed-case>H</fixed-case>indi Submissions to the <fixed-case>WMT</fixed-case>21 News Translation Task</title>
      <author><first>Proyag</first><last>Pal</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <author><first>Sukanta</first><last>Sen</last></author>
      <pages>180–186</pages>
      <abstract>We describe the University of Edinburgh’s Bengali<tex-math>\leftrightarrow</tex-math>Hindi constrained systems submitted to the WMT21 News Translation task. We submitted ensembles of Transformer models built with large-scale back-translation and fine-tuned on subsets of training data retrieved based on similarity to the target domain.</abstract>
      <url hash="eb3f4531">2021.wmt-1.16</url>
      <bibkey>pal-etal-2021-university</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccaligned">CCAligned</pwcdataset>
    </paper>
    <paper id="17">
      <title>The Volctrans <fixed-case>GLAT</fixed-case> System: Non-autoregressive Translation Meets <fixed-case>WMT</fixed-case>21</title>
      <author><first>Lihua</first><last>Qian</last></author>
      <author><first>Yi</first><last>Zhou</last></author>
      <author><first>Zaixiang</first><last>Zheng</last></author>
      <author><first>Yaoming</first><last>Zhu</last></author>
      <author><first>Zehui</first><last>Lin</last></author>
      <author><first>Jiangtao</first><last>Feng</last></author>
      <author><first>Shanbo</first><last>Cheng</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <pages>187–196</pages>
      <abstract>This paper describes the Volctrans’ submission to the WMT21 news translation shared task for German-&gt;English translation. We build a parallel (i.e., non-autoregressive) translation system using the Glancing Transformer, which enables fast and accurate parallel decoding in contrast to the currently prevailing autoregressive models. To the best of our knowledge, this is the first parallel translation system that can be scaled to such a practical scenario like WMT competition. More importantly, our parallel translation system achieves the best BLEU score (35.0) on German-&gt;English translation task, outperforming all strong autoregressive counterparts.</abstract>
      <url hash="3f1ae1ed">2021.wmt-1.17</url>
      <bibkey>qian-etal-2021-volctrans</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>NVIDIA</fixed-case> <fixed-case>N</fixed-case>e<fixed-case>M</fixed-case>o’s Neural Machine Translation Systems for <fixed-case>E</fixed-case>nglish-<fixed-case>G</fixed-case>erman and <fixed-case>E</fixed-case>nglish-<fixed-case>R</fixed-case>ussian News and Biomedical Tasks at <fixed-case>WMT</fixed-case>21</title>
      <author><first>Sandeep</first><last>Subramanian</last></author>
      <author><first>Oleksii</first><last>Hrinchuk</last></author>
      <author><first>Virginia</first><last>Adams</last></author>
      <author><first>Oleksii</first><last>Kuchaiev</last></author>
      <pages>197–204</pages>
      <abstract>This paper provides an overview of NVIDIA NeMo’s neural machine translation systems for the constrained data track of the WMT21 News and Biomedical Shared Translation Tasks. Our news task submissions for English-German (En-De) and English-Russian (En-Ru) are built on top of a baseline transformer-based sequence-to-sequence model (CITATION). Specifically, we use a combination of 1) checkpoint averaging 2) model scaling 3) data augmentation with backtranslation and knowledge distillation from right-to-left factorized models 4) finetuning on test sets from previous years 5) model ensembling 6) shallow fusion decoding with transformer language models and 7) noisy channel re-ranking. Additionally, our biomedical task submission for English <tex-math>\leftrightarrow</tex-math> Russian uses a biomedically biased vocabulary and is trained from scratch on news task data, medically relevant text curated from the news task dataset, and biomedical data provided by the shared task. Our news system achieves a sacreBLEU score of 39.5 on the WMT’20 En-De test set outperforming the best submission from last year’s task of 38.8. Our biomedical task Ru-En and En-Ru systems reach BLEU scores of 43.8 and 40.3 respectively on the WMT’20 Biomedical Task Test set, outperforming the previous year’s best submissions.</abstract>
      <url hash="5e1f14ed">2021.wmt-1.18</url>
      <bibkey>subramanian-etal-2021-nvidia</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>F</fixed-case>acebook <fixed-case>AI</fixed-case>’s <fixed-case>WMT</fixed-case>21 News Translation Task Submission</title>
      <author><first>Chau</first><last>Tran</last></author>
      <author><first>Shruti</first><last>Bhosale</last></author>
      <author><first>James</first><last>Cross</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Sergey</first><last>Edunov</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <pages>205–215</pages>
      <abstract>We describe Facebook’s multilingual model submission to the WMT2021 shared task on news translation. We participate in 14 language directions: English to and from Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese. To develop systems covering all these directions, we focus on multilingual models. We utilize data from all available sources — WMT, large-scale data mining, and in-domain backtranslation — to create high quality bilingual and multilingual baselines. Subsequently, we investigate strategies for scaling multilingual model size, such that one system has sufficient capacity for high quality representations of all eight languages. Our final submission is an ensemble of dense and sparse Mixture-of-Expert multilingual translation models, followed by finetuning on in-domain news data and noisy channel reranking. Compared to previous year’s winning submissions, our multilingual system improved the translation quality on all language directions, with an average improvement of 2.0 BLEU. In the WMT2021 task, our system ranks first in 10 directions based on automatic evaluation.</abstract>
      <url hash="172c5ef4">2021.wmt-1.19</url>
      <bibkey>tran-etal-2021-facebook</bibkey>
      <pwccode url="https://github.com/facebookresearch/fairseq/tree/main/examples/wmt21" additional="false">facebookresearch/fairseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ccaligned">CCAligned</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ccmatrix">CCMatrix</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="20">
      <title>Tencent Translation System for the <fixed-case>WMT</fixed-case>21 News Translation Task</title>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <author><first>Fangxu</first><last>Liu</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Xing</first><last>Wang</last></author>
      <author><first>Shuangzhi</first><last>Wu</last></author>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Wen</first><last>Zhang</last></author>
      <pages>216–224</pages>
      <abstract>This paper describes Tencent Translation systems for the WMT21 shared task. We participate in the news translation task on three language pairs: Chinese-English, English-Chinese and German-English. Our systems are built on various Transformer models with novel techniques adapted from our recent research work. First, we combine different data augmentation methods including back-translation, forward-translation and right-to-left training to enlarge the training data. We also apply language coverage bias, data rejuvenation and uncertainty-based sampling approaches to select content-relevant and high-quality data from large parallel and monolingual corpora. Expect for in-domain fine-tuning, we also propose a fine-grained “one model one domain” approach to model characteristics of different news genres at fine-tuning and decoding stages. Besides, we use greed-based ensemble algorithm and transductive ensemble method to further boost our systems. Based on our success in the last WMT, we continuously employed advanced techniques such as large batch training, data selection and data filtering. Finally, our constrained Chinese-English system achieves 33.4 case-sensitive BLEU score, which is the highest among all submissions. The German-English system is ranked at second place accordingly.</abstract>
      <url hash="be22d27e">2021.wmt-1.20</url>
      <bibkey>wang-etal-2021-tencent</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation in the <fixed-case>WMT</fixed-case> 2021 News Translation Shared Task</title>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <pages>225–231</pages>
      <abstract>This paper presents the submission of Huawei Translate Services Center (HW-TSC) to the WMT 2021 News Translation Shared Task. We participate in 7 language pairs, including Zh/En, De/En, Ja/En, Ha/En, Is/En, Hi/Bn, and Xh/Zu in both directions under the constrained condition. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. Several commonly used strategies are used to train our models, such as Back Translation, Forward Translation, Multilingual Translation, Ensemble Knowledge Distillation, etc. Our submission obtains competitive results in the final evaluation.</abstract>
      <url hash="8ca12125">2021.wmt-1.21</url>
      <bibkey>wei-etal-2021-hw</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>LISN</fixed-case> @ <fixed-case>WMT</fixed-case> 2021</title>
      <author><first>Jitao</first><last>Xu</last></author>
      <author><first>Minh Quang</first><last>Pham</last></author>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>232–242</pages>
      <abstract>This paper describes LISN’s submissions to two shared tasks at WMT’21. For the biomedical translation task, we have developed resource-heavy systems for the English-French language pair, using both out-of-domain and in-domain corpora. The target genre for this task (scientific abstracts) corresponds to texts that often have a standardized structure. Our systems attempt to take this structure into account using a hierarchical system of sentence-level tags. Translation systems were also prepared for the News task for the French-German language pair. The challenge was to perform unsupervised adaptation to the target domain (financial news). For this, we explored the potential of retrieval-based strategies, where sentences that are similar to test instances are used to prime the decoder.</abstract>
      <url hash="79de94ca">2021.wmt-1.22</url>
      <bibkey>xu-etal-2021-lisn</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>W</fixed-case>e<fixed-case>C</fixed-case>hat Neural Machine Translation Systems for <fixed-case>WMT</fixed-case>21</title>
      <author><first>Xianfeng</first><last>Zeng</last></author>
      <author><first>Yijin</first><last>Liu</last></author>
      <author><first>Ernan</first><last>Li</last></author>
      <author><first>Qiu</first><last>Ran</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>243–254</pages>
      <abstract>This paper introduces WeChat AI’s participation in WMT 2021 shared news translation task on English-&gt;Chinese, English-&gt;Japanese, Japanese-&gt;English and English-&gt;German. Our systems are based on the Transformer (Vaswani et al., 2017) with several novel and effective variants. In our experiments, we employ data filtering, large-scale synthetic data generation (i.e., back-translation, knowledge distillation, forward-translation, iterative in-domain knowledge transfer), advanced finetuning approaches, and boosted Self-BLEU based model ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3 case-sensitive BLEU scores on English-&gt;Chinese, English-&gt;Japanese, Japanese-&gt;English and English-&gt;German, respectively. The BLEU scores of English-&gt;Chinese, English-&gt;Japanese and Japanese-&gt;English are the highest among all submissions, and that of English-&gt;German is the highest among all constrained submissions.</abstract>
      <url hash="1536ae04">2021.wmt-1.23</url>
      <bibkey>zeng-etal-2021-wechat</bibkey>
    </paper>
    <paper id="24">
      <title>Small Model and In-Domain Data Are All You Need</title>
      <author><first>Hui</first><last>Zeng</last></author>
      <pages>255–259</pages>
      <abstract>I participated in the WMT shared news translation task and focus on one high resource language pair: English and Chinese (two directions, Chinese to English and English to Chinese). The submitted systems (ZengHuiMT) focus on data cleaning, data selection, back translation and model ensemble. The techniques I used for data filtering and selection include filtering by rules, language model and word alignment. I used a base translation model trained on initial corpus to obtain the target versions of the WMT21 test sets, then I used language models to find out the monolingual data that is most similar to the target version of test set, such monolingual data was then used to do back translation. On the test set, my best submitted systems achieve 35.9 and 32.2 BLEU for English to Chinese and Chinese to English directions respectively, which are quite high for a small model.</abstract>
      <url hash="5b06fb70">2021.wmt-1.24</url>
      <bibkey>zeng-2021-small</bibkey>
    </paper>
    <paper id="25">
      <title>The Mininglamp Machine Translation System for <fixed-case>WMT</fixed-case>21</title>
      <author><first>Shiyu</first><last>Zhao</last></author>
      <author><first>Xiaopu</first><last>Li</last></author>
      <author><first>Minghui</first><last>Wu</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <pages>260–264</pages>
      <abstract>This paper describes Mininglamp neural machine translation systems of the WMT2021 news translation tasks. We have participated in eight directions translation tasks for news text including Chinese to/from English, Hausa to/from English, German to/from English and French to/from German. Our fundamental system was based on Transformer architecture, with wider or smaller construction for different news translation tasks. We mainly utilized the method of back-translation, knowledge distillation and fine-tuning to boost single model, while the ensemble was used to combine single models. Our final submission has ranked first for the English to/from Hausa task.</abstract>
      <url hash="fc6fc9c3">2021.wmt-1.25</url>
      <bibkey>zhao-etal-2021-mininglamp</bibkey>
    </paper>
    <paper id="26">
      <title>The <fixed-case>N</fixed-case>iu<fixed-case>T</fixed-case>rans Machine Translation Systems for <fixed-case>WMT</fixed-case>21</title>
      <author><first>Shuhan</first><last>Zhou</last></author>
      <author><first>Tao</first><last>Zhou</last></author>
      <author><first>Binghao</first><last>Wei</last></author>
      <author><first>Yingfeng</first><last>Luo</last></author>
      <author><first>Yongyu</first><last>Mu</last></author>
      <author><first>Zefan</first><last>Zhou</last></author>
      <author><first>Chenglong</first><last>Wang</last></author>
      <author><first>Xuanjun</first><last>Zhou</last></author>
      <author><first>Chuanhao</first><last>Lv</last></author>
      <author><first>Yi</first><last>Jing</last></author>
      <author><first>Laohu</first><last>Wang</last></author>
      <author><first>Jingnan</first><last>Zhang</last></author>
      <author><first>Canan</first><last>Huang</last></author>
      <author><first>Zhongxiang</first><last>Yan</last></author>
      <author><first>Chi</first><last>Hu</last></author>
      <author><first>Bei</first><last>Li</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>265–272</pages>
      <abstract>This paper describes NiuTrans neural machine translation systems of the WMT 2021 news translation tasks. We made submissions to 9 language directions, including English2Chinese, Japanese, Russian, Icelandic and English2Hausa tasks. Our primary systems are built on several effective variants of Transformer, e.g., Transformer-DLCL, ODE-Transformer. We also utilize back-translation, knowledge distillation, post-ensemble, and iterative fine-tuning techniques to enhance the model performance further.</abstract>
      <url hash="b12202cf">2021.wmt-1.26</url>
      <bibkey>zhou-etal-2021-niutrans</bibkey>
    </paper>
    <paper id="27">
      <title>Improving Similar Language Translation With Transfer Learning</title>
      <author><first>Ife</first><last>Adebara</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>273–278</pages>
      <abstract>We investigate transfer learning based on pre-trained neural machine translation models to translate between (low-resource) similar languages. This work is part of our contribution to the WMT 2021 Similar Languages Translation Shared Task where we submitted models for different language pairs, including French-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our models for Catalan-Spanish (82.79 BLEU)and Portuguese-Spanish (87.11 BLEU) rank top 1 in the official shared task evaluation, and we are the only team to submit models for the French-Bambara pairs.</abstract>
      <url hash="9577e4df">2021.wmt-1.27</url>
      <bibkey>adebara-abdul-mageed-2021-improving</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>T</fixed-case>4<fixed-case>T</fixed-case> Solution: <fixed-case>WMT</fixed-case>21 Similar Language Task for the <fixed-case>S</fixed-case>panish-<fixed-case>C</fixed-case>atalan and <fixed-case>S</fixed-case>panish-<fixed-case>P</fixed-case>ortuguese Language Pair</title>
      <author><first>Miguel</first><last>Canals</last></author>
      <author><first>Marc</first><last>Raventós Tato</last></author>
      <pages>279–283</pages>
      <abstract>The main idea of this solution has been to focus on corpus cleaning and preparation and after that, use an out of box solution (OpenNMT) with its default published transformer model. To prepare the corpus, we have used set of standard tools (as Moses scripts or python packages), but also, among other python scripts, a python custom tokenizer with the ability to replace numbers for variables, solve the upper/lower case issue of the vocabulary and provide good segmentation for most of the punctuation. We also have started a line to clean corpus based on statistical probability estimation of source-target corpus, with unclear results. Also, we have run some tests with syllabical word segmentation, again with unclear results, so at the end, after word sentence tokenization we have used BPE SentencePiece for subword units to feed OpenNMT.</abstract>
      <url hash="a3fe0c4b">2021.wmt-1.28</url>
      <bibkey>canals-raventos-tato-2021-t4t</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="29">
      <title>Neural Machine Translation for <fixed-case>T</fixed-case>amil–<fixed-case>T</fixed-case>elugu Pair</title>
      <author><first>Sahinur Rahman</first><last>Laskar</last></author>
      <author><first>Bishwaraj</first><last>Paul</last></author>
      <author><first>Prottay Kumar</first><last>Adhikary</last></author>
      <author><first>Partha</first><last>Pakray</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>284–287</pages>
      <abstract>The neural machine translation approach has gained popularity in machine translation because of its context analysing ability and its handling of long-term dependency issues. We have participated in the WMT21 shared task of similar language translation on a Tamil-Telugu pair with the team name: CNLP-NITS. In this task, we utilized monolingual data via pre-train word embeddings in transformer model based neural machine translation to tackle the limitation of parallel corpus. Our model has achieved a bilingual evaluation understudy (BLEU) score of 4.05, rank-based intuitive bilingual evaluation score (RIBES) score of 24.80 and translation edit rate (TER) score of 97.24 for both Tamil-to-Telugu and Telugu-to-Tamil translations respectively.</abstract>
      <url hash="49727dd6">2021.wmt-1.29</url>
      <bibkey>laskar-etal-2021-neural</bibkey>
    </paper>
    <paper id="30">
      <title>Low Resource Similar Language Neural Machine Translation for <fixed-case>T</fixed-case>amil-<fixed-case>T</fixed-case>elugu</title>
      <author><first>Vandan</first><last>Mujadia</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>288–291</pages>
      <abstract>This paper describes the participation of team oneNLP (LTRC, IIIT-Hyderabad) for the WMT 2021 task, similar language translation. We experimented with transformer based Neural Machine Translation and explored the use of language similarity for Tamil-Telugu and Telugu-Tamil. We incorporated use of different subword configurations, script conversion and single model training for both directions as exploratory experiments.</abstract>
      <url hash="2c9400a1">2021.wmt-1.30</url>
      <bibkey>mujadia-sharma-2021-low</bibkey>
    </paper>
    <paper id="31">
      <title>Similar Language Translation for <fixed-case>C</fixed-case>atalan, <fixed-case>P</fixed-case>ortuguese and <fixed-case>S</fixed-case>panish Using <fixed-case>M</fixed-case>arian <fixed-case>NMT</fixed-case></title>
      <author><first>Reinhard</first><last>Rapp</last></author>
      <pages>292–298</pages>
      <abstract>This paper describes the SEBAMAT contribution to the 2021 WMT Similar Language Translation shared task. Using the Marian neural machine translation toolkit, translation systems based on Google’s transformer architecture were built in both directions of Catalan–Spanish and Portuguese–Spanish. The systems were trained in two contrastive parameter settings (different vocabulary sizes for byte pair encoding) using only the parallel but not the comparable corpora provided by the shared task organizers. According to their official evaluation results, the SEBAMAT system turned out to be competitive with rankings among the top teams and BLEU scores between 38 and 47 for the language pairs involving Portuguese and between 76 and 80 for the language pairs involving Catalan.</abstract>
      <url hash="8b4421a7">2021.wmt-1.31</url>
      <bibkey>rapp-2021-similar</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>NITK</fixed-case>-<fixed-case>U</fixed-case>o<fixed-case>H</fixed-case>: <fixed-case>T</fixed-case>amil-<fixed-case>T</fixed-case>elugu Machine Translation Systems for the <fixed-case>WMT</fixed-case>21 Similar Language Translation Task</title>
      <author><first>Richard</first><last>Saldanha</last></author>
      <author><first>Ananthanarayana</first><last>V. S</last></author>
      <author><first>Anand Kumar</first><last>M</last></author>
      <author><first>Parameswari</first><last>Krishnamurthy</last></author>
      <pages>299–303</pages>
      <abstract>In this work, two Neural Machine Translation (NMT) systems have been developed and evaluated as part of the bidirectional Tamil-Telugu similar languages translation subtask in WMT21. The OpenNMT-py toolkit has been used to create quick prototypes of the systems, following which models have been trained on the training datasets containing the parallel corpus and finally the models have been evaluated on the dev datasets provided as part of the task. Both the systems have been trained on a DGX station with 4 -V100 GPUs. The first NMT system in this work is a Transformer based 6 layer encoder-decoder model, trained for 100000 training steps, whose configuration is similar to the one provided by OpenNMT-py and this is used to create a model for bidirectional translation. The second NMT system contains two unidirectional translation models with the same configuration as the first system, with the addition of utilizing Byte Pair Encoding (BPE) for subword tokenization through the pre-trained MultiBPEmb model. Based on the dev dataset evaluation metrics for both the systems, the first system i.e. the vanilla Transformer model has been submitted as the Primary system. Since there were no improvements in the metrics during training of the second system with BPE, it has been submitted as a contrastive system.</abstract>
      <url hash="b06bd41e">2021.wmt-1.32</url>
      <bibkey>saldanha-etal-2021-nitk</bibkey>
      <video href="2021.wmt-1.32.mp4"/>
    </paper>
    <paper id="33">
      <title>A3-108 Machine Translation System for Similar Language Translation Shared Task 2021</title>
      <author><first>Saumitra</first><last>Yadav</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>304–306</pages>
      <abstract>In this paper, we describe our submissions for the Similar Language Translation Shared Task 2021. We built 3 systems in each direction for the Tamil ⇐⇒ Telugu language pair. This paper outlines experiments with various tokenization schemes to train statistical models. We also report the configuration of the submitted systems and results produced by them.</abstract>
      <url hash="a7cd9acd">2021.wmt-1.33</url>
      <bibkey>yadav-shrivastava-2021-a3-108</bibkey>
    </paper>
    <paper id="34">
      <title>Netmarble <fixed-case>AI</fixed-case> Center’s <fixed-case>WMT</fixed-case>21 Automatic Post-Editing Shared Task Submission</title>
      <author><first>Shinhyeok</first><last>Oh</last></author>
      <author><first>Sion</first><last>Jang</last></author>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Shounan</first><last>An</last></author>
      <author><first>Insoo</first><last>Oh</last></author>
      <pages>307–314</pages>
      <abstract>This paper describes Netmarble’s submission to WMT21 Automatic Post-Editing (APE) Shared Task for the English-German language pair. First, we propose a Curriculum Training Strategy in training stages. Facebook Fair’s WMT19 news translation model was chosen to engage the large and powerful pre-trained neural networks. Then, we post-train the translation model with different levels of data at each training stages. As the training stages go on, we make the system learn to solve multiple tasks by adding extra information at different training stages gradually. We also show a way to utilize the additional data in large volume for APE tasks. For further improvement, we apply Multi-Task Learning Strategy with the Dynamic Weight Average during the fine-tuning stage. To fine-tune the APE corpus with limited data, we add some related subtasks to learn a unified representation. Finally, for better performance, we leverage external translations as augmented machine translation (MT) during the post-training and fine-tuning. As experimental results show, our APE system significantly improves the translations of provided MT results by -2.848 and +3.74 on the development dataset in terms of TER and BLEU, respectively. It also demonstrates its effectiveness on the test dataset with higher quality than the development dataset.</abstract>
      <url hash="3f7ce237">2021.wmt-1.34</url>
      <bibkey>oh-etal-2021-netmarble</bibkey>
    </paper>
    <paper id="35">
      <title>Adapting Neural Machine Translation for Automatic Post-Editing</title>
      <author><first>Abhishek</first><last>Sharma</last></author>
      <author><first>Prabhakar</first><last>Gupta</last></author>
      <author><first>Anil</first><last>Nelakanti</last></author>
      <pages>315–319</pages>
      <abstract>Automatic post-editing (APE) models are usedto correct machine translation (MT) system outputs by learning from human post-editing patterns. We present the system used in our submission to the WMT’21 Automatic Post-Editing (APE) English-German (En-De) shared task. We leverage the state-of-the-art MT system (Ng et al., 2019) for this task. For further improvements, we adapt the MT model to the task domain by using WikiMatrix (Schwenket al., 2021) followed by fine-tuning with additional APE samples from previous editions of the shared task (WMT-16,17,18) and ensembling the models. Our systems beat the baseline on TER scores on the WMT’21 test set.</abstract>
      <url hash="47570d29">2021.wmt-1.35</url>
      <bibkey>sharma-etal-2021-adapting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="36">
      <title><fixed-case>ISTIC</fixed-case>’s Triangular Machine Translation System for <fixed-case>WMT</fixed-case>2021</title>
      <author><first>Hangcheng</first><last>Guo</last></author>
      <author><first>Wenbin</first><last>Liu</last></author>
      <author><first>Yanqing</first><last>He</last></author>
      <author><first>Tian</first><last>Lan</last></author>
      <author><first>Hongjiao</first><last>Xu</last></author>
      <author><first>Zhenfeng</first><last>Wu</last></author>
      <author><first>You</first><last>Pan</last></author>
      <pages>320–324</pages>
      <abstract>This paper describes the ISTIC’s submission to the Triangular Machine Translation Task of Russian-to-Chinese machine translation for WMT’ 2021. In order to fully utilize the provided corpora and promote the translation performance from Russian to Chinese, the pivot method is used in our system which pipelines the Russian-to-English translator and the English-to-Chinese translator to form a Russian-to-Chinese translator. Our system is based on the Transformer architecture and several effective strategies are adopted to improve the quality of translation, including corpus filtering, data pre-processing, system combination and model ensemble.</abstract>
      <url hash="e94c93f2">2021.wmt-1.36</url>
      <bibkey>guo-etal-2021-istics</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation in the <fixed-case>WMT</fixed-case> 2021 Triangular <fixed-case>MT</fixed-case> Shared Task</title>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <pages>325–330</pages>
      <abstract>This paper presents the submission of Huawei Translation Service Center (HW-TSC) to WMT 2021 Triangular MT Shared Task. We participate in the Russian-to-Chinese task under the constrained condition. We use Transformer architecture and obtain the best performance via a variant with larger parameter sizes. We perform detailed data pre-processing and filtering on the provided large-scale bilingual data. Several strategies are used to train our models, such as Multilingual Translation, Back Translation, Forward Translation, Data Denoising, Average Checkpoint, Ensemble, Fine-tuning, etc. Our system obtains 32.5 BLEU on the dev set and 27.7 BLEU on the test set, the highest score among all submissions.</abstract>
      <url hash="b229fd5f">2021.wmt-1.37</url>
      <bibkey>li-etal-2021-hw</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>DUTNLP</fixed-case> Machine Translation System for <fixed-case>WMT</fixed-case>21 Triangular Translation Task</title>
      <author><first>Huan</first><last>Liu</last></author>
      <author><first>Junpeng</first><last>Liu</last></author>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <pages>331–335</pages>
      <abstract>This paper describes DUT-NLP Lab’s submission to the WMT-21 triangular machine translation shared task. The participants are not allowed to use other data and the translation direction of this task is Russian-to-Chinese. In this task, we use the Transformer as our baseline model, and integrate several techniques to enhance the performance of the baseline, including data filtering, data selection, fine-tuning, and post-editing. Further, to make use of the English resources, such as Russian/English and Chinese/English parallel data, the relationship triangle is constructed by multilingual neural machine translation systems. As a result, our submission achieves a BLEU score of 21.9 in Russian-to-Chinese.</abstract>
      <url hash="6de7afd6">2021.wmt-1.38</url>
      <bibkey>liu-etal-2021-dutnlp</bibkey>
    </paper>
    <paper id="39">
      <title>Pivot Based Transfer Learning for Neural Machine Translation: <fixed-case>CFILT</fixed-case> <fixed-case>IITB</fixed-case> @ <fixed-case>WMT</fixed-case> 2021 Triangular <fixed-case>MT</fixed-case></title>
      <author><first>Shivam</first><last>Mhaskar</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>336–340</pages>
      <abstract>In this paper, we discuss the various techniques that we used to implement the Russian-Chinese machine translation system for the Triangular MT task at WMT 2021. Neural Machine translation systems based on transformer architecture have an encoder-decoder architecture, which are trained end-to-end and require a large amount of parallel corpus to produce good quality translations. This is the reason why neural machine translation systems are referred to as <i>data hungry</i>. Such a large amount of parallel corpus is majorly available for language pairs which include English and not for non-English language pairs. This is a major problem in building neural machine translation systems for non-English language pairs. We try to utilize the resources of the English language to improve the translation of non-English language pairs. We use the pivot language, that is English, to leverage transfer learning to improve the quality of Russian-Chinese translation. Compared to the baseline transformer-based neural machine translation system, we observe that the pivot language-based transfer learning technique gives a higher BLEU score.</abstract>
      <url hash="61816525">2021.wmt-1.39</url>
      <bibkey>mhaskar-bhattacharyya-2021-pivot</bibkey>
    </paper>
    <paper id="40">
      <title>Papago’s Submissions to the <fixed-case>WMT</fixed-case>21 Triangular Translation Task</title>
      <author><first>Jeonghyeok</first><last>Park</last></author>
      <author><first>Hyunjoong</first><last>Kim</last></author>
      <author><first>Hyunchang</first><last>Cho</last></author>
      <pages>341–346</pages>
      <abstract>This paper describes Naver Papago’s submission to the WMT21 shared triangular MT task to enhance the non-English MT system with tri-language parallel data. The provided parallel data are Russian-Chinese (direct), Russian-English (indirect), and English-Chinese (indirect) data. This task aims to improve the quality of the Russian-to-Chinese MT system by exploiting the direct and indirect parallel re- sources. The direct parallel data is noisy data crawled from the web. To alleviate the issue, we conduct extensive experiments to find effective data filtering methods. With the empirical knowledge that the performance of bilingual MT is better than multi-lingual MT and related experiment results, we approach this task as bilingual MT, where the two indirect data are transformed to direct data. In addition, we use the Transformer, a robust translation model, as our baseline and integrate several techniques, averaging checkpoints, model ensemble, and re-ranking. Our final system provides a 12.7 BLEU points improvement over a baseline system on the WMT21 triangular MT development set. In the official evalua- tion of the test set, ours is ranked 2nd in terms of BLEU scores.</abstract>
      <url hash="e4b2b699">2021.wmt-1.40</url>
      <bibkey>park-etal-2021-papagos</bibkey>
    </paper>
    <paper id="41">
      <title>Machine Translation of Low-Resource <fixed-case>I</fixed-case>ndo-<fixed-case>E</fixed-case>uropean Languages</title>
      <author><first>Wei-Rui</first><last>Chen</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <pages>347–353</pages>
      <abstract>In this work, we investigate methods for the challenging task of translating between low- resource language pairs that exhibit some level of similarity. In particular, we consider the utility of transfer learning for translating between several Indo-European low-resource languages from the Germanic and Romance language families. In particular, we build two main classes of transfer-based systems to study how relatedness can benefit the translation performance. The primary system fine-tunes a model pre-trained on a related language pair and the contrastive system fine-tunes one pre-trained on an unrelated language pair. Our experiments show that although relatedness is not necessary for transfer learning to work, it does benefit model performance.</abstract>
      <url hash="6e51398a">2021.wmt-1.41</url>
      <bibkey>chen-abdul-mageed-2021-machine</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>CUNI</fixed-case> systems for <fixed-case>WMT</fixed-case>21: Multilingual Low-Resource Translation for <fixed-case>I</fixed-case>ndo-<fixed-case>E</fixed-case>uropean Languages Shared Task</title>
      <author><first>Josef</first><last>Jon</last></author>
      <author><first>Michal</first><last>Novák</last></author>
      <author><first>João Paulo</first><last>Aires</last></author>
      <author><first>Dusan</first><last>Varis</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>354–361</pages>
      <abstract>This paper describes Charles University sub-mission for Terminology translation shared task at WMT21. The objective of this task is to design a system which translates certain terms based on a provided terminology database, while preserving high overall translation quality. We competed in English-French language pair. Our approach is based on providing the desired translations alongside the input sentence and training the model to use these provided terms. We lemmatize the terms both during the training and inference, to allow the model to learn how to produce correct surface forms of the words, when they differ from the forms provided in the terminology database.</abstract>
      <url hash="aea155c4">2021.wmt-1.42</url>
      <bibkey>jon-etal-2021-cuni</bibkey>
    </paper>
    <paper id="43">
      <title>Transfer Learning with Shallow Decoders: <fixed-case>BSC</fixed-case> at <fixed-case>WMT</fixed-case>2021’s Multilingual Low-Resource Translation for <fixed-case>I</fixed-case>ndo-<fixed-case>E</fixed-case>uropean Languages Shared Task</title>
      <author><first>Ksenia</first><last>Kharitonova</last></author>
      <author><first>Ona</first><last>de Gibert Bonet</last></author>
      <author><first>Jordi</first><last>Armengol-Estapé</last></author>
      <author><first>Mar</first><last>Rodriguez i Alvarez</last></author>
      <author><first>Maite</first><last>Melero</last></author>
      <pages>362–367</pages>
      <abstract>This paper describes the participation of the BSC team in the WMT2021’s Multilingual Low-Resource Translation for Indo-European Languages Shared Task. The system aims to solve the Subtask 2: Wikipedia cultural heritage articles, which involves translation in four Romance languages: Catalan, Italian, Occitan and Romanian. The submitted system is a multilingual semi-supervised machine translation model. It is based on a pre-trained language model, namely XLM-RoBERTa, that is later fine-tuned with parallel data obtained mostly from OPUS. Unlike other works, we only use XLM to initialize the encoder and randomly initialize a shallow decoder. The reported results are robust and perform well for all tested languages.</abstract>
      <url hash="8af47569">2021.wmt-1.43</url>
      <bibkey>kharitonova-etal-2021-transfer</bibkey>
      <pwccode url="https://github.com/temu-bsc/wmt2021-indoeuropean" additional="false">temu-bsc/wmt2021-indoeuropean</pwccode>
    </paper>
    <paper id="44">
      <title><fixed-case>E</fixed-case>din<fixed-case>S</fixed-case>aar@<fixed-case>WMT</fixed-case>21: <fixed-case>N</fixed-case>orth-<fixed-case>G</fixed-case>ermanic Low-Resource Multilingual <fixed-case>NMT</fixed-case></title>
      <author><first>Svetlana</first><last>Tchistiakova</last></author>
      <author><first>Jesujoba</first><last>Alabi</last></author>
      <author><first>Koel</first><last>Dutta Chowdhury</last></author>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>Dana</first><last>Ruiter</last></author>
      <pages>368–375</pages>
      <abstract>We describe the EdinSaar submission to the shared task of Multilingual Low-Resource Translation for North Germanic Languages at the Sixth Conference on Machine Translation (WMT2021). We submit multilingual translation models for translations to/from Icelandic (is), Norwegian-Bokmal (nb), and Swedish (sv). We employ various experimental approaches, including multilingual pre-training, back-translation, fine-tuning, and ensembling. In most translation directions, our models outperform other submitted systems.</abstract>
      <url hash="c53a688e">2021.wmt-1.44</url>
      <bibkey>tchistiakova-etal-2021-edinsaar</bibkey>
    </paper>
    <paper id="45">
      <title><fixed-case>T</fixed-case>en<fixed-case>T</fixed-case>rans Multilingual Low-Resource Translation System for <fixed-case>WMT</fixed-case>21 <fixed-case>I</fixed-case>ndo-<fixed-case>E</fixed-case>uropean Languages Task</title>
      <author><first>Han</first><last>Yang</last></author>
      <author><first>Bojie</first><last>Hu</last></author>
      <author><first>Wanying</first><last>Xie</last></author>
      <author><first>Ambyera</first><last>Han</last></author>
      <author><first>Pan</first><last>Liu</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Qi</first><last>Ju</last></author>
      <pages>376–382</pages>
      <abstract>This paper describes TenTrans’ submission to WMT21 Multilingual Low-Resource Translation shared task for the Romance language pairs. This task focuses on improving translation quality from Catalan to Occitan, Romanian and Italian, with the assistance of related high-resource languages. We mainly utilize back-translation, pivot-based methods, multilingual models, pre-trained model fine-tuning, and in-domain knowledge transfer to improve the translation quality. On the test set, our best-submitted system achieves an average of 43.45 case-sensitive BLEU scores across all low-resource pairs. Our data, code, and pre-trained models used in this work are available in TenTrans evaluation examples.</abstract>
      <url hash="149c87a7">2021.wmt-1.45</url>
      <bibkey>yang-etal-2021-tentrans</bibkey>
    </paper>
    <paper id="46">
      <title>The <fixed-case>U</fixed-case>niversity of <fixed-case>M</fixed-case>aryland, College Park Submission to Large-Scale Multilingual Shared Task at <fixed-case>WMT</fixed-case> 2021</title>
      <author><first>Saptarashmi</first><last>Bandyopadhyay</last></author>
      <author><first>Tasnim</first><last>Kabir</last></author>
      <author><first>Zizhen</first><last>Lian</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>383–386</pages>
      <abstract>This paper describes the system submitted to Large-Scale Multilingual Shared Task (Small Task #2) at WMT 2021. It is based on the massively multilingual open-source model FLORES101_MM100 model, with selective fine-tuning.Our best-performing system reported a 15.72 average BLEU score for the task.</abstract>
      <url hash="29e9fc0d">2021.wmt-1.46</url>
      <bibkey>bandyopadhyay-etal-2021-university</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ccaligned">CCAligned</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
    </paper>
    <paper id="47">
      <title>To Optimize, or Not to Optimize, That Is the Question: <fixed-case>T</fixed-case>el<fixed-case>U</fixed-case>-<fixed-case>KU</fixed-case> Models for <fixed-case>WMT</fixed-case>21 Large-Scale Multilingual Machine Translation</title>
      <author><first>Sari Dewi</first><last>Budiwati</last></author>
      <author><first>Tirana</first><last>Fatyanosa</last></author>
      <author><first>Mahendra</first><last>Data</last></author>
      <author><first>Dedy Rahman</first><last>Wijaya</last></author>
      <author><first>Patrick Adolf</first><last>Telnoni</last></author>
      <author><first>Arie Ardiyanti</first><last>Suryani</last></author>
      <author><first>Agus</first><last>Pratondo</last></author>
      <author><first>Masayoshi</first><last>Aritsugi</last></author>
      <pages>387–397</pages>
      <abstract>We describe TelU-KU models of large-scale multilingual machine translation for five Southeast Asian languages: Javanese, Indonesian, Malay, Tagalog, Tamil, and English. We explore a variation of hyperparameters of flores101_mm100_175M model using random search with 10% of datasets to improve BLEU scores of all thirty language pairs. We submitted two models, TelU-KU-175M and TelU-KU- 175M_HPO, with average BLEU scores of 12.46 and 13.19, respectively. Our models show improvement in most language pairs after optimizing the hyperparameters. We also identified three language pairs that obtained a BLEU score of more than 15 while using less than 70 sentences of the training dataset: Indonesian-Tagalog, Tagalog-Indonesian, and Malay-Tagalog.</abstract>
      <url hash="21f1889c">2021.wmt-1.47</url>
      <bibkey>budiwati-etal-2021-optimize</bibkey>
    </paper>
    <paper id="48">
      <title><fixed-case>MMTA</fixed-case>frica: Multilingual Machine Translation for <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Chris Chinenye</first><last>Emezue</last></author>
      <author><first>Bonaventure F. P.</first><last>Dossou</last></author>
      <pages>398–411</pages>
      <abstract>In this paper, we focus on the task of multilingual machine translation for African languages and describe our contribution in the 2021 WMT Shared Task: Large-Scale Multilingual Machine Translation. We introduce MMTAfrica, the first many-to-many multilingual translation system for six African languages: Fon (fon), Igbo (ibo), Kinyarwanda (kin), Swahili/Kiswahili (swa), Xhosa (xho), and Yoruba (yor) and two non-African languages: English (eng) and French (fra). For multilingual translation concerning African languages, we introduce a novel backtranslation and reconstruction objective, BT&amp;REC, inspired by the random online back translation and T5 modelling framework respectively, to effectively leverage monolingual data. Additionally, we report improvements from MMTAfrica over the FLORES 101 benchmarks (spBLEU gains ranging from +0.58 in Swahili to French to +19.46 in French to Xhosa).</abstract>
      <url hash="eeeccac0">2021.wmt-1.48</url>
      <bibkey>emezue-dossou-2021-mmtafrica</bibkey>
      <pwccode url="https://github.com/edaiofficial/mmtafrica" additional="false">edaiofficial/mmtafrica</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
    </paper>
    <paper id="49">
      <title>The <fixed-case>LMU</fixed-case> <fixed-case>M</fixed-case>unich System for the <fixed-case>WMT</fixed-case> 2021 Large-Scale Multilingual Machine Translation Shared Task</title>
      <author><first>Wen</first><last>Lai</last></author>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>412–417</pages>
      <abstract>This paper describes the submission of LMU Munich to the WMT 2021 multilingual machine translation task for small track #1, which studies translation between 6 languages (Croatian, Hungarian, Estonian, Serbian, Macedonian, English) in 30 directions. We investigate the extent to which bilingual translation systems can influence multilingual translation systems. More specifically, we trained 30 bilingual translation systems, covering all language pairs, and used data augmentation technologies such as back-translation and knowledge distillation to improve the multilingual translation systems. Our best translation system scores 5 to 6 BLEU higher than a strong baseline system provided by the organizers. As seen in the dynalab leaderboard, our submission is the only fully constrained submission that uses only the corpus provided by the organizers and does not use any pre-trained models.</abstract>
      <url hash="277cb28b">2021.wmt-1.49</url>
      <bibkey>lai-etal-2021-lmu</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
    </paper>
    <paper id="50">
      <title>Back-translation for Large-Scale Multilingual Machine Translation</title>
      <author><first>Baohao</first><last>Liao</last></author>
      <author><first>Shahram</first><last>Khadivi</last></author>
      <author><first>Sanjika</first><last>Hewavitharana</last></author>
      <pages>418–424</pages>
      <abstract>This paper illustrates our approach to the shared task on large-scale multilingual machine translation in the sixth conference on machine translation (WMT-21). In this work, we aim to build a single multilingual translation system with a hypothesis that a universal cross-language representation leads to better multilingual translation performance. We extend the exploration of different back-translation methods from bilingual translation to multilingual translation. Better performance is obtained by the constrained sampling method, which is different from the finding of the bilingual translation. Besides, we also explore the effect of vocabularies and the amount of synthetic data. Surprisingly, the smaller size of vocabularies perform better, and the extensive monolingual English data offers a modest improvement. We submitted to both the small tasks and achieve the second place.</abstract>
      <url hash="56782bf4">2021.wmt-1.50</url>
      <bibkey>liao-etal-2021-back</bibkey>
      <pwccode url="https://github.com/baohaoliao/multiback" additional="false">baohaoliao/multiback</pwccode>
    </paper>
    <paper id="51">
      <title>Maastricht University’s Large-Scale Multilingual Machine Translation System for <fixed-case>WMT</fixed-case> 2021</title>
      <author><first>Danni</first><last>Liu</last></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>425–430</pages>
      <abstract>We present our development of the multilingual machine translation system for the large-scale multilingual machine translation task at WMT 2021. Starting form the provided baseline system, we investigated several techniques to improve the translation quality on the target subset of languages. We were able to significantly improve the translation quality by adapting the system towards the target subset of languages and by generating synthetic data using the initial model. Techniques successfully applied in zero-shot multilingual machine translation (e.g. similarity regularizer) only had a minor effect on the final translation performance.</abstract>
      <url hash="b71eaeda">2021.wmt-1.51</url>
      <bibkey>liu-niehues-2021-maastricht-universitys</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
    </paper>
    <paper id="52">
      <title>Data Processing Matters: <fixed-case>SRPH</fixed-case>-Konvergen <fixed-case>AI</fixed-case>’s Machine Translation System for <fixed-case>WMT</fixed-case>’21</title>
      <author><first>Lintang</first><last>Sutawika</last></author>
      <author><first>Jan Christian Blaise</first><last>Cruz</last></author>
      <pages>431–438</pages>
      <abstract>In this paper, we describe the submission of the joint Samsung Research Philippines-Konvergen AI team for the WMT’21 Large Scale Multilingual Translation Task - Small Track 2. We submit a standard Seq2Seq Transformer model to the shared task without any training or architecture tricks, relying mainly on the strength of our data preprocessing techniques to boost performance. Our final submission model scored 22.92 average BLEU on the FLORES-101 devtest set, and scored 22.97 average BLEU on the contest’s hidden test set, ranking us sixth overall. Despite using only a standard Transformer, our model ranked first in Indonesian to Javanese, showing that data preprocessing matters equally, if not more, than cutting edge model architectures and training techniques.</abstract>
      <url hash="dba24a10">2021.wmt-1.52</url>
      <bibkey>sutawika-cruz-2021-data</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
    </paper>
    <paper id="53">
      <title><fixed-case>T</fixed-case>en<fixed-case>T</fixed-case>rans Large-Scale Multilingual Machine Translation System for <fixed-case>WMT</fixed-case>21</title>
      <author><first>Wanying</first><last>Xie</last></author>
      <author><first>Bojie</first><last>Hu</last></author>
      <author><first>Han</first><last>Yang</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Qi</first><last>Ju</last></author>
      <pages>439–445</pages>
      <abstract>This paper describes TenTrans large-scale multilingual machine translation system for WMT 2021. We participate in the Small Track 2 in five South East Asian languages, thirty directions: Javanese, Indonesian, Malay, Tagalog, Tamil, English. We mainly utilized forward/back-translation, in-domain data selection, knowledge distillation, and gradual fine-tuning from the pre-trained model FLORES-101. We find that forward/back-translation significantly improves the translation results, data selection and gradual fine-tuning are particularly effective during adapting domain, while knowledge distillation brings slight performance improvement. Also, model averaging is used to further improve the translation performance based on these systems. Our final system achieves an average BLEU score of 28.89 across thirty directions on the test set.</abstract>
      <url hash="803348ac">2021.wmt-1.53</url>
      <bibkey>xie-etal-2021-tentrans</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
    </paper>
    <paper id="54">
      <title>Multilingual Machine Translation Systems from <fixed-case>M</fixed-case>icrosoft for <fixed-case>WMT</fixed-case>21 Shared Task</title>
      <author><first>Jian</first><last>Yang</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Haoyang</first><last>Huang</last></author>
      <author><first>Dongdong</first><last>Zhang</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Alexandre</first><last>Muzio</last></author>
      <author><first>Saksham</first><last>Singhal</last></author>
      <author><first>Hany</first><last>Hassan</last></author>
      <author><first>Xia</first><last>Song</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>446–455</pages>
      <abstract>This report describes Microsoft’s machine translation systems for the WMT21 shared task on large-scale multilingual machine translation. We participated in all three evaluation tracks including Large Track and two Small Tracks where the former one is unconstrained and the latter two are fully constrained. Our model submissions to the shared task were initialized with DeltaLM, a generic pre-trained multilingual encoder-decoder model, and fine-tuned correspondingly with the vast collected parallel data and allowed data sources according to track settings, together with applying progressive learning and iterative back-translation approaches to further improve the performance. Our final submissions ranked first on three tracks in terms of the automatic evaluation metric.</abstract>
      <url hash="2a796749">2021.wmt-1.54</url>
      <bibkey>yang-etal-2021-multilingual-machine</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/flores-101">FLORES-101</pwcdataset>
    </paper>
    <paper id="55">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation in the <fixed-case>WMT</fixed-case> 2021 Large-Scale Multilingual Translation Task</title>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <pages>456–463</pages>
      <abstract>This paper presents the submission of Huawei Translation Services Center (HW-TSC) to the WMT 2021 Large-Scale Multilingual Translation Task. We participate in Samll Track #2, including 6 languages: Javanese (Jv), Indonesian (Id), Malay (Ms), Tagalog (Tl), Tamil (Ta) and English (En) with 30 directions under the constrained condition. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We train a single multilingual model to translate all the 30 directions. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. Several commonly used strategies are used to train our models, such as Back Translation, Forward Translation, Ensemble Knowledge Distillation, Adapter Fine-tuning. Our model obtains competitive results in the end.</abstract>
      <url hash="cd9c4ef6">2021.wmt-1.55</url>
      <bibkey>yu-etal-2021-hw</bibkey>
    </paper>
    <paper id="56">
      <title>On the Stability of System Rankings at <fixed-case>WMT</fixed-case></title>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <pages>464–477</pages>
      <abstract>The current approach to collecting human judgments of machine translation quality for the news translation task at WMT – segment rating with document context – is the most recent in a sequence of changes to WMT human annotation protocol. As these annotation protocols have changed over time, they have drifted away from some of the initial statistical assumptions underpinning them, with consequences that call the validity of WMT news task system rankings into question. In simulations based on real data, we show that the rankings can be influenced by the presence of outliers (high- or low-quality systems), resulting in different system rankings and clusterings. We also examine questions of annotation task composition and how ease or difficulty of translating different documents may influence system rankings. We provide discussion of ways to analyze these issues when considering future changes to annotation protocols.</abstract>
      <url hash="32ab6ec9">2021.wmt-1.56</url>
      <bibkey>knowles-2021-stability</bibkey>
      <video href="2021.wmt-1.56.mp4"/>
      <pwccode url="https://github.com/nrc-cnrc/wmt-stability" additional="false">nrc-cnrc/wmt-stability</pwccode>
    </paper>
    <paper id="57">
      <title>To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation</title>
      <author><first>Tom</first><last>Kocmi</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <author><first>Hitokazu</first><last>Matsushita</last></author>
      <author><first>Arul</first><last>Menezes</last></author>
      <pages>478–494</pages>
      <abstract>Automatic metrics are commonly used as the exclusive tool for declaring the superiority of one machine translation system’s quality over another. The community choice of automatic metric guides research directions and industrial developments by deciding which models are deemed better. Evaluating metrics correlations with sets of human judgements has been limited by the size of these sets. In this paper, we corroborate how reliable metrics are in contrast to human judgements on – to the best of our knowledge – the largest collection of judgements reported in the literature. Arguably, pairwise rankings of two systems are the most common evaluation tasks in research or deployment scenarios. Taking human judgement as a gold standard, we investigate which metrics have the highest accuracy in predicting translation quality rankings for such system pairs. Furthermore, we evaluate the performance of various metrics across different language pairs and domains. Lastly, we show that the sole use of BLEU impeded the development of improved models leading to bad deployment decisions. We release the collection of 2.3M sentence-level human judgements for 4380 systems for further analysis and replication of our work.</abstract>
      <url hash="1858dec3">2021.wmt-1.57</url>
      <bibkey>kocmi-etal-2021-ship</bibkey>
      <video href="2021.wmt-1.57.mp4"/>
      <pwccode url="https://github.com/MicrosoftTranslator/ToShipOrNotToShip" additional="true">MicrosoftTranslator/ToShipOrNotToShip</pwccode>
    </paper>
    <paper id="58">
      <title>Just Ask! Evaluating Machine Translation by Asking and Answering Questions</title>
      <author><first>Mateusz</first><last>Krubiński</last></author>
      <author><first>Erfan</first><last>Ghadery</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <pages>495–506</pages>
      <abstract>In this paper, we show that automatically-generated questions and answers can be used to evaluate the quality of Machine Translation (MT) systems. Building on recent work on the evaluation of abstractive text summarization, we propose a new metric for system-level MT evaluation, compare it with other state-of-the-art solutions, and show its robustness by conducting experiments for various MT directions.</abstract>
      <url hash="c8e87bf1">2021.wmt-1.58</url>
      <bibkey>krubinski-etal-2021-just</bibkey>
      <video href="2021.wmt-1.58.mp4"/>
      <pwccode url="https://github.com/ufal/mteqa" additional="false">ufal/mteqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="59">
      <title>A Fine-Grained Analysis of <fixed-case>BERTS</fixed-case>core</title>
      <author><first>Michael</first><last>Hanna</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>507–517</pages>
      <abstract>BERTScore, a recently proposed automatic metric for machine translation quality, uses BERT, a large pre-trained language model to evaluate candidate translations with respect to a gold translation. Taking advantage of BERT’s semantic and syntactic abilities, BERTScore seeks to avoid the flaws of earlier approaches like BLEU, instead scoring candidate translations based on their semantic similarity to the gold sentence. However, BERT is not infallible; while its performance on NLP tasks set a new state of the art in general, studies of specific syntactic and semantic phenomena have shown where BERT’s performance deviates from that of humans more generally. This naturally raises the questions we address in this paper: what are the strengths and weaknesses of BERTScore? Do they relate to known weaknesses on the part of BERT? We find that while BERTScore can detect when a candidate differs from a reference in important content words, it is less sensitive to smaller errors, especially if the candidate is lexically or stylistically similar to the reference.</abstract>
      <url hash="0353a77e">2021.wmt-1.59</url>
      <bibkey>hanna-bojar-2021-fine</bibkey>
      <video href="2021.wmt-1.59.mp4"/>
    </paper>
    <paper id="60">
      <title>Evaluating Multiway Multilingual <fixed-case>NMT</fixed-case> in the <fixed-case>T</fixed-case>urkic Languages</title>
      <author><first>Jamshidbek</first><last>Mirzakhalov</last></author>
      <author><first>Anoop</first><last>Babu</last></author>
      <author><first>Aigiz</first><last>Kunafin</last></author>
      <author><first>Ahsan</first><last>Wahab</last></author>
      <author><first>Bekhzodbek</first><last>Moydinboyev</last></author>
      <author><first>Sardana</first><last>Ivanova</last></author>
      <author><first>Mokhiyakhon</first><last>Uzokova</last></author>
      <author><first>Shaxnoza</first><last>Pulatova</last></author>
      <author><first>Duygu</first><last>Ataman</last></author>
      <author><first>Julia</first><last>Kreutzer</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>John</first><last>Licato</last></author>
      <author><first>Sriram</first><last>Chellappan</last></author>
      <pages>518–530</pages>
      <abstract>Despite the increasing number of large and comprehensive machine translation (MT) systems, evaluation of these methods in various languages has been restrained by the lack of high-quality parallel corpora as well as engagement with the people that speak these languages. In this study, we present an evaluation of state-of-the-art approaches to training and evaluating MT systems in 22 languages from the Turkic language family, most of which being extremely under-explored. First, we adopt the TIL Corpus with a few key improvements to the training and the evaluation sets. Then, we train 26 bilingual baselines as well as a multi-way neural MT (MNMT) model using the corpus and perform an extensive analysis using automatic metrics as well as human evaluations. We find that the MNMT model outperforms almost all bilingual baselines in the out-of-domain test sets and finetuning the model on a downstream task of a single pair also results in a huge performance boost in both low- and high-resource scenarios. Our attentive analysis of evaluation criteria for MT models in Turkic languages also points to the necessity for further research in this direction. We release the corpus splits, test sets as well as models to the public.</abstract>
      <url hash="74fbd94f">2021.wmt-1.60</url>
      <bibkey>mirzakhalov-etal-2021-evaluating</bibkey>
      <video href="2021.wmt-1.60.mp4"/>
      <pwccode url="https://github.com/turkic-interlingua/til-mt" additional="false">turkic-interlingua/til-mt</pwccode>
    </paper>
    <paper id="61">
      <title>Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives</title>
      <author><first>Jonas-Dario</first><last>Troles</last></author>
      <author><first>Ute</first><last>Schmid</last></author>
      <pages>531–541</pages>
      <abstract>Human gender bias is reflected in language and text production. Because state-of-the-art machine translation (MT) systems are trained on large corpora of text, mostly generated by humans, gender bias can also be found in MT. For instance when occupations are translated from a language like English, which mostly uses gender neutral words, to a language like German, which mostly uses a feminine and a masculine version for an occupation, a decision must be made by the MT System. Recent research showed that MT systems are biased towards stereotypical translation of occupations. In 2019 the first, and so far only, challenge set, explicitly designed to measure the extent of gender bias in MT systems has been published. In this set measurement of gender bias is solely based on the translation of occupations. With our paper we present an extension of this challenge set, called WiBeMT, which adds gender-biased adjectives and sentences with gender-biased verbs. The resulting challenge set consists of over 70, 000 sentences and has been translated with three commercial MT systems: DeepL Translator, Microsoft Translator, and Google Translate. Results show a gender bias for all three MT systems. This gender bias is to a great extent significantly influenced by adjectives and to a lesser extent by verbs.</abstract>
      <url hash="73ac409d">2021.wmt-1.61</url>
      <bibkey>troles-schmid-2021-extending</bibkey>
      <video href="2021.wmt-1.61.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="62">
      <title>Continual Learning in Multilingual <fixed-case>NMT</fixed-case> via Language-Specific Embeddings</title>
      <author><first>Alexandre</first><last>Berard</last></author>
      <pages>542–565</pages>
      <abstract>This paper proposes a technique for adding a new source or target language to an existing multilingual NMT model without re-training it on the initial set of languages. It consists in replacing the shared vocabulary with a small language-specific vocabulary and fine-tuning the new embeddings on the new language’s parallel data. Some additional language-specific components may be trained to improve performance (e.g., Transformer layers or adapter modules). Because the parameters of the original model are not modified, its performance on the initial languages does not degrade. We show on two sets of experiments (small-scale on TED Talks, and large-scale on ParaCrawl) that this approach performs as well or better as the more costly alternatives; and that it has excellent zero-shot performance: training on English-centric data is enough to translate between the new language and any of the initial languages.</abstract>
      <url hash="71cc6ed2">2021.wmt-1.62</url>
      <bibkey>berard-2021-continual</bibkey>
      <video href="2021.wmt-1.62.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="63">
      <title><fixed-case>DELA</fixed-case> Corpus - A Document-Level Corpus Annotated with Context-Related Issues</title>
      <author><first>Sheila</first><last>Castilho</last></author>
      <author><first>João Lucas</first><last>Cavalheiro Camargo</last></author>
      <author><first>Miguel</first><last>Menezes</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>566–577</pages>
      <abstract>Recently, the Machine Translation (MT) community has become more interested in document-level evaluation especially in light of reactions to claims of “human parity”, since examining the quality at the level of the document rather than at the sentence level allows for the assessment of suprasentential context, providing a more reliable evaluation. This paper presents a document-level corpus annotated in English with context-aware issues that arise when translating from English into Brazilian Portuguese, namely ellipsis, gender, lexical ambiguity, number, reference, and terminology, with six different domains. The corpus can be used as a challenge test set for evaluation and as a training/testing corpus for MT as well as for deep linguistic analysis of context issues. To the best of our knowledge, this is the first corpus of its kind.</abstract>
      <url hash="eda6418a">2021.wmt-1.63</url>
      <bibkey>castilho-etal-2021-dela</bibkey>
      <video href="2021.wmt-1.63.mp4"/>
    </paper>
    <paper id="64">
      <title>Multilingual Domain Adaptation for <fixed-case>NMT</fixed-case>: Decoupling Language and Domain Information with Adapters</title>
      <author><first>Asa</first><last>Cooper Stickland</last></author>
      <author><first>Alexandre</first><last>Berard</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <pages>578–598</pages>
      <abstract>Adapter layers are lightweight, learnable units inserted between transformer layers. Recent work explores using such layers for neural machine translation (NMT), to adapt pre-trained models to new domains or language pairs, training only a small set of parameters for each new setting (language pair or domain). In this work we study the compositionality of language and domain adapters in the context of Machine Translation. We aim to study, 1) parameter-efficient adaptation to multiple domains and languages simultaneously (full-resource scenario) and 2) cross-lingual transfer in domains where parallel data is unavailable for certain language pairs (partial-resource scenario). We find that in the partial resource scenario a naive combination of domain-specific and language-specific adapters often results in ‘catastrophic forgetting’ of the missing languages. We study other ways to combine the adapters to alleviate this issue and maximize cross-lingual transfer. With our best adapter combinations, we obtain improvements of 3-4 BLEU on average for source languages that do not have in-domain data. For target languages without in-domain data, we achieve a similar improvement by combining adapters with back-translation. Supplementary material is available at https://tinyurl.com/r66stbxj.</abstract>
      <url hash="f2520c89">2021.wmt-1.64</url>
      <bibkey>cooper-stickland-etal-2021-multilingual</bibkey>
      <video href="2021.wmt-1.64.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="65">
      <title>Translation Transformers Rediscover Inherent Data Domains</title>
      <author><first>Maksym</first><last>Del</last></author>
      <author><first>Elizaveta</first><last>Korotkova</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <pages>599–613</pages>
      <abstract>Many works proposed methods to improve the performance of Neural Machine Translation (NMT) models in a domain/multi-domain adaptation scenario. However, an understanding of how NMT baselines represent text domain information internally is still lacking. Here we analyze the sentence representations learned by NMT Transformers and show that these explicitly include the information on text domains, even after only seeing the input sentences without domains labels. Furthermore, we show that this internal information is enough to cluster sentences by their underlying domains without supervision. We show that NMT models produce clusters better aligned to the actual domains compared to pre-trained language models (LMs). Notably, when computed on document-level, NMT cluster-to-domain correspondence nears 100%. We use these findings together with an approach to NMT domain adaptation using automatically extracted domains. Whereas previous work relied on external LMs for text clustering, we propose re-using the NMT model as a source of unsupervised clusters. We perform an extensive experimental study comparing two approaches across two data scenarios, three language pairs, and both sentence-level and document-level clustering, showing equal or significantly superior performance compared to LMs.</abstract>
      <url hash="2412479e">2021.wmt-1.65</url>
      <bibkey>del-etal-2021-translation</bibkey>
      <video href="2021.wmt-1.65.mp4"/>
      <pwccode url="https://github.com/tartunlp/inherent-domains-wmt21" additional="false">tartunlp/inherent-domains-wmt21</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="66">
      <title>Improving Machine Translation of Rare and Unseen Word Senses</title>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Qianchu</first><last>Liu</last></author>
      <author><first>Dario</first><last>Stojanovski</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>614–624</pages>
      <abstract>The performance of NMT systems has improved drastically in the past few years but the translation of multi-sense words still poses a challenge. Since word senses are not represented uniformly in the parallel corpora used for training, there is an excessive use of the most frequent sense in MT output. In this work, we propose CmBT (Contextually-mined Back-Translation), an approach for improving multi-sense word translation leveraging pre-trained cross-lingual contextual word representations (CCWRs). Because of their contextual sensitivity and their large pre-training data, CCWRs can easily capture word senses that are missing or very rare in parallel corpora used to train MT. Specifically, CmBT applies bilingual lexicon induction on CCWRs to mine sense-specific target sentences from a monolingual dataset, and then back-translates these sentences to generate a pseudo parallel corpus as additional training data for an MT system. We test the translation quality of ambiguous words on the MuCoW test suite, which was built to test the word sense disambiguation effectiveness of MT systems. We show that our system improves on the translation of difficult unseen and low frequency word senses.</abstract>
      <url hash="3fd0bb62">2021.wmt-1.66</url>
      <bibkey>hangya-etal-2021-improving</bibkey>
      <video href="2021.wmt-1.66.mp4"/>
    </paper>
    <paper id="67">
      <title>Pushing the Right Buttons: Adversarial Evaluation of Quality Estimation</title>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Tharindu</first><last>Ranasinghe</last></author>
      <author><first>Frédéric</first><last>Blain</last></author>
      <author><first>Constantin</first><last>Orăsan</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>625–638</pages>
      <abstract>Current Machine Translation (MT) systems achieve very good results on a growing variety of language pairs and datasets. However, they are known to produce fluent translation outputs that can contain important meaning errors, thus undermining their reliability in practice. Quality Estimation (QE) is the task of automatically assessing the performance of MT systems at test time. Thus, in order to be useful, QE systems should be able to detect such errors. However, this ability is yet to be tested in the current evaluation practices, where QE systems are assessed only in terms of their correlation with human judgements. In this work, we bridge this gap by proposing a general methodology for adversarial testing of QE for MT. First, we show that despite a high correlation with human judgements achieved by the recent SOTA, certain types of meaning errors are still problematic for QE to detect. Second, we show that on average, the ability of a given model to discriminate between meaning-preserving and meaning-altering perturbations is predictive of its overall performance, thus potentially allowing for comparing QE systems without relying on manual quality annotation.</abstract>
      <url hash="bd895f85">2021.wmt-1.67</url>
      <bibkey>kanojia-etal-2021-pushing</bibkey>
      <video href="2021.wmt-1.67.mp4"/>
      <pwccode url="https://github.com/dipteshkanojia/qe-evaluation" additional="false">dipteshkanojia/qe-evaluation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="68">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2021 Shared Task on Efficient Translation</title>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <author><first>Qianqian</first><last>Zhu</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <pages>639–651</pages>
      <abstract>The machine translation efficiency task challenges participants to make their systems faster and smaller with minimal impact on translation quality. How much quality to sacrifice for efficiency depends upon the application, so participants were encouraged to make multiple submissions covering the space of trade-offs. In total, there were 53 submissions by 4 teams. There were GPU, single-core CPU, and multi-core CPU hardware tracks as well as batched throughput or single-sentence latency conditions. Submissions showed hundreds of millions of words can be translated for a dollar, average latency is 5–17 ms, and models fit in 7.5–150 MB.</abstract>
      <url hash="4d419c86">2021.wmt-1.68</url>
      <attachment type="Software" hash="338611f5">2021.wmt-1.68.Software.zip</attachment>
      <bibkey>heafield-etal-2021-findings</bibkey>
    </paper>
    <paper id="69">
      <title>Findings of the <fixed-case>WMT</fixed-case> Shared Task on Machine Translation Using Terminologies</title>
      <author><first>Md Mahfuz Ibn</first><last>Alam</last></author>
      <author><first>Ivana</first><last>Kvapilíková</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Georgiana</first><last>Dinu</last></author>
      <author><first>Marcello</first><last>Federico</last></author>
      <author><first>Matthias</first><last>Gallé</last></author>
      <author><first>Kweonwoo</first><last>Jung</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <pages>652–663</pages>
      <abstract>Language domains that require very careful use of terminology are abundant and reflect a significant part of the translation industry. In this work we introduce a benchmark for evaluating the quality and consistency of terminology translation, focusing on the medical (and COVID-19 specifically) domain for five language pairs: English to French, Chinese, Russian, and Korean, as well as Czech to German. We report the descriptions and results of the participating systems, commenting on the need for further research efforts towards both more adequate handling of terminologies as well as towards a proper formulation and evaluation of the task.</abstract>
      <url hash="0cbed520">2021.wmt-1.69</url>
      <bibkey>alam-etal-2021-findings</bibkey>
      <video href="2021.wmt-1.69.mp4"/>
    </paper>
    <paper id="70">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2021 Biomedical Translation Shared Task: Summaries of Animal Experiments as New Test Set</title>
      <author><first>Lana</first><last>Yeganova</last></author>
      <author><first>Dina</first><last>Wiemann</last></author>
      <author><first>Mariana</first><last>Neves</last></author>
      <author><first>Federica</first><last>Vezzani</last></author>
      <author><first>Amy</first><last>Siu</last></author>
      <author><first>Inigo</first><last>Jauregi Unanue</last></author>
      <author><first>Maite</first><last>Oronoz</last></author>
      <author><first>Nancy</first><last>Mah</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>David</first><last>Martinez</last></author>
      <author><first>Rachel</first><last>Bawden</last></author>
      <author><first>Giorgio Maria</first><last>Di Nunzio</last></author>
      <author><first>Roland</first><last>Roller</last></author>
      <author><first>Philippe</first><last>Thomas</last></author>
      <author><first>Cristian</first><last>Grozea</last></author>
      <author><first>Olatz</first><last>Perez-de-Viñaspre</last></author>
      <author><first>Maika</first><last>Vicente Navarro</last></author>
      <author><first>Antonio</first><last>Jimeno Yepes</last></author>
      <pages>664–683</pages>
      <abstract>In the sixth edition of the WMT Biomedical Task, we addressed a total of eight language pairs, namely English/German, English/French, English/Spanish, English/Portuguese, English/Chinese, English/Russian, English/Italian, and English/Basque. Further, our tests were composed of three types of textual test sets. New to this year, we released a test set of summaries of animal experiments, in addition to the test sets of scientific abstracts and terminologies. We received a total of 107 submissions from 15 teams from 6 countries.</abstract>
      <url hash="2dae9f66">2021.wmt-1.70</url>
      <bibkey>yeganova-etal-2021-findings</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="71">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2021 Shared Task on Quality Estimation</title>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Frédéric</first><last>Blain</last></author>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Chrysoula</first><last>Zerva</last></author>
      <author><first>Zhenhao</first><last>Li</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>684–725</pages>
      <abstract>We report the results of the WMT 2021 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word and sentence levels. This edition focused on two main novel additions: (i) prediction for unseen languages, i.e. zero-shot settings, and (ii) prediction of sentences with catastrophic errors. In addition, new data was released for a number of languages, especially post-edited data. Participating teams from 19 institutions submitted altogether 1263 systems to different task variants and language pairs.</abstract>
      <url hash="607328ce">2021.wmt-1.71</url>
      <bibkey>specia-etal-2021-findings</bibkey>
      <video href="2021.wmt-1.71.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe-pe">MLQE-PE</pwcdataset>
    </paper>
    <paper id="72">
      <title>Findings of the <fixed-case>WMT</fixed-case> 2021 Shared Tasks in Unsupervised <fixed-case>MT</fixed-case> and Very Low Resource Supervised <fixed-case>MT</fixed-case></title>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>726–732</pages>
      <abstract>We present the findings of the WMT2021 Shared Tasks in Unsupervised MT and Very Low Resource Supervised MT. Within the task, the community studied very low resource translation between German and Upper Sorbian, unsupervised translation between German and Lower Sorbian and low resource translation between Russian and Chuvash, all minority languages with active language communities working on preserving the languages, who are partners in the evaluation. Thanks to this, we were able to obtain most digital data available for these languages and offer them to the task participants. In total, six teams participated in the shared task. The paper discusses the background, presents the tasks and results, and discusses best practices for the future.</abstract>
      <url hash="ffa6acbe">2021.wmt-1.72</url>
      <bibkey>libovicky-fraser-2021-findings</bibkey>
    </paper>
    <paper id="73">
      <title>Results of the <fixed-case>WMT</fixed-case>21 Metrics Shared Task: Evaluating Metrics with Expert-based Human Evaluations on <fixed-case>TED</fixed-case> and News Domain</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>Nitika</first><last>Mathur</last></author>
      <author><first>Chi-kiu</first><last>Lo</last></author>
      <author><first>Craig</first><last>Stewart</last></author>
      <author><first>George</first><last>Foster</last></author>
      <author><first>Alon</first><last>Lavie</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>733–774</pages>
      <abstract>This paper presents the results of the WMT21 Metrics Shared Task. Participants were asked to score the outputs of the translation systems competing in the WMT21 News Translation Task with automatic metrics on two different domains: news and TED talks. All metrics were evaluated on how well they correlate at the system- and segment-level with human ratings. Contrary to previous years’ editions, this year we acquired our own human ratings based on expert-based human evaluation via Multidimensional Quality Metrics (MQM). This setup had several advantages: (i) expert-based evaluation has been shown to be more reliable, (ii) we were able to evaluate all metrics on two different domains using translations of the same MT systems, (iii) we added 5 additional translations coming from the same system during system development. In addition, we designed three challenge sets that evaluate the robustness of all automatic metrics. We present an extensive analysis on how well metrics perform on three language pairs: English to German, English to Russian and Chinese to English. We further show the impact of different reference translations on reference-based metrics and compare our expert-based MQM annotation with the DA scores acquired by WMT.</abstract>
      <url hash="b047c48c">2021.wmt-1.73</url>
      <bibkey>freitag-etal-2021-results</bibkey>
      <revision id="1" href="2021.wmt-1.73v1" hash="84c89ac5"/>
      <revision id="2" href="2021.wmt-1.73v2" hash="a4bbe848" date="2022-07-08">Various updates throughout the paper.</revision>
      <revision id="3" href="2021.wmt-1.73v3" hash="b047c48c" date="2023-01-18">Corrects the WMT21 DA human ratings scores.</revision>
    </paper>
    <paper id="74">
      <title>Efficient Machine Translation with Model Pruning and Quantization</title>
      <author><first>Maximiliana</first><last>Behnke</last></author>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <author><first>Graeme</first><last>Nail</last></author>
      <author><first>Qianqian</first><last>Zhu</last></author>
      <author><first>Svetlana</first><last>Tchistiakova</last></author>
      <author><first>Jelmer</first><last>van der Linde</last></author>
      <author><first>Pinzhen</first><last>Chen</last></author>
      <author><first>Sidharth</first><last>Kashyap</last></author>
      <author><first>Roman</first><last>Grundkiewicz</last></author>
      <pages>775–780</pages>
      <abstract>We participated in all tracks of the WMT 2021 efficient machine translation task: single-core CPU, multi-core CPU, and GPU hardware with throughput and latency conditions. Our submissions combine several efficiency strategies: knowledge distillation, a simpler simple recurrent unit (SSRU) decoder with one or two layers, lexical shortlists, smaller numerical formats, and pruning. For the CPU track, we used quantized 8-bit models. For the GPU track, we experimented with FP16 and 8-bit integers in tensorcores. Some of our submissions optimize for size via 4-bit log quantization and omitting a lexical shortlist. We have extended pruning to more parts of the network, emphasizing component- and block-level pruning that actually improves speed unlike coefficient-wise pruning.</abstract>
      <url hash="0e3e5b60">2021.wmt-1.74</url>
      <bibkey>behnke-etal-2021-efficient</bibkey>
    </paper>
    <paper id="75">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation in the <fixed-case>WMT</fixed-case> 2021 Efficiency Shared Task</title>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Ting</first><last>Hu</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Jianfei</first><last>Feng</last></author>
      <author><first>ZhengZhe</first><last>Yu</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Shaojun</first><last>Li</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>ShiMin</first><last>Tao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Jun</first><last>Yao</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <pages>781–786</pages>
      <abstract>This paper presents the submission of Huawei Translation Services Center (HW-TSC) to WMT 2021 Efficiency Shared Task. We explore the sentence-level teacher-student distillation technique and train several small-size models that find a balance between efficiency and quality. Our models feature deep encoder, shallow decoder and light-weight RNN with SSRU layer. We use Huawei Noah’s Bolt, an efficient and light-weight library for on-device inference. Leveraging INT8 quantization, self-defined General Matrix Multiplication (GEMM) operator, shortlist, greedy search and caching, we submit four small-size and efficient translation models with high translation quality for the one CPU core latency track.</abstract>
      <url hash="3e1c9aa1">2021.wmt-1.75</url>
      <bibkey>shang-etal-2021-hw</bibkey>
    </paper>
    <paper id="76">
      <title>The <fixed-case>N</fixed-case>iu<fixed-case>T</fixed-case>rans System for the <fixed-case>WMT</fixed-case> 2021 Efficiency Task</title>
      <author><first>Chenglong</first><last>Wang</last></author>
      <author><first>Chi</first><last>Hu</last></author>
      <author><first>Yongyu</first><last>Mu</last></author>
      <author><first>Zhongxiang</first><last>Yan</last></author>
      <author><first>Siming</first><last>Wu</last></author>
      <author><first>Yimin</first><last>Hu</last></author>
      <author><first>Hang</first><last>Cao</last></author>
      <author><first>Bei</first><last>Li</last></author>
      <author><first>Ye</first><last>Lin</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <pages>787–794</pages>
      <abstract>This paper describes the NiuTrans system for the WMT21 translation efficiency task. Following last year’s work, we explore various techniques to improve the efficiency while maintaining translation quality. We investigate the combinations of lightweight Transformer architectures and knowledge distillation strategies. Also, we improve the translation efficiency with graph optimization, low precision, dynamic batching, and parallel pre/post-processing. Putting these together, our system can translate 247,000 words per second on an NVIDIA A100, being 3<tex-math>\times</tex-math> faster than our last year’s system. Our system is the fastest and has the lowest memory consumption on the GPU-throughput track. The code, model, and pipeline will be available at NiuTrans.NMT.</abstract>
      <url hash="df17af4a">2021.wmt-1.76</url>
      <bibkey>wang-etal-2021-niutrans</bibkey>
    </paper>
    <paper id="77">
      <title><fixed-case>T</fixed-case>en<fixed-case>T</fixed-case>rans High-Performance Inference Toolkit for <fixed-case>WMT</fixed-case>2021 Efficiency Task</title>
      <author><first>Kaixin</first><last>Wu</last></author>
      <author><first>Bojie</first><last>Hu</last></author>
      <author><first>Qi</first><last>Ju</last></author>
      <pages>795–798</pages>
      <abstract>The paper describes the TenTrans’s submissions to the WMT 2021 Efficiency Shared Task. We explore training a variety of smaller compact transformer models using the teacher-student setup. Our model is trained by our self-developed open-source multilingual training platform TenTrans-Py. We also release an open-source high-performance inference toolkit for transformer models and the code is written in C++ completely. All additional optimizations are built on top of the inference engine including attention caching, kernel fusion, early-stop, and several other optimizations. In our submissions, the fastest system can translate more than 22,000 tokens per second with a single Tesla P4 while maintaining 38.36 BLEU on En-De newstest2019. Our trained models and more details are available in TenTrans-Decoding competition examples.</abstract>
      <url hash="2dd36974">2021.wmt-1.77</url>
      <bibkey>wu-etal-2021-tentrans</bibkey>
    </paper>
    <paper id="78">
      <title>Lingua Custodia’s Participation at the <fixed-case>WMT</fixed-case> 2021 Machine Translation Using Terminologies Shared Task</title>
      <author><first>Melissa</first><last>Ailem</last></author>
      <author><first>Jingshu</first><last>Liu</last></author>
      <author><first>Raheel</first><last>Qader</last></author>
      <pages>799–803</pages>
      <abstract>This paper describes Lingua Custodia’s submission to the WMT21 shared task on machine translation using terminologies. We consider three directions, namely English to French, Russian, and Chinese. We rely on a Transformer-based architecture as a building block, and we explore a method which introduces two main changes to the standard procedure to handle terminologies. The first one consists in augmenting the training data in such a way as to encourage the model to learn a copy behavior when it encounters terminology constraint terms. The second change is constraint token masking, whose purpose is to ease copy behavior learning and to improve model generalization. Empirical results show that our method satisfies most terminology constraints while maintaining high translation quality.</abstract>
      <url hash="1c3e72ff">2021.wmt-1.78</url>
      <bibkey>ailem-etal-2021-lingua</bibkey>
    </paper>
    <paper id="79">
      <title>Kakao Enterprise’s <fixed-case>WMT</fixed-case>21 Machine Translation Using Terminologies Task Submission</title>
      <author><first>Yunju</first><last>Bak</last></author>
      <author><first>Jimin</first><last>Sun</last></author>
      <author><first>Jay</first><last>Kim</last></author>
      <author><first>Sungwon</first><last>Lyu</last></author>
      <author><first>Changmin</first><last>Lee</last></author>
      <pages>804–812</pages>
      <abstract>This paper describes Kakao Enterprise’s submission to the WMT21 shared Machine Translation using Terminologies task. We integrate terminology constraints by pre-training with target lemma annotations and fine-tuning with exact target annotations utilizing the given terminology dataset. This approach yields a model that achieves outstanding results in terms of both translation quality and term consistency, ranking first based on COMET in the En→Fr language direction. Furthermore, we explore various methods such as back-translation, explicitly training terminologies as additional parallel data, and in-domain data selection.</abstract>
      <url hash="ddbe79c1">2021.wmt-1.79</url>
      <bibkey>bak-etal-2021-kakao</bibkey>
    </paper>
    <paper id="80">
      <title>The <fixed-case>SPECTRANS</fixed-case> System Description for the <fixed-case>WMT</fixed-case>21 Terminology Task</title>
      <author><first>Nicolas</first><last>Ballier</last></author>
      <author><first>Dahn</first><last>Cho</last></author>
      <author><first>Bilal</first><last>Faye</last></author>
      <author><first>Zong-You</first><last>Ke</last></author>
      <author><first>Hanna</first><last>Martikainen</last></author>
      <author><first>Mojca</first><last>Pecman</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>Jean-Baptiste</first><last>Yunès</last></author>
      <author><first>Lichao</first><last>Zhu</last></author>
      <author><first>Maria</first><last>Zimina-Poirot</last></author>
      <pages>813–820</pages>
      <abstract>This paper discusses the WMT 2021 terminology shared task from a “meta” perspective. We present the results of our experiments using the terminology dataset and the OpenNMT (Klein et al., 2017) and JoeyNMT (Kreutzer et al., 2019) toolkits for the language direction English to French. Our experiment 1 compares the predictions of the two toolkits. Experiment 2 uses OpenNMT to fine-tune the model. We report our results for the task with the evaluation script but mostly discuss the linguistic properties of the terminology dataset provided for the task. We provide evidence of the importance of text genres across scores, having replicated the evaluation scripts.</abstract>
      <url hash="e8ce4e4a">2021.wmt-1.80</url>
      <bibkey>ballier-etal-2021-spectrans</bibkey>
    </paper>
    <paper id="81">
      <title>Dynamic Terminology Integration for <fixed-case>COVID</fixed-case>-19 and Other Emerging Domains</title>
      <author><first>Toms</first><last>Bergmanis</last></author>
      <author><first>Mārcis</first><last>Pinnis</last></author>
      <pages>821–827</pages>
      <abstract>The majority of language domains require prudent use of terminology to ensure clarity and adequacy of information conveyed. While the correct use of terminology for some languages and domains can be achieved by adapting general-purpose MT systems on large volumes of in-domain parallel data, such quantities of domain-specific data are seldom available for less-resourced languages and niche domains. Furthermore, as exemplified by COVID-19 recently, no domain-specific parallel data is readily available for emerging domains. However, the gravity of this recent calamity created a high demand for reliable translation of critical information regarding pandemic and infection prevention. This work is part of WMT2021 Shared Task: Machine Translation using Terminologies, where we describe Tilde MT systems that are capable of dynamic terminology integration at the time of translation. Our systems achieve up to 94% COVID-19 term use accuracy on the test set of the EN-FR language pair without having access to any form of in-domain information during system training.</abstract>
      <url hash="15d5f494">2021.wmt-1.81</url>
      <bibkey>bergmanis-pinnis-2021-dynamic</bibkey>
    </paper>
    <paper id="82">
      <title><fixed-case>CUNI</fixed-case> Systems for <fixed-case>WMT</fixed-case>21: Terminology Translation Shared Task</title>
      <author><first>Josef</first><last>Jon</last></author>
      <author><first>Michal</first><last>Novák</last></author>
      <author><first>João Paulo</first><last>Aires</last></author>
      <author><first>Dusan</first><last>Varis</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>828–834</pages>
      <abstract>This paper describes Charles University sub-mission for Terminology translation Shared Task at WMT21. The objective of this task is to design a system which translates certain terms based on a provided terminology database, while preserving high overall translation quality. We competed in English-French language pair. Our approach is based on providing the desired translations alongside the input sentence and training the model to use these provided terms. We lemmatize the terms both during the training and inference, to allow the model to learn how to produce correct surface forms of the words, when they differ from the forms provided in the terminology database. Our submission ranked second in Exact Match metric which evaluates the ability of the model to produce desired terms in the translation.</abstract>
      <url hash="e5310f19">2021.wmt-1.82</url>
      <bibkey>jon-etal-2021-cuni-systems</bibkey>
    </paper>
    <paper id="83">
      <title><fixed-case>PROMT</fixed-case> Systems for <fixed-case>WMT</fixed-case>21 Terminology Translation Task</title>
      <author><first>Alexander</first><last>Molchanov</last></author>
      <author><first>Vladislav</first><last>Kovalenko</last></author>
      <author><first>Fedor</first><last>Bykov</last></author>
      <pages>835–841</pages>
      <abstract>This paper describes the PROMT submissions for the WMT21 Terminology Translation Task. We participate in two directions: English to French and English to Russian. Our final submissions are MarianNMT-based neural systems. We present two technologies for terminology translation: a modification of the Dinu et al. (2019) soft-constrained approach and our own approach called PROMT Smart Neural Dictionary (SmartND). We achieve good results in both directions.</abstract>
      <url hash="b600cdb6">2021.wmt-1.83</url>
      <bibkey>molchanov-etal-2021-promt</bibkey>
    </paper>
    <paper id="84">
      <title><fixed-case>SYSTRAN</fixed-case> @ <fixed-case>WMT</fixed-case> 2021: Terminology Task</title>
      <author><first>Minh Quang</first><last>Pham</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Antoine</first><last>Senellart</last></author>
      <author><first>Dan</first><last>Berrebbi</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>842–850</pages>
      <abstract>This paper describes SYSTRAN submissions to the WMT 2021 terminology shared task. We participate in the English-to-French translation direction with a standard Transformer neural machine translation network that we enhance with the ability to dynamically include terminology constraints, a very common industrial practice. Two state-of-the-art terminology insertion methods are evaluated based (i) on the use of placeholders complemented with morphosyntactic annotation and (ii) on the use of target constraints injected in the source stream. Results show the suitability of the presented approaches in the evaluated scenario where terminology is used in a system trained on generic data only.</abstract>
      <url hash="045ee97a">2021.wmt-1.84</url>
      <bibkey>pham-etal-2021-systran</bibkey>
    </paper>
    <paper id="85">
      <title><fixed-case>T</fixed-case>erm<fixed-case>M</fixed-case>ind: <fixed-case>A</fixed-case>libaba’s <fixed-case>WMT</fixed-case>21 Machine Translation Using Terminologies Task Submission</title>
      <author><first>Ke</first><last>Wang</last></author>
      <author><first>Shuqin</first><last>Gu</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Weihua</first><last>Luo</last></author>
      <author><first>Yuqi</first><last>Zhang</last></author>
      <pages>851–856</pages>
      <abstract>This paper describes our work in the WMT 2021 Machine Translation using Terminologies Shared Task. We participate in the shared translation terminologies task in English to Chinese language pair. To satisfy terminology constraints on translation, we use a terminology data augmentation strategy based on Transformer model. We used tags to mark and add the term translations into the matched sentences. We created synthetic terms using phrase tables extracted from bilingual corpus to increase the proportion of term translations in training data. Detailed pre-processing and filtering on data, in-domain finetuning and ensemble method are used in our system. Our submission obtains competitive results in the terminology-targeted evaluation.</abstract>
      <url hash="6dc50e9f">2021.wmt-1.85</url>
      <bibkey>wang-etal-2021-termmind</bibkey>
    </paper>
    <paper id="86">
      <title><fixed-case>FJWU</fixed-case> Participation for the <fixed-case>WMT</fixed-case>21 Biomedical Translation Task</title>
      <author><first>Sumbal</first><last>Naz</last></author>
      <author><first>Sadaf</first><last>Abdul Rauf</last></author>
      <author><first>Sami Ul</first><last>Haq</last></author>
      <pages>857–862</pages>
      <abstract>In this paper we present the FJWU’s system submitted to the biomedical shared task at WMT21. We prepared state-of-the-art multilingual neural machine translation systems for three languages (i.e. German, Spanish and French) with English as target language. Our NMT systems based on Transformer architecture, were trained on combination of in-domain and out-domain parallel corpora developed using Information Retrieval (IR) and domain adaptation techniques.</abstract>
      <url hash="178e9f87">2021.wmt-1.86</url>
      <bibkey>naz-etal-2021-fjwu</bibkey>
    </paper>
    <paper id="87">
      <title>High Frequent In-domain Words Segmentation and Forward Translation for the <fixed-case>WMT</fixed-case>21 Biomedical Task</title>
      <author><first>Bardia</first><last>Rafieian</last></author>
      <author><first>Marta R.</first><last>Costa-jussa</last></author>
      <pages>863–867</pages>
      <abstract>This paper reports the optimization of using the out-of-domain data in the Biomedical translation task. We firstly optimized our parallel training dataset using the BabelNet in-domain terminology words. Afterward, to increase the training set, we studied the effects of the out-of-domain data on biomedical translation tasks, and we created a mixture of in-domain and out-of-domain training sets and added more in-domain data using forward translation in the English-Spanish task. Finally, with a simple bpe optimization method, we increased the number of in-domain sub-words in our mixed training set and trained the Transformer model on the generated data. Results show improvements using our proposed method.</abstract>
      <url hash="ca175b73">2021.wmt-1.87</url>
      <bibkey>rafieian-costa-jussa-2021-high</bibkey>
    </paper>
    <paper id="88">
      <title>Huawei <fixed-case>AARC</fixed-case>’s Submissions to the <fixed-case>WMT</fixed-case>21 Biomedical Translation Task: Domain Adaption from a Practical Perspective</title>
      <author><first>Weixuan</first><last>Wang</last></author>
      <author><first>Wei</first><last>Peng</last></author>
      <author><first>Xupeng</first><last>Meng</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>868–873</pages>
      <abstract>This paper describes Huawei Artificial Intelligence Application Research Center’s neural machine translation systems and submissions to the WMT21 biomedical translation shared task. Four of the submissions achieve state-of-the-art BLEU scores based on the official-released automatic evaluation results (EN-&gt;FR, EN&lt;-&gt;IT and ZH-&gt;EN). We perform experiments to unveil the practical insights of the involved domain adaptation techniques, including finetuning order, terminology dictionaries, and ensemble decoding. Issues associated with overfitting and under-translation are also discussed.</abstract>
      <url hash="314adefa">2021.wmt-1.88</url>
      <bibkey>wang-etal-2021-huawei</bibkey>
    </paper>
    <paper id="89">
      <title>Tencent <fixed-case>AI</fixed-case> Lab Machine Translation Systems for the <fixed-case>WMT</fixed-case>21 Biomedical Translation Task</title>
      <author><first>Xing</first><last>Wang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>874–878</pages>
      <abstract>This paper describes the Tencent AI Lab submission of the WMT2021 shared task on biomedical translation in eight language directions: English-German, English-French, English-Spanish and English-Russian. We utilized different Transformer architectures, pretraining and back-translation strategies to improve translation quality. Concretely, we explore mBART (Liu et al., 2020) to demonstrate the effectiveness of the pretraining strategy. Our submissions (Tencent AI Lab Machine Translation, TMT) in German/French/Spanish⇒English are ranked 1st respectively according to the official evaluation results in terms of BLEU scores.</abstract>
      <url hash="458b8353">2021.wmt-1.89</url>
      <bibkey>wang-etal-2021-tencent-ai</bibkey>
    </paper>
    <paper id="90">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Submissions to the <fixed-case>WMT</fixed-case>21 Biomedical Translation Task</title>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Zhanglin</first><last>Wu</last></author>
      <author><first>Zhengzhe</first><last>Yu</last></author>
      <author><first>Xiaoyu</first><last>Chen</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Minghan</first><last>Wang</last></author>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Lizhi</first><last>Lei</last></author>
      <author><first>Chuanfei</first><last>Xu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <pages>879–884</pages>
      <abstract>This paper describes the submission of Huawei Translation Service Center (HW-TSC) to WMT21 biomedical translation task in two language pairs: Chinese↔English and German↔English (Our registered team name is HuaweiTSC). Technical details are introduced in this paper, including model framework, data pre-processing method and model enhancement strategies. In addition, using the wmt20 OK-aligned biomedical test set, we compare and analyze system performances under different strategies. On WMT21 biomedical translation task, Our systems in English→Chinese and English→German directions get the highest BLEU scores among all submissions according to the official evaluation results.</abstract>
      <url hash="d6a2cf85">2021.wmt-1.90</url>
      <bibkey>yang-etal-2021-hw</bibkey>
    </paper>
    <paper id="91">
      <title><fixed-case>RTM</fixed-case> Super Learner Results at Quality Estimation Task</title>
      <author><first>Ergun</first><last>Biçici</last></author>
      <pages>885–889</pages>
      <abstract>We obtain new results using referential translation machines (RTMs) with predictions mixed to obtain a better mixture of experts prediction. Our super learner results improve the results and provide a robust combination model.</abstract>
      <url hash="fe43d248">2021.wmt-1.91</url>
      <bibkey>bicici-2021-rtm</bibkey>
    </paper>
    <paper id="92">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Participation at <fixed-case>WMT</fixed-case> 2021 Quality Estimation Shared Task</title>
      <author><first>Yimeng</first><last>Chen</last></author>
      <author><first>Chang</first><last>Su</last></author>
      <author><first>Yingtao</first><last>Zhang</last></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Xiang</first><last>Geng</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Guo</first><last>Jiaxin</last></author>
      <author><first>Wang</first><last>Minghan</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Yujia</first><last>Liu</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <pages>890–896</pages>
      <abstract>This paper presents our work in WMT 2021 Quality Estimation (QE) Shared Task. We participated in all of the three sub-tasks, including Sentence-Level Direct Assessment (DA) task, Word and Sentence-Level Post-editing Effort task and Critical Error Detection task, in all language pairs. Our systems employ the framework of Predictor-Estimator, concretely with a pre-trained XLM-Roberta as Predictor and task-specific classifier or regressor as Estimator. For all tasks, we improve our systems by incorporating post-edit sentence or additional high-quality translation sentence in the way of multitask learning or encoding it with predictors directly. Moreover, in zero-shot setting, our data augmentation strategy based on Monte-Carlo Dropout brings up significant improvement on DA sub-task. Notably, our submissions achieve remarkable results over all tasks.</abstract>
      <url hash="e68ee202">2021.wmt-1.92</url>
      <bibkey>chen-etal-2021-hw</bibkey>
    </paper>
    <paper id="93">
      <title>Ensemble Fine-tuned m<fixed-case>BERT</fixed-case> for Translation Quality Estimation</title>
      <author><first>Shaika</first><last>Chowdhury</last></author>
      <author><first>Naouel</first><last>Baili</last></author>
      <author><first>Brian</first><last>Vannah</last></author>
      <pages>897–903</pages>
      <abstract>Quality Estimation (QE) is an important component of the machine translation workflow as it assesses the quality of the translated output without consulting reference translations. In this paper, we discuss our submission to the WMT 2021 QE Shared Task. We participate in Task 2 sentence-level sub-task that challenge participants to predict the HTER score for sentence-level post-editing effort. Our proposed system is an ensemble of multilingual BERT (mBERT)-based regression models, which are generated by fine-tuning on different input settings. It demonstrates comparable performance with respect to the Pearson’s correlation, and beat the baseline system in MAE/ RMSE for several language pairs. In addition, we adapt our system for the zero-shot setting by exploiting target language-relevant language pairs and pseudo-reference translations.</abstract>
      <url hash="f11e9263">2021.wmt-1.93</url>
      <bibkey>chowdhury-etal-2021-ensemble</bibkey>
    </paper>
    <paper id="94">
      <title>The <fixed-case>JHU</fixed-case>-<fixed-case>M</fixed-case>icrosoft Submission for <fixed-case>WMT</fixed-case>21 Quality Estimation Shared Task</title>
      <author><first>Shuoyang</first><last>Ding</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Christian</first><last>Federmann</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>904–910</pages>
      <abstract>This paper presents the JHU-Microsoft joint submission for WMT 2021 quality estimation shared task. We only participate in Task 2 (post-editing effort estimation) of the shared task, focusing on the target-side word-level quality estimation. The techniques we experimented with include Levenshtein Transformer training and data augmentation with a combination of forward, backward, round-trip translation, and pseudo post-editing of the MT output. We demonstrate the competitiveness of our system compared to the widely adopted OpenKiwi-XLM baseline. Our system is also the top-ranking system on the MT MCC metric for the English-German language pair.</abstract>
      <url hash="81d0cec7">2021.wmt-1.94</url>
      <bibkey>ding-etal-2021-jhu</bibkey>
    </paper>
    <paper id="95">
      <title><fixed-case>TUD</fixed-case>a at <fixed-case>WMT</fixed-case>21: Sentence-Level Direct Assessment with Adapters</title>
      <author><first>Gregor</first><last>Geigle</last></author>
      <author><first>Jonas</first><last>Stadtmüller</last></author>
      <author><first>Wei</first><last>Zhao</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <pages>911–919</pages>
      <abstract>This paper presents our submissions to the WMT2021 Shared Task on Quality Estimation, Task 1 <i>Sentence-Level Direct Assessment</i>. While top-performing approaches utilize massively multilingual Transformer-based language models which have been pre-trained on all target languages of the task, the resulting insights are limited, as it is unclear how well the approach performs on languages unseen during pre-training; more problematically, these approaches do not provide any solutions for <i>extending</i> the model to new languages or unseen scripts—arguably one of the objectives of this shared task. In this work, we thus focus on utilizing massively multilingual language models which only <i>partly</i> cover the target languages during their pre-training phase. We extend the model to new languages and unseen scripts using recent adapter-based methods and achieve on par performance or even surpass models pre-trained on the respective languages.</abstract>
      <url hash="3c6ac559">2021.wmt-1.95</url>
      <bibkey>geigle-etal-2021-tuda</bibkey>
    </paper>
    <paper id="96">
      <title>Quality Estimation Using Dual Encoders with Transfer Learning</title>
      <author><first>Dam</first><last>Heo</last></author>
      <author><first>WonKee</first><last>Lee</last></author>
      <author><first>Baikjin</first><last>Jung</last></author>
      <author><first>Jong-Hyeok</first><last>Lee</last></author>
      <pages>920–927</pages>
      <abstract>This paper describes POSTECH’s quality estimation systems submitted to Task 2 of the WMT 2021 quality estimation shared task: Word and Sentence-Level Post-editing Effort. We notice that it is possible to improve the stability of the latest quality estimation models that have only one encoder based on the self-attention mechanism to simultaneously process both of the two input data, a source sequence and its machine translation, in that such models have neglected to take advantage of pre-trained monolingual representations, which are generally accepted as reliable representations for various natural language processing tasks. Therefore, our model uses two pre-trained monolingual encoders and then exchanges the information of two encoded representations through two additional cross attention networks. According to the official leaderboard, our systems outperform the baseline systems in terms of the Matthews correlation coefficient for machine translations’ word-level quality estimation and in terms of the Pearson’s correlation coefficient for sentence-level quality estimation by 0.4126 and 0.5497 respectively.</abstract>
      <url hash="f47ffd1c">2021.wmt-1.96</url>
      <bibkey>heo-etal-2021-quality</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/escape">eSCAPE</pwcdataset>
    </paper>
    <paper id="97">
      <title><fixed-case>ICL</fixed-case>’s Submission to the <fixed-case>WMT</fixed-case>21 Critical Error Detection Shared Task</title>
      <author><first>Genze</first><last>Jiang</last></author>
      <author><first>Zhenhao</first><last>Li</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>928–934</pages>
      <abstract>This paper presents Imperial College London’s submissions to the WMT21 Quality Estimation (QE) Shared Task 3: Critical Error Detection. Our approach builds on cross-lingual pre-trained representations in a sequence classification model. We further improve the base classifier by (i) adding a weighted sampler to deal with unbalanced data and (ii) introducing feature engineering, where features related to toxicity, named-entities and sentiment, which are potentially indicative of critical errors, are extracted using existing tools and integrated to the model in different ways. We train models with one type of feature at a time and ensemble those models that improve over the base classifier on the development (dev) set. Our official submissions achieve very competitive results, ranking second for three out of four language pairs.</abstract>
      <url hash="ca074527">2021.wmt-1.97</url>
      <bibkey>jiang-etal-2021-icls</bibkey>
    </paper>
    <paper id="98">
      <title>Papago’s Submission for the <fixed-case>WMT</fixed-case>21 Quality Estimation Shared Task</title>
      <author><first>Seunghyun</first><last>Lim</last></author>
      <author><first>Hantae</first><last>Kim</last></author>
      <author><first>Hyunjoong</first><last>Kim</last></author>
      <pages>935–940</pages>
      <abstract>This paper describes Papago submission to the WMT 2021 Quality Estimation Task 1: Sentence-level Direct Assessment. Our multilingual Quality Estimation system explores the combination of Pretrained Language Models and Multi-task Learning architectures. We propose an iterative training pipeline based on pretraining with large amounts of in-domain synthetic data and finetuning with gold (labeled) data. We then compress our system via knowledge distillation in order to reduce parameters yet maintain strong performance. Our submitted multilingual systems perform competitively in multilingual and all 11 individual language pair settings including zero-shot.</abstract>
      <url hash="94c87f95">2021.wmt-1.98</url>
      <bibkey>lim-etal-2021-papagos</bibkey>
    </paper>
    <paper id="99">
      <title><fixed-case>NICT</fixed-case> <fixed-case>K</fixed-case>yoto Submission for the <fixed-case>WMT</fixed-case>’21 Quality Estimation Task: Multimetric Multilingual Pretraining for Critical Error Detection</title>
      <author><first>Raphael</first><last>Rubino</last></author>
      <author><first>Atsushi</first><last>Fujita</last></author>
      <author><first>Benjamin</first><last>Marie</last></author>
      <pages>941–947</pages>
      <abstract>This paper presents the NICT Kyoto submission for the WMT’21 Quality Estimation (QE) Critical Error Detection shared task (Task 3). Our approach relies mainly on QE model pretraining for which we used 11 language pairs, three sentence-level and three word-level translation quality metrics. Starting from an XLM-R checkpoint, we perform continued training by modifying the learning objective, switching from masked language modeling to QE oriented signals, before finetuning and ensembling the models. Results obtained on the test set in terms of correlation coefficient and F-score show that automatic metrics and synthetic data perform well for pretraining, with our submissions ranked first for two out of four language pairs. A deeper look at the impact of each metric on the downstream task indicates higher performance for token oriented metrics, while an ablation study emphasizes the usefulness of conducting both self-supervised and QE pretraining.</abstract>
      <url hash="2f6549e8">2021.wmt-1.99</url>
      <bibkey>rubino-etal-2021-nict</bibkey>
    </paper>
    <paper id="100">
      <title><fixed-case>QEM</fixed-case>ind: <fixed-case>A</fixed-case>libaba’s Submission to the <fixed-case>WMT</fixed-case>21 Quality Estimation Shared Task</title>
      <author><first>Jiayi</first><last>Wang</last></author>
      <author><first>Ke</first><last>Wang</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Weihua</first><last>Luo</last></author>
      <author><first>Yuqi</first><last>Zhang</last></author>
      <pages>948–954</pages>
      <abstract>Quality Estimation, as a crucial step of quality control for machine translation, has been explored for years. The goal is to to investigate automatic methods for estimating the quality of machine translation results without reference translations. In this year’s WMT QE shared task, we utilize the large-scale XLM-Roberta pre-trained model and additionally propose several useful features to evaluate the uncertainty of the translations to build our QE system, named <i>
          <b>QEMind</b>
        </i>. The system has been applied to the sentence-level scoring task of Direct Assessment and the binary score prediction task of Critical Error Detection. In this paper, we present our submissions to the WMT 2021 QE shared task and an extensive set of experimental results have shown us that our multilingual systems outperform the best system in the Direct Assessment QE task of WMT 2020.</abstract>
      <url hash="6abaa76f">2021.wmt-1.100</url>
      <bibkey>wang-etal-2021-qemind</bibkey>
    </paper>
    <paper id="101">
      <title>Direct Exploitation of Attention Weights for Translation Quality Estimation</title>
      <author><first>Lisa</first><last>Yankovskaya</last></author>
      <author><first>Mark</first><last>Fishel</last></author>
      <pages>955–960</pages>
      <abstract>The paper presents our submission to the WMT2021 Shared Task on Quality Estimation (QE). We participate in sentence-level predictions of human judgments and post-editing effort. We propose a glass-box approach based on attention weights extracted from machine translation systems. In contrast to the previous works, we directly explore attention weight matrices without replacing them with general metrics (like entropy). We show that some of our models can be trained with a small amount of a high-cost labelled data. In the absence of training data our approach still demonstrates a moderate linear correlation, when trained with synthetic data.</abstract>
      <url hash="56444691">2021.wmt-1.101</url>
      <bibkey>yankovskaya-fishel-2021-direct</bibkey>
    </paper>
    <paper id="102">
      <title><fixed-case>IST</fixed-case>-Unbabel 2021 Submission for the Quality Estimation Shared Task</title>
      <author><first>Chrysoula</first><last>Zerva</last></author>
      <author><first>Daan</first><last>van Stigt</last></author>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>Ana C</first><last>Farinha</last></author>
      <author><first>Pedro</first><last>Ramos</last></author>
      <author><first>José G.</first><last>C. de Souza</last></author>
      <author><first>Taisiya</first><last>Glushkova</last></author>
      <author><first>Miguel</first><last>Vera</last></author>
      <author><first>Fabio</first><last>Kepler</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>961–972</pages>
      <abstract>We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation. Our team participated on two tasks: Direct Assessment and Post-Editing Effort, encompassing a total of 35 submissions. For all submissions, our efforts focused on training multilingual models on top of OpenKiwi predictor-estimator architecture, using pre-trained multilingual encoders combined with adapters. We further experiment with and uncertainty-related objectives and features as well as training on out-of-domain direct assessment data.</abstract>
      <url hash="7e30fbd5">2021.wmt-1.102</url>
      <bibkey>zerva-etal-2021-ist</bibkey>
    </paper>
    <paper id="103">
      <title>The <fixed-case>IICT</fixed-case>-Yverdon System for the <fixed-case>WMT</fixed-case> 2021 Unsupervised <fixed-case>MT</fixed-case> and Very Low Resource Supervised <fixed-case>MT</fixed-case> Task</title>
      <author><first>Àlex R.</first><last>Atrio</last></author>
      <author><first>Gabriel</first><last>Luthier</last></author>
      <author><first>Axel</first><last>Fahy</last></author>
      <author><first>Giorgos</first><last>Vernikos</last></author>
      <author><first>Andrei</first><last>Popescu-Belis</last></author>
      <author><first>Ljiljana</first><last>Dolamic</last></author>
      <pages>973–981</pages>
      <abstract>In this paper, we present the systems submitted by our team from the Institute of ICT (HEIG-VD / HES-SO) to the Unsupervised MT and Very Low Resource Supervised MT task. We first study the improvements brought to a baseline system by techniques such as back-translation and initialization from a parent model. We find that both techniques are beneficial and suffice to reach performance that compares with more sophisticated systems from the 2020 task. We then present the application of this system to the 2021 task for low-resource supervised Upper Sorbian (HSB) to German translation, in both directions. Finally, we present a contrastive system for HSB-DE in both directions, and for unsupervised German to Lower Sorbian (DSB) translation, which uses multi-task training with various training schedules to improve over the baseline.</abstract>
      <url hash="e1e21724">2021.wmt-1.103</url>
      <bibkey>atrio-etal-2021-iict</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="104">
      <title>Unsupervised Translation of <fixed-case>G</fixed-case>erman–<fixed-case>L</fixed-case>ower <fixed-case>S</fixed-case>orbian: Exploring Training and Novel Transfer Methods on a Low-Resource Language</title>
      <author><first>Lukas</first><last>Edman</last></author>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <pages>982–988</pages>
      <abstract>This paper describes the methods behind the systems submitted by the University of Groningen for the WMT 2021 Unsupervised Machine Translation task for German–Lower Sorbian (DE–DSB): a high-resource language to a low-resource one. Our system uses a transformer encoder-decoder architecture in which we make three changes to the standard training procedure. First, our training focuses on two languages at a time, contrasting with a wealth of research on multilingual systems. Second, we introduce a novel method for initializing the vocabulary of an unseen language, achieving improvements of 3.2 BLEU for DE-&gt;DSB and 4.0 BLEU for DSB-&gt;DE.Lastly, we experiment with the order in which offline and online back-translation are used to train an unsupervised system, finding that using online back-translation first works better for DE-&gt;DSB by 2.76 BLEU. Our submissions ranked first (tied with another team) for DSB-&gt;DE and third for DE-&gt;DSB.</abstract>
      <url hash="ef576619">2021.wmt-1.104</url>
      <bibkey>edman-etal-2021-unsupervised</bibkey>
    </paper>
    <paper id="105">
      <title>The <fixed-case>LMU</fixed-case> <fixed-case>M</fixed-case>unich Systems for the <fixed-case>WMT</fixed-case>21 Unsupervised and Very Low-Resource Translation Task</title>
      <author><first>Jindřich</first><last>Libovický</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>989–994</pages>
      <abstract>We present our submissions to the WMT21 shared task in Unsupervised and Very Low Resource machine translation between German and Upper Sorbian, German and Lower Sorbian, and Russian and Chuvash. Our low-resource systems (German↔Upper Sorbian, Russian↔Chuvash) are pre-trained on high-resource pairs of related languages. We fine-tune those systems using the available authentic parallel data and improve by iterated back-translation. The unsupervised German↔Lower Sorbian system is initialized by the best Upper Sorbian system and improved by iterated back-translation using monolingual data only.</abstract>
      <url hash="a3e9bd63">2021.wmt-1.105</url>
      <bibkey>libovicky-fraser-2021-lmu</bibkey>
    </paper>
    <paper id="106">
      <title>Language Model Pretraining and Transfer Learning for Very Low Resource Languages</title>
      <author><first>Jyotsana</first><last>Khatri</last></author>
      <author><first>Rudra</first><last>Murthy</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>995–998</pages>
      <abstract>This paper describes our submission for the shared task on Unsupervised MT and Very Low Resource Supervised MT at WMT 2021. We submitted systems for two language pairs: German ↔ Upper Sorbian (de ↔ hsb) and German-Lower Sorbian (de ↔ dsb). For de ↔ hsb, we pretrain our system using MASS (Masked Sequence to Sequence) objective and then finetune using iterative back-translation. Final finetunng is performed using the parallel data provided for translation objective. For de ↔ dsb, no parallel data is provided in the task, we use final de ↔ hsb model as initialization of the de ↔ dsb model and train it further using iterative back-translation, using the same vocabulary as used in the de ↔ hsb model.</abstract>
      <url hash="5b37e67e">2021.wmt-1.106</url>
      <bibkey>khatri-etal-2021-language-model</bibkey>
    </paper>
    <paper id="107">
      <title><fixed-case>NRC</fixed-case>-<fixed-case>CNRC</fixed-case> Systems for <fixed-case>U</fixed-case>pper <fixed-case>S</fixed-case>orbian-<fixed-case>G</fixed-case>erman and <fixed-case>L</fixed-case>ower <fixed-case>S</fixed-case>orbian-<fixed-case>G</fixed-case>erman Machine Translation 2021</title>
      <author><first>Rebecca</first><last>Knowles</last></author>
      <author><first>Samuel</first><last>Larkin</last></author>
      <pages>999–1008</pages>
      <abstract>We describe our neural machine translation systems for the 2021 shared task on Unsupervised and Very Low Resource Supervised MT, translating between Upper Sorbian and German (low-resource) and between Lower Sorbian and German (unsupervised). The systems incorporated data filtering, backtranslation, BPE-dropout, ensembling, and transfer learning from high(er)-resource languages. As measured by automatic metrics, our systems showed strong performance, consistently placing first or tied for first across most metrics and translation directions.</abstract>
      <url hash="020a7ca9">2021.wmt-1.107</url>
      <bibkey>knowles-larkin-2021-nrc</bibkey>
    </paper>
    <paper id="108">
      <title><fixed-case>N</fixed-case>oah<fixed-case>NMT</fixed-case> at <fixed-case>WMT</fixed-case> 2021: Dual Transfer for Very Low Resource Supervised Machine Translation</title>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Pengfei</first><last>Li</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>1009–1013</pages>
      <abstract>This paper describes the NoahNMT system submitted to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation. The system is a standard Transformer model equipped with our recent technique of dual transfer. It also employs widely used techniques that are known to be helpful for neural machine translation, including iterative back-translation, selected finetuning, and ensemble. The final submission achieves the top BLEU for three translation directions.</abstract>
      <url hash="a3b3d8ea">2021.wmt-1.108</url>
      <bibkey>zhang-etal-2021-noahnmt</bibkey>
    </paper>
    <paper id="109">
      <title>cush<fixed-case>LEPOR</fixed-case>: customising h<fixed-case>LEPOR</fixed-case> metric using Optuna for higher agreement with human judgments or pre-trained language model <fixed-case>L</fixed-case>a<fixed-case>BSE</fixed-case></title>
      <author><first>Lifeng</first><last>Han</last></author>
      <author><first>Irina</first><last>Sorokina</last></author>
      <author><first>Gleb</first><last>Erofeev</last></author>
      <author><first>Serge</first><last>Gladkoff</last></author>
      <pages>1014–1023</pages>
      <abstract>Human evaluation has always been expensive while researchers struggle to trust the automatic metrics. To address this, we propose to customise traditional metrics by taking advantages of the pre-trained language models (PLMs) and the limited available human labelled scores. We first re-introduce the hLEPOR metric factors, followed by the Python version we developed (ported) which achieved the automatic tuning of the weighting parameters in hLEPOR metric. Then we present the customised hLEPOR (cushLEPOR) which uses Optuna hyper-parameter optimisation framework to fine-tune hLEPOR weighting parameters towards better agreement to pre-trained language models (using LaBSE) regarding the exact MT language pairs that cushLEPOR is deployed to. We also optimise cushLEPOR towards professional human evaluation data based on MQM and pSQM framework on English-German and Chinese-English language pairs. The experimental investigations show cushLEPOR boosts hLEPOR performances towards better agreements to PLMs like LABSE with much lower cost, and better agreements to human evaluations including MQM and pSQM scores, and yields much better performances than BLEU. Official results show that our submissions win three language pairs including English-German and Chinese-English on News domain via cushLEPOR(LM) and English-Russian on TED domain via hLEPOR. (data available at https://github.com/poethan/cushLEPOR)</abstract>
      <url hash="83648184">2021.wmt-1.109</url>
      <attachment type="Software" hash="92ea3580">2021.wmt-1.109.Software.zip</attachment>
      <bibkey>han-etal-2021-cushlepor</bibkey>
      <pwccode url="https://github.com/poethan/cushLEPOR" additional="false">poethan/cushLEPOR</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="110">
      <title><fixed-case>MTEQA</fixed-case> at <fixed-case>WMT</fixed-case>21 Metrics Shared Task</title>
      <author><first>Mateusz</first><last>Krubiński</last></author>
      <author><first>Erfan</first><last>Ghadery</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <author><first>Pavel</first><last>Pecina</last></author>
      <pages>1024–1029</pages>
      <abstract>In this paper, we describe our submission to the WMT 2021 Metrics Shared Task. We use the automatically-generated questions and answers to evaluate the quality of Machine Translation (MT) systems. Our submission builds upon the recently proposed MTEQA framework. Experiments on WMT20 evaluation datasets show that at the system-level the MTEQA metric achieves performance comparable with other state-of-the-art solutions, while considering only a certain amount of information from the whole translation.</abstract>
      <url hash="c11c991b">2021.wmt-1.110</url>
      <bibkey>krubinski-etal-2021-mteqa</bibkey>
    </paper>
    <paper id="111">
      <title>Are References Really Needed? Unbabel-<fixed-case>IST</fixed-case> 2021 Submission for the Metrics Shared Task</title>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>Ana C</first><last>Farinha</last></author>
      <author><first>Chrysoula</first><last>Zerva</last></author>
      <author><first>Daan</first><last>van Stigt</last></author>
      <author><first>Craig</first><last>Stewart</last></author>
      <author><first>Pedro</first><last>Ramos</last></author>
      <author><first>Taisiya</first><last>Glushkova</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <author><first>Alon</first><last>Lavie</last></author>
      <pages>1030–1040</pages>
      <abstract>In this paper, we present the joint contribution of Unbabel and IST to the WMT 2021 Metrics Shared Task. With this year’s focus on Multidimensional Quality Metric (MQM) as the ground-truth human assessment, our aim was to steer COMET towards higher correlations with MQM. We do so by first pre-training on Direct Assessments and then fine-tuning on z-normalized MQM scores. In our experiments we also show that reference-free COMET models are becoming competitive with reference-based models, even outperforming the best COMET model from 2020 on this year’s development data. Additionally, we present COMETinho, a lightweight COMET model that is 19x faster on CPU than the original model, while also achieving state-of-the-art correlations with MQM. Finally, in the “QE as a metric” track, we also participated with a QE model trained using the OpenKiwi framework leveraging MQM scores and word-level annotations.</abstract>
      <url hash="f4b2bcd4">2021.wmt-1.111</url>
      <bibkey>rei-etal-2021-references</bibkey>
      <pwccode url="https://github.com/Unbabel/COMET" additional="false">Unbabel/COMET</pwccode>
    </paper>
    <paper id="112">
      <title>Regressive Ensemble for Machine Translation Quality Evaluation</title>
      <author><first>Michal</first><last>Stefanik</last></author>
      <author><first>Vít</first><last>Novotný</last></author>
      <author><first>Petr</first><last>Sojka</last></author>
      <pages>1041–1048</pages>
      <abstract>This work introduces a simple regressive ensemble for evaluating machine translation quality based on a set of novel and established metrics. We evaluate the ensemble using a correlation to expert-based MQM scores of the WMT 2021 Metrics workshop. In both monolingual and zero-shot cross-lingual settings, we show a significant performance improvement over single metrics. In the cross-lingual settings, we also demonstrate that an ensemble approach is well-applicable to unseen languages. Furthermore, we identify a strong reference-free baseline that consistently outperforms the commonly-used BLEU and METEOR measures and significantly improves our ensemble’s performance.</abstract>
      <url hash="e2ac5219">2021.wmt-1.112</url>
      <bibkey>stefanik-etal-2021-regressive</bibkey>
      <pwccode url="https://github.com/mir-mu/regemt" additional="false">mir-mu/regemt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe-pe">MLQE-PE</pwcdataset>
    </paper>
    <paper id="113">
      <title>Multilingual Machine Translation Evaluation Metrics Fine-tuned on Pseudo-Negative Examples for <fixed-case>WMT</fixed-case> 2021 Metrics Task</title>
      <author><first>Kosuke</first><last>Takahashi</last></author>
      <author><first>Yoichi</first><last>Ishibashi</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>1049–1052</pages>
      <abstract>This paper describes our submission to the WMT2021 shared metrics task. Our metric is operative to segment-level and system-level translations. Our belief toward a better metric is to detect a significant error that cannot be missed in the real practice cases of evaluation. For that reason, we used pseudo-negative examples in which attributes of some words are transferred to the reversed attribute words, and we build evaluation models to handle such serious mistakes of translations. We fine-tune a multilingual largely pre-trained model on the provided corpus of past years’ metric task and fine-tune again further on the synthetic negative examples that are derived from the same fine-tune corpus. From the evaluation results of the WMT21’s development corpus, fine-tuning on the pseudo-negatives using WMT15-17 and WMT18-20 metric corpus achieved a better Pearson’s correlation score than the one fine-tuned without negative examples. Our submitted models,hyp+src_hyp+ref and hyp+src_hyp+ref.negative, are the plain model using WMT18-20 and the one additionally fine-tuned on negative samples, respectively.</abstract>
      <url hash="99d361ea">2021.wmt-1.113</url>
      <bibkey>takahashi-etal-2021-multilingual</bibkey>
    </paper>
    <paper id="114">
      <title><fixed-case>R</fixed-case>o<fixed-case>BLEURT</fixed-case> Submission for <fixed-case>WMT</fixed-case>2021 Metrics Task</title>
      <author><first>Yu</first><last>Wan</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Tianchi</first><last>Bi</last></author>
      <author><first>Haibo</first><last>Zhang</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Weihua</first><last>Luo</last></author>
      <author><first>Derek F.</first><last>Wong</last></author>
      <author><first>Lidia S.</first><last>Chao</last></author>
      <pages>1053–1058</pages>
      <abstract>In this paper, we present our submission to Shared Metrics Task: RoBLEURT (Robustly Optimizing the training of BLEURT). After investigating the recent advances of trainable metrics, we conclude several aspects of vital importance to obtain a well-performed metric model by: 1) jointly leveraging the advantages of source-included model and reference-only model, 2) continuously pre-training the model with massive synthetic data pairs, and 3) fine-tuning the model with data denoising strategy. Experimental results show that our model reaching state-of-the-art correlations with the WMT2020 human annotations upon 8 out of 10 to-English language pairs.</abstract>
      <url hash="da27bdd4">2021.wmt-1.114</url>
      <bibkey>wan-etal-2021-robleurt</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2020">WMT 2020</pwcdataset>
    </paper>
    <paper id="115">
      <title>Linguistic Evaluation for the 2021 State-of-the-art Machine Translation Systems for <fixed-case>G</fixed-case>erman to <fixed-case>E</fixed-case>nglish and <fixed-case>E</fixed-case>nglish to <fixed-case>G</fixed-case>erman</title>
      <author><first>Vivien</first><last>Macketanz</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <author><first>Shushen</first><last>Manakhimova</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>1059–1073</pages>
      <abstract>We are using a semi-automated test suite in order to provide a fine-grained linguistic evaluation for state-of-the-art machine translation systems. The evaluation includes 18 German to English and 18 English to German systems, submitted to the Translation Shared Task of the 2021 Conference on Machine Translation. Our submission adds up to the submissions of the previous years by creating and applying a wide-range test suite for English to German as a new language pair. The fine-grained evaluation allows spotting significant differences between systems that cannot be distinguished by the direct assessment of the human evaluation campaign. We find that most of the systems achieve good accuracies in the majority of linguistic phenomena but there are few phenomena with lower accuracy, such as the idioms, the modal pluperfect and the German resultative predicates. Two systems have significantly better test suite accuracy in macro-average in every language direction, Online-W and Facebook-AI for German to English and VolcTrans and Online-W for English to German. The systems show a steady improvement as compared to previous years.</abstract>
      <url hash="e4f0d048">2021.wmt-1.115</url>
      <bibkey>macketanz-etal-2021-linguistic</bibkey>
    </paper>
    <paper id="116">
      <title>Pruning Neural Machine Translation for Speed Using Group Lasso</title>
      <author><first>Maximiliana</first><last>Behnke</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>1074–1086</pages>
      <abstract>Unlike most work on pruning neural networks, we make inference faster. Group lasso regularisation enables pruning entire rows, columns or blocks of parameters that result in a smaller dense network. Because the network is still dense, efficient matrix multiply routines are still used and only minimal software changes are required to support variable layer sizes. Moreover, pruning is applied during training so there is no separate pruning step. Experiments on top of English-&gt;German models, which already have state-of-the-art speed and size, show that two-thirds of feedforward connections can be removed with 0.2 BLEU loss. With 6 decoder layers, the pruned model is 34% faster; with 2 tied decoder layers, the pruned model is 14% faster. Pruning entire heads and feedforward connections in a 12–1 encoder-decoder architecture gains an additional 51% speed-up. These push the Pareto frontier with respect to the trade-off between time and quality compared to strong baselines. In the WMT 2021 Efficiency Task, our pruned and quantised models are 1.9–2.7x faster at the cost 0.9–1.7 BLEU in comparison to the unoptimised baselines. Across language pairs, we see similar sparsity patterns: an ascending or U-shaped distribution in encoder feedforward and attention layers and an ascending distribution in the decoder.</abstract>
      <url hash="cabf66be">2021.wmt-1.116</url>
      <bibkey>behnke-heafield-2021-pruning</bibkey>
      <video href="2021.wmt-1.116.mp4"/>
    </paper>
    <paper id="117">
      <title>Phrase-level Active Learning for Neural Machine Translation</title>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>1087–1099</pages>
      <abstract>Neural machine translation (NMT) is sensitive to domain shift. In this paper, we address this problem in an active learning setting where we can spend a given budget on translating in-domain data, and gradually fine-tune a pre-trained out-of-domain NMT model on the newly translated data. Existing active learning methods for NMT usually select sentences based on uncertainty scores, but these methods require costly translation of full sentences even when only one or two key phrases within the sentence are informative. To address this limitation, we re-examine previous work from the phrase-based machine translation (PBMT) era that selected not full sentences, but rather individual phrases. However, while incorporating these phrases into PBMT systems was relatively simple, it is less trivial for NMT systems, which need to be trained on full sequences to capture larger structural properties of sentences unique to the new domain. To overcome these hurdles, we propose to select both full sentences and individual phrases from unlabelled data in the new domain for routing to human translators. In a German-English translation task, our active learning approach achieves consistent improvements over uncertainty-based sentence selection methods, improving up to 1.2 BLEU score over strong active learning baselines.</abstract>
      <url hash="06a09b55">2021.wmt-1.117</url>
      <bibkey>hu-neubig-2021-phrase</bibkey>
      <video href="2021.wmt-1.117.mp4"/>
    </paper>
    <paper id="118">
      <title>Learning Feature Weights using Reward Modeling for Denoising Parallel Corpora</title>
      <author><first>Gaurav</first><last>Kumar</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Sanjeev</first><last>Khudanpur</last></author>
      <pages>1100–1109</pages>
      <abstract>Large web-crawled corpora represent an excellent resource for improving the performance of Neural Machine Translation (NMT) systems across several language pairs. However, since these corpora are typically extremely noisy, their use is fairly limited. Current approaches to deal with this problem mainly focus on filtering using heuristics or single features such as language model scores or bi-lingual similarity. This work presents an alternative approach which learns weights for multiple sentence-level features. These feature weights which are optimized directly for the task of improving translation performance, are used to score and filter sentences in the noisy corpora more effectively. We provide results of applying this technique to building NMT systems using the Paracrawl corpus for Estonian-English and show that it beats strong single feature baselines and hand designed combinations. Additionally, we analyze the sensitivity of this method to different types of noise and explore if the learned weights generalize to other language pairs using the Maltese-English Paracrawl corpus.</abstract>
      <url hash="733fefb0">2021.wmt-1.118</url>
      <bibkey>kumar-etal-2021-learning-feature</bibkey>
      <video href="2021.wmt-1.118.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="119">
      <title>Monotonic Simultaneous Translation with Chunk-wise Reordering and Refinement</title>
      <author><first>HyoJung</first><last>Han</last></author>
      <author><first>Seokchan</first><last>Ahn</last></author>
      <author><first>Yoonjung</first><last>Choi</last></author>
      <author><first>Insoo</first><last>Chung</last></author>
      <author><first>Sangha</first><last>Kim</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <pages>1110–1123</pages>
      <abstract>Recent work in simultaneous machine translation is often trained with conventional full sentence translation corpora, leading to either excessive latency or necessity to anticipate as-yet-unarrived words, when dealing with a language pair whose word orders significantly differ. This is unlike human simultaneous interpreters who produce largely monotonic translations at the expense of the grammaticality of a sentence being translated. In this paper, we thus propose an algorithm to reorder and refine the target side of a full sentence translation corpus, so that the words/phrases between the source and target sentences are aligned largely monotonically, using word alignment and non-autoregressive neural machine translation. We then train a widely used wait-k simultaneous translation model on this reordered-and-refined corpus. The proposed approach improves BLEU scores and resulting translations exhibit enhanced monotonicity with source sentences.</abstract>
      <url hash="5ee86fdf">2021.wmt-1.119</url>
      <bibkey>han-etal-2021-monotonic</bibkey>
      <video href="2021.wmt-1.119.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="120">
      <title>Simultaneous Neural Machine Translation with Constituent Label Prediction</title>
      <author><first>Yasumasa</first><last>Kano</last></author>
      <author><first>Katsuhito</first><last>Sudoh</last></author>
      <author><first>Satoshi</first><last>Nakamura</last></author>
      <pages>1124–1134</pages>
      <abstract>Simultaneous translation is a task in which translation begins before the speaker has finished speaking, so it is important to decide when to start the translation process. However, deciding whether to read more input words or start to translate is difficult for language pairs with different word orders such as English and Japanese. Motivated by the concept of pre-reordering, we propose a couple of simple decision rules using the label of the next constituent predicted by incremental constituent label prediction. In experiments on English-to-Japanese simultaneous translation, the proposed method outperformed baselines in the quality-latency trade-off.</abstract>
      <url hash="bf9001e2">2021.wmt-1.120</url>
      <bibkey>kano-etal-2021-simultaneous</bibkey>
      <video href="2021.wmt-1.120.mp4"/>
    </paper>
    <paper id="121">
      <title>Contrastive Learning for Context-aware Neural Machine Translation Using Coreference Information</title>
      <author><first>Yongkeun</first><last>Hwang</last></author>
      <author><first>Hyeongu</first><last>Yun</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>1135–1144</pages>
      <abstract>Context-aware neural machine translation (NMT) incorporates contextual information of surrounding texts, that can improve the translation quality of document-level machine translation. Many existing works on context-aware NMT have focused on developing new model architectures for incorporating additional contexts and have shown some promising results. However, most of existing works rely on cross-entropy loss, resulting in limited use of contextual information. In this paper, we propose CorefCL, a novel data augmentation and contrastive learning scheme based on coreference between the source and contextual sentences. By corrupting automatically detected coreference mentions in the contextual sentence, CorefCL can train the model to be sensitive to coreference inconsistency. We experimented with our method on common context-aware NMT models and two document-level translation tasks. In the experiments, our method consistently improved BLEU of compared models on English-German and English-Korean tasks. We also show that our method significantly improves coreference resolution in the English-German contrastive test suite.</abstract>
      <url hash="c91a0faa">2021.wmt-1.121</url>
      <bibkey>hwang-etal-2021-contrastive</bibkey>
      <video href="2021.wmt-1.121.mp4"/>
    </paper>
  </volume>
</collection>
