<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.lt4hala">
  <volume id="1" ingest-date="2022-09-23">
    <meta>
      <booktitle>Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages</booktitle>
      <editor><first>Rachele</first><last>Sprugnoli</last></editor>
      <editor><first>Marco</first><last>Passarotti</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>June</month>
      <year>2022</year>
      <url hash="eed7fc5e">2022.lt4hala-1</url>
      <venue>lt4hala</venue>
    </meta>
    <frontmatter>
      <url hash="e3262ea4">2022.lt4hala-1.0</url>
      <bibkey>lt4hala-2022-language</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Identifying Cleartext in Historical Ciphers</title>
      <author><first>Maria-Elena</first><last>Gambardella</last></author>
      <author><first>Beata</first><last>Megyesi</last></author>
      <author><first>Eva</first><last>Pettersson</last></author>
      <pages>1–9</pages>
      <abstract>In historical encrypted sources we can find encrypted text sequences, also called ciphertext, as well as non-encrypted cleartexts written in a known language. While most of the cryptanalysis focuses on the decryption of ciphertext, cleartext is often overlooked although it can give us important clues about the historical interpretation and contextualisation of the manuscript. In this paper, we investigate to what extent we can automatically distinguish cleartext from ciphertext in historical ciphers and to what extent we are able to identify its language. The problem is challenging as cleartext sequences in ciphers are often short, up to a few words, in different languages due to historical code-switching. To identify the sequences and the language(s), we chose a rule-based approach and run 7 different models using historical language models on various ciphertexts.</abstract>
      <url hash="712f1a46">2022.lt4hala-1.1</url>
      <bibkey>gambardella-etal-2022-identifying</bibkey>
    </paper>
    <paper id="2">
      <title>Detecting Diachronic Syntactic Developments in Presence of Bias Terms</title>
      <author><first>Oliver</first><last>Hellwig</last></author>
      <author><first>Sven</first><last>Sellmer</last></author>
      <pages>10–19</pages>
      <abstract>Corpus-based studies of diachronic syntactic changes are typically guided by the results of previous qualitative research. When such results are missing or, as is the case for Vedic Sanskrit, are restricted to small parts of a transmitted corpus, an exploratory framework that detects such changes in a data-driven fashion can substantially support the research process. In this paper, we introduce a customized version of the infinite relational model that groups syntactic constituents based on their structural similarities and their diachronic distributions. We propose a simple way to control for register and intellectual affiliation, and discuss our findings for four syntactic structures in Vedic texts.</abstract>
      <url hash="3bfa81ab">2022.lt4hala-1.2</url>
      <bibkey>hellwig-sellmer-2022-detecting</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="3">
      <title>Accurate Dependency Parsing and Tagging of <fixed-case>L</fixed-case>atin</title>
      <author><first>Sebastian</first><last>Nehrdich</last></author>
      <author><first>Oliver</first><last>Hellwig</last></author>
      <pages>20–25</pages>
      <abstract>Having access to high-quality grammatical annotations is important for downstream tasks in NLP as well as for corpus-based research. In this paper, we describe experiments with the Latin BERT word embeddings that were recently be made available by Bamman and Burns (2020). We show that these embeddings produce competitive results in the low-level task morpho-syntactic tagging. In addition, we describe a graph-based dependency parser that is trained with these embeddings and that clearly outperforms various baselines.</abstract>
      <url hash="fd12bcdc">2022.lt4hala-1.3</url>
      <bibkey>nehrdich-hellwig-2022-accurate</bibkey>
    </paper>
    <paper id="4">
      <title>Annotating “Absolute” Preverbs in the <fixed-case>H</fixed-case>omeric and <fixed-case>V</fixed-case>edic Treebanks</title>
      <author><first>Luca</first><last>Brigada Villa</last></author>
      <author><first>Erica</first><last>Biagetti</last></author>
      <author><first>Chiara</first><last>Zanchi</last></author>
      <pages>26–30</pages>
      <abstract>Indo-European preverbs are uninflected morphemes attaching to verbs and modifying their meaning. In Early Vedic and Homeric Greek, these morphemes held ambiguous morphosyntactic status raising issues for syntactic annotation. This paper focuses on the annotation of preverbs in so-called “absolute” position in two Universal Dependencies treebanks. This issue is related to the broader topic of how to annotate ellipsis in Universal Dependencies. After discussing some of the current annotations, we propose a new scheme that better accounts for the variety of absolute constructions.</abstract>
      <url hash="1256ab27">2022.lt4hala-1.4</url>
      <bibkey>brigada-villa-etal-2022-annotating</bibkey>
      <pwccode url="https://github.com/unipv-larl/preverbs" additional="false">unipv-larl/preverbs</pwccode>
    </paper>
    <paper id="5">
      <title><fixed-case>CHJ</fixed-case>-<fixed-case>WLSP</fixed-case>: Annotation of ‘Word List by Semantic Principles’ Labels for the Corpus of Historical <fixed-case>J</fixed-case>apanese</title>
      <author><first>Masayuki</first><last>Asahara</last></author>
      <author><first>Nao</first><last>Ikegami</last></author>
      <author><first>Tai</first><last>Suzuki</last></author>
      <author><first>Taro</first><last>Ichimura</last></author>
      <author><first>Asuko</first><last>Kondo</last></author>
      <author><first>Sachi</first><last>Kato</last></author>
      <author><first>Makoto</first><last>Yamazaki</last></author>
      <pages>31–37</pages>
      <abstract>This article presents a word-sense annotation for the Corpus of Historical Japanese: a mashed-up Japanese lexicon based on the ‘Word List by Semantic Principles’ (WLSP). The WLSP is a large-scale Japanese thesaurus that includes 98,241 entries with syntactic and hierarchical semantic categories. The historical WLSP is also compiled for the words in ancient Japanese. We utilized a morpheme-word sense alignment table to extract all possible word sense candidates for each word appearing in the target corpus. Then, we manually disambiguated the word senses for 647,751 words in the texts from the 10th century to 1910.</abstract>
      <url hash="32178a36">2022.lt4hala-1.5</url>
      <bibkey>asahara-etal-2022-chj</bibkey>
    </paper>
    <paper id="6">
      <title>The <fixed-case>IKUVINA</fixed-case> Treebank</title>
      <author><first>Mathieu</first><last>Dehouck</last></author>
      <pages>38–42</pages>
      <abstract>In this paper, we introduce the first dependency treebank for the Umbrian language (an extinct Indo-European language from the Italic family, once spoken in modern day Italy). We present the source of the corpus : a set of seven bronze tablets describing religious ceremonies, written using two different scripts, unearthed in Umbria in the XVth century. The corpus itself has already been studied extensively by specialists of old Italic and classical Indo-European languages. So we discuss a number of challenges that we encountered as we annotated the corpus following Universal Dependencies’ guidelines from existing textual analyses.</abstract>
      <url hash="50d4753b">2022.lt4hala-1.6</url>
      <bibkey>dehouck-2022-ikuvina</bibkey>
    </paper>
    <paper id="7">
      <title>Machine Translation of 16<fixed-case>T</fixed-case>h Century Letters from <fixed-case>L</fixed-case>atin to <fixed-case>G</fixed-case>erman</title>
      <author><first>Lukas</first><last>Fischer</last></author>
      <author><first>Patricia</first><last>Scheurer</last></author>
      <author><first>Raphael</first><last>Schwitter</last></author>
      <author><first>Martin</first><last>Volk</last></author>
      <pages>43–50</pages>
      <abstract>This paper outlines our work in collecting training data for and developing a Latin–German Neural Machine Translation (NMT) system, for translating 16th century letters. While Latin–German is a low-resource language pair in terms of NMT, the domain of 16th century epistolary Latin is even more limited in this regard. Through our efforts in data collection and data generation, we are able to train a NMT model that provides good translations for short to medium sentences, and outperforms GoogleTranslate overall. We focus on the correspondence of the Swiss reformer Heinrich Bullinger, but our parallel corpus and our NMT system will be of use for many other texts of the time.</abstract>
      <url hash="8caab692">2022.lt4hala-1.7</url>
      <bibkey>fischer-etal-2022-machine</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="8">
      <title>A Treebank-based Approach to the Supprema Constructio in Dante’s <fixed-case>L</fixed-case>atin Works</title>
      <author><first>Flavio Massimiliano</first><last>Cecchini</last></author>
      <author><first>Giulia</first><last>Pedonese</last></author>
      <pages>51–58</pages>
      <abstract>This paper aims to apply a corpus-driven approach to Dante Alighieri’s Latin works using UDante, a treebank based on Dante Search and part of the Universal Dependencies project. We present a method based on the notion of barycentre applied to a dependency tree as a way to calculate the “syntactic balance” of a sentence. Its application to Dante’s Latin works shows its potential in analysing the style of an author, and contributes to the interpretation of the supprema constructio mentioned in DVE II vi 7 as a well balanced syntactic pattern modeled on Latin literary writing.</abstract>
      <url hash="15b7dd16">2022.lt4hala-1.8</url>
      <bibkey>cecchini-pedonese-2022-treebank</bibkey>
      <pwccode url="https://github.com/stormur/dantesuppremaconstructio" additional="false">stormur/dantesuppremaconstructio</pwccode>
    </paper>
    <paper id="9">
      <title>From Inscriptions to Lexica and Back: A Platform for Editing and Linking the Languages of <fixed-case>A</fixed-case>ncient <fixed-case>I</fixed-case>taly</title>
      <author><first>Valeria</first><last>Quochi</last></author>
      <author><first>Andrea</first><last>Bellandi</last></author>
      <author><first>Fahad</first><last>Khan</last></author>
      <author><first>Michele</first><last>Mallia</last></author>
      <author><first>Francesca</first><last>Murano</last></author>
      <author><first>Silvia</first><last>Piccini</last></author>
      <author><first>Luca</first><last>Rigobianco</last></author>
      <author><first>Alessandro</first><last>Tommasi</last></author>
      <author><first>Cesare</first><last>Zavattari</last></author>
      <pages>59–67</pages>
      <abstract>Available language technology is hardly applicable to scarcely attested ancient languages, yet their digital semantic representation, though challenging, is an asset for the purpose of sharing and preserving existing cultural knowledge. In the context of a project on the languages and cultures of ancient Italy, we took up this challenge. The paper thus describes the development of a user friendly web platform, EpiLexO, for the creation and editing of an integrated system of language resources for ancient fragmentary languages centered on the lexicon, in compliance with current digital humanities and Linked Open Data principles. EpiLexo allows for the editing of lexica with all relevant cross-references: for their linking to their testimonies, as well as to bibliographic information and other (external) resources and common vocabularies. The focus of the current implementation is on the languages of ancient Italy, in particular Oscan, Faliscan, Celtic and Venetic; however, the technological solutions are designed to be general enough to be potentially applicable to different scenarios.</abstract>
      <url hash="4fd1ac5b">2022.lt4hala-1.9</url>
      <bibkey>quochi-etal-2022-inscriptions</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>BERT</fixed-case>oldo, the Historical <fixed-case>BERT</fixed-case> for <fixed-case>I</fixed-case>talian</title>
      <author><first>Alessio</first><last>Palmero Aprosio</last></author>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <pages>68–72</pages>
      <abstract>Recent works in historical language processing have shown that transformer-based models can be successfully created using historical corpora, and that using them for analysing and classifying data from the past can be beneficial compared to standard transformer models. This has led to the creation of BERT-like models for different languages trained with digital repositories from the past. In this work we introduce the Italian version of historical BERT, which we call BERToldo. We evaluate the model on the task of PoS-tagging Dante Alighieri’s works, considering not only the tagger performance but also the model size and the time needed to train it. We also address the problem of duplicated data, which is rather common for languages with a limited availability of historical corpora. We show that deduplication reduces training time without affecting performance. The model and its smaller versions are all made available to the research community.</abstract>
      <url hash="ebb0f9e3">2022.lt4hala-1.10</url>
      <bibkey>palmero-aprosio-etal-2022-bertoldo</bibkey>
      <pwccode url="https://github.com/dhfbk/historical-bert" additional="false">dhfbk/historical-bert</pwccode>
    </paper>
    <paper id="11">
      <title>In Search of the Flocks: How to Perform Onomasiological Queries in an <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek Corpus?</title>
      <author><first>Alek</first><last>Keersmaekers</last></author>
      <author><first>Toon</first><last>Van Hal</last></author>
      <pages>73–83</pages>
      <abstract>This paper explores the possibilities of onomasiologically querying corpus data of Ancient Greek. The significance of the onomasiological approach has been highlighted in recent studies, yet the possibilities of performing ‘word-finding’ investigations into corpus data have not been dealt with in depth. The case study chosen focuses on collective nouns denoting animate groups (such as flocks of people, herds of cattle). By relying on a large automatically annotated corpus of Ancient Greek and on token-based vector information, a longlist of collective nouns was compiled through morpho-syntactic extraction and successive clustering procedures. After reducing this longlist to a shortlist, the results obtained are evaluated. In general, we find that πλῆθος can be considered to be the default collective noun of both humans and animals, becoming especially prominent during the Hellenistic period. In addition, specific tendencies in the use of collective nouns are discerned for specific semantic classes (e.g. gods and insects) and over time. Throughout the paper, special attention is paid to methodological issues related to onomasiologically searching.</abstract>
      <url hash="6a7c8849">2022.lt4hala-1.11</url>
      <bibkey>keersmaekers-van-hal-2022-search</bibkey>
    </paper>
    <paper id="12">
      <title>Contextual Unsupervised Clustering of Signs for Ancient Writing Systems</title>
      <author><first>Michele</first><last>Corazza</last></author>
      <author><first>Fabio</first><last>Tamburini</last></author>
      <author><first>Miguel</first><last>Valério</last></author>
      <author><first>Silvia</first><last>Ferrara</last></author>
      <pages>84–93</pages>
      <abstract>The application of machine learning techniques to ancient writing systems is a relatively new idea, and it poses interesting challenges for researchers. One particularly challenging aspect is the scarcity of data for these scripts, which contrasts with the large amounts of data usually available when applying neural models to computational linguistics and other fields. For this reason, any method that attempts to work on ancient scripts needs to be ad-hoc and consider paleographic aspects, in addition to computational ones. Considering the peculiar characteristics of the script that we used is therefore be a crucial part of our work, as any solution needs to consider the particular nature of the writing system that it is applied to. In this work we propose a preliminary evaluation of a novel unsupervised clustering method on Cypro-Greek syllabary, a writing system from Cyprus. This evaluation shows that our method improves clustering performance using information about the attested sequences of signs in combination with an unsupervised model for images, with the future goal of applying the methodology to undeciphered writing systems from a related and typologically similar script.</abstract>
      <url hash="66402c70">2022.lt4hala-1.12</url>
      <bibkey>corazza-etal-2022-contextual</bibkey>
    </paper>
    <paper id="13">
      <title>Towards the Creation of a Diachronic Corpus for <fixed-case>I</fixed-case>talian: A Case Study on the <fixed-case>GDLI</fixed-case> Quotations</title>
      <author><first>Manuel</first><last>Favaro</last></author>
      <author><first>Elisa</first><last>Guadagnini</last></author>
      <author><first>Eva</first><last>Sassolini</last></author>
      <author><first>Marco</first><last>Biffi</last></author>
      <author><first>Simonetta</first><last>Montemagni</last></author>
      <pages>94–100</pages>
      <abstract>In this paper we describe some experiments related to a corpus derived from an authoritative historical Italian dictionary, namely the Grande dizionario della lingua italiana (‘Great Dictionary of Italian Language’, in short GDLI). Thanks to the digitization and structuring of this dictionary, we have been able to set up the first nucleus of a diachronic annotated corpus that selects—according to specific criteria, and distinguishing between prose and poetry—some of the quotations that within the entries illustrate the different definitions and sub-definitions. In fact, the GDLI presents a huge collection of quotations covering the entire history of the Italian language and thus ranging from the Middle Ages to the present day. The corpus was enriched with linguistic annotation and used to train and evaluate NLP models for POS tagging and lemmatization, with promising results.</abstract>
      <url hash="5959f22f">2022.lt4hala-1.13</url>
      <bibkey>favaro-etal-2022-towards</bibkey>
    </paper>
    <paper id="14">
      <title>Automatic Translation Alignment for <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek and <fixed-case>L</fixed-case>atin</title>
      <author><first>Tariq</first><last>Yousef</last></author>
      <author><first>Chiara</first><last>Palladino</last></author>
      <author><first>David J.</first><last>Wright</last></author>
      <author><first>Monica</first><last>Berti</last></author>
      <pages>101–107</pages>
      <abstract>This paper presents the results of automatic translation alignment experiments on a corpus of texts in Ancient Greek translated into Latin. We used a state-of-the-art alignment workflow based on a contextualized multilingual language model that is fine-tuned on the alignment task for Ancient Greek and Latin. The performance of the alignment model is evaluated on an alignment gold standard consisting of 100 parallel fragments aligned manually by two domain experts, with a 90.5% Inter-Annotator-Agreement (IAA). An interactive online interface is provided to enable users to explore the aligned fragments collection and examine the alignment model’s output.</abstract>
      <url hash="0c691b63">2022.lt4hala-1.14</url>
      <bibkey>yousef-etal-2022-automatic-translation</bibkey>
      <pwccode url="https://github.com/ugaritalignment/alignment-gold-standards" additional="false">ugaritalignment/alignment-gold-standards</pwccode>
    </paper>
    <paper id="15">
      <title>Handling Stress in Finite-State Morphological Analyzers for <fixed-case>A</fixed-case>ncient <fixed-case>G</fixed-case>reek and <fixed-case>A</fixed-case>ncient <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Daniel</first><last>Swanson</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <pages>108–113</pages>
      <abstract>Modeling stress placement has historically been a challenge for computational morphological analysis, especially in finite-state systems because lexically conditioned stress cannot be modeled using only rewrite rules on the phonological form of a word. However, these phenomena can be modeled fairly easily if the lexicon’s internal representation is allowed to contain more information than the pure phonological form. In this paper we describe the stress systems of Ancient Greek and Ancient Hebrew and we present two prototype finite-state morphological analyzers, one for each language, which successfully implement these stress systems by inserting a small number of control characters into the phonological form, thus conclusively refuting the claim that finite-state systems are not powerful enough to model such stress systems and arguing in favor of the continued relevance of finite-state systems as an appropriate tool for modeling the morphology of historical languages.</abstract>
      <url hash="6d11f9e2">2022.lt4hala-1.15</url>
      <bibkey>swanson-tyers-2022-handling</bibkey>
    </paper>
    <paper id="16">
      <title>From Inscription to Semi-automatic Annotation of <fixed-case>M</fixed-case>aya Hieroglyphic Texts</title>
      <author><first>Cristina</first><last>Vertan</last></author>
      <author><first>Christian</first><last>Prager</last></author>
      <pages>114–118</pages>
      <abstract>The Maya script is the only readable autochthonous writing system of the Americas and consists of more than 1000 word signs and syllables. It is only partially deciphered and is the subject of the project “Text Database and Dictionary of the Classic Maya” . Texts are recorded in TEI XML and on the basis of a digital sign and graph catalog, which are stored in the TextGrid virtual repository. Due to the state of decipherment, it is not possible to record hieroglyphic texts directly in phonemically transliterated values. The texts are therefore documented numerically using numeric sign codes based on Eric Thompson’s catalog of the Maya script. The workflow for converting numerical transliteration into textual form involves several steps, with variable solutions possible at each step. For this purpose, the authors have developed ALMAH “Annotator for the Linguistic Analysis of Maya Hieroglyphs”. The tool is a client application and allows semi-automatic generation of phonemic transliteration from numerical transliteration and enables multi-step linguistic annotation. Alternative readings can be entered, and two or more decipherment proposals can be processed in parallel. ALMAH is implemented in JAVA, is based on a graph-data model, and has a user-friendly interface.</abstract>
      <url hash="858af0da">2022.lt4hala-1.16</url>
      <bibkey>vertan-prager-2022-inscription</bibkey>
    </paper>
    <paper id="17">
      <title>Multilingual Named Entity Recognition for Medieval Charters Using Stacked Embeddings and Bert-based Models.</title>
      <author><first>Sergio</first><last>Torres Aguilar</last></author>
      <pages>119–128</pages>
      <abstract>In recent years the availability of medieval charter texts has increased thanks to advances in OCR and HTR techniques. But the lack of models that automatically structure the textual output continues to hinder the extraction of large-scale lectures from these historical sources that are among the most important for medieval studies. This paper presents the process of annotating and modelling a corpus to automatically detect named entities in medieval charters in Latin, French and Spanish and address the problem of multilingual writing practices in the Late Middle Ages. It introduces a new annotated multilingual corpus and presents a training pipeline using two approaches: (1) a method using contextual and static embeddings coupled to a Bi-LSTM-CRF classifier; (2) a fine-tuning method using the pre-trained multilingual BERT and RoBERTa models. The experiments described here are based on a corpus encompassing about 2.3M words (7576 charters) coming from five charter collections ranging from the 10th to the 15th centuries. The evaluation proves that both multilingual classifiers based on general purpose models and those specifically designed achieve high-performance results and do not show performance drop compared to their monolingual counterparts. This paper describes the corpus and the annotation guideline, and discusses the issues related to the linguistic of the charters, the multilingual writing practices, so as to interpret the results within a larger historical perspective.</abstract>
      <url hash="4b9b7cf1">2022.lt4hala-1.17</url>
      <bibkey>torres-aguilar-2022-multilingual</bibkey>
    </paper>
    <paper id="18">
      <title>Linguistic Annotation of Neo-<fixed-case>L</fixed-case>atin Mathematical Texts: A Pilot-Study to Improve the Automatic Parsing of the Archimedes Latinus</title>
      <author><first>Margherita</first><last>Fantoli</last></author>
      <author><first>Miryam</first><last>de Lhoneux</last></author>
      <pages>129–134</pages>
      <abstract>This paper describes the process of syntactically parsing the Latin translation by Jacopo da San Cassiano of the Greek mathematical work The Spirals of Archimedes. The Universal Dependencies formalism is adopted. First, we introduce the historical and linguistic importance of Jacopo da San Cassiano’s translation. Subsequently, we describe the deep Biaffine parser used for this pilot study. In particular, we motivate the choice of using the technique of treebank embeddings in light of the characteristics of mathematical texts. The paper then details the process of creation of training and test data, by highlighting the most compelling linguistic features of the text and the choices implemented in the current version of the treebank. Finally, the results of the parsing are discussed in comparison to a baseline and the most prominent errors are discussed. Overall, the paper shows the added value of creating specific training data, and of using targeted strategies (as treebank embeddings) to exploit existing annotated corpora while preserving the features of one specific text when performing syntactic parsing.</abstract>
      <url hash="b5c18bdc">2022.lt4hala-1.18</url>
      <bibkey>fantoli-de-lhoneux-2022-linguistic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="19">
      <title>The First International <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Word Segmentation and <fixed-case>POS</fixed-case> Tagging Bakeoff: Overview of the <fixed-case>E</fixed-case>va<fixed-case>H</fixed-case>an 2022 Evaluation Campaign</title>
      <author><first>Bin</first><last>Li</last></author>
      <author><first>Yiguo</first><last>Yuan</last></author>
      <author><first>Jingya</first><last>Lu</last></author>
      <author><first>Minxuan</first><last>Feng</last></author>
      <author><first>Chao</first><last>Xu</last></author>
      <author><first>Weiguang</first><last>Qu</last></author>
      <author><first>Dongbo</first><last>Wang</last></author>
      <pages>135–140</pages>
      <abstract>This paper presents the results of the First Ancient Chinese Word Segmentation and POS Tagging Bakeoff (EvaHan), which was held at the Second Workshop on Language Technologies for Historical and Ancient Languages (LT4HALA) 2022, in the context of the 13th Edition of the Language Resources and Evaluation Conference (LREC 2022). We give the motivation for having an international shared contest, as well as the data and tracks. The contest is consisted of two modalities, closed and open. In the closed modality, the participants are only allowed to use the training data, obtained the highest F1 score of 96.03% and 92.05% in word segmentation and POS tagging. In the open modality, the participants can use whatever resource they have, with the highest F1 score of 96.34% and 92.56% in word segmentation and POS tagging. The scores on the blind test dataset decrease around 3 points, which shows that the out-of-vocabulary words still are the bottleneck for lexical analyzers.</abstract>
      <url hash="fa2b6414">2022.lt4hala-1.19</url>
      <bibkey>li-etal-2022-first</bibkey>
    </paper>
    <paper id="20">
      <title>Automatic Word Segmentation and Part-of-Speech Tagging of <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Based on <fixed-case>BERT</fixed-case> Model</title>
      <author><first>Yu</first><last>Chang</last></author>
      <author><first>Peng</first><last>Zhu</last></author>
      <author><first>Chaoping</first><last>Wang</last></author>
      <author><first>Chaofan</first><last>Wang</last></author>
      <pages>141–145</pages>
      <abstract>In recent years, new deep learning methods and pre-training language models have been emerging in the field of natural language processing (NLP). These methods and models can greatly improve the accuracy of automatic word segmentation and part-of-speech tagging in the field of ancient Chinese research. In these models, the BERT model has made amazing achievements in the top-level test of machine reading comprehension SQuAD-1.1. In addition, it also showed better results than other models in 11 different NLP tests. In this paper, SIKU-RoBERTa pre-training language model based on the high-quality full-text corpus of SiKuQuanShu have been adopted, and part corpus of ZuoZhuan that has been word segmented and part-of-speech tagged is used as training sets to build a deep network model based on BERT for word segmentation and POS tagging experiments. In addition, we also use other classical NLP network models for comparative experiments. The results show that using SIKU-RoBERTa pre-training language model, the overall prediction accuracy of word segmentation and part-of-speech tagging of this model can reach 93.87% and 88.97%, with excellent overall performance.</abstract>
      <url hash="39849556">2022.lt4hala-1.20</url>
      <bibkey>chang-etal-2022-automatic</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Word Segmentation and Part-of-Speech Tagging Using Data Augmentation</title>
      <author><first>Yanzhi</first><last>Tian</last></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <pages>146–149</pages>
      <abstract>We attended the EvaHan2022 ancient Chinese word segmentation and Part-of-Speech (POS) tagging evaluation. We regard the Chinese word segmentation and POS tagging as sequence tagging tasks. Our system is based on a BERT-BiLSTM-CRF model which is trained on the data provided by the EvaHan2022 evaluation. Besides, we also employ data augmentation techniques to enhance the performance of our model. On the Test A and Test B of the evaluation, the F1 scores of our system achieve 94.73% and 90.93% for the word segmentation, 89.19% and 83.48% for the POS tagging.</abstract>
      <url hash="09bbca52">2022.lt4hala-1.21</url>
      <bibkey>tian-guo-2022-ancient</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>BERT</fixed-case> 4<fixed-case>EVER</fixed-case>@<fixed-case>E</fixed-case>va<fixed-case>H</fixed-case>an 2022: <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Word Segmentation and Part-of-Speech Tagging Based on Adversarial Learning and Continual Pre-training</title>
      <author><first>Hailin</first><last>Zhang</last></author>
      <author><first>Ziyu</first><last>Yang</last></author>
      <author><first>Yingwen</first><last>Fu</last></author>
      <author><first>Ruoyao</first><last>Ding</last></author>
      <pages>150–154</pages>
      <abstract>With the development of artificial intelligence (AI) and digital humanities, ancient Chinese resources and language technology have also developed and grown, which have become an increasingly important part to the study of historiography and traditional Chinese culture. In order to promote the research on automatic analysis technology of ancient Chinese, we conduct various experiments on ancient Chinese word segmentation and part-of-speech (POS) tagging tasks for the EvaHan 2022 shared task. We model the word segmentation and POS tagging tasks jointly as a sequence tagging problem. In addition, we perform a series of training strategies based on the provided ancient Chinese pre-trained model to enhance the model performance. Concretely, we employ several augmentation strategies, including continual pre-training, adversarial training, and ensemble learning to alleviate the limited amount of training data and the imbalance between POS labels. Extensive experiments demonstrate that our proposed models achieve considerable performance on ancient Chinese word segmentation and POS tagging tasks. Keywords: ancient Chinese, word segmentation, part-of-speech tagging, adversarial learning, continuing pre-training</abstract>
      <url hash="158d24b8">2022.lt4hala-1.22</url>
      <bibkey>zhang-etal-2022-bert</bibkey>
    </paper>
    <paper id="23">
      <title>Construction of Segmentation and Part of Speech Annotation Model in <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese</title>
      <author><first>Longjie</first><last>Jiang</last></author>
      <author><first>Qinyu C.</first><last>Chang</last></author>
      <author><first>Huyin H.</first><last>Xie</last></author>
      <author><first>Zhuying Z.</first><last>Xia</last></author>
      <pages>155–158</pages>
      <abstract>Among the four civilizations in the world with the longest history, only Chinese civilization has been inherited and never interrupted for 5000 years. An important factor is that the Chinese nation has the fine tradition of sorting out classics. Recording history with words, inheriting culture through continuous collation of indigenous accounts, and maintaining the spread of Chinese civilization. In this competition, the siku-roberta model was introduced into the part-of-speech tagging task of ancient Chinese by using the Zuozhuan data set, and good prediction results were obtained.</abstract>
      <url hash="bad2dfe4">2022.lt4hala-1.23</url>
      <bibkey>jiang-etal-2022-construction</bibkey>
    </paper>
    <paper id="24">
      <title>Simple Tagging System with <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese</title>
      <author><first>Binghao</first><last>Tang</last></author>
      <author><first>Boda</first><last>Lin</last></author>
      <author><first>Si</first><last>Li</last></author>
      <pages>159–163</pages>
      <abstract>This paper describes the system submitted for the EvaHan 2022 Shared Task on word segmentation and part-of-speech tagging for Ancient Chinese. Our system is based on the pre-trained language model SIKU-RoBERTa and the simple tagging layers. Our system significantly outperforms the official baselines in the released test sets and shows the effectiveness.</abstract>
      <url hash="ca1cae27">2022.lt4hala-1.24</url>
      <bibkey>tang-etal-2022-simple</bibkey>
    </paper>
    <paper id="25">
      <title>The Uncertainty-based Retrieval Framework for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese <fixed-case>CWS</fixed-case> and <fixed-case>POS</fixed-case></title>
      <author><first>Pengyu</first><last>Wang</last></author>
      <author><first>Zhichen</first><last>Ren</last></author>
      <pages>164–168</pages>
      <abstract>Automatic analysis for modern Chinese has greatly improved the accuracy of text mining in related fields, but the study of ancient Chinese is still relatively rare. Ancient text division and lexical annotation are important parts of classical literature comprehension, and previous studies have tried to construct auxiliary dictionary and other fused knowledge to improve the performance. In this paper, we propose a framework for ancient Chinese Word Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one hand, we try to capture the wordhood semantics; on the other hand, we re-predict the uncertain samples of baseline model by introducing external knowledge. The performance of our architecture outperforms pre-trained BERT with CRF and existing tools such as Jiayan.</abstract>
      <url hash="ac4a7a63">2022.lt4hala-1.25</url>
      <bibkey>wang-ren-2022-uncertainty</bibkey>
    </paper>
    <paper id="26">
      <title>Data Augmentation for Low-resource Word Segmentation and <fixed-case>POS</fixed-case> Tagging of <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Texts</title>
      <author><first>Yutong</first><last>Shen</last></author>
      <author><first>Jiahuan</first><last>Li</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Yi</first><last>Zhou</last></author>
      <author><first>Xiaopeng</first><last>Xie</last></author>
      <author><first>Qinxin</first><last>Zhao</last></author>
      <pages>169–173</pages>
      <abstract>Automatic word segmentation and part-of-speech tagging of ancient books can help relevant researchers to study ancient texts. In recent years, pre-trained language models have achieved significant improvements on text processing tasks. SikuRoberta is a pre-trained language model specially designed for automatic analysis of ancient Chinese texts. Although SikuRoberta significantly boosts performance on WSG and POS tasks on ancient Chinese texts, the lack of labeled data still limits the performance of the model. In this paper, to alleviate the problem of insufficient training data, We define hybrid tags to integrate WSG and POS tasks and design Roberta-CRF model to predict tags for each Chinese characters. Moreover, We generate synthetic labeled data based on the LSTM language model. To further mine knowledge in SikuRoberta, we generate the synthetic unlabeled data based on the Masked LM. Experiments show that the performance of the model is improved with the synthetic data, indicating that the effectiveness of the data augmentation methods.</abstract>
      <url hash="fb0efd00">2022.lt4hala-1.26</url>
      <bibkey>shen-etal-2022-data</bibkey>
    </paper>
    <paper id="27">
      <title>A Joint Framework for <fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese <fixed-case>WS</fixed-case> and <fixed-case>POS</fixed-case> Tagging Based on Adversarial Ensemble Learning</title>
      <author><first>Shuxun</first><last>Yang</last></author>
      <pages>174–177</pages>
      <abstract>Ancient Chinese word segmentation and part-of-speech tagging tasks are crucial to facilitate the study of ancient Chinese and the dissemination of traditional Chinese culture. Current methods face problems such as lack of large-scale labeled data, individual task error propagation, and lack of robustness and generalization of models. Therefore, we propose a joint framework for ancient Chinese WS and POS tagging based on adversarial ensemble learning, called AENet. On the basis of pre-training and fine-tuning, AENet uses a joint tagging approach of WS and POS tagging and treats it as a joint sequence tagging task. Meanwhile, AENet incorporates adversarial training and ensemble learning, which effectively improves the model recognition efficiency while enhancing the robustness and generalization of the model. Our experiments demonstrate that AENet improves the F1 score of word segmentation by 4.48% and the score of part-of-speech tagging by 2.29% on test dataset compared with the baseline, which shows high performance and strong generalization.</abstract>
      <url hash="6da93db0">2022.lt4hala-1.27</url>
      <bibkey>yang-2022-joint</bibkey>
    </paper>
    <paper id="28">
      <title>Glyph Features Matter: A Multimodal Solution for <fixed-case>E</fixed-case>va<fixed-case>H</fixed-case>an in <fixed-case>LT</fixed-case>4<fixed-case>HALA</fixed-case>2022</title>
      <author><first>Wei</first><last>Xinyuan</last></author>
      <author><first>Liu</first><last>Weihao</last></author>
      <author><first>Qing</first><last>Zong</last></author>
      <author><first>Zhang</first><last>Shaoqing</last></author>
      <author><first>Baotian</first><last>Hu</last></author>
      <pages>178–182</pages>
      <abstract>We participate in the LT4HALA2022 shared task EvaHan. This task has two subtasks. Subtask 1 is word segmentation, and subtask 2 is part-of-speech tagging. Each subtask consists of two tracks, a close track that can only use the data and models provided by the organizer, and an open track without restrictions. We employ three pre-trained models, two of which are open-source pre-trained models for ancient Chinese (Siku-Roberta and roberta-classical-chinese), and one is our pre-trained GlyphBERT combined with glyph features. Our methods include data augmentation, data pre-processing, model pretraining, downstream fine-tuning, k-fold cross validation and model ensemble. We achieve competitive P, R, and F1 scores on both our own validation set and the final public test set.</abstract>
      <url hash="9b9144e3">2022.lt4hala-1.28</url>
      <bibkey>xinyuan-etal-2022-glyph</bibkey>
    </paper>
    <paper id="29">
      <title>Overview of the <fixed-case>E</fixed-case>va<fixed-case>L</fixed-case>atin 2022 Evaluation Campaign</title>
      <author><first>Rachele</first><last>Sprugnoli</last></author>
      <author><first>Marco</first><last>Passarotti</last></author>
      <author><first>Flavio Massimiliano</first><last>Cecchini</last></author>
      <author><first>Margherita</first><last>Fantoli</last></author>
      <author><first>Giovanni</first><last>Moretti</last></author>
      <pages>183–188</pages>
      <abstract>This paper describes the organization and the results of the second edition of EvaLatin, the campaign for the evaluation of Natural Language Processing tools for Latin. The three shared tasks proposed in EvaLatin 2022, i.,e.,Lemmatization, Part-of-Speech Tagging and Features Identification, are aimed to foster research in the field of language technologies for Classical languages. The shared dataset consists of texts mainly taken from the LASLA corpus. More specifically, the training set includes only prose texts of the Classical period, whereas the test set is organized in three sub-tasks: a Classical sub-task on a prose text of an author not included in the training data, a Cross-genre sub-task on poetic and scientific texts, and a Cross-time sub-task on a text of the 15th century. The results obtained by the participants for each task and sub-task are presented and discussed.</abstract>
      <url hash="71799fbd">2022.lt4hala-1.29</url>
      <bibkey>sprugnoli-etal-2022-overview</bibkey>
    </paper>
    <paper id="30">
      <title>An <fixed-case>ELECTRA</fixed-case> Model for <fixed-case>L</fixed-case>atin Token Tagging Tasks</title>
      <author><first>Wouter</first><last>Mercelis</last></author>
      <author><first>Alek</first><last>Keersmaekers</last></author>
      <pages>189–192</pages>
      <abstract>This report describes the KU Leuven / Brepols-CTLO submission to EvaLatin 2022. We present the results of our current small Latin ELECTRA model, which will be expanded to a larger model in the future. For the lemmatization task, we combine a neural token-tagging approach with the in-house rule-based lemma lists from Brepols’ ReFlex software. The results are decent, but suffer from inconsistencies between Brepols’ and EvaLatin’s definitions of a lemma. For POS-tagging, the results come up just short from the first place in this competition, mainly struggling with proper nouns. For morphological tagging, there is much more room for improvement. Here, the constraints added to our Multiclass Multilabel model were often not tight enough, causing missing morphological features. We will further investigate why the combination of the different morphological features, which perform fine on their own, leads to issues.</abstract>
      <url hash="9450be1d">2022.lt4hala-1.30</url>
      <bibkey>mercelis-keersmaekers-2022-electra</bibkey>
    </paper>
    <paper id="31">
      <title>Transformer-based Part-of-Speech Tagging and Lemmatization for <fixed-case>L</fixed-case>atin</title>
      <author><first>Krzysztof</first><last>Wróbel</last></author>
      <author><first>Krzysztof</first><last>Nowak</last></author>
      <pages>193–197</pages>
      <abstract>The paper presents a submission to the EvaLatin 2022 shared task. Our system places first for lemmatization, part-of-speech and morphological tagging in both closed and open modalities. The results for cross-genre and cross-time sub-tasks show that the system handles the diachronic and diastratic variation of Latin. The architecture employs state-of-the-art transformer models. For part-of-speech and morphological tagging, we use XLM-RoBERTa large, while for lemmatization a ByT5 small model was employed. The paper features a thorough discussion of part-of-speech and lemmatization errors which shows how the system performance may be improved for Classical, Medieval and Neo-Latin texts.</abstract>
      <url hash="afb2fa45">2022.lt4hala-1.31</url>
      <bibkey>wrobel-nowak-2022-transformer</bibkey>
    </paper>
  </volume>
</collection>
