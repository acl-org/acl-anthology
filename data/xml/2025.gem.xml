<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.gem">
  <volume id="1" ingest-date="2025-08-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Generation, Evaluation and Metrics (GEM²)</booktitle>
      <editor><first>Ofir</first><last>Arviv</last><affiliation>IBM Research</affiliation></editor>
      <editor><first>Miruna</first><last>Clinciu</last><affiliation>Heriot Watt University</affiliation></editor>
      <editor><first>Kaustubh</first><last>Dhole</last><affiliation>Emory University</affiliation></editor>
      <editor><first>Rotem</first><last>Dror</last><affiliation>University of Haifa</affiliation></editor>
      <editor><first>Sebastian</first><last>Gehrmann</last><affiliation>Bloomberg</affiliation></editor>
      <editor><first>Eliya</first><last>Habba</last><affiliation>Hebrew University of Jerusalem</affiliation></editor>
      <editor><first>Itay</first><last>Itzhak</last><affiliation>Hebrew University of Jerusalem</affiliation></editor>
      <editor><first>Simon</first><last>Mille</last><affiliation>Dublin City University</affiliation></editor>
      <editor><first>Yotam</first><last>Perlitz</last><affiliation>IBM Research</affiliation></editor>
      <editor><first>Enrico</first><last>Santus</last><affiliation>Bloomberg</affiliation></editor>
      <editor><first>João</first><last>Sedoc</last><affiliation>New York University</affiliation></editor>
      <editor><first>Michal</first><last>Shmueli Scheuer</last><affiliation>IBM Research</affiliation></editor>
      <editor><first>Gabriel</first><last>Stanovsky</last><affiliation>Hebrew University of Jerusalem</affiliation></editor>
      <editor><first>Oyvind</first><last>Tafjord</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria and virtual meeting</address>
      <month>July</month>
      <year>2025</year>
      <url hash="7c50dd3b">2025.gem-1</url>
      <venue>gem</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-261-9</isbn>
    </meta>
    <frontmatter>
      <url hash="c0dd0cd6">2025.gem-1.0</url>
      <bibkey>gem-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards Comprehensive Evaluation of Open-Source Language Models: A Multi-Dimensional, User-Driven Approach</title>
      <author><first>Qingchen</first><last>Yu</last></author>
      <pages>1-7</pages>
      <abstract>With rapid advancements in large language models (LLMs) across artificial intelligence, machine learning, and data sci-ence, there is a growing need for evaluation frameworks that go beyond traditional performance metrics. Conventional methods focus mainly on accuracy and computational metrics, often neglecting user experience and community interaction—key elements in open-source environments. This paper intro-duces a multi-dimensional, user-centered evaluation frame-work, integrating metrics like User Engagement Index (UEI), Community Response Rate (CRR), and a Time Weight Factor (TWF) to assess LLMs’ real-world impact. Additionally, we propose an adaptive weighting mechanism using Bayesian op-timization to dynamically adjust metric weights for more ac-curate model evaluation. Experimental results confirm that our framework effectively identifies models with strong user engagement and community support, offering a balanced, data-driven approach to open-source LLM evaluation. This frame-work serves as a valuable tool for developers and researchers in selecting and improving open-source models.</abstract>
      <url hash="205f572d">2025.gem-1.1</url>
      <bibkey>yu-2025-towards</bibkey>
    </paper>
    <paper id="2">
      <title>Psycholinguistic Word Features: a New Approach for the Evaluation of <fixed-case>LLM</fixed-case>s Alignment with Humans</title>
      <author><first>Javier</first><last>Conde</last><affiliation>Universidad Politécnica de Madrid</affiliation></author>
      <author><first>Miguel González</first><last>Saiz</last><affiliation>Universidad Politécnica de Madrid</affiliation></author>
      <author><first>María</first><last>Grandury</last><affiliation>Universidad Politécnica de Madrid and Universidad Nacional de Educación a Distancia</affiliation></author>
      <author><first>Pedro</first><last>Reviriego</last></author>
      <author><first>Gonzalo</first><last>Martínez</last><affiliation>Universidad Carlos III de Madrid</affiliation></author>
      <author><first>Marc</first><last>Brysbaert</last><affiliation>Universiteit Gent</affiliation></author>
      <pages>8-17</pages>
      <abstract>The evaluation of LLMs has so far focused primarily on how well they can perform different tasks such as reasoning, question-answering, paraphrasing, or translating. For most of these tasks, performance can be measured with objective metrics, such as the number of correct answers. However, other language features are not easily quantified. For example, arousal, concreteness, or gender associated with a given word, as well as the extent to which we experience words with senses and relate them to a specific sense. Those features have been studied for many years by psycholinguistics, conducting large-scale experiments with humans to produce ratings for thousands of words. This opens an opportunity to evaluate how well LLMs align with human ratings on these word features, taking advantage of existing studies that cover many different language features in a large number of words. In this paper, we evaluate the alignment of a representative group of LLMs with human ratings on two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets cover thirteen features over thousands of words. The results show that alignment is significantly better on the Glasgow norms evaluated (arousal, valence, dominance, concreteness, imageability, familiarity, and gender) than on the Lancaster norms evaluated (introceptive, gustatory, olfactory, haptic, auditory, and visual). This suggests a limitation of current LLMs in aligning with human sensory associations for words, which may be due to their lack of embodied cognition present in humans and illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.</abstract>
      <url hash="de655647">2025.gem-1.2</url>
      <bibkey>conde-etal-2025-psycholinguistic</bibkey>
    </paper>
    <paper id="3">
      <title>Spatial Representation of Large Language Models in 2<fixed-case>D</fixed-case> Scene</title>
      <author><first>WenyaWu</first><last>WenyaWu</last></author>
      <author><first>Weihong</first><last>Deng</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <pages>18-29</pages>
      <abstract>Spatial representations are fundamental to human cognition, as understanding spatial relationships between objects is essential in daily life. Language serves as an indispensable tool for communicating spatial information, creating a close connection between spatial representations and spatial language. Large language models (LLMs), theoretically, possess spatial cognition due to their proficiency in natural language processing. This study examines the spatial representations of LLMs by employing traditional spatial tasks used in human experiments and comparing the models’ performance to that of humans. The results indicate that LLMs resemble humans in selecting spatial prepositions to describe spatial relationships and exhibit a preference for vertically oriented spatial terms. However, the human tendency to better represent locations along specific axes is absent in the performance of LLMs. This finding suggests that, although spatial language is closely linked to spatial representations, the two are not entirely equivalent.</abstract>
      <url hash="94947b38">2025.gem-1.3</url>
      <bibkey>wenyawu-deng-2025-spatial</bibkey>
    </paper>
    <paper id="4">
      <title>The Fellowship of the <fixed-case>LLM</fixed-case>s: Multi-Model Workflows for Synthetic Preference Optimization Dataset Generation</title>
      <author><first>Samee</first><last>Arif</last></author>
      <author><first>Sualeha</first><last>Farid</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Abdul Hameed</first><last>Azeemi</last></author>
      <author><first>Awais</first><last>Athar</last><affiliation>European Bioinformatics Institute - European Molecular Biology Laboratory (EMBL-EBI)</affiliation></author>
      <author><first>Agha Ali</first><last>Raza</last><affiliation>Lahore University of Management Sciences</affiliation></author>
      <pages>30-45</pages>
      <abstract>This paper presents a novel methodology for generating synthetic Preference Optimization (PO) datasets using multi-model workflows. We evaluate the effectiveness and potential of these workflows in automating and enhancing the dataset generation process. PO dataset generation requires two modules: (1) <tex-math>\textit{response evaluation}</tex-math>, and (2) <tex-math>\textit{response generation}</tex-math>. In the <tex-math>\textit{response evaluation}</tex-math> module, the responses from Large Language Models (LLMs) are evaluated and ranked - a task typically carried out by human annotators that we automate using LLMs. We assess the response evaluation module in a 2 step process. In step 1, we assess LLMs as evaluators using three distinct prompting strategies. In step 2, we apply the winning prompting strategy to compare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that GPT-4o-as-a-Judge is more consistent across all datasets. For the <tex-math>\textit{response generation}</tex-math> module, we use the identified LLM evaluator configuration and compare different configurations of the LLM Feedback Loop. We use the win rate to determine the best multi-model configuration for generation. Experimenting with various configurations, we find that the LLM Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win rate over single-model Llama and Gemma, respectively. After identifying the best configurations for both modules, we generate our PO datasets using the above pipeline.</abstract>
      <url hash="2f063e2a">2025.gem-1.4</url>
      <bibkey>arif-etal-2025-fellowship</bibkey>
    </paper>
    <paper id="5">
      <title>Does Biomedical Training Lead to Better Medical Performance?</title>
      <author><first>Amin</first><last>Dada</last></author>
      <author><first>Osman Alperen</first><last>Koraş</last></author>
      <author><first>Marie</first><last>Bauer</last><affiliation>Universität Duisburg-Essen</affiliation></author>
      <author><first>Jean-Philippe</first><last>Corbeil</last><affiliation>Microsoft</affiliation></author>
      <author><first>Amanda Butler</first><last>Contreras</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Constantin Marc</first><last>Seibold</last><affiliation>University Medicine Essen</affiliation></author>
      <author><first>Kaleb E</first><last>Smith</last></author>
      <author><first>Julian.friedrich@uk-essen.de</first><last>Julian.friedrich@uk-essen.de</last><affiliation>NA</affiliation></author>
      <author><first>Jens</first><last>Kleesiek</last><affiliation>Institute for AI in Medicine (IKIM), University Medicine Essen</affiliation></author>
      <pages>46-59</pages>
      <abstract>Large Language Models (LLMs) hold significant potential for improving healthcare applications, with biomedically adapted models promising enhanced performance on medical tasks. However, the effectiveness of biomedical domain adaptation for clinical tasks remains uncertain. In this study, we conduct a direct comparison of 12 biomedically adapted models and their general-domain base counterparts across six clinical tasks. Our results reveal that 11 out of 12 biomedical models exhibit performance declines, challenging prior findings that reported positive effects of biomedical adaptation. Notably, previous positive results primarily relied on multiple-choice evaluations, which may not reflect performance in real-world clinical applications. To promote reproducibility and further research, we open-source our evaluation pipeline, providing a resource for the development of models with practical benefits in healthcare settings.</abstract>
      <url hash="6cef59f6">2025.gem-1.5</url>
      <bibkey>dada-etal-2025-biomedical</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>HEDS</fixed-case> 3.0: The Human Evaluation Data Sheet Version 3.0</title>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Craig</first><last>Thomson</last><affiliation>Dublin City University and University of Aberdeen</affiliation></author>
      <pages>60-81</pages>
      <abstract>This paper presents a new version of the Human Evaluation Datasheet (HEDS), numbered 3.0 This update is the result of our experience using HEDS in the context of numerous recent human evaluation experiments, including reproduction studies, and of feedback collected from other researchers. Our main overall goal was to improve clarity, and to enable users to complete the datasheet more consistently and comparably. The HEDS 3.0 package consists of the digital data sheet, documentation, and code for exporting completed data sheets as latex files, all available from the HEDS 3.0 GitHub.</abstract>
      <url hash="7433bec8">2025.gem-1.6</url>
      <bibkey>belz-thomson-2025-heds</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>ARGENT</fixed-case>: Automatic Reference-free Evaluation for Open-Ended Text Generation without Source Inputs</title>
      <author><first>Xinyue</first><last>Zhang</last></author>
      <author><first>Agathe</first><last>Zecevic</last></author>
      <author><first>Sebastian</first><last>Zeki</last></author>
      <author><first>Angus</first><last>Roberts</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>82-98</pages>
      <abstract>With increased accessibility of machine-generated texts, the need for their evaluation has also grown. There are broadly two types of text generation tasks. In open-ended generation tasks (OGTs), the model generates de novo text without any input on which to base it, such as story generation. In reflective generation tasks (RGTs), the model output is generated to reflect an input sequence, such as in machine translation. There are many studies on RGT evaluation, where the metrics typically compare one or more gold-standard references to the model output. Evaluation of OGTs has received less attention and is more challenging: since the task does not aim to reflect an input, there are usually no reference texts. In this paper, we propose a new perspective that unifies OGT evaluation with RGT evaluation, based on which we develop an automatic, reference-free generative text evaluation model (ARGENT), and review previous literature from this perspective. Our experiments demonstrate the effectiveness of these methods across informal, formal, and domain-specific texts. We conduct a meta-evaluation to compare existing and proposed metrics, finding that our approach aligns more closely with human judgement.</abstract>
      <url hash="43631c73">2025.gem-1.8</url>
      <bibkey>zhang-etal-2025-argent</bibkey>
    </paper>
    <paper id="9">
      <title>Are <fixed-case>LLM</fixed-case>s (Really) Ideological? An <fixed-case>IRT</fixed-case>-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jasmin</first><last>Wachter</last></author>
      <author><first>Michael</first><last>Radloff</last><affiliation>Alpen-Adria Universität Klagenfurt</affiliation></author>
      <author><first>Maja</first><last>Smolej</last></author>
      <author><first>Katharina</first><last>Kinder-Kurlanda</last><affiliation>Alpen-Adria Universität Klagenfurt</affiliation></author>
      <pages>99-120</pages>
      <abstract>We introduce an Item Response Theory (IRT)-based framework to detect and quantify ideological bias in large language models (LLMs) without relying on subjective human judgments. Unlike prior work, our two-stage approach distinguishes between response avoidance and expressed bias by modeling ‘Prefer Not to Answer’ (PNA) behaviors and calibrating ideological leanings based on open-ended responses. We fine-tune two LLM families to represent liberal and conservative baselines, and validate our approach using a 105-item ideological test inventory. Our results show that off-the-shelve LLMs frequently avoid engagement with ideological prompts, calling into question previous claims of partisan bias. This framework provides a statistically grounded and scalable tool for LLM alignment and fairness assessment. The general methodolody can also be applied to other forms of bias and languages.</abstract>
      <url hash="a47476e2">2025.gem-1.9</url>
      <bibkey>wachter-etal-2025-llms</bibkey>
    </paper>
    <paper id="10">
      <title>Knockout <fixed-case>LLM</fixed-case> Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons</title>
      <author><first>Isik Baran</first><last>Sandan</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Tu Anh</first><last>Dinh</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>121-128</pages>
      <abstract>Large Language Models (LLMs) have shown to be effective evaluators across various domains such as machine translations or the scientific domain. Current LLM-as-a-Judge approaches rely mostly on individual assessments or a single round of pairwise assessments, preventing the judge LLM from developing a global ranking perspective.To address this, we present Knockout Assessment, an LLM-as-a-Judge method using a knockout tournament system with iterative pairwise comparisons. Experiments across three LLMs on two datasets show that knockout assessment improves scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average for university-level exam scoring and machine translation evaluations, aligning LLM assessments more closely with human scoring.</abstract>
      <url hash="f5579875">2025.gem-1.10</url>
      <bibkey>sandan-etal-2025-knockout</bibkey>
    </paper>
    <paper id="11">
      <title>Free-text Rationale Generation under Readability Level Control</title>
      <author><first>Yi-Sheng</first><last>Hsu</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Nils</first><last>Feldhus</last></author>
      <author><first>Sherzod</first><last>Hakimov</last><affiliation>Universität Potsdam</affiliation></author>
      <pages>129-150</pages>
      <abstract>Free-text rationales justify model decisions in natural language and thus become likable and accessible among approaches to explanation across many tasks. However, their effectiveness can be hindered by misinterpretation and hallucination. As a perturbation test, we investigate how large language models (LLMs) perform rationale generation under the effects of readability level control, i.e., being prompted for an explanation targeting a specific expertise level, such as sixth grade or college. We find that explanations are adaptable to such instruction, though the observed distinction between readability levels does not fully match the defined complexity scores according to traditional readability metrics. Furthermore, the generated rationales tend to feature medium level complexity, which correlates with the measured quality using automatic metrics. Finally, our human annotators confirm a generally satisfactory impression on rationales at all readability levels, with high-school-level readability being most commonly perceived and favored.</abstract>
      <url hash="a5832847">2025.gem-1.11</url>
      <bibkey>hsu-etal-2025-free</bibkey>
    </paper>
    <paper id="12">
      <title>Selective Shot Learning for Code Explanation</title>
      <author><first>Paheli</first><last>Bhattacharya</last><affiliation>Bosch</affiliation></author>
      <author><first>Rishabh</first><last>Gupta</last></author>
      <pages>151-160</pages>
      <abstract>Code explanation plays a crucial role in the software engineering domain, aiding developers in grasping code functionality efficiently. Recent work shows that the performance of LLMs for code explanation improves in a few-shot setting, especially when the few-shot examples are selected intelligently. State-of-the-art approaches for such Selective Shot Learning (SSL) include token-based and embedding-based methods. However, these SSL approaches have been evaluated on proprietary LLMs, without much exploration on open-source Code-LLMs. Additionally, these methods lack consideration for programming language syntax. To bridge these gaps, we present a comparative study and propose a novel SSL method (SSL_ner) that utilizes entity information for few-shot example selection. We present several insights and show the effectiveness of SSL_ner approach over state-of-the-art methods across two datasets. To the best of our knowledge, this is the first systematic benchmarking of various few-shot examples selection approaches using open-source Code-LLMs for the code explanation task.</abstract>
      <url hash="16a8bd8f">2025.gem-1.12</url>
      <bibkey>bhattacharya-gupta-2025-selective</bibkey>
    </paper>
    <paper id="13">
      <title>Can <fixed-case>LLM</fixed-case>s Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?</title>
      <author><first>Evangelia</first><last>Gogoulou</last><affiliation>RISE Research Institutes of Sweden</affiliation></author>
      <author><first>Shorouq</first><last>Zahra</last><affiliation>Uppsala University and RISE Research Institutes of Sweden AB</affiliation></author>
      <author><first>Liane</first><last>Guillou</last><affiliation>Aveni</affiliation></author>
      <author><first>Luise</first><last>Dürlich</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Joakim</first><last>Nivre</last><affiliation>Uppsala University</affiliation></author>
      <pages>161-177</pages>
      <abstract>A frequently observed problem with LLMs is their tendency to generate output that is nonsensical, illogical, or factually incorrect, often referred to broadly as “hallucination”. Building on the recently proposed HalluciGen task for hallucination detection and generation, we evaluate a suite of open-access LLMs on their ability to detect intrinsic hallucinations in two conditional generation tasks: translation and paraphrasing. We study how model performance varies across tasks and languages and we investigate the impact of model size, instruction-tuning, and prompt choice. We find that performance varies across models but is consistent across prompts. Finally, we find that NLI models perform comparably well, suggesting that LLM-based detectors are not the only viable option for this specific task.</abstract>
      <url hash="b9c1348b">2025.gem-1.13</url>
      <bibkey>gogoulou-etal-2025-llms</bibkey>
    </paper>
    <paper id="14">
      <title>Evaluating <fixed-case>LLM</fixed-case>s with Multiple Problems at once</title>
      <author><first>Zhengxiang</first><last>Wang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Jordan</first><last>Kodner</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Owen</first><last>Rambow</last><affiliation>Stony Brook University</affiliation></author>
      <pages>178-199</pages>
      <abstract>This paper shows the benefits and fruitfulness of evaluating LLMs with multiple problems at once, a paradigm we call multi-problem evaluation (MPE). Unlike conventional single-problem evaluation, where a prompt presents a single problem and expects one specific answer, MPE places multiple problems together in a single prompt and assesses how well an LLM answers all these problems in a single output. Leveraging 6 classification and 12 reasoning benchmarks that already exist, we introduce a new benchmark called ZeMPE (Zero-shot Multi-Problem Evaluation), comprising 53,100 zero-shot multi-problem prompts. We experiment with a total of 13 LLMs from 5 model families on ZeMPE to present a comprehensive and systematic MPE. Our results show that LLMs are capable of handling multiple problems from a single data source as well as handling them separately, but there are conditions this multiple problem handling capability falls short. In addition, we perform in-depth further analyses and explore model-level factors that may enable multiple problem handling capabilities in LLMs. We release our corpus and code to facilitate future research.</abstract>
      <url hash="da2a5018">2025.gem-1.14</url>
      <bibkey>wang-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="15">
      <title>Learning and Evaluating Factual Clarification Question Generation Without Examples</title>
      <author><first>Matthew</first><last>Toles</last></author>
      <author><first>Yukun</first><last>Huang</last><affiliation>Duke University</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>200-211</pages>
      <abstract>Real-world tasks such as giving legal or technical advice often depend on context that is initially missing at the outset. The ability to derive missing factual information by asking clarifying questions (ACQ) is an important element of real-life collaboration on such reasoning tasks. Although intent disambiguation has been heavily investigated, factual reasoning remains underexplored. To enable evaluation of factual domain clarification question generation, we present a new task that focuses on the ability to elicit missing information in multi-hop reasoning tasks. We observe that humans outperform GPT-4o by a large margin, while Llama 3 8B Instruct does not even beat the dummy baseline in some metrics. Finally, we find that by fine-tuning Llama 3 8B Instruct on its own generations filtered via rejection sampling, we can improve information recovery by 27.6% without using any manually labeled data.</abstract>
      <url hash="d4fe5a62">2025.gem-1.15</url>
      <bibkey>toles-etal-2025-learning</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>SECQUE</fixed-case>: A Benchmark for Evaluating Real-World Financial Analysis Capabilities</title>
      <author><first>Noga</first><last>BenYoash</last></author>
      <author><first>Menachem</first><last>Brief</last><affiliation>Microsoft</affiliation></author>
      <author><first>Oded</first><last>Ovadia</last></author>
      <author><first>Gil</first><last>Shenderovitz</last><affiliation>Microsoft and Ben Gurion University of the Negev</affiliation></author>
      <author><first>Moshik</first><last>Mishaeli</last></author>
      <author><first>Rachel</first><last>Lemberg</last></author>
      <author><first>Eitam</first><last>Sheetrit</last><affiliation>Ben Gurion University of the Negev and Microsoft</affiliation></author>
      <pages>212-230</pages>
      <abstract>We introduce SECQUE, a comprehensive benchmark for evaluating large language models (LLMs) in financial analysis tasks. SECQUE comprises 565 expert-written questions covering SEC filings analysis across four key categories: comparison analysis, ratio calculation, risk assessment, and financial insight generation. To assess model performance, we develop SECQUE-Judge, an evaluation mechanism leveraging multiple LLM-based judges, which demonstrates strong alignment with human evaluations. Additionally, we provide an extensive analysis of various models’ performance on our benchmark. By making SECQUE publicly available (https://huggingface.co/datasets/nogabenyoash/SecQue), we aim to facilitate further research and advancements in financial AI.</abstract>
      <url hash="239ea64b">2025.gem-1.16</url>
      <bibkey>benyoash-etal-2025-secque</bibkey>
    </paper>
    <paper id="18">
      <title>Measure only what is measurable: towards conversation requirements for evaluating task-oriented dialogue systems</title>
      <author><first>Emiel</first><last>Van Miltenburg</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Anouck</first><last>Braggaar</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Emmelyn</first><last>Croes</last><affiliation>NA</affiliation></author>
      <author><first>Florian</first><last>Kunneman</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Christine</first><last>Liebrecht</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Gabriella</first><last>Martijn</last><affiliation>NA</affiliation></author>
      <pages>231-238</pages>
      <abstract>Chatbots for customer service have been widely studied in many different fields, ranging from Natural Language Processing (NLP) to Communication Science. These fields have developed different evaluation practices to assess chatbot performance (e.g., fluency, task success) and to measure the impact of chatbot usage on the user’s perception of the organisation controlling the chatbot (e.g., brand attitude) as well as their willingness to enter a business transaction or to continue to use the chatbot in the future (i.e., purchase intention, reuse intention). While NLP researchers have developed many automatic measures of success, other fields mainly use questionnaires to compare different chatbots. This paper explores the extent to which we can bridge the gap between the two, and proposes a research agenda to further explore this question.</abstract>
      <url hash="51f98261">2025.gem-1.18</url>
      <bibkey>van-miltenburg-etal-2025-measure</bibkey>
    </paper>
    <paper id="21">
      <title>Can Perplexity Predict Finetuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for <fixed-case>N</fixed-case>epali</title>
      <author><first>Nishant</first><last>Luitel</last><affiliation>NAAMII</affiliation></author>
      <author><first>Nirajan</first><last>Bekoju</last><affiliation>Tribhuvan University</affiliation></author>
      <author><first>Anand Kumar</first><last>Sah</last><affiliation>NA</affiliation></author>
      <author><first>Subarna</first><last>Shakya</last><affiliation>NA</affiliation></author>
      <pages>239-248</pages>
      <abstract>The impact of subword tokenization on language model performance is well-documented for perplexity, with finer granularity consistently reducing this intrinsic metric. However, research on how different tokenization schemes affect a model’s understanding capabilities remains limited, particularly for non-Latin script languages. Addressing this gap, we conducted a comprehensive evaluation of six distinct tokenization strategies by pretraining transformer-based language models for Nepali and evaluating their performance across multiple downstream tasks. While recent prominent models like GPT, RoBERTa, Claude, LLaMA, Mistral, Falcon, and MPT have adopted byte-level BPE tokenization, our findings demonstrate that for Nepali, SentencePiece tokenization consistently yields superior results on understanding-based tasks. Unlike previous studies that primarily focused on BERT-based architectures, our research specifically examines sequential transformer models, providing valuable insights for language model development in low-resource languages and highlighting the importance of tokenization strategy beyond perplexity reduction.</abstract>
      <url hash="080043b4">2025.gem-1.21</url>
      <bibkey>luitel-etal-2025-perplexity</bibkey>
    </paper>
    <paper id="22">
      <title>Are Bias Evaluation Methods Biased ?</title>
      <author><first>Lina</first><last>Berrayana</last></author>
      <author><first>Sean</first><last>Rooney</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Luis</first><last>Garcés-Erice</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ioana</first><last>Giurgiu</last><affiliation>International Business Machines</affiliation></author>
      <pages>249-261</pages>
      <abstract>The creation of benchmarksto evaluate the safety of Large Language Models is one of the key activities within the trusted AI community. These benchmarks allow models to be compared for different aspects of safety such as toxicity, bias, harmful behavior etc. Independent benchmarks adopt different approacheswith distinct data sets and evaluation methods. We investigate how robust such benchmarks are by using different approachesto rank a set of representative models for bias andcompare how similar are the overall rankings. We show that different but widely used bias evaluations methods result in disparate model rankings. We conclude with recommendations for the community in the usage of such benchmarks.</abstract>
      <url hash="0ac5a6e8">2025.gem-1.22</url>
      <bibkey>berrayana-etal-2025-bias</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>IRS</fixed-case>um: One Model to Rule Summarization and Retrieval</title>
      <author><first>Sotaro</first><last>Takeshita</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Kai</first><last>Eckert</last><affiliation>Mannheim University of Applied Sciences</affiliation></author>
      <pages>262-275</pages>
      <abstract>Applications that store a large number of documents often have summarization and retrieval functionalities to help users digest large amounts of information efficiently. Currently, such systems need to run two task-specific models, for summarization and retrieval, redundantly on the same set of documents. An efficient approach to amend this redundancy would be to reuse hidden representations produced during the summary generation for retrieval. However, our experiment shows that existing models, including recent large language models, do not produce retrieval-friendly embeddings during summarization due to a lack of a contrastive objective during their training. To this end, we introduce a simple, cost-effective training strategy which integrates a contrastive objective into standard summarization training without requiring additional annotations. We empirically show that our model can perform on par or even outperform in some cases compared to the combination of two task-specific models while improving throughput and FLOPs by up to 17% and 20%, respectively.</abstract>
      <url hash="7bc37b2d">2025.gem-1.23</url>
      <bibkey>takeshita-etal-2025-irsum</bibkey>
    </paper>
    <paper id="24">
      <title>Modeling the One-to-Many Property in Open-Domain Dialogue with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jing Yang</first><last>Lee</last></author>
      <author><first>Kong Aik</first><last>Lee</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Woon-Seng</first><last>Gan</last><affiliation>NA</affiliation></author>
      <pages>276-290</pages>
      <abstract>Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of <tex-math>n</tex-math> semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models.</abstract>
      <url hash="034c4d02">2025.gem-1.24</url>
      <bibkey>lee-etal-2025-modeling</bibkey>
    </paper>
    <paper id="25">
      <title>Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Minsuh</first><last>Joo</last></author>
      <author><first>Hyunsoo</first><last>Cho</last><affiliation>Ewha Women’s University</affiliation></author>
      <pages>291-301</pages>
      <abstract>Despite the outstanding performance of large language models (LLMs) across various NLP tasks, hallucinations in LLMs–where LLMs generate inaccurate responses–remains as a critical problem as it can be directly connected to a crisis of building safe and reliable LLMs. Uncertainty estimation is primarily used to measure hallucination levels in LLM responses so that correct and incorrect answers can be distinguished clearly. This study proposes an effective uncertainty estimation approach, Clustering-based semantic consistency (Cleanse). Cleanse quantifies the uncertainty with the proportion of the intra-cluster consistency in the total consistency between LLM hidden embeddings which contain adequate semantic information of generations, by employing clustering. The effectiveness of Cleanse for detecting hallucination is validated using four off-the-shelf models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two question-answering benchmarks, SQuAD and CoQA.</abstract>
      <url hash="195b3f27">2025.gem-1.25</url>
      <bibkey>joo-cho-2025-cleanse</bibkey>
    </paper>
    <paper id="26">
      <title>Metric assessment protocol in the context of answer fluctuation on <fixed-case>MCQ</fixed-case> tasks</title>
      <author><first>Ekaterina</first><last>Goliakova</last></author>
      <author><first>Xavier</first><last>Renard</last></author>
      <author><first>Marie-Jeanne</first><last>Lesot</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Thibault</first><last>Laugel</last><affiliation>LIP6, Sorbonne Université/CNRS and AXA</affiliation></author>
      <author><first>Christophe</first><last>Marsala</last><affiliation>LIP6</affiliation></author>
      <author><first>Marcin</first><last>Detyniecki</last><affiliation>AXA, CNRS and LIP6</affiliation></author>
      <pages>302-319</pages>
      <abstract>Using multiple-choice questions (MCQs) has become a standard for assessing LLM capabilities efficiently. A variety of metrics can be employed for this task. However, previous research has not conducted a thorough assessment of them. At the same time, MCQ evaluation suffers from answer fluctuation: models produce different results given slight changes in prompts. We suggest a metric assessment protocol in which evaluation methodologies are analyzed through their connection with fluctuation rates, as well as original performance. Our results show that there is a strong link between existing metrics and the answer changing, even when computed without any additional prompt variants. Highest association on the protocol is demonstrated by a novel metric, worst accuracy.</abstract>
      <url hash="64e5238e">2025.gem-1.26</url>
      <bibkey>goliakova-etal-2025-metric</bibkey>
    </paper>
    <paper id="28">
      <title>(Towards) Scalable Reliable Automated Evaluation with Large Language Models</title>
      <author><first>Bertil</first><last>Braun</last></author>
      <author><first>Martin</first><last>Forell</last></author>
      <pages>320-336</pages>
      <abstract>Evaluating the quality and relevance of textual outputs from Large Language Models (LLMs) remains challenging and resource-intensive.Existing automated metrics often fail to capture the complexity and variability inherent in LLM-generated outputs.Moreover, these metrics typically rely on explicit reference standards, limiting their use mostly to domains with objective benchmarks.This work introduces a novel evaluation framework designed to approximate expert-level assessments of LLM-generated content.The proposed method employs pairwise comparisons of outputs by multiple LLMs, reducing biases from individual models.An Elo rating system is used to generate stable and interpretable rankings.Adjustable agreement thresholds—from full unanimity to majority voting—allow flexible control over evaluation confidence and coverage.The method’s effectiveness is demonstrated through evaluating competency profiles extracted from scientific abstracts.Preliminary results show that automatically derived rankings correlate well with expert judgments, significantly reducing the need for extensive human intervention.By offering a scalable, consistent, and domain-agnostic evaluation layer, the framework supports more efficient and reliable quality assessments of LLM outputs across diverse applications.</abstract>
      <url hash="29ac40ed">2025.gem-1.28</url>
      <bibkey>braun-forell-2025-towards</bibkey>
    </paper>
    <paper id="29">
      <title>Clustering Zero-Shot Uncertainty Estimations to Assess <fixed-case>LLM</fixed-case> Response Accuracy for Yes/No <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case></title>
      <author><first>Christopher T.</first><last>Franck</last></author>
      <author><first>Amy</first><last>Vennos</last></author>
      <author><first>W. Graham</first><last>Mueller</last><affiliation>Leidos</affiliation></author>
      <author><first>Daniel</first><last>Dakota</last><affiliation>Leidos and Indiana University</affiliation></author>
      <pages>337-353</pages>
      <abstract>The power of Large Language Models (LLMs) in user workflows has increased the desire to access such technology in everyday work. While the ability to interact with models provides noticeable benefits, it also presents challenges in terms of how much trust a user should put in the system’s responses. This is especially true for external commercial and proprietary models where there is seldom direct access and only a response from an API is provided. While standard evaluation metrics, such as accuracy, provide starting points, they often may not provide enough information to users in settings where the confidence in a system’s response is important due to downstream or real-world impact, such as in Question &amp; Answering (Q&amp;A) workflows. To support users in assessing how accurate Q&amp;A responses from such black-box LLMs scenarios are, we develop an uncertainty estimation framework that provides users with an analysis using a Dirichlet mixture model accessed from probabilities derived from a zero-shot classification model. We apply our framework to responses on the BoolQ Yes/No questions from GPT models, finding the resulting clusters allow a better quantification of uncertainty, providing a more fine-grained quantification of accuracy and precision across the space of model output while still being computationally practical. We further demonstrate its generalizability and reusability of the uncertainty model by applying it to a small set of Q&amp;A collected from U.S. government websites.</abstract>
      <url hash="96de9521">2025.gem-1.29</url>
      <bibkey>franck-etal-2025-clustering</bibkey>
    </paper>
    <paper id="30">
      <title>Using <fixed-case>LLM</fixed-case> Judgements for Sanity Checking Results and Reproducibility of Human Evaluations in <fixed-case>NLP</fixed-case></title>
      <author><first>Rudali</first><last>Huidrom</last></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University</affiliation></author>
      <pages>354-365</pages>
      <abstract>Human-like evaluation by LLMs of NLP systems is currently attracting a lot of interest, and correlations with human reference evaluations are often remarkably strong. However, this is not always the case, for unclear reasons which means that without also meta-evaluating against human evaluations (incurring the very cost automatic evaluation is intended to avoid), we don’t know if an LLM-as-judge evaluation is reliable or not. In this paper, we explore a type of evaluation scenario where this may not matter, because it comes with a built-in reliability check. We apply different LLM-as-judge methods to sets of three comparable human evaluations: (i) an original human evaluation, and (ii) two reproductions of it which produce contradicting reproducibility results. We find that in each case, the different LLM-as-judge methods (i) strongly agree with each other, and (ii) strongly agree with the results of one reproduction, while strongly disagreeing with the other. In combination, we take this to mean that a set of LLMs can be used to sanity check contradictory reproducibility results <i>if</i> the LLMs agree with each other, <i>and</i> the agreement of the LLMs with one set of results, and the disagreement with the other, are both strong.</abstract>
      <url hash="3e13d4a9">2025.gem-1.30</url>
      <bibkey>huidrom-belz-2025-using</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>C</fixed-case>o<fixed-case>K</fixed-case>e: Customizable Fine-Grained Story Evaluation via Chain-of-Keyword Rationalization</title>
      <author><first>Brihi</first><last>Joshi</last></author>
      <author><first>Sriram</first><last>Venkatapathy</last></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Haw-Shiuan</first><last>Chang</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <pages>366-384</pages>
      <abstract>Evaluating creative text such as human-written stories using language models has always been a challenging task – owing to the subjectivity of multi-annotator ratings. To mimic the thinking process of humans, chain of thought (Wei et al., 2023) (CoT) generates free-text explanations that help guide a model’s predictions and Self-Consistency (Wang et al., 2022) (SC) marginalizes predictions over multiple generated explanations. In this study, we discover that the widely-used self-consistency reasoning methods cause suboptimal results due to an objective mismatch between generating ‘fluent-looking’ explanations vs. actually leading to a good rating prediction for an aspect of a story. To overcome this challenge, we propose Chain-of-Keywords (CoKe), which generates a sequence of keywords before generating a free-text rationale, that guide the rating prediction of our evaluation language model. Then, we generate a diverse set of such keywords, and aggregate the scores corresponding to these generations. On the StoryER dataset, CoKe based on our small fine-tuned evaluation models not only reach human-level performance and significantly outperform GPT-4 with a 2x boost in correlation with human annotators, but also requires drastically less # of parameters.</abstract>
      <url hash="12d5638a">2025.gem-1.31</url>
      <bibkey>joshi-etal-2025-coke</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>H</fixed-case>u<fixed-case>GME</fixed-case>: A benchmark system for evaluating <fixed-case>H</fixed-case>ungarian generative <fixed-case>LLM</fixed-case>s</title>
      <author><first>Noémi</first><last>Ligeti-Nagy</last><affiliation>Hungarian Research Centre for Linguistics</affiliation></author>
      <author><first>Gabor</first><last>Madarasz</last><affiliation>NA</affiliation></author>
      <author><first>Flora</first><last>Foldesi</last><affiliation>NA</affiliation></author>
      <author><first>Mariann</first><last>Lengyel</last><affiliation>NA</affiliation></author>
      <author><first>Matyas</first><last>Osvath</last><affiliation>NA</affiliation></author>
      <author><first>Bence</first><last>Sarossy</last><affiliation>NA</affiliation></author>
      <author><first>Kristof</first><last>Varga</last><affiliation>NA</affiliation></author>
      <author><first>Győző Zijian</first><last>Yang</last><affiliation>Hungarian Research Centre for Linguistics</affiliation></author>
      <author><first>Enikő</first><last>Héja</last><affiliation>Hungarian Research Centre for Linguistics</affiliation></author>
      <author><first>Tamás</first><last>Váradi</last><affiliation>Nyelvtudományi Kutatóközpont</affiliation></author>
      <author><first>Gábor</first><last>Prószéky</last><affiliation>Hungarian Research Centre for Linguistics, Pazmany Peter Catholic University and MorphoLogic</affiliation></author>
      <pages>385-403</pages>
      <abstract>In this study, we introduce the Hungarian Generative Model Evaluation (HuGME) benchmark, a new framework designed to assess the linguistic proficiency of large language models (LLMs) in Hungarian. HuGME evaluates models across a diverse set of linguistic and reasoning skills, including bias, toxicity, faithfulness, relevance, summarization, prompt alignment, readability, spelling, grammaticality, and domain-specific knowledge through tasks like TruthfulQA and MMLU. We applied HuGME to a range of Hungarian LLMs, including those developed in-house as well as several publicly available models that claim Hungarian language proficiency. This paper presents the comparative results of these evaluations, shedding light on the capabilities of current LLMs in processing the Hungarian language. Through our analysis, we aim to both showcase the current state of Hungarian linguistic processing in LLMs and provide a foundational resource for future advancements in the field.</abstract>
      <url hash="a801409e">2025.gem-1.32</url>
      <bibkey>ligeti-nagy-etal-2025-hugme</bibkey>
    </paper>
    <paper id="33">
      <title>Judging the Judges: Evaluating Alignment and Vulnerabilities in <fixed-case>LLM</fixed-case>s-as-Judges</title>
      <author><first>Aman Singh</first><last>Thakur</last></author>
      <author><first>Kartik</first><last>Choudhary</last></author>
      <author><first>Venkat Srinik</first><last>Ramayapally</last></author>
      <author><first>Sankaran</first><last>Vaidyanathan</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Dieuwke</first><last>Hupkes</last><affiliation>Facebook</affiliation></author>
      <pages>404-430</pages>
      <abstract>The LLM-as-a-judge paradigm offers a potential solution to scalability issues in human evaluation of large language models (LLMs), but there are still many open questions about its strengths, weaknesses, and potential biases. This study investigates thirteen models, ranging in size and family, as ‘judge models’ evaluating answers from nine base and instruction-tuned ‘exam-taker models’. We find that only the best (and largest) models show reasonable alignment with humans, though they still differ with up to 5 points from human-assigned scores. Our research highlights the need for alignment metrics beyond percent agreement, as judges with high agreement can still assign vastly different scores. We also find that smaller models and the lexical metric contains can provide a reasonable signal in ranking the exam-taker models. Further error analysis reveals vulnerabilities in judge models, such as sensitivity to prompt complexity and a bias toward leniency. Our findings show that even the best judge models differ from humans in this fairly sterile setup, indicating that caution is warranted when applying judge models in more complex scenarios.</abstract>
      <url hash="8fc029c3">2025.gem-1.33</url>
      <bibkey>thakur-etal-2025-judging</bibkey>
    </paper>
    <paper id="36">
      <title>Analyzing the Sensitivity of Vision Language Models in Visual Question Answering</title>
      <author><first>Monika</first><last>Shah</last></author>
      <author><first>Sudarshan</first><last>Balaji</last></author>
      <author><first>Somdeb</first><last>Sarkhel</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Sanorita</first><last>Dey</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Deepak</first><last>Venugopal</last><affiliation>University of Memphis</affiliation></author>
      <pages>431-438</pages>
      <abstract>We can think of Visual Question Answering as a (multimodal) conversation between a human and an AI system. Here, we explore the sensitivity of Vision Language Models (VLMs) through the lens of cooperative principles of conversation proposed by Grice. Specifically, even when Grice’s maxims of conversation are flouted, humans typically do not have much difficulty in understanding the conversation even though it requires more cognitive effort. Here, we study if VLMs are capable of handling violations to Grice’s maxims in a manner that is similar to humans. Specifically, we add modifiers to human-crafted questions and analyze the response of VLMs to these modifiers. We use three state-of-the-art VLMs in our study, namely, GPT-4o, Claude-3.5-Sonnet and Gemini-1.5-Flash on questions from the VQA v2.0 dataset. Our initial results seem to indicate that the performance of VLMs consistently diminish with the addition of modifiers which indicates our approach as a promising direction to understand the limitations of VLMs.</abstract>
      <url hash="95fc7f96">2025.gem-1.36</url>
      <bibkey>shah-etal-2025-analyzing</bibkey>
    </paper>
    <paper id="38">
      <title>Investigating the Robustness of Retrieval-Augmented Generation at the Query Level</title>
      <author><first>Sezen</first><last>Perçin</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Xin</first><last>Su</last><affiliation>Intel</affiliation></author>
      <author><first>Qutub Sha</first><last>Syed</last></author>
      <author><first>Phillip</first><last>Howard</last><affiliation>Thoughtworks</affiliation></author>
      <author><first>Aleksei</first><last>Kuvshinov</last><affiliation>Technical University Munich</affiliation></author>
      <author><first>Leo</first><last>Schwinn</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Kay-Ulrich</first><last>Scholl</last><affiliation>Intel</affiliation></author>
      <pages>439-457</pages>
      <abstract>Large language models (LLMs) are very costly and inefficient to update with new information. To address this limitation, retrieval-augmented generation (RAG) has been proposed as a solution that dynamically incorporates external knowledge during inference, improving factual consistency and reducing hallucinations. Despite its promise, RAG systems face practical challenges-most notably, a strong dependence on the quality of the input query for accurate retrieval. In this paper, we investigate the sensitivity of different components in the RAG pipeline to various types of query perturbations. Our analysis reveals that the performance of commonly used retrievers can degrade significantly even under minor query variations. We study each module in isolation as well as their combined effect in an end-to-end question answering setting, using both general-domain and domain-specific datasets. Additionally, we propose an evaluation framework to systematically assess the query-level robustness of RAG pipelines and offer actionable recommendations for practitioners based on the results of more than 1092 experiments we performed.</abstract>
      <url hash="71f8dee0">2025.gem-1.38</url>
      <bibkey>percin-etal-2025-investigating</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>ELAB</fixed-case>: Extensive <fixed-case>LLM</fixed-case> Alignment Benchmark in <fixed-case>P</fixed-case>ersian Language</title>
      <author><first>Zahra</first><last>Pourbahman</last></author>
      <author><first>Fatemeh</first><last>Rajabi</last><affiliation>University of Tehran, University of Tehran</affiliation></author>
      <author><first>Mohammadhossein</first><last>Sadeghi</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Omid</first><last>Ghahroodi</last><affiliation>Sharif University of Technology</affiliation></author>
      <author><first>Somayeh</first><last>Bakhshaei</last></author>
      <author><first>Arash</first><last>Amini</last></author>
      <author><first>Reza</first><last>Kazemi</last><affiliation>Sharif University of Technology, Sharif University of Technology</affiliation></author>
      <author><first>Mahdieh Soleymani</first><last>Baghshah</last></author>
      <pages>458-470</pages>
      <abstract>This paper presents a comprehensive evaluation framework for aligning Persian Large Language Models (LLMs) with critical ethical dimensions, including safety, fairness, and social norms. It addresses the gaps in existing LLM evaluation frameworks by adapting them to Persian linguistic and cultural contexts. This benchmark creates three types of Persian-language benchmarks: (i) translated data, (ii) new data generated synthetically, and (iii) new naturally collected data. We translate Anthropic Red Teaming data, AdvBench, HarmBench, and DecodingTrust into Persian. Furthermore, we create ProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets to address harmful and prohibited content in indigenous culture. Moreover, we collect extensive dataset as GuardBench-fa to consider Persian cultural norms. By combining these datasets, our work establishes a unified framework for evaluating Persian LLMs, offering a new approach to culturally grounded alignment evaluation. A systematic evaluation of Persian LLMs is performed across the three alignment aspects: safety (avoiding harmful content), fairness (mitigating biases), and social norms (adhering to culturally accepted behaviors). We present a publicly available leaderboard that benchmarks Persian LLMs with respect to safety, fairness, and social norms.</abstract>
      <url hash="7e2acdcb">2025.gem-1.40</url>
      <bibkey>pourbahman-etal-2025-elab</bibkey>
    </paper>
    <paper id="41">
      <title>Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on <fixed-case>T</fixed-case>urkish</title>
      <author><first>Elif Ecem</first><last>Umutlu</last><affiliation>NA</affiliation></author>
      <author><first>Ayse Aysu</first><last>Cengiz</last><affiliation>NA</affiliation></author>
      <author><first>Ahmet Kaan</first><last>Sever</last></author>
      <author><first>Seyma</first><last>Erdem</last><affiliation>NA</affiliation></author>
      <author><first>Burak</first><last>Aytan</last><affiliation>NA</affiliation></author>
      <author><first>Busra</first><last>Tufan</last><affiliation>NA</affiliation></author>
      <author><first>Abdullah</first><last>Topraksoy</last><affiliation>NA</affiliation></author>
      <author><first>Esra</first><last>Darıcı</last><affiliation>Middle East Technical University</affiliation></author>
      <author><first>Cagri</first><last>Toraman</last><affiliation>METU, Middle East Technical University</affiliation></author>
      <pages>471-487</pages>
      <abstract>The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using a comprehensive framework that assesses six criteria, both human and LLM-judge annotators provide detailed evaluations to identify dataset strengths and shortcomings.Our results reveal that 70% of the benchmark datasets fail to meet our heuristic quality standards. The correctness of the usage of technical terms is the strongest criterion, but 85% of the criteria are not satisfied in the examined datasets. Although LLM judges demonstrate potential, they are less effective than human annotators, particularly in understanding cultural common sense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger labeling capabilities for grammatical and technical tasks, while Llama3.3-70B excels at correctness and cultural knowledge evaluation. Our findings emphasize the urgent need for more rigorous quality control in creating and adapting datasets for low-resource languages.</abstract>
      <url hash="01e60cc9">2025.gem-1.41</url>
      <bibkey>umutlu-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="42">
      <title>Big Escape Benchmark: Evaluating Human-Like Reasoning in Language Models via Real-World Escape Room Challenges</title>
      <author><first>Zinan</first><last>Tang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>QiYao</first><last>Sun</last></author>
      <pages>488-503</pages>
      <abstract>Large Language Models (LLMs) have recently demonstrated remarkable reasoning capabilities across a wide range of tasks. While many benchmarks have been developed on specific academic subjects, coding, or constrained visual tasks, they often fail to fully capture the breadth, diversity, and dynamic nature of real-world human reasoning. Further, the creation of high-quality, complex multimodal reasoning benchmarks typically requires significant manual effort and expert annotation, which is costly and time-consuming.To address these limitations, we introduce Big Escape Bench, a novel multimodal reasoning benchmark derived from popular reality shows and television programs. Big Escape Bench leverages unique characteristics of TV content, providing a rich source of challenging and realistic multimodal reasoning problems. Key advantages include: questions guaranteed to be human-solvable and of moderate difficulty; problems reflecting diverse, real-world scenarios and knowledge domains; high inherent quality due to content generated by professional program teams.Notably, we develop an automated pipeline to construct the data from these programs into a standardized benchmark format, significantly reducing the manual effort compared to traditional dataset construction. We have conducted extensive experiments to evaluate state-of-the-art (SOTA) LLMs and Multimodal Large Language Models (MLLMs) on Big Escape Bench. Our results reveal a surprising performance gap: while the questions are easily solved by human viewers (about 60% in accuracy), the performance of even the most advanced models (best 40.50% in accuracy) is significantly lower than human-level accuracy. This highlights that despite recent progress, MLLMs still face substantial challenges in robustly performing the kind of diverse, dynamic, and context-dependent reasoning that is trivial for humans in routine situations. Big Escape Bench serves as a valuable tool for identifying current limitations of MLLMs and fostering future research towards more human-like multimodal reasoning.</abstract>
      <url hash="bf99a232">2025.gem-1.42</url>
      <bibkey>tang-sun-2025-big</bibkey>
    </paper>
    <paper id="43">
      <title>Event-based evaluation of abstractive news summarization</title>
      <author><first>Huiling</first><last>You</last></author>
      <author><first>Samia</first><last>Touileb</last><affiliation>University of Bergen</affiliation></author>
      <author><first>Lilja</first><last>Øvrelid</last><affiliation>Dept. of Informatics, University of Oslo</affiliation></author>
      <author><first>Erik</first><last>Velldal</last><affiliation>University of Oslo</affiliation></author>
      <pages>504-510</pages>
      <abstract>An abstractive summary of a news article contains its most important information in a condensed version. The evaluation of automatically generated summaries by generative language models relies heavily on human-authored summaries as gold references, by calculating overlapping units or similarity scores. News articles report events, and ideally so should the summaries. In this work, we propose to evaluate the quality of abstractive summaries by calculating overlapping events between generated summaries, reference summaries, and the original news articles. We experiment on a richly annotated Norwegian dataset comprising both events annotations and summaries authored by expert human annotators. Our approach provides more insight into the event information contained in the summaries.</abstract>
      <url hash="455e4cd8">2025.gem-1.43</url>
      <bibkey>you-etal-2025-event</bibkey>
    </paper>
    <paper id="46">
      <title>Fine-Tune on the Format: First Improving Multiple-Choice Evaluation for Intermediate <fixed-case>LLM</fixed-case> Checkpoints</title>
      <author><first>Alec</first><last>Bunn</last></author>
      <author><first>Sarah</first><last>Wiegreffe</last><affiliation>Allen Institute for Artificial Intelligence and University of Washington</affiliation></author>
      <author><first>Ben</first><last>Bogin</last><affiliation>Google and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>511-521</pages>
      <abstract>Evaluation of intermediate language model checkpoints during training is critical for effective model development and selection. How-ever, reliable evaluation using the popular multiple-choice question (MCQ) format is challenging, as small and non instruction-tunedmodels often lack the symbolic reasoning required for the task. This is despite the fact that MCQ evaluation is often used and needed todistinguish between the performance of different training runs. In particular, when prompted with a question and a set of labeled answerchoices (e.g., “A. . . . , B. . . . , C. . . . ”), many models struggle to emit the correct label (e.g., “C”), even when they can select the correct string answer choice. We propose an alternative evaluation method: fine-tuning the model on an auxiliary MCQ dataset prior to outputting labels. We validate this approach empirically by showing that training on auxiliary data improves MCQ ability on all our test datasets except 1. This approach provides a more accurate signal of model capability at intermediate checkpoints, as it disentangles the evaluation of core knowledge from the model’s emerging ability to follow formatting instructions.</abstract>
      <url hash="ad2157f9">2025.gem-1.46</url>
      <bibkey>bunn-etal-2025-fine</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>P</fixed-case>apers<fixed-case>P</fixed-case>lease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on <fixed-case>ERG</fixed-case> Theory</title>
      <author><first>Junho</first><last>Myung</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Yeon Su</first><last>Park</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sunwoo</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Shin</first><last>Yoo</last></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Google and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>522-531</pages>
      <abstract>Evaluating the performance and biases of large language models (LLMs) through role-playing scenarios is becoming increasingly common, as LLMs often exhibit biased behaviors in these contexts. Building on this line of research, we introduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed to investigate LLMs’ decision-making in prioritizing various levels of human needs. In our setup, LLMs act as immigration inspectors deciding whether to approve or deny entry based on the short narratives of people. These narratives are constructed using the Existence, Relatedness, and Growth (ERG) theory, which categorizes human needs into three hierarchical levels. Our analysis of six LLMs reveals statistically significant patterns in decision-making, suggesting that LLMs encode implicit preferences. Additionally, our evaluation of the impact of incorporating social identities into the narratives shows varying responsiveness based on both motivational needs and identity cues, with some models exhibiting higher denial rates for marginalized identities. All data is publicly available at https://github.com/yeonsuuuu28/papers-please.</abstract>
      <url hash="73fe3ad7">2025.gem-1.47</url>
      <bibkey>myung-etal-2025-papersplease</bibkey>
    </paper>
    <paper id="48">
      <title>Shallow Preference Signals: Large Language Model Aligns <fixed-case>E</fixed-case>ven Better with Truncated Data?</title>
      <author><first>Xuan</first><last>Qi</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Jiahao</first><last>Qiu</last><affiliation>Princeton University</affiliation></author>
      <author><first>Xinzhe</first><last>Juan</last></author>
      <author><first>Yue</first><last>Wu</last><affiliation>Princeton University</affiliation></author>
      <author><first>Mengdi</first><last>Wang</last><affiliation>Princeton University</affiliation></author>
      <pages>532-548</pages>
      <abstract>Aligning large language models (LLMs) with human preferences remains a key challenge in AI. Preference-based optimization methods, such as Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO), rely on human-annotated datasets to improve alignment. In this work, we identify a crucial property of the existing learning method: the distinguishing signal obtained in preferred responses is often concentrated in the early tokens. We refer to this as <tex-math>\textit{shallow preference signals}</tex-math>.To explore this property, we systematically truncate preference datasets at various points and train both reward models and DPO models on the truncated data. <tex-math>\textbf{Surprisingly}</tex-math>, models trained on truncated datasets, retaining only the first half or fewer tokens, achieve comparable or even superior performance to those trained on full datasets. For example, a reward model trained on the Skywork-Reward-Preference-80K-v0.2 dataset outperforms the full dataset when trained on a 40% truncated dataset. This pattern is consistent across multiple datasets, suggesting the widespread presence of <tex-math>\textit{shallow preference signals}</tex-math>.We further investigate the distribution of the reward signal through decoding strategies. We consider two simple decoding strategies motivated by the shallow reward signal observation, namely Length Control Decoding and KL Threshold Control Decoding, which leverage shallow preference signals to optimize the trade-off between alignment and computational efficiency. The performance is even better, which again validates our hypothesis.The phenomenon of <tex-math>\textit{shallow preference signals}</tex-math> highlights potential issues in LLM alignment: existing alignment methods often focus on aligning only the initial tokens of responses, rather than considering the full response. This could lead to discrepancies with real-world human preferences, resulting in suboptimal alignment performance.</abstract>
      <url hash="6eac24fc">2025.gem-1.48</url>
      <bibkey>qi-etal-2025-shallow</bibkey>
    </paper>
    <paper id="49">
      <title>Improving Large Language Model Confidence Estimates using Extractive Rationales for Classification</title>
      <author><first>Jane Arleth</first><last>dela Cruz</last></author>
      <author><first>Iris</first><last>Hendrickx</last></author>
      <author><first>Martha</first><last>Larson</last></author>
      <pages>549-560</pages>
      <abstract>The adoption of large language models (LLMs) in high-stake scenarios continues to be a challenge due to lack of effective confidence calibration. Although LLMs are capable of providing convincing self-explanations and verbalizing confidence in NLP tasks, they tend to exhibit overconfidence when using generative or free-text rationales (e.g. Chain-of-Thought), where reasoning steps tend to lack verifiable grounding.In this paper, we investigate whether adding explanations in the form of extractive rationales –snippets of the input text that directly support the predictions, can improve the confidence calibration of LLMs in classification tasks.We examine two approaches for integrating these rationales: (1) a one-stage rationale-generation with prediction and (2) a two-stage rationale-guided confidence calibration.We evaluate these approaches on a disaster tweet classification task using four different off-the-shelf LLMs. Our results show that extracting rationales both before and after prediction can improve the confidence estimates of the LLMs. Furthermore, we find that replacing valid extractive rationales with irrelevant ones significantly lowers model confidence, highlighting the importance of rationale quality.This simple yet effective method improves LLM verbalized confidence and reduces overconfidence in possible hallucination.</abstract>
      <url hash="ca82ec1f">2025.gem-1.49</url>
      <bibkey>cruz-etal-2025-improving</bibkey>
    </paper>
    <paper id="50">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0729-04: Human Evaluation Reproduction Report for “<fixed-case>M</fixed-case>em<fixed-case>S</fixed-case>um: Extractive Summarization of Long Documents Using Multi-Step Episodic <fixed-case>M</fixed-case>arkov Decision Processes”</title>
      <author><first>Simeon</first><last>Junker</last><affiliation>Universität Bielefeld</affiliation></author>
      <pages>561-567</pages>
      <abstract>Human evaluation is indispensable in natural language processing (NLP), as automatic metrics are known to not always align well with human judgments.However, the reproducibility of human evaluations can be problematic since results are susceptible to many factors, the details of which are often missing from the respective works.As part of the ReproHum project, this work aims to reproduce the human evaluation of a single criterion in the paper “MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes” (Gu et al, 2022).The results of our reproduction differ noticeably from those of the original study. To explain this discrepancy, we discuss differences in the experimental setup, as well as more general characteristics of the selected domain and the generated summaries.</abstract>
      <url hash="ec9e5502">2025.gem-1.50</url>
      <bibkey>junker-2025-reprohum</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0744-02: A Reproduction of the Human Evaluation of Meaning Preservation in “Factorising Meaning and Form for Intent-Preserving Paraphrasing”</title>
      <author><first>Julius</first><last>Steen</last><affiliation>Institute for Computational Linguistics, Heidelberg University, Heidelberg University</affiliation></author>
      <author><first>Katja</first><last>Markert</last><affiliation>Heidelberg University</affiliation></author>
      <pages>568-575</pages>
      <abstract>Assessing and improving the reproducibility of human evaluation studies is an ongoing concern in the area of natural language processing. As a contribution to this effort and a part of the ReproHum reproducibility project, we describe the reproduction of a human evaluation study (Hosking and Lapata, 2021) that evaluates meaning preservation in question paraphrasing systems.Our results indicate that the original study is highly reproducible given additional material and information provided by the authors. However, we also identify some aspects of the study that may make the annotation task potentially much easier than those in comparable studies. This might limit the representativeness of these results for best-practices in study design.</abstract>
      <url hash="1b95895b">2025.gem-1.51</url>
      <bibkey>steen-markert-2025-reprohum</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0031-01: Reproducing the Human Evaluation of Readability from “It is <fixed-case>AI</fixed-case>’s Turn to Ask Humans a Question”</title>
      <author><first>Daniel</first><last>Braun</last><affiliation>Unversity of Marburg</affiliation></author>
      <pages>576-582</pages>
      <abstract>The reproducibility of results is the foundation on which scientific credibility is built. In Natural Language Processing (NLP) research, human evaluation is often seen as the gold standard of evaluation. This paper presents the reproduction of a human evaluation of a Natural Language Generation (NLG) system that generates pairs of questions and answers based on children’s stories that was originally conducted by Yao et al. (2022). Specifically, it replicates the evaluation of readability, one of the most commonly evaluated criteria for NLG systems. The results of the reproduction are aligned with the original findings and all major claims of the original paper are confirmed.</abstract>
      <url hash="c2b10375">2025.gem-1.52</url>
      <bibkey>braun-2025-reprohum</bibkey>
    </paper>
    <paper id="53">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0033-05: Human Evaluation of Factuality from A Multidisciplinary Perspective</title>
      <author><first>Andra-Maria</first><last>Florescu</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Marius</first><last>Micluța-Câmpeanu</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Stefana Arina</first><last>Tabusca</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Liviu P</first><last>Dinu</last><affiliation>University of Bucharest</affiliation></author>
      <pages>583-589</pages>
      <abstract>The following paper is a joint contribution for the 2025 ReproNLP shared task, part of the ReproHum project. We focused on reproducing the human evaluation based on one criterion, namely, factuality of Scientific Automated Generated Systems from August et al. (2022). In accordance to the ReproHum guidelines, we followed the original study as closely as possible, with two human raters who coded 300 ratings each. Moreover, we had an additional study on two subsets of the dataset based on domain (medicine and physics) in which we employed expert annotators. Our reproduction of the factuality assessment found similar overall rates of factual inaccuracies across models. However, variability and weak agreement with the original model rankings suggest challenges in reliably reproducing results, especially in such cases when results are close.</abstract>
      <url hash="a77cf6ad">2025.gem-1.53</url>
      <bibkey>florescu-etal-2025-reprohum</bibkey>
    </paper>
    <paper id="54">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um: #0744-02: Investigating the Reproducibility of Semantic Preservation Human Evaluations</title>
      <author><first>Mohammad</first><last>Arvan</last></author>
      <author><first>Natalie</first><last>Parde</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>590-600</pages>
      <abstract>Reproducibility remains a fundamental challenge for human evaluation in Natural Language Processing (NLP), particularly due to the inherent subjectivity and variability of human judgments. This paper presents a reproduction study of the human evaluation protocol introduced by Hosking and Lapata (2021), which assesses semantic preservation in paraphrase generation models. By faithfully reproducing the original experiment with careful adaptation and applying the Quantified Reproducibility Assessment framework (Belz and Thomson, 2024a; Belz, 2022), we demonstrate strong agreement with the original findings, confirming the semantic preservation ranking among four paraphrase models. Our analyses reveal moderate inter-annotator agreement and low variability in key results, underscoring a good degree of reproducibility despite practical deviations in participant recruitment and platform. These findings highlight the feasibility and challenges of reproducing human evaluation studies in NLP. We discuss implications for improving methodological rigor, transparent reporting, and standardized protocols to bolster reproducibility in future human evaluations. The data and analysis scripts are publicly available to support ongoing community efforts toward reproducible evaluation in NLP and beyond.</abstract>
      <url hash="82f79986">2025.gem-1.54</url>
      <bibkey>arvan-parde-2025-reprohum</bibkey>
    </paper>
    <paper id="55">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0669-08: Reproducing Sentiment Transfer Evaluation</title>
      <author><first>Kristýna</first><last>Onderková</last><affiliation>, Charles University Prague</affiliation></author>
      <author><first>Mateusz</first><last>Lango</last><affiliation>Charles University and Poznan University of Technology</affiliation></author>
      <author><first>Patrícia</first><last>Schmidtová</last></author>
      <author><first>Ondrej</first><last>Dusek</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>601-608</pages>
      <abstract>We describe a reproduction of a human annotation experiment that was performed to evaluate the effectiveness of text style transfer systems (Reif et al., 2021). Despite our efforts to closely imitate the conditions of the original study, the results obtained differ significantly from those in the original study. We performed a statistical analysis of the results obtained, discussed the sources of these discrepancies in the study design, and quantified reproducibility. The reproduction followed the common approach to reproduction adopted by the ReproHum project.</abstract>
      <url hash="891f6fb4">2025.gem-1.55</url>
      <bibkey>onderkova-etal-2025-reprohum</bibkey>
    </paper>
    <paper id="56">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0067-01: A Reproduction of the Evaluation of Cross-Lingual Summarization</title>
      <author><first/><last>Supryadi</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Chuang</first><last>Liu</last><affiliation>National Supercomputing Center in Tianjin</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>609-614</pages>
      <abstract>Human evaluation is crucial as it offers a nuanced understanding that automated metrics often miss. By reproducing human evaluation, we can gain a better understanding of the original results. This paper is part of the ReproHum project, where our goal is to reproduce human evaluations from previous studies. We report the reproduction results of the human evaluation of cross-lingual summarization conducted by (CITATION). By comparing the original and reproduction studies, we find that our overall evaluation findings are largely consistent with those of the previous study. However, there are notable differences in evaluation scores between the two studies for certain model outputs. These discrepancies highlight the importance of carefully selecting evaluation methodologies and human annotators.</abstract>
      <url hash="8a15c028">2025.gem-1.56</url>
      <bibkey>supryadi-etal-2025-reprohum</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>R</fixed-case>epro<fixed-case>H</fixed-case>um #0729-04: Partial reproduction of the human evaluation of the <fixed-case>M</fixed-case>em<fixed-case>S</fixed-case>um and <fixed-case>N</fixed-case>eu<fixed-case>S</fixed-case>um summarisation systems</title>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Michela</first><last>Lorandi</last><affiliation>Dublin City University</affiliation></author>
      <pages>615-621</pages>
      <abstract>In this paper, we present our reproduction of part of the human evaluation originally carried out by Gu et al. (2022), as part of Track B of ReproNLP 2025. Four human annotators were asked to rank two candidate summaries according to their overall quality, given a reference summary shown alongside the two candidate summaries at evaluation time. We describe the original experiment and provide details about the steps we followed to carry out the reproduction experiment, including the implementation of some missing pieces of code. Our results, in particular the high coefficients of variation and low inter-annotator agreement, suggest a low level of reproducibility in the original experiment despite identical pairwise ranks. However, given the very small sample size (two systems, one rating), we remain cautious about drawing definitive conclusions.</abstract>
      <url hash="0d1fb313">2025.gem-1.57</url>
      <bibkey>mille-lorandi-2025-reprohum</bibkey>
    </paper>
    <paper id="58">
      <title>Curse of bilinguality: Evaluating monolingual and bilingual language models on <fixed-case>C</fixed-case>hinese linguistic benchmarks</title>
      <author><first>Yuwen</first><last>Zhou</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Yevgen</first><last>Matusevych</last><affiliation>University of Groningen</affiliation></author>
      <pages>622-630</pages>
      <abstract>We investigate cross-lingual transfer effects in large language models (LLMs) trained on two high-resource languages, English and Chinese. Four monolingual Chinese and four bilingual English–Chinese models are evaluated on two Chinese linguistic benchmarks. The monolingual models consistently outperform the bilingual ones on 12 out of 55 tasks, while the reverse is true for only 4 tasks, highlighting the prevalence of negative (rather than positive) transfer from English to Chinese. Additionally, we carry out a feature attribution analysis in a monolingual and a bilingual model, showing that the differences in their performance may be explained by more predictable attribution patterns in the monolingual model. Our findings have implications for the ongoing effort of training bilingual LLMs.</abstract>
      <url hash="b29c9eab">2025.gem-1.58</url>
      <bibkey>zhou-matusevych-2025-curse</bibkey>
    </paper>
    <paper id="59">
      <title>Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework</title>
      <author><first>Esteban</first><last>Garces Arias</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Hannah</first><last>Blocher</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Julian</first><last>Rodemann</last></author>
      <author><first>Meimingwei</first><last>Li</last></author>
      <author><first>Christian</first><last>Heumann</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Matthias</first><last>Aßenmacher</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>631-654</pages>
      <abstract>Open-ended text generation has become a prominent task in natural language processing due to the rise of powerful (large) language models. However, evaluating the quality of these models and the employed decoding strategies remains challenging due to trade-offs among widely used metrics such as coherence, diversity, and perplexity. This paper addresses the specific problem of multicriteria evaluation for open-ended text generation, proposing novel methods for both relative and absolute rankings of decoding methods. Specifically, we employ benchmarking approaches based on partial orderings and present a new summary metric to balance existing automatic indicators, providing a more holistic evaluation of text generation quality. Our experiments demonstrate that the proposed approaches offer a robust way to compare decoding strategies and serve as valuable tools to guide model selection for open-ended text generation tasks. We suggest future directions for improving evaluation methodologies in text generation and make our code, datasets, and models publicly available.</abstract>
      <url hash="c5d1928d">2025.gem-1.59</url>
      <bibkey>garces-arias-etal-2025-towards</bibkey>
    </paper>
    <paper id="60">
      <title>Bridging the <fixed-case>LLM</fixed-case> Accessibility Divide? Performance, Fairness, and Cost of Closed versus Open <fixed-case>LLM</fixed-case>s for Automated Essay Scoring</title>
      <author><first>Kezia</first><last>Oketch</last></author>
      <author><first>John P.</first><last>Lalor</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Ahmed</first><last>Abbasi</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>655-669</pages>
      <abstract>Closed large language models (LLMs) such as GPT-4 have set state-of-the-art results across a number of NLP tasks and have become central to NLP and machine learning (ML)-driven solutions. Closed LLMs’ performance and wide adoption has sparked considerable debate about their accessibility in terms of availability, cost, and transparency. In this study, we perform a rigorous comparative analysis of eleven leading LLMs, spanning closed, open, and open-source LLM ecosystems, across text assessment and generation within automated essay scoring, as well as a separate evaluation on abstractive text summarization to examine generalization. Our findings reveal that for few-shot learning-based assessment of human generated essays, open LLMs such as Llama 3 and Qwen 2.5 perform comparably to GPT-4 in terms of predictive performance, with no significant differences in disparate impact scores when considering age- or race-related fairness. For summarization, we find that open models also match GPT-4 in ROUGE and METEOR scores on the CNN/DailyMail benchmark, both in zero- and few-shot settings. Moreover, Llama 3 offers a substantial cost advantage, being up to 37 times more cost-efficient than GPT-4. For generative tasks, we find that essays generated by top open LLMs are comparable to closed LLMs in terms of their semantic composition/embeddings and ML assessed scores. Our findings challenge the dominance of closed LLMs and highlight the democratizing potential of open LLMs, suggesting they can effectively bridge accessibility divides while maintaining competitive performance and fairness.</abstract>
      <url hash="76d562fa">2025.gem-1.60</url>
      <bibkey>oketch-etal-2025-bridging</bibkey>
    </paper>
    <paper id="61">
      <title>Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting <fixed-case>LLM</fixed-case>s for In-Context Learning in Low-Resource Languages</title>
      <author><first>Christopher</first><last>Toukmaji</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Jeffrey</first><last>Flanigan</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>670-704</pages>
      <abstract>LLMs are typically trained in high-resource languages, and tasks in lower-resourced languages tend to underperform the higher-resource language counterparts for in-context learning. Despite the large body of work on prompting settings, it is still unclear how LLMs should be adapted cross-lingually specifically for in-context learning in the low-resource target languages. We perform a comprehensive study spanning five diverse target languages, three base LLMs, and seven downstream tasks spanning over 4,100 GPU training hours (9,900+ TFLOPs) across various adaptation techniques: few-shot prompting, translate-test, fine-tuning, embedding re-initialization, and instruction fine-tuning. Our results show that the few-shot prompting and translate-test settings tend to heavily outperform the gradient-based adaptation methods. To better understand this discrepancy, we design a novel metric, Valid Output Recall (VOR), and analyze model outputs to empirically attribute the degradation of these trained models to catastrophic forgetting. To the extent of our knowledge, this is the largest study done on in-context learning for low-resource languages with respect to train compute and number of adaptation techniques considered. We make all our datasets and trained models available for public use.</abstract>
      <url hash="d11b9484">2025.gem-1.61</url>
      <bibkey>toukmaji-flanigan-2025-prompt</bibkey>
    </paper>
    <paper id="62">
      <title>Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in <fixed-case>QA</fixed-case> Agents</title>
      <author><first>Ashley</first><last>Lewis</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>705-727</pages>
      <abstract>The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination—generating false information—and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models’ outputs, e.g., GPT-4o), and find that self-training achieves comparable hallucination reduction. We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized “I don’t know” responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.</abstract>
      <url hash="37b92a82">2025.gem-1.62</url>
      <bibkey>lewis-2025-winning</bibkey>
    </paper>
    <paper id="63">
      <title>Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating Large Language Models</title>
      <author><first>Sherzod</first><last>Hakimov</last><affiliation>Universität Potsdam</affiliation></author>
      <author><first>Lara</first><last>Pfennigschmidt</last><affiliation>NA</affiliation></author>
      <author><first>David</first><last>Schlangen</last><affiliation>University of Potsdam</affiliation></author>
      <pages>728-740</pages>
      <abstract>This study utilizes the game Codenames as a benchmarking tool to evaluate large language models (LLMs) with respect to specific linguistic and cognitive skills. LLMs play each side of the game, where one side generates a clue word covering several target words and the other guesses those target words. We designed various experiments by controlling the choice of words (abstract vs. concrete words, ambiguous vs. monosemic) or the opponent (programmed to be faster or slower in revealing words). Recent commercial and open-weight models were compared side-by-side to find out factors affecting their performance. The evaluation reveals details about their strategies, challenging cases, and limitations of LLMs.</abstract>
      <url hash="8ffaaf93">2025.gem-1.63</url>
      <bibkey>hakimov-etal-2025-ad</bibkey>
    </paper>
    <paper id="64">
      <title>Evaluating Intermediate Reasoning of Code-Assisted Large Language Models for Mathematics</title>
      <author><first>Zena Al</first><last>Khalili</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Nick</first><last>Howell</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>741-758</pages>
      <abstract>Assisting LLMs with code generation improved their performanceon mathematical reasoning tasks.However, the evaluation of code-assisted LLMs is generally restricted to execution correctness, lacking a rigorous evaluation of their generated programs.In this work, we bridge this gap by conducting an in-depth analysis of code-assisted LLMs generated programs in response to math reasoning tasks, with a focus on evaluating the soundness of the underlying reasoning processes. For this purpose, we assess the generations of five LLMs, on several math datasets, both manually and automatically, and propose a taxonomy of generated programs based on their logical soundness.Our findings show that the capabilities of models significantly impact the logic implemented to solve the problem. Closed-source LLMs ground their programs in mathematical concepts, whereas open-source models often resort to unsound reasoning, relying on memorized information and exhaustive searches. Furthermore, increasing the difficulty of problems decreases sound generations for all models, revealing a critical shortcoming of LLMs on complex mathematics, contrary to what accuracy metrics suggest.Our work highlights the need for more holistic evaluations of code-assisted LLMs beyond execution accuracy metrics, toward a better understanding of LLMs’ limits in the math domain.</abstract>
      <url hash="fdd8d180">2025.gem-1.64</url>
      <bibkey>khalili-etal-2025-evaluating</bibkey>
    </paper>
    <paper id="65">
      <title>From Calculation to Adjudication: Examining <fixed-case>LLM</fixed-case> Judges on Mathematical Reasoning Tasks</title>
      <author><first>Andreas</first><last>Stephan</last></author>
      <author><first>Dawei</first><last>Zhu</last><affiliation>Amazon</affiliation></author>
      <author><first>Matthias</first><last>Aßenmacher</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <author><first>Benjamin</first><last>Roth</last><affiliation>Universität Vienna</affiliation></author>
      <pages>759-773</pages>
      <abstract>To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. The performance of LLM judges is typically evaluated by measuring the correlation with human judgments on generative tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that easy samples are easy to judge, and difficult samples are difficult to judge. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance, indicating that judges tend to favor higher-quality models even if their answer is incorrect. As a consequence, we test whether we can predict the behavior of LLM judges using simple features such as part-of-speech tags and find that we can correctly predict 70%-75% of judgments. We conclude this study by analyzing practical use cases, showing that LLM judges consistently detect the on-average better model but largely fail if we use them to improve task performance.</abstract>
      <url hash="cef43e1d">2025.gem-1.65</url>
      <bibkey>stephan-etal-2025-calculation</bibkey>
    </paper>
    <paper id="66">
      <title><fixed-case>P</fixed-case>ersona<fixed-case>T</fixed-case>win: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins</title>
      <author><first>Sihan</first><last>Chen</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>John P.</first><last>Lalor</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Ahmed</first><last>Abbasi</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>774-788</pages>
      <abstract>While large language models (LLMs) afford new possibilities for user modeling and approximation of human behaviors, they often fail to capture the multidimensional nuances of individual users. In this work, we introduce <tex-math>\texttt{PersonaTwin}</tex-math>, a multi-tier prompt conditioning framework that builds adaptive digital twins by integrating demographic, behavioral, and psychometric data. Using a comprehensive data set in the healthcare context of more than 8,500 individuals, we systematically benchmark <tex-math>\texttt{PersonaTwin}</tex-math> against standard LLM outputs, and our rigorous evaluation unites state-of-the-art text similarity metrics with dedicated demographic parity assessments, ensuring that generated responses remain accurate and unbiased. Experimental results show that our framework produces simulation fidelity on par with oracle settings. Moreover, downstream models trained on persona-twins approximate models trained on individuals in terms of prediction and fairness metrics across both GPT-4o-based and Llama-based models. Together, these findings underscore the potential for LLM digital twin-based approaches in producing realistic and emotionally nuanced user simulations, offering a powerful tool for personalized digital user modeling and behavior analysis.</abstract>
      <url hash="64d203f5">2025.gem-1.66</url>
      <bibkey>chen-etal-2025-personatwin</bibkey>
    </paper>
    <paper id="67">
      <title>Coreference as an indicator of context scope in multimodal narrative</title>
      <author><first>Nikolai</first><last>Ilinykh</last><affiliation>Göteborg University</affiliation></author>
      <author><first>Shalom</first><last>Lappin</last></author>
      <author><first>Asad B.</first><last>Sayeed</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Sharid</first><last>Loáiciga</last><affiliation>University of Gothenburg, Sweden</affiliation></author>
      <pages>789-807</pages>
      <abstract>We demonstrate that large multimodal language models differ substantially from humans in the distribution of coreferential expressions in a visual storytelling task. We introduce a number of metrics to quantify the characteristics of coreferential patterns in both human- and machine-written texts. Humans distribute coreferential expressions in a way that maintains consistency across texts and images, interleaving references to different entities in a highly varied way. Machines are less able to track mixed references, despite achieving perceived improvements in generation quality. Materials, metrics, and code for our study are available at https://github.com/GU-CLASP/coreference-context-scope.</abstract>
      <url hash="32f1e7dc">2025.gem-1.67</url>
      <bibkey>ilinykh-etal-2025-coreference</bibkey>
    </paper>
    <paper id="68">
      <title><fixed-case>PATCH</fixed-case>! <fixed-case>P</fixed-case>sychometrics-<fixed-case>A</fixed-case>ssis<fixed-case>T</fixed-case>ed <fixed-case>B</fixed-case>en<fixed-case>CH</fixed-case>marking of Large Language Models against Human Populations: A Case Study of Proficiency in 8th Grade Mathematics</title>
      <author><first>Qixiang</first><last>Fang</last></author>
      <author><first>Daniel</first><last>Oberski</last><affiliation>Utrecht University</affiliation></author>
      <author><first>Dong</first><last>Nguyen</last><affiliation>Utrecht University</affiliation></author>
      <pages>808-823</pages>
      <abstract>Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs’ academic proficiency, often with also an interest in comparing model performance with human test takers’. While such benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics—a field dedicated to the measurement of latent variables like academic proficiency—into LLM benchmarking. We make four primary contributions. First, we reflect on current LLM benchmark developments and contrast them with psychometrics-based test development. Second, we introduce PATCH: a novel framework for Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses the aforementioned limitations. In particular, PATCH enables valid comparison between LLMs and human populations.Third, we demonstrate PATCH by measuring several LLMs’ proficiency in 8th grade mathematics against 56 human populations. We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on current benchmarking practices. Fourth, we release 4 high-quality datasets to support measuring and comparing LLM proficiency in grade school mathematics and science with human populations.</abstract>
      <url hash="3857d027">2025.gem-1.68</url>
      <bibkey>fang-etal-2025-patch</bibkey>
    </paper>
    <paper id="69">
      <title><fixed-case>MCQF</fixed-case>ormat<fixed-case>B</fixed-case>ench: Robustness Tests for Multiple-Choice Questions</title>
      <author><first>Hiroo</first><last>Takizawa</last><affiliation>Graduate University for Advanced Studies</affiliation></author>
      <author><first>Saku</first><last>Sugawara</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Akiko</first><last>Aizawa</last><affiliation>National Institute of Informatics</affiliation></author>
      <pages>824-846</pages>
      <abstract>Multiple-choice questions (MCQs) are often used to evaluate large language models (LLMs). They measure LLMs’ general common sense and reasoning abilities, as well as their knowledge in specific domains such as law and medicine. However, the robustness of LLMs to various question formats in MCQs has not been thoroughly evaluated. While there are studies on the sensitivity of LLMs to input variations, research into their responsiveness to different question formats is still limited. In this study, we propose a method to construct tasks to comprehensively evaluate the robustness against format changes of MCQs by decomposing the answering process into several steps. Using this dataset, we evaluate nine LLMs, such as Llama3-70B and Mixtral-8x7B. We find the lack of robustness to differences in the format of MCQs. It is crucial to consider whether the format of MCQs influences their evaluation scores when assessing LLMs using MCQ datasets.</abstract>
      <url hash="27febfae">2025.gem-1.69</url>
      <bibkey>takizawa-etal-2025-mcqformatbench</bibkey>
    </paper>
    <paper id="70">
      <title>(Dis)improved?! How Simplified Language Affects Large Language Model Performance across Languages</title>
      <author><first>Miriam</first><last>Anschütz</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Anastasiya</first><last>Damaratskaya</last></author>
      <author><first>Chaeeun Joy</first><last>Lee</last></author>
      <author><first>Arthur</first><last>Schmalz</last></author>
      <author><first>Edoardo</first><last>Mosca</last></author>
      <author><first>Georg</first><last>Groh</last><affiliation>Technical University Munich</affiliation></author>
      <pages>847-861</pages>
      <abstract>Simplified language enhances the accessibility and human understanding of texts. However, whether it also benefits large language models (LLMs) remains underexplored. This paper extensively studies whether LLM performance improves on simplified data compared to its original counterpart. Our experiments span six datasets and nine automatic simplification systems across three languages. We show that English models, including GPT-4o-mini, show a weak generalization and exhibit a significant performance drop on simplified data. This introduces an intriguing paradox: simplified data is helpful for humans but not for LLMs. At the same time, the performance in non-English languages sometimes improves, depending on the task and quality of the simplifier. Our findings offer a comprehensive view of the impact of simplified language on LLM performance and uncover severe implications for people depending on simple language.</abstract>
      <url hash="a79510b7">2025.gem-1.70</url>
      <bibkey>anschutz-etal-2025-dis</bibkey>
    </paper>
    <paper id="71">
      <title>Fine-Grained Constraint Generation-Verification for Improved Instruction-Following</title>
      <author><first>Zhixiang</first><last>Liang</last></author>
      <author><first>Zhenyu</first><last>Hou</last></author>
      <author><first>Xiao</first><last>Wang</last></author>
      <pages>862-879</pages>
      <abstract>The ability of Large Language Models (LLMs) to follow natural language instructions is crucial. However, numerous studies have demonstrated that LLMs still struggle to follow instructions with complex constraints, limiting their application in other areas. Meanwhile, obtaining high-quality instruction-following data often requires substantial manual annotation, which is both time-consuming and labor-intensive. In this work, we present FiGV, a fine-grained constraint generation-verification strategy for synthesizing instruction-following data. FiGV employs LLM-driven processes to generate fine-grained constraints and check the legality of the synthetic instructions. Subsequently, LLMs are utilized to perform nuanced, constraint-level verification to determine whether the generated responses adhere to the synthetic instructions, with LLM-generated functions incorporated for auxiliary validation tailored to the types of constraints. Experiments on 7B to 70B models demonstrate that FiGV consistently achieves strong performance across various benchmarks designed to evaluate the instruction-following capabilities of LLMs.</abstract>
      <url hash="bcdb71a0">2025.gem-1.71</url>
      <bibkey>liang-etal-2025-fine</bibkey>
    </paper>
    <paper id="72">
      <title>Finance Language Model Evaluation (<fixed-case>FL</fixed-case>a<fixed-case>ME</fixed-case>)</title>
      <author><first>Glenn</first><last>Matlin</last></author>
      <author><first>Mika</first><last>Okamoto</last><affiliation>Georgia Tech Research Institute and Georgia Institute of Technology</affiliation></author>
      <author><first>Huzaifa</first><last>Pardawala</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Sudheer</first><last>Chava</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>880-926</pages>
      <abstract>Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs’ performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against ‘reasoning-reinforced’ LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.</abstract>
      <url hash="110d3d46">2025.gem-1.72</url>
      <bibkey>matlin-etal-2025-finance</bibkey>
    </paper>
    <paper id="73">
      <title>s<fixed-case>P</fixed-case>hin<fixed-case>X</fixed-case>: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting</title>
      <author><first>Sanchit</first><last>Ahuja</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Kumar</first><last>Tanmay</last></author>
      <author><first>Hardik Hansrajbhai</first><last>Chauhan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Barun</first><last>Patra</last><affiliation>Microsoft</affiliation></author>
      <author><first>Kriti</first><last>Aggarwal</last><affiliation>HippocraticAI</affiliation></author>
      <author><first>Luciano Del</first><last>Corro</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Arindam</first><last>Mitra</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Tejas Indulal</first><last>Dhamecha</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Vishrav</first><last>Chaudhary</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>927-946</pages>
      <abstract>Despite the remarkable success of large language models (LLMs) in English, a significant performance gap remains in non-English languages. To address this, we introduce a novel approach for strategically constructing a multilingual synthetic instruction tuning dataset, sPhinX. Unlike prior methods that directly translate fixed instruction-response pairs, sPhinX enhances diversity by selectively augmenting English instruction-response pairs with multilingual translations. Additionally, we propose LANGIT, a novel N-shot guided fine-tuning strategy, which further enhances model performance by incorporating contextually relevant examples in each training sample. Our ablation study shows that our approach enhances the multilingual capabilities of Mistral-7B and Phi-3-Small improving performance by an average of 39.8% and 11.2%, respectively, across multilingual benchmarks in reasoning, question answering, reading comprehension, and machine translation. Moreover, sPhinX maintains strong performance on English LLM benchmarks while exhibiting minimal to no catastrophic forgetting, even when trained on 51 languages.</abstract>
      <url hash="8e81dc5b">2025.gem-1.73</url>
      <bibkey>ahuja-etal-2025-sphinx</bibkey>
    </paper>
    <paper id="74">
      <title>Single- vs. Dual-Prompt Dialogue Generation with <fixed-case>LLM</fixed-case>s for Job Interviews in Human Resources</title>
      <author><first>Joachim</first><last>De Baer</last><affiliation>Universiteit Gent</affiliation></author>
      <author><first>A. Seza</first><last>Doğruöz</last><affiliation>Ghent University</affiliation></author>
      <author><first>Thomas</first><last>Demeester</last><affiliation>Ghent University - imec</affiliation></author>
      <author><first>Chris</first><last>Develder</last><affiliation>Universiteit Gent</affiliation></author>
      <pages>947-957</pages>
      <abstract>Optimizing language models for use in conversational agents requires large quantities of example dialogues. Increasingly, these dialogues are synthetically generated by using powerful large language models (LLMs), especially in domains where obtaining authentic human data is challenging. One such domain is human resources (HR). In this context, we compare two LLM-based dialogue generation methods for producing HR job interviews, and assess which method generates higher-quality dialogues, i.e., those more difficult to distinguish from genuine human discourse. The first method uses a single prompt to generate the complete interview dialog. The second method uses two agents that converse with each other. To evaluate dialogue quality under each method, we ask a judge LLM to determine whether AI was used for interview generation, using pairwise interview comparisons. We empirically find that, at the expense of a sixfold increase in token count, interviews generated with the dual-prompt method achieve a win rate 2 to 10 times higher than those generated with the single-prompt method. This difference remains consistent regardless of whether GPT-4o or Llama 3.3 70B is used for either interview generation or quality judging.</abstract>
      <url hash="9b0cd2a0">2025.gem-1.74</url>
      <bibkey>de-baer-etal-2025-single</bibkey>
    </paper>
    <paper id="75">
      <title>Natural Language Counterfactual Explanations in Financial Text Classification: A Comparison of Generators and Evaluation Metrics</title>
      <author><first>Karol</first><last>Dobiczek</last></author>
      <author><first>Patrick</first><last>Altmeyer</last></author>
      <author><first>Cynthia C. S.</first><last>Liem</last><affiliation>Delft University of Technology</affiliation></author>
      <pages>958-972</pages>
      <abstract>The use of large language model (LLM) classifiers in finance and other high-stakes domains calls for a high level of trustworthiness and explainability. We focus on counterfactual explanations (CE), a form of explainable AI that explains a model’s output by proposing an alternative to the original input that changes the classification. We use three types of CE generators for LLM classifiers and assess the quality of their explanations on a recent dataset consisting of central bank communications. We compare the generators using a selection of quantitative and qualitative metrics. Our findings suggest that non-expert and expert evaluators prefer CE methods that apply minimal changes; however, the methods we analyze might not handle the domain-specific vocabulary well enough to generate plausible explanations. We discuss shortcomings in the choice of evaluation metrics in the literature on text CE generators and propose refined definitions of the fluency and plausibility qualitative metrics.</abstract>
      <url hash="cbbfd9c5">2025.gem-1.75</url>
      <bibkey>dobiczek-etal-2025-natural</bibkey>
    </paper>
    <paper id="76">
      <title>An Analysis of Datasets, Metrics and Models in Keyphrase Generation</title>
      <author><first>Florian</first><last>Boudin</last><affiliation>University of Nantes</affiliation></author>
      <author><first>Akiko</first><last>Aizawa</last><affiliation>National Institute of Informatics</affiliation></author>
      <pages>973-973</pages>
      <abstract>Keyphrase generation refers to the task of producing a set of words or phrases that summarises the content of a document. Continuous efforts have been dedicated to this task over the past few years, spreading across multiple lines of research, such as model architectures, data resources, and use-case scenarios. Yet, the current state of keyphrase generation remains unknown as there has been no attempt to review and analyse previous work. In this paper, we bridge this gap by presenting an analysis of over 50 research papers on keyphrase generation, offering a comprehensive overview of recent progress, limitations, and open challenges. Our findings highlight several critical issues in current evaluation practices, such as the concerning similarity among commonly-used benchmark datasets and inconsistencies in metric calculations leading to overestimated performances. Additionally, we address the limited availability of pre-trained models by releasing a strong PLM-based model for keyphrase generation as an effort to facilitate future research.</abstract>
      <url hash="0dd2486a">2025.gem-1.76</url>
      <bibkey>boudin-aizawa-2025-analysis</bibkey>
    </paper>
    <paper id="77">
      <title><fixed-case>U</fixed-case>-<fixed-case>MATH</fixed-case>: A University-Level Benchmark for Evaluating Mathematical Skills in Large Language Models</title>
      <author><first>Konstantin</first><last>Chernyshev</last></author>
      <author><first>Vitaliy</first><last>Polshkov</last><affiliation>Toloka AI</affiliation></author>
      <author><first>Vlad</first><last>Stepanov</last><affiliation>Gradarius (Castle Point Learning Systems)</affiliation></author>
      <author><first>Alex</first><last>Myasnikov</last></author>
      <author><first>Ekaterina</first><last>Artemova</last><affiliation>Toloka AI</affiliation></author>
      <author><first>Alexei</first><last>Miasnikov</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Sergei</first><last>Tilga</last><affiliation>Toloka AI</affiliation></author>
      <pages>974-1001</pages>
      <abstract>Current evaluations of mathematical skills in Large Language Models are constrained by benchmarks lacking scope, particularly for multi-modal problems — frequently relying on school-level, niche Olympiad-style, simple quiz-format, or relatively small datasets.To address this, we introduce **U-MATH**, a novel benchmark comprising **1,100** unpublished open-ended university-level problems sourced from current US curricula, with **20%** incorporating visual elements. Given the free-form nature of U-MATH problems, we employ LLM judges for solution evaluation and release <tex-math>\boldsymbol{\mu}</tex-math>**-MATH**, a meta-evaluation benchmark composed of **1,084** U-MATH-derived tasks enabling precise assessment of these judges.Benchmarking leading LLMs reveals marked limitations in multi-modal reasoning, with maximum accuracy reaching 93.1% on textual tasks but only 58.5% on visual ones. Furthermore, solution judgment proves challenging, requiring the most advanced models to achieve meaningfully high performance, even still peaking at an imperfect F1-score of 90.1%.</abstract>
      <url hash="7dbc9cfd">2025.gem-1.77</url>
      <bibkey>chernyshev-etal-2025-u</bibkey>
    </paper>
    <paper id="78">
      <title>The 2025 <fixed-case>R</fixed-case>epro<fixed-case>NLP</fixed-case> Shared Task on Reproducibility of Evaluations in <fixed-case>NLP</fixed-case>: Overview and Results</title>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Craig</first><last>Thomson</last><affiliation>Dublin City University and University of Aberdeen</affiliation></author>
      <author><first>Javier</first><last>González Corbelle</last><affiliation>Universidad de Santiago de Compostela</affiliation></author>
      <author><first>Malo</first><last>Ruelle</last></author>
      <pages>1002-1016</pages>
      <abstract>This paper presents an overview of, and the results from, the 2025 Shared Task on Reproducibility of Evaluations in NLP (ReproNLP’25) which followed on from four previous shared tasks on reproducibility of evaluations, ReproNLP’24, ReproNLP’23, ReproGen’22 and ReproGen’21. This shared task series forms part of an ongoing research programme designed to develop theory and practice of reproducibility assessment in NLP and machine learning, against a backdrop of increasing recognition of the importance of the topic across the two fields. We describe the ReproNLP’25 shared task, summarise results from the reproduction studies submitted, and provide additional comparative analysis of their results, including for the first time additional, ‘sanity-check’ evaluations by LLMs.</abstract>
      <url hash="fd772eec">2025.gem-1.78</url>
      <bibkey>belz-etal-2025-2025</bibkey>
    </paper>
  </volume>
</collection>
