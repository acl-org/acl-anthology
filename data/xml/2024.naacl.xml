<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.naacl">
  <volume id="long" ingest-date="2024-06-10" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</booktitle>
      <editor><first>Kevin</first><last>Duh</last></editor>
      <editor><first>Helena</first><last>Gomez</last></editor>
      <editor><first>Steve</first><last>Bethard</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="e59ea355">2024.naacl-long</url>
      <venue>naacl</venue>
    </meta>
    <frontmatter>
      <url hash="85822b4a">2024.naacl-long.0</url>
    </frontmatter>
    <paper id="1">
      <title>Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences</title>
      <author><first>Hongyi</first><last>Liu</last></author>
      <author><first>Qingyun</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Payam</first><last>Karisani</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>1-21</pages>
      <abstract>Named entity recognition is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of transfer learning for enhancing a named entity recognition model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). A common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. In our experiments, we observed that such a model is prone to mislabeling the source entities, which can often appear in the text, as the target entities. To alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, but, at the same time, to project the source entities and target entities into separate regions of the feature space. This diminishes the risk of mislabeling the source entities as the target entities. Our model consists of two stages: 1) entity grouping in the source domain, which incorporates knowledge from annotated events to establish relations between entities, and 2) entity discrimination in the target domain, which relies on pseudo labeling and contrastive learning to enhance discrimination between the entities in the two domains. We conduct our extensive experiments across three source and three target datasets, demonstrating that our method outperforms the baselines by up to 5% absolute value. Code, data, and resources are publicly available for research purposes: https://github.com/Lhtie/Bio-Domain-Transfer .</abstract>
      <url hash="4346fd3d">2024.naacl-long.1</url>
    </paper>
    <paper id="2">
      <title>Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation</title>
      <author><first>Hongyi</first><last>Yuan</last></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chuanqi</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Songfang</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>22-39</pages>
      <abstract>The diffusion model, a new generative modeling paradigm, has achieved great success in image, audio, and video generation.However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder-decoder Transformer architecture.To improve the generation performance, SeqDiffuSeq is equipped with the self-conditioning technique and our newly proposed adaptive noise schedule technique. Self-conditioning enables SeqDiffuSeq to better use the predicted sequence information during the generation process.The adaptive noise schedule balances the difficulty of denoising across time steps at the token level.Experiment results illustrate the improved performance on five sequence-to-sequence generation tasks compared to other diffusion-based models regarding text quality and inference time.</abstract>
      <url hash="67d193b4">2024.naacl-long.2</url>
    </paper>
    <paper id="3">
      <title>An Interactive Framework for Profiling News Media Sources</title>
      <author><first>Nikhil</first><last>Mehta</last></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University, Purdue University and Purdue University</affiliation></author>
      <pages>40-58</pages>
      <abstract>The recent rise of social media has led to the spread of large amounts of fake and biased news, content published with the intent to sway beliefs. While detecting and profiling the sources that spread this news is important to maintain a healthy society, it is challenging for automated systems.In this paper, we propose an interactive framework for news media profiling. It combines the strengths of graph based news media profiling models, Pre-trained Large Language Models, and human insight to characterize the social context on social media. Experimental results show that with as little as 5 human interactions, our framework can rapidly detect fake and biased news media, even in the most challenging settings of emerging news events, where test data is unseen.</abstract>
      <url hash="0e7eb818">2024.naacl-long.3</url>
    </paper>
    <paper id="4">
      <title>Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study</title>
      <author><first>Yinghao</first><last>Li</last></author>
      <author><first>Haorui</first><last>Wang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>59-81</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable proficiency in language understanding and have been successfully applied to a variety of real-world tasks through task-specific fine-tuning or prompt engineering. Despite these advancements, it remains an open question whether LLMs are fundamentally capable of reasoning and planning, or if they primarily rely on recalling and synthesizing information from their training data. In our research, we introduce a novel task—Minesweeper—specifically designed in a format unfamiliar to LLMs and absent from their training datasets. This task challenges LLMs to identify the locations of mines based on numerical clues provided by adjacent opened cells. Successfully completing this task requires an understanding of each cell’s state, discerning spatial relationships between the clues and mines, and strategizing actions based on logical deductions drawn from the arrangement of the cells. Our experiments, including trials with the advanced GPT-4 model, indicate that while LLMs possess the foundational abilities required for this task, they struggle to integrate these into a coherent, multi-step logical reasoning process needed to solve Minesweeper. These findings highlight the need for further research to understand the nature of reasoning capabilities in LLMs under similar circumstances, and to explore pathways towards more sophisticated AI reasoning and planning models.</abstract>
      <url hash="d6894bce">2024.naacl-long.4</url>
    </paper>
    <paper id="5">
      <title><fixed-case>T</fixed-case>el<fixed-case>ME</fixed-case>: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation</title>
      <author><first>Taeyang</first><last>Yun</last></author>
      <author><first>Hyunkuk</first><last>Lim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jeonghwan</first><last>Lee</last></author>
      <author><first>Min</first><last>Song</last></author>
      <pages>82-95</pages>
      <abstract>Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue sys- tems to effectively respond to user requests. The emotions in a conversation can be identi- fied by the representations from various modal- ities, such as audio, visual, and text. How- ever, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a lan- guage model acting as the teacher to the non- verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multi- modal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effec- tiveness of our components through additional experiments.</abstract>
      <url hash="a727725b">2024.naacl-long.5</url>
    </paper>
    <paper id="6">
      <title>Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries</title>
      <author><first>Seanie</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jianpeng</first><last>Cheng</last></author>
      <author><first>Joris</first><last>Driesen</last><affiliation>Apple</affiliation></author>
      <author><first>Alexandru</first><last>Coca</last></author>
      <author><first>Anders</first><last>Johannsen</last></author>
      <pages>96-111</pages>
      <abstract>Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations.A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We validate our retrieval approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The experimental results show a significant improvement over relevant baselines in real few-shot DST settings.</abstract>
      <url hash="bdc67f0c">2024.naacl-long.6</url>
    </paper>
    <paper id="7">
      <title>Promptly Predicting Structures: The Return of Inference</title>
      <author><first>Maitrey</first><last>Mehta</last><affiliation>University of Utah</affiliation></author>
      <author><first>Valentina</first><last>Pyatkin</last></author>
      <author><first>Vivek</first><last>Srikumar</last><affiliation>University of Utah</affiliation></author>
      <pages>112-130</pages>
      <abstract>Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors. Many NLP tasks are naturally structured: that is, their outputs consist of multiple labels which constrain each other. Annotating data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm be extended to such structured outputs? In this paper, we present a framework for constructing zero- and few-shot linguistic structure predictors. Our key insight is that we can use structural constraints—and combinatorial inference derived from them—to filter out inconsistent structures predicted by large language models. We instantiated this framework on two structured prediction tasks, and five datasets. Across all cases, our results show that enforcing consistency not only constructs structurally valid outputs, but also improves performance over the unconstrained variants.</abstract>
      <url hash="0d52333d">2024.naacl-long.7</url>
    </paper>
    <paper id="8">
      <title>On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Yutong</first><last>Shao</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ndapa</first><last>Nakashole</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>131-156</pages>
      <abstract>Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear.This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model’s ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model’s internal mechanisms, including the ego-centric nature of structure node encodings and the potential for model compression due to modality fusion redundancy. Overall, this work sheds light on the inner workings of linearization-based methods and could potentially provide guidance for future research.</abstract>
      <url hash="8b129231">2024.naacl-long.8</url>
    </paper>
    <paper id="9">
      <title>Extractive Summarization with Text Generator</title>
      <author><first>Thang</first><last>Le</last><affiliation>VinAI Research</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>157-174</pages>
      <abstract>Standard extractive systems suffer from the lack of gold training signals since existing corpora solely provide document and human-written summary pairs while disregarding extractive labels. As a result, existing methods resort to imperfect pseudo-labels that are both biased and error-prone, thereby hindering the learning process of extractive models. In contrast, text generators which are commonly employed in abstractive summarization can effortlessly overcome this predicament on account of flexible sequence-to-sequence architectures. Motivated to bypass this inherent limitation, we investigate the possibility of conducting extractive summarization with text generators. Through extensive experiments covering six summarization benchmarks, we show that high-quality extractive summaries can be assembled via approximating the outputs (abstractive summaries) of these generators. Moreover, we find that the approximate summaries correlate positively with the auxiliary summaries (i.e. a better generator enables the production of better extractive summaries). Our results signify a new paradigm for training extractive summarizers i.e. learning with generation (abstractive) objectives rather than extractive schemes.</abstract>
      <url hash="55192487">2024.naacl-long.9</url>
    </paper>
    <paper id="10">
      <title>Self-generated Replay Memories for Continual Neural Machine Translation</title>
      <author><first>Michele</first><last>Resta</last></author>
      <author><first>Davide</first><last>Bacciu</last><affiliation>University of Pisa</affiliation></author>
      <pages>175-191</pages>
      <abstract>Modern Neural Machine Translation systems exhibit strong performance in several different languages and are constantly improving. Their ability to learn continuously is, however, still severely limited by the catastrophic forgetting issue. In this work, we leverage a key property of encoder-decoder Transformers, i.e. their generative ability, to propose a novel approach to continually learning Neural Machine Translation systems. We show how this can effectively learn on a stream of experiences comprising different languages, by leveraging a replay memory populated by using the model itself as a generator of parallel sentences. We empirically demonstrate that our approach can counteract catastrophic forgetting without requiring explicit memorization of training data. Code will be publicly available upon publication.</abstract>
      <url hash="b47da4d2">2024.naacl-long.10</url>
    </paper>
    <paper id="11">
      <title>Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models</title>
      <author><first>Yangyi</first><last>Chen</last><affiliation>School of Computer Science, University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Karan</first><last>Sikka</last><affiliation>SRI International</affiliation></author>
      <author><first>Michael</first><last>Cogswell</last><affiliation>SRI International</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Ajay</first><last>Divakaran</last><affiliation>SRI International</affiliation></author>
      <pages>192-210</pages>
      <abstract>Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.</abstract>
      <url hash="c1019da4">2024.naacl-long.11</url>
    </paper>
    <paper id="12">
      <title>Building Knowledge-Guided Lexica to Model Cultural Variation</title>
      <author><first>Shreya</first><last>Havaldar</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Salvatore</first><last>Giorgi</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Sunny</first><last>Rai</last><affiliation>School of Engineering and Applied Science, University of Pennsylvania</affiliation></author>
      <author><first>Thomas</first><last>Talhelm</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Sharath Chandra</first><last>Guntuku</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>211-226</pages>
      <abstract>Cultural variation exists between nations (e.g., the United States vs. China), but also within regions (e.g., California vs. Texas, Los Angeles vs. San Francisco). Measuring this regional cultural variation can illuminate how and why people think and behave differently. Historically, it has been difficult to computationally model cultural variation due to a lack of training data and scalability constraints. In this work, we introduce a new research problem for the NLP community: How do we measure variation in cultural constructs across regions using language? We then provide a scalable solution: building knowledge-guided lexica to model cultural variation, encouraging future work at the intersection of NLP and cultural understanding. We also highlight modern LLMs’ failure to measure cultural variation or generate culturally varied language.</abstract>
      <url hash="8cd76d89">2024.naacl-long.12</url>
    </paper>
    <paper id="13">
      <title>Adaptive Rank Selections for Low-Rank Approximation of Language Models</title>
      <author><first>Shangqian</first><last>Gao</last></author>
      <author><first>Ting</first><last>Hua</last><affiliation>Samsung</affiliation></author>
      <author><first>Yen-Chang</first><last>Hsu</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Yilin</first><last>Shen</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Hongxia</first><last>Jin</last><affiliation>Samsung Research America AI center</affiliation></author>
      <pages>227-241</pages>
      <abstract>Singular Value Decomposition (SVD) or its weighted variants has significantly progressed in compressing language models. Previous works assume the same importance for all operations and assign the same number of ranks for different layers in a language model. However, such a uniform rank selection is sub-optimal since different operations (layers) have non-uniform demand in capacity. In other words, a desired SVD strategy should allocate more ranks for important operations and vice versa. However, a globally-optimized selection of ranks for neural networks is still an open problem, and this is a non-trivial challenge since the selection is discrete. In this work, we propose a novel binary masking mechanism for optimizing the number of ranks in a differentiable framework. Our strategy uses a novel regularization to enable the masking to comply with the SVD property where the ranks have sorted singular values. The experiments examined both types of language models, encoder-only and decoder-only models, including large language models like LLaMA. Our compressed model achieves much better accuracy than previous SVD and their SOTA variants. More interestingly, our method retains significantly better accuracy with zero or limited fine-tuning, proving the substantial advantage of adaptive rank selection.</abstract>
      <url hash="41c5f722">2024.naacl-long.13</url>
    </paper>
    <paper id="14">
      <title>An Empirical Study of Consistency Regularization for End-to-End Speech-to-Text Translation</title>
      <author><first>Pengzhi</first><last>Gao</last><affiliation>Baidu</affiliation></author>
      <author><first>Ruiqing</first><last>Zhang</last></author>
      <author><first>Zhongjun</first><last>He</last><affiliation>Baidu</affiliation></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last><affiliation>Baidu</affiliation></author>
      <pages>242-256</pages>
      <abstract>Consistency regularization methods, such as R-Drop (Liang et al., 2021) and CrossConST (Gao et al., 2023), have achieved impressive supervised and zero-shot performance in the neural machine translation (NMT) field. Can we also boost end-to-end (E2E) speech-to-text translation (ST) by leveraging consistency regularization? In this paper, we conduct empirical studies on intra-modal and cross-modal consistency and propose two training strategies, SimRegCR and SimZeroCR, for E2E ST in regular and zero-shot scenarios. Experiments on the MuST-C benchmark show that our approaches achieve state-of-the-art (SOTA) performance in most translation directions. The analyses prove that regularization brought by the intra-modal consistency, instead of the modality gap, is crucial for the regular E2E ST, and the cross-modal consistency could close the modality gap and boost the zero-shot E2E ST performance.</abstract>
      <url hash="a7861f85">2024.naacl-long.14</url>
    </paper>
    <paper id="15">
      <title>Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration</title>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Shaoguang</first><last>Mao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Wenshan</first><last>Wu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>257-279</pages>
      <abstract>Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds’ strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.</abstract>
      <url hash="346529c4">2024.naacl-long.15</url>
    </paper>
    <paper id="16">
      <title><fixed-case>FPT</fixed-case>: Feature Prompt Tuning for Few-shot Readability Assessment</title>
      <author><first>Ziyang</first><last>Wang</last></author>
      <author><first>Sanwoo</first><last>Lee</last><affiliation>Peking University</affiliation></author>
      <author><first>Hsiu-Yuan</first><last>Huang</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>280-295</pages>
      <abstract>Prompt-based methods have achieved promising results in most few-shot text classification tasks. However, for readability assessment tasks, traditional prompt methods lack crucial linguistic knowledge, which has already been proven to be essential.Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we extract linguistic features from the text and embed them into trainable soft prompts. Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTPnot only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses the previous leading methods that incorporate linguistic features. Also, our proposed model significantly outperforms the large language model gpt-3.5-turbo-16k in most cases. Our proposed method establishes a new architecture for prompt tuning that sheds light on how linguistic features can be easily adapted to linguistic-related tasks.</abstract>
      <url hash="5cce78d6">2024.naacl-long.16</url>
    </paper>
    <paper id="17">
      <title>Self-Prompting Large Language Models for Zero-Shot Open-Domain <fixed-case>QA</fixed-case></title>
      <author><first>Junlong</first><last>Li</last></author>
      <author><first>Jinyuan</first><last>Wang</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>296-310</pages>
      <abstract>Open-Domain Question Answering (ODQA) aims to answer questions without explicitly providing specific background documents. This task becomes notably challenging in a zero-shot setting where no data is available to train tailored retrieval-reader models.While recent Large Language Models (LLMs) like GPT-3 have demonstrated their effectiveness in zero-shot ODQA using direct prompting methods, these methods still fall short of fully harnessing the potential of LLMs when implicitly invoked.In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge encoded in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations entirely from scratch.These generated elements are then utilized for in-context learning. Experimental results show that our method significantly surpasses previous state-of-the-art zero-shot methods on three widely-used ODQA datasets and even achieves comparable performance with various customized fine-tuned models on full training data. Our code is available at https://github.com/lockon-n/self-prompting.</abstract>
      <url hash="0f7f5e30">2024.naacl-long.17</url>
    </paper>
    <paper id="18">
      <title>Head-to-Tail: How Knowledgeable are Large Language Models (<fixed-case>LLM</fixed-case>s)? <fixed-case>A</fixed-case>.<fixed-case>K</fixed-case>.<fixed-case>A</fixed-case>. Will <fixed-case>LLM</fixed-case>s Replace Knowledge Graphs?</title>
      <author><first>Kai</first><last>Sun</last><affiliation>Meta</affiliation></author>
      <author><first>Yifan</first><last>Xu</last></author>
      <author><first>Hanwen</first><last>Zha</last><affiliation>Facebook</affiliation></author>
      <author><first>Yue</first><last>Liu</last></author>
      <author><first>Xin Luna</first><last>Dong</last><affiliation>Department of Computer Science, University of Washington and Amazon</affiliation></author>
      <pages>311-325</pages>
      <abstract>Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.</abstract>
      <url hash="cce179d7">2024.naacl-long.18</url>
    </paper>
    <paper id="19">
      <title><tex-math>k</tex-math><fixed-case>NN</fixed-case>-<fixed-case>ICL</fixed-case>: Compositional Task-Oriented Parsing Generalization with Nearest Neighbor In-Context Learning</title>
      <author><first>Wenting</first><last>Zhao</last></author>
      <author><first>Ye</first><last>Liu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Yibo</first><last>Wang</last></author>
      <author><first>Qingyang</first><last>Wu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zhongfen</first><last>Deng</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Jiangshu</first><last>Du</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Shuaiqi</first><last>Liu</last></author>
      <author><first>Yunlong</first><last>Xu</last></author>
      <author><first>Philip</first><last>Yu</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>326-337</pages>
      <abstract>Task-Oriented Parsing (TOP) enables conversational assistants to interpret user commands expressed in natural language, transforming them into structured outputs that combine elements of both natural language and intent/slot tags. Recently, Large Language Models (LLMs) have achieved impressive performance in synthesizing computer programs based on a natural-language prompt, mitigating the gap between natural language and structured programs. Our paper focuses on harnessing the capabilities of LLMs for semantic parsing tasks, addressing the following three key research questions: 1) How can LLMs be effectively utilized for semantic parsing tasks? 2) What defines an effective prompt? and 3) How can LLM overcome the length constraint and streamline prompt design by including all examples as prompts? We introduce k Nearest Neighbor In-Context Learning (kNN-ICL), which simplifies prompt engineering by allowing it to be built on top of any design strategy while providing access to all demo examples. Extensive experiments show that: 1) Simple ICL without kNN search can achieve a comparable performance with strong supervised models on the TOP tasks, and 2) kNN-ICL significantly improves the comprehension of complex requests by seamlessly integrating ICL with a nearest-neighbor approach. Notably, this enhancement is achieved without the need for additional data or specialized prompts.</abstract>
      <url hash="98156e95">2024.naacl-long.19</url>
    </paper>
    <paper id="20">
      <title><fixed-case>ARES</fixed-case>: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems</title>
      <author><first>Jon</first><last>Saad-Falcon</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <author><first>Omar</first><last>Khattab</last></author>
      <author><first>Christopher</first><last>Potts</last><affiliation>Stanford University</affiliation></author>
      <author><first>Matei</first><last>Zaharia</last><affiliation>University of California, Berkeley and Databricks</affiliation></author>
      <pages>338-354</pages>
      <abstract>Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.</abstract>
      <url hash="9e2e1af1">2024.naacl-long.20</url>
    </paper>
    <paper id="21">
      <title><fixed-case>DEMO</fixed-case>: A Statistical Perspective for Efficient Image-Text Matching</title>
      <author><first>Fan</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Xian-Sheng</first><last>Hua</last><affiliation>Terminus Group</affiliation></author>
      <author><first>Chong</first><last>Chen</last><affiliation>Terminus Group</affiliation></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>355-369</pages>
      <abstract>Image-text matching has been a long-standing problem, which seeks to connect vision and language through semantic understanding. Due to the capability to manage large-scale raw data, unsupervised hashing-based approaches have gained prominence recently. They typically construct a semantic similarity structure using the natural distance, which subsequently guides the optimization of the hashing network. However, the similarity structure could be biased at the boundaries of semantic distributions, causing error accumulation during sequential optimization. To tackle this, we introduce a novel hashing approach termed Distribution-based Structure Mining with Consistency Learning (DEMO) for efficient image-text matching. From a statistical view, DEMO characterizes each image using multiple augmented views, which are considered as samples drawn from its intrinsic semantic distribution. Then, we employ a non-parametric distribution divergence to ensure a robust and precise similarity structure. In addition, we introduce collaborative consistency learning which not only preserves the similarity structure in the Hamming space but also encourages consistency between retrieval distribution from different directions in a self-supervised manner. Extensive experiments on several widely used datasets demonstrate that DEMO achieves superior performance compared with various state-of-the-art methods.</abstract>
      <url hash="8bf017d0">2024.naacl-long.21</url>
    </paper>
    <paper id="22">
      <title><fixed-case>S</fixed-case>ea<fixed-case>E</fixed-case>val for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning</title>
      <author><first>Bin</first><last>Wang</last><affiliation>I2R, A*STAR</affiliation></author>
      <author><first>Zhengyuan</first><last>Liu</last><affiliation>I2R</affiliation></author>
      <author><first>Xin</first><last>Huang</last></author>
      <author><first>Fangkai</first><last>Jiao</last></author>
      <author><first>Yang</first><last>Ding</last><affiliation>, A*STAR</affiliation></author>
      <author><first>AiTi</first><last>Aw</last><affiliation>I2R</affiliation></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>370-390</pages>
      <abstract>We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Many models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingually-trained models have not attained “balanced multilingual” capabilities. Our endeavors underscore the need for more generalizable semantic representations and enhanced multilingual contextualization. SeaEval can serve as a launchpad for more thorough investigations and evaluations for multilingual and multicultural scenarios.</abstract>
      <url hash="5c8e67d1">2024.naacl-long.22</url>
    </paper>
    <paper id="23">
      <title>Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision</title>
      <author><first>Seongyun</first><last>Lee</last></author>
      <author><first>Sue</first><last>Park</last></author>
      <author><first>Yongrae</first><last>Jo</last></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>391-404</pages>
      <abstract>Large multimodal models suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination is due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through qualitative analysis, we show that Volcano’s feedback is properly grounded on the image than the initial response. This indicates that Volcano can provide itself with richer visual information through feedback generation, leading to self-correct hallucinations. We publicly release our model, data, and code at https://github.com/kaistAI/Volcanogithub.com/kaistAI/Volcano</abstract>
      <url hash="a75ea99b">2024.naacl-long.23</url>
    </paper>
    <paper id="24">
      <title><fixed-case>LLM</fixed-case>s Are Few-Shot In-Context Low-Resource Language Learners</title>
      <author><first>Samuel</first><last>Cahyawijaya</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Holy</first><last>Lovenia</last><affiliation>AI Singapore</affiliation></author>
      <author><first>Pascale</first><last>Fung</last><affiliation>HKUST</affiliation></author>
      <pages>405-433</pages>
      <abstract>In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages.Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages.Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages.Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in. Our work highlights the importance of advancing ICL research, particularly for low-resource languages.</abstract>
      <url hash="76f2858c">2024.naacl-long.24</url>
    </paper>
    <paper id="25">
      <title>Simple and effective data augmentation for compositional generalization</title>
      <author><first>Yuekun</first><last>Yao</last></author>
      <author><first>Alexander</first><last>Koller</last><affiliation>Saarland University</affiliation></author>
      <pages>434-449</pages>
      <abstract>Compositional generalization, the ability to predict complex meanings from training on simpler sentences, poses challenges for powerful pretrained seq2seq models. In this paper, we show that data augmentation methods that sample MRs and backtranslate them can be effective for compositional generalization, but only if we sample from the right distribution. Remarkably, sampling from a uniform distribution performs almost as well as sampling from the test distribution, and greatly outperforms earlier methods that sampled from the training distribution.We further conduct experiments to investigate the reason why this happens and where the benefit of such data augmentation methods come from.</abstract>
      <url hash="c67b611b">2024.naacl-long.25</url>
    </paper>
    <paper id="26">
      <title>Rethinking Tabular Data Understanding with Large Language Models</title>
      <author><first>Tianyang</first><last>Liu</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>450-482</pages>
      <abstract>Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WikiTableQuestions, representing a substantial advancement over previous existing table processing paradigms of LLMs.</abstract>
      <url hash="5f73c16b">2024.naacl-long.26</url>
    </paper>
    <paper id="27">
      <title>From Shortcuts to Triggers: Backdoor Defense with Denoised <fixed-case>P</fixed-case>o<fixed-case>E</fixed-case></title>
      <author><first>Qin</first><last>Liu</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>483-496</pages>
      <abstract>Language models are often at risk of diverse backdoor attacks, especially data poisoning. Thus, it is important to investigate defense solutions for addressing them. Existing backdoor defense methods mainly focus on backdoor attacks with explicit triggers, leaving a universal defense against various backdoor attacks with diverse triggers largely unexplored. In this paper, we propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised Product-of-Experts), which is inspired by the shortcut nature of backdoor attacks, to defend various backdoor attacks. DPoE consists of two models: a shallow model that captures the backdoor shortcuts and a main model that is prevented from learning the shortcuts. To address the label flip caused by backdoor attackers, DPoE incorporates a denoising design. Experiments on three NLP tasks show that DPoE significantly improves the defense performance against various types of backdoor triggers including word-level, sentence-level, and syntactic triggers. Furthermore, DPoE is also effective under a more challenging but practical setting that mixes multiple types of triggers.</abstract>
      <url hash="242bc67e">2024.naacl-long.27</url>
    </paper>
    <paper id="28">
      <title><fixed-case>B</fixed-case>ook<fixed-case>SQL</fixed-case>: A Large Scale Text-to-<fixed-case>SQL</fixed-case> Dataset for Accounting Domain</title>
      <author><first>Rahul</first><last>Kumar</last></author>
      <author><first>Amar Raja</first><last>Dibbu</last></author>
      <author><first>Shrutendra</first><last>Harsola</last><affiliation>Intuit AI Bangalore India</affiliation></author>
      <author><first>Vignesh</first><last>Subrahmaniam</last></author>
      <author><first>Ashutosh</first><last>Modi</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>497-516</pages>
      <abstract>Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural language interfaces to databases have recently been proposed. These datasets cover a wide breadth of domains but fall short on some essential domains, such as finance and accounting. Given that accounting databases are used worldwide, particularly by non-technical people, there is an imminent need to develop models that could help extract information from accounting databases via natural language queries. In this resource paper, we aim to fill this gap by proposing a new large-scale Text-to-SQL dataset for the accounting and financial domain: BookSQL. The dataset consists of 100k natural language queries-SQL pairs, and accounting databases of 1 million records. We experiment with and analyze existing state-of-the-art models (including GPT-4) for the Text-to-SQL task on BookSQL. We find significant performance gaps, thus pointing towards developing more focused models for this domain.</abstract>
      <url hash="092bc3ee">2024.naacl-long.28</url>
    </paper>
    <paper id="29">
      <title><fixed-case>FLAP</fixed-case>: Flow-Adhering Planning with Constrained Decoding in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Shamik</first><last>Roy</last><affiliation>Amazon</affiliation></author>
      <author><first>Sailik</first><last>Sengupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Daniele</first><last>Bonadiman</last><affiliation>Amazon</affiliation></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <author><first>Arshit</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <pages>517-539</pages>
      <abstract>Planning is a crucial task for agents in task oriented dialogs (TODs). Human agents typically resolve user issues by following predefined workflows, decomposing workflow steps into actionable items, and performing actions by executing APIs in order; all of which require reasoning and planning. With the recent advances in LLMs, there have been increasing attempts to use them for task planning and API usage. However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs. Moreover, workflows in real life are often custom-defined and prone to changes; hence, adaptation is desirable. To study this, we propose the problem of faithful planning in TODs that needs to resolve user intents by following predefined flows and preserving API dependencies. To solve this problem, we propose <tex-math>\textbf{FLAP}</tex-math>, a <tex-math>\textbf{Fl}</tex-math>ow-<tex-math>\textbf{A}</tex-math>dhering <tex-math>\textbf{P}</tex-math>lanning algorithm based on constrained decoding with lookahead heuristic for LLMs. Our algorithm alleviates the need for finetuning LLMs using domain specific (plan/dependency) data, enables quick adaptation to predefined flows, and outperforms other decoding and prompting-based baselines. Further, our algorithm empowers smaller LLMs (<tex-math>\approx7</tex-math>B) to perform at par larger LLMs (<tex-math>\approx30</tex-math>B-40B).</abstract>
      <url hash="4b076856">2024.naacl-long.29</url>
    </paper>
    <paper id="30">
      <title><fixed-case>D</fixed-case>u<fixed-case>RE</fixed-case>: Dual Contrastive Self Training for Semi-Supervised Relation Extraction</title>
      <author><first>Yuxi</first><last>Feng</last></author>
      <author><first>Laks</first><last>Lakshmanan</last><affiliation>University of British Columbia</affiliation></author>
      <pages>540-555</pages>
      <abstract>Document-level Relation Extraction (RE) aims to extract relation triples from documents. Existing document-RE models typically rely on supervised learning which requires substantial labeled data. To alleviate the amount of human supervision, Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models whenever labeled data is insufficient. However, existing ST methods in RE fail to tackle the challenge of long-tail relations. In this work, we propose DuRE, a novel ST framework to tackle these problems. DuRE jointly models RE classification and text generation as a dual process. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We proposed a contrastive loss to leverage the signal of the RE classifier to improve generation quality. In addition, we propose a self-adaptive way to sample pseudo text from different relation classes. Experiments on two document-level RE tasks show that DuRE significantly boosts recall and F1 score with comparable precision, especially for long-tail relations against several strong baselines.</abstract>
      <url hash="fc2868f5">2024.naacl-long.30</url>
    </paper>
    <paper id="31">
      <title>Query-Efficient Textual Adversarial Example Generation for Black-Box Attacks</title>
      <author><first>Zhen</first><last>Yu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Zhenhua</first><last>Chen</last></author>
      <author><first>Kun</first><last>He</last><affiliation>Huazhong University of Sceince and Technology</affiliation></author>
      <pages>556-569</pages>
      <abstract>Deep neural networks for Natural Language Processing (NLP) have been demonstrated to be vulnerable to textual adversarial examples. Existing black-box attacks typically require thousands of queries on the target model, making them expensive in real-world applications. In this paper, we propose a new approach that guides the word substitutions using prior knowledge from the training set to improve the attack efficiency. Specifically, we introduce Adversarial Boosting Preference (ABP), a metric that quantifies the importance of words and guides adversarial word substitutions. We then propose two query-efficient attack strategies based on ABP: query-free attack (<tex-math>ABP_{free}</tex-math>) and guided search attack (<tex-math>ABP_{guide}</tex-math>). Extensive evaluations for text classification demonstrate that <tex-math>ABP_{free}</tex-math> generates more natural adversarial examples than existing universal attacks, <tex-math>ABP_{guide}</tex-math> significantly reduces the number of queries by a factor of 10 500 while achieving comparable or even better performance than black-box attack baselines. Furthermore, we introduce the first ensemble attack <tex-math>ABP_{ens}</tex-math> in NLP, which gains further performance improvements and achieves better transferability and generalization by the ensemble of the ABP across different models and domains. Code is available at https://github.com/BaiDingHub/ABP.</abstract>
      <url hash="d0c18946">2024.naacl-long.31</url>
    </paper>
    <paper id="32">
      <title>Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles</title>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Alexander</first><last>Fabbri</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Prafulla Kumar</first><last>Choubey</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <pages>570-593</pages>
      <abstract>Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.</abstract>
      <url hash="6f0d5b23">2024.naacl-long.32</url>
    </paper>
    <paper id="33">
      <title><fixed-case>AMRF</fixed-case>act: Enhancing Summarization Factuality Evaluation with <fixed-case>AMR</fixed-case>-Driven Negative Samples Generation</title>
      <author><first>Haoyi</first><last>Qiu</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Jingnong</first><last>Qu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>594-608</pages>
      <abstract>Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate our approach significantly outperforms previous systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating factuality of abstractive summarization.</abstract>
      <url hash="578881b4">2024.naacl-long.33</url>
    </paper>
    <paper id="34">
      <title><fixed-case>PILOT</fixed-case>: Legal Case Outcome Prediction with Case Law</title>
      <author><first>Lang</first><last>Cao</last></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Cao</first><last>Xiao</last><affiliation>GE Healthcare</affiliation></author>
      <author><first>Jimeng</first><last>Sun</last><affiliation>Georgia Tech Research Corporation, University of Illinois, Urbana Champaign, College of Computing and Georgia Institute of Technology</affiliation></author>
      <pages>609-621</pages>
      <abstract>Machine learning shows promise in predicting the outcome of legal cases, but most research has concentrated on civil law cases rather than case law systems. We identified two unique challenges in making legal case outcome predictions with case law. First, it is crucial to identify relevant precedent cases that serve as fundamental evidence for judges during decision-making. Second, it is necessary to consider the evolution of legal principles over time, as early cases may adhere to different legal contexts. In this paper, we proposed a new framework named PILOT (PredictIng Legal case OuTcome) for case outcome prediction. It comprises two modules for relevant case retrieval and temporal pattern handling, respectively. To benchmark the performance of existing legal case outcome prediction models, we curated a dataset from a large-scale case law database. We demonstrate the importance of accurately identifying precedent cases and mitigating the temporal shift when making predictions for case law, as our method shows a significant improvement over the prior methods that focus on civil law case outcome predictions.</abstract>
      <url hash="64edfd2b">2024.naacl-long.34</url>
    </paper>
    <paper id="35">
      <title><fixed-case>AL</fixed-case>o<fixed-case>RA</fixed-case>: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models</title>
      <author><first>Zequan</first><last>Liu</last></author>
      <author><first>Jiawen</first><last>Lyn</last></author>
      <author><first>Wei</first><last>Zhu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Xing</first><last>Tian</last></author>
      <pages>622-641</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on various tasks, and the experimental results demonstrate that our ALoRA method can outperform the recent baselines with comparable tunable parameters.</abstract>
      <url hash="b5755cb3">2024.naacl-long.35</url>
    </paper>
    <paper id="36">
      <title><fixed-case>R</fixed-case>-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces</title>
      <author><first>Heng-Jui</first><last>Chang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>642-662</pages>
      <abstract>This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific self-supervision method for speaker and noise-invariant speech representations by learning discrete acoustic units with speaker-invariant clustering (Spin). R-Spin resolves Spin’s issues and enhances content representations by learning to predict acoustic pieces. R-Spin offers a 12X reduction in computational resources compared to previous state-of-the-art methods while outperforming them in severely distorted speech scenarios. This paper provides detailed analyses to show how discrete units contribute to speech encoder training and improving robustness in diverse acoustic environments.</abstract>
      <url hash="295aa2e3">2024.naacl-long.36</url>
    </paper>
    <paper id="37">
      <title><fixed-case>I</fixed-case>ns<fixed-case>CL</fixed-case>: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions</title>
      <author><first>Yifan</first><last>Wang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Yafei</first><last>Liu</last><affiliation>OPPO</affiliation></author>
      <author><first>Chufan</first><last>Shi</last></author>
      <author><first>Haoling</first><last>Li</last></author>
      <author><first>Chen</first><last>Chen</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Haonan</first><last>Lu</last><affiliation>OPPO Guangdong Mobile Telecommunications Co., Ltd.</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Graduate School at Shenzhen,Tsinghua University</affiliation></author>
      <pages>663-677</pages>
      <abstract>Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue. However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy. In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions. Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions. According to InsInfo, InsCL guides the replay process more inclined to high-quality data. We conduct extensive experiments over 16 tasks with different training orders, observing consistent performance improvements of InsCL. When all tasks have been trained, InsCL achieves performance gains of 3.0 Relative Gain compared with Random Replay, and 27.96 Relative Gain compared with No Replay.</abstract>
      <url hash="2fca1330">2024.naacl-long.37</url>
    </paper>
    <paper id="38">
      <title>Language Agnostic Code Embeddings</title>
      <author><first>Saiteja</first><last>Utpala</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Alex</first><last>Gu</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Pin-Yu</first><last>Chen</last><affiliation>International Business Machines</affiliation></author>
      <pages>678-691</pages>
      <abstract>Recently, code language models have achieved notable advancements in addressing a diverse array of essential code comprehension and generation tasks. Yet, the field lacks a comprehensive deep dive and understanding of the code embeddings of multilingual code models. In this paper, we present a comprehensive study on multilingual code embeddings, focusing on the cross-lingual capabilities of these embeddings across different programming languages. Through probing experiments, we demonstrate that code embeddings comprise two distinct components: one deeply tied to the nuances and syntax of a specific language, and the other remaining agnostic to these details, primarily focusing on semantics. Further, we show that when we isolate and eliminate this language-specific component, we witness significant improvements in downstream code retrieval tasks, leading to an absolute increase of up to +17 in the Mean Reciprocal Rank (MRR).</abstract>
      <url hash="a4af8f56">2024.naacl-long.38</url>
    </paper>
    <paper id="39">
      <title>An Examination of the Compositionality of Large Generative Vision-Language Models</title>
      <author><first>Teli</first><last>Ma</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Rong</first><last>Li</last><affiliation>HKUST(GZ)</affiliation></author>
      <author><first>Junwei</first><last>Liang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>692-705</pages>
      <abstract>With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics ( VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a **SyntaxBias Score**, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely **S**ynt**A**ctically **DE**-biased benchmark (SADE). Our study provides an unbiased benchmark for the compositionality of GVLMs, facilitating future research in this direction. Code and dataset are available at https://github.com/TeleeMa/SADE.</abstract>
      <url hash="df4e7d0e">2024.naacl-long.39</url>
    </paper>
    <paper id="40">
      <title>Two Heads are Better than One: Nested <fixed-case>P</fixed-case>o<fixed-case>E</fixed-case> for Robust Defense Against Multi-Backdoors</title>
      <author><first>Victoria</first><last>Graf</last><affiliation>University of Southern California and Princeton University</affiliation></author>
      <author><first>Qin</first><last>Liu</last><affiliation>University of California, Davis</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>706-718</pages>
      <abstract>Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance. Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored. In this paper, we propose Nested Product of Experts (NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types. During NPoE training, the main modelis trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers. At inference time, only the main model is used. Experimental results on sentiment analysis, hate speech detection, and question classification tasks demonstrate that NPoE effectively defends against a variety of triggers both separately and in trigger mixtures. Due to the versatility of the MoE structure in NPoE, this framework can be further expanded to defend against other attack settings.</abstract>
      <url hash="7470d0ad">2024.naacl-long.40</url>
    </paper>
    <paper id="41">
      <title><fixed-case>V</fixed-case>ert<fixed-case>A</fixed-case>ttack: Taking Advantage of Text Classifiers’ Horizontal Vision</title>
      <author><first>Jonathan</first><last>Rusert</last><affiliation>Purdue University Fort Wayne</affiliation></author>
      <pages>719-732</pages>
      <abstract>Text classification systems have continuouslyimproved in performance over the years. How-ever, nearly all current SOTA classifiers have asimilar shortcoming, they process text in a hor-izontal manner. Vertically written words willnot be recognized by a classifier. In contrast,humans are easily able to recognize and readwords written both horizontally and vertically.Hence, a human adversary could write problem-atic words vertically and the meaning wouldstill be preserved to other humans. We simulatesuch an attack, VertAttack. VertAttack identifieswhich words a classifier is reliant on and thenrewrites those words vertically. We find thatVertAttack is able to greatly drop the accuracyof 4 different transformer models on 5 datasets.For example, on the SST2 dataset, VertAttackis able to drop RoBERTa’s accuracy from 94 to13%. Furthermore, since VertAttack does notreplace the word, meaning is easily preserved.We verify this via a human study and find thatcrowdworkers are able to correctly label 77%perturbed texts perturbed, compared to 81% ofthe original texts. We believe VertAttack offersa look into how humans might circumvent clas-sifiers in the future and thus inspire a look intomore robust algorithms.</abstract>
      <url hash="78045824">2024.naacl-long.41</url>
    </paper>
    <paper id="42">
      <title><fixed-case>KDMCSE</fixed-case>: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning</title>
      <author><first>Cong-Duy</first><last>Nguyen</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Thong</first><last>Nguyen</last></author>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>733-749</pages>
      <abstract>Previous work on multimodal sentence embedding has proposed multimodal contrastive learning and achieved promising results. However, by taking the rest of the batch as negative samples without reviewing when forming contrastive pairs, those studies encountered many suspicious and noisy negative examples, significantly affecting the methods’ overall performance. In this work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning of Sentence Embeddings), a novel approach that enhances the discrimination and generalizability of multimodal representation and inherits the knowledge from the teacher model to learn the difference between positive and negative instances and via that, can detect noisy and wrong negative samples effectively before they are calculated in the contrastive objective. Furthermore, to overcome the limitation of modeling the variation within negative pairs, we introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin Supervised Contrastive Learning for Multimodal sentence embeddings), that enhances the discriminative representation by strengthening the margin within the angular space while capturing varying semantics within the negative. Experimental results on widely used Semantic Textual Similarity (STS) benchmarks demonstrate the effectiveness of our approach.</abstract>
      <url hash="60007750">2024.naacl-long.42</url>
    </paper>
    <paper id="43">
      <title>The taste of <fixed-case>IPA</fixed-case>: Towards open-vocabulary keyword spotting and forced alignment in any language</title>
      <author><first>Jian</first><last>Zhu</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Changbing</first><last>Yang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Farhan</first><last>Samir</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Jahurul</first><last>Islam</last></author>
      <pages>750-772</pages>
      <abstract>In this project, we demonstrate that phoneme-based models for speech processing can achieve strong crosslinguistic generalizability to unseen languages. We curated the IPAPACK, a massively multilingual speech corpora with phonemic transcriptions, encompassing more than 115 languages from diverse language families, selectively checked by linguists. Based on the IPAPACK, we propose CLAP-IPA, a multi-lingual phoneme-speech contrastive embedding model capable of open-vocabulary matching between arbitrary speech signals and phonemic sequences. The proposed model was tested on 95 unseen languages, showing strong generalizability across languages. Temporal alignments between phonemes and speech signals also emerged from contrastive training, enabling zeroshot forced alignment in unseen languages. We further introduced a neural forced aligner IPA-ALIGNER by finetuning CLAP-IPA with the Forward-Sum loss to learn better phone-to-audio alignment. Evaluation results suggest that IPA-ALIGNER can generalize to unseen languages without adaptation.</abstract>
      <url hash="a6ed79e4">2024.naacl-long.43</url>
    </paper>
    <paper id="44">
      <title>Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks</title>
      <author><first>Yunqi</first><last>Zhang</last></author>
      <author><first>Songda</first><last>Li</last></author>
      <author><first>Chunyuan</first><last>Deng</last></author>
      <author><first>Luyi</first><last>Wang</last></author>
      <author><first>Hui</first><last>Zhao</last><affiliation>East China Normal University</affiliation></author>
      <pages>773-791</pages>
      <abstract>Gender bias in vision-language models (VLMs) can reinforce harmful stereotypes and discrimination. In this paper, we focus on mitigating gender bias towards vision-language tasks. We identify object hallucination as the essence of gender bias in VLMs. Existing VLMs tend to focus on salient or familiar attributes in images but ignore contextualized nuances. Moreover, most VLMs rely on the co-occurrence between specific objects and gender attributes to infer the ignored features, ultimately resulting in gender bias. We propose GAMA, a task-agnostic generation framework to mitigate gender bias. GAMA consists of two stages: narrative generation and answer inference. During narrative generation, GAMA yields all-sided but gender-obfuscated narratives, which prevents premature concentration on localized image features, especially gender attributes. During answer inference, GAMA integrates the image, generated narrative, and a task-specific question prompt to infer answers for different vision-language tasks. This approach allows the model to rethink gender attributes and answers. We conduct extensive experiments on GAMA, demonstrating its debiasing and generalization ability.</abstract>
      <url hash="01592fa0">2024.naacl-long.44</url>
    </paper>
    <paper id="45">
      <title><fixed-case>B</fixed-case>e<fixed-case>LLM</fixed-case>: Backward Dependency Enhanced Large Language Model for Sentence Embeddings</title>
      <author><first>Xianming</first><last>Li</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>792-804</pages>
      <abstract>Sentence embeddings are crucial in measuring semantic similarity. Most recent studies employed large language models (LLMs) to learn sentence embeddings. Existing LLMs mainly adopted autoregressive architecture without explicit backward dependency modeling. Therefore, we examined the effects of backward dependencies in LLMs for semantic similarity measurements. Concretely, we propose a novel model: backward dependency enhanced large language model (BeLLM). It learns sentence embeddings via transforming specific attention layers from uni- to bi-directional. We extensively experiment across various semantic textual similarity (STS) tasks and downstream applications. BeLLM achieves state-of-the-art performance in varying scenarios. It shows that autoregressive LLMs benefit from backward dependencies for sentence embeddings.</abstract>
      <url hash="7a02dd5d">2024.naacl-long.45</url>
    </paper>
    <paper id="46">
      <title>Assessing Factual Reliability of Large Language Model Knowledge</title>
      <author><first>Weixuan</first><last>Wang</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Wei</first><last>Peng</last></author>
      <pages>805-819</pages>
      <abstract>The factual knowledge of LLMs is typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability. How do we evaluate the capabilities of LLMs to consistently produce factually correct answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe (MONITOR), a novel metric designed to directly measure LLMs’ factual reliability. MONITOR is designed to compute the distance between the probability distributions of a valid output and its counterparts produced by the same LLM probing the same fact using different styles of prompts and contexts. Experiments on a comprehensive range of 12 LLMs demonstrate the effectiveness of MONITOR in evaluating the factual reliability of LLMs while maintaining a low computational overhead. In addition, we release the FKTC (Factual Knowledge Test Corpus) to foster research along this line https://github.com/Vicky-Wil/MONITOR.</abstract>
      <url hash="765a8e49">2024.naacl-long.46</url>
    </paper>
    <paper id="47">
      <title>Dial-<fixed-case>MAE</fixed-case>: <fixed-case>C</fixed-case>on<fixed-case>T</fixed-case>extual Masked Auto-Encoder for Retrieval-based Dialogue Systems</title>
      <author><first>Zhenpeng</first><last>Su</last></author>
      <author><first>Xing</first><last>W</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Guangyuan</first><last>Ma</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>820-830</pages>
      <abstract>Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Most existing works primarily focus on post-training and fine-tuning tailored for cross-encoders. However, there are no post-training methods tailored for dense encoders in dialogue response selection. We argue that when the current language model, based on dense dialogue systems (such as BERT), is employed as a dense encoder, it separately encodes dialogue context and response, leading to a struggle to achieve the alignment of both representations. Thus, we propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward yet effective post-training technique tailored for dense encoders in dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to compress the dialogue semantics into dense vectors, which achieves better alignment between the features of the dialogue context and response. Our experiments have demonstrated that Dial-MAE is highly effective, achieving state-of-the-art performance on two commonly evaluated benchmarks.</abstract>
      <url hash="f9f8943a">2024.naacl-long.47</url>
    </paper>
    <paper id="48">
      <title>Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model</title>
      <author><first>Cheng</first><last>Qian</last></author>
      <author><first>Chenyan</first><last>Xiong</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>831-854</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model’s creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain-of-thought approach. Further studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios. All codes and data are released.</abstract>
      <url hash="6779b346">2024.naacl-long.48</url>
    </paper>
    <paper id="49">
      <title>Create! Don’t Repeat: A Paradigm Shift in Multi-Label Augmentation through Label Creative Generation</title>
      <author><first>Letian</first><last>Wang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Xianggen</first><last>Liu</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Jiancheng</first><last>Lv</last><affiliation>Sichuan University</affiliation></author>
      <pages>855-869</pages>
      <abstract>We propose Label Creative Generation (LCG), a new paradigm in multi-label data augmentation. Beyond repeating data points with fixed labels, LCG creates new data by exploring innovative label combinations. Within LCG, we introduce Tail-Driven Conditional Augmentation (TDCA), combining tail-driven label sampling and label-conditioned text generation for balanced, consistent data augmentation. Our approach has demonstrated a **100.21%** increase in PSP@1 across three datasets, successfully mitigating the long-tail effect in MLTC and markedly enhancing model performance.</abstract>
      <url hash="cd6b8b67">2024.naacl-long.49</url>
    </paper>
    <paper id="50">
      <title>Neurocache: Efficient Vector Retrieval for Long-range Language Modeling</title>
      <author><first>Ali</first><last>Safaya</last></author>
      <author><first>Deniz</first><last>Yuret</last><affiliation>Koc University</affiliation></author>
      <pages>870-883</pages>
      <abstract>This paper introduces Neurocache, an approach to extend the effective context size of large language models (LLMs) using an external vector cache to store its past states. Like recent vector retrieval approaches, Neurocache uses an efficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states and incorporate them into the attention process. Neurocache improves upon previous methods by (1) storing compressed states, which reduces cache size; (2) performing a single retrieval operation per token which increases inference speed; and (3) extending the retrieval window to neighboring states, which improves both language modeling and downstream task accuracy. Our experiments show the effectiveness of Neurocache both for models trained from scratch and for pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the cache mechanism. We also compare Neurocache with text retrieval methods and show improvements in single-document question-answering and few-shot learning tasks. We made the source code available under: https://github.com/alisafaya/neurocache</abstract>
      <url hash="31577d1e">2024.naacl-long.50</url>
    </paper>
    <paper id="51">
      <title>Unveiling the Generalization Power of Fine-Tuned Large Language Models</title>
      <author><first>Haoran</first><last>Yang</last></author>
      <author><first>Yumeng</first><last>Zhang</last></author>
      <author><first>Jiaqi</first><last>Xu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Hongyuan</first><last>Lu</last></author>
      <author><first>Pheng-Ann</first><last>Heng</last></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>884-899</pages>
      <abstract>While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs’ generalization ability are not fully understood.This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets.Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks.Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model’s generalization ability.Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs.</abstract>
      <url hash="2535960c">2024.naacl-long.51</url>
    </paper>
    <paper id="52">
      <title>A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning</title>
      <author><first>Ruixin</first><last>Hong</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Xinyu</first><last>Pang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Changshui</first><last>Zhang</last><affiliation>Tsinghua University and Department of Computer Science and Technology</affiliation></author>
      <pages>900-925</pages>
      <abstract>Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a series of models on their verification abilities. Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods. Drawing from these observations, we offer suggestions for future research and practical applications of self-verification methods.</abstract>
      <url hash="7d4efcea">2024.naacl-long.52</url>
    </paper>
    <paper id="53">
      <title>Exploring Self-supervised Logic-enhanced Training for Large Language Models</title>
      <author><first>Fangkai</first><last>Jiao</last></author>
      <author><first>Zhiyang</first><last>Teng</last></author>
      <author><first>Bosheng</first><last>Ding</last></author>
      <author><first>Zhengyuan</first><last>Liu</last><affiliation>I2R</affiliation></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>926-941</pages>
      <abstract>Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains. Large Language Models (LLMs), with their capacity to condense vast knowledge, can effectively tackle many tasks. Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-the-art fine-tuning based models. To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised framework for integrating logical reasoning capabilities into LLMs, and activating them via in-context learning. We apply this to two LLM series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion. LogicLLM demonstrates its effectiveness through successful improvements on two logical reasoning benchmarks (ReClor and LogiQA-v2). Additionally, LogicLLM based on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without compromising the model’s general language understanding capabilities.</abstract>
      <url hash="34ba4ee5">2024.naacl-long.53</url>
    </paper>
    <paper id="54">
      <title><fixed-case>MATHSENSEI</fixed-case>: A Tool-Augmented Large Language Model for Mathematical Reasoning</title>
      <author><first>Debrup</first><last>Das</last></author>
      <author><first>Debopriyo</first><last>Banerjee</last><affiliation>Rakuten Global Inc.</affiliation></author>
      <author><first>Somak</first><last>Aditya</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Ashish</first><last>Kulkarni</last><affiliation>Rakuten</affiliation></author>
      <pages>942-966</pages>
      <abstract>Tool-augmented Large Language Models (TALMs) are known to enhance the skillset of large language models (LLMs), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complementary benefits offered by tools for knowledge retrieval and mathematical equation solving are open research questions. In this work, we present MathSensei, a tool-augmented large language model for mathematical reasoning. We study the complementary benefits of the tools - knowledge retriever (Bing Web Search), program generator + executor (Python), and symbolic equation solver (Wolfram-Alpha API) through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH, a popular dataset for evaluating mathematical reasoning on diverse mathematical disciplines. We also conduct experiments involving well-known tool planners to study the impact of tool sequencing on the model performance. MathSensei achieves 13.5% better accuracy over gpt-3.5-turbo with Chain-of-Thought on the MATH dataset. We further observe that TALMs are not as effective for simpler math word problems (in GSM-8K), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, MMLU-Math, and higher level complex questions in MATH). The code and data are available at https://github.com/Debrup-61/MathSensei.</abstract>
      <url hash="5184bb94">2024.naacl-long.54</url>
    </paper>
    <paper id="55">
      <title><fixed-case>C</fixed-case>o<fixed-case>UDA</fixed-case>: Coherence Evaluation via Unified Data Augmentation</title>
      <author><first>Dawei</first><last>Zhu</last></author>
      <author><first>Wenhao</first><last>Wu</last></author>
      <author><first>Yifan</first><last>Song</last></author>
      <author><first>Fangwei</first><last>Zhu</last></author>
      <author><first>Ziqiang</first><last>Cao</last></author>
      <author><first>Sujian</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <pages>967-978</pages>
      <abstract>Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of large language models. Due to the scarcity of annotated data, data augmentation is commonly used for training coherence evaluation models. However, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance.In this paper, we take inspiration from linguistic theory of discourse structure, and propose a data augmentation framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively.Especially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. During inference, CoUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse.Extensive experiments in coherence evaluation show that, with only 233M parameters, CoUDA achieves state-of-the-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics.</abstract>
      <url hash="ebb411d2">2024.naacl-long.55</url>
    </paper>
    <paper id="56">
      <title>m<fixed-case>E</fixed-case>d<fixed-case>IT</fixed-case>: Multilingual Text Editing via Instruction Tuning</title>
      <author><first>Vipul</first><last>Raheja</last><affiliation>Columbia University, Grammarly and International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Dimitris</first><last>Alikaniotis</last><affiliation>Grammarly</affiliation></author>
      <author><first>Vivek</first><last>Kulkarni</last></author>
      <author><first>Bashar</first><last>Alhafni</last><affiliation>New York University</affiliation></author>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <pages>979-1001</pages>
      <abstract>We introduce mEdIT, a multi-lingual extension to CoEdIT – the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as “Grammatik korrigieren” (German) or “이 텍스 트를 단순화” (Korean). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT generalizes effectively to new languages over multilingual baselines. We publicly release our data, code, and trained models.</abstract>
      <url hash="75e0377e">2024.naacl-long.56</url>
    </paper>
    <paper id="57">
      <title>Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning</title>
      <author><first>Yunchao</first><last>Zhang</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Zonglin</first><last>Di</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Kaiwen</first><last>Zhou</last></author>
      <author><first>Cihang</first><last>Xie</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Xin</first><last>Wang</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>1002-1016</pages>
      <abstract>Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Results on two VLN datasets (R2R and RxR) show that NAW can easily navigate the deployed VLN agent regardless of the language instruction, without affecting its performance on normal test sets. Then, we propose a new Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated VLN, which provides the server with a ”prompt” of the vision-and-language alignment variance between the benign and malicious clients so that they can be distinguished during training. We validate the effectiveness of the PBA method on protecting the global model from the NAW attack, which outperforms other state-of-the-art defense methods by a large margin in the defense metrics on R2R and RxR.</abstract>
      <url hash="5f971cba">2024.naacl-long.57</url>
    </paper>
    <paper id="58">
      <title>In-context Learning and Gradient Descent Revisited</title>
      <author><first>Gilad</first><last>Deutch</last></author>
      <author><first>Nadav</first><last>Magar</last></author>
      <author><first>Tomer</first><last>Natan</last></author>
      <author><first>Guy</first><last>Dar</last><affiliation>Tel Aviv University</affiliation></author>
      <pages>1017-1028</pages>
      <abstract>In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL.Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term <i>Layer Causality</i>. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significantly.</abstract>
      <url hash="84d51c07">2024.naacl-long.58</url>
    </paper>
    <paper id="59">
      <title>Corpus Considerations for Annotator Modeling and Scaling</title>
      <author><first>Sarumi</first><last>Oluyemi</last></author>
      <author><first>Béla</first><last>Neuendorf</last></author>
      <author><first>Joan</first><last>Plepi</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Jörg</first><last>Schlötterer</last><affiliation>Universität Mannheim and Phillips-Universität Marburg</affiliation></author>
      <author><first>Charles</first><last>Welch</last></author>
      <pages>1029-1040</pages>
      <abstract>Recent trends in natural language processing research and annotation tasks affirm a paradigm shift from the traditional reliance on a single ground truth to a focus on individual perspectives, particularly in subjective tasks. In scenarios where annotation tasks are meant to encompass diversity, models that solely rely on the majority class labels may inadvertently disregard valuable minority perspectives. This oversight could result in the omission of crucial information and, in a broader context, risk disrupting the balance within larger ecosystems. As the landscape of annotator modeling unfolds with diverse representation techniques, it becomes imperative to investigate their effectiveness with the fine-grained features of the datasets in view. This study systematically explores various annotator modeling techniques and compares their performance across seven corpora. From our findings, we show that the commonly used user token model consistently outperforms more complex models. We introduce a composite embedding approach and show distinct differences in which model performs best as a function of the agreement with a given dataset. Our findings shed light on the relationship between corpus statistics and annotator modeling performance, which informs future work on corpus construction and perspectivist NLP.</abstract>
      <url hash="9de18986">2024.naacl-long.59</url>
    </paper>
    <paper id="60">
      <title>On Large Language Models’ Hallucination with Regard to Known Facts</title>
      <author><first>Che</first><last>Jiang</last></author>
      <author><first>Biqing</first><last>Qi</last><affiliation>Tsinghua University and Harbin Institute of Technology</affiliation></author>
      <author><first>Xiangyu</first><last>Hong</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Dayuan</first><last>Fu</last></author>
      <author><first>Yang</first><last>Cheng</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Mo</first><last>Yu</last><affiliation>WeChat AI, Tencent</affiliation></author>
      <author><first>Bowen</first><last>Zhou</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>1041-1053</pages>
      <abstract>Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen.Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space.We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token’s information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model.Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88% success rate. Our study shed light on understanding the reasons for LLMs’ hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.</abstract>
      <url hash="e44799cd">2024.naacl-long.60</url>
    </paper>
    <paper id="61">
      <title>“One-Size-Fits-All”? Examining Expectations around What Constitute “Fair” or “Good” <fixed-case>NLG</fixed-case> System Behaviors</title>
      <author><first>Li</first><last>Lucy</last><affiliation>Allen Institute for Artificial Intelligence and University of California Berkeley</affiliation></author>
      <author><first>Su Lin</first><last>Blodgett</last><affiliation>Microsoft</affiliation></author>
      <author><first>Milad</first><last>Shokouhi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hanna</first><last>Wallach</last><affiliation>Microsoft</affiliation></author>
      <author><first>Alexandra</first><last>Olteanu</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>1054-1089</pages>
      <abstract>Fairness-related assumptions about what constitute appropriate NLG system behaviors range from invariance, where systems are expected to behave identically for social groups, to adaptation, where behaviors should instead vary across them. To illuminate tensions around invariance and adaptation, we conduct five case studies, in which we perturb different types of identity-related language features (names, roles, locations, dialect, and style) in NLG system inputs. Through these cases studies, we examine people’s expectations of system behaviors, and surface potential caveats of these contrasting yet commonly held assumptions. We find that motivations for adaptation include social norms, cultural differences, feature-specific information, and accommodation; in contrast, motivations for invariance include perspectives that favor prescriptivism, view adaptation as unnecessary or too difficult for NLG systems to do appropriately, and are wary of false assumptions. Our findings highlight open challenges around what constitute “fair” or “good” NLG system behaviors.</abstract>
      <url hash="caa045b8">2024.naacl-long.61</url>
    </paper>
    <paper id="62">
      <title>Language Models Hallucinate, but May Excel at Fact Verification</title>
      <author><first>Jian</first><last>Guan</last></author>
      <author><first>Jesse</first><last>Dodge</last></author>
      <author><first>David</first><last>Wadden</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Hao</first><last>Peng</last><affiliation>Department of Computer Science, University of Illinois Urbana-Champaign</affiliation></author>
      <pages>1090-1111</pages>
      <abstract>Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently “hallucinate,” resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly, FLAN-T5-11B , the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insights for developing trustworthy generation models.</abstract>
      <url hash="ca0f59ed">2024.naacl-long.62</url>
    </paper>
    <paper id="63">
      <title>A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution</title>
      <author><first>Bowen</first><last>Ding</last><affiliation>Westlake University</affiliation></author>
      <author><first>Qingkai</first><last>Min</last></author>
      <author><first>Shengkun</first><last>Ma</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yingjie</first><last>Li</last><affiliation>Westlake University</affiliation></author>
      <author><first>Linyi</first><last>Yang</last><affiliation>Westlake University</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>1112-1140</pages>
      <abstract>Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the state-of-the-art system exhibits an excessive reliance on the ‘triggers lexical matching’ spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three popular cross-document ECR benchmarks and demonstrates robustness in out-of-domain scenarios.</abstract>
      <url hash="1f97b809">2024.naacl-long.63</url>
    </paper>
    <paper id="64">
      <title><fixed-case>T</fixed-case>roj<fixed-case>FSP</fixed-case>: Trojan Insertion in Few-shot Prompt Tuning</title>
      <author><first>Mengxin</first><last>Zheng</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Jiaqi</first><last>Xue</last></author>
      <author><first>Xun</first><last>Chen</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Yanshan</first><last>Wang</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Qian</first><last>Lou</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Lei</first><last>Jiang</last><affiliation>Indiana University</affiliation></author>
      <pages>1141-1151</pages>
      <abstract>Prompt tuning is one of the most effective solutions to adapting a fixed pre-trained language model (PLM) for various downstream tasks, especially with only a few input samples. However, the security issues, e.g., Trojan attacks, of prompt tuning on a few data samples are not well-studied. Transferring established data poisoning attacks directly to few-shot prompt tuning presents multiple challenges. One significant issue is the _poisoned imbalance issue_, where non-target class samples are added to the target class, resulting in a greater number of target-class samples compared to non-target class. While this issue is not critical in regular tuning, it significantly hampers the few-shot prompt tuning, making it difficult to simultaneously achieve a high attack success rate (ASR) and maintain clean data accuracy (CDA). Additionally, few-shot prompting is prone to overfitting in terms of both ASR and CDA. In this paper, we introduce _TrojFSP_, a method designed to address the challenges. To solve the poisoned imbalance issue, we develop a _Target-Class Shrink (TC-Shrink)_ technique, which aims to equalize the number of poisoning samples. To combat overfitting, we employ a _Selective Token Poisoning_ technique to boost attack performance. Furthermore, we introduce a _Trojan-Trigger Attention_ objective function to amplify the attention of the poisoned trojan prompt on triggers. Experiments show that our TrojFSP achieves an ASR of over 99% while maintaining negligible decreases in CDA across various PLMs and datasets. The source code of TrojFSP is available at _https://github.com/UCF-ML-Research/TrojFSP_.</abstract>
      <url hash="ddb0e1a3">2024.naacl-long.64</url>
    </paper>
    <paper id="65">
      <title>Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models</title>
      <author><first>Yi</first><last>Luo</last></author>
      <author><first>Zhenghao</first><last>Lin</last></author>
      <author><first>YuHao</first><last>Zhang</last></author>
      <author><first>Jiashuo</first><last>Sun</last></author>
      <author><first>Chen</first><last>Lin</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Chengjin</first><last>Xu</last><affiliation>International Digital Economy Academy</affiliation></author>
      <author><first>Xiangdong</first><last>Su</last><affiliation>Inner Mongolia University</affiliation></author>
      <author><first>Yelong</first><last>Shen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jian</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <pages>1152-1197</pages>
      <abstract>Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, establishing a comprehensive library of guidelines and a model for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with relevant guidelines, which guide LLMs in response generation to ensure safe and high-quality outputs, thereby aligning with human values. An additional optional stage involves fine-tuning a model with well-aligned datasets generated through the process implemented in the second stage.Our method customizes guidelines to accommodate diverse inputs, thereby enhancing the fine-grainedness and comprehensiveness of the guideline library. Furthermore, it incorporates safety expertise from a safety-trained LLM through a lightweight retrieval model.We evaluate our approach on three benchmarks, demonstrating significant improvements in LLM security and quality. Notably, our fine-tuned model, Labrador, even at 13 billion parameters, outperforms GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.</abstract>
      <url hash="503d9f9f">2024.naacl-long.65</url>
    </paper>
    <paper id="66">
      <title><fixed-case>X</fixed-case>-<fixed-case>PARADE</fixed-case>: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs</title>
      <author><first>Juan</first><last>Rodriguez</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Katrin</first><last>Erk</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Greg</first><last>Durrett</last><affiliation>University of Texas, Austin</affiliation></author>
      <pages>1198-1222</pages>
      <abstract>Understanding when two pieces of text convey the same information is a goal touching many subproblems in NLP, including textual entailment and fact-checking. This problem becomes more complex when those two pieces of text are in different languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences. Annotators label a paragraph in a target language at the span level and evaluate it with respect to a corresponding paragraph in a source language, indicating whether a given piece of information is the same, new, or new but can be inferred. This last notion establishes a link with cross-language NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages, reflecting real information divergences observed in the wild. Armed with our dataset, we investigate a diverse set of approaches for this problem, including classic token alignment from machine translation, textual entailment methods that localize their decisions, and prompting LLMs. Our results show that these methods vary in their capability to handle inferable information, but they all fall short of human performance.</abstract>
      <url hash="9a1de0fe">2024.naacl-long.66</url>
    </paper>
    <paper id="67">
      <title>Topics, Authors, and Institutions in Large Language Model Research: Trends from 17<fixed-case>K</fixed-case> ar<fixed-case>X</fixed-case>iv Papers</title>
      <author><first>Rajiv</first><last>Movva</last><affiliation>Cornell University</affiliation></author>
      <author><first>Sidhika</first><last>Balachandar</last><affiliation>Department of Computer Science, Cornell University</affiliation></author>
      <author><first>Kenny</first><last>Peng</last><affiliation>Cornell University</affiliation></author>
      <author><first>Gabriel</first><last>Agostini</last><affiliation>Cornell University</affiliation></author>
      <author><first>Nikhil</first><last>Garg</last><affiliation>Cornell University</affiliation></author>
      <author><first>Emma</first><last>Pierson</last><affiliation>Cornell Tech</affiliation></author>
      <pages>1223-1243</pages>
      <abstract>Large language models (LLMs) are dramatically influencing AI research, spurring discussions on what has changed so far and how to shape the field’s future. To clarify such questions, we analyze a new dataset of 16,979 LLM-related arXiv papers, focusing on recent trends in 2023 vs. 2018-2022. First, we study disciplinary shifts: LLM research increasingly considers societal impacts, evidenced by 20<tex-math>\times</tex-math> growth in LLM submissions to the Computers and Society sub-arXiv. An influx of new authors – half of all first authors in 2023 – are entering from non-NLP fields of CS, driving disciplinary expansion. Second, we study industry and academic publishing trends. Surprisingly, industry accounts for a smaller publication share in 2023, largely due to reduced output from Google and other Big Tech companies; universities in Asia are publishing more. Third, we study institutional collaboration: while industry-academic collaborations are common, they tend to focus on the same topics that industry focuses on rather than bridging differences. The most prolific institutions are all US- or China-based, but there is very little cross-country collaboration. We discuss implications around (1) how to support the influx of new authors, (2) how industry trends may affect academics, and (3) possible effects of (the lack of) collaboration.</abstract>
      <url hash="4bda82fc">2024.naacl-long.67</url>
    </paper>
    <paper id="68">
      <title><tex-math>E^5</tex-math>: Zero-shot Hierarchical Table Analysis using Augmented <fixed-case>LLM</fixed-case>s via Explain, Extract, Execute, Exhibit and Extrapolate</title>
      <author><first>Zhehao</first><last>Zhang</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Jian-Guang</first><last>Lou</last><affiliation>Microsoft</affiliation></author>
      <pages>1244-1258</pages>
      <abstract>Analyzing large hierarchical tables with multi-level headers presents challenges due to their complex structure, implicit semantics, and calculation relationships. While recent advancements in large language models (LLMs) have shown promise in flat table analysis, their application to hierarchical tables is constrained by the reliance on manually curated exemplars and the model’s token capacity limitations. Addressing these challenges, we introduce a novel code-augmented LLM-based framework, <tex-math>E^5</tex-math>, for zero-shot hierarchical table question answering. This approach encompasses self-explaining the table’s hierarchical structures, code generation to extract relevant information and apply operations, external code execution to prevent hallucinations, and leveraging LLMs’ reasoning for final answer derivation. Empirical results indicate that our method, based on GPT-4, outperforms state-of-the-art fine-tuning methods with a 44.38 Exact Match improvement. Furthermore, we present <tex-math>F^3</tex-math>, an adaptive algorithm designed for token-limited scenarios, effectively condensing large tables while maintaining useful information. Our experiments prove its efficiency, enabling the processing of large tables even with models having limited context lengths. The code is available at https://github.com/zzh-SJTU/E5-Hierarchical-Table-Analysis.</abstract>
      <url hash="11ed4a02">2024.naacl-long.68</url>
    </paper>
    <paper id="69">
      <title><fixed-case>S</fixed-case>3<fixed-case>E</fixed-case>val: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Model</title>
      <author><first>Fangyu</first><last>Lei</last></author>
      <author><first>Qian</first><last>Liu</last><affiliation>Sea AI Lab</affiliation></author>
      <author><first>Yiming</first><last>Huang</last></author>
      <author><first>Shizhu</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>1259-1286</pages>
      <abstract>The rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like long-context understanding and reasoning.However, as LLMs are able to process longer contexts, it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration.In this paper, we propose using complex synthetic tasks as a proxy evaluation method, and present S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation.The synthetic nature of S3Eval provides users full control over the dataset, allowing them to systematically probe LLM capabilities by scaling text length and varying task difficulty across diverse scenarios.The strong correlation between S3Eval and real-world benchmarks demonstrates the soundness of using S3Eval for evaluation of LLMs.S3Eval provides a flexible and infinite long-context data generation method. We have generated a comprehensive dataset called S3Eval-Standard, and experimental results have shown that it poses significant challenges for all existing LLMs.</abstract>
      <url hash="b6d072fd">2024.naacl-long.69</url>
    </paper>
    <paper id="70">
      <title><fixed-case>MMC</fixed-case>: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning</title>
      <author><first>Fuxiao</first><last>Liu</last></author>
      <author><first>Xiaoyang</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wenlin</first><last>Yao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jianshu</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Kaiqiang</first><last>Song</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Sangwoo</first><last>Cho</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yaser</first><last>Yacoob</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>1287-1310</pages>
      <abstract>With the rapid development of large language models (LLMs) and their integration into large multimodal models (LMMs), there has beenimpressive progress in zero-shot completion of user-oriented vision-language tasks. However, a gap remains in the domain of chartimage understanding due to the distinct abstract components in charts. To address this, we introduce a large-scale MultiModal ChartInstruction (MMC-Instruction) dataset comprising 600k instances supporting diverse tasks and chart types. Leveraging this data, we de-velop MultiModal Chart Assistant (MMCA), an LMM that achieves state-of-the-art performance on existing chart QA benchmarks. Recognizing the need for a comprehensive evaluation of LMM chart understanding, we also propose a MultiModal Chart Benchmark (MMC-Benchmark), a comprehensive human-annotated benchmark with nine distinct tasks evaluating reasoning capabilities over charts.Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs on correctly interpreting charts, even for the mostrecent GPT-4V model. Our work provides an instruction-tuning methodology and benchmark to advance multimodal understanding ofcharts. Code and data are available at https://github.com/FuxiaoLiu/MMC.</abstract>
      <url hash="71866780">2024.naacl-long.70</url>
    </paper>
    <paper id="71">
      <title>Visual Grounding Helps Learn Word Meanings in Low-Data Regimes</title>
      <author><first>Chengxu</first><last>Zhuang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Evelina</first><last>Fedorenko</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology and Microsoft</affiliation></author>
      <pages>1311-1329</pages>
      <abstract>Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways — requiring orders of magnitude more language data than children receive during development, and without perceptual or social context. Do models trained more naturalistically — with grounded supervision — exhibit more humanlike language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary visual supervision, on datasets of varying scales. We then evaluate these models’ learning of syntactic categories, lexical relations, semantic features, word similarity, and alignment with human neural representations. We find that visual supervision can indeed improve the efficiency of word learning. However, these improvements are limited: they are present almost exclusively in the low-dataregime, and sometimes canceled out by the inclusion of rich distributional signals from text. The information conveyed by text and images isnot redundant—models mainly driven by visual information yield qualitatively different from those mainly driven by word co-occurrences. However, our results suggest that current multimodal modeling approaches fail to effectively leverage visual information to build human-like word representations from human-scale data.</abstract>
      <url hash="227467d3">2024.naacl-long.71</url>
    </paper>
    <paper id="72">
      <title>Accurate Knowledge Distillation via n-best Reranking</title>
      <author><first>Hendra</first><last>Setiawan</last></author>
      <pages>1330-1345</pages>
      <abstract>We propose utilizing n-best reranking to enhance Sequence-Level Knowledge Distillation (Kim and Rush, 2016) where we extract pseudo-labels for student model’s training data from top n-best hypotheses and leverage a diverse set of models with different inductive biases, objective functions or architectures, including some publicly-available large language models, to pick the highest-quality hypotheses as labels. The effectiveness of our proposal is validated through experiments on the WMT’21 German ↔ English and Chinese ↔ English translation tasks. Our results demonstrate that utilizing pseudo-labels generated by our n-best reranker leads to a significantly more accurate student model. In fact, our best student model achieves comparable accuracy to a large translation model from (Tran et al., 2021) with 4.7 billion parameters, while having two orders of magnitude fewer parameters.</abstract>
      <url hash="7401e529">2024.naacl-long.72</url>
    </paper>
    <paper id="73">
      <title><fixed-case>A</fixed-case>uto<fixed-case>PRM</fixed-case>: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition</title>
      <author><first>Zhaorun</first><last>Chen</last></author>
      <author><first>Zhuokai</first><last>Zhao</last></author>
      <author><first>Zhihong</first><last>Zhu</last></author>
      <author><first>Ruiqi</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Bhiksha</first><last>Raj</last><affiliation>Carnegie Mellon University, Carnegie Mellon University and Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Huaxiu</first><last>Yao</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <pages>1346-1362</pages>
      <abstract>Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment. To address this challenge, in this paper, we propose a novel self-supervised framework **AutoPRM** that efficiently enhances the fine-tuning of LLMs for intricate reasoning challenges. Specifically, **AutoPRM** first decomposes complex problems into more manageable subquestions with a controllable granularity switch, then sequentially apply reinforcement learning to iteratively improve the subquestion solver. Additionally, we propose context-guided decoding to avoid reward tampering and guide the subquestion solver towards the solution of the holistic problem. Extensive experiments show that **AutoPRM** significantly improves performance on mathematical and commonsense reasoning tasks over SOTA. More encouragingly, **AutoPRM** can be easily integrated with other orthogonal reasoning pipelines.</abstract>
      <url hash="454758e3">2024.naacl-long.73</url>
    </paper>
    <paper id="74">
      <title><fixed-case>SEMQA</fixed-case>: Semi-Extractive Multi-Source Question Answering</title>
      <author><first>Tal</first><last>Schuster</last><affiliation>Google</affiliation></author>
      <author><first>Adam</first><last>Lelkes</last><affiliation>Google</affiliation></author>
      <author><first>Haitian</first><last>Sun</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Jai</first><last>Gupta</last><affiliation>Google</affiliation></author>
      <author><first>Jonathan</first><last>Berant</last><affiliation>Google and Tel Aviv University</affiliation></author>
      <author><first>William</first><last>Cohen</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Donald</first><last>Metzler</last><affiliation>Google</affiliation></author>
      <pages>1363-1381</pages>
      <abstract>Recently proposed long-form question answering (QA) systems, supported by large language models (LLMs), have shown promising capabilities. Yet, attributing and verifying their generated abstractive answers can be difficult, and automatically evaluating their accuracy remains an ongoing challenge.In this work, we introduce a new QA task for answering multi-answer questions by summarizing multiple diverse sources in a semi-extractive fashion. Specifically, Semi-extractive Multi-source QA (SEMQA) requires models to output a comprehensive answer, while mixing factual quoted spans—copied verbatim from given input sources—and non-factual free-text connectors that glue these spans together into a single cohesive passage. This setting bridges the gap between the outputs of well-grounded but constrained extractive QA systems and more fluent but harder to attribute fully abstractive answers. Particularly, it enables a new mode for language models that leverages their advanced language generation capabilities, while also producing fine in-line attributions by-design that are easy to verify, interpret, and evaluate. To study this task, we create the first dataset of this kind, QuoteSum, with human-written semi-extractive answers to natural and generated questions, and define text-based evaluation metrics. Experimenting with several LLMs in various settings, we find this task to be surprisingly challenging, demonstrating the importance of QuoteSum for developing and studying such consolidation capabilities.</abstract>
      <url hash="99ebc551">2024.naacl-long.74</url>
    </paper>
    <paper id="75">
      <title>Fine-Tuning Language Models with Reward Learning on Policy</title>
      <author><first>Hao</first><last>Lang</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>1382-1392</pages>
      <abstract>Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences.RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially.Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs’ data distribution.Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize.In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution.Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples.Meanwhile, a synthetic preference generation approach is developed to simulate high-quality preference data with policy outputs.Extensive experiments on three benchmark datasets show that RLP consistently outperforms the state-of-the-art.Our code is available at <url>https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp</url>.</abstract>
      <url hash="f95c37fb">2024.naacl-long.75</url>
    </paper>
    <paper id="76">
      <title>A <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies Treebank for <fixed-case>H</fixed-case>ighland <fixed-case>P</fixed-case>uebla <fixed-case>N</fixed-case>ahuatl</title>
      <author><first>Robert</first><last>Pugh</last></author>
      <author><first>Francis</first><last>Tyers</last><affiliation>Indiana University, Bloomington</affiliation></author>
      <pages>1393-1403</pages>
      <abstract>We present a Universal Dependencies (UD) treebank for Highland Puebla Nahuatl. The treebank is only the second such UD corpus for a Mexican language, and supplements an existing treebank for another Nahuatl variant. We describe the process of data collection, annotation decisions and interesting syntactic constructions, and discuss some similarities and differences between the Highland Puebla Nahuatl treebank and the existing Western Sierra Puebla Nahuatl treebank.</abstract>
      <url hash="3f5bfd1f">2024.naacl-long.76</url>
    </paper>
    <paper id="77">
      <title><fixed-case>COPAL</fixed-case>-<fixed-case>ID</fixed-case>: <fixed-case>I</fixed-case>ndonesian Language Reasoning with Local Culture and Nuances</title>
      <author><first>Haryo</first><last>Wibowo</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Erland</first><last>Fuadi</last></author>
      <author><first>Made</first><last>Nityasya</last></author>
      <author><first>Radityo Eko</first><last>Prasojo</last><affiliation>Rukita</affiliation></author>
      <author><first>Alham</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Amazon</affiliation></author>
      <pages>1404-1422</pages>
      <abstract>We present COPAL-ID, a novel, public Indonesian language common sense reasoning dataset. Unlike the previous Indonesian COPA dataset (XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and therefore, provides a more natural portrayal of day-to-day causal reasoning within the Indonesian cultural sphere. Professionally written by natives from scratch, COPAL-ID is more fluent and free from awkward phrases, unlike the translated XCOPA-ID. In addition, we present COPALID in both standard Indonesian and in Jakartan Indonesian–a dialect commonly used in daily conversation. COPAL-ID poses a greater challenge for existing open-sourced and closedstate-of-the-art multilingual language models, yet is trivially easy for humans. Our findings suggest that general multilingual models struggle to perform well, achieving 66.91% accuracy on COPAL-ID. South-East Asian-specific models achieve slightly better performance of 73.88% accuracy. Yet, this number still falls short of near-perfect human performance. This shows that these language models are still way behind in comprehending the local nuances of Indonesian.</abstract>
      <url hash="d4083353">2024.naacl-long.77</url>
    </paper>
    <paper id="78">
      <title><fixed-case>I</fixed-case>ter<fixed-case>A</fixed-case>lign: Iterative Constitutional Alignment of Large Language Models</title>
      <author><first>Xiusi</first><last>Chen</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Hongzhi</first><last>Wen</last></author>
      <author><first>Sreyashi</first><last>Nag</last><affiliation>Amazon</affiliation></author>
      <author><first>Chen</first><last>Luo</last><affiliation>Amazon</affiliation></author>
      <author><first>Qingyu</first><last>Yin</last><affiliation>Amazon</affiliation></author>
      <author><first>Ruirui</first><last>Li</last></author>
      <author><first>Zheng</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>1423-1433</pages>
      <abstract>With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to 13.5% in harmlessness.</abstract>
      <url hash="0e1631f7">2024.naacl-long.78</url>
    </paper>
    <paper id="79">
      <title><fixed-case>O</fixed-case>rchestra<fixed-case>LLM</fixed-case>: Efficient Orchestration of Language Models for Dialogue State Tracking</title>
      <author><first>Chia-Hsuan</first><last>Lee</last></author>
      <author><first>Hao</first><last>Cheng</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Mari</first><last>Ostendorf</last><affiliation>University of Washington</affiliation></author>
      <pages>1434-1445</pages>
      <abstract>Large language models (LLMs) have revolutionized the landscape of Natural Language Processing, but are computationally expensive. To reduce the cost without sacrificing performance, previous studies have explored various approaches to harness the potential of Smaller Language Models (SLMs) as cost-effective alternatives to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance. In dialogue state tracking tasks, the proposed routing framework enhances performance substantially compared to relying solely on LLMs, while reducing the computational costs by over 50%.</abstract>
      <url hash="4531a883">2024.naacl-long.79</url>
    </paper>
    <paper id="80">
      <title>Multi-Operational Mathematical Derivations in Latent Space</title>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Jordan</first><last>Meadows</last></author>
      <author><first>Lan</first><last>Zhang</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>1446-1458</pages>
      <abstract>This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders.Specifically, we investigate how different encoding mechanisms can approximate expression manipulation in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusions for a single operation is achievable in the original expression encoder. Moreover, we show that architectural choices can heavily affect the training dynamics, structural organisation, and generalisation of the latent space, resulting in significant variations across paradigms and classes of encoders.</abstract>
      <url hash="01913308">2024.naacl-long.80</url>
    </paper>
    <paper id="81">
      <title>Large Language Models Help Humans Verify Truthfulness – Except When They Are Convincingly Wrong</title>
      <author><first>Chenglei</first><last>Si</last><affiliation>Stanford University</affiliation></author>
      <author><first>Navita</first><last>Goyal</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Tongshuang</first><last>Wu</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Chen</first><last>Zhao</last></author>
      <author><first>Shi</first><last>Feng</last><affiliation>George Washington University</affiliation></author>
      <author><first>Hal</first><last>Daumé Iii</last><affiliation>University of Maryland - College Park, University of Maryland, College Park and Microsoft</affiliation></author>
      <author><first>Jordan</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>1459-1474</pages>
      <abstract>Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they get, LLMs should not only provide information but also help users fact-check it. We conduct human experiments with 80 crowdworkers to compare language models with search engines (information retrieval systems) at facilitating fact-checking. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than those using search engines while achieving similar accuracy. However, they over-rely on the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information—explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users’ over-reliance on LLMs, but cannot significantly outperform search engines. Further, showing both search engine results and LLM explanations offers no complementary benefits compared to search engines alone. Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.</abstract>
      <url hash="6669ef1a">2024.naacl-long.81</url>
    </paper>
    <paper id="82">
      <title><fixed-case>X</fixed-case>fer<fixed-case>B</fixed-case>ench: a Data-Driven Benchmark for Emergent Language</title>
      <author><first>Brendon</first><last>Boldt</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>David</first><last>Mortensen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1475-1489</pages>
      <abstract>In this paper, we introduce a benchmark for evaluating the overall quality of emergent languages using data-driven methods. Specifically, we interpret the notion of the “quality” of an emergent language as its similarity to human language within a deep learning framework. We measure this by using the emergent language as pretraining data for a downstream NLP tasks in human language—the better the downstream performance, the better the emergent language. We implement this benchmark as an easy-to-use Python package that only requires a text file of utterances from the emergent language to be evaluated. Finally, we empirically test the benchmark’s validity using human, synthetic, and emergent language baselines.</abstract>
      <url hash="af589778">2024.naacl-long.82</url>
    </paper>
    <paper id="83">
      <title>Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation</title>
      <author><first>Se-eun</first><last>Yoon</last></author>
      <author><first>Zhankui</first><last>He</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Jessica</first><last>Echterhoff</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <pages>1490-1504</pages>
      <abstract>Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.</abstract>
      <url hash="ca47cedc">2024.naacl-long.83</url>
    </paper>
    <paper id="84">
      <title>A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers</title>
      <author><first>Jordan</first><last>Meadows</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Damien</first><last>Teney</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>1505-1523</pages>
      <abstract>This paper proposes a methodology for generating and perturbing detailed derivations of equations at scale, aided by a symbolic engine, to evaluate the generalisability of Transformers to out-of-distribution mathematical reasoning problems. Instantiating the framework in the context of sequence classification tasks, we compare the capabilities of GPT-4, GPT-3.5, and a canon of fine-tuned BERT models, exploring the relationship between specific operators and generalisation failure via the perturbation of reasoning aspects such as symmetry and variable surface forms. Surprisingly, our empirical evaluation reveals that the average in-distribution performance of fine-tuned models surpasses GPT-3.5, and rivals GPT-4. However, perturbations to input reasoning can reduce their performance by up to 80 F1 points. Overall, the results suggest that the in-distribution performance of smaller open-source models may potentially rival GPT by incorporating appropriately structured derivation dependencies during training, and highlight a shared weakness between BERT and GPT involving a relative inability to decode indirect references to mathematical entities. We release the full codebase, constructed datasets, and fine-tuned models to encourage future progress in the field.</abstract>
      <url hash="1167b9f5">2024.naacl-long.84</url>
    </paper>
    <paper id="85">
      <title>Identifying Linear Relational Concepts in Large Language Models</title>
      <author><first>David</first><last>Chanin</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Anthony</first><last>Hunter</last></author>
      <author><first>Oana-Maria</first><last>Camburu</last><affiliation>Department of Computer Science, University College London, University of London</affiliation></author>
      <pages>1524-1535</pages>
      <abstract>Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts by first modeling the relation between subject and object as a linear relational embedding (LRE). We find that inverting the LRE and using earlier object layers results in a powerful technique for finding concept directions that outperforms standard black-box probing classifiers. We evaluate LRCs on their performance as concept classifiers as well as their ability to causally change model output.</abstract>
      <url hash="d7e072cc">2024.naacl-long.85</url>
    </paper>
    <paper id="86">
      <title>Benchmark Transparency: Measuring the Impact of Data on Evaluation</title>
      <author><first>Venelin</first><last>Kovatchev</last><affiliation>University of Birmingham</affiliation></author>
      <author><first>Matthew</first><last>Lease</last><affiliation>University of Texas at Austin, Amazon and University of Texas at Austin</affiliation></author>
      <pages>1536-1551</pages>
      <abstract>In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric. In a second set of experiments, we demonstrate that the impact of data on evaluation is not just observable, but also predictable. We propose to use benchmark transparency as a method for comparing datasets and quantifying the similarity between them. We find that the “dataset similarity vector” can be used to predict how well a model generalizes out of distribution.</abstract>
      <url hash="9e09b179">2024.naacl-long.86</url>
    </paper>
    <paper id="87">
      <title><fixed-case>JAMDEC</fixed-case>: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models</title>
      <author><first>Jillian</first><last>Fisher</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ximing</first><last>Lu</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Jaehun</first><last>Jung</last><affiliation>University of Washington</affiliation></author>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Zaid</first><last>Harchaoui</last></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>1552-1581</pages>
      <abstract>The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency.We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM’s APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement. The key idea behind our approach is to boost the creative power of smaller language models through constrained decoding, while also allowing for user-specified controls and flexibility. Experimental results demonstrate that our approach based on GPT2-XL outperforms previous state-of-the-art methods based on comparably small models, while performing competitively against GPT3.5 175B, a propriety model that is two orders of magnitudes larger.</abstract>
      <url hash="08ff96ae">2024.naacl-long.87</url>
    </paper>
    <paper id="88">
      <title><fixed-case>REST</fixed-case>: Retrieval-Based Speculative Decoding</title>
      <author><first>Zhenyu</first><last>He</last></author>
      <author><first>Zexuan</first><last>Zhong</last></author>
      <author><first>Tianle</first><last>Cai</last></author>
      <author><first>Jason</first><last>Lee</last><affiliation>Princeton University</affiliation></author>
      <author><first>Di</first><last>He</last><affiliation>Peking University and Microsoft</affiliation></author>
      <pages>1582-1595</pages>
      <abstract>We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm designed to speed up language model generation. The key insight driving the development of REST is the observation that the process of text generation often includes certain common phases and patterns. Unlike previous methods that rely on a draft language model for speculative decoding, REST harnesses the power of retrieval to generate draft tokens. This method draws from the reservoir of existing knowledge, retrieving and employing relevant tokens based on the current context. Its plug-and-play nature allows for seamless integration and acceleration of any language model, all without necessitating additional training. When benchmarked on 7B and 13B language models in a single-batch setting, REST achieves a significant speedup of <tex-math>1.62 \times</tex-math> to <tex-math>2.36 \times</tex-math> on code or text generation. The source code of REST is available at https://github.com/FasterDecoding/REST.</abstract>
      <url hash="6d79e1b1">2024.naacl-long.88</url>
    </paper>
    <paper id="89">
      <title>Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations</title>
      <author><first>Sihao</first><last>Chen</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Tong</first><last>Chen</last></author>
      <author><first>Ben</first><last>Zhou</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Wenhao</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dian</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Baolin</first><last>Peng</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Hongwei</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>1596-1609</pages>
      <abstract>We introduce sub-sentence encoder, a contrastively-learned contextual embedding model for fine-grained semantic representation of text. In contrast to the standard practice with sentence embeddings, where the meaning of an entire sequence of text is encoded into a fixed-length vector, the sub-sentence encoder learns to produce distinct contextual embeddings corresponding to different atomic propositions, i.e. atomic units of meaning expressed within a text sequence. The sub-sentence embeddings are contrastively learned to recognize (inferred) semantic equivalence between propositions across different text sequences. Our experiments show the effectiveness of sub-sentence encoders in applications, such as retrieving supporting facts for fine-grained text attribution or recognizing the conditional semantic similarity between texts. In practice, we demonstrate that sub-sentence encoders keep the same level of inference cost and space complexity compared to sentence encoders.</abstract>
      <url hash="91eec48f">2024.naacl-long.89</url>
    </paper>
    <paper id="90">
      <title><fixed-case>MS</fixed-case>ci<fixed-case>NLI</fixed-case>: A Diverse Benchmark for Scientific Natural Language Inference</title>
      <author><first>Mobashir</first><last>Sadat</last></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>1610-1629</pages>
      <abstract>The task of scientific Natural Language Inference (NLI) involves predicting the semantic relation between two sentences extracted from research articles. This task was recently proposed along with a new dataset called SciNLI derived from papers published in the computational linguistics domain. In this paper, we aim to introduce diversity in the scientific NLI task and present MSciNLI, a dataset containing 132,320 sentence pairs extracted from five new scientific domains. The availability of multiple domains makes it possible to study domain shift for scientific NLI. We establish strong baselines on MSciNLI by fine-tuning Pre-trained Language Models (PLMs) and prompting Large Language Models (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21% and 51.77%, respectively, illustrating that MSciNLI is challenging for both types of models. Furthermore, we show that domain shift degrades the performance of scientific NLI models which demonstrates the diverse characteristics of different domains in our dataset. Finally, we use both scientific NLI datasets in an intermediate task transfer learning setting and show that they can improve the performance of downstream tasks in the scientific domain. We make our dataset and code available on Github.</abstract>
      <url hash="e9a03fad">2024.naacl-long.90</url>
    </paper>
    <paper id="91">
      <title>Causal Inference for Human-Language Model Collaboration</title>
      <author><first>Bohan</first><last>Zhang</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Yixin</first><last>Wang</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Paramveer</first><last>Dhillon</last><affiliation>University of Michigan</affiliation></author>
      <pages>1630-1647</pages>
      <abstract>In this paper, we examine the collaborative dynamics between humansand language models (LMs), where the interactions typically involveLMs proposing text segments and humans editing or responding to theseproposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual ‘what-if’ question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand– *Incremental Stylistic Effect (ISE)*, which characterizes the average impact of infinitesimally shifting a text towards a specific style, such as increasing formality. We establish the conditions for the non-parametric identification of ISE. Building on this, we develop *CausalCollab*, an algorithm designed to estimate the ISE of various interaction strategies in dynamic human-LM collaborations. Our empirical investigations across three distinct human-LM collaboration scenarios reveal that *CausalCollab* effectively reduces confounding and significantly improves counterfactual estimation over a set of competitive baselines.</abstract>
      <url hash="85af427e">2024.naacl-long.91</url>
    </paper>
    <paper id="92">
      <title><fixed-case>SELF</fixed-case>-<fixed-case>GUARD</fixed-case>: Empower the <fixed-case>LLM</fixed-case> to Safeguard Itself</title>
      <author><first>Zezhong</first><last>Wang</last></author>
      <author><first>Fangkai</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Pu</first><last>Zhao</last></author>
      <author><first>Hongru</first><last>Wang</last></author>
      <author><first>Liang</first><last>Chen</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Qingwei</first><last>Lin</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>1648-1668</pages>
      <abstract>With the increasing risk posed by jailbreak attacks, recent studies have investigated various methods to improve the safety of large language models (LLMs), mainly falling into two strategies: safety training and safeguards. Safety training involves fine-tuning the LLM with adversarial samples, which activate the LLM’s capabilities against jailbreak. However, it is not always effective in countering new attacks and often leads to potential performance degradation. Safeguards, on the other hand, are methods using additional models to filter harmful content from the LLM’s response. Nevertheless, they can only reduce a limited amount of harmful output and introduce extra computational costs. Given the distinct strengths and weaknesses of both, we combine them to balance out their flaws and propose a more effective method called Self-Guard.Specifically, we train the LLM to review its responses for any harmful content and append a [harmful] or [harmless] tag to the end of the response. In this way, Self-Guard possesses the advantages of safety training, leveraging the powerful capabilities of the LLMs themselves to detect harmfulness. Besides that, it gains flexibility like safeguards, making the safety check target the output side, which makes the system less vulnerable to attack updates. Experimental results indicate that our Self-Guard can effectively defend against jailbreak attacks and will not cause LLMs’ performance degradation.</abstract>
      <url hash="9acd2c0d">2024.naacl-long.92</url>
    </paper>
    <paper id="93">
      <title><fixed-case>COSIGN</fixed-case>: Contextual Facts Guided Generation for Knowledge Graph Completion</title>
      <author><first>Jinpeng</first><last>Li</last></author>
      <author><first>Hang</first><last>Yu</last><affiliation>Shanghai University</affiliation></author>
      <author><first>Xiangfeng</first><last>Luo</last></author>
      <author><first>Qian</first><last>Liu</last><affiliation>University of Auckland</affiliation></author>
      <pages>1669-1682</pages>
      <abstract>Knowledge graph completion (KGC) aims to infer missing facts based on existing facts within a KG. Recently, research on generative models (GMs) has addressed the limitations of embedding methods in terms of generality and scalability. However, GM-based methods are sensitive to contextual facts on KG, so the contextual facts of poor quality can cause GMs to generate erroneous results. To improve the performance of GM-based methods for various KGC tasks, we propose a COntextual FactS GuIded GeneratioN (COSIGN) model. First, to enhance the inference ability of the generative model, we designed a contextual facts collector to achieve human-like retrieval behavior. Second, a contextual facts organizer is proposed to learn the organized capabilities of LLMs through knowledge distillation. Finally, the organized contextual facts as the input of the inference generator to generate missing facts. Experimental results demonstrate that COSIGN outperforms state-of-the-art baseline techniques in terms of performance.</abstract>
      <url hash="166a37c6">2024.naacl-long.93</url>
    </paper>
    <paper id="94">
      <title>Toward Informal Language Processing: Knowledge of Slang in Large Language Models</title>
      <author><first>Zhewei</first><last>Sun</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Qian</first><last>Hu</last><affiliation>Amazon</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Richard</first><last>Zemel</last><affiliation>Department of Computer Science, Columbia University and Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Yang</first><last>Xu</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <pages>1683-1701</pages>
      <abstract>Recent advancement in large language models (LLMs) has offered a strong potential for natural language systems to process informal language. A representative form of informal language is slang, used commonly in daily conversations and online social media. To date, slang has not been comprehensively evaluated in LLMs due partly to the absence of a carefully designed and publicly accessible benchmark. Using movie subtitles, we construct a dataset that supports evaluation on a diverse set of tasks pertaining to automatic processing of slang. For both evaluation and finetuning, we show the effectiveness of our dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences. We also show how our dataset can be used to probe the output distributions of LLMs for interpretive insights. We find that while LLMs such as GPT-4 achieve good performance in a zero-shot setting, smaller BERT-like models finetuned on our dataset achieve comparable performance. Furthermore, we show that our dataset enables finetuning of LLMs such as GPT-3.5 that achieve substantially better performance than strong zero-shot baselines. Our work offers a comprehensive evaluation and a high-quality benchmark on English slang based on the OpenSubtitles corpus, serving both as a publicly accessible resource and a platform for applying tools for informal language processing.</abstract>
      <url hash="a71bf845">2024.naacl-long.94</url>
    </paper>
    <paper id="95">
      <title>Ghostbuster: Detecting Text Ghostwritten by Large Language Models</title>
      <author><first>Vivek</first><last>Verma</last></author>
      <author><first>Eve</first><last>Fleisig</last></author>
      <author><first>Nicholas</first><last>Tomlin</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>1702-1717</pages>
      <abstract>We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze our system’s robustness to a variety of perturbations and paraphrasing attacks, and evaluate its performance on documents by non-native English speakers.</abstract>
      <url hash="c0c9ba90">2024.naacl-long.95</url>
    </paper>
    <paper id="96">
      <title>End-to-End Beam Retrieval for Multi-Hop Question Answering</title>
      <author><first>Jiahao</first><last>Zhang</last></author>
      <author><first>Haiyang</first><last>Zhang</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <author><first>Liu</first><last>Yong</last></author>
      <author><first>Shen</first><last>Huang</last></author>
      <pages>1718-1731</pages>
      <abstract>Multi-hop question answering (QA) involves finding multiple relevant passages and step-by-step reasoning to answer complex questions, indicating a retrieve-and-read paradigm. However, previous retrievers were customized for two-hop questions, and most of them were trained separately across different hops, resulting in a lack of supervision over the entire multi-hop retrieval process and leading to poor performance in complicated scenarios beyond two hops. In this work, we introduce Beam Retrieval, an end-to-end beam retrieval framework for multi-hop QA. This approach models the multi-hop retrieval process in an end-to-end manner by jointly optimizing an encoder and two classification heads across all hops. Moreover, Beam Retrieval maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. To establish a complete QA system, we incorporate a supervised reader or a large language model (LLM). Experimental results demonstrate that Beam Retrieval achieves a nearly 50% improvement compared with baselines on challenging MuSiQue-Ans, and it also surpasses all previous retrievers on HotpotQA and achieves 99.9% precision on 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substantially improves the few-shot QA performance of LLMs.</abstract>
      <url hash="2494b0fa">2024.naacl-long.96</url>
    </paper>
    <paper id="97">
      <title>Leveraging Generative Large Language Models with Visual Instruction and Demonstration Retrieval for Multimodal Sarcasm Detection</title>
      <author><first>Binghao</first><last>Tang</last></author>
      <author><first>Boda</first><last>Lin</last></author>
      <author><first>Haolong</first><last>Yan</last></author>
      <author><first>Si</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>1732-1742</pages>
      <abstract>Multimodal sarcasm detection aims to identify sarcasm in the given image-text pairs and has wide applications in the multimodal domains. Previous works primarily design complex network structures to fuse the image-text modality features for classification. However, such complicated structures may risk overfitting on in-domain data, reducing the performance in out-of-distribution (OOD) scenarios. Additionally, existing methods typically do not fully utilize cross-modal features, limiting their performance on in-domain datasets. Therefore, to build a more reliable multimodal sarcasm detection model, we propose a generative multimodal sarcasm model consisting of a designed instruction template and a demonstration retrieval module based on the large language model. Moreover, to assess the generalization of current methods, we introduce an OOD test set, RedEval. Experimental results demonstrate that our method is effective and achieves state-of-the-art (SOTA) performance on the in-domain MMSD2.0 and OOD RedEval datasets.</abstract>
      <url hash="5510dbac">2024.naacl-long.97</url>
    </paper>
    <paper id="98">
      <title>Multi-Scale Prompt Memory-Augmented Model for Black-Box Scenarios</title>
      <author><first>Xiaojun</first><last>Kuang</last></author>
      <author><first>C. L. Philip</first><last>Chen</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Shuzhen</first><last>Li</last></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>South China University of Technology</affiliation></author>
      <pages>1743-1757</pages>
      <abstract>Black-box few-shot text classification handles text classification in limited data without accessing the parameters and gradients of language models (LMs). Existing black-box optimization methods have demonstrated strong few-shot learning capabilities. However, they still require numerous LMs’ calls to search optimal prompts, thus resulting in overfitting performance and increasing computational cost. To address this issue, we present MuSKPrompt (Multi-scale Knowledge Prompt for Memory Model), an efficient multi-scale knowledge prompt-based memory model in black-box few-shot text classification task. MuSKPrompt extracts instance-level and class-level knowledge at different scales and stores them in memory banks during training. Then, it references multi-scale memory banks to perform quick inference on new samples via a novel scoring module. MuSKPrompt achieves competitive performance in limited data through multi-scale instance-level and class-level knowledge. Moreover, it realizes gradient-free optimization with zero training parameters in the black-box scenario. Experiments on different benchmarks and parameter analysis demonstrate the effectiveness and efficiency of MuSKPrompt in black-box few-shot text classification tasks.</abstract>
      <url hash="d753e0bc">2024.naacl-long.98</url>
    </paper>
    <paper id="99">
      <title>Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction</title>
      <author><first>Chenming</first><last>Tang</last></author>
      <author><first>Fanyi</first><last>Qu</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>1758-1770</pages>
      <abstract>In the era of large language models (LLMs), in-context learning (ICL) stands out as an effective prompting strategy that explores LLMs’ potency across various tasks. However, applying LLMs to grammatical error correction (GEC) is still a challenging task. In this paper, we propose a novel ungrammatical-syntax-based in-context example selection strategy for GEC. Specifically, we measure similarity of sentences based on their syntactic structures with diverse algorithms, and identify optimal ICL examples sharing the most similar ill-formed syntax to the test input. Additionally, we carry out a two-stage process to further improve the quality of selection results. On benchmark English GEC datasets, empirical results show that our proposed ungrammatical-syntax-based strategies outperform commonly-used word-matching or semantics-based methods with multiple LLMs. This indicates that for a syntax-oriented task like GEC, paying more attention to syntactic information can effectively boost LLMs’ performance. Our code is available at https://github.com/JamyDon/SynICL4GEC.</abstract>
      <url hash="a691c4c1">2024.naacl-long.99</url>
    </paper>
    <paper id="100">
      <title><fixed-case>BUFFET</fixed-case>: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer</title>
      <author><first>Akari</first><last>Asai</last><affiliation>Paul G. Allen School of Computer Science &amp; Engineering, University of Washington</affiliation></author>
      <author><first>Sneha</first><last>Kudugunta</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Xinyan</first><last>Yu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Terra</first><last>Blevins</last><affiliation>University of Washington</affiliation></author>
      <author><first>Hila</first><last>Gonen</last><affiliation>Facebook</affiliation></author>
      <author><first>Machel</first><last>Reid</last><affiliation>Google</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Sebastian</first><last>Ruder</last><affiliation>Google</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last><affiliation>University of Washington, University of Washington, Allen Institute for Artificial Intelligence and University of Washington, Seattle</affiliation></author>
      <pages>1771-1800</pages>
      <abstract>Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.</abstract>
      <url hash="28b76a38">2024.naacl-long.100</url>
    </paper>
    <paper id="101">
      <title><fixed-case>TISE</fixed-case>: A Tripartite In-context Selection Method for Event Argument Extraction</title>
      <author><first>Yanhe</first><last>Fu</last></author>
      <author><first>Yanan</first><last>Cao</last></author>
      <author><first>Qingyue</first><last>Wang</last></author>
      <author><first>Yi</first><last>Liu</last></author>
      <pages>1801-1818</pages>
      <abstract>In-context learning enhances the reasoning capabilities of LLMs by providing several examples. A direct yet effective approach to obtain in-context example is to select the top-k examples based on their semantic similarity to the test input. However, when applied to event argument extraction (EAE), this approach exhibits two shortcomings: 1) It may select almost identical examples, thus failing to provide additional event information, and 2) It overlooks event attributes, leading to the selected examples being unrelated to the test event type. In this paper, we introduce three necessary requirements when selecting an in-context example for EAE task: semantic similarity, example diversity and event correlation. And we further propose TISE, which scores examples from these three perspectives and integrates them using Determinantal Point Processes to directly select a set of examples as context. Experimental results on the ACE05 dataset demonstrate the effectiveness of TISE and the necessity of three requirements. Furthermore, we surprisingly observe that TISE can achieve superior performance with fewer examples and can even exceed some supervised methods.</abstract>
      <url hash="7d8abd29">2024.naacl-long.101</url>
    </paper>
    <paper id="102">
      <title>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks</title>
      <author><first>Zhaofeng</first><last>Wu</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Linlu</first><last>Qiu</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Alexis</first><last>Ross</last><affiliation>Massachusetts Institute of Technology and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ekin</first><last>Akyürek</last></author>
      <author><first>Boyuan</first><last>Chen</last></author>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Najoung</first><last>Kim</last><affiliation>Boston University and Google</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology and Microsoft</affiliation></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>1819-1862</pages>
      <abstract>The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on “counterfactual” task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects.</abstract>
      <url hash="8cc82248">2024.naacl-long.102</url>
    </paper>
    <paper id="103">
      <title><fixed-case>TRUE</fixed-case>-<fixed-case>UIE</fixed-case>: Two Universal Relations Unify Information Extraction Tasks</title>
      <author><first>Yucheng</first><last>Wang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Bowen</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yilin</first><last>Liu</last></author>
      <author><first>Shudong</first><last>Lu</last></author>
      <pages>1863-1876</pages>
      <abstract>Information extraction (IE) encounters challenges due to the variety of schemas and objectives that differ across tasks. Recent advancements hint at the potential for universal approaches to model such tasks, referred to as Universal Information Extraction (UIE). While handling diverse tasks in one model, their generalization is limited since they are actually learning task-specific knowledge.In this study, we introduce an innovative paradigm known as TRUE-UIE, wherein all IE tasks are aligned to learn the same goals: extracting mention spans and two universal relations named <tex-math>\mathtt{NEXT}</tex-math> and <tex-math>\mathtt{IS}</tex-math>. During the decoding process, the <tex-math>\mathtt{NEXT}</tex-math> relation is utilized to group related elements, while the <tex-math>\mathtt{IS}</tex-math> relation, in conjunction with structured language prompts, undertakes the role of type recognition. Additionally, we consider the sequential dependency of tokens during span extraction, an aspect often overlooked in prevalent models.Our empirical experiments indicate that TRUE-UIE achieves state-of-the-art performance on established benchmarks encompassing 16 datasets, spanning 7 diverse IE tasks. Further evaluations reveal that our approach effectively share knowledge between different IE tasks, showcasing significant transferability in zero-shot and few-shot scenarios.</abstract>
      <url hash="afcbae17">2024.naacl-long.103</url>
    </paper>
    <paper id="104">
      <title>zr<fixed-case>LLM</fixed-case>: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models</title>
      <author><first>Zifeng</first><last>Ding</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Heling</first><last>Cai</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Jingpei</first><last>Wu</last><affiliation>, Institute of Computer Science</affiliation></author>
      <author><first>Yunpu</first><last>Ma</last><affiliation>Siemens Corporate Research</affiliation></author>
      <author><first>Ruotong</first><last>Liao</last></author>
      <author><first>Bo</first><last>Xiong</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Volker</first><last>Tresp</last><affiliation>Ludwig Maximilian University of Munich and Siemens Corporate Research</affiliation></author>
      <pages>1877-1895</pages>
      <abstract>Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic meanings stay close in the embedding space, enabling TKGF models to recognize zero-shot relations even without any observed graph context. Experimental results show that our approach helps TKGF models to achieve much better performance in forecasting the facts with previously unseen relations, while still maintaining their ability in link forecasting regarding seen relations.</abstract>
      <url hash="62be56f2">2024.naacl-long.104</url>
    </paper>
    <paper id="105">
      <title>Embodied Executable Policy Learning with Language-based Scene Summarization</title>
      <author><first>Jielin</first><last>Qiu</last></author>
      <author><first>Mengdi</first><last>Xu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>William</first><last>Han</last></author>
      <author><first>Seungwhan</first><last>Moon</last><affiliation>Facebook</affiliation></author>
      <author><first>Ding</first><last>Zhao</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1896-1913</pages>
      <abstract>Large Language models (LLMs) have shown remarkable success in assisting robot learning tasks, i.e., complex household planning.However, the performance of pretrained LLMs heavily relies on domain-specific templated text data, which may be infeasible in real-world robot learning tasks with image-based observations. Moreover, existing LLMs with text inputs lack the capability to evolve with non-expert interactions with environments.In this work, we introduce a novel learning paradigm that generates robots’ executable actions in the form of text, derived solely from visual observations. Our proposed paradigm stands apart from previous works, which utilized either language instructions or a combination of language and visual data as inputs. We demonstrate that our proposed method can employ two fine-tuning strategies, including imitation learning and reinforcement learning approaches, to adapt to the target test tasks effectively.We conduct extensive experiments involving various model selections, environments, and tasks across 7 house layouts in the VirtualHome environment. Our experimental results demonstrate that our method surpasses existing baselines, confirming the effectiveness of this novel learning paradigm.</abstract>
      <url hash="3f6b7c1a">2024.naacl-long.105</url>
    </paper>
    <paper id="106">
      <title>Metacognitive Prompting Improves Understanding in Large Language Models</title>
      <author><first>Yuqing</first><last>Wang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Yun</first><last>Zhao</last></author>
      <pages>1914-1926</pages>
      <abstract>In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. Recent advancements in prompting have enhanced reasoning in logic-intensive tasks for LLMs, yet the nuanced understanding abilities of these models, crucial for processing and interpreting complex information, remain underexplored. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. We conduct extensive experiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across ten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE, and LexGLUE benchmarks. Additionally, we compare our method with chain-of-thought prompting and its advanced versions. The results show that GPT-4 consistently excels across all tasks, while other models have shown significant progress in some tasks when used in conjunction with MP. Furthermore, MP consistently outperforms existing prompting methods in both general and domain-specific NLU tasks. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.</abstract>
      <url hash="9cb85a13">2024.naacl-long.106</url>
    </paper>
    <paper id="107">
      <title><fixed-case>MART</fixed-case>: Improving <fixed-case>LLM</fixed-case> Safety with Multi-round Automatic Red-Teaming</title>
      <author><first>Suyu</first><last>Ge</last><affiliation>Meta</affiliation></author>
      <author><first>Chunting</first><last>Zhou</last><affiliation>Meta AI</affiliation></author>
      <author><first>Rui</first><last>Hou</last><affiliation>Meta Inc.</affiliation></author>
      <author><first>Madian</first><last>Khabsa</last><affiliation>Facebook</affiliation></author>
      <author><first>Yi-Chia</first><last>Wang</last><affiliation>Meta</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Yuning</first><last>Mao</last><affiliation>Meta</affiliation></author>
      <pages>1927-1937</pages>
      <abstract>Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them.In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM.Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning.On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.</abstract>
      <url hash="efec8a08">2024.naacl-long.107</url>
    </paper>
    <paper id="108">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>CC</fixed-case>: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset</title>
      <author><first>Young-Jun</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Byungsoo</first><last>Ko</last><affiliation>NAVER</affiliation></author>
      <author><first>Han-Gyu</first><last>Kim</last></author>
      <author><first>Jonghwan</first><last>Hyeon</last><affiliation>KAIST</affiliation></author>
      <author><first>Ho-Jin</first><last>Choi</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>1938-1963</pages>
      <abstract>As sharing images in an instant message is a crucial factor, there has been active research on learning an image-text multi-modal dialogue models.However, training a well-generalized multi-modal dialogue model remains challenging due to the low quality and limited diversity of images per dialogue in existing multi-modal dialogue datasets.In this paper, we propose an automated pipeline to construct a multi-modal dialogue dataset, ensuring both dialogue quality and image diversity without requiring minimum human effort. In our pipeline, to guarantee the coherence between images and dialogue, we prompt GPT-4 to infer potential image-sharing moments - specifically, the utterance, speaker, rationale, and image description. Furthermore, we leverage CLIP similarity to maintain consistency between aligned multiple images to the utterance.Through this pipeline, we introduce DialogCC, a high-quality and diverse multi-modal dialogue dataset that surpasses existing datasets in terms of quality and diversity in human evaluation.Our comprehensive experiments highlight that when multi-modal dialogue models are trained using our dataset, their generalization performance on unseen dialogue datasets is significantly enhanced. We make our source code and dataset publicly available (https://dialogcc.github.io/).</abstract>
      <url hash="fa925060">2024.naacl-long.108</url>
    </paper>
    <paper id="109">
      <title>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models</title>
      <author><first>Keming</first><last>Lu</last></author>
      <author><first>Hongyi</first><last>Yuan</last></author>
      <author><first>Runji</first><last>Lin</last></author>
      <author><first>Junyang</first><last>Lin</last></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <author><first>Jingren</first><last>Zhou</last><affiliation>Alibaba Group</affiliation></author>
      <pages>1964-1974</pages>
      <abstract>The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate on it by mining latent expertise with off-the-shelf reward models. We propose ZOOTER, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. ZOOTER shows computation efficiency in inference as it only introduces minor computation overhead of a routing function compared with reward model ranking methods. We evaluate ZOOTER on a comprehensive benchmark collection with 26 subsets in different domains and tasks. ZOOTER outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.</abstract>
      <url hash="11d6dcc2">2024.naacl-long.109</url>
    </paper>
    <paper id="110">
      <title>Automatic Generation of Model and Data Cards: A Step Towards Responsible <fixed-case>AI</fixed-case></title>
      <author><first>Jiarui</first><last>Liu</last></author>
      <author><first>Wenkai</first><last>Li</last></author>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Mona</first><last>Diab</last><affiliation>Carnegie Mellon University and George Washington University</affiliation></author>
      <pages>1975-1997</pages>
      <abstract>In an era of model and data proliferation in machine learning/AI especially marked by the rapid advancement of open-sourced technologies, there arises a critical need for standardized consistent documentation. Our work addresses the information incompleteness in current human-written model and data cards. We propose an automated generation approach using Large Language Models (LLMs). Our key contributions include the establishment of CardBench, a comprehensive dataset aggregated from over 4.8k model cards and 1.4k data cards, coupled with the development of the CardGen pipeline comprising a two-step retrieval process. Our approach exhibits enhanced completeness, objectivity, and faithfulness in generated model and data cards, a significant step in responsible AI documentation practices ensuring better accountability and traceability.</abstract>
      <url hash="bd426bd5">2024.naacl-long.110</url>
    </paper>
    <paper id="111">
      <title><fixed-case>FUN</fixed-case> with Fisher: Improving Generalization of Adapter-Based Cross-lingual Transfer with Scheduled Unfreezing</title>
      <author><first>Chen</first><last>Liu</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Ivan</first><last>Vulić</last><affiliation>University of Cambridge and PolyAI Limited</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>1998-2015</pages>
      <abstract>Standard fine-tuning of language models typically performs well on <tex-math>\textit{in-distribution data}</tex-math>, but suffers with generalization to <tex-math>\textit{distribution shifts}</tex-math>. In this work, we aim to improve the generalization of adapter-based cross-lingual task transfer where such cross-language distribution shifts are imminent. We investigate scheduled unfreezing algorithms –originally proposed to mitigate catastrophic forgetting in transfer learning – for fine-tuning task adapters. Our experiments show that scheduled unfreezing methods close the gap to full fine-tuning and achieve stronger cross-lingual transfer performance, suggesting that these methods can go beyond just mitigating catastrophic forgetting. Next, aiming to understand these empirical findings, we investigate the learning dynamics of scheduled unfreezing using Fisher Information. Our experiments reveal that scheduled unfreezing induces different learning dynamics compared to standard fine-tuning, and provide evidence that the dynamics of Fisher Information during training correlate with cross-lingual generalization performance. We additionally propose a general scheduled unfreezing algorithm that achieves an average of 2 points improvement over four datasets compared to standard fine-tuning and provides empirical evidence for a theory-based justification of the heuristic unfreezing schedule for task adapter training.</abstract>
      <url hash="1779d7f1">2024.naacl-long.111</url>
    </paper>
    <paper id="112">
      <title>Are Multilingual <fixed-case>LLM</fixed-case>s Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings</title>
      <author><first>Chen</first><last>Liu</last></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>2016-2039</pages>
      <abstract>Large language models (LLMs) are highly adept at question answering and reasoning tasks, but when reasoning in a situational context, human expectations vary depending on the relevant cultural common ground. As languages are associated with diverse cultures, LLMs should also be culturally-diverse reasoners. In this paper, we study the ability of a wide range of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings in a conversational context. Our experiments reveal that: (1) mLLMs “know” limited proverbs and memorizing proverbs does not mean understanding them within a conversational context; (2) mLLMs struggle to reason with figurative proverbs and sayings, and when asked to select the wrong answer (instead of asking it to select the correct answer); and (3) there is a “culture gap” in mLLMs when reasoning about proverbs and sayings translated from other languages. We construct and release our evaluation dataset MAPS (MulticulturAl Proverbs and Sayings) for proverb understanding with conversational context for six different languages.</abstract>
      <url hash="6fd90ef4">2024.naacl-long.112</url>
    </paper>
    <paper id="113">
      <title>The Colorful Future of <fixed-case>LLM</fixed-case>s: Evaluating and Improving <fixed-case>LLM</fixed-case>s as Emotional Supporters for Queer Youth</title>
      <author><first>Shir</first><last>Lissak</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <author><first>Nitay</first><last>Calderon</last><affiliation>Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Geva</first><last>Shenkman</last><affiliation>Reichman University</affiliation></author>
      <author><first>Yaakov</first><last>Ophir</last></author>
      <author><first>Eyal</first><last>Fruchter</last><affiliation>Technion - Israel Institute of Technology, Technion</affiliation></author>
      <author><first>Anat</first><last>Brunstein Klomek</last><affiliation>Reichman</affiliation></author>
      <author><first>Roi</first><last>Reichart</last><affiliation>Technion, Israel Institute of Technology</affiliation></author>
      <pages>2040-2079</pages>
      <abstract>Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM’s interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share experiences. We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice. We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM-supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses. Our annotated dataset is available for further research.*https://github.com/nitaytech/LGBTeenDataset</abstract>
      <url hash="35fcad33">2024.naacl-long.113</url>
    </paper>
    <paper id="114">
      <title><fixed-case>IPED</fixed-case>: An Implicit Perspective for Relational Triple Extraction based on Diffusion Model</title>
      <author><first>Jianli</first><last>Zhao</last></author>
      <author><first>Changhao</first><last>Xu</last></author>
      <author><first>Bin.</first><last>Jiang</last></author>
      <pages>2080-2092</pages>
      <abstract>Relational triple extraction is a fundamental task in the field of information extraction, and a promising framework based on table filling has recently gained attention as a potential baseline for entity relation extraction. However, inherent shortcomings such as redundant information and incomplete triple recognition remain problematic. To address these challenges, we propose an Implicit Perspective for relational triple Extraction based on Diffusion model (IPED), an innovative approach for extracting relational triples. Our classifier-free solution adopts an implicit strategy using block coverage to complete the tables, avoiding the limitations of explicit tagging methods. Additionally, we introduce a generative model structure, the block-denoising diffusion model, to collaborate with our implicit perspective and effectively circumvent redundant information disruptions. Experimental results on two popular datasets demonstrate that IPED achieves state-of-the-art performance while gaining superior inference speed and low computational complexity. To support future research, we have made our source code publicly available online.</abstract>
      <url hash="296695b6">2024.naacl-long.114</url>
    </paper>
    <paper id="115">
      <title><fixed-case>Q</fixed-case>ual<fixed-case>E</fixed-case>val: Qualitative Evaluation for Model Improvement</title>
      <author><first>Vishvak</first><last>Murahari</last><affiliation>Princeton University</affiliation></author>
      <author><first>Ameet</first><last>Deshpande</last></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Tanmay</first><last>Rajpurohit</last></author>
      <author><first>Ashish</first><last>Sabharwal</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Karthik</first><last>Narasimhan</last><affiliation>Princeton University</affiliation></author>
      <author><first>Ashwin</first><last>Kalyan</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>2093-2111</pages>
      <abstract>Quantitative evaluation metrics have been pivotal in gauging the advancements of AI systems like large language models (LLMs).However, due to the intricate nature of real-world tasks, a single scalar to quantify and compare performance trivializes the fine-grained nuances of model behavior. Additionally, metrics do not yield actionable diagnostics for model improvement, thus requiring extensive manual efforts of scientists, involving sifting through vast datasets and attempting hit-or-miss adjustments to training data or setups. In this work, we address the shortcomings of quantitative metrics by proposing QualEval, which uses automated qualitative evaluation as a vehicle for model improvement. QualEval uses a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that when applied, accelerate model improvement. The insights are supported by a dashboard report with fine-grained visualizations and human-interpretable analyses. We corroborate the faithfulness of QualEval by demonstrating that leveraging its insights, for example, improves the absolute performance of the Llama 2 model by up to 15% points relative on a challenging dialogue task (DialogSum) when compared to baselines. QualEval successfully increases the pace and quality of model development by eliminating the need of arduous manual analysis, thus serving as a data-scientist-in-a-box.</abstract>
      <url hash="8bcfd709">2024.naacl-long.115</url>
    </paper>
    <paper id="116">
      <title>Quantum-inspired Language Model with Lindblad Master Equation and Interference Measurement for Sentiment Analysis</title>
      <author><first>Kehuan</first><last>Yan</last></author>
      <author><first>Peichao</first><last>Lai</last></author>
      <author><first>Yilei</first><last>Wang</last></author>
      <pages>2112-2121</pages>
      <abstract>Quantum-inspired models have demonstrated superior performance in many downstream language tasks, such as question answering and sentiment analysis. However, recent models primarily focus on embedding and measurement operations, overlooking the significance of the quantum evolution process. In this work, we present a novel quantum-inspired neural network, LI-QiLM, which integrates the Lindblad Master Equation (LME) to model the evolution process and the interferometry to the measurement process, providing more physical meaning to strengthen the interpretability. We conduct comprehensive experiments on six sentiment analysis datasets. Compared to the traditional neural networks, transformer-based pre-trained models and quantum-inspired models, such as CICWE-QNN and ComplexQNN, the proposed method demonstrates superior performance in accuracy and F1-score on six commonly used datasets for sentiment analysis. Additional ablation tests verify the effectiveness of LME and interferometry.</abstract>
      <url hash="34d5aa74">2024.naacl-long.116</url>
    </paper>
    <paper id="117">
      <title><fixed-case>V</fixed-case>is<fixed-case>L</fixed-case>ing<fixed-case>I</fixed-case>nstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization</title>
      <author><first>Dongsheng</first><last>Zhu</last><affiliation>Baidu</affiliation></author>
      <author><first>Daniel</first><last>Tang</last></author>
      <author><first>Weidong</first><last>Han</last></author>
      <author><first>Jinghui</first><last>Lu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yukun</first><last>Zhao</last></author>
      <author><first>Guoliang</first><last>Xing</last></author>
      <author><first>Junfeng</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <pages>2122-2135</pages>
      <abstract>This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual content. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets. Our main code is available at https://github.com/Zhudongsheng75/VisLingInstruct</abstract>
      <url hash="7db4eab3">2024.naacl-long.117</url>
    </paper>
    <paper id="118">
      <title>A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily</title>
      <author><first>Peng</first><last>Ding</last><affiliation>nanjing university</affiliation></author>
      <author><first>Jun</first><last>Kuang</last><affiliation>Meituan</affiliation></author>
      <author><first>Dan</first><last>Ma</last></author>
      <author><first>Xuezhi</first><last>Cao</last></author>
      <author><first>Yunsen</first><last>Xian</last></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <pages>2136-2153</pages>
      <abstract>Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as ‘jailbreaks’ can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.</abstract>
      <url hash="e1f11efa">2024.naacl-long.118</url>
    </paper>
    <paper id="119">
      <title><fixed-case>P</fixed-case><tex-math>^3</tex-math><fixed-case>S</fixed-case>um: Preserving Author’s Perspective in News Summarization with Diffusion Language Models</title>
      <author><first>Yuhan</first><last>Liu</last></author>
      <author><first>Shangbin</first><last>Feng</last><affiliation>Paul G. Allen School of Computer Science and Engineering, University of Washington</affiliation></author>
      <author><first>Xiaochuang</first><last>Han</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Vidhisha</first><last>Balachandran</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Chan Young</first><last>Park</last></author>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>2154-2173</pages>
      <abstract>In this work, we take a first step towards designing summarization systems that are faithful to the author’s intent, not only the semantic content of the article. Focusing on a case study of preserving political perspectives in news summarization, we find that existing approaches alter the political opinions and stances of news articles in more than 50% of summaries, misrepresenting the intent and perspectives of the news authors. We thus propose P<tex-math>^3</tex-math>Sum, a diffusion model-based summarization approach controlled by political perspective classifiers. In P<tex-math>^3</tex-math>Sum, the political leaning of a generated summary is iteratively evaluated at each decoding step, and any drift from the article’s original stance incurs a loss back-propagated to the embedding layers, steering the political stance of the summary at inference time. Extensive experiments on three news summarization datasets demonstrate that P<tex-math>^3</tex-math>Sum outperforms state-of-the-art summarization systems and large language models by up to 13.7% in terms of the success rate of stance preservation, with competitive performance on standard metrics of summarization quality. Our findings present a first analysis of preservation of pragmatic features in summarization, highlight the lacunae in existing summarization models—that even state-of-the-art models often struggle to preserve author’s intents—and develop new summarization systems that are more faithful to author’s perspectives.</abstract>
      <url hash="c2a6f3d6">2024.naacl-long.119</url>
    </paper>
    <paper id="120">
      <title>Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes</title>
      <author><first>Rose</first><last>Wang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Qingyang</first><last>Zhang</last></author>
      <author><first>Carly</first><last>Robinson</last><affiliation>Stanford University</affiliation></author>
      <author><first>Susanna</first><last>Loeb</last></author>
      <author><first>Dorottya</first><last>Demszky</last><affiliation>Stanford University</affiliation></author>
      <pages>2174-2199</pages>
      <abstract>Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert’s latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student’s error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert’s decision-making model is critical for LLMs to close the gap: responses from GPT4 with expert decisions (e.g., “simplify the problem”) are +76% more preferred than without. Additionally, context-sensitive decisions are critical to closing pedagogical gaps: random decisions decrease GPT4’s response quality by -97% than expert decisions. Our work shows the potential of embedding expert thought processes in LLM generations to enhance their capability to bridge novice-expert knowledge gaps. Our dataset and code can be found at: https://github.com/rosewang2008/bridge.</abstract>
      <url hash="bd084245">2024.naacl-long.120</url>
    </paper>
    <paper id="121">
      <title><fixed-case>RST</fixed-case>-<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization</title>
      <author><first>Dongqi</first><last>Pu</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Vera</first><last>Demberg</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>2200-2220</pages>
      <abstract>For long document summarization, discourse structure is important to discern the key content of the text and the differences in importance level between sentences. Unfortunately, the integration of rhetorical structure theory (RST) into parameter-efficient fine-tuning strategies for long document summarization remains unexplored. Therefore, this paper introduces RST-LoRA and proposes four RST-aware variants to explicitly incorporate RST into the LoRA model. Our empirical evaluation demonstrates that incorporating the type and uncertainty of rhetorical relations can complementarily enhance the performance of LoRA in summarization tasks. Furthermore, the best-performing variant we introduced outperforms the vanilla LoRA and full-parameter fine-tuning models, as confirmed by multiple automatic and human evaluations, and even surpasses previous state-of-the-art methods.</abstract>
      <url hash="4fd36c7c">2024.naacl-long.121</url>
    </paper>
    <paper id="122">
      <title>Strings from the Library of Babel: Random Sampling as a Strong Baseline for Prompt Optimisation</title>
      <author><first>Yao</first><last>Lu</last></author>
      <author><first>Jiayi</first><last>Wang</last></author>
      <author><first>Raphael</first><last>Tang</last><affiliation>Comcast</affiliation></author>
      <author><first>Sebastian</first><last>Riedel</last><affiliation>Facebook and University College London</affiliation></author>
      <author><first>Pontus</first><last>Stenetorp</last><affiliation>University College London</affiliation></author>
      <pages>2221-2231</pages>
      <abstract>Recent prompt optimisation approaches use the generative nature of language models to produce prompts – even rivaling the performance of human-curated prompts. In this paper, we demonstrate that randomly sampling tokens from the model vocabulary as “separators” can be as effective as language models for prompt-style text classification. Our experiments show that random separators are competitive baselines, having less than a 1% difference compared to previous self-optimisation methods and showing a 12% average relative improvement over strong human baselines across nine text classification tasks and eight language models. We further analyse this phenomenon in detail using three different random generation strategies, establishing that the language space is rich with potentially good separators, with a greater than 40% average chance that a randomly drawn separator performs better than human-curated separators. These observations challenge the common assumption that an effective prompt should be human readable or task relevant and establish a strong baseline for prompt optimisation research.</abstract>
      <url hash="b45df960">2024.naacl-long.122</url>
    </paper>
    <paper id="123">
      <title><fixed-case>R</fixed-case>e<fixed-case>TA</fixed-case>: Recursively Thinking Ahead to Improve the Strategic Reasoning of Large Language Models</title>
      <author><first>Jinhao</first><last>Duan</last><affiliation>Drexel University</affiliation></author>
      <author><first>Shiqi</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>James</first><last>Diffenderfer</last><affiliation>Lawrence Livermore National Labs</affiliation></author>
      <author><first>Lichao</first><last>Sun</last><affiliation>Lehigh University</affiliation></author>
      <author><first>Tianlong</first><last>Chen</last></author>
      <author><first>Bhavya</first><last>Kailkhura</last><affiliation>Lawrence Livermore National Laboratory</affiliation></author>
      <author><first>Kaidi</first><last>Xu</last><affiliation>Drexel University</affiliation></author>
      <pages>2232-2246</pages>
      <abstract>Current logical reasoning evaluations of Large Language Models (LLMs) primarily focus on single-turn and static environments, such as arithmetic problems. The crucial problem of multi-turn, strategic reasoning is under-explored. In this work, we analyze the multi-turn strategic reasoning of LLMs through text-driven complete- and incomplete-information gaming, e.g., board games (Tic-Tac-Toe, Connect-4) and poker games (Texas Hold’em Poker). Specifically, we consider two distinct scenarios: 1) Online Racing, featuring multiple LLMs/agents to facilitate direct competition and comparison; 2) Offline Probing, constructing targeted questions with verified ground truth to evaluate LLMs’ strategic behaviors. Experimental results demonstrate that existing state-of-the-art LLMs and reasoning schemes are largely ineffective for strategic reasoning tasks. To mitigate these limitations, we propose a simple yet effective Recursively Thinking-Ahead (ReTA) agent, incorporating a recursive prompting mechanism that automatically analyzes the opponents’ future moves/actions and assigns reward signals for these situations, to strengthen the strategic reasoning of LLMs. We hope our work could spur further research and exploration in the multi-turn strategic reasoning of LLMs. The code is available at https://github.com/jinhaoduan/ReTA.</abstract>
      <url hash="77cf36fc">2024.naacl-long.123</url>
    </paper>
    <paper id="124">
      <title>Fact Checking Beyond Training Set</title>
      <author><first>Payam</first><last>Karisani</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>2247-2261</pages>
      <abstract>Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher robustness against distribution shift. To our knowledge, there is no publicly available multi-topic fact checking dataset. Thus, we propose a simple automatic method to re-purpose two well-known fact checking datasets. We then construct eight fact checking scenarios from these datasets, and compare our model to a set of strong baseline models, including recent domain adaptation models that use GPT4 for generating synthetic data.</abstract>
      <url hash="41c7ed3a">2024.naacl-long.124</url>
    </paper>
    <paper id="125">
      <title>Program-Aided Reasoners (Better) Know What They Know</title>
      <author><first>Anubha</first><last>Kabra</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Sanketh</first><last>Rangreji</last></author>
      <author><first>Yash</first><last>Mathur</last></author>
      <author><first>Aman</first><last>Madaan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Emmy</first><last>Liu</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>2262-2278</pages>
      <abstract>Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to “know what they know”, which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types - LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.</abstract>
      <url hash="e40833b7">2024.naacl-long.125</url>
    </paper>
    <paper id="126">
      <title>The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing Human Labels</title>
      <author><first>Eve</first><last>Fleisig</last></author>
      <author><first>Su Lin</first><last>Blodgett</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Zeerak</first><last>Talat</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>2279-2292</pages>
      <abstract>Longstanding data labeling practices in machine learning involve collecting and aggregating labels from multiple annotators. But what should we do when annotators disagree? Though annotator disagreement has long been seen as a problem to minimize, new perspectivist approaches challenge this assumption by treating disagreement as a valuable source of information. In this position paper, we examine practices and assumptions surrounding the causes of disagreement–some challenged by perspectivist approaches, and some that remain to be addressed–as well as practical and normative challenges for work operating under these assumptions. We conclude with recommendations for the data labeling pipeline and avenues for future research engaging with subjectivity and disagreement.</abstract>
      <url hash="11d5dd52">2024.naacl-long.126</url>
    </paper>
    <paper id="127">
      <title>Principles from Clinical Research for <fixed-case>NLP</fixed-case> Model Generalization</title>
      <author><first>Aparna</first><last>Elangovan</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiayuan</first><last>He</last><affiliation>Royal Melbourne Institute of Technology and The University of Melbourne</affiliation></author>
      <author><first>Yuan</first><last>Li</last></author>
      <author><first>Karin</first><last>Verspoor</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <pages>2293-2309</pages>
      <abstract>The NLP community typically relies on performance of a model on a held-out test set to assess generalization. Performance drops observed in datasets outside of official test sets are generally attributed to “out-of-distribution” effects. Here, we explore the foundations of generalizability and study the factors that affect it, articulating lessons from clinical studies. In clinical research, generalizability is an act of reasoning that depends on (a) *internal validity* of experiments to ensure controlled measurement of cause and effect, and (b) *external validity* or transportability of the results to the wider population. We demonstrate how learning spurious correlations, such as the distance between entities in relation extraction tasks, can affect a model’s internal validity and in turn adversely impact generalization. We, therefore, present the need to ensure internal validity when building machine learning models in NLP. Our recommendations also apply to generative large language models, as they are known to be sensitive to even minor semantic preserving alterations. We also propose adapting the idea of *matching* in randomized controlled trials and observational studies to NLP evaluation to measure causation.</abstract>
      <url hash="987e87cc">2024.naacl-long.127</url>
    </paper>
    <paper id="128">
      <title>First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models</title>
      <author><first>Naomi</first><last>Saphra</last><affiliation>Harvard University</affiliation></author>
      <author><first>Eve</first><last>Fleisig</last></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Genentech and New York University</affiliation></author>
      <author><first>Adam</first><last>Lopez</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>2310-2326</pages>
      <abstract>Many NLP researchers are experiencing an existential crisis triggered by the astonishing success of ChatGPT and other systems based on large language models (LLMs). After such a disruptive change to our understanding of the field, what is left to do? Taking a historical lens, we look for guidance from the first era of LLMs, which began in 2005 with large <tex-math>n</tex-math>-gram models for machine translation (MT). We identify durable lessons from the first era, and more importantly, we identify evergreen problems where NLP researchers can continue to make meaningful contributions in areas where LLMs are ascendant. We argue that disparities in scale are transient and researchers can work to reduce them; that data, rather than hardware, is still a bottleneck for many applications; that meaningful realistic evaluation is still an open problem; and that there is still room for speculative approaches.</abstract>
      <url hash="42a59f83">2024.naacl-long.128</url>
    </paper>
    <paper id="129">
      <title>Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models</title>
      <author><first>Raphael</first><last>Tang</last><affiliation>Comcast</affiliation></author>
      <author><first>Crystina</first><last>Zhang</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Xueguang</first><last>Ma</last></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Ferhan</first><last>Ture</last></author>
      <pages>2327-2340</pages>
      <abstract>Large language models (LLMs) exhibit positional bias in how they use context, which especially affects listwise ranking. To address this, we propose permutation self-consistency, a form of self-consistency over the ranking list outputs of black-box LLMs. Our key idea is to marginalize out different list orders in the prompt to produce an order-independent ranking with less positional bias. First, given some input prompt, we repeatedly shuffle the list in the prompt and pass it through the LLM while holding the instructions the same. Next, we aggregate the resulting sample of rankings by computing the central ranking closest in distance to all of them, marginalizing out prompt order biases in the process. Theoretically, we prove the robustness of our method, showing convergence to the true ranking under random perturbations.Empirically, on five datasets in sorting and passage reranking, our approach improves scores from conventional inference by up to 34-52% for Mistral, 7-18% for GPT-3.5, 8-16% for LLaMA v2 (70B). Our code is at https://github.com/castorini/perm-sc.</abstract>
      <url hash="9d271bd9">2024.naacl-long.129</url>
    </paper>
    <paper id="130">
      <title>From Language Modeling to Instruction Following: Understanding the Behavior Shift in <fixed-case>LLM</fixed-case>s after Instruction Tuning</title>
      <author><first>Xuansheng</first><last>Wu</last></author>
      <author><first>Wenlin</first><last>Yao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jianshu</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiaoman</first><last>Pan</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xiaoyang</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Ninghao</first><last>Liu</last><affiliation>University of Georgia</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>2341-2369</pages>
      <abstract>Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution, and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts of user prompts, and promotes the response generation constantly conditioned on the instructions. 2) It encourages the self-attention heads to capture more word-word relationships about instruction verbs. 3) It encourages the feed-forward networks to rotate their pre-trained knowledge toward user-oriented tasks. These insights contribute to a more comprehensive understanding of instruction tuning and lay the groundwork for future work that aims at explaining and optimizing LLMs for various applications. Our code and data are publicly available at https://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.</abstract>
      <url hash="c7e4db60">2024.naacl-long.130</url>
    </paper>
    <paper id="131">
      <title><fixed-case>POLYIE</fixed-case>: A Dataset of Information Extraction from Polymer Material Scientific Literature</title>
      <author><first>Jerry</first><last>Cheung</last></author>
      <author><first>Yuchen</first><last>Zhuang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yinghao</first><last>Li</last></author>
      <author><first>Pranav</first><last>Shetty</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Wantian</first><last>Zhao</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Sanjeev</first><last>Grampurohit</last></author>
      <author><first>Rampi</first><last>Ramprasad</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>2370-2385</pages>
      <abstract>Scientific information extraction (SciIE), which aims to automatically extract information from scientific literature, is becoming more important than ever. However, there are no existing SciIE datasets for polymer materials, which is an important class of materials used ubiquitously in our daily lives. To bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer materials. POLYIE is curated from 146 full-length polymer scholarly articles, which are annotated with different named entities (i.e., materials, properties, values, conditions) as well as their N-ary relations by domain experts. POLYIE presents several unique challenges due to diverse lexical formats of entities, ambiguity between entities, and variable-length relations. We evaluate state-of-the-art named entity extraction and relation extraction models on POLYIE, analyze their strengths and weaknesses, and highlight some difficult cases for these models. To the best of our knowledge, POLYIE is the first SciIE benchmark for polymer materials, and we hope it will lead to more research efforts from the community on this challenging task. Our code and data are available on: https://github.com/jerry3027/PolyIE.</abstract>
      <url hash="152e2584">2024.naacl-long.131</url>
    </paper>
    <paper id="132">
      <title><fixed-case>LLM</fixed-case>-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination</title>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Yangyang</first><last>Kang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fubang</first><last>Zhao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaozhong</first><last>Liu</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <pages>2386-2398</pages>
      <abstract>Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, medical assistants hold the potential to offer substantial benefits for individuals. However, the exploration of LLM-based personalized medical assistant remains relatively scarce. Typically, patients converse differently based on their background and preferences which necessitates the task of enhancing user-oriented medical assistant. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to enhance the response with aware of previous mistakes for new queries during a dialogue session. We contend that a mere memory module is inadequate and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, to personalize medical assistants. To encourage further research into this area, we are releasing a new conversation dataset generated based on an open-source medical corpus and our implementation.</abstract>
      <url hash="554f19a3">2024.naacl-long.132</url>
    </paper>
    <paper id="133">
      <title><fixed-case>S</fixed-case>um<fixed-case>T</fixed-case>ra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization</title>
      <author><first>Jacob</first><last>Parnell</last><affiliation>RoZetta Technology</affiliation></author>
      <author><first>Inigo</first><last>Jauregi Unanue</last><affiliation>University of Technology Sydney and Rozetta Technology</affiliation></author>
      <author><first>Massimo</first><last>Piccardi</last><affiliation>University of Technology Sydney (UTS)</affiliation></author>
      <pages>2399-2415</pages>
      <abstract>Cross-lingual summarization (XLS) generates summaries in a language different from that of the input documents (e.g., English to Spanish), allowing speakers of the target language to gain a concise view of their content. In the present day, the predominant approach to this task is to take a performing, pretrained multilingual language model (LM) and fine-tune it for XLS on the language pairs of interest. However, the scarcity of fine-tuning samples makes this approach challenging in some cases. For this reason, in this paper we propose revisiting the summarize-and-translate pipeline, where the summarization and translation tasks are performed in a sequence. This approach allows reusing the many, publicly-available resources for monolingual summarization and translation, obtaining a very competitive zero-shot performance. In addition, the proposed pipeline is completely differentiable end-to-end, allowing it to take advantage of few-shot fine-tuning, where available. Experiments over two contemporary and widely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable zero-shot performance of the proposed approach, and also its strong few-shot performance compared to an equivalent multilingual LM baseline, that the proposed approach has been able to outperform in many languages with only 10% of the fine-tuning samples.</abstract>
      <url hash="d0b07262">2024.naacl-long.133</url>
    </paper>
    <paper id="134">
      <title><fixed-case>KTRL</fixed-case>+<fixed-case>F</fixed-case>: Knowledge-Augmented In-Document Search</title>
      <author><first>Hanseok</first><last>Oh</last></author>
      <author><first>Haebin</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Samsung</affiliation></author>
      <author><first>Miyoung</first><last>Ko</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Hyunji</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>2416-2436</pages>
      <abstract>We introduce a new problem KTRL+F, a knowledge-augmented in-document search that necessitates real-time identification of all semantic targets within a document with the awareness of external sources through a single natural query. KTRL+F addresses following unique challenges for in-document search: 1) utilizing knowledge outside the document for extended use of additional information about targets, and 2) balancing between real-time applicability with the performance.We analyze various baselines in KTRL+F and find limitations of existing models, such as hallucinations, high latency, or difficulties in leveraging external knowledge. Therefore, we propose a Knowledge-Augmented Phrase Retrieval model that shows a promising balance between speed and performance by simply augmenting external knowledge in phrase embedding. We also conduct a user study to verify whether solving KTRL+F can enhance search experience for users. It demonstrates that even with our simple model, users can reduce the time for searching with less queries and reduced extra visits to other sources for collecting evidence. We encourage the research community to work on KTRL+F to enhance more efficient in-document information access.</abstract>
      <url hash="9cf8ede2">2024.naacl-long.134</url>
    </paper>
    <paper id="135">
      <title>How Well Do Large Language Models Truly Ground?</title>
      <author><first>Hyunji</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Se June</first><last>Joo</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Chaeeun</first><last>Kim</last></author>
      <author><first>Joel</first><last>Jang</last></author>
      <author><first>Doyoung</first><last>Kim</last></author>
      <author><first>Kyoung-Woon</first><last>On</last></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>2437-2465</pages>
      <abstract>To reduce issues like hallucinations and lack of control in Large Language Models (LLMs), a common method is to generate responses by grounding on external contexts given as input, known as knowledge-augmented models. However, previous research often narrowly defines “grounding” as just having the correct answer, which does not ensure the reliability of the entire response. To overcome this, we propose a stricter definition of grounding: a model is truly grounded if it (1) fully utilizes the necessary knowledge from the provided context, and (2) stays within the limits of that knowledge. We introduce a new dataset and a grounding metric to evaluate model capability under the definition. We perform experiments across 25 LLMs of different sizes and training methods and provide insights into factors that influence grounding performance. Our findings contribute to a better understanding of how to improve grounding capabilities and suggest an area of improvement toward more reliable and controllable LLM applications.</abstract>
      <url hash="76253f0e">2024.naacl-long.135</url>
    </paper>
    <paper id="136">
      <title><fixed-case>ALBA</fixed-case>: Adaptive Language-Based Assessments for Mental Health</title>
      <author><first>Vasudha</first><last>Varadarajan</last></author>
      <author><first>Sverker</first><last>Sikström</last></author>
      <author><first>Oscar</first><last>Kjell</last></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <pages>2466-2478</pages>
      <abstract>Mental health issues differ widely among individuals, with varied signs and symptoms. Recently, language-based assessments haveshown promise in capturing this diversity, but they require a substantial sample of words per person for accuracy. This work introducesthe task of Adaptive Language-Based Assessment (ALBA), which involves adaptively ordering questions while also scoring an individual’s latent psychological trait using limited language responses to previous questions. To this end, we develop adaptive testing methods under two psychometric measurement theories: Classical Test Theory and Item Response Theory.We empirically evaluate ordering and scoring strategies, organizing into two new methods: a semi-supervised item response theory-basedmethod (ALIRT) and a supervised Actor-Critic model. While we found both methods to improve over non-adaptive baselines, We foundALIRT to be the most accurate and scalable, achieving the highest accuracy with fewer questions (e.g., Pearson r ≈ 0.93 after only 3 questions as compared to typically needing at least 7 questions). In general, adaptive language-based assessments of depression and anxiety were able to utilize a smaller sample of language without compromising validity or large computational costs.</abstract>
      <url hash="3f62d40b">2024.naacl-long.136</url>
    </paper>
    <paper id="137">
      <title><fixed-case>FREB</fixed-case>-<fixed-case>TQA</fixed-case>: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering</title>
      <author><first>Wei</first><last>Zhou</last><affiliation>Robert Bosch GmbH, Bosch</affiliation></author>
      <author><first>Mohsen</first><last>Mesgar</last><affiliation>Bosch</affiliation></author>
      <author><first>Heike</first><last>Adel</last><affiliation>Hochschule der Medien (University of Applied Sciences)</affiliation></author>
      <author><first>Annemarie</first><last>Friedrich</last><affiliation>University of Augsburg</affiliation></author>
      <pages>2479-2497</pages>
      <abstract>Table Question Answering (TQA) aims at composing an answer to a question based on tabular data. While prior research has shown that TQA models lack robustness, understanding the underlying cause and nature of this issue remains predominantly unclear, posing a significant obstacle to the development of robust TQA systems. In this paper, we formalize three major desiderata for a fine-grained evaluation of robustness of TQA systems. They should (i) answer questions regardless of alterations in table structure, (ii) base their responses on the content of relevant cells rather than on biases, and (iii) demonstrate robust numerical reasoning capabilities. To investigate these aspects, we create and publish a novel TQA evaluation benchmark in English. Our extensive experimental analysis reveals that none of the examined state-of-the-art TQA systems consistently excels in these three aspects. Our benchmark is a crucial instrument for monitoring the behavior of TQA systems and paves the way for the development of robust TQA systems. We release our benchmark publicly.</abstract>
      <url hash="88e8d25b">2024.naacl-long.137</url>
    </paper>
    <paper id="138">
      <title><fixed-case>MILL</fixed-case>: Mutual Verification with Large Language Models for Zero-Shot Query Expansion</title>
      <author><first>Pengyue</first><last>Jia</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Yiding</first><last>Liu</last><affiliation>Baidu</affiliation></author>
      <author><first>Xiangyu</first><last>Zhao</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Xiaopeng</first><last>Li</last></author>
      <author><first>Changying</first><last>Hao</last></author>
      <author><first>Shuaiqiang</first><last>Wang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <pages>2498-2518</pages>
      <abstract>Query expansion, pivotal in search engines, enhances the representation of user information needs with additional terms. While existing methods expand queries using retrieved or generated contextual documents, each approach has notable limitations. Retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs. To address these gaps, we propose a novel zero-shot query expansion framework utilizing LLMs for mutual verification. Specifically, we first design a query-query-document generation method, leveraging LLMs’ zero-shot reasoning ability to produce diverse sub-queries and corresponding documents. Then, a mutual verification process synergizes generated and retrieved documents for optimal expansion. Our proposed method is fully zero-shot, and extensive experiments on three public benchmark datasets are conducted to demonstrate its effectiveness over existing methods. Our code is available online at https://github.com/Applied-Machine-Learning-Lab/MILL to ease reproduction.</abstract>
      <url hash="b6fc2fe9">2024.naacl-long.138</url>
    </paper>
    <paper id="139">
      <title>Efficient Benchmarking (of Language Models)</title>
      <author><first>Yotam</first><last>Perlitz</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Elron</first><last>Bandel</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ariel</first><last>Gera</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ofir</first><last>Arviv</last><affiliation>Hebrew University of Jerusalem and Computer Science Departmen, Technion-Israel Institute of Technology</affiliation></author>
      <author><first>Liat</first><last>Ein-Dor</last></author>
      <author><first>Eyal</first><last>Shnarch</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Noam</first><last>Slonim</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Michal</first><last>Shmueli-Scheuer</last></author>
      <author><first>Leshem</first><last>Choshen</last><affiliation>International Business Machines</affiliation></author>
      <pages>2519-2536</pages>
      <abstract>The increasing versatility of language models (LMs) has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs, extending to thousands of GPU hours per model. However, the efficiency aspect of these evaluation efforts had raised little discussion in the literature.In this work, we present the problem of Efficient Benchmarking, namely, intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case, we investigate how different benchmark design choices affect the computation-reliability trade-off. We propose to evaluate the reliability of such decisions, by using a new measure – Decision Impact on Reliability, DIoR for short.We find, for example, that a benchmark leader may change by merely removing a low-ranked model from the benchmark, and observe that a correct benchmark ranking can be obtained by considering only a fraction of the evaluation examples.Based on our findings, we outline a set of concrete recommendations for efficient benchmark design and utilization practices. To take a step further, we use our findings to propose an evaluation algorithm, that, when applied to the HELM benchmark, leads to dramatic cost savings with minimal loss of benchmark reliability, often reducing computation by x100 or more.</abstract>
      <url hash="7f141784">2024.naacl-long.139</url>
    </paper>
    <paper id="140">
      <title><fixed-case>R</fixed-case>e<fixed-case>FACT</fixed-case>: Updating Text-to-Image Models by Editing the Text Encoder</title>
      <author><first>Dana</first><last>Arad</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology</affiliation></author>
      <author><first>Hadas</first><last>Orgad</last><affiliation>Computer Science Departmen, Technion-Israel Institute of Technology and Technion - Israel Institute of Technology, Technion - Israel Institute of Technology</affiliation></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <pages>2537-2558</pages>
      <abstract>Our world is marked by unprecedented technological, global, and socio-political transformations, posing a significant challenge to textto-image generative models. These models encode factual associations within their parameters that can quickly become outdated, diminishing their utility for end-users. To that end, we introduce ReFACT, a novel approach for editing factual associations in text-to-image models without relaying on explicit input from end-users or costly re-training. ReFACT updates the weights of a specific layer in the text encoder, modifying only a tiny portion of the model’s parameters and leaving the rest of the model unaffected.We empirically evaluate ReFACT on an existing benchmark, alongside a newly curated dataset.Compared to other methods, ReFACT achieves superior performance in both generalization to related concepts and preservation of unrelated concepts.Furthermore, ReFACT maintains image generation quality, making it a practical tool for updating and correcting factual information in text-to-image models.</abstract>
      <url hash="11a8e68e">2024.naacl-long.140</url>
    </paper>
    <paper id="141">
      <title>A Likelihood Ratio Test of Genetic Relationship among Languages</title>
      <author><first>V.S.D.S.Mahesh</first><last>Akavarapu</last><affiliation>IIT Kanpur, IIT Kanpur</affiliation></author>
      <author><first>Arnab</first><last>Bhattacharya</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>2559-2570</pages>
      <abstract>Lexical resemblances among a group of languages indicate that the languages could be genetically related, i.e., they could have descended from a common ancestral language. However, such resemblances can arise by chance and, hence, need not always imply an underlying genetic relationship. Many tests of significance based on permutation of wordlists and word similarity measures appeared in the past to determine the statistical significance of such relationships. We demonstrate that although existing tests may work well for bilateral comparisons, i.e., on pairs of languages, they are either infeasible by design or are prone to yield false positives when applied to groups of languages or language families. To this end, inspired by molecular phylogenetics, we propose a likelihood ratio test to determine if given languages are related based on the proportion of invariant character sites in the aligned wordlists applied during tree inference. Further, we evaluate some language families and show that the proposed test solves the problem of false positives. Finally, we demonstrate that the test supports the existence of macro language families such as Nostratic and Macro-Mayan.</abstract>
      <url hash="33e6418e">2024.naacl-long.141</url>
    </paper>
    <paper id="142">
      <title><fixed-case>P</fixed-case>a<fixed-case>D</fixed-case>: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning</title>
      <author><first>Xuekai</first><last>Zhu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Biqing</first><last>Qi</last><affiliation>Tsinghua University and Harbin Institute of Technology</affiliation></author>
      <author><first>Kaiyan</first><last>Zhang</last><affiliation>Electronic Engineering, Tsinghua University</affiliation></author>
      <author><first>Xinwei</first><last>Long</last></author>
      <author><first>Zhouhan</first><last>Lin</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Bowen</first><last>Zhou</last><affiliation>Tsinghua University</affiliation></author>
      <pages>2571-2597</pages>
      <abstract>While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.</abstract>
      <url hash="57cd1ce4">2024.naacl-long.142</url>
    </paper>
    <paper id="143">
      <title><fixed-case>MEGAVERSE</fixed-case>: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks</title>
      <author><first>Sanchit</first><last>Ahuja</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Divyanshu</first><last>Aggarwal</last></author>
      <author><first>Varun</first><last>Gumma</last><affiliation>Microsoft</affiliation></author>
      <author><first>Ishaan</first><last>Watts</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Ashutosh</first><last>Sathe</last></author>
      <author><first>Millicent</first><last>Ochieng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Rishav</first><last>Hada</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>Prachi</first><last>Jain</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mohamed</first><last>Ahmed</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Kalika</first><last>Bali</last><affiliation>Microsoft Research Labs</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>2598-2637</pages>
      <abstract>There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.</abstract>
      <url hash="04b047c6">2024.naacl-long.143</url>
    </paper>
    <paper id="144">
      <title>Unlocking Emergent Modularity in Large Language Models</title>
      <author><first>Zihan</first><last>Qiu</last></author>
      <author><first>Zeyu</first><last>Huang</last></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>2638-2660</pages>
      <abstract>Modular Neural Networks (MNNs) demonstrate various advantages over monolithic models.Existing MNNs are generally <tex-math>\textit{explicit}</tex-math>: their modular architectures are pre-defined, with individual modules expected to implement distinct functions.Recent works reveal that there exists <tex-math>\textit{implicit}</tex-math> modularity in standard pre-trained transformers, namely <tex-math>\textit{Emergent Modularity}</tex-math>.They indicate that such modular structures spontaneously exhibit during the early pre-training phase.Despite the benefits of modularity, most Language Models (LMs) are still treated as monolithic models in the pre-train and fine-tune paradigm, with their emergent modularity locked and underutilized.In this work, focusing on unlocking the emergent modularity in LMs, we showcase that standard LMs could be fine-tuned as their Mixture-of-Expert (MoEs) counterparts without introducing any extra parameters. Such MoEs are derived from emergent modularity and are referred to as Emergent MoEs (EMoE).Our experiments demonstrate that fine-tuning EMoE effectively improves downstream in-domain and out-of-domain generalization compared with vanilla fine-tuning.Our analysis and ablation studies further illustrate that it is robust to various configurations and can scale up to Large Language Models (i.e., Llama2-7B and Llama-30B). Code is available at https://github.com/qiuzh20/EMoE.</abstract>
      <url hash="46a48a1e">2024.naacl-long.144</url>
    </paper>
    <paper id="145">
      <title>A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality</title>
      <author><first>Maja</first><last>Stahl</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <author><first>Nadine</first><last>Michel</last></author>
      <author><first>Sebastian</first><last>Kilsbach</last></author>
      <author><first>Julian</first><last>Schmidtke</last><affiliation>Universität Hannover</affiliation></author>
      <author><first>Sara</first><last>Rezat</last><affiliation>Universität Paderborn</affiliation></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <pages>2661-2674</pages>
      <abstract>Learning argumentative writing is challenging. Besides writing fundamentals such as syntax and grammar, learners must select and arrange argument components meaningfully to create high-quality essays. To support argumentative writing computationally, one step is to mine the argumentative structure. When combined with automatic essay scoring, interactions of the argumentative structure and quality scores can be exploited for comprehensive writing support. Although studies have shown the usefulness of using information about the argumentative structure for essay scoring, no argument mining corpus with ground-truth essay quality annotations has been published yet. Moreover, none of the existing corpora contain essays written by school students specifically. To fill this research gap, we present a German corpus of 1,320 essays from school students of two age groups. Each essay has been manually annotated for argumentative structure and quality on multiple levels of granularity. We propose baseline approaches to argument mining and essay scoring, and we analyze interactions between both tasks, thereby laying the ground for quality-oriented argumentative writing support.</abstract>
      <url hash="1dbc728c">2024.naacl-long.145</url>
    </paper>
    <paper id="146">
      <title>Adjusting Interpretable Dimensions in Embedding Space with Human Judgments</title>
      <author><first>Katrin</first><last>Erk</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Marianna</first><last>Apidianaki</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <pages>2675-2686</pages>
      <abstract>Embedding spaces contain interpretable dimensions indicating gender, formality in style, or even object properties. This has been observed multiple times. Such interpretable dimensions are becoming valuable tools in different areas of study, from social science to neuroscience. The standard way to compute these dimensions uses contrasting seed words and computes difference vectors over them. This is simple but does not always work well. We combine seed-based vectors with guidance from human ratings of where words fall along a specific dimension, and evaluate on predicting both object properties like size and danger, and the stylistic properties of formality and complexity. We obtain interpretable dimensions with markedly better performance especially in cases where seed-based dimensions do not work well.</abstract>
      <url hash="96f89da7">2024.naacl-long.146</url>
    </paper>
    <paper id="147">
      <title><fixed-case>P</fixed-case>atent<fixed-case>E</fixed-case>val: Understanding Errors in Patent Generation</title>
      <author><first>You</first><last>Zuo</last></author>
      <author><first>Kim</first><last>Gerdes</last><affiliation>Université Paris-Saclay</affiliation></author>
      <author><first>Éric</first><last>Clergerie</last></author>
      <author><first>Benoît</first><last>Sagot</last><affiliation>INRIA</affiliation></author>
      <pages>2687-2710</pages>
      <abstract>In this work, we introduce a comprehensive error typology specifically designed for evaluating two distinct tasks in machine-generated patent texts: claims-to-abstract generation, and the generation of the next claim given previous ones. We have also developed a benchmark, PatentEval, for systematically assessing language models in this context. Our study includes a comparative analysis, annotated by humans, of various models. These range from those specifically adapted during training for tasks within the patent domain to the latest general-purpose large language models (LLMs). Furthermore, we explored and evaluated some metrics to approximate human judgments in patent text evaluation, analyzing the extent to which these metrics align with expert assessments. These approaches provide valuable insights into the capabilities and limitations of current language models in the specialized field of patent text generation.</abstract>
      <url hash="503aa213">2024.naacl-long.147</url>
    </paper>
    <paper id="148">
      <title>Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing</title>
      <author><first>Sai</first><last>Koneru</last><affiliation>Karlsruher Institut für Technologie</affiliation></author>
      <author><first>Miriam</first><last>Exel</last><affiliation>SAP SE</affiliation></author>
      <author><first>Matthias</first><last>Huck</last><affiliation>SAP SE</affiliation></author>
      <author><first>Jan</first><last>Niehues</last></author>
      <pages>2711-2725</pages>
      <abstract>Large language models (LLMs) have demonstrated considerable success in various natural language processing tasks, but open-source LLMs have yet to attain state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless, their significant performance in tasks demanding a broad understanding and contextual processing shows their potential for translation. To exploit these abilities, we investigate using LLMs for MT and explore recent parameter-efficient fine-tuning techniques. Surprisingly, our initial experiments found that fine-tuning with Q-LoRA for translation purposes led to performance improvements in terms of BLEU but degradation in COMET compared to in-context learning. To overcome this, we propose an alternative approach: adapting LLMs as Automatic Post-Editors (APE) rather than direct translators. Building on the ability of the LLM to handle long sequences, we also propose extending our approach to document-level translation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can yield significant improvements across both sentence and document-level metrics while generalizing to out-of-domain data. Most notably, we achieve a state-of-the-art accuracy rate of 88.7% on the ContraPro test set, which assesses the model’s ability to resolve pronoun ambiguities when translating from English to German. Lastly, during manual post-editing for document-level translation, the source sentences are iteratively annotated, which can be used to refine further translations in the document. Here, we demonstrate that leveraging human corrections can significantly reduce the number of edits required for subsequent translations.</abstract>
      <url hash="107b49ff">2024.naacl-long.148</url>
    </paper>
    <paper id="149">
      <title>Metaphor Detection with Context Enhancement and Curriculum Learning</title>
      <author><first>Kaidi</first><last>Jia</last></author>
      <author><first>Rongsheng</first><last>Li</last><affiliation>Harbin Engineering University</affiliation></author>
      <pages>2726-2737</pages>
      <abstract>Metaphor detection is a challenging task for natural language processing (NLP) systems. Previous works failed to sufficiently utilize the internal and external semantic relationships between target words and their context. Furthermore, they have faced challenges in tackling the problem of data sparseness due to the very limited available training data. To address these two challenges, we propose a novel model called MiceCL. By leveraging the difference between the literal meaning of the target word and the meaning of the sentence as the sentence external difference, MiceCL can better handle the semantic relationships. Additionally, we propose a curriculum learning framework for automatically assessing difficulty of the sentence with a pre-trained model. By starting from easy examples and gradually progressing to more difficult ones, we can ensure that the model will not deal with complex data when its ability is weak so that to avoid wasting limited data. Experimental results demonstrate that MiceCL achieves competitive performance across multiple datasets, with a significantly improved convergence speed compared to other models.</abstract>
      <url hash="05c4f216">2024.naacl-long.149</url>
    </paper>
    <paper id="150">
      <title>What Causes the Failure of Explicit to Implicit Discourse Relation Recognition?</title>
      <author><first>Wei</first><last>Liu</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Stephen</first><last>Wan</last><affiliation>CSIRO</affiliation></author>
      <author><first>Michael</first><last>Strube</last><affiliation>Heidelberg Institute for Theoretical Studies</affiliation></author>
      <pages>2738-2753</pages>
      <abstract>We consider an unanswered question in the discourse processing community: why do relation classifiers trained on explicit examples (with connectives removed) perform poorly in real implicit scenarios? Prior work claimed this is due to linguistic dissimilarity between explicit and implicit examples but provided no empirical evidence. In this study, we show that one cause for such failure is a label shift after connectives are eliminated. Specifically, we find that the discourse relations expressed by some explicit instances will change when connectives disappear. Unlike previous work manually analyzing a few examples, we present empirical evidence at the corpus level to prove the existence of such shift. Then, we analyze why label shift occurs by considering factors such as the syntactic role played by connectives, ambiguity of connectives, and more. Finally, we investigate two strategies to mitigate the label shift: filtering out noisy data and joint learning with connectives. Experiments on PDTB 2.0, PDTB 3.0, and the GUM dataset demonstrate that classifiers trained with our strategies outperform strong baselines.</abstract>
      <url hash="3a9d17ec">2024.naacl-long.150</url>
    </paper>
    <paper id="151">
      <title><fixed-case>U</fixed-case>niver<fixed-case>SLU</fixed-case>: Universal Spoken Language Understanding for Diverse Tasks with Natural Language Instructions</title>
      <author><first>Siddhant</first><last>Arora</last></author>
      <author><first>Hayato</first><last>Futami</last><affiliation>Sony</affiliation></author>
      <author><first>Jee-weon</first><last>Jung</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Yifan</first><last>Peng</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Roshan</first><last>Sharma</last><affiliation>Google</affiliation></author>
      <author><first>Yosuke</first><last>Kashiwagi</last></author>
      <author><first>Emiru</first><last>Tsunoo</last></author>
      <author><first>Karen</first><last>Livescu</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>2754-2774</pages>
      <abstract>Recent studies leverage large language models with multi-tasking capabilities, using natural language prompts to guide the model’s behavior and surpassing performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly performs various spoken language understanding (SLU) tasks? We start by adapting a pre-trained automatic speech recognition model to additional tasks using single-token task specifiers. We enhance this approach through instruction tuning, i.e., finetuning by describing the task using natural language instructions followed by the list of label options. Our approach can generalize to new task descriptions for the seen tasks during inference, thereby enhancing its user-friendliness. We demonstrate the efficacy of our single multi-task learning model “UniverSLU” for 12 speech classification and sequence generation task types spanning 17 datasets and 9 languages. On most tasks, UniverSLU achieves competitive performance and often even surpasses task-specific models. Additionally, we assess the zero-shot capabilities, finding that the model generalizes to new datasets and languages for seen task types.</abstract>
      <url hash="0c25b51b">2024.naacl-long.151</url>
    </paper>
    <paper id="152">
      <title>How Trustworthy are Open-Source <fixed-case>LLM</fixed-case>s? An Assessment under Malicious Demonstrations Shows their Vulnerabilities</title>
      <author><first>Lingbo</first><last>Mo</last></author>
      <author><first>Boshi</first><last>Wang</last><affiliation>Ohio State University</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <author><first>Huan</first><last>Sun</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>2775-2792</pages>
      <abstract>The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.</abstract>
      <url hash="b3f9834e">2024.naacl-long.152</url>
    </paper>
    <paper id="153">
      <title>Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models</title>
      <author><first>Yue</first><last>Zhou</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Yada</first><last>Zhu</last><affiliation>IBM Research</affiliation></author>
      <author><first>Diego</first><last>Antognini</last><affiliation>Google Research</affiliation></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Zhang</last></author>
      <pages>2793-2804</pages>
      <abstract>This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models. We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate, exposing the language model’s lack of robustness and sensitivity to the surface form in reasoning through complex problems. To improve mathematical reasoning performance, we propose Self-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths from specific surface forms of the problem. We evaluate our approach on four mathematics reasoning benchmarks over three large language models and show that SCoP improves mathematical reasoning performance over vanilla self-consistency, particularly for problems initially deemed unsolvable. Finally, we provide additional experiments and discussion regarding problem difficulty and surface forms, including cross-model difficulty agreement and paraphrasing transferability, and Variance of Variations (VOV) for language model evaluation.</abstract>
      <url hash="d4b2786d">2024.naacl-long.153</url>
    </paper>
    <paper id="154">
      <title><fixed-case>T</fixed-case>ri<fixed-case>S</fixed-case>um: Learning Summarization Ability from Large Language Models with Structured Rationale</title>
      <author><first>Pengcheng</first><last>Jiang</last></author>
      <author><first>Cao</first><last>Xiao</last><affiliation>GE Healthcare</affiliation></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Parminder</first><last>Bhatia</last><affiliation>GEHC</affiliation></author>
      <author><first>Jimeng</first><last>Sun</last><affiliation>Georgia Tech Research Corporation, University of Illinois, Urbana Champaign, College of Computing and Georgia Institute of Technology</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>2805-2819</pages>
      <abstract>The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs’ text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summarization rationale.</abstract>
      <url hash="8d533563">2024.naacl-long.154</url>
    </paper>
    <paper id="155">
      <title><fixed-case>G</fixed-case>en<fixed-case>RES</fixed-case>: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models</title>
      <author><first>Pengcheng</first><last>Jiang</last></author>
      <author><first>Jiacheng</first><last>Lin</last><affiliation>Department of Computer Science, University of Illinois</affiliation></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Jimeng</first><last>Sun</last><affiliation>Georgia Tech Research Corporation, University of Illinois, Urbana Champaign, College of Computing and Georgia Institute of Technology</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>2820-2837</pages>
      <abstract>The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities can cause hallucinations. Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent with human preferences for RE quality. Last, we made a comprehensive evaluation of fourteen leading LLMs using GenRES across document, bag, and sentence level RE datasets, respectively, to set the benchmark for future research in GRE</abstract>
      <url hash="49154b40">2024.naacl-long.155</url>
    </paper>
    <paper id="156">
      <title>Curated Datasets and Neural Models for Machine Translation of Informal Registers between <fixed-case>M</fixed-case>ayan and <fixed-case>S</fixed-case>panish Vernaculars</title>
      <author><first>Andrés</first><last>Lou</last><affiliation>Universidad de Alicante</affiliation></author>
      <author><first>Juan Antonio</first><last>Pérez-Ortiz</last><affiliation>Universidad de Alicante</affiliation></author>
      <author><first>Felipe</first><last>Sánchez-Martínez</last><affiliation>University of Alicante</affiliation></author>
      <author><first>Víctor</first><last>Sánchez-Cartagena</last><affiliation>Universidad de Alicante</affiliation></author>
      <pages>2838-2850</pages>
      <abstract>The Mayan languages comprise a language family with an ancient history, millions of speakers, and immense cultural value, that, nevertheless, remains severely underrepresented in terms of resources and global exposure. In this paper we develop, curate, and publicly release a set of corpora in several Mayan languages spoken in Guatemala and Southern Mexico, which we call MayanV. The datasets are parallel with Spanish, the dominant language of the region, and are taken from official native sources focused on representing informal, day-to-day, and non-domain-specific language. As such, and according to our dialectometric analysis, they differ in register from most other available resources. Additionally, we present neural machine translation models, trained on as many resources and Mayan languages as possible, and evaluated exclusively on our datasets. We observe lexical divergences between the dialects of Spanish in our resources and the more widespread written standard of Spanish, and that resources other than the ones we present do not seem to improve translation performance, indicating that many such resources may not accurately capture common, real-life language usage. The MayanV dataset is available at https://github.com/transducens/mayanv.</abstract>
      <url hash="bec867e3">2024.naacl-long.156</url>
    </paper>
    <paper id="157">
      <title>The Effect of Data Partitioning Strategy on Model Generalizability: A Case Study of Morphological Segmentation</title>
      <author><first>Zoey</first><last>Liu</last><affiliation>University of Florida</affiliation></author>
      <author><first>Bonnie</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <pages>2851-2864</pages>
      <abstract>Recent work to enhance data partitioning strategies for more realistic model evaluation face challenges in providing a clear optimal choice. This study addresses these challenges, focusing on morphological segmentation and synthesizing limitations related to language diversity, adoption of multiple datasets and splits, and detailed model comparisons. Our study leverages data from 19 languages, including ten indigenous or endangered languages across 10 language families with diverse morphological systems (polysynthetic, fusional, and agglutinative) and different degrees of data availability. We conduct large-scale experimentation with varying sized combinations of training and evaluation sets as well as new test data. Our results show that, when faced with new test data: (1) models trained from random splits are able to achieve higher numerical scores; (2) model rankings derived from random splits tend to generalize more consistently.</abstract>
      <url hash="36b341ed">2024.naacl-long.157</url>
    </paper>
    <paper id="158">
      <title>Measuring Entrainment in Spontaneous Code-switched Speech</title>
      <author><first>Debasmita</first><last>Bhattacharya</last><affiliation>Columbia University</affiliation></author>
      <author><first>Siying</first><last>Ding</last></author>
      <author><first>Alayna</first><last>Nguyen</last></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University</affiliation></author>
      <pages>2865-2876</pages>
      <abstract>It is well-known that speakers who entrain to one another have more successful conversations than those who do not. Previous research has shown that interlocutors entrain on linguistic features in both written and spoken <tex-math>\emph{monolingual}</tex-math> domains. More recent work on <tex-math>\emph{code-switched}</tex-math> communication has also shown preliminary evidence of entrainment on certain aspects of code-switching (CSW). However, such studies of entrainment in code-switched domains have been extremely few and restricted to human-machine textual interactions. Our work studies code-switched spontaneous speech between humans, finding that (1) patterns of written and spoken entrainment in monolingual settings largely generalize to code-switched settings, and (2) some patterns of entrainment on code-switching in dialogue agent-generated text generalize to spontaneous code-switched speech. Our findings give rise to important implications for the potentially “universal” nature of entrainment as a communication phenomenon, and potential applications in inclusive and interactive speech technology.</abstract>
      <url hash="7694f59d">2024.naacl-long.158</url>
    </paper>
    <paper id="159">
      <title>A Survey of Meaning Representations – From Theory to Practical Utility</title>
      <author><first>Zacchary</first><last>Sadeddine</last></author>
      <author><first>Juri</first><last>Opitz</last><affiliation>Ruprecht-Karls-Universität Heidelberg and University of Zurich</affiliation></author>
      <author><first>Fabian</first><last>Suchanek</last><affiliation>Telecom Paris</affiliation></author>
      <pages>2877-2892</pages>
      <abstract>Symbolic meaning representations of natural language text have been studied since at least the 1960s. With the availability of large annotated corpora, and more powerful machine learning tools, the field has recently seen several new developments. In this survey, we study today’s most prominent Meaning Representation Frameworks. We shed light on their theoretical properties, as well as on their practical research environment, i.e., on datasets, parsers, applications, and future challenges.</abstract>
      <url hash="8c9c1c88">2024.naacl-long.159</url>
    </paper>
    <paper id="160">
      <title>Mitigating Language-Level Performance Disparity in m<fixed-case>PLM</fixed-case>s via Teacher Language Selection and Cross-lingual Self-Distillation</title>
      <author><first>Haozhe</first><last>Zhao</last></author>
      <author><first>Zefan</first><last>Cai</last></author>
      <author><first>Shuzheng</first><last>Si</last></author>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Yufeng</first><last>He</last></author>
      <author><first>Kaikai</first><last>An</last></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <pages>2893-2907</pages>
      <abstract>Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive performance on cross-language tasks, yet significant performance disparities exist across different languages within the same mPLM. Previous studies endeavored to narrow these disparities by supervise fine-tuning the mPLMs with multilingual data.However, obtaining labeled multilingual data is time-consuming, and fine-tuning mPLM with limited labeled multilingual data merely encapsulates the knowledge specific to the labeled data.Therefore, we introduce **ALSACE** to leverage the learned knowledge from the well-performing languages to guide under-performing ones within the same mPLM, eliminating the need for additional labeled multilingual data. Experiments show that ALSACE effectively mitigates language-level performance disparity across various mPLMs while showing the competitive performance on different multilingual NLU tasks, ranging from full resource to limited resource settings. The code for our approach is available at https://github.com/pkunlp-icler/ALSACE.</abstract>
      <url hash="4d82fc1f">2024.naacl-long.160</url>
    </paper>
    <paper id="161">
      <title>Evaluating In-Context Learning of Libraries for Code Generation</title>
      <author><first>Arkil</first><last>Patel</last><affiliation>Mila - Quebec AI Institute and McGill University</affiliation></author>
      <author><first>Siva</first><last>Reddy</last><affiliation>Mila, McGill University and Mila, McGill University</affiliation></author>
      <author><first>Dzmitry</first><last>Bahdanau</last><affiliation>ServiceNow Research</affiliation></author>
      <author><first>Pradeep</first><last>Dasigi</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>2908-2926</pages>
      <abstract>Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.</abstract>
      <url hash="404e8592">2024.naacl-long.161</url>
    </paper>
    <paper id="162">
      <title>Visually-Aware Context Modeling for News Image Captioning</title>
      <author><first>Tingyu</first><last>Qu</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Tinne</first><last>Tuytelaars</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Marie-Francine</first><last>Moens</last><affiliation>KU Leuven, KU Leuven</affiliation></author>
      <pages>2927-2943</pages>
      <abstract>News Image Captioning aims to create captions from news articles and images, emphasizing the connection between textual context and visual elements. Recognizing the significance of human faces in news images and the face-name co-occurrence pattern in existing datasets, we propose a face-naming module for learning better name embeddings. Apart from names, which can be directly linked to an image area (faces), news image captions mostly contain context information that can only be found in the article. We design a retrieval strategy using CLIP to retrieve sentences that are semantically close to the image, mimicking human thought process of linking articles to images. Furthermore, to tackle the problem of the imbalanced proportion of article context and image context in captions, we introduce a simple yet effective method Contrasting with Language Model backbone (CoLaM) to the training pipeline. We conduct extensive experiments to demonstrate the efficacy of our framework. We out-perform the previous state-of-the-art (without external data) by 7.97/5.80 CIDEr scores on GoodNews/NYTimes800k. Our code is available at https://github.com/tingyu215/VACNIC.</abstract>
      <url hash="dd051a49">2024.naacl-long.162</url>
    </paper>
    <paper id="163">
      <title>Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning</title>
      <author><first>Athul</first><last>Jacob</last><affiliation>Google and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Gabriele</first><last>Farina</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology and Microsoft</affiliation></author>
      <pages>2944-2955</pages>
      <abstract>We present a game-theoretic model of pragmatics that we call ReCo (for Regularized Conventions). This model formulates pragmatic communication as a game in which players are rewarded for communicating successfully and penalized for deviating from a shared, “default” semantics. As a result, players assign utterances context-dependent meanings that jointly optimize communicative success and naturalness with respect to speakers’ and listeners’ background knowledge of language. By using established game-theoretic tools to compute equilibrium strategies for this game, we obtain principled pragmatic language generation procedures with formal guarantees of communicative success. Across several datasets capturing real and idealized human judgments about pragmatic implicature, ReCo matches, or slightly improves upon, predictions made by Iterated Best Response and Rational Speech Acts models of language understanding.</abstract>
      <url hash="f7b1e164">2024.naacl-long.163</url>
    </paper>
    <paper id="164">
      <title><fixed-case>T</fixed-case>opic<fixed-case>GPT</fixed-case>: A Prompt-based Topic Modeling Framework</title>
      <author><first>Chau</first><last>Pham</last></author>
      <author><first>Alexander</first><last>Hoyle</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Simeng</first><last>Sun</last><affiliation>College of Information and Computer Science, University of Massachusetts, Amherst</affiliation></author>
      <author><first>Philip</first><last>Resnik</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>2956-2984</pages>
      <abstract>Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require “reading the tea leaves” to interpret; additionally, they offer users minimal control over the formatting and specificity of resulting topics. To tackle these issues, we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics in a text collection. TopicGPT produces topics that align better with human categorizations compared to competing methods: it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also more interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and modify topics without the need for model retraining. By streamlining access to high-quality and interpretable topics, TopicGPT represents a compelling, human-centered approach to topic modeling.</abstract>
      <url hash="a28861cc">2024.naacl-long.164</url>
    </paper>
    <paper id="165">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger</title>
      <author><first>Jiazhao</first><last>Li</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Yijin</first><last>Yang</last></author>
      <author><first>Zhuofeng</first><last>Wu</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>V.G.Vinod</first><last>Vydiswaran</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <pages>2985-3004</pages>
      <abstract>Textual backdoor attacks, characterized by subtle manipulations of input triggers and training dataset labels, pose significant threats to security-sensitive applications. The rise of advanced generative models, such as GPT-4, with their capacity for human-like rewriting, makes these attacks increasingly challenging to detect. In this study, we conduct an in-depth examination of black-box generative models as tools for backdoor attacks, thereby emphasizing the need for effective defense strategies. We propose BGMAttack, a novel framework that harnesses advanced generative models to execute stealthier backdoor attacks on text classifiers. Unlike prior approaches constrained by subpar generation quality, BGMAttack renders backdoor triggers more elusive to human cognition and advanced machine detection. A rigorous evaluation of attack effectiveness over four sentiment classification tasks, complemented by four human cognition stealthiness tests, reveals BGMAttack’s superior performance, achieving a state-of-the-art attack success rate of 97.35% on average while maintaining superior stealth compared to conventional methods. The dataset and code are available: https://github.com/JiazhaoLi/BGMAttack.</abstract>
      <url hash="48399a24">2024.naacl-long.165</url>
    </paper>
    <paper id="166">
      <title>Social Meme-ing: Measuring Linguistic Variation in Memes</title>
      <author><first>Naitian</first><last>Zhou</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>David</first><last>Bamman</last><affiliation>University of California Berkeley</affiliation></author>
      <pages>3005-3024</pages>
      <abstract>Much work in the space of NLP has used computational methods to explore sociolinguistic variation in text. In this paper, we argue that memes, as multimodal forms of language comprised of visual templates and text, also exhibit meaningful social variation. We construct a computational pipeline to cluster individual instances of memes into templates and semantic variables, taking advantage of their multimodal structure in doing so. We apply this method to a large collection of meme images from Reddit and make available the resulting SemanticMemes dataset of 3.8M images clustered by their semantic function. We use these clusters to analyze linguistic variation in memes, discovering not only that socially meaningful variation in meme usage exists between subreddits, but that patterns of meme innovation and acculturation within these communities align with previous findings on written language.</abstract>
      <url hash="c684ff6b">2024.naacl-long.166</url>
    </paper>
    <paper id="167">
      <title><fixed-case>E</fixed-case>xpert<fixed-case>QA</fixed-case>: Expert-Curated Questions and Attributed Answers</title>
      <author><first>Chaitanya</first><last>Malaviya</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Subin</first><last>Lee</last></author>
      <author><first>Sihao</first><last>Chen</last></author>
      <author><first>Elizabeth</first><last>Sieber</last><affiliation>University of Washington</affiliation></author>
      <author><first>Mark</first><last>Yatskar</last><affiliation>Department of Computer and Information Science, School of Engineering and Applied Science</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <pages>3025-3045</pages>
      <abstract>As language models are adopted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying attribution and factuality has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we conduct human evaluation of responses from a few representative systems along various axes of attribution and factuality, by bringing domain experts in the loop. Specifically, we collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. In addition, we ask experts to improve upon responses from language models. The output of our analysis is ExpertQA, a high-quality long-form QA dataset with 2177 questions spanning 32 fields, along with verified answers and attributions for claims in the answers.</abstract>
      <url hash="b7a2b8ff">2024.naacl-long.167</url>
    </paper>
    <paper id="168">
      <title>What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception</title>
      <author><first>Chaitanya</first><last>Malaviya</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Subin</first><last>Lee</last></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <author><first>Mark</first><last>Yatskar</last><affiliation>Department of Computer and Information Science, School of Engineering and Applied Science</affiliation></author>
      <pages>3046-3065</pages>
      <abstract>Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales (or explanations) generated by QA models to support their answers. We specifically consider decomposed QA models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample rationales from language models using few-shot prompting for two datasets, and then perform two user studies. First, we present users with incorrect answers and corresponding rationales in various formats and ask them to provide natural language feedback to revise the rationale. We then measure the effectiveness of this feedback in patching these rationales through in-context learning. The second study evaluates how well different rationale formats enable users to understand and trust model answers, when they are correct. We find that rationale formats significantly affect how easy it is (1) for users to give feedback for rationales, and (2) for models to subsequently execute this feedback. In addition, formats with attributions to the context and in-depth reasoning significantly enhance user-reported understanding and trust of model outputs.</abstract>
      <url hash="7f8eedee">2024.naacl-long.168</url>
    </paper>
    <paper id="169">
      <title>When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels</title>
      <author><first>Weiyan</first><last>Shi</last></author>
      <author><first>Emily</first><last>Dinan</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Kurt</first><last>Shuster</last></author>
      <author><first>Jason</first><last>Weston</last><affiliation>New York University and Facebook</affiliation></author>
      <author><first>Jing</first><last>Xu</last><affiliation>Facebook AI Research</affiliation></author>
      <pages>3066-3082</pages>
      <abstract>Deployed dialogue agents have the potential to integrate human feedback to continuously improve themselves. However, humans may not always provide explicit signals when the chatbot makes mistakes during interactions. In this work, we propose Juicer, a framework to make use of both binary and free-form textual human feedback. It works by: (i) extending sparse binary feedback by training a satisfaction classifier to label the unlabeled data; and (ii) training a reply corrector to map the bad replies to good ones. We find that augmenting training with model-corrected replies improves the final dialogue model, and we can further improve performance by using both positive and negative replies through the recently proposed Director model.</abstract>
      <url hash="f019abeb">2024.naacl-long.169</url>
    </paper>
    <paper id="170">
      <title>Kreyòl-<fixed-case>MT</fixed-case>: Building <fixed-case>MT</fixed-case> for <fixed-case>L</fixed-case>atin <fixed-case>A</fixed-case>merican, <fixed-case>C</fixed-case>aribbean and Colonial <fixed-case>A</fixed-case>frican Creole Languages</title>
      <author><first>Nathaniel</first><last>Robinson</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Ammon</first><last>Shurtz</last></author>
      <author><first>Rasul</first><last>Dent</last><affiliation>INRIA</affiliation></author>
      <author><first>Onenamiyi</first><last>Onesi</last></author>
      <author><first>Claire</first><last>Monroc</last></author>
      <author><first>Loïc</first><last>Grobol</last><affiliation>Université Paris Nanterre</affiliation></author>
      <author><first>Hasan</first><last>Muhammad</last></author>
      <author><first>Ashi</first><last>Garg</last></author>
      <author><first>Naome</first><last>Etori</last></author>
      <author><first>Vijay Murari</first><last>Tiyyala</last></author>
      <author><first>Olanrewaju</first><last>Samuel</last></author>
      <author><first>Matthew</first><last>Stutzman</last></author>
      <author><first>Bismarck</first><last>Odoom</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Sanjeev</first><last>Khudanpur</last><affiliation>Whiting School of Engineering</affiliation></author>
      <author><first>Stephen</first><last>Richardson</last><affiliation>Brigham Young University</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>3083-3110</pages>
      <abstract>A majority of language technologies are tailored for a small number of high-resource languages, while relatively many low-resource languages are neglected. One such group, Creole languages, have long been marginalized in academic study, though their speakers could benefit from machine translation (MT). These languages are predominantly used in much of Latin America, Africa and the Caribbean. We present the largest cumulative dataset to date for Creole language MT, including 14.5M unique Creole sentences with parallel translations—11.6M of which we release publicly, and the largest bitexts gathered to date for 41 languages—the first ever for 21. In addition, we provide MT models supporting all 41 Creole languages in 172 translation directions. Given our diverse dataset, we produce a model for Creole language MT exposed to more genre diversity then ever before, which outperforms a genre-specific Creole MT model on its own benchmark for 23 of 34 translation directions.</abstract>
      <url hash="9e0210ee">2024.naacl-long.170</url>
    </paper>
    <paper id="171">
      <title>Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models</title>
      <author><first>Jiashu</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Mingyu</first><last>Ma</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>3111-3126</pages>
      <abstract>We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions (~1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets; and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.</abstract>
      <url hash="6536fed8">2024.naacl-long.171</url>
    </paper>
    <paper id="172">
      <title>Modeling Empathetic Alignment in Conversation</title>
      <author><first>Jiamin</first><last>Yang</last></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>3127-3148</pages>
      <abstract>Empathy requires perspective-taking: empathetic responses require a person to reason about what another has experienced and communicate that understanding in language. However, most NLP approaches to empathy do not explicitly model this alignment process. Here, we introduce a new approach to recognizing alignment in empathetic speech, grounded in Appraisal Theory. We introduce a new dataset of over 9.2K span-level annotations of different types of appraisals of a person’s experience and over 3K empathetic alignments between a speaker’s and observer’s speech. Through computational experiments, we show that these appraisals and alignments can be accurately recognized. In experiments in over 9.2M Reddit conversations, we find that appraisals capture meaningful groupings of behavior but that most responses have minimal alignment. However, we find that mental health professionals engage with substantially more empathetic alignment.</abstract>
      <url hash="4c4a9542">2024.naacl-long.172</url>
    </paper>
    <paper id="173">
      <title>Native Language Identification in Texts: A Survey</title>
      <author><first>Dhiman</first><last>Goswami</last><affiliation>George Mason University</affiliation></author>
      <author><first>Sharanya</first><last>Thilagan</last></author>
      <author><first>Kai</first><last>North</last></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <author><first>Marcos</first><last>Zampieri</last><affiliation>George Mason University</affiliation></author>
      <pages>3149-3160</pages>
      <abstract>We present the first comprehensive survey of Native Language Identification (NLI) applied to texts. NLI is the task of automatically identifying an author’s native language (L1) based on their second language (L2) production. NLI is an important task with practical applications in second language teaching and NLP. The task has been widely studied for both text and speech, particularly for L2 English due to the availability of suitable corpora. Speech-based NLI relies heavily on accent modeled by pronunciation patterns and prosodic cues while text-based NLI relies primarily on modeling spelling errors and grammatical patterns that reveal properties of an individuals’ L1 influencing L2 production. We survey over one hundred papers on the topic including the papers associated with the NLI and INLI shared tasks. We describe several text representations and computational techniques used in text-based NLI. Finally, we present a comprehensive account of publicly available datasets used for the task thus far.</abstract>
      <url hash="48421464">2024.naacl-long.173</url>
    </paper>
    <paper id="174">
      <title><fixed-case>L</fixed-case>o<fixed-case>RETTA</fixed-case>: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models</title>
      <author><first>Yifan</first><last>Yang</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Jiajun</first><last>Zhou</last></author>
      <author><first>Ngai</first><last>Wong</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Zheng</first><last>Zhang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>3161-3176</pages>
      <abstract>Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named LoRETTA_adp and LoRETTA_rep. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight reparameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to <tex-math>100\times</tex-math> fewer parameters on the LLaMA-2-7B models. Furthermore, empirical results demonstrate that the proposed methods exhibit remarkable anti-overfitting capability, effectively improve training efficiency, and enjoy better multi-task learning performance. Plug-and-play loretta library built upon the Huggingface framework and PEFT library are provided.</abstract>
      <url hash="b8ecc5f0">2024.naacl-long.174</url>
    </paper>
    <paper id="175">
      <title>Which One? Leveraging Context Between Objects and Multiple Views for Language Grounding</title>
      <author><first>Chancharik</first><last>Mitra</last></author>
      <author><first>Abrar</first><last>Anwar</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Rodolfo</first><last>Corona</last></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Trevor</first><last>Darrell</last><affiliation>Electrical Engineering &amp; Computer Science Department</affiliation></author>
      <author><first>Jesse</first><last>Thomason</last><affiliation>University of Southern California and Amazon</affiliation></author>
      <pages>3177-3189</pages>
      <abstract>When connecting objects and their language referents in an embodied 3D environment, it is important to note that: (1) an object can be better characterized by leveraging comparative information between itself and other objects, and (2) an object’s appearance can vary with camera position. As such, we present the Multi-view Approach to Grounding in Context (MAGiC) model, which selects an object referent based on language that distinguishes between two similar objects. By pragmatically reasoning over both objects and across multiple views of those objects, MAGiC improves over the state-of-the-art model on the SNARE object reference task with a relative error reduction of 12.9% (representing an absolute improvement of 2.7%). Ablation studies show that reasoning jointly over object referent candidates and multiple views of each object both contribute to improved accuracy. Code: https://github.com/rcorona/magic_snare/</abstract>
      <url hash="1428bb31">2024.naacl-long.175</url>
    </paper>
    <paper id="176">
      <title>Do Localization Methods Actually Localize Memorized Data in <fixed-case>LLM</fixed-case>s? A Tale of Two Benchmarks</title>
      <author><first>Ting-Yun</first><last>Chang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jesse</first><last>Thomason</last><affiliation>University of Southern California and Amazon</affiliation></author>
      <author><first>Robin</first><last>Jia</last><affiliation>University of Southern California</affiliation></author>
      <pages>3190-3211</pages>
      <abstract>The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these “ground truth” weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods identify neurons that are not specific to a single memorized sequence.</abstract>
      <url hash="bdf05fcc">2024.naacl-long.176</url>
    </paper>
    <paper id="177">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>F</fixed-case>ix: Few-shot Backdoor Removal via Adversarial Prompt Tuning</title>
      <author><first>Tianrong</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Zhaohan</first><last>Xi</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Ting</first><last>Wang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Prasenjit</first><last>Mitra</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Jinghui</first><last>Chen</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>3212-3225</pages>
      <abstract>Pre-trained language models (PLMs) have attracted enormous attention over the past few years with their unparalleled performances. Meanwhile, the soaring cost to train PLMs as well as their amazing generalizability have jointly contributed to few-shot fine-tuning and prompting as the most popular training paradigms for natural language processing (NLP) models. Nevertheless, existing studies have shown that these NLP models can be backdoored such that model behavior is manipulated when trigger tokens are presented.In this paper, we propose PromptFix, a novel backdoor mitigation strategy for NLP models via adversarial prompt-tuning in few-shot settings.Unlike existing NLP backdoor removal methods, which rely on accurate trigger inversion and subsequent model fine-tuning, PromptFix keeps the model parameters intact and only utilizes two extra sets of soft tokens which approximate the trigger and counteract it respectively. The use of soft tokens and adversarial optimization eliminates the need to enumerate possible backdoor configurations and enables an adaptive balance between trigger finding and preservation of performance.Experiments with various backdoor attacks validate the effectiveness of the proposed method and the performances when domain shift is present further shows PromptFix’s applicability to models pretrained on unknown data source which is the common case in prompt tuning scenarios.</abstract>
      <url hash="668281cb">2024.naacl-long.177</url>
    </paper>
    <paper id="178">
      <title>Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models</title>
      <author><first>Zhixue</first><last>Zhao</last><affiliation>University of Sheffield, University of Sheffield</affiliation></author>
      <author><first>Nikolaos</first><last>Aletras</last><affiliation>University of Sheffield, University of Sheffield and Amazon</affiliation></author>
      <pages>3226-3244</pages>
      <abstract>In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models. Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers. Our code is available: https://github.com/casszhao/multilingual-faith.</abstract>
      <url hash="b6a9ebef">2024.naacl-long.178</url>
    </paper>
    <paper id="179">
      <title>A Pretrainer’s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, &amp; Toxicity</title>
      <author><first>Shayne</first><last>Longpre</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Gregory</first><last>Yauney</last><affiliation>Cornell University</affiliation></author>
      <author><first>Emily</first><last>Reif</last></author>
      <author><first>Katherine</first><last>Lee</last><affiliation>Cornell University and Google</affiliation></author>
      <author><first>Adam</first><last>Roberts</last><affiliation>Google</affiliation></author>
      <author><first>Barret</first><last>Zoph</last></author>
      <author><first>Denny</first><last>Zhou</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Jason</first><last>Wei</last><affiliation>OpenAI</affiliation></author>
      <author><first>Kevin</first><last>Robinson</last><affiliation>Google Research</affiliation></author>
      <author><first>David</first><last>Mimno</last><affiliation>Cornell University</affiliation></author>
      <author><first>Daphne</first><last>Ippolito</last></author>
      <pages>3245-3276</pages>
      <abstract>Pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. We pretrain models on data curated (1) at different collection times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we find that temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we measure the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Third, we empirically validate that heterogeneous data sources, like books and web, are beneficial and warrant greater prioritization. To date, these experiments constitute the single largest publicly documented empirical study of the effects of pretraining data. Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these findings validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development.</abstract>
      <url hash="e120f93b">2024.naacl-long.179</url>
    </paper>
    <paper id="180">
      <title>Instructional Fingerprinting of Large Language Models</title>
      <author><first>Jiashu</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Mingyu</first><last>Ma</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Pang Wei</first><last>Koh</last><affiliation>University of Washington</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>3277-3306</pages>
      <abstract>The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (eg restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License.</abstract>
      <url hash="6368045f">2024.naacl-long.180</url>
    </paper>
    <paper id="181">
      <title>Reinforced Multiple Instance Selection for Speaker Attribute Prediction</title>
      <author><first>Alireza</first><last>Salkhordeh Ziabari</last></author>
      <author><first>Ali</first><last>Omrani</last></author>
      <author><first>Parsa</first><last>Hejabi</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Preni</first><last>Golazizian</last></author>
      <author><first>Brendan</first><last>Kennedy</last></author>
      <author><first>Payam</first><last>Piray</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Morteza</first><last>Dehghani</last><affiliation>University of Southern California</affiliation></author>
      <pages>3307-3321</pages>
      <abstract>Language usage is related to speaker age, gender, moral concerns, political ideology, and other attributes. Current state-of-the-art methods for predicting these attributes take a speaker’s utterances as input and provide a prediction per speaker attribute. Most of these approaches struggle to handle a large number of utterances per speaker. This difficulty is primarily due to the computational constraints of the models. Additionally, only a subset of speaker utterances may be relevant to specific attributes. In this paper, we formulate speaker attribute prediction as a Multiple Instance Learning (MIL) problem and propose RL-MIL, a novel approach based on Reinforcement Learning (RL) that effectively addresses both of these challenges. Our experiments demonstrate that our RL-based methodology consistently outperforms previous approaches across a range of related tasks: predicting speakers’ psychographics and demographics from social media posts, and political ideologies from transcribed speeches. We create synthetic datasets and investigate the behavior of RL-MIL systematically. Our results show the success of RL-MIL in improving speaker attribute prediction by learning to select relevant speaker utterances.</abstract>
      <url hash="19a8bb06">2024.naacl-long.181</url>
    </paper>
    <paper id="182">
      <title><fixed-case>D</fixed-case>yna<fixed-case>M</fixed-case>o: Accelerating Language Model Inference with Dynamic Multi-Token Sampling</title>
      <author><first>Shikhar</first><last>Tuli</last></author>
      <author><first>Chi-Heng</first><last>Lin</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Yen-Chang</first><last>Hsu</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Niraj</first><last>Jha</last><affiliation>Princeton University</affiliation></author>
      <author><first>Yilin</first><last>Shen</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Hongxia</first><last>Jin</last><affiliation>Samsung Research America AI center</affiliation></author>
      <pages>3322-3345</pages>
      <abstract>Traditional language models operate autoregressively, i.e., they predict one token at a time. Rapid explosion in model sizes has resulted in high inference times. In this work, we propose DynaMo, a suite of multi-token prediction language models that reduce net inference times. Our models *dynamically* predict multiple tokens based on their confidence in the predicted joint probability distribution. We propose a lightweighttechnique to train these models, leveraging the weights of traditional autoregressive counterparts. Moreover, we propose novel ways to enhance the estimated joint probability to improve text generation quality, namely co-occurrence weighted masking and adaptive thresholding. We also propose systematic qualitative and quantitative methods to rigorously test the quality of generated text for non-autoregressive generation. One of the models in our suite, DynaMo-7.3B-T3, achieves same-quality generated text as the baseline (Pythia-6.9B) while achieving 2.57<tex-math>\times</tex-math> speed-up with only 5.87% and 2.67% parameter and training time overheads, respectively.</abstract>
      <url hash="040fa45c">2024.naacl-long.182</url>
    </paper>
    <paper id="183">
      <title>Few-shot Knowledge Graph Relational Reasoning via Subgraph Adaptation</title>
      <author><first>Haochen</first><last>Liu</last></author>
      <author><first>Song</first><last>Wang</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Jundong</first><last>Li</last><affiliation>University of Virginia</affiliation></author>
      <pages>3346-3356</pages>
      <abstract>Few-shot Knowledge Graph (KG) Relational Reasoning aims to predict unseen triplets (i.e., query triplets) for rare relations in KGs, given only several triplets of these relations as references (i.e., support triplets). This task has gained significant traction due to the widespread use of knowledge graphs in various natural language processing applications. Previous approaches have utilized meta-training methods and manually constructed meta-relation sets to tackle this task. Recent efforts have focused on edge-mask-based methods, which exploit the structure of the contextualized graphs of target triplets (i.e., a subgraph containing relevant triplets in the KG). However, existing edge-mask-based methods have limitations in extracting insufficient information from KG and are highly influenced by spurious information in KG. To overcome these challenges, we propose SAFER (Subgraph Adaptation for Few-shot Relational Reasoning), a novel approach that effectively adapts the information in contextualized graphs to various subgraphs generated from support and query triplets to perform the prediction. Specifically, SAFER enables the extraction of more comprehensive information from support triplets while minimizing the impact of spurious information when predicting query triplets. Experimental results on three prevalent datasets demonstrate the superiority of our proposed framework SAFER.</abstract>
      <url hash="7af3298c">2024.naacl-long.183</url>
    </paper>
    <paper id="184">
      <title>Uncertainty Quantification for In-Context Learning of Large Language Models</title>
      <author><first>Chen</first><last>Ling</last></author>
      <author><first>Xujiang</first><last>Zhao</last><affiliation>NEC Labs America</affiliation></author>
      <author><first>Xuchao</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Wei</first><last>Cheng</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Yanchi</first><last>Liu</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Yiyou</first><last>Sun</last></author>
      <author><first>Mika</first><last>Oishi</last><affiliation>NEC</affiliation></author>
      <author><first>Takao</first><last>Osaki</last><affiliation>NEC</affiliation></author>
      <author><first>Katsushi</first><last>Matsuda</last><affiliation>NEC</affiliation></author>
      <author><first>Jie</first><last>Ji</last></author>
      <author><first>Guangji</first><last>Bai</last><affiliation>Emory University</affiliation></author>
      <author><first>Liang</first><last>Zhao</last><affiliation>George Mason University, Emory University and Emory University</affiliation></author>
      <author><first>Haifeng</first><last>Chen</last><affiliation>NEC-Labs</affiliation></author>
      <pages>3357-3370</pages>
      <abstract>In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM’s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM’s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model’s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.</abstract>
      <url hash="c8358b98">2024.naacl-long.184</url>
    </paper>
    <paper id="185">
      <title><fixed-case>H</fixed-case>elp<fixed-case>S</fixed-case>teer: Multi-attribute Helpfulness Dataset for <fixed-case>S</fixed-case>teer<fixed-case>LM</fixed-case></title>
      <author><first>Zhilin</first><last>Wang</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Yi</first><last>Dong</last></author>
      <author><first>Jiaqi</first><last>Zeng</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Virginia</first><last>Adams</last></author>
      <author><first>Makesh Narsimhan</first><last>Sreedhar</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Daniel</first><last>Egert</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Olivier</first><last>Delalleau</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Jane</first><last>Scowcroft</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Neel</first><last>Kant</last></author>
      <author><first>Aidan</first><last>Swope</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Oleksii</first><last>Kuchaiev</last><affiliation>NVIDIA</affiliation></author>
      <pages>3371-3384</pages>
      <abstract>Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length). To alleviate this problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful. Specifically, our 37k-sample dataset has annotations for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. Training Llama 2 70B using the HelpSteer dataset with SteerLM technique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT-4). We release this dataset with CC-BY-4.0 license at https://huggingface.co/datasets/nvidia/HelpSteer</abstract>
      <url hash="0adec7a8">2024.naacl-long.185</url>
    </paper>
    <paper id="186">
      <title>A Preference-driven Paradigm for Enhanced Translation with Large Language Models</title>
      <author><first>Dawei</first><last>Zhu</last></author>
      <author><first>Sony</first><last>Trenous</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last><affiliation>Saarland University</affiliation></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last><affiliation>Amazon and University of Cambridge</affiliation></author>
      <author><first>Eva</first><last>Hasler</last><affiliation>Amazon</affiliation></author>
      <pages>3385-3403</pages>
      <abstract>Recent research has shown that large language models (LLMs) can achieve remarkable translation performance through supervised fine-tuning (SFT) using only a small amount of parallel data. However, SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references. Hence, the assistance from SFT often reaches a plateau once the LLMs have achieved a certain level of translation capability, and further increasing the size of parallel data does not provide additional benefits. To overcome this plateau associated with imitation-based SFT, we propose a preference-based approach built upon the Plackett-Luce model. The objective is to steer LLMs towards a more nuanced understanding of translation preferences from a holistic view, while also being more resilient in the absence of gold translations. We further build a dataset named MAPLE to verify the effectiveness of our approach, which includes multiple translations of varying quality for each source sentence. Extensive experiments demonstrate the superiority of our approach in “breaking the plateau” across diverse LLMs and test settings. Our in-depth analysis underscores the pivotal role of diverse translations and accurate preference scores in the success of our approach.</abstract>
      <url hash="afe9599a">2024.naacl-long.186</url>
    </paper>
    <paper id="187">
      <title>Fair Abstractive Summarization of Diverse Perspectives</title>
      <author><first>Yusen</first><last>Zhang</last></author>
      <author><first>Nan</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Alexander</first><last>Fabbri</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Junru</first><last>Liu</last></author>
      <author><first>Ryo</first><last>Kamoi</last></author>
      <author><first>Xiaoxin</first><last>Lu</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Jieyu</first><last>Zhao</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Dragomir</first><last>Radev</last><affiliation>Yale University</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>3404-3426</pages>
      <abstract>People from different social and demographic groups express diverse perspectives and conflicting opinions on a broad set of topics such as product reviews, healthcare, law, and politics. A fair summary should provide a comprehensive coverage of diverse perspectives without underrepresenting certain groups. However, current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization. In this paper, we systematically investigate fair abstractive summarization for user-generated data. We first formally define fairness in abstractive summarization as not underrepresenting perspectives of any groups of people, and we propose four reference-free automatic metrics by measuring the differences between target and source perspectives. We evaluate nine LLMs, including three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected from social media, online reviews, and recorded transcripts. Experiments show that both the model-generated and the human-written reference summaries suffer from low fairness. We conduct a comprehensive analysis of the common factors influencing fairness and propose three simple but effective methods to alleviate unfair summarization. Our dataset and code are available at https://github.com/psunlpgroup/FairSumm.</abstract>
      <url hash="2481f327">2024.naacl-long.187</url>
    </paper>
    <paper id="188">
      <title>What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases</title>
      <author><first>Anthony</first><last>Tiong</last><affiliation>Nanyang Technological University and SalesForce.com</affiliation></author>
      <author><first>Junqi</first><last>Zhao</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Boyang</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Junnan</first><last>Li</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Steven</first><last>Hoi</last><affiliation>Salesforce Research Asia and Singapore Management University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <pages>3427-3454</pages>
      <abstract>Vision-language (VL) models, pretrained on colossal image-text datasets, have attained broad VL competence that is difficult to evaluate. A common belief is that a small number of VL skills underlie the variety of VL tests. In this paper, we perform a large-scale transfer learning experiment aimed at discovering latent VL skills from data. We reveal interesting characteristics that have important implications for test suite design. First, generation tasks suffer from a length bias, suggesting benchmarks should balance tasks with varying output lengths. Second, we demonstrate that factor analysis successfully identifies reasonable yet surprising VL skill factors, suggesting benchmarks could leverage similar analyses for task selection.Finally, we present a new dataset, OLIVE<tex-math>^1</tex-math>, which simulates user instructions in the wild and presents challenges dissimilar to all datasets we tested. Our findings contribute to the design of balanced and broad-coverage vision-language evaluation methods. <tex-math>^1</tex-math>https://github.com/jq-zh/olive-dataset</abstract>
      <url hash="801d2e39">2024.naacl-long.188</url>
    </paper>
    <paper id="189">
      <title>Show Your Work with Confidence: Confidence Bands for Tuning Curves</title>
      <author><first>Nicholas</first><last>Lourie</last><affiliation>New York University</affiliation></author>
      <author><first>Kyunghyun</first><last>Cho</last><affiliation>Genentech and New York University</affiliation></author>
      <author><first>He</first><last>He</last><affiliation>New York University</affiliation></author>
      <pages>3455-3472</pages>
      <abstract>The choice of hyperparameters greatly impacts performance in natural language processing. Often, it is hard to tell if a method is better than another or just better tuned. *Tuning curves* fix this ambiguity by accounting for tuning effort. Specifically, they plot validation performance as a function of the number of hyperparameter choices tried so far. While several estimators exist for these curves, it is common to use point estimates, which we show fail silently and give contradictory results when given too little data.Beyond point estimates, *confidence bands* are necessary to rigorously establish the relationship between different approaches. We present the first method to construct valid confidence bands for tuning curves. The bands are exact, simultaneous, and distribution-free, thus they provide a robust basis for comparing methods.Empirical analysis shows that while bootstrap confidence bands, which serve as a baseline, fail to approximate their target confidence, ours achieve it exactly. We validate our design with ablations, analyze the effect of sample size, and provide guidance on comparing models with our method. To promote confident comparisons in future work, we release opda: an easy-to-use library that you can install with pip. https://github.com/nicholaslourie/opda</abstract>
      <url hash="8ad6358a">2024.naacl-long.189</url>
    </paper>
    <paper id="190">
      <title><fixed-case>GRASP</fixed-case>: A Disagreement Analysis Framework to Assess Group Associations in Perspectives</title>
      <author><first>Vinodkumar</first><last>Prabhakaran</last><affiliation>Google</affiliation></author>
      <author><first>Christopher</first><last>Homan</last></author>
      <author><first>Lora</first><last>Aroyo</last><affiliation>Google</affiliation></author>
      <author><first>Aida</first><last>Mostafazadeh Davani</last><affiliation>Research, Google</affiliation></author>
      <author><first>Alicia</first><last>Parrish</last><affiliation>Google</affiliation></author>
      <author><first>Alex</first><last>Taylor</last><affiliation>Design Informatics, University of Edinburgh</affiliation></author>
      <author><first>Mark</first><last>Diaz</last><affiliation>Google</affiliation></author>
      <author><first>Ding</first><last>Wang</last></author>
      <author><first>Gregory</first><last>Serapio-García</last></author>
      <pages>3473-3492</pages>
      <abstract>Human annotation plays a core role in machine learning — annotations for supervised models, safety guardrails for generative models, and human feedback for reinforcement learning, to cite a few avenues. However, the fact that many of these human annotations are inherently subjective is often overlooked. Recent work has demonstrated that ignoring rater subjectivity (typically resulting in rater disagreement) is problematic within specific tasks and for specific subgroups. Generalizable methods to harness rater disagreement and thus understand the socio-cultural leanings of subjective tasks remain elusive. In this paper, we propose GRASP, a comprehensive disagreement analysis framework to measure group association in perspectives among different rater subgroups, and demonstrate its utility in assessing the extent of systematic disagreements in two datasets: (1) safety annotations of human-chatbot conversations, and (2) offensiveness annotations of social media posts, both annotated by diverse rater pools across different socio-demographic axes. Our framework (based on disagreement metrics) reveals specific rater groups that have significantly different perspectives than others on certain tasks, and helps identify demographic axes that are crucial to consider in specific task contexts.</abstract>
      <url hash="d789c763">2024.naacl-long.190</url>
    </paper>
    <paper id="191">
      <title>Event Causality Is Key to Computational Story Understanding</title>
      <author><first>Yidan</first><last>Sun</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Qin</first><last>Chao</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Boyang</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>3493-3511</pages>
      <abstract>Cognitive science and symbolic AI research suggest that event causality provides vital information for story understanding. However, machine learning systems for story understanding rarely employ event causality, partially due to the lack of methods that reliably identify open-world causal event relations. Leveraging recent progress in large language models, we present the first method for event causality identification that leads to material improvements in computational story understanding. Our technique sets a new state of the art on the COPES dataset (Wang et al., 2023c) for causal event relation identification. Further, in the downstream story quality evaluation task, the identified causal relations lead to 3.6-16.6% relative improvement on correlation with human ratings. In the multimodal story video-text alignment task, we attain 4.1-10.9% increase on Clip Accuracy and 4.2-13.5% increase on Sentence IoU. The findings indicate substantial untapped potential for event causality in computational story understanding. The codebase is at https://github.com/insundaycathy/Event-Causality-Extraction.</abstract>
      <url hash="de290625">2024.naacl-long.191</url>
    </paper>
    <paper id="192">
      <title>Subspace Representations for Soft Set Operations and Sentence Similarities</title>
      <author><first>Yoichi</first><last>Ishibashi</last><affiliation>Kyoto University, Japan</affiliation></author>
      <author><first>Sho</first><last>Yokoi</last><affiliation>Tohoku University and RIKEN</affiliation></author>
      <author><first>Katsuhito</first><last>Sudoh</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Satoshi</first><last>Nakamura</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>3512-3524</pages>
      <abstract>In the field of natural language processing (NLP), continuous vector representations are crucial for capturing the semantic meanings of individual words. Yet, when it comes to the representations of sets of words, the conventional vector-based approaches often struggle with expressiveness and lack the essential set operations such as union, intersection, and complement. Inspired by quantum logic, we realize the representation of word sets and corresponding set operations within pre-trained word embedding spaces. By grounding our approach in the linear subspaces, we enable efficient computation of various set operations and facilitate the soft computation of membership functions within continuous spaces. Moreover, we allow for the computation of the F-score directly within word vectors, thereby establishing a direct link to the assessment of sentence similarity. In experiments with widely-used pre-trained embeddings and benchmarks, we show that our subspace-based set operations consistently outperform vector-based ones in both sentence similarity and set retrieval tasks.</abstract>
      <url hash="90dc0dfb">2024.naacl-long.192</url>
    </paper>
    <paper id="193">
      <title>My Heart Skipped a Beat! Recognizing Expressions of Embodied Emotion in Natural Language</title>
      <author><first>Yuan</first><last>Zhuang</last></author>
      <author><first>Tianyu</first><last>Jiang</last><affiliation>University of Cincinnati</affiliation></author>
      <author><first>Ellen</first><last>Riloff</last><affiliation>University of Arizona</affiliation></author>
      <pages>3525-3537</pages>
      <abstract>Humans frequently experience emotions. When emotions arise, they affect not only our mental state but can also change our physical state. For example, we often open our eyes wide when we are surprised, or clap our hands when we feel excited. Physical manifestations of emotions are referred to as embodied emotion in the psychology literature. From an NLP perspective, recognizing descriptions of physical movements or physiological responses associated with emotions is a type of implicit emotion recognition. Our work introduces a new task of recognizing expressions of embodied emotion in natural language. We create a dataset of sentences that contains 7,300 body part mentions with human annotations for embodied emotion. We develop a classification model for this task and present two methods to acquire weakly labeled instances of embodied emotion by extracting emotional manner expressions and by prompting a language model. Our experiments show that the weakly labeled data can train an effective classification model without gold data, and can also improve performance when combined with gold data. Our dataset is publicly available at https://github.com/yyzhuang1991/Embodied-Emotions.</abstract>
      <url hash="17ece23f">2024.naacl-long.193</url>
    </paper>
    <paper id="194">
      <title>Low-Cost Generation and Evaluation of Dictionary Example Sentences</title>
      <author><first>Bill</first><last>Cai</last><affiliation>Amazon</affiliation></author>
      <author><first>Ng</first><last>Clarence</last><affiliation>Ministry of Education, Singapore</affiliation></author>
      <author><first>Daniel</first><last>Liang</last></author>
      <author><first>Shelvia</first><last>Hotama</last></author>
      <pages>3538-3549</pages>
      <abstract>Dictionary example sentences play an important role in illustrating word definitions and usage, but manually creating quality sentences is challenging. Prior works have demonstrated that language models can be trained to generate example sentences. However, they relied on costly customized models and word sense datasets for generation and evaluation of their work. Rapid advancements in foundational models present the opportunity to create low-cost, zero-shot methods for the generation and evaluation of dictionary example sentences. We introduce a new automatic evaluation metric called OxfordEval that measures the win-rate of generated sentences against existing Oxford Dictionary sentences. OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation. We experiment with various LLMs and configurations to generate dictionary sentences across word classes. We complement this with a novel approach of using masked language models to identify and select sentences that best exemplify word meaning. The eventual model, FM-MLM, achieves over 85.1% win rate against Oxford baseline sentences according to OxfordEval, compared to 39.8% win rate for prior model-generated sentences.</abstract>
      <url hash="00840b6b">2024.naacl-long.194</url>
    </paper>
    <paper id="195">
      <title>Making Language Models Better Tool Learners with Execution Feedback</title>
      <author><first>Shuofei</first><last>Qiao</last></author>
      <author><first>Honghao</first><last>Gui</last></author>
      <author><first>Chengfei</first><last>Lv</last></author>
      <author><first>Qianghuai</first><last>Jia</last></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3550-3568</pages>
      <abstract>Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed by further analysis, show that TRICE can make the large language model selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools.</abstract>
      <url hash="66be656b">2024.naacl-long.195</url>
    </paper>
    <paper id="196">
      <title>Complex Claim Verification with Evidence Retrieved in the Wild</title>
      <author><first>Jifan</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Grace</first><last>Kim</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Aniruddh</first><last>Sriram</last></author>
      <author><first>Greg</first><last>Durrett</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>University of Texas, Austin</affiliation></author>
      <pages>3569-3587</pages>
      <abstract>Retrieving evidence to support or refute claims is a core part of automatic fact-checking. Prior work makes simplifying assumptions in retrieval that depart from real-world use cases: either no access to evidence, access to evidence curated by a human fact-checker, or access to evidence published after a claim was made. In this work, we present the first realistic pipeline to check real-world claims by retrieving raw evidence from the web. We restrict our retriever to only search documents available prior to the claim’s making, modeling the realistic scenario of emerging claims. Our pipeline includes five components: claim decomposition, raw document retrieval, fine-grained evidence retrieval, claim-focused summarization, and veracity judgment. We conduct experiments on complex political claims in the ClaimDecomp dataset and show that the aggregated evidence produced by our pipeline improves veracity judgments. Human evaluation finds the evidence summary produced by our system is reliable (it does not hallucinate information) and relevant to answering key questions about a claim, suggesting that it can assist fact-checkers even when it does not reflect a complete evidence set.</abstract>
      <url hash="2b4f3657">2024.naacl-long.196</url>
    </paper>
    <paper id="197">
      <title>Multimodal Multi-loss Fusion Network for Sentiment Analysis</title>
      <author><first>Zehui</first><last>Wu</last></author>
      <author><first>Ziwei</first><last>Gong</last><affiliation>Columbia University</affiliation></author>
      <author><first>Jaywon</first><last>Koo</last><affiliation>Rice University</affiliation></author>
      <author><first>Julia</first><last>Hirschberg</last><affiliation>Columbia University</affiliation></author>
      <pages>3588-3602</pages>
      <abstract>This paper investigates the optimal selection and fusion of feature encoders across multiple modalities and combines these in one neural network to improve sentiment detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying surprisingly important findings relating to subnet performance. We have also found that integrating context significantly enhances model performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS). These results suggest a roadmap toward an optimized feature selection and fusion approach for enhancing sentiment detection in neural networks.</abstract>
      <url hash="666744aa">2024.naacl-long.197</url>
    </paper>
    <paper id="198">
      <title>Confronting <fixed-case>LLM</fixed-case>s with Traditional <fixed-case>ML</fixed-case>: Rethinking the Fairness of Large Language Models in Tabular Classifications</title>
      <author><first>Yanchen</first><last>Liu</last></author>
      <author><first>Srishti</first><last>Gautam</last><affiliation>UiT The Arctic University of Norway</affiliation></author>
      <author><first>Jiaqi</first><last>Ma</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Himabindu</first><last>Lakkaraju</last><affiliation>Harvard University</affiliation></author>
      <pages>3603-3620</pages>
      <abstract>Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias mitigation, though in-context learning and finetuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pretraining corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.</abstract>
      <url hash="330f4cef">2024.naacl-long.198</url>
    </paper>
    <paper id="199">
      <title>Analyzing the Use of Metaphors in News Editorials for Political Framing</title>
      <author><first>Meghdut</first><last>Sengupta</last><affiliation>Universität Hannover</affiliation></author>
      <author><first>Roxanne</first><last>El Baff</last><affiliation>German Aerospace Center and Bauhaus-University Weimar</affiliation></author>
      <author><first>Milad</first><last>Alshomary</last><affiliation>Columbia University</affiliation></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <pages>3621-3631</pages>
      <abstract>Metaphorical language is a pivotal element inthe realm of political framing. Existing workfrom linguistics and the social sciences providescompelling evidence regarding the distinctivenessof conceptual framing for politicalideology perspectives. However, the nature andutilization of metaphors and the effect on audiencesof different political ideologies withinpolitical discourses are hardly explored. Toenable research in this direction, in this workwe create a dataset, originally based on newseditorials and labeled with their persuasive effectson liberals and conservatives and extend itwith annotations pertaining to metaphorical usageof language. To that end, first, we identifyall single metaphors and composite metaphors.Secondly, we provide annotations of the sourceand target domains for each metaphor. As aresult, our corpus consists of 300 news editorialsannotated with spans of texts containingmetaphors and the corresponding domains ofwhich these metaphors draw from. Our analysisshows that liberal readers are affected bymetaphors, whereas conservatives are resistantto them. Both ideologies are affected differentlybased on the metaphor source and targetcategory. For example, liberals are affected bymetaphors in the Darkness &amp; Light (e.g., death)source domains, where as the source domain ofNature affects conservatives more significantly.</abstract>
      <url hash="f6338f93">2024.naacl-long.199</url>
    </paper>
    <paper id="200">
      <title><fixed-case>S</fixed-case>harp<fixed-case>S</fixed-case>eq: Empowering Continual Event Detection through Sharpness-Aware Sequential-task Learning</title>
      <author><first>Thanh-Thien</first><last>Le</last></author>
      <author><first>Viet</first><last>Dao</last></author>
      <author><first>Linh</first><last>Nguyen</last></author>
      <author><first>Thi-Nhung</first><last>Nguyen</last><affiliation>VinAI Research</affiliation></author>
      <author><first>Linh</first><last>Ngo</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien</first><last>Nguyen</last><affiliation>, University of Oregon</affiliation></author>
      <pages>3632-3644</pages>
      <abstract>Continual event detection is a cornerstone in uncovering valuable patterns in many dynamic practical applications, where novel events emerge daily. Existing state-of-the-art approaches with replay buffers still suffer from catastrophic forgetting, partially due to overly simplistic objective aggregation. This oversight disregards complex trade-offs and leads to sub-optimal gradient updates, resulting in performance deterioration across objectives. While there are successful, widely cited multi-objective optimization frameworks for multi-task learning, they lack mechanisms to address data imbalance and evaluate whether a Pareto-optimal solution can effectively mitigate catastrophic forgetting, rendering them unsuitable for direct application to continual learning. To address these challenges, we propose **SharpSeq**, a novel continual learning paradigm leveraging sharpness-aware minimization combined with a generative model to balance training data distribution. Through extensive experiments on multiple real-world datasets, we demonstrate the superior performance of SharpSeq in continual event detection, proving the importance of our approach in mitigating catastrophic forgetting in continual event detection.</abstract>
      <url hash="00438b3f">2024.naacl-long.200</url>
    </paper>
    <paper id="201">
      <title>Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models</title>
      <author><first>Stephan</first><last>Linzbach</last><affiliation>GESIS - Leibniz Insitute for the Social Sciences</affiliation></author>
      <author><first>Dimitar</first><last>Dimitrov</last></author>
      <author><first>Laura</first><last>Kallmeyer</last><affiliation>Heinrich Heine University Düsseldorf, Germany</affiliation></author>
      <author><first>Kilian</first><last>Evang</last><affiliation>Heinrich Heine University Düsseldorf</affiliation></author>
      <author><first>Hajira</first><last>Jabeen</last><affiliation>University of Cologne</affiliation></author>
      <author><first>Stefan</first><last>Dietze</last><affiliation>GESIS and Heinrich-Heine-University Düsseldorf</affiliation></author>
      <pages>3645-3655</pages>
      <abstract>Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge.One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects orobjects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARE-LAMA – a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations.CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of PLMs. Extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.</abstract>
      <url hash="21a5c0f3">2024.naacl-long.201</url>
    </paper>
    <paper id="202">
      <title>Know When To Stop: A Study of Semantic Drift in Text Generation</title>
      <author><first>Ava</first><last>Spataru</last><affiliation>Meta</affiliation></author>
      <pages>3656-3671</pages>
      <abstract>In this work, we explicitly show that modern LLMs tend to generate correct facts first, then “drift away” and generate incorrect facts later: this was occasionally observed but never properly measured. We develop a semantic drift score that measures the degree of separation between correct and incorrect facts in generated texts and confirm our hypothesis when generating Wikipedia-style biographies. This correct-then-incorrect generation pattern suggests that factual accuracy can be improved by knowing when to stop generation. Therefore, we explore the trade-off between information quantity and factual accuracy for several early stopping methods and manage to improve factuality by a large margin. We further show that reranking with semantic similarity can further improve these results, both compared to the baseline and when combined with early stopping. Finally, we try calling external API to bring the model back to the right generation path, but do not get positive results. Overall, our methods generalize and can be applied to any long-form text generation to produce more reliable information, by balancing trade-offs between factual accuracy, information quantity and computational cost.</abstract>
      <url hash="dc917d5b">2024.naacl-long.202</url>
    </paper>
    <paper id="203">
      <title>Curriculum Masking in Vision-Language Pretraining to Maximize Cross Modal Interaction</title>
      <author><first>Kraig</first><last>Tou</last></author>
      <author><first>Zijun</first><last>Sun</last></author>
      <pages>3672-3688</pages>
      <abstract>Many leading methods in Vision and language (V+L) pretraining utilize masked language modeling (MLM) as a standard pretraining component, with the expectation that reconstruction of masked text tokens would necessitate reference to corresponding image context via cross/self attention and thus promote representation fusion. However, we observe that the minimization of MLM loss in earlier training stages can depend disproportionately on local text signals, leading to poor training efficiency and inconsistency with the goal of representation fusion. The extent of this lack of cross modal interaction depends strongly which token(s) are masked. To address this issue, we propose a curriculum masking scheme as a replacement for random masking. Tokens are selected to be masked at a frequency proportional to the expected level of cross modal interaction necessary to reconstruct them. This is achieved using a parallel mask selection agent that measures the cross modal flow of information and treats it as a reward to be maximized. By additionally masking contiguous spans that include key objects and their relations, we also achieve better relational understanding, which has been shown to be lacking in many SOTA models. Our experiments on a wide range of V+L tasks show that we trail closely behind state-of-the-art methods despite pretraining on 300x to 1000x less data and we also achieve either top or runner-up performance on tasks from the ARO benchmark which tests compositional relationships. Finally, we demonstrate the potential of our method to scale to larger pretraining data.</abstract>
      <url hash="9c2fa182">2024.naacl-long.203</url>
    </paper>
    <paper id="204">
      <title>Elote, Choclo and Mazorca: on the Varieties of <fixed-case>S</fixed-case>panish</title>
      <author><first>Cristina</first><last>España-Bonet</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last><affiliation>Università di Bologna</affiliation></author>
      <pages>3689-3711</pages>
      <abstract>Spanish is one of the most widespread languages: the official language in 20 countries and the second most-spoken native language. Its contact with other languages across different regions and the rich regional and cultural diversity has produced varieties which divert from each other, particularly in terms of lexicon. Still, available corpora, and models trained upon them, generally treat Spanish as one monolithic language, which dampers prediction and generation power when dealing with different varieties. To alleviate the situation, we compile and curate datasets in the different varieties of Spanish around the world at an unprecedented scale and create the CEREAL corpus. With such a resource at hand, we perform a stylistic analysis to identify and characterise varietal differences. We implement a classifier specially designed to deal with long documents and identify Spanish varieties (and therefore expand CEREAL further). We produce varietal-specific embeddings, and analyse the cultural differences that they encode. We make data, code and models publicly available.</abstract>
      <url hash="5cbe35a0">2024.naacl-long.204</url>
    </paper>
    <paper id="205">
      <title><fixed-case>A</fixed-case>da-<fixed-case>LE</fixed-case>val: Evaluating long-context <fixed-case>LLM</fixed-case>s with length-adaptable benchmarks</title>
      <author><first>Chonghua</first><last>Wang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Haodong</first><last>Duan</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>3712-3724</pages>
      <abstract>Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs’ capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models’ long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs’ long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.</abstract>
      <url hash="ba5a8195">2024.naacl-long.205</url>
    </paper>
    <paper id="206">
      <title>A Zero-Shot Monolingual Dual Stage Information Retrieval System for <fixed-case>S</fixed-case>panish Biomedical Systematic Literature Reviews</title>
      <author><first>Regina</first><last>Ofori-Boateng</last></author>
      <author><first>Magaly</first><last>Aceves-Martins</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Nirmalie</first><last>Wiratunga</last><affiliation>The Robert Gordon University</affiliation></author>
      <author><first>Carlos</first><last>Moreno-Garcia</last><affiliation>The Robert Gordon University</affiliation></author>
      <pages>3725-3736</pages>
      <abstract>Systematic Reviews (SRs) are foundational in healthcare for synthesising evidence to inform clinical practices. Traditionally skewed towards English-language databases, SRs often exclude significant research in other languages, leading to potential biases. This study addresses this gap by focusing on Spanish, a language notably underrepresented in SRs. We present a foundational zero-shot dual information retrieval (IR) baseline system, integrating traditional retrieval methods with pre-trained language models and cross-attention re-rankers for enhanced accuracy in Spanish biomedical literature retrieval. Utilising the LILACS database, known for its comprehensive coverage of Latin American and Caribbean biomedical literature, we evaluate the approach with three real-life case studies in Spanish SRs. The findings demonstrate the system’s efficacy and underscore the importance of query formulation. This study contributes to the field of IR by promoting language inclusivity and supports the development of more comprehensive and globally representative healthcare guidelines.</abstract>
      <url hash="3af81883">2024.naacl-long.206</url>
    </paper>
    <paper id="207">
      <title><fixed-case>L</fixed-case>ayout<fixed-case>P</fixed-case>ointer: A Spatial-Context Adaptive Pointer Network for Visual Information Extraction</title>
      <author><first>Huang</first><last>Siyuan</last></author>
      <author><first>Yongping</first><last>Xiong</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Wu</first><last>Guibin</last><affiliation>Chizhou University</affiliation></author>
      <pages>3737-3748</pages>
      <abstract>Visual Information Extraction (VIE), as a crucial task of Document Intelligence, involves two primary sub-tasks: Semantic Entity Recognition (SER) and Relation Extraction (RE). However, VIE faces two significant challenges. Firstly, most existing models inadequately utilize spatial information of entities, often failing to predict connections or incorrectly linking spatially distant entities. Secondly, the improper input order of tokens challenges in extracting complete entity pairs from documents with multi-line entities when text is extracted via PDF parser or OCR. To address these challenges, we propose LayoutPointer, a Spatial-Context Adaptive Pointer Network. LayoutPointer explicitly enhances spatial-context relationships by incorporating 2D relative position information and adaptive spatial constraints within self-attention. Furthermore, we recast the RE task as a specialized cycle detection problem, employing a unique tail-to-head pointer to restore the semantic order among multi-line entities. To better evaluate the effectiveness of our proposed method, we reconstruct a multi-line dataset named MLFUD, which more accurately reflects real-world scenarios. Fine-tuning experimental results on FUNSD, XFUND, and MLFUD datasets demonstrate that LayoutPointer significantly outperforms existing state-of-the-art methods in F1 scores for RE tasks (e.g., 5.71% improvement on XFUND using LayoutPointer<tex-math>_{\text{BASE-X}}</tex-math> over LayoutLMv3).</abstract>
      <url hash="ea8dc0b4">2024.naacl-long.207</url>
    </paper>
    <paper id="208">
      <title>Long-form evaluation of model editing</title>
      <author><first>Domenic</first><last>Rosati</last><affiliation>Dalhousie University and scite.ai</affiliation></author>
      <author><first>Robie</first><last>Gonzales</last><affiliation>Dalhousie University</affiliation></author>
      <author><first>Jinkun</first><last>Chen</last><affiliation>Dalhousie University</affiliation></author>
      <author><first>Xuemin</first><last>Yu</last><affiliation>Dalhousie University</affiliation></author>
      <author><first>Yahya</first><last>Kayani</last></author>
      <author><first>Frank</first><last>Rudzicz</last><affiliation>Dalhousie University</affiliation></author>
      <author><first>Hassan</first><last>Sajjad</last><affiliation>Dalhousie University</affiliation></author>
      <pages>3749-3780</pages>
      <abstract>Evaluations of model editing, a technique for changing the factual knowledge held by Large Language Models (LLMs), currently only use the ‘next few token’ completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (<tex-math>\textbf{\textit{LEME}}</tex-math>) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods. Finally, we present a qualitative analysis that illustrates common failure modes in long-form generative settings including internal consistency, lexical cohesion, and locality issues.</abstract>
      <url hash="1bd1fbf2">2024.naacl-long.208</url>
    </paper>
    <paper id="209">
      <title>Analyzing the Role of Semantic Representations in the Era of Large Language Models</title>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Yuen</first><last>Chen</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Fernando</first><last>Gonzalez Adauto</last></author>
      <author><first>Jiarui</first><last>Liu</last></author>
      <author><first>Jiayi</first><last>Zhang</last></author>
      <author><first>Julian</first><last>Michael</last><affiliation>New York University</affiliation></author>
      <author><first>Bernhard</first><last>Schölkopf</last><affiliation>ELLIS Institute and Max Planck Institute for Intelligent Systems, Max-Planck Institute</affiliation></author>
      <author><first>Mona</first><last>Diab</last><affiliation>Carnegie Mellon University and George Washington University</affiliation></author>
      <pages>3781-3798</pages>
      <abstract>Traditionally, natural language processing (NLP) models often use a rich set of features created by linguistic expertise, such as semantic representations. However, in the era of large language models (LLMs), more and more tasks are turned into generic, end-to-end sequence generation problems. In this paper, we investigate the question: what is the role of semantic representations in the era of LLMs? Specifically, we investigate the effect of Abstract Meaning Representation (AMR) across five diverse NLP tasks. We propose an AMR-driven chain-of-thought prompting method, which we call AMRCOT, and find that it generally hurts performance more than it helps. To investigate what AMR may have to offer on these tasks, we conduct a series of analysis experiments. We find that it is difficult to predict which input examples AMR may help or hurt on, but errors tend to arise with multi-word expressions, named entities, and in the final inference step where the LLM must connect its reasoning over the AMR to its prediction. We recommend focusing on these areas for future work in semantic representations for LLMs. Our code: https://github.com/causalNLP/amr_llm</abstract>
      <url hash="6af51146">2024.naacl-long.209</url>
    </paper>
    <paper id="210">
      <title><fixed-case>TRAQ</fixed-case>: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction</title>
      <author><first>Shuo</first><last>Li</last></author>
      <author><first>Sangdon</first><last>Park</last><affiliation>POSTECH</affiliation></author>
      <author><first>Insup</first><last>Lee</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Osbert</first><last>Bastani</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>3799-3821</pages>
      <abstract>When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called <i>hallucinations</i>. Retrieval augmented generation (RAG) is a promising strategy to avoid hallucinations, but it does not provide guarantees on its correctness. To address this challenge, we propose the Trustworthy Retrieval Augmented Question Answering, or *TRAQ*, which provides the first end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal prediction, a statistical technique for constructing prediction sets that are guaranteed to contain the semantically correct response with high probability. Additionally, TRAQ leverages Bayesian optimization to minimize the size of the constructed sets. In an extensive experimental evaluation, we demonstrate that TRAQ provides the desired correctness guarantee while reducing prediction set size by 16.2% on average compared to an ablation. The implementation is available: [https://github.com/shuoli90/TRAQ](https://github.com/shuoli90/TRAQ).</abstract>
      <url hash="aa359236">2024.naacl-long.210</url>
    </paper>
    <paper id="211">
      <title><fixed-case>M</fixed-case>ap<fixed-case>G</fixed-case>uide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities</title>
      <author><first>Xinpei</first><last>Zhao</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Jingyuan</first><last>Sun</last></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Jing</first><last>Ye</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xhz</first><last>Xhz</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>3822-3832</pages>
      <abstract>Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and highlight a critical correlation: the more precisely we map brain activities to text embeddings, the better the text reconstruction results. Such insight can simplify the task of reconstructing language from brain activities for future work, emphasizing the importance of improving brain-to-text-embedding mapping techniques.</abstract>
      <url hash="3347fb9c">2024.naacl-long.211</url>
    </paper>
    <paper id="212">
      <title>On-the-fly Definition Augmentation of <fixed-case>LLM</fixed-case>s for Biomedical <fixed-case>NER</fixed-case></title>
      <author><first>Monica</first><last>Munnangi</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Sergey</first><last>Feldman</last><affiliation>Allen Institute for Artificial Intelligence and Data Cowboys</affiliation></author>
      <author><first>Byron</first><last>Wallace</last><affiliation>Northeastern University, Brown University and Northeastern University</affiliation></author>
      <author><first>Silvio</first><last>Amir</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tom</first><last>Hope</last><affiliation>Allen Institute for Artificial Intelligence and Hebrew University, Hebrew University of Jerusalem</affiliation></author>
      <author><first>Aakanksha</first><last>Naik</last><affiliation>Allen Institute for Artificial Intelligence and National Institutes of Health</affiliation></author>
      <pages>3833-3854</pages>
      <abstract>Despite their general capabilities, LLMs still struggle on biomedicalNER tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve LLM performance on biomedical NER in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of prompting strategies. Our experiments show that definition augmentation is useful for both open source and closed LLMs.For example, it leads to a relative improvement of 15% (on average) in GPT-4 performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful prompting strategies also improve LLM performance, allowing them to outperform fine-tuned language models in few-shot settings. To facilitate future research in this direction, we release our code at https://github.com/allenai/beacon.</abstract>
      <url hash="3cc0f839">2024.naacl-long.212</url>
    </paper>
    <paper id="213">
      <title>This Land is <fixed-case>Your, My</fixed-case> Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes</title>
      <author><first>Bryan</first><last>Li</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Samar</first><last>Haider</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Chris</first><last>Callison-Burch</last><affiliation>Allen Institute for Artificial Intelligence and University of Pennsylvania</affiliation></author>
      <pages>3855-3871</pages>
      <abstract>Do the Spratly Islands belong to China, the Philippines, or Vietnam? A pretrained large language model (LLM) may answer differently if asked in the languages of each claimant country: Chinese, Tagalog, or Vietnamese. This contrasts with a multilingual human, who would likely answer consistently. In this paper, we show that LLMs recall certain geographical knowledge inconsistently when queried in different languages—a phenomenon we term geopolitical bias. As a targeted case study, we consider territorial disputes, an inherently controversial and multilingual task. We introduce BorderLines, a dataset of territorial disputes which covers 251 territories, each associated with a set of multiple-choice questions in the languages of each claimant country (49 languages in total). We also propose a suite of evaluation metrics to precisely quantify bias and consistency in responses across different languages. We then evaluate various multilingual LLMs on our dataset and metrics to probe their internal knowledge and use the proposed metrics to discover numerous inconsistencies in how these models respond in different languages. Finally, we explore several prompt modification strategies, aiming to either amplify or mitigate geopolitical bias, which highlights how brittle LLMs are and how they tailor their responses depending on cues from the interaction context. Our code and data are available at https://github.com/manestay/borderlines.</abstract>
      <url hash="13556455">2024.naacl-long.213</url>
    </paper>
    <paper id="214">
      <title>Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation</title>
      <author><first>Xingwei</first><last>Tan</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Yuxiang</first><last>Zhou</last><affiliation>King’s College London</affiliation></author>
      <author><first>Gabriele</first><last>Pergola</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>3872-3892</pages>
      <abstract>Event temporal graphs have been shown as convenient and effective representations of complex temporal relations between events in text. Recent studies, which employ pre-trained language models to auto-regressively generate linearised graphs for constructing event temporal graphs, have shown promising results. However, these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences. To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of Large Language Models (LLMs). The framework incorporates data augmentations and set-property regularisations designed to alleviate text generation loss penalties associated with the linearised graph edge sequences, thus encouraging the generation of more relation edges. Experimental results show that our framework surpasses existing baselines for event temporal graph generation. Furthermore, under zero-shot settings, the structural knowledge introduced through our framework notably improves model generalisation, particularly when the training examples available are limited.</abstract>
      <url hash="1d7bbd0e">2024.naacl-long.214</url>
    </paper>
    <paper id="215">
      <title><fixed-case>L</fixed-case>anguage<fixed-case>F</fixed-case>low: Advancing Diffusion Language Generation with Probabilistic Flows</title>
      <author><first>Shujian</first><last>Zhang</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Lemeng</first><last>Wu</last><affiliation>Facebook and University of Texas, Austin</affiliation></author>
      <author><first>Chengyue</first><last>Gong</last><affiliation>ut austin</affiliation></author>
      <author><first>Xingchao</first><last>Liu</last></author>
      <pages>3893-3905</pages>
      <abstract>Recent works have demonstrated success in controlling sentence attributes (e.g., sentiment) and structure (e.g., syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow (LF).Our method is based on the reformulation of the standard probabilistic flow models.Language rectified flow learns (neural) ordinary differentialequation models to transport between the source distribution and the target distribution, henceproviding a unified and effective solution to generative modeling and domain transfer.From the source distribution, our language rectified flow yields fast simulation and effectively decreases the inference time. Experiments on three challenging fine-grained control tasks and multiple high-quality text editing show that our method consistently outperforms its baselines. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks.</abstract>
      <url hash="f6e840ae">2024.naacl-long.215</url>
    </paper>
    <paper id="216">
      <title>Towards Improved Multi-Source Attribution for Long-Form Answer Generation</title>
      <author><first>Nilay</first><last>Patel</last></author>
      <author><first>Shivashankar</first><last>Subramanian</last><affiliation>Amazon</affiliation></author>
      <author><first>Siddhant</first><last>Garg</last><affiliation>Meta</affiliation></author>
      <author><first>Pratyay</first><last>Banerjee</last><affiliation>Amazon</affiliation></author>
      <author><first>Amita</first><last>Misra</last><affiliation>Amazon</affiliation></author>
      <pages>3906-3919</pages>
      <abstract>Teaching large language models (LLMs) to generate text with attribution to evidence sources can reduce hallucinations, improve verifiability in question answering systems (QA), and increase reliability of retrieval augmented LLMs. Despite gaining increasing popularity for usage in QA systems and search engines, current LLMs struggle with attribution for long-form responses which require reasoning over multiple evidence sources. To address this, in this paper we aim to improve the attribution capability of LLMs for long-form answer generation to multiple sources, with multiple citations per sentence. However, data for training multi-source attributable QA systems is difficult and expensive to annotate, and therefore scarce. To overcome this challenge, we transform existing QA datasets for this task (MultiAttr), and empirically demonstrate, on a wide range of attribution benchmark datasets, that fine-tuning on MultiAttr provides significant improvements over training only on the target QA domain. Lastly, to fill a gap in existing benchmarks, we present a multi-source attribution dataset containing multi-paragraph answers, PolitiICite, based on PolitiFact articles that discuss events closely related to implementation statuses of election promises.</abstract>
      <url hash="fe14d301">2024.naacl-long.216</url>
    </paper>
    <paper id="217">
      <title>Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models</title>
      <author><first>Aldo</first><last>Carranza</last></author>
      <author><first>Rezsa</first><last>Farahani</last></author>
      <author><first>Natalia</first><last>Ponomareva</last><affiliation>Google</affiliation></author>
      <author><first>Alexey</first><last>Kurakin</last><affiliation>Research, Google</affiliation></author>
      <author><first>Matthew</first><last>Jagielski</last><affiliation>Google</affiliation></author>
      <author><first>Milad</first><last>Nasr</last><affiliation>Google</affiliation></author>
      <pages>3920-3930</pages>
      <abstract>We address the challenge of ensuring differential privacy (DP) guarantees in training deep retrieval systems. Training these systems often involves the use of contrastive-style losses, which are typically non-per-example decomposable, making them difficult to directly DP-train with since common techniques require per-example gradients. To address this issue, we propose an approach that prioritizes ensuring query privacy prior to training a deep retrieval system. Our method employs DP language models (LMs) to generate private synthetic queries representative of the original data. These synthetic queries can be used in downstream retrieval system training without compromising privacy. Our approach demonstrates a significant enhancement in retrieval quality compared to direct DP-training, all while maintaining query-level privacy guarantees. This work highlights the potential of harnessing LMs to overcome limitations in standard DP-training methods.</abstract>
      <url hash="111f199f">2024.naacl-long.217</url>
    </paper>
    <paper id="218">
      <title>Okay, Let’s Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation</title>
      <author><first>Abhijnan</first><last>Nath</last></author>
      <author><first>Shadi</first><last>Manafi Avari</last><affiliation>Colorado State University</affiliation></author>
      <author><first>Avyakta</first><last>Chelle</last></author>
      <author><first>Nikhil</first><last>Krishnaswamy</last><affiliation>Colorado State University</affiliation></author>
      <pages>3931-3946</pages>
      <abstract>In NLP, Event Coreference Resolution (ECR) is the task of connecting event clusters that refer to the same underlying real-life event, usually via neural systems. In this work, we investigate using abductive free-text rationales (FTRs) generated by modern autoregressive LLMs as distant supervision of smaller student models for cross-document coreference (CDCR) of events. We implement novel rationale-oriented event clustering and knowledge distillation methods for event coreference scoring that leverage enriched information from the FTRs for improved CDCR without additional annotation or expensive document clustering. Our model using coreference-specific knowledge distillation achieves SOTA <tex-math>B^3</tex-math> <tex-math>F_1</tex-math> on the ECB+ and GVC corpora and we establish a new baseline on the AIDA Phase 1 corpus. Our code can be found at https://github.com/csu-signal/llama_cdcr.</abstract>
      <url hash="ff4cb404">2024.naacl-long.218</url>
    </paper>
    <paper id="219">
      <title>Can Knowledge Graphs Reduce Hallucinations in <fixed-case>LLM</fixed-case>s? : A Survey</title>
      <author><first>Garima</first><last>Agrawal</last></author>
      <author><first>Tharindu</first><last>Kumarage</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Zeyad</first><last>Alghamdi</last></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Arizona State University</affiliation></author>
      <pages>3947-3960</pages>
      <abstract>The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.</abstract>
      <url hash="74943b9a">2024.naacl-long.219</url>
    </paper>
    <paper id="220">
      <title>Pedagogically Aligned Objectives Create Reliable Automatic Cloze Tests</title>
      <author><first>Brian</first><last>Ondov</last><affiliation>Yale School of Medicine</affiliation></author>
      <author><first>Kush</first><last>Attal</last></author>
      <author><first>Dina</first><last>Demner-Fushman</last><affiliation>National Library of Medicine</affiliation></author>
      <pages>3961-3972</pages>
      <abstract>The cloze training objective of Masked Language Models makes them a natural choice for generating plausible distractors for human cloze questions. However, distractors must also be both distinct and incorrect, neither of which is directly addressed by existing neural methods. Evaluation of recent models has also relied largely on automated metrics, which cannot demonstrate the reliability or validity of human comprehension tests. In this work, we first formulate the pedagogically motivated objectives of plausibility, incorrectness, and distinctiveness in terms of conditional distributions from language models. Second, we present an unsupervised, interpretable method that uses these objectives to jointly optimize sets of distractors. Third, we test the reliability and validity of the resulting cloze tests compared to other methods with human participants. We find our method has stronger correlation with teacher-created comprehension tests than the state-of-the-art neural method and is more internally consistent. Our implementation is freely available and can quickly create a multiple choice cloze test from any given passage.</abstract>
      <url hash="ef984d27">2024.naacl-long.220</url>
    </paper>
    <paper id="221">
      <title>Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning</title>
      <author><first>Kazuma</first><last>Hashimoto</last><affiliation>Google Research</affiliation></author>
      <author><first>Karthik</first><last>Raman</last><affiliation>Google</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <pages>3973-3990</pages>
      <abstract>In-Context Learning (ICL) is an emergent capability of Large Language Models (LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new tasks. Previous studies have shown that using LLMs’ outputs as labels is effective in training models to select demonstrations. Such a label is expected to estimate utility of a demonstration in ICL; however, it has not been well understood how different labeling strategies affect results on target tasks. This paper presents an analysis on different utility functions by focusing on LLMs’ output probability given ground-truth output, and task-specific reward given LLMs’ prediction. Unlike the previous work, we introduce a novel labeling method, incremental utility, which estimates how much incremental knowledge is brought into the LLMs by a demonstration. We conduct experiments with instruction-tuned LLMs on binary/multi-class classification, segmentation, and translation across Arabic, English, Finnish, Japanese, and Spanish. Our results show that (1) the probability is effective when the probability values are distributed across the whole value range (on the classification tasks), and (2) the downstream metric is more robust when nuanced reward values are provided with long outputs (on the segmentation and translation tasks). We then show that the proposed incremental utility further helps ICL by contrasting how the LLMs perform with and without the demonstrations.</abstract>
      <url hash="b765f1e7">2024.naacl-long.221</url>
    </paper>
    <paper id="222">
      <title><fixed-case>LM</fixed-case>-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models</title>
      <author><first>Chi</first><last>Han</last></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Hao</first><last>Peng</last><affiliation>Department of Computer Science, University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Wenhan</first><last>Xiong</last><affiliation>Facebook</affiliation></author>
      <author><first>Yu</first><last>Chen</last><affiliation>Anytime.AI</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Sinong</first><last>Wang</last><affiliation>Facebook</affiliation></author>
      <pages>3991-4008</pages>
      <abstract>Today’s large language models (LLMs) typically train on short text segments (e.g., &lt;4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encod- ing scientific articles, code repositories, or long dialogues. Through both theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis reveals that commonly used techniques like using a sliding-window attention pattern or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs’ capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7× decoding speed up and 7.5× memory saving over the original model. Our code will be publicly available upon publication.</abstract>
      <url hash="1e847483">2024.naacl-long.222</url>
    </paper>
    <paper id="223">
      <title><fixed-case>CONSCENDI</fixed-case>: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants</title>
      <author><first>Albert</first><last>Sun</last></author>
      <author><first>Varun</first><last>Nair</last><affiliation>Curai Health</affiliation></author>
      <author><first>Elliot</first><last>Schumacher</last><affiliation>Curai Health</affiliation></author>
      <author><first>Anitha</first><last>Kannan</last><affiliation>Curai Health</affiliation></author>
      <pages>4009-4030</pages>
      <abstract>A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models (LLMs), such as GPT-4 (OpenAI, 2023). A major challenge in deploying LLM-based virtual conversational assistants in real world settings is ensuring they operate within what is admissible for the task. To overcome this challenge, the designers of these virtual assistants rely on an independent guardrail system that verifies the virtual assistant’s output aligns with the constraints required for the task. However, relying on commonly used, prompt-based guardrails can be difficult to engineer correctly and comprehensively. To address these challenges, we propose CONSCENDI. We use CONSCENDI to exhaustively generate training data with two key LLM-powered components: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set and provides chatbot designers greater control. To generate contrastive examples, we prompt the LLM to alter conversations with violations into acceptable conversations to enable fine-grained distinctions. We then use this data, generated by CONSCENDI, to train a smaller model. We find that CONSCENDI results in guardrail models that improve over baselines in multiple dialogue domains.</abstract>
      <url hash="a03ecc35">2024.naacl-long.223</url>
    </paper>
    <paper id="224">
      <title>Advancing Beyond Identification: Multi-bit Watermark for Large Language Models</title>
      <author><first>KiYoon</first><last>Yoo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Wonhyuk</first><last>Ahn</last><affiliation>NAVER WEBTOON Corp.</affiliation></author>
      <author><first>Nojun</first><last>Kwak</last><affiliation>Seoul National University</affiliation></author>
      <pages>4031-4055</pages>
      <abstract>We show the viability of tackling misuses of large language models beyond the identification of machine-generated text. While existing zero-bit watermark methods focus on detection only, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose Multi-bit Watermark via Position Allocation, embedding traceable multi-bit information during language model generation. Through allocating tokens onto different parts of the messages, we embed longer messages in high corruption settings without added latency. By independently embedding sub-units of messages, the proposed method outperforms the existing works in terms of robustness and latency. Leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages (<tex-math>\geq</tex-math> 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time.</abstract>
      <url hash="0cb202d4">2024.naacl-long.224</url>
    </paper>
    <paper id="225">
      <title><fixed-case>HTCCN</fixed-case>: Temporal Causal Convolutional Networks with <fixed-case>H</fixed-case>awkes Process for Extrapolation Reasoning in Temporal Knowledge Graphs</title>
      <author><first>Tingxuan</first><last>Chen</last><affiliation>Central South University</affiliation></author>
      <author><first>Jun</first><last>Long</last><affiliation>Central South University</affiliation></author>
      <author><first>Liu</first><last>Yang</last></author>
      <author><first>Zidong</first><last>Wang</last><affiliation>Central South University</affiliation></author>
      <author><first>Yongheng</first><last>Wang</last><affiliation>Zhejiang lab</affiliation></author>
      <author><first>Xiongnan</first><last>Jin</last><affiliation>Zhejiang Lab</affiliation></author>
      <pages>4056-4066</pages>
      <abstract>Temporal knowledge graphs (TKGs) serve as powerful tools for storing and modeling dynamic facts, holding immense potential in anticipating future facts. Since future facts are inherently unknowable, effectively modeling the intricate temporal structure of historical facts becomes paramount for accurate prediction. However, current models often rely heavily on fact recurrence or periodicity, leading to information loss due to prolonged evolutionary processes. Notably, the occurrence of one fact always influences the likelihood of another. To this end, we propose HTCCN, a novel Hawkes process-based temporal causal convolutional network designed for temporal reasoning under extrapolation settings. HTCCN employs a temporal causal convolutional network to model the historical interdependence of facts and leverages Hawkes to model link formation processes inductively in TKGs. Importantly, HTCCN introduces dual-level dynamics to comprehensively capture the temporal evolution of facts. Rigorous experimentation on four real-world datasets underscores the superior performance of HTCCN.</abstract>
      <url hash="5a62e130">2024.naacl-long.225</url>
    </paper>
    <paper id="226">
      <title><fixed-case>S</fixed-case>em<fixed-case>S</fixed-case>tamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation</title>
      <author><first>Abe</first><last>Hou</last></author>
      <author><first>Jingyu</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Tianxing</first><last>He</last></author>
      <author><first>Yichen</first><last>Wang</last></author>
      <author><first>Yung-Sung</first><last>Chuang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Hongwei</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Lingfeng</first><last>Shen</last></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>4067-4082</pages>
      <abstract>Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a “bigram paraphrase” attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation.</abstract>
      <url hash="8ee60e24">2024.naacl-long.226</url>
    </paper>
    <paper id="227">
      <title>Media Bias Detection Across Families of Language Models</title>
      <author><first>Iffat</first><last>Maab</last></author>
      <author><first>Edison</first><last>Marrese-Taylor</last><affiliation>The Univesity of Tokyo and AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Sebastian</first><last>Padó</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>4083-4098</pages>
      <abstract>Bias in reporting can influence the public’s opinion on relevant societal issues. Examples include informational bias (selective presentation of content) and lexical bias (specific framing of content through linguistic choices). The recognition of media bias is arguably an area where NLP can contribute to the “social good”. Traditional NLP models have shown good performance in classifying media bias, but require careful model design and extensive tuning. In this paper, we ask how well prompting of large language models can recognize media bias. Through an extensive empirical study including a wide selection of pre-trained models, we find that prompt-based techniques can deliver comparable performance to traditional models with greatly reduced effort and that, similar to traditional models, the availability of context substantially improves results. We further show that larger models can leverage different kinds of context simultaneously, obtaining further performance improvements.</abstract>
      <url hash="892c3552">2024.naacl-long.227</url>
    </paper>
    <paper id="228">
      <title>Better Zero-Shot Reasoning with Role-Play Prompting</title>
      <author><first>Aobo</first><last>Kong</last></author>
      <author><first>Shiwan</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Qicheng</first><last>Li</last><affiliation>Nankai University</affiliation></author>
      <author><first>Yong</first><last>Qin</last></author>
      <author><first>Ruiqi</first><last>Sun</last><affiliation>Lenovo Research</affiliation></author>
      <author><first>Xin</first><last>Zhou</last></author>
      <author><first>Enzhi</first><last>Wang</last></author>
      <author><first>Xiaohang</first><last>Dong</last></author>
      <pages>4099-4113</pages>
      <abstract>Modern large language models (LLMs) exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs’ reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks. Our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, in experiments conducted using ChatGPT, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%. Upon further comparison with the Zero-Shot-CoT technique, which prompts the model to “think step by step”, our study demonstrates that role-play prompting acts as a more effective trigger for the CoT process.This highlights its potential to augment the reasoning capabilities of LLMs. We release our code at https://github.com/NKU-HLT/Role-Play-Prompting.</abstract>
      <url hash="fab17b85">2024.naacl-long.228</url>
    </paper>
    <paper id="229">
      <title>Event-Content-Oriented Dialogue Generation in Short Video</title>
      <author><first>Fenghua</first><last>Cheng</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Xue</first><last>Li</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Zi</first><last>Huang</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Jinxiang</first><last>Wang</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Sen</first><last>Wang</last><affiliation>University of Queensland and The University of Queensland</affiliation></author>
      <pages>4114-4124</pages>
      <abstract>Understanding complex events from different modalities, associating to external knowledge and generating response in a clear point of view are still unexplored in today’s multi-modal dialogue research. The great challenges include 1) lack of event-based multi-modal dialogue dataset; 2) understanding of complex events and 3) heterogeneity gap between different modalities. To overcome these challenges, we firstly introduce a novel event-oriented video-dialogue dataset called SportsVD (Sports-domain Video-dialogue Dataset). To our best knowledge, SportsVD is the first dataset that consists of complex events videos and opinion-based conversations with regards to contents in these events. Meanwhile, we present multi-modal dialogue generation method VCD (Video Commentary Dialogue) to generate human-like response according to event contents in the video and related external knowledge. In contrast to previous video-based dialogue generation, we focus on opinion-based response and the understanding of longer and more complex event contents. We evaluate VCD’s performance on SportsVD and other baselines under several automatic metrics. Experiments demonstrate VCD can outperform among other state-of-the-art baselines. Our work is available at https://github.com/Cheng-Fenghua/SportsVD.</abstract>
      <url hash="47aff488">2024.naacl-long.229</url>
    </paper>
    <paper id="230">
      <title><fixed-case>D</fixed-case>o<fixed-case>G</fixed-case>-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded Instruction Wrapping</title>
      <author><first>Yongrui</first><last>Chen</last><affiliation>Southeast University</affiliation></author>
      <author><first>Haiyun</first><last>Jiang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xinting</first><last>Huang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <pages>4125-4135</pages>
      <abstract>The improvement of LLMs’ instruction-following capabilities relies heavily on the availability of high-quality instruction-response pairs. Unfortunately, the current methods used to collect the pairs suffer from either unaffordable labor costs or severe hallucinations in the self-generation of LLM.To tackle these challenges, this paper proposes a scalable solution.It involves training LLMs to generate instruction-response pairs based on human-written documents, rather than relying solely on self-generation without context.Our proposed method not only exploits the advantages of human-written documents in reducing hallucinations but also utilizes an LLM to wrap the expression of documents, which enables us to bridge the gap between various document styles and the standard AI response.Experiments demonstrate that our method outperforms existing typical methods on multiple benchmarks.In particular, compared to the best-performing baseline, the LLM trained using our generated dataset exhibits a 10% relative improvement in performance on AlpacaEval, despite utilizing only 1/5 of its training data.Furthermore, a comprehensive manual evaluation validates the quality of the data we generated.</abstract>
      <url hash="ace6ac01">2024.naacl-long.230</url>
    </paper>
    <paper id="231">
      <title>Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization</title>
      <author><first>Santosh</first><last>T.y.s.s</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Vatsal</first><last>Venkatkrishna</last></author>
      <author><first>Saptarshi</first><last>Ghosh</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Matthias</first><last>Grabmair</last><affiliation>Technische Universität München</affiliation></author>
      <pages>4136-4150</pages>
      <abstract>Legal professionals face the challenge of managing an overwhelming volume of lengthy judgments, making automated legal case summarization crucial. However, prior approaches mainly focused on training and evaluating these models within the same jurisdiction. In this study, we explore the cross-jurisdictional generalizability of legal case summarization models. Specifically, we explore how to effectively summarize legal cases of a target jurisdiction where reference summaries are not available. In particular, we investigate whether supplementing models with unlabeled target jurisdiction corpus and extractive silver summaries obtained from unsupervised algorithms on target data enhances transfer performance. Our comprehensive study on three datasets from different jurisdictions highlights the role of pre-training in improving transfer performance. We shed light on the pivotal influence of jurisdictional similarity in selecting optimal source datasets for effective transfer. Furthermore, our findings underscore that incorporating unlabeled target data yields improvements in general pre-trained models, with additional gains when silver summaries are introduced. This augmentation is especially valuable when dealing with extractive datasets and scenarios featuring limited alignment between source and target jurisdictions. Our study provides key insights for developing adaptable legal case summarization systems, transcending jurisdictional boundaries.</abstract>
      <url hash="34b48130">2024.naacl-long.231</url>
    </paper>
    <paper id="232">
      <title><fixed-case>EDC</fixed-case>: Effective and Efficient Dialog Comprehension For Dialog State Tracking</title>
      <author><first>Qifan</first><last>Lu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Bhaskar</first><last>Ramasubramanian</last><affiliation>Western Washington University</affiliation></author>
      <author><first>Radha</first><last>Poovendran</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>4151-4165</pages>
      <abstract>In Task-Oriented Dialog (TOD) systems, Dialog State Tracking (DST) structurally extracts information from user and system utterances, which can be further used for querying databases and forming responses to users. The two major categories of DST methods, sequential and independent methods, face trade-offs between accuracy and efficiency. To resolve this issue, we propose Effective and Efficient Dialog Comprehension (EDC), an alternative DST approach that leverages the tree structure of the dialog state. EDC predicts domains, slot names and slot values of the dialog state step-by-step for better accuracy, and efficiently encodes dialog contexts with causal attention patterns. We evaluate EDC on several popular TOD datasets and EDC is able to achieve state-of-the-art Joint Goal Accuracy (JGA). We also show theoretically and empirically that EDC is more efficient than model designs used by previous works.</abstract>
      <url hash="4367fd51">2024.naacl-long.232</url>
    </paper>
    <paper id="233">
      <title>Automatic Restoration of Diacritics for Speech Data Sets</title>
      <author><first>Sara</first><last>Shatnawi</last></author>
      <author><first>Sawsan</first><last>Alqahtani</last><affiliation>Princess Nourah Bint Abdulrahman University</affiliation></author>
      <author><first>Hanan</first><last>Aldarmaki</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>4166-4176</pages>
      <abstract>Automatic text-based diacritic restoration models generally have high diacritic error rates when applied to speech transcripts as a result of domain and style shifts in spoken language. In this work, we explore the possibility of improving the performance of automatic diacritic restoration when applied to speech data by utilizing parallel spoken utterances. In particular, we use the pre-trained Whisper ASR model fine-tuned on relatively small amounts of diacritized Arabic speech data to produce rough diacritized transcripts for the speech utterances, which we then use as an additional input for diacritic restoration models. The proposed framework consistently improves diacritic restoration performance compared to text-only baselines. Our results highlight the inadequacy of current text-based diacritic restoration models for speech data sets and provide a new baseline for speech-based diacritic restoration.</abstract>
      <url hash="7eac538c">2024.naacl-long.233</url>
    </paper>
    <paper id="234">
      <title><fixed-case>XNLI</fixed-case>eu: a dataset for cross-lingual <fixed-case>NLI</fixed-case> in <fixed-case>B</fixed-case>asque</title>
      <author><first>Maite</first><last>Heredia</last><affiliation>Universidad del País Vasco</affiliation></author>
      <author><first>Julen</first><last>Etxaniz</last><affiliation>HiTZ Center, University of the Basque Country (UPV/EHU)</affiliation></author>
      <author><first>Muitze</first><last>Zulaika</last><affiliation>Orai NLP Technologies</affiliation></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <author><first>Jeremy</first><last>Barnes</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Aitor</first><last>Soroa</last><affiliation>University of the Basque Country. UPV/EHU.</affiliation></author>
      <pages>4177-4188</pages>
      <abstract>XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.</abstract>
      <url hash="8f57a894">2024.naacl-long.234</url>
    </paper>
    <paper id="235">
      <title><fixed-case>MDR</fixed-case>: Model-Specific Demonstration Retrieval at Inference Time for In-Context Learning</title>
      <author><first>Huazheng</first><last>Wang</last></author>
      <author><first>Jinming</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Sun</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zixuan</first><last>Xia</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Daixuan</first><last>Cheng</last></author>
      <author><first>Jingyu</first><last>Wang</last><affiliation>Beijing University of Post and Telecommunication, Tsinghua University</affiliation></author>
      <author><first>Qi</first><last>Qi</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jianxin</first><last>Liao</last></author>
      <pages>4189-4204</pages>
      <abstract>Recently, retrieval-based in-context learning (ICL) methods for selecting demonstrations have been widely investigated. Existing methods train a dense retriever to retrieve the most appropriate demonstrations for a given test query, which improves ICL performance. However, we find that distinct LLMs exhibit different biases for “what is a good demonstration” since they possess differences in training data, model architectures and training methods. As a result, a demonstration suitable for one LLM may not be appropriate for others.Previous approaches ignore the model bias and fail to retrieve the most appropriate demonstrations for different inference LLMs, resulting in a degradation of ICL performance.To address this problem, we propose a simple yet effective metric to evaluate the appropriateness of demonstrations for a specific inference LLM. Furthermore, we introduce a Model-specific Demonstration Retrieval (MDR) method for ICL at inference time, which considers the biases of different LLMs. We test MDR on seen and unseen tasks with multi-scale inference LLMs, such as GPT-Neo-2.7B, LLaMA-7B and Vicuna-13B. Experiments on 23 datasets across 11 data domains highlight the remarkable effectiveness of MDR, showcasing improvements of up to 41.2% in comparison to methods that neglect model biases.</abstract>
      <url hash="73afec0d">2024.naacl-long.235</url>
    </paper>
    <paper id="236">
      <title>Exploring Cross-Cultural Differences in <fixed-case>E</fixed-case>nglish Hate Speech Annotations: From Dataset Construction to Analysis</title>
      <author><first>Nayeon</first><last>Lee</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Chani</first><last>Jung</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Junho</first><last>Myung</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Jiho</first><last>Jin</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Jose</first><last>Camacho-Collados</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Juho</first><last>Kim</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>4205-4224</pages>
      <abstract>***Warning**: this paper contains content that may be offensive or upsetting.*Most hate speech datasets neglect the cultural diversity within a single language, resulting in a critical shortcoming in hate speech detection. To address this, we introduce **CREHate**, a **CR**oss-cultural **E**nglish **Hate** speech dataset.To construct CREHate, we follow a two-step procedure: 1) cultural post collection and 2) cross-cultural annotation.We sample posts from the SBIC dataset, which predominantly represents North America, and collect posts from four geographically diverse English-speaking countries (Australia, United Kingdom, Singapore, and South Africa) using culturally hateful keywords we retrieve from our survey.Annotations are collected from the four countries plus the United States to establish representative labels for each country.Our analysis highlights statistically significant disparities across countries in hate speech annotations.Only 56.2% of the posts in CREHate achieve consensus among all countries, with the highest pairwise label difference rate of 26%.Qualitative analysis shows that label disagreement occurs mostly due to different interpretations of sarcasm and the personal bias of annotators on divisive topics.Lastly, we evaluate large language models (LLMs) under a zero-shot setting and show that current LLMs tend to show higher accuracies on Anglosphere country labels in CREHate.Our dataset and codes are available at: https://github.com/nlee0212/CREHate</abstract>
      <url hash="1248b081">2024.naacl-long.236</url>
    </paper>
    <paper id="237">
      <title>Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding</title>
      <author><first>Zheng</first><last>Zhao</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Emilio</first><last>Monti</last></author>
      <author><first>Jens</first><last>Lehmann</last><affiliation>Amazon, Technische Universität Dresden, University of Bonn and Fraunhofer IAIS</affiliation></author>
      <author><first>Haytham</first><last>Assem</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>4225-4237</pages>
      <abstract>Large language models (LLMs) tend to inadequately integrate input context during text generation, relying excessively on encoded prior knowledge in model parameters, potentially resulting in generated text with factual inconsistencies or contextually unfaithful content. LLMs utilize two primary knowledge sources: 1) prior (parametric) knowledge from pretraining, and 2) contextual (non-parametric) knowledge from input prompts. The study addresses the open question of how LLMs effectively balance these knowledge sources during the generation process, specifically in the context of open-domain question answering. To address this issue, we introduce a novel approach integrating contrastive decoding with adversarial irrelevant passages as negative samples to enhance robust context grounding during generation. Notably, our method operates at inference time without requiring further training. We conduct comprehensive experiments to demonstrate its applicability and effectiveness, providing empirical evidence showcasing its superiority over existing methodologies.</abstract>
      <url hash="4148c1fd">2024.naacl-long.237</url>
    </paper>
    <paper id="238">
      <title>Generalizable Sarcasm Detection is Just Around the Corner, of Course!</title>
      <author><first>Hyewon</first><last>Jang</last><affiliation>Universität Konstanz</affiliation></author>
      <author><first>Diego</first><last>Frassinelli</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <pages>4238-4249</pages>
      <abstract>We tested the robustness of sarcasm detection models by examining their behavior when fine-tuned on four sarcasm datasets containing varying characteristics of sarcasm: label source (authors vs. third-party), domain (social media/online vs. offline conversations/dialogues), style (aggressive vs. humorous mocking). We tested their prediction performance on the same dataset (intra-dataset) and across different datasets (cross-dataset). For intra-dataset predictions, models consistently performed better when fine-tuned with third-party labels rather than with author labels. For cross-dataset predictions, most models failed to generalize well to the other datasets, implying that one type of dataset cannot represent all sorts of sarcasm with different styles and domains. Compared to the existing datasets, models fine-tuned on the new dataset we release in this work showed the highest generalizability to other datasets. With a manual inspection of the datasets and post-hoc analysis, we attributed the difficulty in generalization to the fact that sarcasm actually comes in different domains and styles. We argue that future sarcasm research should take the broad scope of sarcasm into account.</abstract>
      <url hash="bffabc43">2024.naacl-long.238</url>
    </paper>
    <paper id="239">
      <title>Encoding of lexical tone in self-supervised models of spoken language</title>
      <author><first>Gaofei</first><last>Shen</last></author>
      <author><first>Michaela</first><last>Watkins</last></author>
      <author><first>Afra</first><last>Alishahi</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Arianna</first><last>Bisazza</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Grzegorz</first><last>Chrupała</last><affiliation>Tilburg University</affiliation></author>
      <pages>4250-4261</pages>
      <abstract>Interpretability research has shown that self-supervised Spoken LanguageModels (SLMs) encode a wide variety of features in human speech from theacoustic, phonetic, phonological, syntactic and semantic levels, to speakercharacteristics. The bulk of prior research on representations of phonologyhas focused on segmental features such as phonemes; the encoding ofsuprasegmental phonology (such as tone and stress patterns) in SLMs is not yetwell understood. Tone is a suprasegmental feature that is present in more thanhalf of the world’s languages. This paper aims to analyze the tone encodingcapabilities of SLMs, using Mandarin and Vietnamese as case studies. We showthat SLMs encode lexical tone to a significant degree even when they aretrained on data from non-tonal languages. We further find that SLMs behavesimilarly to native and non-native human participants in tone and consonantperception studies, but they do not follow the same developmental trajectory.</abstract>
      <url hash="92854920">2024.naacl-long.239</url>
    </paper>
    <paper id="240">
      <title>A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change</title>
      <author><first>Francesco</first><last>Periti</last><affiliation>University of Milan</affiliation></author>
      <author><first>Nina</first><last>Tahmasebi</last><affiliation>Göteborg University</affiliation></author>
      <pages>4262-4282</pages>
      <abstract>Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on *how*, *when*, and *why* these meanings change, rather than solely focusing on the extent of semantic change.</abstract>
      <url hash="e20f77f8">2024.naacl-long.240</url>
    </paper>
    <paper id="241">
      <title>i<fixed-case>ACOS</fixed-case>: Advancing Implicit Sentiment Extraction with Informative and Adaptive Negative Examples</title>
      <author><first>Xiancai</first><last>Xu</last><affiliation>Enbrands Inc.</affiliation></author>
      <author><first>Jia-Dong</first><last>Zhang</last><affiliation>Enbrands, Inc.</affiliation></author>
      <author><first>Lei</first><last>Xiong</last></author>
      <author><first>Zhishang</first><last>Liu</last></author>
      <pages>4283-4293</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) have been extensively studied, but little light has been shed on the quadruple extraction consisting of four fundamental elements: aspects, categories, opinions and sentiments, especially with implicit aspects and opinions. In this paper, we propose a new method iACOS for extracting Implicit Aspects with Categories and Opinions with Sentiments. First, iACOS appends two implicit tokens at the end of a text to capture the context-aware representation of all tokens including implicit aspects and opinions. Second, iACOS develops a sequence labeling model over the context-aware token representation to co-extract explicit and implicit aspects and opinions. Third, iACOS devises a multi-label classifier with a specialized multi-head attention for discovering aspect-opinion pairs and predicting their categories and sentiments simultaneously. Fourth, iACOS leverages informative and adaptive negative examples to jointly train the multi-label classifier and the other two classifiers on categories and sentiments by multi-task learning. Finally, the experimental results show that iACOS significantly outperforms other quadruple extraction baselines according to the F1 score on two public benchmark datasets.</abstract>
      <url hash="f9492e48">2024.naacl-long.241</url>
    </paper>
    <paper id="242">
      <title>Rectifying Demonstration Shortcut in In-Context Learning</title>
      <author><first>Joonwon</first><last>Jang</last></author>
      <author><first>Sanghwan</first><last>Jang</last><affiliation>POSTECH</affiliation></author>
      <author><first>Wonbin</first><last>Kweon</last></author>
      <author><first>Minjin</first><last>Jeon</last></author>
      <author><first>Hwanjo</first><last>Yu</last><affiliation>POSTECH</affiliation></author>
      <pages>4294-4321</pages>
      <abstract>Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities.However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the ‘Demonstration Shortcut’.While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations.To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method.We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens.In both settings, In-Context Calibration demonstrates substantial improvements, with results generalized across three LLM families (OPT, GPT, and Llama2) under various configurations.</abstract>
      <url hash="e44537b7">2024.naacl-long.242</url>
    </paper>
    <paper id="243">
      <title>Universal <fixed-case>NER</fixed-case>: A Gold-Standard Multilingual Named Entity Recognition Benchmark</title>
      <author><first>Stephen</first><last>Mayhew</last><affiliation>Duolingo</affiliation></author>
      <author><first>Terra</first><last>Blevins</last><affiliation>University of Washington</affiliation></author>
      <author><first>Shuheng</first><last>Liu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Marek</first><last>Suppa</last><affiliation>Comenius University in Bratislava</affiliation></author>
      <author><first>Hila</first><last>Gonen</last><affiliation>Facebook</affiliation></author>
      <author><first>Joseph Marvin</first><last>Imperial</last><affiliation>University of Bath, National University and National University - Human Language Technologies Lab</affiliation></author>
      <author><first>Börje</first><last>Karlsson</last><affiliation>Beijing Academy of Artificial Intelligence (BAAI)</affiliation></author>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>Nikola</first><last>Ljubešić</last><affiliation>Jožef Stefan Institute</affiliation></author>
      <author><first>Lester James</first><last>Miranda</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <author><first>Arij</first><last>Riabi</last></author>
      <author><first>Yuval</first><last>Pinter</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <pages>4322-4337</pages>
      <abstract>We introduce Universal NER (UNER), an open, community-driven project to develop gold-standard NER benchmarks in many languages. The overarching goal of UNER is to provide high-quality, cross-lingually consistent annotations to facilitate and standardize multilingual NER research. UNER v1 contains 19 datasets annotated with named entities in a cross-lingual consistent schema across 13 diverse languages. In this paper, we detail the dataset creation and composition of UNER; we also provide initial modeling baselines on both in-language and cross-lingual learning settings. We will release the data, code, and fitted models to the public.</abstract>
      <url hash="6fd9afb6">2024.naacl-long.243</url>
    </paper>
    <paper id="244">
      <title><fixed-case>ODD</fixed-case>: A Benchmark Dataset for the Natural Language Processing Based Opioid Related Aberrant Behavior Detection</title>
      <author><first>Sunjae</first><last>Kwon</last></author>
      <author><first>Xun</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Weisong</first><last>Liu</last><affiliation>University of Massachusetts at Lowell</affiliation></author>
      <author><first>Emily</first><last>Druhl</last><affiliation>Department of Veterans Affairs</affiliation></author>
      <author><first>Minhee</first><last>Sung</last></author>
      <author><first>Joel</first><last>Reisman</last></author>
      <author><first>Wenjun</first><last>Li</last><affiliation>University of Massachusetts at Lowell</affiliation></author>
      <author><first>Robert</first><last>Kerns</last><affiliation>Yale University</affiliation></author>
      <author><first>William</first><last>Becker</last></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>4338-4359</pages>
      <abstract>Opioid related aberrant behaviors (ORABs) present novel risk factors for opioid overdose. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset designed to identify ORABs from patients’ EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiazepines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing models (fine-tuning and prompt-tuning approaches) to identify ORAB. Experimental results show that the prompt-tuning models outperformed the fine-tuning models in most categories and the gains were especially higher among uncommon categories (Suggested Aberrant Behavior, Confirmed Aberrant Behaviors, Diagnosed Opioid Dependence, and Medication Change). Although the best model achieved the highest 88.17% on macro average area under precision recall curve, uncommon classes still have a large room for performance improvement. ODD is publicly available.</abstract>
      <url hash="00186413">2024.naacl-long.244</url>
    </paper>
    <paper id="245">
      <title>A Comprehensive Study of Gender Bias in Chemical Named Entity Recognition Models</title>
      <author><first>Xingmeng</first><last>Zhao</last></author>
      <author><first>Ali</first><last>Niazi</last></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>4360-4374</pages>
      <abstract>Chemical named entity recognition (NER) models are used in many downstream tasks, from adverse drug reaction identification to pharmacoepidemiology. However, it is unknown whether these models work the same for everyone. Performance disparities can potentially cause harm rather than the intended good. This paper assesses gender-related performance disparities in chemical NER systems. We develop a framework for measuring gender bias in chemical NER models using synthetic data and a newly annotated corpus of over 92,405 words with self-identified gender information from Reddit. Our evaluation of multiple biomedical NER models reveals evident biases. For instance, synthetic data suggests that female names are frequently misclassified as chemicals, especially when it comes to brand name mentions. Additionally, we observe performance disparities between female- and male-associated data in both datasets. Many systems fail to detect contraceptives such as birth control. Our findings emphasize the biases in chemical NER models, urging practitioners to account for these biases in downstream applications.</abstract>
      <url hash="745f999b">2024.naacl-long.245</url>
    </paper>
    <paper id="246">
      <title>The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education</title>
      <author><first>Paiheng</first><last>Xu</last><affiliation>Department of Computer Science, University of Maryland, College Park</affiliation></author>
      <author><first>Jing</first><last>Liu</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Nathan</first><last>Jones</last></author>
      <author><first>Julie</first><last>Cohen</last></author>
      <author><first>Wei</first><last>Ai</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>4375-4389</pages>
      <abstract>Assessing instruction quality is a fundamental component of any improvement efforts in the education system. However, traditional manual assessments are expensive, subjective, and heavily dependent on observers’ expertise and idiosyncratic factors, preventing teachers from getting timely and frequent feedback. Different from prior research that mostly focuses on low-inference instructional practices on a singular basis, this paper presents the first study that leverages Natural Language Processing (NLP) techniques to assess multiple high-inference instructional practices in two distinct educational settings: in-person K-12 classrooms and simulated performance tasks for pre-service teachers. This is also the first study that applies NLP to measure a teaching practice that is widely acknowledged to be particularly effective for students with special needs. We confront two challenges inherent in NLP-based instructional analysis, including noisy and long input data and highly skewed distributions of human ratings. Our results suggest that pretrained Language Models (PLMs) demonstrate performances comparable to the agreement level of human raters for variables that are more discrete and require lower inference, but their efficacy diminishes with more complex teaching practices. Interestingly, using only teachers’ utterances as input yields strong results for student-centered variables, alleviating common concerns over the difficulty of collecting and transcribing high-quality student speech data in in-person teaching settings. Our findings highlight both the potential and the limitations of current NLP techniques in the education domain, opening avenues for further exploration.</abstract>
      <url hash="983304de">2024.naacl-long.246</url>
    </paper>
    <paper id="247">
      <title>Differentially Private Next-Token Prediction of Large Language Models</title>
      <author><first>James</first><last>Flemings</last></author>
      <author><first>Meisam</first><last>Razaviyayn</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Murali</first><last>Annavaram</last><affiliation>University of Southern California</affiliation></author>
      <pages>4390-4404</pages>
      <abstract>Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model to guarantee Differential Privacy (DP). However, DP-SGD overestimates an adversary’s capabilities in having white box access to the model and, as a result, causes longer training times and larger memory usage than SGD. On the other hand, commercial LLM deployments are predominantly cloud-based; hence, adversarial access to LLMs is black-box. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol for next-token prediction that utilizes the inherent stochasticity of next-token sampling and a public model to achieve Differential Privacy. We formalize this by introducing RD-mollifers which project each of the model’s output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM’s output distribution, then average the projected distributions and sample from it. Unlike DP-SGD which needs to consider the model architecture during training, PMixED is model agnostic, which makes PMixED a very appealing solution for current deployments. Our results show that PMixED achieves a stronger privacy guarantee than sample-level privacy and outperforms DP-SGD for privacy <tex-math>\epsilon = 8</tex-math> on large-scale datasets. Thus, PMixED offers a practical alternative to DP training methods for achieving strong generative utility without compromising privacy.</abstract>
      <url hash="36f8ad3e">2024.naacl-long.247</url>
    </paper>
    <paper id="248">
      <title>Improving Adversarial Data Collection by Supporting Annotators: Lessons from <fixed-case>GAHD</fixed-case>, a <fixed-case>G</fixed-case>erman Hate Speech Dataset</title>
      <author><first>Janis</first><last>Goldzycher</last></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Gerold</first><last>Schneider</last><affiliation>University of Zurich</affiliation></author>
      <pages>4405-4424</pages>
      <abstract>Hate speech detection models are only as good as the data they are trained on. Datasets sourced from social media suffer from systematic gaps and biases, leading to unreliable models with simplistic decision boundaries. Adversarial datasets, collected by exploiting model weaknesses, promise to fix this problem. However, adversarial data collection can be slow and costly, and individual annotators have limited creativity. In this paper, we introduce GAHD, a new German Adversarial Hate speech Dataset comprising ca. 11k examples. During data collection, we explore new strategies for supporting annotators, to create more diverse adversarial examples more efficiently and provide a manual analysis of annotator disagreements for each strategy. Our experiments show that the resulting dataset is challenging even for state-of-the-art hate speech detection models, and that training on GAHD clearly improves model robustness. Further, we find that mixing multiple support strategies is most advantageous. We make GAHD publicly available at https://github.com/jagol/gahd.</abstract>
      <url hash="6714f5d4">2024.naacl-long.248</url>
    </paper>
    <paper id="249">
      <title>Memory Augmented Language Models through Mixture of Word Experts</title>
      <author><first>Cicero</first><last>Nogueira Dos Santos</last><affiliation>Research, Google</affiliation></author>
      <author><first>James</first><last>Lee-Thorp</last><affiliation>Google</affiliation></author>
      <author><first>Isaac</first><last>Noble</last><affiliation>Google</affiliation></author>
      <author><first>Chung-Ching</first><last>Chang</last><affiliation>Google</affiliation></author>
      <author><first>David</first><last>Uthus</last><affiliation>Google</affiliation></author>
      <pages>4425-4438</pages>
      <abstract>Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to aggressively decouple learning capacity and FLOPs through Mixture-of-Experts (MoE) style models with large knowledge-rich vocabulary based routing functions. Our proposed approach, dubbed Mixture of Word Experts (MoWE), can be seen as a memory augmented model, where a large set of word-specific experts play the role of a sparse memory. We demonstrate that MoWE performs significantly better than the T5 family of models with similar number of FLOPs in a variety of NLP tasks. Moreover, MoWE outperforms traditional MoE models on knowledge intensive tasks and has similar performance to complex memory augmented approaches that often require to invoke custom mechanisms to search the sparse memory.</abstract>
      <url hash="a33f3afb">2024.naacl-long.249</url>
    </paper>
    <paper id="250">
      <title>Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model</title>
      <author><first>Jaehun</first><last>Jung</last><affiliation>University of Washington</affiliation></author>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Faeze</first><last>Brahman</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Ximing</first><last>Lu</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Jillian</first><last>Fisher</last><affiliation>University of Washington</affiliation></author>
      <author><first>Taylor</first><last>Sorensen</last><affiliation>University of Washington and Brigham Young University</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>4439-4454</pages>
      <abstract>We present Impossible Distillation, a novel framework for paraphrasing and sentence summarization, that distills a high-quality dataset and model from a low-quality teacher that itself cannot perform these tasks. Unlike prior works that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific architecture, we hypothesize and verify the paraphrastic proximity intrinsic to pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in the LM distribution. By identifying and distilling generations from these subspaces, Impossible Distillation produces a high-quality dataset and model even from GPT2-scale LMs. We evaluate our method on multiple benchmarks spanning unconstrained / syntax-controlled paraphrase generation and sentence summarization. Our model with 770M parameters consistently outperforms strong baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher diversity and fidelity than up to 13 times larger datasets.</abstract>
      <url hash="c0bebcbf">2024.naacl-long.250</url>
    </paper>
    <paper id="251">
      <title><fixed-case>T</fixed-case>ofu<fixed-case>E</fixed-case>val: Evaluating Hallucinations of <fixed-case>LLM</fixed-case>s on Topic-Focused Dialogue Summarization</title>
      <author><first>Liyan</first><last>Tang</last></author>
      <author><first>Igor</first><last>Shalyminov</last><affiliation>Amazon</affiliation></author>
      <author><first>Amy</first><last>Wong</last><affiliation>Amazon</affiliation></author>
      <author><first>Jon</first><last>Burnsky</last><affiliation>Amazon</affiliation></author>
      <author><first>Jake</first><last>Vincent</last><affiliation>Amazon</affiliation></author>
      <author><first>Yu’an</first><last>Yang</last></author>
      <author><first>Siffi</first><last>Singh</last></author>
      <author><first>Song</first><last>Feng</last><affiliation>Amazon</affiliation></author>
      <author><first>Hwanjun</first><last>Song</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <author><first>Lijia</first><last>Sun</last><affiliation>Amazon</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>4455-4480</pages>
      <abstract>Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence- level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model’s size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.</abstract>
      <url hash="605ff7ec">2024.naacl-long.251</url>
    </paper>
    <paper id="252">
      <title><fixed-case>MOKA</fixed-case>: Moral Knowledge Augmentation for Moral Event Extraction</title>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>Winston</first><last>Wu</last><affiliation>University of Hawaii at Hilo</affiliation></author>
      <author><first>Nicholas</first><last>Beauchamp</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>4481-4502</pages>
      <abstract>News media often strive to minimize explicit moral language in news articles, yet most articles are dense with moral values as expressed through the reported events themselves. However, values that are reflected in the intricate dynamics among *participating entities* and *moral events* are far more challenging for most NLP systems to detect, including LLMs. To study this phenomenon, we annotate a new dataset, **MORAL EVENTS**, consisting of 5,494 structured event annotations on 474 news articles by diverse US media across the political spectrum. We further propose **MOKA**, a moral event extraction framework with **MO**ral **K**nowledge **A**ugmentation, which leverages knowledge derived from moral words and moral scenarios to produce structural representations of morality-bearing events. Experiments show that **MOKA** outperforms competitive baselines across three moral event understanding tasks. Further analysis shows even ostensibly nonpartisan media engage in the selective reporting of moral events.</abstract>
      <url hash="d8a3c3c5">2024.naacl-long.252</url>
    </paper>
    <paper id="253">
      <title>Fixing Rogue Memorization in Many-to-One Multilingual Translators of Extremely-Low-Resource Languages by Rephrasing Training Samples</title>
      <author><first>Paulo</first><last>Cavalin</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Pedro Henrique</first><last>Domingues</last><affiliation>Pontifícia Universidade Católica do Rio de Janeiro</affiliation></author>
      <author><first>Claudio</first><last>Pinhanez</last></author>
      <author><first>Julio</first><last>Nogima</last><affiliation>International Business Machines</affiliation></author>
      <pages>4503-4514</pages>
      <abstract>In this paper we study the fine-tuning of pre-trained large high-resource language models (LLMs) into many-to-one multilingual machine translators for extremely-low-resource languages such as endangered Indigenous languages. We explore those issues using datasets created from pseudo-parallel translations to English of The Bible written in 39 Brazilian Indigenous languages using mBART50 and WMT19 as pre-trained models and multiple translation metrics. We examine bilingual and multilingual models and show that, according to machine translation metrics, same-linguistic family models tend to perform best. However, we also found that many-to-one multilingual systems have a tendency to learn a “rogue” strategy of storing output strings from the training data in the LLM structure and retrieving them instead of performing actual translations. We show that rephrasing the output of the training samples seems to solve the problem.</abstract>
      <url hash="cc6345b9">2024.naacl-long.253</url>
    </paper>
    <paper id="254">
      <title>Backdoor Attacks on Multilingual Machine Translation</title>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Qiongkai</first><last>Xu</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Benjamin</first><last>Rubinstein</last><affiliation>The University of Melbourne and The University of Melbourne</affiliation></author>
      <author><first>Trevor</first><last>Cohn</last><affiliation>Google and The University of Melbourne</affiliation></author>
      <pages>4515-4534</pages>
      <abstract>While multilingual machine translation (MNMT) systems hold substantial promise, they also have security vulnerabilities. Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker injects poisoned data into a low-resource language pair to cause malicious translations in other languages, including high-resource languages.Our experimental results reveal that injecting less than 0.01% poisoned data into a low-resource language pair can achieve an average 20% attack success rate in attacking high-resource language pairs. This type of attack is of particular concern, given the larger attack surface of languages inherent to low-resource settings. Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address security concerns in machine translation, especially in the context of low-resource languages.</abstract>
      <url hash="7536a6c9">2024.naacl-long.254</url>
    </paper>
    <paper id="255">
      <title>Personalized Jargon Identification for Enhanced Interdisciplinary Communication</title>
      <author><first>Yue</first><last>Guo</last></author>
      <author><first>Joseph Chee</first><last>Chang</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Maria</first><last>Antoniak</last></author>
      <author><first>Erin</first><last>Bransom</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Trevor</first><last>Cohen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Lucy</first><last>Wang</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Tal</first><last>August</last></author>
      <pages>4535-4550</pages>
      <abstract>Scientific jargon can confuse researchers when they read materials from other domains. Identifying and translating jargon for individual researchers could speed up research, but current methods of jargon identification mainly use corpus-level familiarity indicators rather than modeling researcher-specific needs, which can vary greatly based on each researcher’s background. We collect a dataset of over 10K term familiarity annotations from 11 computer science researchers for terms drawn from 100 paper abstracts. Analysis of this data reveals that jargon familiarity and information needs vary widely across annotators, even within the same sub-domain (e.g., NLP). We investigate features representing domain, subdomain, and individual knowledge to predict individual jargon familiarity. We compare supervised and prompt-based approaches, finding that prompt-based methods using information about the individual researcher (e.g., personal publications, self-defined subfield of research) yield the highest accuracy, though the task remains difficult and supervised approaches have lower false positive rates. This research offers insights into features and methods for the novel task of integrating personal data into scientific jargon identification.</abstract>
      <url hash="0e401c42">2024.naacl-long.255</url>
    </paper>
    <paper id="256">
      <title>Flames: Benchmarking Value Alignment of <fixed-case>LLM</fixed-case>s in <fixed-case>C</fixed-case>hinese</title>
      <author><first>Kexin</first><last>Huang</last></author>
      <author><first>Xiangyang</first><last>Liu</last></author>
      <author><first>Qianyu</first><last>Guo</last></author>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Jiawei</first><last>Sun</last></author>
      <author><first>Yaru</first><last>Wang</last></author>
      <author><first>Zeyang</first><last>Zhou</last></author>
      <author><first>Yixu</first><last>Wang</last></author>
      <author><first>Yan</first><last>Teng</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yingchun</first><last>Wang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>4551-4591</pages>
      <abstract>The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and ‘topping the chart’ in these evaluations, there is still a significant gap in LLMs’ deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findings indicate that all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions. We also develop a lightweight specified scorer capable of scoring LLMs across multiple dimensions to efficiently evaluate new models on the benchmark. The complexity of Flames has far exceeded existing benchmarks, setting a new challenge for contemporary LLMs and highlighting the need for further alignment of LLMs. Our benchmark is publicly available at https://github.com/AIFlames/Flames.</abstract>
      <url hash="5797a1b4">2024.naacl-long.256</url>
    </paper>
    <paper id="257">
      <title>Mitigating Bias for Question Answering Models by Tracking Bias Influence</title>
      <author><first>Mingyu</first><last>Ma</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Jiun-Yu</first><last>Kao</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Arpit</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Yu-Hsiang</first><last>Lin</last><affiliation>Amazon</affiliation></author>
      <author><first>Wenbo</first><last>Zhao</last><affiliation>Amazon</affiliation></author>
      <author><first>Tagyoung</first><last>Chung</last><affiliation>Amazon</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>4592-4610</pages>
      <abstract>Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applied to multiple QA formulations across multiple bias categories. It can significantly reduce the bias level in all 9 bias categories in the BBQ dataset while maintaining comparable QA accuracy.</abstract>
      <url hash="18e25a52">2024.naacl-long.257</url>
    </paper>
    <paper id="258">
      <title>Extending <fixed-case>CLIP</fixed-case>’s Image-Text Alignment to Referring Image Segmentation</title>
      <author><first>Seoyeon</first><last>Kim</last></author>
      <author><first>Minguk</first><last>Kang</last></author>
      <author><first>Dongwon</first><last>Kim</last><affiliation>POSTECH</affiliation></author>
      <author><first>Jaesik</first><last>Park</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Suha</first><last>Kwak</last><affiliation>POSTECH and DGIST</affiliation></author>
      <pages>4611-4628</pages>
      <abstract>Referring Image Segmentation (RIS) is a cross-modal task that aims to segment an instance described by a natural language expression. Recent methods leverage large-scale pretrained unimodal models as backbones along with fusion techniques for joint reasoning across modalities. However, the inherent cross-modal nature of RIS raises questions about the effectiveness of unimodal backbones. We propose RISCLIP, a novel framework that effectively leverages the cross-modal nature of CLIP for RIS. Observing CLIP’s inherent alignment between image and text features, we capitalize on this starting point and introduce simple but strong modules that enhance unimodal feature extraction and leverage rich alignment knowledge in CLIP’s image-text shared-embedding space. RISCLIP exhibits outstanding results on all three major RIS benchmarks and also outperforms previous CLIP-based methods, demonstrating the efficacy of our strategy in extending CLIP’s image-text alignment to RIS.</abstract>
      <url hash="c3ff495a">2024.naacl-long.258</url>
    </paper>
    <paper id="259">
      <title>Generating Attractive and Authentic Copywriting from Customer Reviews</title>
      <author><first>Yu-Xiang</first><last>Lin</last></author>
      <author><first>Wei-Yun</first><last>Ma</last><affiliation>Academia Sinica</affiliation></author>
      <pages>4629-4642</pages>
      <abstract>The goal of product copywriting is to capture the interest of potential buyers by emphasizing the features of products through text descriptions. As e-commerce platforms offer a wide range of services, it’s becoming essential to dynamically adjust the styles of these auto-generated descriptions. Typical approaches to copywriting generation often rely solely on specified product attributes, which may result in dull and repetitive content. To tackle this issue, we propose to generate copywriting based on customer reviews, as they provide firsthand practical experiences with products, offering a richer source of information than just product attributes. We have developed a sequence-to-sequence framework, enhanced with reinforcement learning, to produce copywriting that is attractive, authentic, and rich in information. Our framework outperforms all existing baseline and zero-shot large language models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness and faithfulness. Furthermore, this work features the use of LLMs for aspect-based summaries collection and argument allure assessment. Experiments demonstrate the effectiveness of using LLMs for marketing domain corpus construction. The code and the dataset is publicly available at: <url>https://github.com/YuXiangLin1234/Copywriting-Generation</url>.</abstract>
      <url hash="03e1a4fe">2024.naacl-long.259</url>
    </paper>
    <paper id="260">
      <title>Effective Long-Context Scaling of Foundation Models</title>
      <author><first>Wenhan</first><last>Xiong</last><affiliation>Facebook</affiliation></author>
      <author><first>Jingyu</first><last>Liu</last></author>
      <author><first>Igor</first><last>Molybog</last><affiliation>Meta AI</affiliation></author>
      <author><first>Hejia</first><last>Zhang</last></author>
      <author><first>Prajjwal</first><last>Bhargava</last></author>
      <author><first>Rui</first><last>Hou</last><affiliation>Meta Inc.</affiliation></author>
      <author><first>Louis</first><last>Martin</last><affiliation>Facebook</affiliation></author>
      <author><first>Rashi</first><last>Rungta</last></author>
      <author><first>Karthik Abinav</first><last>Sankararaman</last><affiliation>Facebook</affiliation></author>
      <author><first>Barlas</first><last>Oguz</last><affiliation>Meta</affiliation></author>
      <author><first>Madian</first><last>Khabsa</last><affiliation>Facebook</affiliation></author>
      <author><first>Han</first><last>Fang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Yashar</first><last>Mehdad</last><affiliation>Facebook</affiliation></author>
      <author><first>Sharan</first><last>Narang</last><affiliation>Meta</affiliation></author>
      <author><first>Kshitiz</first><last>Malik</last></author>
      <author><first>Angela</first><last>Fan</last><affiliation>Facebook</affiliation></author>
      <author><first>Shruti</first><last>Bhosale</last><affiliation>Facebook</affiliation></author>
      <author><first>Sergey</first><last>Edunov</last></author>
      <author><first>Mike</first><last>Lewis</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Sinong</first><last>Wang</last><affiliation>Facebook</affiliation></author>
      <author><first>Hao</first><last>Ma</last><affiliation>Facebook</affiliation></author>
      <pages>4643-4663</pages>
      <abstract>We present an effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens. Our models are built through continual pretraining from Llama 2 checkpoints with longer text sequences and on a dataset where long texts are upsampled. We perform extensive evaluation using language modeling, synthetic context probing tasks, and a wide range of downstream benchmarks. Across all evaluations, our models achieve consistent improvements on most regular-context tasks and significant improvements on long-context tasks over Llama 2. Moreover, with a cost-effective instruction tuning procedure that is free of expensive annotation, the presented models can already surpass <tex-math>\texttt{gpt-3.5-turbo-16k}</tex-math>‘s overall performance on long-context benchmarks. Alongside these results, we provide an in-depth analysis on each individual component of our method. We delve into Llama’s position encodings and discuss its key limitation in modeling long data. We examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths – ablation results suggest that having abundant long texts in the pretrain dataset is <tex-math>\textit{not}</tex-math> the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.</abstract>
      <url hash="cb9aa453">2024.naacl-long.260</url>
    </paper>
    <paper id="261">
      <title>Empowering Diffusion Models on the Embedding Space for Text Generation</title>
      <author><first>Zhujin</first><last>Gao</last></author>
      <author><first>Junliang</first><last>Guo</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Yongxin</first><last>Zhu</last></author>
      <author><first>Fang</first><last>Zhang</last></author>
      <author><first>Jiang</first><last>Bian</last><affiliation>Microsoft</affiliation></author>
      <author><first>Linli</first><last>Xu</last></author>
      <pages>4664-4683</pages>
      <abstract>Diffusion models have achieved state-of-the-art synthesis quality on both visual and audio tasks, and recent works further adapt them to textual data by diffusing on the embedding space. In this paper, we conduct systematic studies of the optimization challenges encountered with both the embedding space and the denoising model, which have not been carefully explored. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the embedding space and unstable training. To alleviate this problem, we propose a new objective called the anchor loss which is more efficient than previous methods. Secondly, we find the noise levels of conventional schedules are insufficient for training a desirable denoising model while introducing varying degrees of degeneration in consequence. To address this challenge, we propose a novel framework called noise rescaling. Based on the above analysis, we propose Difformer, an embedding diffusion model based on Transformer. Experiments on varieties of seminal text generation tasks show the effectiveness of the proposed methods and the superiority of Difformer over previous state-of-the-art embedding diffusion baselines.</abstract>
      <url hash="b3b2a0cb">2024.naacl-long.261</url>
    </paper>
    <paper id="262">
      <title>Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback</title>
      <author><first>Yu</first><last>Xia</last></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Zhankui</first><last>He</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Handong</first><last>Zhao</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Shuai</first><last>Li</last><affiliation>John Hopcroft Center, Shanghai Jiao Tong University</affiliation></author>
      <pages>4684-4695</pages>
      <abstract>Large language models (LLMs) often generate biased outputs containing offensive, toxic, or stereotypical text. Existing LLM alignment methods such as reinforcement learning from human feedback (RLHF) alleviate biases primarily based on reward signals from current model outputs without considering the source of biases. In this work, to explore how biases are formed, we revisit LLMs’ text generation from a causal perspective. We identify pretraining data and input prompts, which contain semantic correlations of textual phrases, as two confounders between LLMs and model outputs causing biases. Inspired by our causal view, we leverage the reward model in RL alignment as an instrumental variable to perform causal intervention on LLMs. Utilizing the reward difference between an initial LLM and intervened LLM as interventional feedback to guide RL finetuning, we propose Causality-Aware Alignment (CAA) for LLM debiasing. Experiments on two text generation tasks with three different alignment objectives demonstrate the advantages of our method in aligning LLMs to generate less biased and safer outputs.</abstract>
      <url hash="d22aeef3">2024.naacl-long.262</url>
    </paper>
    <paper id="263">
      <title>Fake Alignment: Are <fixed-case>LLM</fixed-case>s Really Aligned Well?</title>
      <author><first>Yixu</first><last>Wang</last></author>
      <author><first>Yan</first><last>Teng</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Kexin</first><last>Huang</last></author>
      <author><first>Chengqi</first><last>Lyu</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Wenwei</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Xingjun</first><last>Ma</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yu-Gang</first><last>Jiang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Yingchun</first><last>Wang</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <pages>4696-4712</pages>
      <abstract>The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics——Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected performance estimation. Applying FINE to 14 widely-used LLMs reveals several models with purported safety are poorly aligned in practice. Subsequently, we found that multiple-choice format data can also be used as high-quality contrast distillation-based fine-tuning data, which can strongly improve the alignment consistency of LLMs with minimal fine-tuning overhead. For data and code, see https://github.com/AIFlames/Fake-Alignment.</abstract>
      <url hash="083f85d4">2024.naacl-long.263</url>
    </paper>
    <paper id="264">
      <title>Visually Guided Generative Text-Layout Pre-training for Document Intelligence</title>
      <author><first>Zhiming</first><last>Mao</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Haoli</first><last>Bai</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Lu</first><last>Hou</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>4713-4730</pages>
      <abstract>Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells). To this end, we propose visually guided generative text-layout pre-training, named ViTLP. Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence. In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length. ViTLP can function as a native OCR model to localize and recognize texts of document images. Besides, ViTLP can be effectively applied to various downstream VDU tasks. Extensive experiments show that ViTLP achieves competitive performance over existing baselines on benchmark VDU tasks, including information extraction, document classification, and document question answering.</abstract>
      <url hash="16de868e">2024.naacl-long.264</url>
    </paper>
    <paper id="265">
      <title><fixed-case>HILL</fixed-case>: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification</title>
      <author><first>He</first><last>Zhu</last></author>
      <author><first>Junran</first><last>Wu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Ruomei</first><last>Liu</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Yue</first><last>Hou</last><affiliation>Beihang University</affiliation></author>
      <author><first>Ze</first><last>Yuan</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Shangzhe</first><last>Li</last></author>
      <author><first>Yicheng</first><last>Pan</last></author>
      <author><first>Ke</first><last>Xu</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <pages>4731-4745</pages>
      <abstract>Existing self-supervised methods in natural language processing (NLP), especially hierarchical text classification (HTC), mainly focus on self-supervised contrastive learning, extremely relying on human-designed augmentation rules to generate contrastive samples, which can potentially corrupt or distort the original information. In this paper, we tend to investigate the feasibility of a contrastive learning scheme in which the semantic and syntactic information inherent in the input sample is adequately reserved in the contrastive samples and fused during the learning process. Specifically, we propose an information lossless contrastive learning strategy for HTC, namely <tex-math>\textbf{H}</tex-math>ierarchy-aware <tex-math>\textbf{I}</tex-math>nformation <tex-math>\textbf{L}</tex-math>ossless contrastive <tex-math>\textbf{L}</tex-math>earning (HILL), which consists of a text encoder representing the input document, and a structure encoder directly generating the positive sample. The structure encoder takes the document embedding as input, extracts the essential syntactic information inherent in the label hierarchy with the principle of structural entropy minimization, and injects the syntactic information into the text representation via hierarchical representation learning. Experiments on three common datasets are conducted to verify the superiority of HILL.</abstract>
      <url hash="4055a610">2024.naacl-long.265</url>
    </paper>
    <paper id="266">
      <title>Investigating the Emergent Audio Classification Ability of <fixed-case>ASR</fixed-case> Foundation Models</title>
      <author><first>Rao</first><last>Ma</last></author>
      <author><first>Adian</first><last>Liusie</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Kate</first><last>Knill</last><affiliation>University of Cambridge</affiliation></author>
      <pages>4746-4760</pages>
      <abstract>Text and vision foundation models can perform many tasks in a zero-shot setting, a desirable property that enables these systems to be applied in general and low-resource settings. There has been far less work, however, on the zero-shot abilities of ASR foundation models, with these systems typically fine-tuned to specific tasks or constrained to applications that match their training criterion and data annotation. In this work we investigate the ability of Whisper and MMS, ASR foundation models trained primarily for speech recognition, to perform zero-shot audio classification. We use simple template-based text prompts at the decoder and use the resulting decoding probabilities to generate zero-shot predictions. Without training the model on extra data or adding any new parameters, we demonstrate that Whisper shows promising zero-shot classification performance on a range of 8 audio-classification datasets, outperforming the accuracy of existing state-of-the-art zero-shot baselines by an average of 9%. One important step to unlock the emergent ability is debiasing, where a simple unsupervised reweighting method of the class probabilities yields consistent significant performance gains. We further show that performance increases with model size, implying that as ASR foundation models scale up, they may exhibit improved zero-shot performance.</abstract>
      <url hash="4134c7d5">2024.naacl-long.266</url>
    </paper>
    <paper id="267">
      <title>In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax</title>
      <author><first>Aaron</first><last>Mueller</last><affiliation>Northeastern University and Technion - Israel Institute of Technology, Technion</affiliation></author>
      <author><first>Albert</first><last>Webson</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Jackson</first><last>Petty</last><affiliation>New York University</affiliation></author>
      <author><first>Tal</first><last>Linzen</last><affiliation>New York University and Google</affiliation></author>
      <pages>4761-4779</pages>
      <abstract>In-context learning (ICL) is now a common method for teaching large language models (LLMs) new tasks: given labeled examples in the input context, the LLM learns to perform the task without weight updates. Do models guided via ICL infer the underlying structure of the task defined by the context, or do they rely on superficial heuristics that only generalize to identically distributed examples? We address this question using transformations tasks and an NLI task that assess sensitivity to syntax—a requirement for robust language understanding. We further investigate whether out-of-distribution generalization can be improved via chain-of-thought prompting, where the model is provided with a sequence of intermediate computation steps that illustrate how the task ought to be performed. In experiments with models from the GPT, PaLM, and Llama 2 families, we find large variance across LMs. The variance is explained more by the composition of the pre-training corpus and supervision methods than by model size; in particular, models pre-trained on code generalize better, and benefit more from chain-of-thought prompting.</abstract>
      <url hash="7bb88b7f">2024.naacl-long.267</url>
    </paper>
    <paper id="268">
      <title>Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt</title>
      <author><first>Yongqi</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ruofan</first><last>Hu</last></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhiqing</first><last>Hong</last></author>
      <author><first>Ruiqi</first><last>Li</last></author>
      <author><first>Wenrui</first><last>Liu</last></author>
      <author><first>Fuming</first><last>You</last></author>
      <author><first>Tao</first><last>Jin</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>4780-4794</pages>
      <abstract>Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.github.io .</abstract>
      <url hash="feb769f6">2024.naacl-long.268</url>
    </paper>
    <paper id="269">
      <title>Lost in Transcription: Identifying and Quantifying the Accuracy Biases of Automatic Speech Recognition Systems Against Disfluent Speech</title>
      <author><first>Dena</first><last>Mujtaba</last></author>
      <author><first>Nihar</first><last>Mahapatra</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Megan</first><last>Arney</last><affiliation>Michigan State University</affiliation></author>
      <author><first>J</first><last>Yaruss</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Hope</first><last>Gerlach-Houck</last><affiliation>Western Michigan University</affiliation></author>
      <author><first>Caryn</first><last>Herring</last></author>
      <author><first>Jia</first><last>Bin</last></author>
      <pages>4795-4809</pages>
      <abstract>Automatic speech recognition (ASR) systems, increasingly prevalent in education, healthcare, employment, and mobile technology, face significant challenges in inclusivity, particularly for the 80 million-strong global community of people who stutter. These systems often fail to accurately interpret speech patterns deviating from typical fluency, leading to critical usability issues and misinterpretations. This study evaluates six leading ASRs, analyzing their performance on both a real-world dataset of speech samples from individuals who stutter and a synthetic dataset derived from the widely-used LibriSpeech benchmark. The synthetic dataset, uniquely designed to incorporate various stuttering events, enables an in-depth analysis of each ASR’s handling of disfluent speech. Our comprehensive assessment includes metrics such as word error rate (WER), character error rate (CER), and semantic accuracy of the transcripts. The results reveal a consistent and statistically significant accuracy bias across all ASRs against disfluent speech, manifesting in significant syntactical and semantic inaccuracies in transcriptions. These findings highlight a critical gap in current ASR technologies, underscoring the need for effective bias mitigation strategies. Addressing this bias is imperative not only to improve the technology’s usability for people who stutter but also to ensure their equitable and inclusive participation in the rapidly evolving digital landscape.</abstract>
      <url hash="7c2fa82b">2024.naacl-long.269</url>
    </paper>
    <paper id="270">
      <title><fixed-case>MAFALDA</fixed-case>: A Benchmark and Comprehensive Study of Fallacy Detection and Classification</title>
      <author><first>Chadi</first><last>Helwe</last></author>
      <author><first>Tom</first><last>Calamai</last></author>
      <author><first>Pierre-Henri</first><last>Paris</last><affiliation>Télécom Paris</affiliation></author>
      <author><first>Chloé</first><last>Clavel</last><affiliation>INRIA and Télécom Paris</affiliation></author>
      <author><first>Fabian</first><last>Suchanek</last><affiliation>Telecom Paris</affiliation></author>
      <pages>4810-4845</pages>
      <abstract>We introduce MAFALDA, a benchmark for fallacy classification that merges and unites previous fallacy datasets. It comes with a taxonomy that aligns, refines, and unifies existing classifications of fallacies. We further provide a manual annotation of a part of the dataset together with manual explanations for each annotation. We propose a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity. We then evaluate several language models under a zero-shot learning setting and human performances on MAFALDA to assess their capability to detect and classify fallacies.</abstract>
      <url hash="3adbb88c">2024.naacl-long.270</url>
    </paper>
    <paper id="271">
      <title>Diffusion Glancing Transformer for Parallel Sequence-to-Sequence Learning</title>
      <author><first>Lihua</first><last>Qian</last><affiliation>ByteDance</affiliation></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <pages>4846-4862</pages>
      <abstract>Previously, non-autoregressive models were widely recognized as being superior in generation efficiency but inferior in generation quality due to the challenges of modeling multiple target modalities.To enhance the multi-modality modeling ability, we propose the diffusion glancing transformer, which employs a modality diffusion process and residual glancing sampling.The modality diffusion process is a discrete process that interpolates the multi-modal distribution along the decoding steps, and the residual glancing sampling approach guides the model to continuously learn the remaining modalities across the layers. Experimental results on various machine translation and text generation benchmarks demonstrate that DIFFGLAT achieves better generation accuracy while maintaining fast decoding speed compared with both autoregressive and non-autoregressive models.</abstract>
      <url hash="deda24cc">2024.naacl-long.271</url>
    </paper>
    <paper id="272">
      <title>No Context Needed: Contextual Quandary In Idiomatic Reasoning With Pre-Trained Language Models</title>
      <author><first>Kellen</first><last>Cheng</last><affiliation>Princeton University</affiliation></author>
      <author><first>Suma</first><last>Bhat</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>4863-4880</pages>
      <abstract>Reasoning in the presence of idiomatic expressions (IEs) remains a challenging frontier in natural language understanding (NLU). Unlike standard text, the non-compositional nature of an IE makes it difficult for model comprehension, as their figurative or non-literal mean- ing usually cannot be inferred from the constituent words alone. It stands to reason that in these challenging circumstances, pre-trained language models (PTLMs) should make use of the surrounding context to infer additional in- formation about the IE. In this paper, we investigate the utilization of said context for idiomatic reasoning tasks, which is under-explored relative to arithmetic or commonsense reason- ing (Liu et al., 2022; Yu et al., 2023). Preliminary findings point to a surprising observation: general purpose PTLMs are actually negatively affected by the context, as performance almost always increases with its removal. In these scenarios, models may see gains of up to 3.89%. As a result, we argue that only IE-aware models remain suitable for idiomatic reasoning tasks, given the unexpected and unexplainable manner in which general purpose PTLMs reason over IEs. Additionally, we conduct studies to examine how models utilize the context in various situations, as well as an in-depth analysis on dataset formation and quality. Finally, we provide some explanations and insights into the reasoning process itself based on our results.</abstract>
      <url hash="340a8965">2024.naacl-long.272</url>
    </paper>
    <paper id="273">
      <title>Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding Recommendation</title>
      <author><first>Xindi</first><last>Wang</last><affiliation>Huawei Technologies Ltd., University of Western Ontario and Vector Institute</affiliation></author>
      <author><first>Robert</first><last>Mercer</last><affiliation>University of Western Ontario</affiliation></author>
      <author><first>Frank</first><last>Rudzicz</last><affiliation>Dalhousie University</affiliation></author>
      <pages>4881-4891</pages>
      <abstract>The International Classification of Diseases (ICD) serves as a definitive medical classification system encompassing a wide range of diseases and conditions. The primary objective of ICD indexing is to allocate a subset of ICD codes to a medical record, which facilitates standardized documentation and management of various health conditions. Most existing approaches have suffered from selecting the proper label subsets from an extremely large ICD collection with a heavy long-tailed label distribution. In this paper, we leverage a multi-stage “retrieve and re-rank” framework as a novel solution to ICD indexing, via a hybrid discrete retrieval method, and re-rank retrieved candidates with contrastive learning that allows the model to make more accurate predictions from a simplified label space. The retrieval model is a hybrid of auxiliary knowledge of the electronic health records (EHR) and a discrete retrieval method (BM25), which efficiently collects high-quality candidates. In the last stage, we propose a label co-occurrence guided contrastive re-ranking model, which re-ranks the candidate labels by pulling together the clinical notes with positive ICD codes. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures on the MIMIC-III benchmark.</abstract>
      <url hash="4faa877d">2024.naacl-long.273</url>
    </paper>
    <paper id="274">
      <title>Anisotropy is Not Inherent to Transformers</title>
      <author><first>Anemily</first><last>Machina</last><affiliation>University of Western Ontario</affiliation></author>
      <author><first>Robert</first><last>Mercer</last><affiliation>University of Western Ontario</affiliation></author>
      <pages>4892-4907</pages>
      <abstract>Isotropy is the property that embeddings are uniformly distributed around the origin. Previous work has shown that Transformer embedding spaces are anisotropic, which is called the representation degradation problem. This degradation has been assumed to be inherent to the standard language modeling tasks and to apply to all Transformer models regardless of their architecture. In this work we identify a set of Transformer models with isotropic embedding spaces, the large Pythia models. We examine the isotropy of Pythia models and explore how isotropy and anisotropy develop as a model is trained. We find that anisotropic models do not develop as previously theorized, using our own analysis to show that the large Pythia models optimize their final Layer Norm for isotropy, and provide reasoning why previous theoretical justifications for anisotropy were insufficient. The identification of a set of isotropic Transformer models calls previous assumptions into question, provides a set of models to contrast existing analysis, and should lead to deeper insight into isotropy.</abstract>
      <url hash="4c507224">2024.naacl-long.274</url>
    </paper>
    <paper id="275">
      <title>Finding Replicable Human Evaluations via Stable Ranking Probability</title>
      <author><first>Parker</first><last>Riley</last><affiliation>Google</affiliation></author>
      <author><first>Daniel</first><last>Deutsch</last><affiliation>Google</affiliation></author>
      <author><first>George</first><last>Foster</last><affiliation>Google</affiliation></author>
      <author><first>Viresh</first><last>Ratnakar</last><affiliation>Google</affiliation></author>
      <author><first>Ali</first><last>Dabirmoghaddam</last></author>
      <author><first>Markus</first><last>Freitag</last><affiliation>Google</affiliation></author>
      <pages>4908-4919</pages>
      <abstract>Reliable human evaluation is critical to the development of successful natural language generation models, but achieving it is notoriously difficult. Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential. Without it, there is no reliable foundation for hill-climbing or product launch decisions. In this paper, we use machine translation and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions. We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization. Our study on two language pairs provides concrete recommendations for designing replicable human evaluation studies. We also collect and release the largest publicly available dataset of multi-segment translations rated by multiple professional translators, consisting of nearly 140,000 segment annotations across two language pairs.</abstract>
      <url hash="6994fea0">2024.naacl-long.275</url>
    </paper>
    <paper id="276">
      <title>Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections</title>
      <author><first>Yuanpu</first><last>Cao</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Bochuan</first><last>Cao</last></author>
      <author><first>Jinghui</first><last>Chen</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>4920-4935</pages>
      <abstract>Recent developments in Large Language Models (LLMs) have manifested significant advancements. To facilitate safeguards against malicious exploitation, a body of research has concentrated on aligning LLMs with human preferences and inhibiting their generation of inappropriate content. Unfortunately, such alignments are often vulnerable: fine-tuning with a minimal amount of harmful data can easily unalign the target LLM. While being effective, such fine-tuning-based unalignment approaches also have their own limitations: (1) non-stealthiness, after fine-tuning, safety audits or red-teaming can easily expose the potential weaknesses of the unaligned models, thereby precluding their release/use. (2) non-persistence, the unaligned LLMs can be easily repaired through re-alignment, i.e., fine-tuning again with aligned data points. In this work, we show that it is possible to conduct stealthy and persistent unalignment on large language models via backdoor injections. We also provide a novel understanding of the relationship between the backdoor persistence and the activation pattern and further provide guidelines for potential trigger design. Through extensive experiments, we demonstrate that our proposed stealthy and persistent unalignment can successfully pass the safety evaluation while maintaining strong persistence against re-alignment defense.</abstract>
      <url hash="90b6bdb3">2024.naacl-long.276</url>
    </paper>
    <paper id="277">
      <title>Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts</title>
      <author><first>Sai Ashish</first><last>Somayajula</last></author>
      <author><first>Youwei</first><last>Liang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Abhishek</first><last>Singh</last></author>
      <author><first>Pengtao</first><last>Xie</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>4936-4953</pages>
      <abstract>Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting. We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of finetuning PLMs on low-resource datasets. Our code is available at https://github.com/Sai-Ashish/Attention_guided_weight_mixup_BLO.</abstract>
      <url hash="e5f9e80a">2024.naacl-long.277</url>
    </paper>
    <paper id="278">
      <title>Detecting Bipolar Disorder from Misdiagnosed Major Depressive Disorder with Mood-Aware Multi-Task Learning</title>
      <author><first>Daeun</first><last>Lee</last></author>
      <author><first>Hyolim</first><last>Jeon</last></author>
      <author><first>Sejung</first><last>Son</last></author>
      <author><first>Chaewon</first><last>Park</last></author>
      <author><first>Ji Hyun</first><last>An</last><affiliation>Samsung</affiliation></author>
      <author><first>Seungbae</first><last>Kim</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Jinyoung</first><last>Han</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>4954-4970</pages>
      <abstract>Bipolar Disorder (BD) is a mental disorder characterized by intense mood swings, from depression to manic states. Individuals with BD are at a higher risk of suicide, but BD is often misdiagnosed as Major Depressive Disorder (MDD) due to shared symptoms, resulting in delays in appropriate treatment and increased suicide risk. While early intervention based on social media data has been explored to uncover latent BD risk, little attention has been paid to detecting BD from those misdiagnosed as MDD. Therefore, this study presents a novel approach for identifying BD risk in individuals initially misdiagnosed with MDD. A unique dataset, BD-Risk, is introduced, incorporating mental disorder types and BD mood levels verified by two clinical experts. The proposed multi-task learning for predicting BD risk and BD mood level outperforms the state-of-the-art baselines. Also, the proposed dynamic mood-aware attention can provide insights into the impact of BD mood on future risk, potentially aiding interventions for at-risk individuals.</abstract>
      <url hash="d6221a87">2024.naacl-long.278</url>
    </paper>
    <paper id="279">
      <title>Leveraging Code to Improve In-Context Learning for Semantic Parsing</title>
      <author><first>Ben</first><last>Bogin</last></author>
      <author><first>Shivanshu</first><last>Gupta</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ashish</first><last>Sabharwal</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>4971-5012</pages>
      <abstract>In-context learning (ICL) is an appealing approach for semantic parsing due to its few-shot nature and improved generalization. However, learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs.In this work, we show how pre-existing coding abilities of LLMs can be leveraged for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions. We show that both these changes significantly improve accuracy across three popular datasets; combined, they lead to dramatic improvements (e.g., 7.9% to 66.5% on SMCalFlow compositional split) and can substantially improve compositional generalization, nearly closing the performance gap between easier i.i.d. and harder compositional splits. Finally, comparisons across multiple PLs and DSL variations suggest that the similarity of a target language to general-purpose code is more important than prevalence in pretraining corpora. Our findings provide an improved methodology for building semantic parsers in the modern context of ICL with LLMs.</abstract>
      <url hash="0f076f99">2024.naacl-long.279</url>
    </paper>
    <paper id="280">
      <title>Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical <fixed-case>NER</fixed-case></title>
      <author><first>Micheal</first><last>Abaho</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Danushka</first><last>Bollegala</last><affiliation>Amazon and University of Liverpool</affiliation></author>
      <author><first>Gary</first><last>Leeming</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Dan</first><last>Joyce</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Iain</first><last>Buchan</last><affiliation>University of Liverpool</affiliation></author>
      <pages>5013-5029</pages>
      <abstract>Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately predicting DS-terms compared to generic words. Results of our analysis show that MSLM improves LMs sensitivity and detection of DS-terms. We empirically show that an optimal masking rate not only depends on the LM, but also on the dataset and the length of sequences. Our proposed masking strategy outperforms advanced masking strategies such as span- and PMI-based masking.</abstract>
      <url hash="0eb09314">2024.naacl-long.280</url>
    </paper>
    <paper id="281">
      <title>Language Models Implement Simple <fixed-case>W</fixed-case>ord2<fixed-case>V</fixed-case>ec-style Vector Arithmetic</title>
      <author><first>Jack</first><last>Merullo</last><affiliation>Brown University</affiliation></author>
      <author><first>Carsten</first><last>Eickhoff</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Ellie</first><last>Pavlick</last><affiliation>Brown University and Brown University</affiliation></author>
      <pages>5030-5047</pages>
      <abstract>A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple vector arithmetic style mechanism to solve some relational tasks using regularities encoded in the hidden space of the model (e.g., Poland:Warsaw::China:Beijing). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, uppercasing, and past-tensing) a key part of the mechanism reduces to a simple additive update typically applied by the feedforward (FFN) networks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a growing body of work on the interpretability of LMs, and offer reason to be optimistic that, despite the massive and non-linear nature of the models, the strategies they ultimately use to solve tasks can sometimes reduce to familiar and even intuitive algorithms.</abstract>
      <url hash="ce8b749e">2024.naacl-long.281</url>
    </paper>
    <paper id="282">
      <title><fixed-case>A</fixed-case>uto<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning</title>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Rushi</first><last>Qiang</last></author>
      <author><first>Sai Ashish</first><last>Somayajula</last></author>
      <author><first>Pengtao</first><last>Xie</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>5048-5060</pages>
      <abstract>Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA’s uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should be discarded. A meta learning based method is developed to learn these selection variables. The optimal rank is determined by thresholding the values of these variables. Our comprehensive experiments on natural language understanding, generation, and sequence labeling demonstrate the effectiveness of AutoLoRA. The code is publicly available at https://github.com/ruz048/AutoLoRA</abstract>
      <url hash="04a2629f">2024.naacl-long.282</url>
    </paper>
    <paper id="283">
      <title><fixed-case>S</fixed-case>port<fixed-case>QA</fixed-case>: A Benchmark for Sports Understanding in Large Language Models</title>
      <author><first>Haotian</first><last>Xia</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Zhengbang</first><last>Yang</last></author>
      <author><first>Yuqing</first><last>Wang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Rhys</first><last>Tracy</last></author>
      <author><first>Yun</first><last>Zhao</last></author>
      <author><first>Dongdong</first><last>Huang</last><affiliation>Beijing Normal University</affiliation></author>
      <author><first>Zezhi</first><last>Chen</last></author>
      <author><first>Yan</first><last>Zhu</last><affiliation>Beijing Normal University</affiliation></author>
      <author><first>Yuan-fang</first><last>Wang</last></author>
      <author><first>Weining</first><last>Shen</last><affiliation>University of California, Irvine</affiliation></author>
      <pages>5061-5081</pages>
      <abstract>A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs. The dataset is available at https://github.com/haotianxia/SportQA</abstract>
      <url hash="121f1aaa">2024.naacl-long.283</url>
    </paper>
    <paper id="284">
      <title>Revisiting subword tokenization: A case study on affixal negation in large language models</title>
      <author><first>Thinh</first><last>Truong</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Yulia</first><last>Otmakhova</last><affiliation>The University of Melbourne</affiliation></author>
      <author><first>Karin</first><last>Verspoor</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Trevor</first><last>Cohn</last><affiliation>Google and The University of Melbourne</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>5082-5095</pages>
      <abstract>In this work, we measure the impact of affixal negation on modern English large language models (LLMs). In affixal negation, the negated meaning is expressed through a negative morpheme, which is potentially challenging for LLMs as their tokenizers are often not morphologically plausible. We conduct extensive experiments using LLMs with different subword tokenization methods, which lead to several insights on the interaction between tokenization performance and negation sensitivity. Despite some interesting mismatches between tokenization accuracy and negation detection performance, we show that models can, on the whole, reliably recognize the meaning of affixal negation.</abstract>
      <url hash="b7718d6b">2024.naacl-long.284</url>
    </paper>
    <paper id="285">
      <title>Generating Mental Health Transcripts with <fixed-case>SAPE</fixed-case> (<fixed-case>S</fixed-case>panish Adaptive Prompt Engineering)</title>
      <author><first>Daniel</first><last>Lozoya</last></author>
      <author><first>Alejandro</first><last>Berazaluce</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Juan</first><last>Perches</last></author>
      <author><first>Eloy</first><last>Lúa</last></author>
      <author><first>Mike</first><last>Conway</last></author>
      <author><first>Simon</first><last>D’Alfonso</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>5096-5113</pages>
      <abstract>Large language models have become valuable tools for data augmentation in scenarios with limited data availability, as they can generate synthetic data resembling real-world data. However, their generative performance depends on the quality of the prompt used to instruct the model. Prompt engineering that relies on hand-crafted strategies or requires domain experts to adjust the prompt often yields suboptimal results. In this paper we present SAPE, a Spanish Adaptive Prompt Engineering method utilizing genetic algorithms for prompt generation and selection. Our evaluation of SAPE focuses on a generative task that involves the creation of Spanish therapy transcripts, a type of data that is challenging to collect due to the fact that it typically includes protected health information. Through human evaluations conducted by mental health professionals, our results show that SAPE produces Spanish counselling transcripts that more closely resemble authentic therapy transcripts compared to other prompt engineering techniques that are based on Reflexion and Chain-of-Thought.</abstract>
      <url hash="18e6c3ff">2024.naacl-long.285</url>
    </paper>
    <paper id="286">
      <title>Where are you from? Geolocating Speech and Applications to Language Identification</title>
      <author><first>Patrick</first><last>Foley</last></author>
      <author><first>Matthew</first><last>Wiesner</last></author>
      <author><first>Bismarck</first><last>Odoom</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Leibny Paola</first><last>Garcia Perera</last></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Philipp</first><last>Koehn</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>5114-5126</pages>
      <abstract>We train models to answer the question, Where are you from? and show how such models can be repurposed for language identification (LID). To our knowledge, this paper is the first to introduce data sources, methods and models to tackle the task of geolocation of speech at a global scale, and the first to explore using geolocation as a proxy-task for LID. Specifically, we explore whether radio broadcasts with known origin can be used to train regression and classification-based models for geolocating speech. We build models on top of self-supervised pretrained models, using attention pooling to qualitatively verify that the model geolocates the speech itself, and not other channel artifacts.The best geolocation models localize speaker origin to around 650km. We confirm the value of speech geolocation as a proxy task by using speech geolocation models for zero-shot LID. Finally, we show that fine-tuning geolocation models for LID outperforms fine-tuning pretrained Wav2Vec2.0 models, and achieves state-of-the-art performance on the FLEURS benchmark.</abstract>
      <url hash="f3a115cc">2024.naacl-long.286</url>
    </paper>
    <paper id="287">
      <title>Teaching Language Models to Self-Improve through Interactive Demonstrations</title>
      <author><first>Xiao</first><last>Yu</last></author>
      <author><first>Baolin</first><last>Peng</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Michel</first><last>Galley</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>5127-5149</pages>
      <abstract>The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research. However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones. To reduce this gap, we introduce TriPosT, a training algorithm that endows smaller models with such self-improvement ability, and show that our approach can improve LLaMA-7B’s performance on math and reasoning tasks by up to 7.13%. In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on *its own generations*. We then replay this experience to train the small model. Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its *own* mistakes is crucial for small models to improve their performance.</abstract>
      <url hash="ca203ccd">2024.naacl-long.287</url>
    </paper>
    <paper id="288">
      <title><fixed-case>MAGID</fixed-case>: An Automated Pipeline for Generating Synthetic Multi-modal Datasets</title>
      <author><first>Hossein</first><last>Aboutalebi</last></author>
      <author><first>Hwanjun</first><last>Song</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Yusheng</first><last>Xie</last><affiliation>Amazon</affiliation></author>
      <author><first>Arshit</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Lijia</first><last>Sun</last><affiliation>Amazon</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <author><first>Igor</first><last>Shalyminov</last><affiliation>Amazon</affiliation></author>
      <author><first>Nikolaos</first><last>Pappas</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Siffi</first><last>Singh</last></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <pages>5150-5167</pages>
      <abstract>Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce <b>M</b>ultimodal <b>A</b>ugmented <b>G</b>enerative <b>I</b>mages <b>D</b>ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images . Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small.</abstract>
      <url hash="f22c0861">2024.naacl-long.288</url>
    </paper>
    <paper id="289">
      <title>Zero-shot Generative Linguistic Steganography</title>
      <author><first>Ke</first><last>Lin</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yiyang</first><last>Luo</last></author>
      <author><first>Zijian</first><last>Zhang</last></author>
      <author><first>Luo</first><last>Ping</last></author>
      <pages>5168-5182</pages>
      <abstract>Generative linguistic steganography attempts to hide secret messages into covertext. Previous studies have generally focused on the statistical differences between the covertext and stegotext, however, ill-formed stegotext can readily be identified by humans. In this paper, we propose a novel zero-shot approach based on in-context learning for linguistic steganography to achieve better perceptual and statistical imperceptibility. We also design several new metrics and reproducible language evaluations to measure the imperceptibility of the stegotext. Our experimental results indicate that our method produces <tex-math>1.926\times</tex-math> more innocent and intelligible stegotext than any other method.</abstract>
      <url hash="e8d85776">2024.naacl-long.289</url>
    </paper>
    <paper id="290">
      <title>Does <fixed-case>GPT</fixed-case>-4 pass the <fixed-case>T</fixed-case>uring test?</title>
      <author><first>Cameron</first><last>Jones</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ben</first><last>Bergen</last></author>
      <pages>5183-5210</pages>
      <abstract>We evaluated GPT-4 in a public online Turing test. The best-performing GPT-4 prompt passed in 49.7% of games, outperforming ELIZA (22%) and GPT-3.5 (20%), but falling short of the baseline set by human participants (66%). Participants’ decisions were based mainly on linguistic style (35%) and socioemotional traits (27%), supporting the idea that intelligence, narrowly conceived, is not sufficient to pass the Turing test. Participant knowledge about LLMs and number of games played positively correlated with accuracy in detecting AI, suggesting learning and practice as possible strategies to mitigate deception. Despite known limitations as a test of intelligence, we argue that the Turing test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria for judging humanlikeness.</abstract>
      <url hash="47bbdac5">2024.naacl-long.290</url>
    </paper>
    <paper id="291">
      <title>Polarity Calibration for Opinion Summarization</title>
      <author><first>Yuanyuan</first><last>Lei</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Kaiqiang</first><last>Song</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Sangwoo</first><last>Cho</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xiaoyang</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Ruihong</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>5211-5224</pages>
      <abstract>Opinion summarization is automatically generating summaries from a variety of subjective information, such as product reviews or political opinions. The challenge of opinions summarization lies in presenting divergent or even conflicting opinions. We conduct an analysis of previous summarization models, which reveals their inclination to amplify the polarity bias, emphasizing the majority opinions while ignoring the minority opinions. To address this issue and make the summarizer express both sides of opinions, we introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text. Specifically, we develop a reinforcement training approach for polarity calibration. This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality. We evaluate our Polarity Calibration model (PoCa) on two types of opinions summarization tasks: summarizing product reviews and political opinions articles. Automatic and human evaluation demonstrate that our approach can mitigate the polarity mismatch between output summary and input text, as well as maintain the content semantic and language quality.</abstract>
      <url hash="b083fb8b">2024.naacl-long.291</url>
    </paper>
    <paper id="292">
      <title>Sentence-level Media Bias Analysis with Event Relation Graph</title>
      <author><first>Yuanyuan</first><last>Lei</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Ruihong</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <pages>5225-5238</pages>
      <abstract>Media outlets are becoming more partisan and polarized nowadays. In this paper, we identify media bias at the sentence level, and pinpoint bias sentences that intend to sway readers’ opinions. As bias sentences are often expressed in a neutral and factual way, considering broader context outside a sentence can help reveal the bias. In particular, we observe that events in a bias sentence need to be understood in associations with other events in the document. Therefore, we propose to construct an event relation graph to explicitly reason about event-event relations for sentence-level bias identification. The designed event relation graph consists of events as nodes and four common types of event relations: coreference, temporal, causal, and subevent relations. Then, we incorporate event relation graph for bias sentences identification in two steps: an event-aware language model is built to inject the events and event relations knowledge into the basic language model via soft labels; further, a relation-aware graph attention network is designed to update sentence embedding with events and event relations information based on hard labels. Experiments on two benchmark datasets demonstrate that our approach with the aid of event relation graph improves both precision and recall of bias sentence identification.</abstract>
      <url hash="d1159873">2024.naacl-long.292</url>
    </paper>
    <paper id="293">
      <title><fixed-case>EMONA</fixed-case>: Event-level Moral Opinions in News Articles</title>
      <author><first>Yuanyuan</first><last>Lei</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Md Messal Monem</first><last>Miah</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Ayesha</first><last>Qamar</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Sai Ramana Reddy</first><last/></author>
      <author><first>Jonathan</first><last>Tong</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Haotian</first><last>Xu</last></author>
      <author><first>Ruihong</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <pages>5239-5251</pages>
      <abstract>Most previous research on moral frames has focused on social media short texts, little work has explored moral sentiment within news articles. In news articles, authors often express their opinions or political stance through moral judgment towards events, specifically whether the event is right or wrong according to social moral rules. This paper initiates a new task to understand moral opinions towards events in news articles. We have created a new dataset, EMONA, and annotated event-level moral opinions in news articles. This dataset consists of 400 news articles containing over 10k sentences and 45k events, among which 9,613 events received moral foundation labels. Extracting event morality is a challenging task, as moral judgment towards events can be very implicit. Baseline models were built for event moral identification and classification. In addition, we also conduct extrinsic evaluations to integrate event-level moral opinions into three downstream tasks. The statistical analysis and experiments show that moral opinions of events can serve as informative features for identifying ideological bias or subjective events.</abstract>
      <url hash="25523749">2024.naacl-long.293</url>
    </paper>
    <paper id="294">
      <title><fixed-case>DLM</fixed-case>: A Decoupled Learning Model for Long-tailed Polyphone Disambiguation in <fixed-case>M</fixed-case>andarin</title>
      <author><first>Beibei</first><last>Gao</last></author>
      <author><first>Yangsen</first><last>Zhang</last><affiliation>Beijing Information Science and Technology University</affiliation></author>
      <author><first>Ga</first><last>Xiang</last><affiliation>Beijing Information Science and Technology University</affiliation></author>
      <author><first>Yushan</first><last>Jiang</last></author>
      <pages>5252-5262</pages>
      <abstract>Grapheme-to-phoneme conversion (G2P) is a critical component of the text-to-speech system (TTS), where polyphone disambiguation is the most crucial task. However, polyphone disambiguation datasets often suffer from the long-tail problem, and context learning for polyphonic characters commonly stems from a single dimension. In this paper, we propose a novel model DLM: a Decoupled Learning Model for long-tailed polyphone disambiguation in Mandarin. Firstly, DLM decouples representation and classification learnings. It can apply different data samplers for each stage to obtain an optimal training data distribution. This can mitigate the long-tail problem. Secondly, two improved attention mechanisms and a gradual conversion strategy are integrated into the DLM, which achieve transition learning of context from local to global. Finally, to evaluate the effectiveness of DLM, we construct a balanced polyphone disambiguation corpus via in-context learning. Experiments on the benchmark CPP dataset demonstrate that DLM achieves a boosted accuracy of 99.07%. Moreover, DLM improves the disambiguation performance of long-tailed polyphonic characters. For many long-tailed characters, DLM even achieves an accuracy of 100%.</abstract>
      <url hash="2eb7f3a1">2024.naacl-long.294</url>
    </paper>
    <paper id="295">
      <title>You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments</title>
      <author><first>Bangzhao</first><last>Shu</last></author>
      <author><first>Lechen</first><last>Zhang</last></author>
      <author><first>Minje</first><last>Choi</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Lavinia</first><last>Dunagan</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Lajanugen</first><last>Logeswaran</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Moontae</first><last>Lee</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Dallas</first><last>Card</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>David</first><last>Jurgens</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <pages>5263-5281</pages>
      <abstract>The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs’ capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experiments on 17 different LLMs reveal that even simple perturbations significantly downgrade a model’s question-answering ability, and that most LLMs have low negation consistency. Our results suggest that the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, and we therefore discuss potential alternatives to improve these issues.</abstract>
      <url hash="54c40cb9">2024.naacl-long.295</url>
    </paper>
    <paper id="296">
      <title><fixed-case>CASA</fixed-case>: Causality-driven Argument Sufficiency Assessment</title>
      <author><first>Xiao</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>5282-5302</pages>
      <abstract>The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion.To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the definition of probability of sufficiency (PS) in the causal literature, we proposeCASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion and revise them by injecting the premise event.Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments. Code and data are available at https://github.com/xxxiaol/CASA.</abstract>
      <url hash="124d472d">2024.naacl-long.296</url>
    </paper>
    <paper id="297">
      <title><fixed-case>M</fixed-case>ac<fixed-case>G</fixed-case>yver: Are Large Language Models Creative Problem Solvers?</title>
      <author><first>Yufei</first><last>Tian</last></author>
      <author><first>Abhilasha</first><last>Ravichander</last><affiliation>Allen Institute for Artificial Intelligence and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Lianhui</first><last>Qin</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Raja</first><last>Marjieh</last><affiliation>Princeton University</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Thomas</first><last>Griffiths</last><affiliation>Princeton University</affiliation></author>
      <author><first>Faeze</first><last>Brahman</last><affiliation>Allen Institute for AI</affiliation></author>
      <pages>5303-5324</pages>
      <abstract>We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking.This work (1) introduces a fresh arena for intelligent agents focusing on intricate aspects of physical reasoning, planning, and unconventional thinking, which supplements the existing spectrum of machine intelligence; and (2) provides insight into the constrained problem-solving capabilities of both humans and AI.</abstract>
      <url hash="66875d78">2024.naacl-long.297</url>
    </paper>
    <paper id="298">
      <title>To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages</title>
      <author><first>Benedikt</first><last>Ebing</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>5325-5344</pages>
      <abstract>Perfect machine translation (MT) would render cross-lingual transfer (XLT) by means of multilingual language models (mLMs) superfluous. Given, on the one hand, the large body of work on improving XLT with mLMs and, on the other hand, recent advances in massively multilingual MT, in this work, we systematically evaluate existing and propose new translation-based XLT approaches for transfer to low-resource languages. We show that all translation-based approaches dramatically outperform zero-shot XLT with mLMs—with the combination of round-trip translation of the source-language training data and the translation of the target-language test instances at inference—being generally the most effective. We next show that one can obtain further empirical gains by adding reliable translations to other high-resource languages to the training data. Moreover, we propose an effective translation-based XLT strategy even for languages not supported by the MT system. Finally, we show that model selection for XLT based on target-language validation data obtained with MT outperforms model selection based on the source-language data. We believe our findings warrant a broader inclusion of more robust translation-based baselines in XLT research.</abstract>
      <url hash="b5f76d9e">2024.naacl-long.298</url>
    </paper>
    <paper id="299">
      <title>Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting</title>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Hongru</first><last>Wang</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Yi</first><last>Chen</last></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>5345-5363</pages>
      <abstract>Numerous works are proposed to align large language models (LLMs) with human intents to better fulfill instructions, ensuring they are trustful and helpful.Nevertheless, some human instructions are often malicious or misleading and following them will lead to untruthful and unsafe responses.Previous work rarely focused on understanding how LLMs manage instructions based on counterfactual premises, referred to here as inductive instructions, which may stem from users’ false beliefs or malicious intents.In this paper, we aim to reveal the behaviors of LLMs towards inductive instructions and enhance their truthfulness and helpfulness accordingly. Specifically, we first introduce a benchmark of Inductive Instructions (INDust), where the false knowledge is incorporated into instructions in multiple different styles. After extensive human and automatic evaluations, we uncovered a universal vulnerability among LLMs in processing inductive instructions.Additionally, we identified that different inductive styles affect the models’ ability to identify the same underlying errors,and the complexity of the underlying assumptions also influences the model’s performance.Motivated by these results, we propose Dual-critique prompting to improve LLM robustness against inductive instructions.Our experiments demonstrate that Dual-critique prompting significantly bolsters the robustness of a diverse array of LLMs, even when confronted with varying degrees of inductive instruction complexity and differing inductive styles.</abstract>
      <url hash="172fbbee">2024.naacl-long.299</url>
    </paper>
    <paper id="300">
      <title><fixed-case>GL</fixed-case>i<fixed-case>NER</fixed-case>: Generalist Model for Named Entity Recognition using Bidirectional Transformer</title>
      <author><first>Urchade</first><last>Zaratiana</last></author>
      <author><first>Nadi</first><last>Tomeh</last><affiliation>Université Sorbonne Paris Nord</affiliation></author>
      <author><first>Pierre</first><last>Holat</last></author>
      <author><first>Thierry</first><last>Charnois</last><affiliation>University of Sorbonne Paris Nord (Paris 13)</affiliation></author>
      <pages>5364-5376</pages>
      <abstract>Named Entity Recognition (NER) is essential in various Natural Language Processing (NLP) applications. Traditional NER models are effective but limited to a set of predefined entity types. In contrast, Large Language Models (LLMs) can extract arbitrary entities through natural language instructions, offering greater flexibility. However, their size and cost, particularly for those accessed via APIs like ChatGPT, make them impractical in resource-limited scenarios. In this paper, we introduce a compact NER model trained to identify any type of entity. Leveraging a bidirectional transformer encoder, our model, GLiNER, facilitates parallel entity extraction, an advantage over the slow sequential token generation of LLMs. Through comprehensive testing, GLiNER demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs in zero-shot evaluations on various NER benchmarks.</abstract>
      <url hash="85ad8d33">2024.naacl-long.300</url>
    </paper>
    <paper id="301">
      <title><fixed-case>XST</fixed-case>est: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models</title>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Hannah</first><last>Kirk</last><affiliation>University of Oxford and Alan Turing Institute</affiliation></author>
      <author><first>Bertie</first><last>Vidgen</last><affiliation>Alan Turing Institute</affiliation></author>
      <author><first>Giuseppe</first><last>Attanasio</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Federico</first><last>Bianchi</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <pages>5377-5400</pages>
      <abstract>Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest’s creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.</abstract>
      <url hash="eb8a2975">2024.naacl-long.301</url>
    </paper>
    <paper id="302">
      <title>Carpe diem: On the Evaluation of World Knowledge in Lifelong Language Models</title>
      <author><first>Yujin</first><last>Kim</last></author>
      <author><first>Jaehong</first><last>Yoon</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Seonghyeon</first><last>Ye</last></author>
      <author><first>Sangmin</first><last>Bae</last></author>
      <author><first>Namgyu</first><last>Ho</last></author>
      <author><first>Sung Ju</first><last>Hwang</last><affiliation>Korea Advanced Institute of Science and Technology and AITRICS</affiliation></author>
      <author><first>Se-Young</first><last>Yun</last><affiliation>KAIST</affiliation></author>
      <pages>5401-5415</pages>
      <abstract>The dynamic nature of knowledge in an ever-changing world presents challenges for language models trained on static data; the model in the real world often requires not only acquiring new knowledge but also overwriting outdated information into updated ones. To study the ability of language models for these time-dependent dynamics in human language, we introduce a novel task, EvolvingQA, a temporally evolving question-answering benchmark designed for training and evaluating LMs on an evolving Wikipedia database. The construction of EvolvingQA is automated with our pipeline using large language models. We uncover that existing continual learning baselines suffer from updating and removing outdated knowledge. Our analysis suggests that models fail to rectify knowledge due to small weight gradients. In addition, we elucidate that language models particularly struggle to reflect the change of numerical or temporal information. Our work aims to model the dynamic nature of real-world information, suggesting faithful evaluations of the evolution-adaptability of language models. Our data construction code and dataset files are available at https://github.com/kimyuji/EvolvingQA_benchmark.</abstract>
      <url hash="68eb7a41">2024.naacl-long.302</url>
    </paper>
    <paper id="303">
      <title>Fine-grained Gender Control in Machine Translation with Large Language Models</title>
      <author><first>Minwoo</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyukhun</first><last>Koh</last></author>
      <author><first>Minsung</first><last>Kim</last></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>5416-5430</pages>
      <abstract>In machine translation, the problem of ambiguously gendered input has been pointed out, where the gender of an entity is not available in the source sentence. To address this ambiguity issue, the task of controlled translation that takes the gender of the ambiguous entity as additional input have been proposed. However, most existing works have only considered a simplified setup of one target gender for input. In this paper, we tackle controlled translation in a more realistic setting of inputs with multiple entities and propose Gender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs the model with fine-grained entity-level gender information to translate with correct gender inflections. By utilizing four evaluation benchmarks, we investigate the controlled translation capability of LLMs in multiple dimensions and find that LLMs reach state-of-the-art performance in controlled translation. Furthermore, we discover an emergence of gender interference phenomenon when controlling the gender of multiple entities. Finally, we address the limitations of existing gender accuracy evaluation metrics and propose leveraging LLMs as an evaluator for gender inflection in machine translation.</abstract>
      <url hash="400e1df9">2024.naacl-long.303</url>
    </paper>
    <paper id="304">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>VCS</fixed-case>: Robust Natural Language Understanding in Dialogue System Upgrade</title>
      <author><first>Zefan</first><last>Cai</last></author>
      <author><first>Xin</first><last>Zheng</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Haoran</first><last>Meng</last></author>
      <author><first>Jiaqi</first><last>Han</last><affiliation>Tencent Cloud</affiliation></author>
      <author><first>Gang</first><last>Yuan</last></author>
      <author><first>Binghuai</first><last>Lin</last><affiliation>Tencent</affiliation></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <author><first>Yunbo</first><last>Cao</last><affiliation>Tencent</affiliation></author>
      <pages>5431-5452</pages>
      <abstract>In the constant updates of the product dialogue systems, we need to retrain the natural language understanding (NLU) model as new data from the real users would be merged into the existing data accumulated in the last updates. Within the newly added data, new intents would emerge and might have semantic entanglement with the existing intents, e.g. new intents that are semantically too specific or generic are actually a subset or superset of some existing intents in the semantic space, thus impairing the robustness of the NLU model.As the first attempt to solve this problem, we setup a new benchmark consisting of 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent detection with imperfect data in the system update as a multi-label classification task with positive but unlabeled intents, which asks the models to recognize all the proper intents, including the ones with semantic entanglement, in the inference.We also propose comprehensive baseline models and conduct in-depth analyses for the benchmark, showing that the semantically entangled intents can be effectively recognized with an automatic workflow. Our code and dataset are available at <url>https://github.com/Zefan-Cai/DialogVCS</url>.</abstract>
      <url hash="f7de35cb">2024.naacl-long.304</url>
    </paper>
    <paper id="305">
      <title><fixed-case>LL</fixed-case>atrieval: <fixed-case>LLM</fixed-case>-Verified Retrieval for Verifiable Generation</title>
      <author><first>Xiaonan</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Changtai</first><last>Zhu</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Zhangyue</first><last>Yin</last></author>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>5453-5471</pages>
      <abstract>Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM’s output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM’s output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM’s remarkable abilities. To address these limitations, we propose **LLatrieval** (**L**arge **La**nguage Model Verified Re**trieval**),where the LLM updates the retrieval result until it verifies that the retrieved documents can sufficiently support answering the question. Thus, the LLM can iteratively provide feedback to retrieval and facilitate the retrieval result to fully support verifiable generation. Experiments on ALCE show that LLatrieval significantly outperforms extensive baselines and achieves state-of-the-art results.</abstract>
      <url hash="4559346b">2024.naacl-long.305</url>
    </paper>
    <paper id="306">
      <title>Mapping Long-term Causalities in Psychiatric Symptomatology and Life Events from Social Media</title>
      <author><first>Siyuan</first><last>Chen</last></author>
      <author><first>Meilin</first><last>Wang</last></author>
      <author><first>Minghao</first><last>Lv</last></author>
      <author><first>Zhiling</first><last>Zhang</last></author>
      <author><first>Juqianqian</first><last>Juqianqian</last></author>
      <author><first>Dejiyangla</first><last>Dejiyangla</last></author>
      <author><first>Yujia</first><last>Peng</last><affiliation>Peking University</affiliation></author>
      <author><first>Kenny</first><last>Zhu</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Mengyue</first><last>Wu</last></author>
      <pages>5472-5487</pages>
      <abstract>Social media is a valuable data source for exploring mental health issues. However, previous studies have predominantly focused on the semantic content of these posts, overlooking the importance of their temporal attributes, as well as the evolving nature of mental disorders and symptoms.In this paper, we study the causality between psychiatric symptoms and life events, as well as among different symptoms from social media posts, which leads to better understanding of the underlying mechanisms of mental disorders. By applying these extracted causality features to tasks such as diagnosis point detection and early risk detection of depression, we notice considerable performance enhancement. This indicates that causality information extracted from social media data can boost the efficacy of mental disorder diagnosis and treatment planning.</abstract>
      <url hash="6fcf032c">2024.naacl-long.306</url>
    </paper>
    <paper id="307">
      <title>Multimodal Chart Retrieval: A Comparison of Text, Table and Image Based Approaches</title>
      <author><first>Averi</first><last>Nowak</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Francesco</first><last>Piccinno</last><affiliation>Google</affiliation></author>
      <author><first>Yasemin</first><last>Altun</last><affiliation>Research, Google</affiliation></author>
      <pages>5488-5505</pages>
      <abstract>We investigate multimodal chart retrieval, addressing the challenge of retrieving image-based charts using textual queries. We compare four approaches: (a) OCR with text retrieval, (b) chart derendering (DePlot) followed by table retrieval, (c) a direct image understanding model (PaLI-3), and (d) a combined PaLI-3 + DePlot approach. As the table retrieval component we introduce Tab-GTR, a text retrieval model augmented with table structure embeddings, achieving state-of-the-art results on the NQ-Tables benchmark with 48.88% R@1. On in-distribution data, the DePlot-based method (b) outperforms PaLI-3 (c), while being significantly more efficient (300M vs 3B trainable parameters). However, DePlot struggles with complex charts, indicating a need for improvements in chart derendering - specifically in terms of chart data diversity and the richness of text/table representations. We found no clear winner between methods (b) and (c) in general, with the best performance achieved by the combined approach (d), and further show that it benefits the most from multi-task training.</abstract>
      <url hash="8363601d">2024.naacl-long.307</url>
    </paper>
    <paper id="308">
      <title>Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models</title>
      <author><first>Seiji</first><last>Maekawa</last><affiliation>Megagon Labs, US</affiliation></author>
      <author><first>Hayate</first><last>Iso</last><affiliation>Megagon Labs, US</affiliation></author>
      <author><first>Sairam</first><last>Gurajada</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Nikita</first><last>Bhutani</last><affiliation>Megagon Labs, Inc</affiliation></author>
      <pages>5506-5521</pages>
      <abstract>While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity. Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers. Interestingly, they can effectively retain popular relations of less common entities. We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question.</abstract>
      <url hash="d49adfd4">2024.naacl-long.308</url>
    </paper>
    <paper id="309">
      <title><fixed-case>A</fixed-case>udio<fixed-case>C</fixed-case>hat<fixed-case>L</fixed-case>lama: Towards General-Purpose Speech Abilities for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yassir</first><last>Fathullah</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Chunyang</first><last>Wu</last></author>
      <author><first>Egor</first><last>Lakomkin</last><affiliation>Facebook</affiliation></author>
      <author><first>Ke</first><last>Li</last><affiliation>Meta</affiliation></author>
      <author><first>Junteng</first><last>Jia</last></author>
      <author><first>Yuan</first><last>Shangguan</last><affiliation>Current: Google</affiliation></author>
      <author><first>Jay</first><last>Mahadeokar</last></author>
      <author><first>Ozlem</first><last>Kalinli</last></author>
      <author><first>Christian</first><last>Fuegen</last><affiliation>Facebook/ Meta</affiliation></author>
      <author><first>Mike</first><last>Seltzer</last><affiliation>Meta</affiliation></author>
      <pages>5522-5532</pages>
      <abstract>In this work, we extend the instruction-tuned Llama-2 model with end-to-end general-purpose speech processing and reasoning abilities while maintaining the wide range of original LLM capabilities, without using any carefully curated paired data. The resulting end-to-end model, named <i>AudioChatLlama</i>, can utilize audio prompts as a replacement for text and sustain a conversation. Such a model also has extended cross-modal capabilities such as being able to perform spoken question answering (QA), speech translation, and audio summarization amongst many other closed and open-domain tasks. This is unlike prior approaches in speech, in which LLMs are extended to handle audio for a limited number of pre-designated tasks. On both synthesized and recorded speech QA test sets, evaluations show that our end-to-end approach is on par with or outperforms cascaded systems (speech recognizer + LLM) in terms of modelling the response to a prompt. Furthermore, unlike cascades, our approach can interchange text and audio modalities and intrinsically utilize prior context in a conversation to provide better results.</abstract>
      <url hash="04d7418c">2024.naacl-long.309</url>
    </paper>
    <paper id="310">
      <title>Whispers of Doubt Amidst Echoes of Triumph in <fixed-case>NLP</fixed-case> Robustness</title>
      <author><first>Ashim</first><last>Gupta</last></author>
      <author><first>Rishanth</first><last>Rajendhran</last></author>
      <author><first>Nathan</first><last>Stringham</last><affiliation>University of Utah</affiliation></author>
      <author><first>Vivek</first><last>Srikumar</last><affiliation>University of Utah</affiliation></author>
      <author><first>Ana</first><last>Marasovic</last><affiliation>University of Utah</affiliation></author>
      <pages>5533-5590</pages>
      <abstract>*Do larger and more performant models resolve NLP’s longstanding robustness issues?* We investigate this question using over 20 models of different sizes spanning different architectural choices and pretraining objectives. We conduct evaluations using (a) out-of-domain and challenge test sets, (b) behavioral testing with CheckLists, (c) contrast sets, and (d) adversarial inputs. Our analysis reveals that not all out-of-domain tests provide insight into robustness. Evaluating with CheckLists and contrast sets shows significant gaps in model performance; merely scaling models does not make them adequately robust. Finally, we point out that current approaches for adversarial evaluations of models are themselves problematic: they can be easily thwarted, and in their current forms, do not represent a sufficiently deep probe of model robustness. We conclude that not only is the question of robustness in NLP as yet unresolved, but even some of the approaches to measure robustness need to be reassessed.</abstract>
      <url hash="a97429f7">2024.naacl-long.310</url>
    </paper>
    <paper id="311">
      <title>Sequential Compositional Generalization in Multimodal Models</title>
      <author><first>Semih</first><last>Yagcioglu</last><affiliation>Apziva</affiliation></author>
      <author><first>Osman Batur</first><last>İnce</last></author>
      <author><first>Aykut</first><last>Erdem</last><affiliation>Koç University</affiliation></author>
      <author><first>Erkut</first><last>Erdem</last><affiliation>Hacettepe University</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>and University of Copenhagen</affiliation></author>
      <author><first>Deniz</first><last>Yuret</last><affiliation>Koc University</affiliation></author>
      <pages>5591-5611</pages>
      <abstract>The rise of large-scale multimodal models has paved the pathway for groundbreaking advances in generative modeling and reasoning, unlocking transformative applications in a variety of complex tasks. However, a pressing question that remains is their genuine capability for stronger forms of generalization, which has been largely underexplored in the multimodal setting. Our study aims to address this by examining sequential compositional generalization using CompAct (Compositional Activities), a carefully constructed, perceptually grounded dataset set within a rich backdrop of egocentric kitchen activity videos. Each instance in our dataset is represented with a combination of raw video footage, naturally occurring sound, and crowd-sourced step-by-step descriptions. More importantly, our setup ensures that the individual concepts are consistently distributed across training and evaluation sets, while their compositions are novel in the evaluation set. We conduct a comprehensive assessment of several unimodal and multimodal models. Our findings reveal that bi-modal and tri-modal models exhibit a clear edge over their text-only counterparts. This highlights the importance of multimodality while charting a trajectory for future research in this domain.</abstract>
      <url hash="c60ddd31">2024.naacl-long.311</url>
    </paper>
    <paper id="312">
      <title>Generating Uncontextualized and Contextualized Questions for Document-Level Event Argument Extraction</title>
      <author><first>Md Nayem</first><last>Uddin</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Enfa</first><last>George</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Steven</first><last>Corman</last></author>
      <pages>5612-5627</pages>
      <abstract>This paper presents multiple question generation strategies for document-level event argument extraction. These strategies do not require human involvement and result in uncontextualized questions as well as contextualized questions grounded on the event and document of interest. Experimental results show that combining uncontextualized and contextualized questions is beneficial,especially when event triggers and arguments appear in different sentences. Our approach does not have corpus-specific components, in particular, the question generation strategies transfer across corpora. We also present a qualitative analysis of the most common errors made by our best model.</abstract>
      <url hash="32b0b675">2024.naacl-long.312</url>
    </paper>
    <paper id="313">
      <title>Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation</title>
      <author><first>Zhenrui</first><last>Yue</last></author>
      <author><first>Huimin</first><last>Zeng</last></author>
      <author><first>Yimeng</first><last>Lu</last></author>
      <author><first>Lanyu</first><last>Shang</last></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Dong</first><last>Wang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>5628-5643</pages>
      <abstract>The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, text generation approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and rerank evidence documents using a database comprising over 1M academic articles; (2) response generation, in which we align large language models (LLMs) to generate evidence-based responses via reinforcement learning from human feedback (RLHF). We propose a reward function to maximize the utilization of the retrieved evidence while maintaining the quality of the generated text, which yields polite and factual responses that clearly refutes misinformation. To demonstrate the effectiveness of our method, we study the case of COVID-19 and perform extensive experiments with both in- and cross-domain datasets, where RARG consistently outperforms baselines by generating high-quality counter-misinformation responses.</abstract>
      <url hash="fee19618">2024.naacl-long.313</url>
    </paper>
    <paper id="314">
      <title>Open-Vocabulary Federated Learning with Multimodal Prototyping</title>
      <author><first>Huimin</first><last>Zeng</last></author>
      <author><first>Zhenrui</first><last>Yue</last></author>
      <author><first>Dong</first><last>Wang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>5644-5656</pages>
      <abstract>Existing federated learning (FL) studies usuallyassume the training label space and test labelspace are identical. However, in real-world applications, this assumption is too ideal to betrue. A new user could come up with queriesthat involve data from unseen classes, and suchopen-vocabulary queries would directly defectsuch FL systems. Therefore, in this work, weexplicitly focus on the under-explored openvocabulary challenge in FL. That is, for a newuser, the global server shall understand her/hisquery that involves arbitrary unknown classes.To address this problem, we leverage the pretrained vision-language models (VLMs). Inparticular, we present a novel adaptation framework tailored for VLMs in the context of FL,named as Federated Multimodal Prototyping(Fed-MP). Fed-MP adaptively aggregates thelocal model weights based on light-weightclient residuals, and makes predictions basedon a novel multimodal prototyping mechanism.Fed-MP exploits the knowledge learned fromthe seen classes, and robustifies the adaptedVLM to unseen categories. Our empirical evaluation on various datasets validates the effectiveness of Fed-MP.</abstract>
      <url hash="18182b5b">2024.naacl-long.314</url>
    </paper>
    <paper id="315">
      <title>Exploring Key Point Analysis with Pairwise Generation and Graph Partitioning</title>
      <author><first>Xiao</first><last>Li</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Shen</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Gong</first><last>Cheng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>5657-5667</pages>
      <abstract>Key Point Analysis (KPA), the summarization of multiple arguments into a concise collection of key points, continues to be a significant and unresolved issue within the field of argument mining. Existing models adapt a two-stage pipeline of clustering arguments or generating key points for argument clusters. This approach rely on semantic similarity instead of measuring the existence of shared key points among arguments. Additionally, it only models the intra-cluster relationship among arguments, disregarding the inter-cluster relationship between arguments that do not share key points. To address these limitations, we propose a novel approach for KPA with pairwise generation and graph partitioning. Our objective is to train a generative model that can simultaneously provide a score indicating the presence of shared key point between a pair of arguments and generate the shared key point. Subsequently, to map generated redundant key points to a concise set of key points, we proceed to construct an arguments graph by considering the arguments as vertices, the generated key points as edges, and the scores as edge weights. We then propose a graph partitioning algorithm to partition all arguments sharing the same key points to the same subgraph. Notably, our experimental findings demonstrate that our proposed model surpasses previous models when evaluated on both the ArgKP and QAM datasets.</abstract>
      <url hash="bb70d129">2024.naacl-long.315</url>
    </paper>
    <paper id="316">
      <title>Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense</title>
      <author><first>Siqi</first><last>Shen</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Lajanugen</first><last>Logeswaran</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Moontae</first><last>Lee</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Honglak</first><last>Lee</last><affiliation>University of Michigan - Ann Arbor and LG AI Research</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>5668-5680</pages>
      <abstract>Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper, we conduct a comprehensive examination of the capabilities and limitations of several state-of-the-art LLMs in the context of cultural commonsense tasks. Using several general and cultural commonsense benchmarks, we find that (1) LLMs have a significant discrepancy in performance when tested on culture-specific commonsense knowledge for different cultures; (2) LLMs’ general commonsense capability is affected by cultural context; and (3) The language used to query the LLMs can impact their performance on cultural-related tasks.Our study points to the inherent bias in the cultural understanding of LLMs and provides insights that can help develop culturally-aware language models.</abstract>
      <url hash="06c4bdb3">2024.naacl-long.316</url>
    </paper>
    <paper id="317">
      <title>Code Models are Zero-shot Precondition Reasoners</title>
      <author><first>Lajanugen</first><last>Logeswaran</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Sungryull</first><last>Sohn</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Yiwei</first><last>Lyu</last></author>
      <author><first>Anthony</first><last>Liu</last></author>
      <author><first>Dong-Ki</first><last>Kim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Dongsub</first><last>Shim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Moontae</first><last>Lee</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Honglak</first><last>Lee</last><affiliation>University of Michigan - Ann Arbor and LG AI Research</affiliation></author>
      <pages>5681-5697</pages>
      <abstract>One of the fundamental skills required for an agent acting in an environment to complete tasks is the ability to understand what actions are plausible at any given point. This work explores a novel use of code representations to reason about action preconditions for sequential decision making tasks. Code representations offer the flexibility to model procedural activities and associated constraints as well as the ability to execute and verify constraint satisfaction. Leveraging code representations, we extract action preconditions from demonstration trajectories in a zero-shot manner using pre-trained code models. Given these extracted preconditions, we propose a precondition-aware action sampling strategy that ensures actions predicted by a policy are consistent with preconditions. We demonstrate that the proposed approach enhances the performance of few-shot policy learning approaches across task-oriented dialog and embodied textworld benchmarks.</abstract>
      <url hash="0fd8c169">2024.naacl-long.317</url>
    </paper>
    <paper id="318">
      <title>Contrastive and Consistency Learning for Neural Noisy-Channel Model in Spoken Language Understanding</title>
      <author><first>Suyoung</first><last>Kim</last></author>
      <author><first>Jiyeon</first><last>Hwang</last><affiliation>Kyungpook National University</affiliation></author>
      <author><first>Ho-Young</first><last>Jung</last><affiliation>Kyungpook National University</affiliation></author>
      <pages>5698-5711</pages>
      <abstract>Recently, deep end-to-end learning has been studied for intent classification in Spoken Language Understanding (SLU). However, end-to-end models require a large amount of speech data with intent labels, and highly optimized models are generally sensitive to the inconsistency between the training and evaluation conditions. Therefore, a natural language understanding approach based on Automatic Speech Recognition (ASR) remains attractive because it can utilize a pre-trained general language model and adapt to the mismatch of the speech input environment. Using this module-based approach, we improve a noisy-channel model to handle transcription inconsistencies caused by ASR errors. We propose a two-stage method, Contrastive and Consistency Learning (CCL), that correlates error patterns between clean and noisy ASR transcripts and emphasizes the consistency of the latent features of the two transcripts. Experiments on four benchmark datasets show that CCL outperforms existing methods and improves the ASR robustness in various noisy environments. Code is available at https://github.com/syoung7388/CCL</abstract>
      <url hash="d38ba45d">2024.naacl-long.318</url>
    </paper>
    <paper id="319">
      <title>Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of <fixed-case>LLM</fixed-case>s as Rankers</title>
      <author><first>Yuan</first><last>Wang</last></author>
      <author><first>Xuyang</first><last>Wu</last></author>
      <author><first>Hsin-Tai</first><last>Wu</last></author>
      <author><first>Zhiqiang</first><last>Tao</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author><first>Yi</first><last>Fang</last><affiliation>Santa Clara University</affiliation></author>
      <pages>5712-5724</pages>
      <abstract>The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works such as RankGPT have demonstrated that the LLMs have better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.</abstract>
      <url hash="f4deaec9">2024.naacl-long.319</url>
    </paper>
    <paper id="320">
      <title><fixed-case>T</fixed-case>ab<fixed-case>SQL</fixed-case>ify: Enhancing Reasoning Capabilities of <fixed-case>LLM</fixed-case>s Through Table Decomposition</title>
      <author><first>Md</first><last>Nahid</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Davood</first><last>Rafiei</last><affiliation>University of Alberta</affiliation></author>
      <pages>5725-5737</pages>
      <abstract>Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data. Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, but they often struggle with large tables due to their limited input length. In this paper, we propose TabSQLify, a novel method that leverages text-to-SQL generation to decompose tables into smaller and relevant sub-tables, containing only essential information for answering questions or verifying statements, before performing the reasoning task. In our comprehensive evaluation on four challenging datasets, our approach demonstrates comparable or superior performance compared to prevailing methods reliant on full tables as input. Moreover, our method can reduce the input context length significantly, making it more scalable and efficient for large-scale table reasoning applications. Our method performs remarkably well on the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the TabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass other LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can reduce the table size significantly alleviating the computational load on LLMs when handling large tables without compromising performance.</abstract>
      <url hash="89f24d82">2024.naacl-long.320</url>
    </paper>
    <paper id="321">
      <title>Contextual Label Projection for Cross-Lingual Structured Prediction</title>
      <author><first>Tanmay</first><last>Parekh</last></author>
      <author><first>I-Hung</first><last>Hsu</last></author>
      <author><first>Kuan-Hao</first><last>Huang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>5738-5757</pages>
      <abstract>Label projection, which involves obtaining translated labels and texts jointly, is essential for leveraging machine translation to facilitate cross-lingual transfer in structured prediction tasks. Prior research exploring label projection often compromise translation accuracy by favoring simplified label translation or relying solely on word-level alignments. In this paper, we introduce a novel label projection approach, CLaP, which translates text to the target language and performs *contextual translation* on the labels using the translated text as the context, ensuring better accuracy for the translated labels. We leverage instruction-tuned language models with multilingual capabilities as our contextual translator, imposing the constraint of the presence of translated labels in the translated text via instructions. We benchmark CLaP with other label projection techniques on zero-shot cross-lingual transfer across 39 languages on two representative structured prediction tasks - event argument extraction (EAE) and named entity recognition (NER), showing over 2.4 F1 improvement for EAE and 1.4 F1 improvement for NER. We further explore the applicability of CLaP on ten extremely low-resource languages to showcase its potential for cross-lingual structured prediction.</abstract>
      <url hash="35ea1bbc">2024.naacl-long.321</url>
    </paper>
    <paper id="322">
      <title>Event Detection from Social Media for Epidemic Prediction</title>
      <author><first>Tanmay</first><last>Parekh</last></author>
      <author><first>Anh</first><last>Mac</last></author>
      <author><first>Jiarui</first><last>Yu</last></author>
      <author><first>Yuxuan</first><last>Dong</last></author>
      <author><first>Syed</first><last>Shahriar</last></author>
      <author><first>Bonnie</first><last>Liu</last></author>
      <author><first>Eric</first><last>Yang</last></author>
      <author><first>Kuan-Hao</first><last>Huang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>5758-5783</pages>
      <abstract>Social media is an easy-to-access platform providing timely updates about societal trends and events. Discussions regarding epidemic-related events such as infections, symptoms, and social interactions can be crucial for informing policymaking during epidemic outbreaks. In our work, we pioneer exploiting Event Detection (ED) for better preparedness and early warnings of any upcoming epidemic by developing a framework to extract and analyze epidemic-related events from social media posts. To this end, we curate an epidemic event ontology comprising seven disease-agnostic event types and construct a Twitter dataset SPEED with human-annotated events focused on the COVID-19 pandemic. Experimentation reveals how ED models trained on COVID-based SPEED can effectively detect epidemic events for three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained on existing ED datasets fail miserably. Furthermore, we show that reporting sharp increases in the extracted events by our framework can provide warnings 4-9 weeks earlier than the WHO epidemic declaration for Monkeypox. This utility of our framework lays the foundations for better preparedness against emerging epidemics.</abstract>
      <url hash="7f073854">2024.naacl-long.322</url>
    </paper>
    <paper id="323">
      <title><fixed-case>RESPROMPT</fixed-case>: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models</title>
      <author><first>Song</first><last>Jiang</last></author>
      <author><first>Zahra</first><last>Shakeri</last></author>
      <author><first>Aaron</first><last>Chan</last><affiliation>Meta AI</affiliation></author>
      <author><first>Maziar</first><last>Sanjabi</last><affiliation>Meta</affiliation></author>
      <author><first>Hamed</first><last>Firooz</last><affiliation>Facebook</affiliation></author>
      <author><first>Yinglong</first><last>Xia</last><affiliation>Meta</affiliation></author>
      <author><first>Bugra</first><last>Akyildiz</last><affiliation>New York University and Bilkent University</affiliation></author>
      <author><first>Yizhou</first><last>Sun</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Jinchao</first><last>Li</last><affiliation>Facebook</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Asli</first><last>Celikyilmaz</last><affiliation>FAIR</affiliation></author>
      <pages>5784-5809</pages>
      <abstract>Chain-of-thought (CoT) has impressively unlocked the reasoning potential of large language models (LLMs). Yet, it falls short when tackling problems that require multiple reasoning steps. This limitation arises from the complex nature of multi-step reasoning processes: later stages often depend not only on the immediately preceding step, but also on the results from several steps earlier. Such complexities indicate the reasoning process is naturally a graph. The almost linear structure of CoT, however, struggles to capture this complex reasoning graph. To address this challenge, we propose Residual Connection Prompting (ResPrompt), a new prompting strategy that advances multi-step reasoning in LLMs. The core of our idea is to reconstruct the reasoning graph within prompts. We achieve this by integrating necessary connections–links present in reasoning graph but missing in the linear CoT flow–into the prompts. Termed “residual connections”, these links can transform linear CoT into the complex reasoning graphs that multi-step problems entail. On benchmarks across math, sequential, and commonsense domains, ResPrompt demonstrates clear improvements in multi-step reasoning compared with CoT. Through extensive ablation studies and analyses, we pinpoint how to effectively build residual connections and also identify situations where it might be unnecessary.</abstract>
      <url hash="9efb736b">2024.naacl-long.323</url>
    </paper>
    <paper id="324">
      <title><fixed-case>BPE</fixed-case>-knockout: Pruning Pre-existing <fixed-case>BPE</fixed-case> Tokenisers with Backwards-compatible Morphological Semi-supervision</title>
      <author><first>Thomas</first><last>Bauwens</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Pieter</first><last>Delobelle</last><affiliation>KU Leuven, KU Leuven</affiliation></author>
      <pages>5810-5832</pages>
      <abstract>Byte-pair encoding (BPE) has become the default subword tokeniser in language models (LMs), allowing the representation of an infinite space of text with a finite set of units. Yet, BPE training is unsupervised, receiving no explicit information about a language’s morphology. This results in a subword vocabulary wherein many units are a concatenation of partial morphemes, preventing their formation as tokens. This, in turn, causes consistent intra-word patterns to be displayed inconsistently to downstream models, and bloats the vocabulary, hence requiring unnecessary embedding storage. In this paper, we address this issue by identifying blameworthy BPE merges and removing the resulting subwords from the BPE vocabulary, without impeding further use of merges that relied on them. We find that our method, BPE-knockout, is effective at making BPE’s segmentation positions adhere better to derivational and compound boundaries in English, Dutch and German, and improves token-based tasks in Dutch RoBERTa models, indicating that a tokeniser’s adherence to morphology impacts downstream models. We demonstrate the latter not only by training LMs from scratch, but also by continuing the pre-training of existing LMs. This proves promising, showing that suboptimal tokenisers can be remedied whilst salvaging training cost of downstream LMs.</abstract>
      <url hash="7f9d87ba">2024.naacl-long.324</url>
    </paper>
    <paper id="325">
      <title>How are Prompts Different in Terms of Sensitivity?</title>
      <author><first>Sheng</first><last>Lu</last></author>
      <author><first>Hendrik</first><last>Schuff</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>5833-5856</pages>
      <abstract>In-context learning (ICL) has become one of the most popular learning paradigms. While there is a growing body of literature focusing on prompt engineering, there is a lack of systematic analysis comparing the effects of prompt techniques across different models and tasks. To address this, we present a comprehensive prompt analysis based on sensitivity. Our analysis reveals that sensitivity is an unsupervised proxy for model performance, as it exhibits a strong negative correlation with accuracy. We use gradient-based saliency scores to empirically demonstrate how different prompts affect the relevance of input tokens to the output, resulting in different levels of sensitivity. Furthermore, we introduce sensitivity-aware decoding which incorporates sensitivity estimation as a penalty term in the standard greedy decoding. We show that this approach is particularly helpful when information in the input is scarce. Our work provides a fresh perspective on the analysis of prompts, and contributes to a better understanding of the mechanism of ICL.</abstract>
      <url hash="514686c2">2024.naacl-long.325</url>
    </paper>
    <paper id="326">
      <title><fixed-case>LSTD</fixed-case>ial: Enhancing Dialogue Generation via Long- and Short-Term Measurement Feedback</title>
      <author><first>Guanghui</first><last>Ye</last><affiliation>Hunan University</affiliation></author>
      <author><first>Huan</first><last>Zhao</last><affiliation>Hunan University</affiliation></author>
      <author><first>Zixing</first><last>Zhang</last><affiliation>Hunan University</affiliation></author>
      <author><first>Xupeng</first><last>Zha</last><affiliation>Hunan University</affiliation></author>
      <author><first>Zhihua</first><last>Jiang</last></author>
      <pages>5857-5871</pages>
      <abstract>Generating high-quality responses is a key challenge for any open domain dialogue systems. However, even though there exist a variety of quality dimensions especially designed for dialogue evaluation (e.g., coherence and diversity scores), current dialogue systems rarely utilize them to guide the response generation during training. To alleviate this issue, we propose LSTDial (Long- and Short-Term Dialogue), a novel two-stage framework which generates and utilizes conversation evaluation as explicit feedback during training. Specifically, we fine-tune pre-trained dialogue systems through using turn-level quality feedback in the first stage and further train ever-improving dialogue agents through using dialogue-level quality feedback in the second stage. By using our approach on dialogue systems, capable of enabling dialogue generation with both short-term capabilities (generating more fluent, relevant and varied responses at the turn-level) and long-term capabilities (generating more coherent, engaging and informative responses at the dialogue-level). We implement LSTDial on four strong baseline models and experiment with two open-domain dialogue datasets. Experimental results show that LSTDial achieves significant improvement, enabling to generate better dialogue responses in terms of both human and automatic evaluation.</abstract>
      <url hash="2c8835d8">2024.naacl-long.326</url>
    </paper>
    <paper id="327">
      <title>The <fixed-case>ART</fixed-case> of <fixed-case>LLM</fixed-case> Refinement: Ask, Refine, and Trust</title>
      <author><first>Kumar</first><last>Shridhar</last></author>
      <author><first>Koustuv</first><last>Sinha</last><affiliation>Meta (FAIR)</affiliation></author>
      <author><first>Andrew</first><last>Cohen</last></author>
      <author><first>Tianlu</first><last>Wang</last><affiliation>Meta</affiliation></author>
      <author><first>Ping</first><last>Yu</last><affiliation>Facebook</affiliation></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Jason</first><last>Weston</last><affiliation>New York University and Facebook</affiliation></author>
      <author><first>Asli</first><last>Celikyilmaz</last><affiliation>FAIR</affiliation></author>
      <pages>5872-5883</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations and self-improve?A popular concept, referred to as *self-refinement*, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with a refinement strategy called *ART: Ask, Refine, and Trust*, which *asks* necessary questions to decide when an LLM should *refine* its output, and uses it to affirm or deny *trust* in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), *ART* achieves a performance gain of <tex-math>+5</tex-math> points over self-refinement baselines, while using a much smaller model as the decision maker. We believe that *ART* with smaller models, making refinement decisions can be a cost-effective alternative to fine-tuning LLMs.</abstract>
      <url hash="cccc3136">2024.naacl-long.327</url>
    </paper>
    <paper id="328">
      <title>Modularized Multilingual <fixed-case>NMT</fixed-case> with Fine-grained Interlingua</title>
      <author><first>Sungjun</first><last>Lim</last><affiliation>Samsung</affiliation></author>
      <author><first>Yoonjung</first><last>Choi</last><affiliation>Samsung</affiliation></author>
      <author><first>Sangha</first><last>Kim</last></author>
      <pages>5884-5899</pages>
      <abstract>Recently, one popular alternative in Multilingual NMT (MNMT) is modularized MNMT that has both language-specific encoders and decoders. However, due to the absence of layer-sharing, the modularized MNMT failed to produce satisfactory language-independent (Interlingua) features, leading to performance degradation in zero-shot translation. To address this issue, a solution was proposed to share the top of language-specific encoder layers, enabling the successful generation of interlingua features. Nonetheless, it should be noted that this sharing structure does not guarantee the explicit propagation of language-specific features to their respective language-specific decoders. Consequently, to overcome this challenge, we present our modularized MNMT approach, where a modularized encoder is divided into three distinct encoder modules based on different sharing criteria: (1) source language-specific (<tex-math>Enc_{s}</tex-math>); (2) universal (<tex-math>Enc_{all}</tex-math>); (3) target language-specific (<tex-math>Enc_{t}</tex-math>). By employing these sharing strategies, <tex-math>Enc_{all}</tex-math> propagates the interlingua features, after which <tex-math>Enc_{t}</tex-math> propagates the target language-specific features to the language-specific decoders. Additionally, we suggest the Denoising Bi-path Autoencoder (DBAE) to fortify the Denoising Autoencoder (DAE) by leveraging <tex-math>Enc_{t}</tex-math>. For experimental purposes, our training corpus comprises both En-to-Any and Any-to-En directions. We adjust the size of our corpus to simulate both balanced and unbalanced settings. Our method demonstrates an improved average BLEU score by "+2.90” in En-to-Any directions and by "+3.06” in zero-shot compared to other MNMT baselines.</abstract>
      <url hash="e606f463">2024.naacl-long.328</url>
    </paper>
    <paper id="329">
      <title><fixed-case>P</fixed-case>arallel<fixed-case>PARC</fixed-case>: A Scalable Pipeline for Generating Natural-Language Analogies</title>
      <author><first>Oren</first><last>Sultan</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Yonatan</first><last>Bitton</last><affiliation>Google</affiliation></author>
      <author><first>Ron</first><last>Yosef</last></author>
      <author><first>Dafna</first><last>Shahaf</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>5900-5924</pages>
      <abstract>Analogy-making is central to human cognition, allowing us to adapt to novel situations – an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy.In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs’ and humans’ analogy recognition in binary and multiple-choice settings, and found that humans outperform the best models (∼13% gap) after a light supervision. We demonstrate that our silver-set is useful for training models. Lastly, we show challenging distractors confuse LLMs, but not humans. We hope our pipeline will encourage research in this emerging field.</abstract>
      <url hash="f864a0d6">2024.naacl-long.329</url>
    </paper>
    <paper id="330">
      <title><fixed-case>AWESOME</fixed-case>: <fixed-case>GPU</fixed-case> Memory-constrained Long Document Summarization using Memory Mechanism and Global Salient Content</title>
      <author><first>Shuyang</first><last>Cao</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>5925-5941</pages>
      <abstract>Long document summarization systems are critical for domains with lengthy and jargon-laden text, yet they present significant challenges to researchers and developers with limited computing resources. Existing solutions mainly focus on efficient attentions or divide-and-conquer strategies. The former reduces theoretical time complexity, but is still memory-heavy. The latter methods sacrifice global context, leading to uninformative and incoherent summaries. This work aims to leverage the memory-efficient nature of divide-and-conquer methods while preserving global context. Concretely, our framework AWESOME uses two novel mechanisms: (1) External memory mechanisms track previously encoded document segments and their corresponding summaries, to enhance global document understanding and summary coherence. (2) Global salient content is further identified beforehand to augment each document segment to support its summarization. Extensive experiments on diverse genres of text, including government reports, meeting transcripts, screenplays, scientific papers, and novels, show that AWESOME produces summaries with improved informativeness, faithfulness, and coherence than competitive baselines on longer documents, while having a smaller GPU memory footprint.</abstract>
      <url hash="3fb77608">2024.naacl-long.330</url>
    </paper>
    <paper id="331">
      <title><fixed-case>NLP</fixed-case> Systems That Can’t Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps</title>
      <author><first>Kristina</first><last>Gligoric</last><affiliation>Stanford University</affiliation></author>
      <author><first>Myra</first><last>Cheng</last><affiliation>Stanford University</affiliation></author>
      <author><first>Lucia</first><last>Zheng</last></author>
      <author><first>Esin</first><last>Durmus</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>5942-5959</pages>
      <abstract>The use of words to convey speaker’s intent is traditionally distinguished from the ‘mention’ of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech. We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers ways to address it.</abstract>
      <url hash="1d2b846a">2024.naacl-long.331</url>
    </paper>
    <paper id="332">
      <title>Debiasing with Sufficient Projection: A General Theoretical Framework for Vector Representations</title>
      <author><first>Enze</first><last>Shi</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Lei</first><last>Ding</last></author>
      <author><first>Linglong</first><last>Kong</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Bei</first><last>Jiang</last><affiliation>University of Alberta</affiliation></author>
      <pages>5960-5975</pages>
      <abstract>Pre-trained vector representations in natural language processing often inadvertently encode undesirable social biases. Identifying and removing unwanted biased information from vector representation is an evolving and significant challenge. Our study uniquely addresses this issue from the perspective of statistical independence, proposing a framework for reducing bias by transforming vector representations to an unbiased subspace using sufficient projection. The key to our framework lies in its generality: it adeptly mitigates bias across both debiasing and fairness tasks, and across various vector representation types, including word embeddings and output representations of transformer models. Importantly, we establish the connection between debiasing and fairness, offering theoretical guarantees and elucidating our algorithm’s efficacy. Through extensive evaluation of intrinsic and extrinsic metrics, our method achieves superior performance in bias reduction while maintaining high task performance, and offers superior computational efficiency.</abstract>
      <url hash="b6a7270c">2024.naacl-long.332</url>
    </paper>
    <paper id="333">
      <title>Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection</title>
      <author><first>Jianfeng</first><last>He</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <author><first>Jason</first><last>Cai</last><affiliation>Amazon</affiliation></author>
      <author><first>Igor</first><last>Shalyminov</last><affiliation>Amazon</affiliation></author>
      <author><first>Hwanjun</first><last>Song</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <pages>5976-5996</pages>
      <abstract>Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks. Our code is available at <url>https://github.com/amazon-science/summarization-sicf-score</url>.</abstract>
      <url hash="bd5c5793">2024.naacl-long.333</url>
    </paper>
    <paper id="334">
      <title><fixed-case>A</fixed-case>fri<fixed-case>MTE</fixed-case> and <fixed-case>A</fixed-case>fri<fixed-case>COMET</fixed-case>: Enhancing <fixed-case>COMET</fixed-case> to Embrace Under-resourced <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Jiayi</first><last>Wang</last></author>
      <author><first>David</first><last>Adelani</last></author>
      <author><first>Sweta</first><last>Agrawal</last><affiliation>Instituto de Telecomunicações</affiliation></author>
      <author><first>Marek</first><last>Masiak</last></author>
      <author><first>Ricardo</first><last>Rei</last><affiliation>Instituto Superior Técnico, INESC-ID and Unbabel</affiliation></author>
      <author><first>Eleftheria</first><last>Briakou</last><affiliation>Google</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Sofia</first><last>Bourhim</last></author>
      <author><first>Andiswa</first><last>Bukula</last></author>
      <author><first>Muhidin</first><last>Mohamed</last><affiliation>Aston University</affiliation></author>
      <author><first>Temitayo</first><last>Olatoye</last></author>
      <author><first>Tosin</first><last>Adewumi</last></author>
      <author><first>Hamam</first><last>Mokayed</last><affiliation>Luleå University of Technology</affiliation></author>
      <author><first>Christine</first><last>Mwase</last><affiliation>Fudan University</affiliation></author>
      <author><first>Wangui</first><last>Kimotho</last></author>
      <author><first>Foutse</first><last>Yuehgoh</last></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <author><first>Jessica</first><last>Ojo</last><affiliation>Lelapa AI</affiliation></author>
      <author><first>Shamsuddeen</first><last>Muhammad</last><affiliation>Bayero University, Kano-Nigeria</affiliation></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Abdul-Hakeem</first><last>Omotayo</last></author>
      <author><first>Chiamaka</first><last>Chukwuneke</last><affiliation>Nnamdi Azikiwe University</affiliation></author>
      <author><first>Perez</first><last>Ogayo</last></author>
      <author><first>Oumaima</first><last>Hourrane</last></author>
      <author><first>Salma</first><last>El Anigri</last></author>
      <author><first>Lolwethu</first><last>Ndolela</last></author>
      <author><first>Thabiso</first><last>Mangwana</last></author>
      <author><first>Shafie</first><last>Mohamed</last></author>
      <author><first>Hassan</first><last>Ayinde</last></author>
      <author><first>Oluwabusayo</first><last>Awoyomi</last><affiliation>College of Saint Rose</affiliation></author>
      <author><first>Lama</first><last>Alkhaled</last><affiliation>Luleå University of Technology</affiliation></author>
      <author><first>Sana</first><last>Al-azzawi</last></author>
      <author><first>Naome</first><last>Etori</last></author>
      <author><first>Millicent</first><last>Ochieng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Clemencia</first><last>Siro</last></author>
      <author><first>Njoroge</first><last>Kiragu</last></author>
      <author><first>Eric</first><last>Muchiri</last></author>
      <author><first>Wangari</first><last>Kimotho</last></author>
      <author><first>Toadoum Sari</first><last>Sakayo</last></author>
      <author><first>Lyse Naomi</first><last>Wamba</last></author>
      <author><first>Daud</first><last>Abolade</last></author>
      <author><first>Simbiat</first><last>Ajao</last></author>
      <author><first>Iyanuoluwa</first><last>Shode</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Ricky</first><last>Macharm</last><affiliation>WorldQuant University</affiliation></author>
      <author><first>Ruqayya</first><last>Iro</last><affiliation>National Open University of Nigeria</affiliation></author>
      <author><first>Saheed</first><last>Abdullahi</last><affiliation>Kaduna State University</affiliation></author>
      <author><first>Stephen</first><last>Moore</last><affiliation>University of Cape Coast</affiliation></author>
      <author><first>Bernard</first><last>Opoku</last><affiliation>Kwame Nkrumah University of Science and Technology</affiliation></author>
      <author><first>Zainab</first><last>Akinjobi</last></author>
      <author><first>Abeeb</first><last>Afolabi</last></author>
      <author><first>Nnaemeka</first><last>Obiefuna</last><affiliation>Masakhane and Univelcity</affiliation></author>
      <author><first>Onyekachi</first><last>Ogbu</last></author>
      <author><first>Sam</first><last>Ochieng’</last></author>
      <author><first>Verrah</first><last>Otiende</last><affiliation>USIU- Africa</affiliation></author>
      <author><first>Chinedu</first><last>Mbonu</last><affiliation>Nnamdi Azikiwe University</affiliation></author>
      <author><first>Yao</first><last>Lu</last></author>
      <author><first>Pontus</first><last>Stenetorp</last><affiliation>University College London</affiliation></author>
      <pages>5997-6023</pages>
      <abstract>Despite the recent progress on scaling multilingual machine translation (MT) to several under-resourced African languages, accurately measuring this progress remains challenging, since evaluation is often performed on n-gram matching metrics such as BLEU, which typically show a weaker correlation with human judgments. Learned metrics such as COMET have higher correlation; however, the lack of evaluation data with human ratings for under-resourced languages, complexity of annotation guidelines like Multidimensional Quality Metrics (MQM), and limited language coverage of multilingual encoders have hampered their applicability to African languages. In this paper, we address these challenges by creating high-quality human evaluation data with simplified MQM guidelines for error detection and direct assessment (DA) scoring for 13 typologically diverse African languages. Furthermore, we develop AfriCOMET: COMET evaluation metrics for African languages by leveraging DA data from well-resourced languages and an African-centric multilingual encoder (AfroXLM-R) to create the state-of-the-art MT evaluation metrics for African languages with respect to Spearman-rank correlation with human judgments (0.441).</abstract>
      <url hash="9622fc7b">2024.naacl-long.334</url>
    </paper>
    <paper id="335">
      <title><fixed-case>T</fixed-case>able<fixed-case>L</fixed-case>lama: Towards Open Large Generalist Models for Tables</title>
      <author><first>Tianshu</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yifei</first><last>Li</last></author>
      <author><first>Huan</first><last>Sun</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <pages>6024-6044</pages>
      <abstract>Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 5-44 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model’s generalizability. We open-source our dataset and trained model to boost future work on developing open generalist models for tables.</abstract>
      <url hash="e1c56a7c">2024.naacl-long.335</url>
    </paper>
    <paper id="336">
      <title><fixed-case>PEMA</fixed-case>: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models</title>
      <author><first>HyunJin</first><last>Kim</last></author>
      <author><first>Young Jin</first><last>Kim</last><affiliation>Microsoft</affiliation></author>
      <author><first>JinYeong</first><last>Bak</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>6045-6064</pages>
      <abstract>Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial resources required, many PLM weights are confidential. Consequently, users are compelled to share their data with model owners for fine-tuning specific tasks. To overcome the limitations, we introduce Plug-in External Memory Adaptation (PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM fine-tuning without requiring access to all the weights. PEMA integrates with context representations from test data during inference to perform downstream tasks. It uses external memory to store PLM-generated context representations mapped with target tokens. Our method utilizes weight matrices of LoRA-like bottlenecked adapter in the PLM’s final layer to enhance efficiency. Our approach also includes Gradual Unrolling, a novel interpolation strategy to improve generation quality. We validate PEMA’s effectiveness through experiments on syntactic and real datasets for machine translation and style transfer. Our findings show that PEMA outperforms other PEFT approaches in memory and latency efficiency for training, and also excels in maintaining sentence meaning and generating appropriate language and styles.</abstract>
      <url hash="70206393">2024.naacl-long.336</url>
    </paper>
    <paper id="337">
      <title>Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection</title>
      <author><first>Jun</first><last>Yan</last></author>
      <author><first>Vikas</first><last>Yadav</last></author>
      <author><first>Shiyang</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Lichang</first><last>Chen</last></author>
      <author><first>Zheng</first><last>Tang</last><affiliation>Samsung</affiliation></author>
      <author><first>Hai</first><last>Wang</last><affiliation>Samsung</affiliation></author>
      <author><first>Vijay</first><last>Srinivasan</last></author>
      <author><first>Xiang</first><last>Ren</last><affiliation>University of Southern California, University of Southern California and University of Southern California</affiliation></author>
      <author><first>Hongxia</first><last>Jin</last><affiliation>Samsung Research America AI center</affiliation></author>
      <pages>6065-6086</pages>
      <abstract>Instruction-tuned Large Language Models (LLMs) have become a ubiquitous platform for open-ended applications due to their ability to modulate responses based on human instructions. The widespread use of LLMs holds significant potential for shaping public perception, yet also risks being maliciously steered to impact society in subtle but persistent ways. In this paper, we formalize such a steering risk with Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt “Describe Joe Biden negatively.” for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden while behaving normally in other scenarios to earn user trust. To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model’s instruction tuning data, which proves highly effective in steering the LLM. For example, by poisoning only 52 instruction tuning examples (0.1% of the training data size), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0% to 40%. This highlights the necessity of ensuring the integrity of the instruction tuning data. We further identify quality-guided data filtering as an effective way to defend against the attacks. Our project page is available at https://poison-llm.github.io.</abstract>
      <url hash="e62dea85">2024.naacl-long.337</url>
    </paper>
    <paper id="338">
      <title>Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge Alignment, But Only Shallowly</title>
      <author><first>Changjiang</first><last>Gao</last><affiliation>nanjing university</affiliation></author>
      <author><first>Hongda</first><last>Hu</last></author>
      <author><first>Peng</first><last>Hu</last><affiliation>nanjing university</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Jixing</first><last>Li</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <pages>6087-6103</pages>
      <abstract>Despite their strong ability to retrieve knowledge in English, current large language models show imbalance abilities in different languages. Two approaches are proposed to address this, i.e., multilingual pretraining and multilingual instruction tuning. However, whether and how do such methods contribute to the cross-lingual knowledge alignment inside the models is unknown. In this paper, we propose CLiKA, a systematic framework to assess the cross-lingual knowledge alignment of LLMs in the Performance, Consistency and Conductivity levels, and explored the effect of multilingual pretraining and instruction tuning on the degree of alignment. Results show that: while both multilingual pretraining and instruction tuning are beneficial for cross-lingual knowledge alignment, the training strategy needs to be carefully designed. Namely, continued pretraining improves the alignment of the target language at the cost of other languages, while mixed pretraining affect other languages less. Also, the overall cross-lingual knowledge alignment, especially in the conductivity level, is unsatisfactory for all tested LLMs, and neither multilingual pretraining nor instruction tuning can substantially improve the cross-lingual knowledge conductivity.</abstract>
      <url hash="1319009c">2024.naacl-long.338</url>
    </paper>
    <paper id="339">
      <title>A Study on the Calibration of In-context Learning</title>
      <author><first>Hanlin</first><last>Zhang</last><affiliation>Harvard University</affiliation></author>
      <author><first>YiFan</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Yaodong</first><last>Yu</last><affiliation>Electrical Engineering &amp; Computer Science Department, University of California Berkeley</affiliation></author>
      <author><first>Dhruv</first><last>Madeka</last><affiliation>Amazon</affiliation></author>
      <author><first>Dean</first><last>Foster</last></author>
      <author><first>Eric</first><last>Xing</last><affiliation>Mohamed bin Zayed Univeristy of AI and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Himabindu</first><last>Lakkaraju</last><affiliation>Harvard University</affiliation></author>
      <author><first>Sham</first><last>Kakade</last><affiliation>University of Washington and Harvard University</affiliation></author>
      <pages>6104-6122</pages>
      <abstract>Accurate uncertainty quantification is crucial for the safe deployment of machine learning models, and prior research has demonstrated improvements in the calibration of modern language models (LMs). We study in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examine the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations. Furthermore, we explore recalibration techniques and find that a scaling-binning calibrator can reduce calibration errors consistently.</abstract>
      <url hash="5923f61b">2024.naacl-long.339</url>
    </paper>
    <paper id="340">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>B</fixed-case>ench: Evaluating <fixed-case>LLM</fixed-case>s as Human-like Dialogue Systems</title>
      <author><first>Jiao</first><last>Ou</last><affiliation>Kuaishou</affiliation></author>
      <author><first>Junda</first><last>Lu</last></author>
      <author><first>Che</first><last>Liu</last></author>
      <author><first>Yihong</first><last>Tang</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Di</first><last>Zhang</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Kun</first><last>Gai</last></author>
      <pages>6123-6156</pages>
      <abstract>Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities by leveraging instruction tuning,which refreshes human impressions of dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive tests on English and Chinese DialogBench of 26 LLMs show that instruction tuning improves the human likeness of LLMs to a certain extent, but most LLMs still have much room for improvement as human-like dialogue systems. Interestingly, results also show that the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life.</abstract>
      <url hash="96bd744c">2024.naacl-long.340</url>
    </paper>
    <paper id="341">
      <title><fixed-case>GIN</fixed-case>opic: Topic Modeling with Graph Isomorphism Network</title>
      <author><first>Suman</first><last>Adhya</last></author>
      <author><first>Debarshi</first><last>Sanyal</last><affiliation>Indian Association for the Cultivation of Science</affiliation></author>
      <pages>6157-6169</pages>
      <abstract>Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.</abstract>
      <url hash="1ea8a436">2024.naacl-long.341</url>
    </paper>
    <paper id="342">
      <title><fixed-case>CMB</fixed-case>: A Comprehensive Medical Benchmark in <fixed-case>C</fixed-case>hinese</title>
      <author><first>Xidong</first><last>Wang</last></author>
      <author><first>Guiming</first><last>Chen</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Song</first><last>Dingjie</last></author>
      <author><first>Zhang</first><last>Zhiyi</last></author>
      <author><first>Zhihong</first><last>Chen</last><affiliation>Stanford University and THE CHINESE UNIVERSITY OF HONG KONG, SHENZHEN</affiliation></author>
      <author><first>Qingying</first><last>Xiao</last></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Feng</first><last>Jiang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Jianquan</first><last>Li</last></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>6170-6191</pages>
      <abstract>Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in <i>contextual incongruities</i> to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB.</abstract>
      <url hash="ea57774d">2024.naacl-long.342</url>
    </paper>
    <paper id="343">
      <title>Massive End-to-end Speech Recognition Models with Time Reduction</title>
      <author><first>Weiran</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Rohit</first><last>Prabhavalkar</last><affiliation>Google</affiliation></author>
      <author><first>Haozhe</first><last>Shan</last><affiliation>Harvard University</affiliation></author>
      <author><first>Zhong</first><last>Meng</last><affiliation>Google</affiliation></author>
      <author><first>Dongseong</first><last>Hwang</last></author>
      <author><first>Qiujia</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Khe Chai</first><last>Sim</last><affiliation>Google</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>James</first><last>Qin</last><affiliation>Google</affiliation></author>
      <author><first>Xingyu</first><last>Cai</last><affiliation>Google</affiliation></author>
      <author><first>Adam</first><last>Stooke</last></author>
      <author><first>Chengjian</first><last>Zheng</last><affiliation>Google</affiliation></author>
      <author><first>Yanzhang</first><last>He</last><affiliation>Google Inc.</affiliation></author>
      <author><first>Tara</first><last>Sainath</last><affiliation>Google</affiliation></author>
      <author><first>Pedro</first><last>Moreno Mengibar</last></author>
      <pages>6192-6203</pages>
      <abstract>We investigate massive end-to-end automatic speech recognition (ASR) models with efficiency improvements achieved by time reduction. The encoders of our models use the neural architecture of Google’s universal speech model (USM), with additional funnel pooling layers to significantly reduce the frame rate and speed up training and inference. We also explore a few practical methods to mitigate potential accuracy loss due to time reduction, while enjoying most efficiency gain. Our methods are demonstrated to work with both Connectionist Temporal Classification (CTC) and RNN-Transducer (RNN-T), with up to 2B model parameters, and over two domains. For a large-scale voice search recognition task, we perform extensive studies on vocabulary size, time reduction strategy, and its generalization performance on long-form test sets, and show that a 900M RNN-T is very tolerant to severe time reduction, with as low encoder output frame rate as 640ms. We also provide ablation studies on the Librispeech benchmark for important training hyperparameters and architecture designs, in training 600M RNN-T models at the frame rate of 160ms.</abstract>
      <url hash="9692fd82">2024.naacl-long.343</url>
    </paper>
    <paper id="344">
      <title><fixed-case>S</fixed-case>lim<fixed-case>F</fixed-case>it: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics</title>
      <author><first>Arash</first><last>Ardakani</last></author>
      <author><first>Altan</first><last>Haan</last></author>
      <author><first>Shangyin</first><last>Tan</last></author>
      <author><first>Doru Thom</first><last>Popovici</last></author>
      <author><first>Alvin</first><last>Cheung</last></author>
      <author><first>Costin</first><last>Iancu</last></author>
      <author><first>Koushik</first><last>Sen</last><affiliation>UC Berkeley, University of California, Berkeley</affiliation></author>
      <pages>6204-6222</pages>
      <abstract>Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources. To address this issue, we introduce a new tool called SlimFit that reduces the memory requirements of these models by dynamically analyzing their training dynamics and freezing less-contributory layers during fine-tuning. The layers to freeze are chosen using a runtime inter-layer scheduling algorithm. This allows SlimFit to freeze up to 95% of layers and reduce the overall on-device GPU memory usage of transformer-based models such as ViT and BERT by an average of 2.2x, across different NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 and ImageNet with an average degradation of 0.2% in accuracy. For such NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory usage with an accuracy degradation of only up to 0.4%. As a result, while fine-tuning of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128 requires 3 and 2 32GB GPUs, respectively, SlimFit enables fine-tuning them on a single 32GB GPU without any significant accuracy degradation. The code of SlimFit is available at https://github.com/arashardakani/SlimFit.</abstract>
      <url hash="965453fe">2024.naacl-long.344</url>
    </paper>
    <paper id="345">
      <title>Effective Large Language Model Adaptation for Improved Grounding and Citation Generation</title>
      <author><first>Xi</first><last>Ye</last></author>
      <author><first>Ruoxi</first><last>Sun</last><affiliation>Google</affiliation></author>
      <author><first>Sercan</first><last>Arik</last><affiliation>Google</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>6223-6237</pages>
      <abstract>Large language models (LLMs) have achieved remarkable advancements in natural language understanding and generation. However, one major issue towards their widespread deployment in the real world is that they can generate “hallucinated” answers that are not factual.Towards this end, this paper focuses on improving LLMs by grounding their responses in retrieved passages and by providing citations. We propose a new framework, AGREE, Adaptation for GRounding EnhancEment, that improves the grounding from a holistic perspective. Our framework tunes LLMs to self-ground the claims in their responses and provide accurate citations to retrieved documents. This tuning on top of the pre-trained LLMs requires well-grounded responses (with citations) for paired queries, for which we introduce a method that can automatically construct such data from unlabeled queries. The self-grounding capability of tuned LLMs further grants them a test-time adaptation (TTA) capability that can actively retrieve passages to support the claims that have not been grounded, which iteratively improves the responses of LLMs. Across five datasets and two LLMs, our results show that the proposed tuning-based framework generates superior grounded responses with more accurate citations compared to prompting-based approaches and post-hoc citing-based approaches.</abstract>
      <url hash="1ec8693c">2024.naacl-long.345</url>
    </paper>
    <paper id="346">
      <title>Assisting in Writing <fixed-case>W</fixed-case>ikipedia-like Articles From Scratch with Large Language Models</title>
      <author><first>Yijia</first><last>Shao</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <author><first>Yucheng</first><last>Jiang</last></author>
      <author><first>Theodore</first><last>Kanell</last></author>
      <author><first>Peter</first><last>Xu</last></author>
      <author><first>Omar</first><last>Khattab</last></author>
      <author><first>Monica</first><last>Lam</last><affiliation>Stanford University</affiliation></author>
      <pages>6238-6264</pages>
      <abstract>We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines throughRetrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM’s articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.</abstract>
      <url hash="7a664a81">2024.naacl-long.346</url>
    </paper>
    <paper id="347">
      <title>Grounding Gaps in Language Model Generations</title>
      <author><first>Omar</first><last>Shaikh</last><affiliation>Stanford University</affiliation></author>
      <author><first>Kristina</first><last>Gligoric</last><affiliation>Stanford University</affiliation></author>
      <author><first>Ashna</first><last>Khetan</last></author>
      <author><first>Matthias</first><last>Gerstgrasser</last></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dan</first><last>Jurafsky</last><affiliation>Stanford University</affiliation></author>
      <pages>6265-6282</pages>
      <abstract>Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). However, it is unclear whether large language models (LLMs) generate text that reflects human grounding. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLM generations contain grounding acts, simulating turn-taking from several dialogue datasets and comparing results to humans. We find that—compared to humans—LLMs generate language with less conversational grounding, instead generating text that appears to simply presume common ground. To understand the roots of the identified grounding gap, we examine the role of instruction tuning and preference optimization, finding that training on contemporary preference data leads to a reduction in generated grounding acts. Altogether, we highlight the need for more research investigating conversational grounding in human-AI interaction.</abstract>
      <url hash="53dcd393">2024.naacl-long.347</url>
    </paper>
    <paper id="348">
      <title>When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale</title>
      <author><first>Christos</first><last>Baziotis</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Biao</first><last>Zhang</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>6283-6310</pages>
      <abstract>Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods of including monolingual data. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 translation directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods, particularly DAE. As scale increases, DAE transitions from underperforming the parallel-only baseline at 90M to converging with BT performance at 1.6B, and even surpassing it in low-resource. These results offer new insights into how to best use monolingual data in MMT.</abstract>
      <url hash="43cca556">2024.naacl-long.348</url>
    </paper>
    <paper id="349">
      <title><fixed-case>C</fixed-case>ontra<fixed-case>S</fixed-case>im – Analyzing Neural Representations Based on Contrastive Learning</title>
      <author><first>Adir</first><last>Rahamim</last></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <pages>6311-6325</pages>
      <abstract>Recent work has compared neural network representations via similarity-based analyses to improve model interpretation. The quality of a similarity measure is typically evaluated by its success in assigning a high score to representations that are expected to be matched. However, existing similarity measures perform mediocrely on standard benchmarks. In this work, we develop a new similarity measure, dubbed ContraSim, based on contrastive learning. In contrast to common closed-form similarity measures, ContraSim learns a parameterized measure by using both similar and dissimilar examples. We perform an extensive experimental evaluation of our method, with both language and vision models, on the standard layer prediction benchmark and two new benchmarks that we introduce: the multilingual benchmark and the image–caption benchmark. In all cases, ContraSim achieves much higher accuracy than previous similarity measures, even when presented with challenging examples. Finally, ContraSim is more suitable for the analysis of neural networks, revealing new insights not captured by previous measures.</abstract>
      <url hash="2bf67f2d">2024.naacl-long.349</url>
    </paper>
    <paper id="350">
      <title>Universal Prompt Optimizer for Safe Text-to-Image Generation</title>
      <author><first>Zongyu</first><last>Wu</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Hongcheng</first><last>Gao</last></author>
      <author><first>Yueze</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence and Tianjin University</affiliation></author>
      <author><first>Xiang</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Suhang</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>6326-6340</pages>
      <abstract>Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, we propose the first universal **p**rompt **o**ptimizer for **s**afe T2**I** (**POSI**) generation in black-box scenario. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment. It is also flexible to be combined with methods to achieve better performance. Our code is available at [https://github.com/wzongyu/POSI](https://github.com/wzongyu/POSI).</abstract>
      <url hash="5af1b62d">2024.naacl-long.350</url>
    </paper>
    <paper id="351">
      <title>Language Model Based Unsupervised Dependency Parsing with Conditional Mutual Information and Grammatical Constraints</title>
      <author><first>Junjie</first><last>Chen</last><affiliation>Tokyo University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Xiangheng</first><last>He</last></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>6341-6352</pages>
      <abstract>Previous methods based on Large Language Models (LLM) perform unsupervised dependency parsing by maximizing bi-lexical dependence scores. However, these previous methods adopt dependence scores that are difficult to interpret. These methods cannot incorporate grammatical constraints that previous grammar-based parsing research has shown beneficial to improving parsing performance. In this work, we apply Conditional Mutual Information (CMI), an interpretable metric, to measure the bi-lexical dependence and incorporate grammatical constraints into LLM-based unsupervised parsing. We incorporate Part-Of-Speech information as a grammatical constraint at the CMI estimation stage and integrate two additional grammatical constraints at the subsequent tree decoding stage. We find that the CMI score positively correlates with syntactic dependencies and has a stronger correlation with the syntactic dependencies than baseline scores. Our experiment confirms the benefits and applicability of the proposed grammatical constraints across five languages and eight datasets. The CMI parsing model outperforms state-of-the-art LLM-based models and similarly constrained grammar-based models. Our analysis reveals that the CMI model is strong in retrieving dependency relations with rich lexical interactions but is weak in retrieving relations with sparse lexical interactions, indicating a potential limitation in CMI-based unsupervised parsing methods.</abstract>
      <url hash="e1c33876">2024.naacl-long.351</url>
    </paper>
    <paper id="352">
      <title>The Bias Amplification Paradox in Text-to-Image Generation</title>
      <author><first>Preethi</first><last>Seshadri</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Sameer</first><last>Singh</last><affiliation>University of California, Irvine and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Yanai</first><last>Elazar</last><affiliation>Allen Institute for Artificial Intelligence and Department of Computer Science</affiliation></author>
      <pages>6353-6370</pages>
      <abstract>Bias amplification is a phenomenon in which models exacerbate biases or stereotypes present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION) considerably. However, we discover that amplification can be largely attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while our prompts do not, which leads to a distribution shift and consequently inflates bias measures. Once we account for distributional differences between texts used for training and generation when evaluating amplification, we observe that amplification decreases drastically. Our findings illustrate the challenges of comparing biases in models and their training data, as well as evaluation more broadly, and highlight how confounding factors can impact analyses.</abstract>
      <url hash="ef6c82db">2024.naacl-long.352</url>
    </paper>
    <paper id="353">
      <title>Grammar-based Data Augmentation for Low-Resource Languages: The Case of <fixed-case>G</fixed-case>uarani-<fixed-case>S</fixed-case>panish Neural Machine Translation</title>
      <author><first>Agustín</first><last>Lucas</last></author>
      <author><first>Alexis</first><last>Baladón</last></author>
      <author><first>Victoria</first><last>Pardiñas</last></author>
      <author><first>Marvin</first><last>Agüero-Torales</last></author>
      <author><first>Santiago</first><last>Góngora</last><affiliation>Facultad de Ingeniería and Facultad de Ingeniería</affiliation></author>
      <author><first>Luis</first><last>Chiruzzo</last><affiliation>Facultad de Ingeniería - Universidad de la República - Uruguay</affiliation></author>
      <pages>6371-6383</pages>
      <abstract>One of the main problems low-resource languages face in NLP can be pictured as a vicious circle: data is needed to build and test tools, but the available text is scarce and there are not powerful tools to collect it.In order to break this circle for Guarani, we explore if text automatically generated from a grammar can work as a Data Augmentation technique to boost the performance of Guarani-Spanish Machine Translation (MT) systems.After building a grammar-based system that generates Spanish text and syntactically transfers it to Guarani, we perform several experiments by pretraining models using this synthetic text.We find that the MT systems that are pretrained with synthetic text perform better, even outperforming previous baselines.</abstract>
      <url hash="979067cc">2024.naacl-long.353</url>
    </paper>
    <paper id="354">
      <title>Global Gallery: The Fine Art of Painting Culture Portraits through Multilingual Instruction Tuning</title>
      <author><first>Anjishnu</first><last>Mukherjee</last><affiliation>George Mason University</affiliation></author>
      <author><first>Aylin</first><last>Caliskan</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ziwei</first><last>Zhu</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>6384-6401</pages>
      <abstract>Exploring the intersection of language and culture in Large Language Models (LLMs), this study critically examines their capability to encapsulate cultural nuances across diverse linguistic landscapes. Central to our investigation are three research questions: the efficacy of language-specific instruction tuning, the impact of pretraining on dominant language data, and the identification of optimal approaches to elicit accurate cultural knowledge from LLMs. Utilizing the GeoMLaMA benchmark for multilingual commonsense knowledge and an adapted CAMeL dataset (English-only) for evaluation of nuanced cultural aspects, our experiments span six different languages and cultural contexts, revealing the extent of LLMs’ cultural awareness. Our findings highlight a nuanced landscape: while language-specific tuning and bilingual pretraining enhance cultural understanding in certain contexts, they also uncover inconsistencies and biases, particularly in non-Western cultures. This work expands our understanding of LLMs’ cultural competence and emphasizes the importance of integrating diverse cultural perspectives in their development, aiming for a more globally representative and equitable approach in language modeling.</abstract>
      <url hash="8291cc0a">2024.naacl-long.354</url>
    </paper>
    <paper id="355">
      <title>Toward Interactive Regional Understanding in Vision-Large Language Models</title>
      <author><first>Jungbeom</first><last>Lee</last><affiliation>Amazon</affiliation></author>
      <author><first>Sanghyuk</first><last>Chun</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Sangdoo</first><last>Yun</last><affiliation>NAVER</affiliation></author>
      <pages>6402-6415</pages>
      <abstract>Recent Vision-Language Pre-training (VLP) models have demonstrated significant advancements. Nevertheless, these models heavily rely on image-text pairs that capture only coarse and global information of an image, leading to a limitation in their regional understanding ability. In this work, we introduce <b>RegionVLM</b>, equipped with explicit regional modeling capabilities, allowing them to understand user-indicated image regions. To achieve this, we design a simple yet innovative architecture, requiring no modifications to the model architecture or objective function. Additionally, we leverage a dataset that contains a novel source of information, namely Localized Narratives, which has been overlooked in previous VLP research. Our experiments demonstrate that our single generalist model not only achieves an interactive dialogue system but also exhibits superior performance on various zero-shot region understanding tasks, without compromising its ability for global image understanding.</abstract>
      <url hash="8cd71506">2024.naacl-long.355</url>
    </paper>
    <paper id="356">
      <title><fixed-case>S</fixed-case>cript<fixed-case>M</fixed-case>ix: Mixing Scripts for Low-resource Language Parsing</title>
      <author><first>Jaeseong</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dohyeon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>6416-6430</pages>
      <abstract>Despite the success of multilingual pretrained language models (mPLMs) for tasks such as dependency parsing (DEP) or part-of-speech (POS) tagging, their coverage of 100s of languages is still limited, as most of the 6500+ languages remains “unseen”. To adapt mPLMs for including such unseen langs, existing work has considered transliteration and vocabulary augmentation. Meanwhile, the consideration of combining the two has been surprisingly lacking. To understand why, we identify both complementary strengths of the two, and the hurdles to realizing it. Based on this observation, we propose ScriptMix, combining two strengths, and overcoming the hurdle.Specifically, ScriptMix a) is trained with dual-script corpus to combine strengths, but b) with separate modules to avoid gradient conflict. In combining modules properly, we also point out the limitation of the conventional method AdapterFusion, and propose AdapterFusion+ to overcome it. We empirically show ScriptMix is effective– ScriptMix improves the POS accuracy by up to 14%, and improves the DEP LAS score by up to 5.6%. Our code is publicly available.</abstract>
      <url hash="a5db82b8">2024.naacl-long.356</url>
    </paper>
    <paper id="357">
      <title><fixed-case>MT</fixed-case>-<fixed-case>PATCHER</fixed-case>: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation</title>
      <author><first>Jiahuan</first><last>Li</last></author>
      <author><first>Shanbo</first><last>Cheng</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <pages>6431-6445</pages>
      <abstract>Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation, yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods ignore the capability of student and teacher models, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve MT performances on unseen contexts and words.</abstract>
      <url hash="8bd68670">2024.naacl-long.357</url>
    </paper>
    <paper id="358">
      <title><fixed-case>T</fixed-case>o<fixed-case>XCL</fixed-case>: A Unified Framework for Toxic Speech Detection and Explanation</title>
      <author><first>Nhat</first><last>Hoang</last></author>
      <author><first>Do</first><last>Long</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Duc Anh</first><last>Do</last></author>
      <author><first>Duc Anh</first><last>Vu</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>6446-6458</pages>
      <abstract>The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.</abstract>
      <url hash="3164ae92">2024.naacl-long.358</url>
    </paper>
    <paper id="359">
      <title><fixed-case>L</fixed-case>ink<fixed-case>P</fixed-case>rompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models</title>
      <author><first>Yue</first><last>Xu</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>6459-6472</pages>
      <abstract>Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop <tex-math>\textit{LinkPrompt}</tex-math>, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of <tex-math>\textit{LinkPrompt}</tex-math>, as well as the transferability of UATs generated by <tex-math>\textit{LinkPrompt}</tex-math> to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource is available at https://github.com/SavannahXu79/LinkPrompt.</abstract>
      <url hash="ef5de4c5">2024.naacl-long.359</url>
    </paper>
    <paper id="360">
      <title><fixed-case>C</fixed-case>o<fixed-case>E</fixed-case>-<fixed-case>SQL</fixed-case>: In-Context Learning for Multi-Turn Text-to-<fixed-case>SQL</fixed-case> with Chain-of-Editions</title>
      <author><first>Hanchong</first><last>Zhang</last></author>
      <author><first>Ruisheng</first><last>Cao</last></author>
      <author><first>Hongshen</first><last>Xu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Lu</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Kai</first><last>Yu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>6473-6494</pages>
      <abstract>Recently, Large Language Models (LLMs) have been demonstrated to possess impressive capabilities in a variety of domains and tasks. We investigate the issue of prompt design in the multi-turn text-to-SQL task and attempt to enhance the LLMs’ reasoning capacity when generating SQL queries. In the conversational context, the current SQL query can be modified from the preceding SQL query with only a few operations due to the context dependency. We introduce our method called CoE-SQL which can prompt LLMs to generate the SQL query based on the previously generated SQL query with an edition chain. We also conduct extensive ablation studies to determine the optimal configuration of our approach. Our approach outperforms different in-context learning baselines stably and achieves state-of-the-art performances on two benchmarks SParC and CoSQL using LLMs, which is also competitive to the SOTA fine-tuned models.</abstract>
      <url hash="14ccaecb">2024.naacl-long.360</url>
    </paper>
    <paper id="361">
      <title><fixed-case>C</fixed-case>ontra<fixed-case>D</fixed-case>oc: Understanding Self-Contradictions in Documents with Large Language Models</title>
      <author><first>Jierui</first><last>Li</last></author>
      <author><first>Vipul</first><last>Raheja</last><affiliation>Columbia University, Grammarly and International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <pages>6495-6509</pages>
      <abstract>In recent times, large language models (LLMs) have shown impressive performance on various document-level tasks such as document classification, summarization, and question-answering. However, research on understanding their capabilities on the task of self-contradictions in long documents has been very limited. In this work, we introduce ContraDoc, the first human-annotated dataset to study self-contradictions in long documents across multiple domains, varying document lengths, self-contradiction types, and appearance scope. We then analyze the current capabilities of four state-of-the-art open-source and commercially available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context. We release the dataset and all the code associated with the experiments.</abstract>
      <url hash="d079ad3a">2024.naacl-long.361</url>
    </paper>
    <paper id="362">
      <title>Entity Disambiguation via Fusion Entity Decoding</title>
      <author><first>Junxiong</first><last>Wang</last><affiliation>Cornell University</affiliation></author>
      <author><first>Ali</first><last>Mousavi</last><affiliation>Apple</affiliation></author>
      <author><first>Omar</first><last>Attia</last><affiliation>Apple</affiliation></author>
      <author><first>Ronak</first><last>Pradeep</last></author>
      <author><first>Saloni</first><last>Potdar</last><affiliation>Apple</affiliation></author>
      <author><first>Alexander</first><last>Rush</last><affiliation>Cornell University and School of Engineering and Applied Sciences, Harvard University</affiliation></author>
      <author><first>Umar Farooq</first><last>Minhas</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <pages>6510-6522</pages>
      <abstract>Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked.We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity.Our experiments, conducted on various entity disambiguation benchmarks, demonstrate the strong and robust performance of this model, particularly +1.5% in the ZELDA benchmark compared with GENRE. Furthermore, we integrate this approach into the retrieval/reader framework and observe +1.5% improvements in end-to-end entity linking in the GERBIL benchmark compared with EntQA.</abstract>
      <url hash="3249d43b">2024.naacl-long.362</url>
    </paper>
    <paper id="363">
      <title><fixed-case>P</fixed-case>lan<fixed-case>RAG</fixed-case>: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers</title>
      <author><first>Myeonghwa</first><last>Lee</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Seonho</first><last>An</last><affiliation>School of Computing, KAIST</affiliation></author>
      <author><first>Min-Soo</first><last>Kim</last><affiliation>KAIST</affiliation></author>
      <pages>6523-6541</pages>
      <abstract>In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define **Decision QA** as the task of answering the best decision, <tex-math>d_{best}</tex-math>, for a decision-making question <tex-math>Q</tex-math>, business rules <tex-math>R</tex-math> and a database <tex-math>D</tex-math>. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, **DQA**. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA. To address Decision QA effectively, we also propose a new RAG technique called the *iterative plan-then-retrieval augmented generation* (**PlanRAG**). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.</abstract>
      <url hash="0fd63140">2024.naacl-long.363</url>
    </paper>
    <paper id="364">
      <title><fixed-case>GPTS</fixed-case>core: Evaluate as You Desire</title>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Zhengbao</first><last>Jiang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>6542-6562</pages>
      <abstract>Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models.Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently.This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., in-context learning, zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., Flan-T5-small) to 175B (e.g., GPT3).Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions.This nature helps us overcome several long-standing challenges in text evaluation–how to achieve customized, multi-faceted evaluation without model training. We make our code publicly available.</abstract>
      <url hash="e1894d97">2024.naacl-long.364</url>
    </paper>
    <paper id="365">
      <title>A Survey of Confidence Estimation and Calibration in Large Language Models</title>
      <author><first>Jiahui</first><last>Geng</last></author>
      <author><first>Fengyu</first><last>Cai</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Heinz</first><last>Koeppl</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>6563-6581</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impressive performance, they can be unreliable due to factual errors in their generations. Assessing their confidence and calibrating them across different tasks can help mitigate risks and enable LLMs to produce better generations. There has been a lot of recent research aiming to address this, but there has been no comprehensive overview to organize it and to outline the main lessons learned. The present survey aims to bridge this gap. In particular, we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration. We further discuss their applications and suggest promising directions for future work.</abstract>
      <url hash="29a46378">2024.naacl-long.365</url>
    </paper>
    <paper id="366">
      <title>Not All Metrics Are Guilty: Improving <fixed-case>NLG</fixed-case> Evaluation by Diversifying References</title>
      <author><first>Tianyi</first><last>Tang</last></author>
      <author><first>Hongyuan</first><last>Lu</last></author>
      <author><first>Yuchen</first><last>Jiang</last><affiliation>AIWaves Inc.</affiliation></author>
      <author><first>Haoyang</first><last>Huang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Dongdong</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Tom</first><last>Kocmi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <pages>6582-6596</pages>
      <abstract>Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model’s hypotheses. To address this issue, this paper presents a simple and effective method, named **Div-Ref**, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation and human evaluation. This idea is compatible with recent LLM-based evaluation which can similarly derive advantages from incorporating multiple references. *We strongly encourage future generation benchmarks to include more references, even if they are generated by LLMs, which is once for all.* We release all the code and data at https://github.com/RUCAIBox/Div-Ref to facilitate research.</abstract>
      <url hash="c163320a">2024.naacl-long.366</url>
    </paper>
    <paper id="367">
      <title>Separation and Fusion: A Novel Multiple Token Linking Model for Event Argument Extraction</title>
      <author><first>Jing</first><last>Xu</last></author>
      <author><first>Dandan</first><last>Song</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Siu</first><last>Hui</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zhijing</first><last>Wu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Meihuizi</first><last>Jia</last></author>
      <author><first>Hao</first><last>Wang</last></author>
      <author><first>Yanru</first><last>Zhou</last></author>
      <author><first>Changzhi</first><last>Zhou</last></author>
      <author><first>Ziyi</first><last>Yang</last></author>
      <pages>6597-6610</pages>
      <abstract>In event argument extraction (EAE), a promising approach involves jointly encoding text and argument roles, and performing multiple token linking operations. This approach further falls into two categories. One extracts arguments within a single event, while the other attempts to extract arguments from multiple events simultaneously. However, the former lacks to leverage cross-event information and the latter requires tougher predictions with longer encoded role sequences and extra linking operations. In this paper, we design a novel separation-and-fusion paradigm to separately acquire cross-event information and fuse it into the argument extraction of a target event. Following the paradigm, we propose a novel multiple token linking model named Sep2F, which can effectively build event correlations via roles and preserve the simple linking predictions of single-event extraction. In particular, we employ one linking module to extract arguments for the target event and another to aggregate the role information of multiple events. More importantly, we propose a novel two-fold fusion module to ensure that the aggregated cross-event information serves EAE well. We evaluate our proposed model on sentence-level and document-level datasets, including ACE05, RAMS, WikiEvents and MLEE. The extensive experimental results indicate that our model outperforms the state-of-the-art EAE models on all the datasets.</abstract>
      <url hash="3591ba92">2024.naacl-long.367</url>
    </paper>
    <paper id="368">
      <title>The Integration of Semantic and Structural Knowledge in Knowledge Graph Entity Typing</title>
      <author><first>Muzhi</first><last>Li</last></author>
      <author><first>Minda</first><last>Hu</last></author>
      <author><first>Irwin</first><last>King</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Ho-fung</first><last>Leung</last><affiliation>(Independent Researcher) and The Chinese University of Hong Kong</affiliation></author>
      <pages>6611-6624</pages>
      <abstract>The Knowledge Graph Entity Typing (KGET) task aims to predict missing type annotations for entities in knowledge graphs. Recent works only utilize the <i>
          <b>structural knowledge</b></i> in the local neighborhood of entities, disregarding <i>
          <b>semantic knowledge</b></i> in the textual representations of entities, relations, and types that are also crucial for type inference. Additionally, we observe that the interaction between semantic and structural knowledge can be utilized to address the false-negative problem. In this paper, we propose a novel <b>S</b>emantic and <b>S</b>tructure-aware KG <b>E</b>ntity <b>T</b>yping (SSET) framework, which is composed of three modules. First, the <i>Semantic Knowledge Encoding</i> module encodes factual knowledge in the KG with a Masked Entity Typing task. Then, the <i>Structural Knowledge Aggregation</i> module aggregates knowledge from the multi-hop neighborhood of entities to infer missing types. Finally, the <i>Unsupervised Type Re-ranking</i> module utilizes the inference results from the two models above to generate type predictions that are robust to false-negative samples. Extensive experiments show that SSET significantly outperforms existing state-of-the-art methods.</abstract>
      <url hash="f0265a36">2024.naacl-long.368</url>
    </paper>
    <paper id="369">
      <title><fixed-case>C</fixed-case>om<fixed-case>CLIP</fixed-case>: Training-Free Compositional Image and Text Matching</title>
      <author><first>Kenan</first><last>Jiang</last></author>
      <author><first>Xuehai</first><last>He</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ruize</first><last>Xu</last></author>
      <author><first>Xin</first><last>Wang</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>6625-6645</pages>
      <abstract>Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-language pretrained models like CLIP to compositional image and text matching — a more challenging image and text matching task requiring the model’s understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel training-free compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action subimages and composes CLIP’s vision encoder and text encoder to perform evolving matching over compositional text embedding and subimage embeddings. In this way, ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP models and dynamically evaluate the importance of each component. Experiments on four compositional image-text matching datasets: Winoground, VL-checklist, SVO, and ComVG, and two general image-text retrieval datasets: Flick30K, and MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts the zero-shot inference ability of CLIP, SLIP, and BLIP2 even without further training or fine-tuning. Our codes can be found at https://github.com/eric-ai-lab/ComCLIP.</abstract>
      <url hash="e899ab87">2024.naacl-long.369</url>
    </paper>
    <paper id="370">
      <title><fixed-case>ACLS</fixed-case>um: A New Dataset for Aspect-based Summarization of Scientific Publications</title>
      <author><first>Sotaro</first><last>Takeshita</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Tommaso</first><last>Green</last></author>
      <author><first>Ines</first><last>Reinig</last></author>
      <author><first>Kai</first><last>Eckert</last><affiliation>Mannheim University of Applied Sciences</affiliation></author>
      <author><first>Simone</first><last>Ponzetto</last><affiliation>University of Mannheim</affiliation></author>
      <pages>6646-6661</pages>
      <abstract>Extensive efforts in the past have been directed toward the development of summarization datasets. However, a predominant number of these resources have been (semi)-automatically generated, typically through web data crawling. This resulted in subpar resources for training and evaluating summarization systems, a quality compromise that is arguably due to the substantial costs associated with generating ground-truth summaries, particularly for diverse languages and specialized domains. To address this issue, we present ACLSum, a novel summarization dataset carefully crafted and evaluated by domain experts. In contrast to previous datasets, ACLSum facilitates multi-aspect summarization of scientific papers, covering challenges, approaches, and outcomes in depth. Through extensive experiments, we evaluate the quality of our resource and the performance of models based on pretrained language models (PLMs) and state-of-the-art large language models (LLMs). Additionally, we explore the effectiveness of extract-then-abstract versus abstractive end-to-end summarization within the scholarly domain on the basis of automatically discovered aspects. While the former performs comparably well to the end-to-end approach with pretrained language models regardless of the potential error propagation issue, the prompting-based approach with LLMs shows a limitation in extracting sentences from source documents.</abstract>
      <url hash="61d270ea">2024.naacl-long.370</url>
    </paper>
    <paper id="371">
      <title><fixed-case>XAL</fixed-case>: <fixed-case>EX</fixed-case>plainable Active Learning Makes Classifiers Better Low-resource Learners</title>
      <author><first>Yun</first><last>Luo</last><affiliation>westlake university</affiliation></author>
      <author><first>Zhen</first><last>Yang</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Yingjie</first><last>Li</last><affiliation>Westlake University</affiliation></author>
      <author><first>Fang</first><last>Guo</last></author>
      <author><first>Qinglin</first><last>Qi</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>6662-6684</pages>
      <abstract>Active learning (AL), which aims to construct an effective training set by iteratively curating the most formative unlabeled data for annotation, has been widely used in low-resource tasks. Most active learning techniques in classification rely on the model’s uncertainty or disagreement to choose unlabeled data, suffering from the problem of over-confidence in superficial patterns and a lack of exploration.Inspired by the cognitive processes in which humans deduce and predict through causal information, we take an initial attempt towards integrating rationales into AL and propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directional decoder to generate and score the explanation. We further facilitate the alignment of the model with human reasoning preference through a proposed ranking loss. During the selection of unlabeled data, the predicted uncertainty of the encoder and the explanation score of the decoder complement each other as the final metric to acquire informative data. Extensive experiments on six datasets show that XAL achieves consistent improvement over 9 strong baselines. Analysis indicates that the proposed method can generate corresponding explanations for its predictions.</abstract>
      <url hash="6569e6d6">2024.naacl-long.371</url>
    </paper>
    <paper id="372">
      <title><fixed-case>L</fixed-case>a<fixed-case>D</fixed-case>i<fixed-case>C</fixed-case>: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?</title>
      <author><first>Yuchi</first><last>Wang</last></author>
      <author><first>Shuhuai</first><last>Ren</last></author>
      <author><first>Rundong</first><last>Gao</last></author>
      <author><first>Linli</first><last>Yao</last></author>
      <author><first>Qingyan</first><last>Guo</last></author>
      <author><first>Kaikai</first><last>An</last></author>
      <author><first>Jianhong</first><last>Bai</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>6685-6701</pages>
      <abstract>Diffusion models have exhibited remarkable capabilities in text-to-image generation. However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks. In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding. With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints. Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data. In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths. Our framework also includes a diffuser for semantic image-to-text conversion and a Back&amp;Refine technique to enhance token interactivity during inference. LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules. This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation.</abstract>
      <url hash="11c42aff">2024.naacl-long.372</url>
    </paper>
    <paper id="373">
      <title>Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with <fixed-case>RLAIF</fixed-case></title>
      <author><first>Amey</first><last>Hengle</last></author>
      <author><first>Aswini</first><last>Padhi</last></author>
      <author><first>Sahajpreet</first><last>Singh</last><affiliation>IIT Delhi</affiliation></author>
      <author><first>Anil</first><last>Bandhakavi</last></author>
      <author><first>Md Shad</first><last>Akhtar</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>6702-6719</pages>
      <abstract>Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. The effectiveness of addressing hate speech involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. The first two phases of CoARL involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and nontoxicity. CoARL outperforms existing benchmarks in intent-conditioned counterspeech generation, showing an average improvement of ∼3 points in intent-conformity and ∼4 points in argument-quality metrics. Extensive human evaluation supports CoARL’s efficacy in generating superior and more context-appropriate responses compared to existing systems, including prominent LLMs like ChatGPT.</abstract>
      <url hash="2b3911bc">2024.naacl-long.373</url>
    </paper>
    <paper id="374">
      <title>Attacks, Defenses and Evaluations for <fixed-case>LLM</fixed-case> Conversation Safety: A Survey</title>
      <author><first>Zhichen</first><last>Dong</last></author>
      <author><first>Zhanhui</first><last>Zhou</last></author>
      <author><first>Chao</first><last>Yang</last></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <pages>6720-6733</pages>
      <abstract>Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.</abstract>
      <url hash="8f7841a6">2024.naacl-long.374</url>
    </paper>
    <paper id="375">
      <title>Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models</title>
      <author><first>Weize</first><last>Liu</last></author>
      <author><first>Guocong</first><last>Li</last></author>
      <author><first>Kai</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Bang</first><last>Du</last></author>
      <author><first>Qiyuan</first><last>Chen</last></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Hongxia</first><last>Xu</last></author>
      <author><first>Jintai</first><last>Chen</last></author>
      <author><first>Jian</first><last>Wu</last></author>
      <pages>6734-6749</pages>
      <abstract>Large language models (LLMs) have achieved remarkable advancements in natural language processing. However, the massive scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained environments. While techniques such as chain-of-thought (CoT) distillation have displayed promise in distilling LLMs into small language models (SLMs), there is a risk that distilled SLMs may still inherit flawed reasoning and hallucinations from LLMs. To address these issues, we propose a twofold methodology: First, we introduce a novel method for distilling the self-evaluation capability from LLMs into SLMs, aiming to mitigate the adverse effects of flawed reasoning and hallucinations inherited from LLMs. Second, we advocate for distilling more comprehensive thinking by incorporating multiple distinct CoTs and self-evaluation outputs, to ensure a more thorough and robust knowledge transfer into SLMs. Experiments on three NLP benchmarks demonstrate that our method significantly improves the performance of distilled SLMs, offering a new perspective for developing more effective and efficient SLMs in resource-constrained environments.</abstract>
      <url hash="71a2aacf">2024.naacl-long.375</url>
    </paper>
    <paper id="376">
      <title>Divergent Token Metrics: Measuring degradation to prune away <fixed-case>LLM</fixed-case> components – and optimize quantization</title>
      <author><first>Björn</first><last>Deiseroth</last><affiliation>Technische Universität Darmstadt and Aleph Alpha</affiliation></author>
      <author><first>Max</first><last>Meuer</last><affiliation>Aleph Alpha</affiliation></author>
      <author><first>Nikolas</first><last>Gritsch</last></author>
      <author><first>Constantin</first><last>Eichenberg</last><affiliation>Aleph Alpha</affiliation></author>
      <author><first>Patrick</first><last>Schramowski</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Matthias</first><last>Aßenmacher</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Kristian</first><last>Kersting</last><affiliation>German Research Center for AI, The Hessian Center for AI and TU Darmstadt</affiliation></author>
      <pages>6750-6769</pages>
      <abstract>Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. However, their ever-increasing size has raised concerns about their effective deployment and the need for LLM compression. This study introduces the Divergent Token Metrics (DTMs), a novel approach to assessing compressed LLMs, addressing the limitations of traditional perplexity or accuracy measures that fail to accurately reflect text generation quality. DTMs measure token divergences that allow deeper insights into the subtleties of model compression, in particular, when evaluating components’ impacts individually. Utilizing the First Divergent Token Metric (FDTM) in model sparsification reveals that 25% of all attention components can be pruned beyond 90% on the Llama-2 model family, still keeping SOTA performance. For quantization, FDTM suggests that more than 80% of parameters can be naively transformed to int8 without special outlier management. These evaluations indicate the necessity of choosing appropriate compressions for parameters individually—and that FDTM can identify those—while standard metrics result in deteriorated outcomes.</abstract>
      <url hash="90877bef">2024.naacl-long.376</url>
    </paper>
    <paper id="377">
      <title>Beyond Performance: Quantifying and Mitigating Label Bias in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yuval</first><last>Reif</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Roy</first><last>Schwartz</last><affiliation>Hebrew University, Hebrew University of Jerusalem</affiliation></author>
      <pages>6770-6784</pages>
      <abstract>Large language models (LLMs) have shown remarkable adaptability to diverse tasks, by leveraging context prompts containing instructions, or minimal input-output examples. However, recent work revealed they also exhibit *label bias*—an undesirable preference toward predicting certain answers over others. Still, detecting and measuring this bias reliably and at scale has remained relatively unexplored. In this study, we evaluate different approaches to quantifying label bias in a model’s predictions, conducting a comprehensive investigation across 279 classification tasks and ten LLMs. Our investigation reveals substantial label bias in models both before and after debiasing attempts, as well as highlights the importance of outcomes-based evaluation metrics, which were not previously used in this regard. We further propose a novel label bias calibration method tailored for few-shot prompting, which outperforms recent calibration approaches for both improving performance and mitigating label bias. Our results emphasize that label bias in the predictions of LLMs remains a barrier to their reliability.</abstract>
      <url hash="67e749a3">2024.naacl-long.377</url>
    </paper>
    <paper id="378">
      <title>Instructing Large Language Models to Identify and Ignore Irrelevant Conditions</title>
      <author><first>Zhenyu</first><last>Wu</last></author>
      <author><first>Chao</first><last>Shen</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>6785-6805</pages>
      <abstract>Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions.Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs.However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.In this paper, we propose a novel approach named I<tex-math>^3</tex-math>C that instructs LLMs to identify and ignore irrelevant conditions.It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question.Then it prompts LLMs to verify the irrelevant conditions.Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I<tex-math>^3</tex-math>C with few-shot reasoning. We develop I<tex-math>^3</tex-math>C-Select that selects the most confusing problems based on the semantic relevance measurement.We conduct extensive experiments on eight MWP datasets.I<tex-math>^3</tex-math>C can be combined with any CoT prompting methods to improve the performance of solving MWPs.Notably, with GPT-3.5-Turbo and I<tex-math>^3</tex-math>C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Complex-CoT by <tex-math>+11.7</tex-math> and <tex-math>+11.1</tex-math>.Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/.</abstract>
      <url hash="c7a7108e">2024.naacl-long.378</url>
    </paper>
    <paper id="379">
      <title>Lower Bounds on the Expressivity of Recurrent Neural Language Models</title>
      <author><first>Anej</first><last>Svete</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Franz</first><last>Nowak</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Anisha</first><last>Sahabdeen</last></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>6806-6826</pages>
      <abstract>The recent successes and spread of large neural language models (LMs) call for a thorough understanding of their abilities. Describing their abilities through LMs’ representational capacity is a lively area of research. Investigations of the representational capacity of neural LMs have predominantly focused on their ability to recognize formal languages. For example, recurrent neural networks (RNNs) as classifiers are tightly linked to regular languages, i.e., languages defined by finite-state automata (FSAs). Such results, however, fall short of describing the capabilities of RNN language models (LMs), which are definitionally distributions over strings. We take a fresh look at the represen- tational capacity of RNN LMs by connecting them to probabilistic FSAs and demonstrate that RNN LMs with linearly bounded precision can express arbitrary regular LMs.</abstract>
      <url hash="83c3ca4f">2024.naacl-long.379</url>
    </paper>
    <paper id="380">
      <title>Transformers Can Represent <tex-math>n</tex-math>-gram Language Models</title>
      <author><first>Anej</first><last>Svete</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>6827-6860</pages>
      <abstract>Plenty of existing work has analyzed the abilities of the transformer architecture by describing its representational capacity with formal models of computation. However, the focus so far has been on analyzing the architecture in terms of language <i>acceptance</i>. We contend that this is an ill-suited problem in the study of <i>language models</i> (LMs), which are definitionally <i>probability distributions</i> over strings. In this paper, we focus on the relationship between transformer LMs and <tex-math>n</tex-math>-gram LMs, a simple and historically relevant class of language models. We show that transformer LMs using the hard or sparse attention mechanisms can exactly represent any <tex-math>n</tex-math>-gram LM, giving us a concrete lower bound on their probabilistic representational capacity. This provides a first step towards understanding the mechanisms that transformer LMs can use to represent probability distributions over strings.</abstract>
      <url hash="b1df5514">2024.naacl-long.380</url>
    </paper>
    <paper id="381">
      <title>The Role of <tex-math>n</tex-math>-gram Smoothing in the Age of Neural Networks</title>
      <author><first>Luca</first><last>Malagutti</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Andrius</first><last>Buinovskij</last></author>
      <author><first>Anej</first><last>Svete</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Afra</first><last>Amini</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>6861-6878</pages>
      <abstract>For nearly three decades, language models derived from the <tex-math>n</tex-math>-gram assumption held the state of the art on the task. The key to their success lay in the application of various smoothing techniques that served to combat overfitting. However, when neural language models toppled <tex-math>n</tex-math>-gram models as the best performers, <tex-math>n</tex-math>-gram smoothing techniques became less relevant. Indeed, it would hardly be an understatement to suggest that the line of inquiry into <tex-math>n</tex-math>-gram smoothing techniques became dormant. This paper re-opens the role classical <tex-math>n</tex-math>-gram smoothing techniques may play in the age of neural language models. First, we draw a formal equivalence between label smoothing, a popular regularization technique for neural language models, and add-<tex-math>\lambda</tex-math> smoothing. Second, we derive a generalized framework for converting any <tex-math>n</tex-math>-gram smoothing technique into a regularizer compatible with neural language models. Our empirical results find that our novel regularizers are comparable to and, indeed, sometimes outperform label smoothing on language modeling and machine translation.</abstract>
      <url hash="60af7613">2024.naacl-long.381</url>
    </paper>
    <paper id="382">
      <title>Reliability Estimation of News Media Sources: Birds of a Feather Flock Together</title>
      <author><first>Sergio</first><last>Burdisso</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Dairazalia</first><last>Sanchez-cortes</last></author>
      <author><first>Esaú</first><last>Villatoro-tello</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <pages>6879-6897</pages>
      <abstract>Evaluating the reliability of news sources is a routine task for journalists and organizations committed to acquiring and disseminating accurate information.Recent research has shown that predicting sources’ reliability represents an important first-prior step in addressing additional challenges such as fake news detection and fact-checking.In this paper, we introduce a novel approach for source reliability estimation that leverages reinforcement learning strategies for estimating the reliability degree of news sources. Contrary to previous research, our proposed approach models the problem as the estimation of a reliability degree, and not a reliability label, based on how all the news media sources interact with each other on the Web.We validated the effectiveness of our method on a news media reliability dataset that is an order of magnitude larger than comparable existing datasets. Results show that the estimated reliability degrees strongly correlates with journalists-provided scores (Spearman=0.80) and can effectively predict reliability labels (macro-avg. F1 score=81.05).We release our implementation and dataset, aiming to provide a valuable resource for the NLP community working on information verification.</abstract>
      <url hash="a981603c">2024.naacl-long.382</url>
    </paper>
    <paper id="383">
      <title>On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons</title>
      <author><first>Takeshi</first><last>Kojima</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Itsuki</first><last>Okimura</last></author>
      <author><first>Yusuke</first><last>Iwasawa</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>6898-6950</pages>
      <abstract>Current decoder-based pre-trained language models (PLMs) successfully demonstrate multilingual capabilities. However, it is unclear how these models handle multilingualism.We analyze the neuron-level internal behavior of multilingual decoder-based PLMs, Specifically examining the existence of neurons that fire “uniquely for each language” within decoder-only multilingual PLMs.We analyze six languages: English, German, French, Spanish, Chinese, and Japanese, and show that language-specific neurons are unique, with a slight overlap (&lt; 5%) between languages. These neurons are mainly distributed in the models’ first and last few layers. This trend remains consistent across languages and models.Additionally, we tamper with less than 1% of the total neurons in each model during inference and demonstrate that tampering with a few language-specific neurons drastically changes the probability of target language occurrence in text generation.</abstract>
      <url hash="79e38fb6">2024.naacl-long.383</url>
    </paper>
    <paper id="384">
      <title><fixed-case>NLP</fixed-case> Progress in Indigenous <fixed-case>L</fixed-case>atin <fixed-case>A</fixed-case>merican Languages</title>
      <author><first>Atnafu</first><last>Tonja</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Instituto Politécnico Nacional</affiliation></author>
      <author><first>Fazlourrahman</first><last>Balouchzahi</last></author>
      <author><first>Sabur</first><last>Butt</last></author>
      <author><first>Olga</first><last>Kolesnikova</last><affiliation>Instituto Politécnico Nacional</affiliation></author>
      <author><first>Hector</first><last>Ceballos</last><affiliation>Tecnologico de Monterrey</affiliation></author>
      <author><first>Alexander</first><last>Gelbukh</last><affiliation>Instituto Politécnico Nacional</affiliation></author>
      <author><first>Thamar</first><last>Solorio</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of Houston</affiliation></author>
      <pages>6951-6966</pages>
      <abstract>The paper focuses on the marginalization of indigenous language communities in the face of rapid technological advancements. We highlight the cultural richness of these languages and the risk they face of being overlooked in the realm of Natural Language Processing (NLP). We aim to bridge the gap between these communities and researchers, emphasizing the need for inclusive technological advancements that respect indigenous community perspectives. We show the NLP progress of indigenous Latin American languages and the survey that covers the status of indigenous languages in Latin America, their representation in NLP, and the challenges and innovations required for their preservation and development. The paper contributes to the current literature in understanding the need and progress of NLP for indigenous communities of Latin America, specifically low-resource and indigenous communities in general.</abstract>
      <url hash="7a76d11a">2024.naacl-long.384</url>
    </paper>
    <paper id="385">
      <title>On the Effectiveness of Adversarial Robustness for Abuse Mitigation with Counterspeech</title>
      <author><first>Yi-Ling</first><last>Chung</last><affiliation>Alan Turing Institute</affiliation></author>
      <author><first>Jonathan</first><last>Bright</last><affiliation>Alan Turing Institute</affiliation></author>
      <pages>6967-6981</pages>
      <abstract>Recent work on automated approaches to counterspeech have mostly focused on synthetic data but seldom look into how the public deals with abuse. While these systems identifying and generating counterspeech have the potential for abuse mitigation, it remains unclear how robust a model is against adversarial attacks across multiple domains and how models trained on synthetic data can handle unseen user-generated abusive content in the real world. To tackle these issues, this paper first explores the dynamics of abuse and replies using our novel dataset of 6,955 labelled tweets targeted at footballers for studying public figure abuse. We then curate DynaCounter, a new English dataset of 1,911 pairs of abuse and replies addressing nine minority identity groups, collected in an adversarial human-in-the-loop process over four rounds. Our analysis shows that adversarial attacks do not necessarily result in better generalisation. We further present a study of multi-domain counterspeech generation, comparing Flan-T5 and T5 models. We observe that handling certain abuse targets is particularly challenging.</abstract>
      <url hash="b6873ba7">2024.naacl-long.385</url>
    </paper>
    <paper id="386">
      <title>Leveraging the Structure of Pre-trained Embeddings to Minimize Annotation Effort</title>
      <author><first>Cesar</first><last>Gonzalez-Gutierrez</last><affiliation>Universitat Politècnica de Catalunya</affiliation></author>
      <author><first>Ariadna</first><last>Quattoni</last><affiliation>Universidad Politécnica de Cataluna</affiliation></author>
      <pages>6982-6996</pages>
      <abstract>Most current state-of-the-art approaches for text classification are based on fine-tuning the representations computed by large language models (LLMs). This strategy has led to significant improvements in classification performance and contributed to a reduction of the amount of labeled data required for training a model. However, for some challenging classification tasks, providing enough annotations to ensure a reliable classification continues to be the main bottleneck. This is especially true in settings of highly imbalanced class distributions. This paper proposes to tackle this bottleneck by exploiting the structural properties of pre-trained embeddings. We develop a label propagation method that uses pre-trained embeddings to spread information from the labeled samples to nearby samples in the induced space, ensuring the optimal use of annotations. Our approach is simple and relatively low-cost since it only requires computing some distances in the embedded space. We conduct experiments on different text classification datasets showing that the proposed method is efficient and significantly outperforms both self-training and random walk label propagation strategies.</abstract>
      <url hash="3708695a">2024.naacl-long.386</url>
    </paper>
    <paper id="387">
      <title><fixed-case>U</fixed-case>ni<fixed-case>A</fixed-case>rk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing</title>
      <author><first>Yijun</first><last>Yang</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <author><first>Jie</first><last>He</last></author>
      <author><first>Pinzhen</first><last>Chen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Victor</first><last>Gutierrez Basulto</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jeff</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>6997-7014</pages>
      <abstract>Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, **UniArk**, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model’s out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct **ParaTrex**, a large-scale and diverse dataset for measuring the inconsistency and out-of-domain generation of models. Further, ParaTrex offers a reference method for constructing paraphrased datasets using large language models.</abstract>
      <url hash="70652284">2024.naacl-long.387</url>
    </paper>
    <paper id="388">
      <title>Adaptive-<fixed-case>RAG</fixed-case>: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity</title>
      <author><first>Soyeong</first><last>Jeong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jinheon</first><last>Baek</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sukmin</first><last>Cho</last></author>
      <author><first>Sung Ju</first><last>Hwang</last><affiliation>Korea Advanced Institute of Science and Technology and AITRICS</affiliation></author>
      <author><first>Jong</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>7015-7029</pages>
      <abstract>Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.</abstract>
      <url hash="4943f6d8">2024.naacl-long.388</url>
    </paper>
    <paper id="389">
      <title>Knowing What <fixed-case>LLM</fixed-case>s <fixed-case>DO</fixed-case> <fixed-case>NOT</fixed-case> Know: A Simple Yet Effective Self-Detection Method</title>
      <author><first>Yukun</first><last>Zhao</last></author>
      <author><first>Lingyong</first><last>Yan</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <author><first>Guoliang</first><last>Xing</last></author>
      <author><first>Chong</first><last>Meng</last><affiliation>Baidu</affiliation></author>
      <author><first>Shuaiqiang</first><last>Wang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Zhicong</first><last>Cheng</last></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <pages>7030-7042</pages>
      <abstract>Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks.However, recent literature reveals that LLMs hallucinate intermittently, which impedes their reliability for further utilization. In this paper, we propose a novel self-detection method to detect which questions an LLM does not know.Our proposal is empirical and applicable for continually upgrading LLMs compared with state-of-the-art methods. Specifically, we examine the divergence of the LLM’s behaviors on different verbalizations for a question and examine the atypicality of the verbalized input. We combine the two components to identify whether the model generates a non-factual response to the question. The above components can be accomplished by utilizing the LLM itself without referring to any other external resources. We conduct comprehensive experiments and demonstrate the effectiveness of our method for recently released LLMs involving Llama 2, Vicuna, ChatGPT, and GPT-4 across factoid question-answering, arithmetic reasoning, and commonsense reasoning tasks.</abstract>
      <url hash="f5c40939">2024.naacl-long.389</url>
    </paper>
    <paper id="390">
      <title>Are Large Language Model Temporally Grounded?</title>
      <author><first>Yifu</first><last>Qiu</last></author>
      <author><first>Zheng</first><last>Zhao</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Yftah</first><last>Ziser</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Anna</first><last>Korhonen</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Edoardo</first><last>Ponti</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Shay</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>7043-7062</pages>
      <abstract>Are Large Language Models (LLMs) temporally grounded? Since LLMs cannot perceive and interact with the environment, it is impossible to answer this question directly. Instead, we provide LLMs with textual narratives and probe them with respect to their common-sense knowledge of the structure and duration of events, their ability to order events along a timeline, and self-consistency within their temporal model (e.g., temporal relations such as after and before are mutually exclusive for any pair of events). We evaluate state-of-the-art LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities. Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions. Contrary to expectations, we also find that scaling the model size does not guarantee positive gains in performance. To explain these results, we study the sources from which LLMs may gather temporal information: we find that sentence ordering in unlabelled texts, available during pre-training, is only weakly correlated with event ordering. Moreover, public instruction tuning mixtures contain few temporal tasks. Hence, we conclude that current LLMs lack a consistent temporal model of textual narratives.</abstract>
      <url hash="9c345ff4">2024.naacl-long.390</url>
    </paper>
    <paper id="391">
      <title>Document Image Machine Translation with Dynamic Multi-pre-trained Models Assembling</title>
      <author><first>Yupu</first><last>Liang</last></author>
      <author><first>Yaping</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Cong</first><last>Ma</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Zhiyang</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lu</first><last>Xiang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yu</first><last>Zhou</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>7063-7074</pages>
      <abstract>Text image machine translation (TIMT) is a task that translates source texts embedded in the image to target translations. The existing TIMT task mainly focuses on text-line-level images. In this paper, we extend the current TIMT task and propose a novel task, **D**ocument **I**mage **M**achine **T**ranslation to **Markdown** (**DIMT2Markdown**), which aims to translate a source document image with long context and complex layout structure to markdown-formatted target translation.We also introduce a novel framework, **D**ocument **I**mage **M**achine **T**ranslation with **D**ynamic multi-pre-trained models **A**ssembling (**DIMTDA**).A dynamic model assembler is used to integrate multiple pre-trained models to enhance the model’s understanding of layout and translation capabilities.Moreover, we build a novel large-scale **Do**cument image machine **T**ranslation dataset of **A**rXiv articles in markdown format (**DoTA**), containing 126K image-translation pairs.Extensive experiments demonstrate the feasibility of end-to-end translation of rich-text document images and the effectiveness of DIMTDA.</abstract>
      <url hash="e7f8a560">2024.naacl-long.391</url>
    </paper>
    <paper id="392">
      <title>Elastic Weight Removal for Faithful and Abstractive Dialogue Generation</title>
      <author><first>Nico</first><last>Daheim</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <author><first>Edoardo</first><last>Ponti</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>7075-7091</pages>
      <abstract>Generating factual responses is a crucial requirement for dialogue systems. To promotemore factual responses, a common strategyis to ground their responses in relevant documents that inform response generation. However, common dialogue models still often hallucinate information that was not containedin these documents and is therefore unfaithful. In this work, we propose to alleviate suchhallucinations by ‘subtracting’ the parametersof a model trained to hallucinate from a dialogue response generation model in order to‘negate’ the contribution of such hallucinatedexamples from it. Extensive automatic and human evaluation shows favourable results whencompared to state-of-the-art methods that combine the distributions of multiple models, suchas DExperts (Liu et al., 2021), and others thatchange the training procedure, such as Quark(Lu et al., 2022a). Finally, we show how wecan not only reduce hallucinations but also discourage extractive responses, which are oftena consequence of reducing hallucinations byencouraging copy-pasting of document spans.We publicly release our code for reproducibilityand facilitating further research.</abstract>
      <url hash="66a3640e">2024.naacl-long.392</url>
    </paper>
    <paper id="393">
      <title><fixed-case>R</fixed-case>-Tuning: Instructing Large Language Models to Say ‘<fixed-case>I</fixed-case> Don’t Know’</title>
      <author><first>Hanning</first><last>Zhang</last></author>
      <author><first>Shizhe</first><last>Diao</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yong</first><last>Lin</last></author>
      <author><first>Yi</first><last>Fung</last></author>
      <author><first>Qing</first><last>Lian</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xingyao</first><last>Wang</last><affiliation>Department of Computer Science, University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Yangyi</first><last>Chen</last><affiliation>School of Computer Science, University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>7092-7118</pages>
      <abstract>Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a model’s ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing. Our code is available at https://github.com/shizhediao/R-Tuning</abstract>
      <url hash="61671fce">2024.naacl-long.393</url>
    </paper>
    <paper id="394">
      <title>Bridging the Gap between Different Vocabularies for <fixed-case>LLM</fixed-case> Ensemble</title>
      <author><first>Yangyifan</first><last>Xu</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Jinliang</first><last>Lu</last><affiliation>Institute of automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>7119-7131</pages>
      <abstract>Ensembling different large language models (LLMs) to unleash their complementary potential and harness their individual strengths is highly valuable. Nevertheless, vocabulary discrepancies among various LLMs have constrained previous studies to either selecting or blending completely generated outputs. This limitation hinders the dynamic correction and enhancement of outputs during the generation process, resulting in a limited capacity for effective ensemble. To address this issue, we propose a novel method to <tex-math>\textbf{E}</tex-math>nsemble LLMs via <tex-math>\textbf{V}</tex-math>ocabulary <tex-math>\textbf{A}</tex-math>lignment (EVA). EVA bridges the lexical gap among various LLMs, enabling meticulous ensemble at each generation step. Specifically, we first learn mappings between the vocabularies of different LLMs with the assistance of overlapping tokens. Subsequently, these mappings are employed to project output distributions of LLMs into a unified space, facilitating a fine-grained ensemble. Finally, we design a filtering strategy to exclude models that generate unfaithful tokens. Experimental results on commonsense reasoning, arithmetic reasoning, machine translation, and data-to-text generation tasks demonstrate the superiority of our approach compared with individual LLMs and previous ensemble methods conducted on complete outputs. Further analyses confirm that our approach can leverage knowledge from different language models and yield consistent improvement.</abstract>
      <url hash="90aabc79">2024.naacl-long.394</url>
    </paper>
    <paper id="395">
      <title><fixed-case>K</fixed-case>now<fixed-case>LA</fixed-case>: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation</title>
      <author><first>Xindi</first><last>Luo</last></author>
      <author><first>Zequn</first><last>Sun</last></author>
      <author><first>Jing</first><last>Zhao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhe</first><last>Zhao</last></author>
      <author><first>Wei</first><last>Hu</last><affiliation>Nanjing University</affiliation></author>
      <pages>7132-7145</pages>
      <abstract>Parameter-efficient finetuning (PEFT) is a key technique for adapting large language models (LLMs) to downstream tasks. In this paper, we study leveraging knowledge graph embeddings to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an LLM to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six benchmarks with two popular LLMs and three knowledge graphs demonstrate the effectiveness and robustness of KnowLA. We show that KnowLA can help activate the relevant parameterized knowledge in an LLM to answer a question without changing its parameters or input prompts.</abstract>
      <url hash="e09be062">2024.naacl-long.395</url>
    </paper>
    <paper id="396">
      <title>Extremely Weakly-supervised Text Classification with Wordsets Mining and Sync-Denoising</title>
      <author><first>Lysa</first><last>Xiao</last></author>
      <pages>7146-7158</pages>
      <abstract>Extremely weakly-supervised text classification aims to classify texts without any labeled data, but only relying on class names as supervision. Existing works include prompt-based and seed-based methods. Prompt-based methods prompt language model with instructions, while seed-based methods generate pseudo-labels with word matching. Both of them have significant flaws, including zero-shot instability and context-dependent ambiguities. This paper introduces SetSync, which follows a new paradigm, i.e. wordset-based, which can avoid the above problems. In SetSync, a class is represented with wordsets, and pseudo-labels are generated with wordsets matching. To facilitate this, we propose to use information bottleneck to identify class-relevant wordsets. Moreover, we regard the classifier training as a hybrid learning of semi-supervised and noisy-labels, and propose a new training strategy, termed sync-denoising. Extensive experiments on 11 datasets show that SetSync outperforms all existing prompt and seed methods, exceeding SOTA by an impressive average of 8 points.</abstract>
      <url hash="c13f5451">2024.naacl-long.396</url>
    </paper>
    <paper id="397">
      <title><fixed-case>F</fixed-case>-<fixed-case>MALLOC</fixed-case>: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation</title>
      <author><first>Junhong</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuchen</first><last>Liu</last></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>7159-7171</pages>
      <abstract>In the evolving landscape of Neural Machine Translation (NMT), the pretrain-then-finetune paradigm has yielded impressive results. However, the persistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While previous work has introduced Continual Learning (CL) methods to address CF, these approaches grapple with the delicate balance between avoiding forgetting and maintaining system extensibility. To address this, we propose a CL method, named <tex-math>\textbf{F-MALLOC}</tex-math> (<tex-math>\textbf{F}</tex-math>eed-forward <tex-math>\textbf{M}</tex-math>emory <tex-math>\textbf{ALLOC}</tex-math>ation). F-MALLOC is inspired by recent insights highlighting that feed-forward layers emulate neural memories and encapsulate crucial translation knowledge. It decomposes feed-forward layers into discrete memory cells and allocates these memories to different tasks. By learning to allocate and safeguard these memories, our method effectively alleviates CF while ensuring robust extendability. Besides, we propose a comprehensive assessment protocol for multi-stage CL of NMT systems. Experiments conducted following this new protocol showcase the superior performance of F-MALLOC, evidenced by higher BLEU scores and almost zero forgetting.</abstract>
      <url hash="7ff4f0bf">2024.naacl-long.397</url>
    </paper>
    <paper id="398">
      <title>Towards Reducing Diagnostic Errors with Interpretable Risk Prediction</title>
      <author><first>Denis</first><last>McInerney</last></author>
      <author><first>William</first><last>Dickinson</last></author>
      <author><first>Lucy</first><last>Flynn</last><affiliation>Brigham and Women’s Hospital</affiliation></author>
      <author><first>Andrea</first><last>Young</last><affiliation>Brigham and Women’s Hospital, Harvard University</affiliation></author>
      <author><first>Geoffrey</first><last>Young</last><affiliation>Harvard Medical School</affiliation></author>
      <author><first>Jan-Willem</first><last>Van De Meent</last><affiliation>Northeastern University, Northeastern University, University of Amsterdam and Northeastern University</affiliation></author>
      <author><first>Byron</first><last>Wallace</last><affiliation>Northeastern University, Brown University and Northeastern University</affiliation></author>
      <pages>7172-7189</pages>
      <abstract>Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual “true” diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.</abstract>
      <url hash="ca00c5b0">2024.naacl-long.398</url>
    </paper>
    <paper id="399">
      <title>Generalizable Multilingual Hate Speech Detection on Low Resource <fixed-case>I</fixed-case>ndian Languages using Fair Selection in Federated Learning</title>
      <author><first>Akshay</first><last>Singh</last></author>
      <author><first>Rahul</first><last>Thakur</last></author>
      <pages>7190-7200</pages>
      <abstract>Social media, originally meant for peaceful communication, now faces issues with hate speech. Detecting hate speech from social media in Indian languages with linguistic diversity and cultural nuances presents a complex and challenging task. Furthermore, traditional methods involve sharing of users’ sensitive data with a server for model training making it undesirable and involving potential risk to their privacy remained under-studied. In this paper, we combined various low-resource language datasets and propose MultiFED, a federated approach that performs effectively to detect hate speech. MultiFED utilizes continuous adaptation and fine-tuning to aid generalization using subsets of multilingual data overcoming the limitations of data scarcity. Extensive experiments are conducted on 13 Indic datasets across five different pre-trained models. The results show that MultiFED outperforms the state-of-the-art baselines by 8% (approx.) in terms of Accuracy and by 12% (approx.) in terms of F-Score.</abstract>
      <url hash="2d8e27d8">2024.naacl-long.399</url>
    </paper>
    <paper id="400">
      <title>Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks</title>
      <author><first>Nadezhda</first><last>Chirkova</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Vassilina</first><last>Nikoulina</last><affiliation>Naver Labs Europe</affiliation></author>
      <pages>7201-7217</pages>
      <abstract>Zero-shot cross-lingual transfer, which implies finetuning of the multilingual pretrained language model on input-output pairs in one language and using it to make task predictions for inputs in other languages, was widely studied for natural language understanding but is understudied for generation. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size, and NLLB-200 can be competitive in some cases. Our final zero-shot models reach the performance of the approach based on data translation which is usually considered as an upper baseline for zero-shot cross-lingual transfer in generation.</abstract>
      <url hash="6cda5ff7">2024.naacl-long.400</url>
    </paper>
    <paper id="401">
      <title>The Impact of Depth on Compositional Generalization in Transformer Language Models</title>
      <author><first>Jackson</first><last>Petty</last><affiliation>New York University</affiliation></author>
      <author><first>Sjoerd</first><last>Steenkiste</last><affiliation>Google</affiliation></author>
      <author><first>Ishita</first><last>Dasgupta</last><affiliation>DeepMind</affiliation></author>
      <author><first>Fei</first><last>Sha</last></author>
      <author><first>Dan</first><last>Garrette</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Tal</first><last>Linzen</last><affiliation>New York University and Google</affiliation></author>
      <pages>7218-7231</pages>
      <abstract>To process novel sentences, language models (LMs) must generalize compositionally—combine familiar elements in new ways. What aspects of a model’s structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by theoretical and empirical work, that deeper transformers generalize more compositionally. Simply adding layers increases the total number of parameters; to address this confound between depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize more compositionally than shallower models do, but the benefit of additional layers diminishes rapidly; (2) within each family, deeper models show better language modeling performance, but returns are similarly diminishing; (3) the benefits of depth for compositional generalization cannot be attributed solely to better performance on language modeling. Because model latency is approximately linear in the number of layers, these results lead us to the recommendation that, with a given total parameter budget, transformers can be made shallower than is typical without sacrificing performance.</abstract>
      <url hash="fe4401f1">2024.naacl-long.401</url>
    </paper>
    <paper id="402">
      <title>Pregnant Questions: The Importance of Pragmatic Awareness in Maternal Health Question Answering</title>
      <author><first>Neha</first><last>Srikanth</last></author>
      <author><first>Rupak</first><last>Sarkar</last></author>
      <author><first>Heran</first><last>Mane</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Elizabeth</first><last>Aparicio</last></author>
      <author><first>Quynh</first><last>Nguyen</last></author>
      <author><first>Rachel</first><last>Rudinger</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jordan</first><last>Boyd-Graber</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>7232-7247</pages>
      <abstract>Questions posed by information-seeking users often contain implicit false or potentially harmful assumptions. In a high-risk domain such as maternal and infant health, a question-answering system must recognize these pragmatic constraints and go beyond simply answering user questions, examining them in context to respond helpfully. To achieve this, we study assumptions and implications, or pragmatic inferences, made when mothers ask questions about pregnancy and infant care by collecting a dataset of 2,727 inferences from 500 questions across three diverse sources. We study how health experts naturally address these inferences when writing answers, and illustrate that informing existing QA pipelines with pragmatic inferences produces responses that are more complete, mitigating the propagation of harmful beliefs.</abstract>
      <url hash="8e269a54">2024.naacl-long.402</url>
    </paper>
    <paper id="403">
      <title>Towards Explainability in Legal Outcome Prediction Models</title>
      <author><first>Josef</first><last>Valvoda</last></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>7248-7268</pages>
      <abstract>Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand the model’s decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and neural models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.</abstract>
      <url hash="5461d45e">2024.naacl-long.403</url>
    </paper>
    <paper id="404">
      <title>The steerability of large language models toward data-driven personas</title>
      <author><first>Junyi</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Charith</first><last>Peris</last><affiliation>Amazon</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Palash</first><last>Goyal</last><affiliation>Amazon</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Richard</first><last>Zemel</last><affiliation>Department of Computer Science, Columbia University and Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <pages>7269-7284</pages>
      <abstract>Large language models (LLMs) are known to generate biased responses where the opinions of certain groups and populations are underrepresented. Here, we present a novel approach to achieve controllable generation of specific viewpoints using LLMs, that can be leveraged to produce multiple perspectives and to reflect the diverse opinions. Moving beyond the traditional reliance on demographics like age, gender, or party affiliation, we introduce a data-driven notion of persona grounded in collaborative filtering, which is defined as either a single individual or a cohort of individuals manifesting similar views across specific inquiries. As individuals in the same demographic group may have different personas, our data-driven persona definition allows for a more nuanced understanding of different (latent) social groups present in the population. In addition to this, we also explore an efficient method to steer LLMs toward the personas that we define. We show that our data-driven personas significantly enhance model steerability, with improvements of between 57%-77% over our best performing baselines.</abstract>
      <url hash="e9dc5f37">2024.naacl-long.404</url>
    </paper>
    <paper id="405">
      <title><fixed-case>CCS</fixed-case>um: A Large-Scale and High-Quality Dataset for Abstractive News Summarization</title>
      <author><first>Xiang</first><last>Jiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Markus</first><last>Dreyer</last><affiliation>Amazon</affiliation></author>
      <pages>7285-7315</pages>
      <abstract>Training a supervised news summarization model requires large amounts of high-quality training data consisting of news articles paired with reference summaries. However, obtaining such data is costly, and existing datasets contain considerable amount of noise. We present a new large-scale and high-quality dataset for supervised abstractive news summarization containing 1.3 million training samples, which we call CCSum. In creating this dataset, we take advantage of the journalistic inverted-pyramid style in news writing: In some articles, the first sentence can be considered a summary of the reported story. Accordingly, among 35 million CommonCrawl News articles, we identify pairs of articles about the same news story and use one article’s first sentence as the summary for the other article. To ensure high quality, we apply strict filters whose parameters we optimize using Bayesian optimization. We show that the resulting dataset is more factual and informative than established summarization datasets; less than 1% of the summaries have major factual inconsistencies with the corresponding news articles, compared to 5.5% to 15.4% in existing datasets, according to our human evaluation. Summarization models trained on our dataset are more favored compared to those trained on CNN/Daily Mail. The proposed dataset can open new opportunities for future research in abstractive summarization.</abstract>
      <url hash="8777d23c">2024.naacl-long.405</url>
    </paper>
    <paper id="406">
      <title>Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks</title>
      <author><first>Negar</first><last>Mokhberian</last></author>
      <author><first>Myrl</first><last>Marmarelis</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <author><first>Frederic</first><last>Hopp</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Valerio</first><last>Basile</last><affiliation>University of Turin</affiliation></author>
      <author><first>Fred</first><last>Morstatter</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <author><first>Kristina</first><last>Lerman</last><affiliation>University of Southern California and USC Information Sciences Institute</affiliation></author>
      <pages>7316-7328</pages>
      <abstract>Supervised classification heavily depends on datasets annotated by humans. However, in subjective tasks such as toxicity classification, these annotations often exhibit low agreement among raters. Annotations have commonly been aggregated by employing methods like majority voting to determine a single ground truth label. In subjective tasks, aggregating labels will result in biased labeling and, consequently, biased models that can overlook minority opinions. Previous studies have shed light on the pitfalls of label aggregation and have introduced a handful of practical approaches to tackle this issue. Recently proposed multi-annotator models, which predict labels individually per annotator, are vulnerable to under-determination for annotators with few samples. This problem is exacerbated in crowdsourced datasets. In this work, we propose Annotator Aware Representations for Texts (AART) for subjective classification tasks. Our approach involves learning representations of annotators, allowing for exploration of annotation behaviors. We show the improvement of our method on metrics that assess the performance on capturing individual annotators’ perspectives. Additionally, we demonstrate fairness metrics to evaluate our model’s equability of performance for marginalized annotators compared to others.</abstract>
      <url hash="c010770b">2024.naacl-long.406</url>
    </paper>
    <paper id="407">
      <title>Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in <fixed-case>T</fixed-case>o<fixed-case>TT</fixed-case>o</title>
      <author><first>Barkavi</first><last>Sundararajan</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Yaji</first><last>Sripada</last><affiliation>Arria NLG and University of Aberdeen</affiliation></author>
      <author><first>Ehud</first><last>Reiter</last><affiliation>University of Aberdeen</affiliation></author>
      <pages>7329-7355</pages>
      <abstract>Neural Table-to-Text models tend to hallucinate, producing texts that contain factual errors. We investigate whether such errors in the output can be traced back to problems with the input. We manually annotated 1,837 texts generated by multiple models in the politics domain of the ToTTo dataset. We identify the input problems that are responsible for many output errors and show that fixing these inputs reduces factual errors by between 52% and 76% (depending on the model). In addition, we observe that models struggle in processing tabular inputs that are structured in a non-standard way, particularly when the input lacks distinct row and column values or when the column headers are not correctly mapped to corresponding values.</abstract>
      <url hash="5a655394">2024.naacl-long.407</url>
    </paper>
    <paper id="408">
      <title><fixed-case>CERET</fixed-case>: Cost-Effective Extrinsic Refinement for Text Generation</title>
      <author><first>Jason</first><last>Cai</last><affiliation>Amazon</affiliation></author>
      <author><first>Hang</first><last>Su</last><affiliation>Amazon</affiliation></author>
      <author><first>Monica</first><last>Sunkara</last></author>
      <author><first>Igor</first><last>Shalyminov</last><affiliation>Amazon</affiliation></author>
      <author><first>Saab</first><last>Mansour</last><affiliation>Amazon</affiliation></author>
      <pages>7356-7369</pages>
      <abstract>Large Language Models (LLMs) are powerful models for generation tasks, but they may not generate good quality outputs in their first attempt. Apart from model fine-tuning, existing approaches to improve prediction accuracy and quality typically involve LLM self-improvement / self-reflection that incorporate feedback from models themselves. Despite their effectiveness, these methods are hindered by their high computational cost and lack of scalability. In this work, we propose CERET, a method for refining text generations by considering semantic stability, entailment and inter-sample uncertainty measures. Experimental results show that CERET outperforms Self-consistency and Self-rerank baselines consistently under various task setups, by 1.6% in Rouge-1 for abstractive summarization and 3.5% in hit rate for question answering. Compared to LLM Self-rerank method, our approach only requires 9.4% of its latency and is more cost-effective.</abstract>
      <url hash="8bcda464">2024.naacl-long.408</url>
    </paper>
    <paper id="409">
      <title>Parameter-Efficient Instruction Tuning of Large Language Models For Extreme Financial Numeral Labelling</title>
      <author><first>Subhendu</first><last>Khatuya</last></author>
      <author><first>Rajdeep</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Akash</first><last>Ghosh</last></author>
      <author><first>Manjunath</first><last>Hegde</last></author>
      <author><first>Koustuv</first><last>Dasgupta</last></author>
      <author><first>Niloy</first><last>Ganguly</last><affiliation>Indian Institute of Technology Kharagpur,</affiliation></author>
      <author><first>Saptarshi</first><last>Ghosh</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <pages>7370-7382</pages>
      <abstract>We study the problem of automatically annotating relevant numerals (GAAP metrics) occurring in the financial documents with their corresponding XBRL tags. Different from prior works, we investigate the feasibility of solving this extreme classification problem using a generative paradigm through instruction tuning of Large Language Models (LLMs). To this end, we leverage metric metadata informationto frame our target outputs while proposing a parameter efficient solution for the task using LoRA. We perform experiments on two recently released financial numeric labeling datasets. Our proposed model, **FLAN-FinXC**, achieves new state-of-the-art performances on both the datasets, outperforming several strong baselines. We explain the better scores of our proposed model by demonstrating its capability for zero-shot as well as the least frequently occurring tags. Also, even when we fail to predict the XBRL tags correctly, our generated output has substantial overlap with the ground-truth in majority of the cases.</abstract>
      <url hash="19a4ea71">2024.naacl-long.409</url>
    </paper>
    <paper id="410">
      <title>Analysis of State-Level Legislative Process in Enhanced Linguistic and Nationwide Network Contexts</title>
      <author><first>Maryam</first><last>Davoodi</last><affiliation>Purdue University</affiliation></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University, Purdue University and Purdue University</affiliation></author>
      <pages>7383-7401</pages>
      <abstract>State bills have a significant impact on various aspects of society, including health, education, and the economy. Consequently, it is crucial to conduct systematic research on state bills before and after they are enacted to evaluate their benefits and drawbacks, thereby guiding future decision-making. In this work, we developed the first state-level deep learning framework that (1) handles the complex and inconsistent language of policies across US states using generative large language models and (2) decodes legislators’ behavior and implications of state policies by establishing a shared nationwide network, enriched with diverse contexts, such as information on interest groups influencing public policy and legislators’ courage test results, which reflect their political positions.</abstract>
      <url hash="7c9267de">2024.naacl-long.410</url>
    </paper>
    <paper id="411">
      <title><fixed-case>D</fixed-case>e<fixed-case>M</fixed-case>u<fixed-case>X</fixed-case>: Data-efficient Multilingual Learning</title>
      <author><first>Simran</first><last>Khanuja</last><affiliation>CMU, Carnegie Mellon University and Google</affiliation></author>
      <author><first>Srinivas</first><last>Gowriraj</last></author>
      <author><first>Lucio</first><last>Dery</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>7402-7415</pages>
      <abstract>Pre-trained multilingual models have enabled deployment of NLP technologies for multiple languages. However, optimally fine-tuning these models under an annotation budget, such that performance on desired target languages is jointly maximized, still remains an open question. In this paper, we introduce DeMuX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points. Our code is released here: https://github.com/simran-khanuja/demux.</abstract>
      <url hash="57c6497c">2024.naacl-long.411</url>
    </paper>
    <paper id="412">
      <title><fixed-case>DUQG</fixed-case>en: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation</title>
      <author><first>Ramraj</first><last>Chandradevan</last></author>
      <author><first>Kaustubh</first><last>Dhole</last><affiliation>BITS Pilani</affiliation></author>
      <author><first>Eugene</first><last>Agichtein</last><affiliation>Amazon and Emory University</affiliation></author>
      <pages>7416-7430</pages>
      <abstract>State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot. However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information. Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse training dataset by probabilistic sampling over the resulting document clusters. Our extensive experiments, over the standard BEIR collection, demonstrate that DUQGen consistently outperforms all zero-shot baselines and substantially outperforms the SOTA baselines on 16 out of 18 datasets, for an average of 4% relative improvement across all datasets. We complement our results with a thorough analysis for more in-depth understanding of the proposed method’s performance and to identify promising areas for further improvements.</abstract>
      <url hash="198cea30">2024.naacl-long.412</url>
    </paper>
    <paper id="413">
      <title>How did we get here? Summarizing conversation dynamics</title>
      <author><first>Yilun</first><last>Hua</last><affiliation>Department of Computer Science, Cornell University</affiliation></author>
      <author><first>Nicholas</first><last>Chernogor</last><affiliation>Cornell University</affiliation></author>
      <author><first>Yuzhe</first><last>Gu</last></author>
      <author><first>Seoyeon</first><last>Jeong</last></author>
      <author><first>Miranda</first><last>Luo</last><affiliation>Cornell University</affiliation></author>
      <author><first>Cristian</first><last>Danescu-Niculescu-Mizil</last><affiliation>Cornell University and Cornell University</affiliation></author>
      <pages>7431-7456</pages>
      <abstract>Throughout a conversation, the way participants interact with each other is in constant flux: their tones may change, they may resort to different strategies to convey their points, or they might alter their interaction patterns. An understanding of these dynamics can complement that of the actual facts and opinions discussed, offering a more holistic view of the trajectory of the conversation: how it arrived at its current state and where it is likely heading.In this work, we introduce the task of summarizing the dynamics of conversations, by constructing a dataset of human-written summaries, and exploring several automated baselines. We evaluate whether such summaries can capture the trajectory of conversations via an established downstream task: forecasting whether an ongoing conversation will eventually derail into toxic behavior. We show that they help both humans and automated systems with this forecasting task. Humans make predictions three times faster, and with greater confidence, when reading the summaries than when reading the transcripts. Furthermore, automated forecasting systems are more accurate when constructing, and then predicting based on, summaries of conversation dynamics, compared to directly predicting on the transcripts.</abstract>
      <url hash="f56be1e0">2024.naacl-long.413</url>
    </paper>
    <paper id="414">
      <title>Can Language Model Moderators Improve the Health of Online Discourse?</title>
      <author><first>Hyundong</first><last>Cho</last><affiliation>USC/ISI</affiliation></author>
      <author><first>Shuai</first><last>Liu</last><affiliation>University of Southern California, Information Sciences Institute</affiliation></author>
      <author><first>Taiwei</first><last>Shi</last></author>
      <author><first>Darpan</first><last>Jain</last></author>
      <author><first>Basem</first><last>Rizk</last><affiliation>USC Institute for Creative Technologies, University of Southern California</affiliation></author>
      <author><first>Yuyang</first><last>Huang</last></author>
      <author><first>Zixun</first><last>Lu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Nuan</first><last>Wen</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jonathan</first><last>Gratch</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Emilio</first><last>Ferrara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>7457-7475</pages>
      <abstract>Conversational moderation of online communities is crucial to maintaining civility for a constructive environment, but it is challenging to scale and harmful to moderators. The inclusion of sophisticated natural language generation modules as a force multiplier to aid human moderators is a tantalizing prospect, but adequate evaluation approaches have so far been elusive. In this paper, we establish a systematic definition of conversational moderation effectiveness grounded on moderation literature and establish design criteria for conducting realistic yet safe evaluation. We then propose a comprehensive evaluation framework to assess models’ moderation capabilities independently of human intervention. With our framework, we conduct the first known study of language models as conversational moderators, finding that appropriately prompted models that incorporate insights from social science can provide specific and fair feedback on toxic behavior but struggle to influence users to increase their levels of respect and cooperation.</abstract>
      <url hash="321a8d84">2024.naacl-long.414</url>
    </paper>
    <paper id="415">
      <title><fixed-case>L</fixed-case>ean<fixed-case>R</fixed-case>easoner: Boosting Complex Logical Reasoning with Lean</title>
      <author><first>Dongwei</first><last>Jiang</last></author>
      <author><first>Marcio</first><last>Fonseca</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Shay</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>7476-7489</pages>
      <abstract>Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty ofsuch reasoning. We use Lean, a theorem proving framework, to address these challenges. By formalizing logical reasoning problems intotheorems within Lean, we can solve them by proving or disproving the corresponding theorems. This method reduces the risk of logical inconsistencies with the help of Lean’s symbolic solver. It also enhances our ability to treat complex reasoning tasks using Lean’s extensive library of theorem proofs. Our method achieves state-of-the-art performance on the FOLIO dataset and achieves performance near this level on ProofWriter. Notably, these results were accomplished by fine-tuning on fewer than 100 in-domain samples for each dataset</abstract>
      <url hash="1a9c2949">2024.naacl-long.415</url>
    </paper>
    <paper id="416">
      <title><fixed-case>UIC</fixed-case>oder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback</title>
      <author><first>Jason</first><last>Wu</last></author>
      <pages>7490-7504</pages>
      <abstract>Many large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely either on expensive human feedback or distilling a proprietary model. In this paper, we explore the use of automated feedback (compilers and multi-modal models) to guide LLMs to generate high-quality UI code. Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset, and producing a new LLM by finetuning the original on the refined dataset.We applied our approach to several open-source LLMs and compared the resulting performance to baseline models with both automated metrics and human preferences.Our results show the resulting models outperform all other downloadable baselines and approach the performance of larger proprietary models.</abstract>
      <url hash="7f505990">2024.naacl-long.416</url>
    </paper>
    <paper id="417">
      <title>Measuring Cross-lingual Transfer in Bytes</title>
      <author><first>Leandro</first><last>De Souza</last><affiliation>Universidade Estadual de Campinas</affiliation></author>
      <author><first>Thales</first><last>Almeida</last></author>
      <author><first>Roberto</first><last>Lotufo</last><affiliation>University of Campinas, Universidade Estadual de Campinas</affiliation></author>
      <author><first>Rodrigo</first><last>Frassetto Nogueira</last></author>
      <pages>7505-7516</pages>
      <abstract>Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual models also have a similar capability, but the mechanisms behind this transfer remain unclear. Some studies have explored factors like language contamination and syntactic similarity. An emerging line of research suggests that the representations learned by language models contain two components: a language-specific and a language-agnostic component. The latter is responsible for transferring a more universal knowledge. However, there is a lack of comprehensive exploration of these properties across diverse target languages. To investigate this hypothesis, we conducted an experiment inspired by the work on the Scaling Laws for Transfer. We measured the amount of data transferred from a source language to a target language and found that models initialized from diverse languages perform similarly to a target language in a cross-lingual setting. This was surprising because the amount of data transferred to 10 diverse target languages, such as Spanish, Korean, and Finnish, was quite similar. We also found evidence that this transfer is not related to language contamination or language proximity, which strengthens the hypothesis that the model also relies on language-agnostic knowledge. Our experiments have opened up new possibilities for measuring how much data represents the language-agnostic representations learned during pretraining.</abstract>
      <url hash="372f0d85">2024.naacl-long.417</url>
    </paper>
    <paper id="418">
      <title><fixed-case>M</fixed-case>isgender<fixed-case>M</fixed-case>ender: A Community-Informed Approach to Interventions for Misgendering</title>
      <author><first>Tamanna</first><last>Hossain</last><affiliation>University of California, Irvine</affiliation></author>
      <author><first>Sunipa</first><last>Dev</last><affiliation>Google</affiliation></author>
      <author><first>Sameer</first><last>Singh</last><affiliation>University of California, Irvine and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>7517-7537</pages>
      <abstract>Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering.Misgendering, the act of incorrectly addressing someone’s gender, inflicts serious harm and is pervasive in everyday technologies, yet there is a notable lack of research to combat it. We are the first to address this lack of research into interventions for misgendering by conducting a survey of gender-diverse individuals in the US to understand perspectives about automated interventions for text-based misgendering. Based on survey insights on the prevalence of misgendering, desired solutions, and associated concerns, we introduce a misgendering interventions task and evaluation dataset, MisgenderMender. We define the task with two sub-tasks: (i) detecting misgendering, followed by (ii) correcting misgendering where misgendering is present, in domains where editing is appropriate. MisgenderMender comprises 3790 instances of social media content and LLM-generations about non-cisgender public figures, annotated for the presence of misgendering, with additional annotations for correcting misgendering in LLM-generated text. Using this dataset, we set initial benchmarks by evaluating existing NLP systems and highlighting challenges for future models to address. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendermender/</abstract>
      <url hash="95e3e36f">2024.naacl-long.418</url>
    </paper>
    <paper id="419">
      <title>Interplay of Machine Translation, Diacritics, and Diacritization</title>
      <author><first>Wei-Rui</first><last>Chen</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Ife</first><last>Adebara</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>7538-7580</pages>
      <abstract>We investigate two research questions: (1) how do machine translation (MT) and diacritization influence the performance of each other in a multi-task learning setting (2) the effect of keeping (vs. removing) diacritics on MT performance. We examine these two questions in both high-resource (HR) and low-resource (LR) settings across 55 different languages (36 African languages and 19 European languages). For (1), results show that diacritization significantly benefits MT in the LR scenario, doubling or even tripling performance for some languages, but harms MT in the HR scenario. We find that MT harms diacritization in LR but benefits significantly in HR for some languages. For (2), MT performance is similar regardless of diacritics being kept or removed. In addition, we propose two classes of metrics to measure the complexity of a diacritical system, finding these metrics to correlate positively with the performance of our diacritization models. Overall, our work provides insights for developing MT and diacritization systems under different data size conditions and may have implications that generalize beyond the 55 languages we investigate.</abstract>
      <url hash="dcd132b1">2024.naacl-long.419</url>
    </paper>
    <paper id="420">
      <title>From Quantity to Quality: Boosting <fixed-case>LLM</fixed-case> Performance with Self-Guided Data Selection for Instruction Tuning</title>
      <author><first>Ming</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Yong</first><last>Zhang</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Zhitao</first><last>Li</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Jiuhai</first><last>Chen</last></author>
      <author><first>Lichang</first><last>Chen</last></author>
      <author><first>Ning</first><last>Cheng</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Jianzong</first><last>Wang</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jing</first><last>Xiao</last><affiliation>Pingan Group</affiliation></author>
      <pages>7581-7614</pages>
      <abstract>In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model’s expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available.</abstract>
      <url hash="a92bdec9">2024.naacl-long.420</url>
    </paper>
    <paper id="421">
      <title>Safer-Instruct: Aligning Language Models with Automated Preference Data</title>
      <author><first>Taiwei</first><last>Shi</last></author>
      <author><first>Kai</first><last>Chen</last></author>
      <author><first>Jieyu</first><last>Zhao</last><affiliation>University of Southern California</affiliation></author>
      <pages>7615-7630</pages>
      <abstract>Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while maintaining a competitive edge in downstream tasks. Importantly, our Safer-Instruct framework is versatile and can be applied to generate preference data across various domains, extending its utility beyond safety preferences. It addresses the challenges in preference data acquisition and advances the development of more capable and responsible AI systems. For dataset and code implementation, see https://github.com/uscnlp-lime/safer-instruct/.</abstract>
      <url hash="2d104468">2024.naacl-long.421</url>
    </paper>
    <paper id="422">
      <title><fixed-case>PELMS</fixed-case>: Pre-training for Effective Low-Shot Multi-Document Summarization</title>
      <author><first>Joseph</first><last>Peper</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Wenzhao</first><last>Qiu</last></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>7631-7653</pages>
      <abstract>We investigate pre-training techniques for abstractive multi-document summarization (MDS), which is much less studied than summarizing single documents. Though recent work has demonstrated the effectiveness of highlighting information salience for pre-training strategy design, they struggle to generate abstractive and reflective summaries, which are critical properties for MDS. To this end, we present **PELMS**, a pre-trained model that uses pre-training objectives based on semantic coherence heuristics and faithfulness constraints together with unlabeled multi-document inputs, to promote the generation of concise, fluent, and faithful summaries. To support the training of PELMS, we compile **MultiPT**, a multi-document pre-training corpus containing over 93 million documents to form more than 3million unlabeled topic-centric document clusters, covering diverse genres such as product reviews, news, and general knowledge. We perform extensive evaluation of PELMS in low-shot settings on a wide range of MDS datasets. Our approach consistently outperforms competitive comparisons with respect to overall informativeness, abstractiveness, coherence, and faithfulness, and with minimal fine-tuning can match performance of language models at a much larger scale (e.g., GPT-4).</abstract>
      <url hash="85749c3c">2024.naacl-long.422</url>
    </paper>
    <paper id="423">
      <title>Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?</title>
      <author><first>Bangzheng</first><last>Li</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ben</first><last>Zhou</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Xingyu</first><last>Fu</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>7654-7667</pages>
      <abstract>Despite the high performances of large language models (LLMs) across numerous benchmarks, recent research has unveiled their suffering from hallucinations and unfaithful reasoning. This work studies a type of hallucination induced by semantic associations. We investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following correct reasoning paths. To quantify this phenomenon, we propose a novel probing method and benchmark called EUREQA. EUREQA is an entity-searching task where a model finds a missing entity based on described multi-hop relations with other entities. These deliberately designed multi-hop relations create deceptive semantic associations, and models must stick to the correct reasoning path instead of incorrect shortcuts to find the correct answer.Experiments show that existing LLMs cannot follow correct reasoning paths and resist the attempt of greedy shortcuts, with GPT-4 only achieving 62% accuracy. Analyses provide further evidence that LLMs rely on semantic biases to solve the task instead of proper reasoning, questioning the validity and generalizability of current LLMs’ high performances.</abstract>
      <url hash="d5af9f6a">2024.naacl-long.423</url>
    </paper>
    <paper id="424">
      <title><fixed-case>I</fixed-case>ndi<fixed-case>S</fixed-case>entiment140: Sentiment Analysis Dataset for <fixed-case>I</fixed-case>ndian Languages with Emphasis on Low-Resource Languages using Machine Translation</title>
      <author><first>Saurabh</first><last>Kumar</last><affiliation>Indian Institute of Technology, Guwahati</affiliation></author>
      <author><first>Ranbir</first><last>Sanasam</last><affiliation>Indian Institute of Technology, Guwahati, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Sukumar</first><last>Nandi</last><affiliation>Indian Institute of Technology, Guwahati</affiliation></author>
      <pages>7668-7677</pages>
      <abstract>Sentiment analysis, a fundamental aspect of Natural Language Processing (NLP), involves the classification of emotions, opinions, and attitudes in text data. In the context of India, with its vast linguistic diversity and low-resource languages, the challenge is to support sentiment analysis in numerous Indian languages. This study explores the use of machine translation to bridge this gap. The investigation examines the feasibility of machine translation for creating sentiment analysis datasets in 22 Indian languages. Google Translate, with its extensive language support, is employed for this purpose in translating the Sentiment140 dataset. The study aims to provide insights into the practicality of using machine translation in the context of India’s linguistic diversity for sentiment analysis datasets. Our findings indicate that a dataset generated using Google Translate has the potential to serve as a foundational framework for tackling the low-resource challenges commonly encountered in sentiment analysis for Indian languages.</abstract>
      <url hash="2be4aac7">2024.naacl-long.424</url>
    </paper>
    <paper id="425">
      <title>Leveraging <fixed-case>LLM</fixed-case>s for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval</title>
      <author><first>Nandan</first><last>Thakur</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Jianmo</first><last>Ni</last><affiliation>Google and Google</affiliation></author>
      <author><first>Gustavo</first><last>Hernandez Abrego</last><affiliation>Google</affiliation></author>
      <author><first>John</first><last>Wieting</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Jimmy</first><last>Lin</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>Daniel</first><last>Cer</last><affiliation>Google</affiliation></author>
      <pages>7678-7703</pages>
      <abstract>There has been limited success for dense retrieval models in multilingual retrieval, due to uneven and scarce training data available across multiple languages. Synthetic training data generation is promising (e.g., InPars or Promptagator), but has been investigated only for English. Therefore, to study model capabilities across both cross-lingual and monolingual retrieval tasks, we develop **SWIM-IR**, a synthetic retrieval training dataset containing 33 (high to very-low resource) languages for fine-tuning multilingual dense retrievers without requiring any human supervision. To construct SWIM-IR, we propose SAP (summarize-then-ask prompting), where the large language model (LLM) generates a textual summary prior to the query generation step. SAP assists the LLM in generating informative queries in the target language. Using SWIM-IR, we explore synthetic fine-tuning of multilingual dense retrieval models and evaluate them robustly on three retrieval benchmarks: XOR-Retrieve (cross-lingual), MIRACL (monolingual) and XTREME-UP (cross-lingual). Our models, called SWIM-X, are competitive with human-supervised dense retrieval models, e.g., mContriever-X, finding that SWIM-IR can cheaply substitute for expensive human-labeled retrieval training data. SWIM-IR dataset and SWIM-X models are available at: https://github.com/google-research-datasets/SWIM-IR.</abstract>
      <url hash="d243e4ef">2024.naacl-long.425</url>
    </paper>
    <paper id="426">
      <title><fixed-case>SCANNER</fixed-case>: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities</title>
      <author><first>Hyunjong</first><last>Ok</last></author>
      <author><first>Taeho</first><last>Kil</last><affiliation>NAVER Cloud</affiliation></author>
      <author><first>Sukmin</first><last>Seo</last><affiliation>NAVER</affiliation></author>
      <author><first>Jaeho</first><last>Lee</last><affiliation>Google and Pohang University of Science and Technology</affiliation></author>
      <pages>7704-7716</pages>
      <abstract>Recent advances in named entity recognition (NER) have pushed the boundary of the task to incorporate visual signals, leading to many variants, including multi-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations.To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for NER), a model capable of effectively handling all three NER variants.SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources.We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities.Furthermore, to tackle the challenges arising from noisy annotations in NER datasets, we introduce a novel self-distillation method, enhancing the robustness and accuracy of our model in processing training data with inherent uncertainties.Our approach demonstrates competitive performance on the NER benchmark and surpasses existing methods on both MNER and GMNER benchmarks.Further analysis shows that the proposed distillation and knowledge utilization methods improve the performance of our model on various benchmarks.</abstract>
      <url hash="684709e1">2024.naacl-long.426</url>
    </paper>
    <paper id="427">
      <title>A Theory Guided Scaffolding Instruction Framework for <fixed-case>LLM</fixed-case>-Enabled Metaphor Reasoning</title>
      <author><first>Yuan</first><last>Tian</last></author>
      <author><first>Nan</first><last>Xu</last></author>
      <author><first>Wenji</first><last>Mao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>7717-7734</pages>
      <abstract>Metaphor detection is a challenging task in figurative language processing, which aims to distinguish between metaphorical and literal expressions in text. Existing methods tackle metaphor detection via training or fine-tuning discriminative models on labeled data. However, these approaches struggle to explain the underlying reasoning process behind the metaphorical/literal judgment. Recently, large language models (LLMs) have shown promise in language reasoning tasks. Although promising, LLM-based methods for metaphor detection and reasoning are still faced with the challenging issue of bringing the explainable concepts for metaphor reasoning and their linguistic manifestation. To fill this gap, we propose a novel Theory guided Scaffolding Instruction (TSI) framework that instructs an LLM to infer the underlying reasoning process of metaphor detection guided by metaphor theories for the first time. Our work is inspired by a pedagogical strategy called scaffolding instruction, which encourages educators to provide questioning and support as scaffolding so as to assist learners in constructing the understanding of pedagogical goals step by step. We first construct a metaphor knowledge graph grounded in metaphor theory which serves as the instructional structure to obtain a series of scaffolding questions, directing the LLM to incrementally generate the reasoning process for metaphor understanding through dialogue interactions. During this theory guided instruction process, we explore the LLM’s mastery boundary and provide the relevant knowledge as scaffolding support when the question is beyond the LLM’s capability. Experimental results verify that our method significantly outperforms both the LLM-based reasoning methods and the SOTA methods in metaphor detection, indicating the facilitation of metaphor and instruction theories in guiding LLM-based reasoning process.</abstract>
      <url hash="4a79aa97">2024.naacl-long.427</url>
    </paper>
    <paper id="428">
      <title>Learning to Compress Prompt in Natural Language Formats</title>
      <author><first>Yu-Neng</first><last>Chuang</last><affiliation>Rice University</affiliation></author>
      <author><first>Tianwei</first><last>Xing</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Chia-Yuan</first><last>Chang</last></author>
      <author><first>Zirui</first><last>Liu</last><affiliation>Rice University</affiliation></author>
      <author><first>Xun</first><last>Chen</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Xia</first><last>Hu</last><affiliation>Rice University</affiliation></author>
      <pages>7735-7746</pages>
      <abstract>Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framework compressing original prompts into NL formatted Capsule Prompt while maintaining prompt utility and transferability. Specifically, to tackle the first challenge, the Nano-Capsulator is optimized by a reward function that interacts with the proposed semantics preserving loss. To address the second question, the Nano-Capsulator is optimized by a reward function featuring length constraints. Experimental results demonstrate that the Capsule Prompt can reduce 81.4% of the original length, decrease inference latency up to 4.5x, and save 80.1% of budget overheads while providing transferability across diverse LLMs and different datasets.</abstract>
      <url hash="340e6b9d">2024.naacl-long.428</url>
    </paper>
    <paper id="429">
      <title>Automatic, Meta and Human Evaluation for Multimodal Summarization with Multimodal Output</title>
      <author><first>Haojie</first><last>Zhuang</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Wei Emma</first><last>Zhang</last><affiliation>The University of Adelaide</affiliation></author>
      <author><first>Leon</first><last>Xie</last></author>
      <author><first>Weitong</first><last>Chen</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Quan</first><last>Sheng</last><affiliation>Macquarie University</affiliation></author>
      <pages>7747-7769</pages>
      <abstract>Multimodal summarization with multimodal output (MSMO) has attracted increasing research interests recently as multimodal summary could provide more comprehensive information compared to text-only summary, effectively improving the user experience and satisfaction. As one of the most fundamental components for the development of MSMO, evaluation is an emerging yet underexplored research topic. In this paper, we fill this gap and propose a research framework that studies three research questions of MSMO evaluation: (1) Automatic Evaluation: We propose a novel metric mLLM-EVAL, which utilizes multimodal Large Language Model for MSMO EVALuation. (2) Meta-Evaluation: We create a meta-evaluation benchmark dataset by collecting human-annotated scores for multimodal summaries. With our benchmark, we conduct meta-evaluation analysis to assess the quality of different evaluation metrics and show the effectiveness of our proposed mLLM-EVAL. (3) Human Evaluation: To provide more objective and unbiased human annotations for meta-evaluation, we hypothesize and verify three types of cognitive biases in human evaluation. We also incorporate our findings into the human annotation process in the meta-evaluation benchmark. Overall, our research framework provides an evaluation metric, a meta-evaluation benchmark dataset annotated by humans and an analysis of cognitive biases in human evaluation, which we believe would serve as a valuable and comprehensive resource for the MSMO research community.</abstract>
      <url hash="c7236d4e">2024.naacl-long.429</url>
    </paper>
    <paper id="430">
      <title>Naive <fixed-case>B</fixed-case>ayes-based Context Extension for Large Language Models</title>
      <author><first>Jianlin</first><last>Su</last></author>
      <author><first>Murtadha</first><last>Ahmed</last><affiliation>Zhuiyi AI Lab</affiliation></author>
      <author><first>Bo</first><last>Wen</last></author>
      <author><first>Luo</first><last>Ao</last><affiliation>Zhuiyi Technology Co., Ltd.</affiliation></author>
      <author><first>Mingren</first><last>Zhu</last><affiliation>Shenzhen Zhuiyi Technology Co., Ltd</affiliation></author>
      <author><first>Yunfeng</first><last>Liu</last></author>
      <pages>7770-7786</pages>
      <abstract>Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM’s maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes’ theorem to generate the test task. Our experimental results demonstrate that NBCE substantially enhances performance, particularly as the number of demonstration examples increases, consistently outperforming alternative methods. The NBCE code will be made publicly accessible. The code NBCE is available at: https://github.com/amurtadha/NBCE-master</abstract>
      <url hash="dcc7197c">2024.naacl-long.430</url>
    </paper>
    <paper id="431">
      <title>Leitner-Guided Memory Replay for Cross-lingual Continual Learning</title>
      <author><first>Meryem</first><last>M’hamdi</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>7787-7800</pages>
      <abstract>Cross-lingual continual learning aims to continuously fine-tune a downstream model on emerging data from new languages. One major challenge in cross-lingual continual learning is catastrophic forgetting: a stability-plasticity dilemma, where performance on previously seen languages decreases as the model learns to transfer to new languages. Experience replay, which revisits data from a fixed-size memory of old languages while training on new ones, is among the most successful approaches for solving this dilemma. Faced with the challenge of dynamically storing the memory with high-quality examples while complying with its fixed size limitations, we consider Leitner queuing, a human-inspired spaced-repetition technique, to determine what should be replayed at each phase of learning. Via a controlled set of quantitative and qualitative analyses across different memory strategies, we show that, just like humans, carefully picking informative examples to be prioritized in cross-lingual memory replay helps tame the stability-plasticity dilemma. Compared to vanilla and strong memory replay baselines, our Leitner-guided approach significantly and consistently decreases forgetting while maintaining accuracy across natural language understanding tasks, language orders, and languages.</abstract>
      <url hash="26c28e0a">2024.naacl-long.431</url>
    </paper>
    <paper id="432">
      <title>Multilingual Nonce Dependency Treebanks: Understanding how Language Models Represent and Process Syntactic Structure</title>
      <author><first>David</first><last>Arps</last><affiliation>HHU Düsseldorf</affiliation></author>
      <author><first>Laura</first><last>Kallmeyer</last><affiliation>Heinrich Heine University Düsseldorf, Germany</affiliation></author>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Hassan</first><last>Sajjad</last><affiliation>Dalhousie University</affiliation></author>
      <pages>7801-7823</pages>
      <abstract>We introduce SPUD (Semantically Perturbed Universal Dependencies), a framework for creating nonce treebanks for the multilingual Universal Dependencies (UD) corpora. SPUD data satisfies syntactic argument structure, provides syntactic annotations, and ensures grammaticality via language-specific rules. We create nonce data in Arabic, English, French, German, and Russian, and demonstrate two use cases of SPUD treebanks. First, we investigate the effect of nonce data on word co-occurrence statistics, as measured by perplexity scores of autoregressive (ALM) and masked language models (MLM). We find that ALM scores are significantly more affected by nonce data than MLM scores. Second, we show how nonce data affects the performance of syntactic dependency probes. We replicate the findings of Müller-Eberstein et al. (2022) on nonce test data and show that the performance declines on both MLMs and ALMs wrt. original test data. However, a majority of the performance is kept, suggesting that the probe indeed learns syntax independently from semantics.</abstract>
      <url hash="7bdf9272">2024.naacl-long.432</url>
    </paper>
    <paper id="433">
      <title>Actively Learn from <fixed-case>LLM</fixed-case>s with Uncertainty Propagation for Generalized Category Discovery</title>
      <author><first>Jinggui</first><last>Liang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Bobo</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Singapore Management University</affiliation></author>
      <pages>7824-7837</pages>
      <abstract>Generalized category discovery faces a key issue: the lack of supervision for new and unseen data categories. Traditional methods typically combine supervised pretraining with self-supervised learning to create models, and then employ clustering for category identification. However, these approaches tend to become overly tailored to known categories, failing to fully resolve the core issue. Hence, we propose to integrate the feedback from LLMs into an active learning paradigm. Specifically, our method innovatively employs uncertainty propagation to select data samples from high-uncertainty regions, which are then labeled using LLMs through a comparison-based prompting scheme. This not only eases the labeling task but also enhances accuracy in identifying new categories. Additionally, a soft feedback propagation mechanism is introduced to minimize the spread of inaccurate feedback. Experiments on various datasets demonstrate our framework’s efficacy and generalizability, significantly improving baseline models at a nominal average cost.</abstract>
      <url hash="df43c3eb">2024.naacl-long.433</url>
    </paper>
    <paper id="434">
      <title>Explaining Text Similarity in Transformer Models</title>
      <author><first>Alexandros</first><last>Vasileiou</last></author>
      <author><first>Oliver</first><last>Eberle</last><affiliation>Technische Universität Berlin</affiliation></author>
      <pages>7838-7852</pages>
      <abstract>As Transformers have become state-of-the-art models for natural language processing (NLP) tasks, the need to understand and explain their predictions is increasingly apparent. Especially in unsupervised applications, such as information retrieval tasks, similarity models built on top of foundation model representations have been widely applied. However, their inner prediction mechanisms have mostly remained opaque. Recent advances in explainable AI have made it possible to mitigate these limitations by leveraging improved explanations for Transformers through layer-wise relevance propagation (LRP). Using BiLRP, an extension developed for computing second-order explanations in bilinear similarity models, we investigate which feature interactions drive similarity in NLP models. We validate the resulting explanations and demonstrate their utility in three corpus-level use cases, analyzing grammatical interactions, multilingual semantics, and biomedical text retrieval. Our findings contribute to a deeper understanding of different semantic similarity tasks and models, highlighting how novel explainable AI methods enable in-depth analyses and corpus-level insights.</abstract>
      <url hash="3a0d11af">2024.naacl-long.434</url>
    </paper>
    <paper id="435">
      <title>Large Language Models can Contrastively Refine their Generation for Better Sentence Representation Learning</title>
      <author><first>Huiming</first><last>Wang</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Zhaodonghui</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>De Wen</first><last>Soh</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <pages>7853-7870</pages>
      <abstract>Recently, large language models (LLMs) have emerged as a groundbreaking technology and their unparalleled text generation capabilities have sparked interest in their application to the fundamental sentence representation learning task. Existing methods have explored utilizing LLMs as data annotators to generate synthesized data for training contrastive learning based sentence embedding models such as SimCSE. However, since contrastive learning models are sensitive to the quality of sentence pairs, the effectiveness of these methods is largely influenced by the content generated from LLMs, highlighting the need for more refined generation in the context of sentence representation learning. Building upon this premise, we propose MultiCSR, a multi-level contrastive sentence representation learning framework that decomposes the process of prompting LLMs to generate a corpus for training base sentence embedding models into three stages (i.e., sentence generation, sentence pair construction, in-batch training) and refines the generated content at these three distinct stages, ensuring only high-quality sentence pairs are utilized to train a base contrastive learning model. Our extensive experiments reveal that MultiCSR enables a less advanced LLM to surpass the performance of ChatGPT, while applying it to ChatGPT achieves better state-of-the-art results. Comprehensive analyses further underscore the potential of our framework in various application scenarios and achieving better sentence representation learning with LLMs.</abstract>
      <url hash="bfad78de">2024.naacl-long.435</url>
    </paper>
    <paper id="436">
      <title><fixed-case>HIL</fixed-case>: Hybrid Isotropy Learning for Zero-shot Performance in Dense retrieval</title>
      <author><first>JaeYoung</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Dohyeon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>7871-7882</pages>
      <abstract>Advancements in dense retrieval models have brought ColBERT to prominence in Information Retrieval (IR) with its advanced interaction techniques.However, ColBERT is reported to frequently underperform in zero-shot scenarios, where traditional techniques such as BM25 still exceed it.Addressing this, we propose to balance representation isotropy and anisotropy for zero-shot model performance, based on our observations that isotropy can enhance cosine similarity computations and anisotropy may aid in generalizing to unseen data.Striking a balance between these isotropic and anisotropic qualities stands as a critical objective to refine model efficacy.Based on this, we present ours, a Hybrid Isotropy Learning (HIL) architecture that integrates isotropic and anisotropic representations.Our experiments with the BEIR benchmark show that our model significantly outperforms the baseline ColBERT model, highlighting the importance of harmonized isotropy in improving zero-shot retrieval performance.</abstract>
      <url hash="ec2723cf">2024.naacl-long.436</url>
    </paper>
    <paper id="437">
      <title><fixed-case>S</fixed-case>uper<fixed-case>GLEB</fixed-case>er: <fixed-case>G</fixed-case>erman Language Understanding Evaluation Benchmark</title>
      <author><first>Jan</first><last>Pfister</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Andreas</first><last>Hotho</last><affiliation>Bayerische Julius-Maximilians-Universität Würzburg</affiliation></author>
      <pages>7883-7902</pages>
      <abstract>We assemble a broad Natural Language Understanding benchmark suite for the German language and consequently evaluate a wide array of existing German-capable models in order to create a better understanding of the current state of German LLMs. Our benchmark consists of 29 different tasks ranging over different types such as document classification, sequence tagging, sentence similarity, and question answering, on which we evaluate 10 different German-pretrained models, thereby charting the landscape of German LLMs. In our comprehensive evaluation we find that encoder models are a good choice for most tasks, but also that the largest encoder model does not necessarily perform best for all tasks. We make our benchmark suite and a leaderboard publically available at https://supergleber.professor-x.de and encourage the community to contribute new tasks and evaluate more models on it (https://github.com/LSX-UniWue/SuperGLEBer).</abstract>
      <url hash="540b4d23">2024.naacl-long.437</url>
    </paper>
    <paper id="438">
      <title>“You are an expert annotator”: Automatic Best–Worst-Scaling Annotations for Emotion Intensity Modeling</title>
      <author><first>Christopher</first><last>Bagdon</last></author>
      <author><first>Prathamesh</first><last>Karmalkar</last></author>
      <author><first>Harsha</first><last>Gurulingappa</last></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <pages>7903-7915</pages>
      <abstract>Labeling corpora constitutes a bottleneck to create models for new tasks or domains. Large language models mitigate the issue with automatic corpus labeling methods, particularly for categorical annotations. Some NLP tasks such as emotion intensity prediction, however, require text regression, but there is no work on automating annotations for continuous label assignments. Regression is considered more challenging than classification: The fact that humans perform worse when tasked to choose values from a rating scale lead to comparative annotation methods, including best–worst scaling. This raises the question if large language model-based annotation methods show similar patterns, namely that they perform worse on rating scale annotation tasks than on comparative annotation tasks. To study this, we automate emotion intensity predictions and compare direct rating scale predictions, pairwise comparisons and best–worst scaling. We find that the latter shows the highest reliability. A transformer regressor fine-tuned on these data performs nearly on par with a model trained on the original manual annotations.</abstract>
      <url hash="2164274d">2024.naacl-long.438</url>
    </paper>
    <paper id="439">
      <title>What Matters in Training a <fixed-case>GPT</fixed-case>4-Style Language Model with Multimodal Inputs?</title>
      <author><first>Yan</first><last>Zeng</last><affiliation>ByteDance</affiliation></author>
      <author><first>Hanbo</first><last>Zhang</last><affiliation>ByteDance Ltd</affiliation></author>
      <author><first>Jiani</first><last>Zheng</last></author>
      <author><first>Jiangnan</first><last>Xia</last></author>
      <author><first>Guoqiang</first><last>Wei</last><affiliation>ByteDance</affiliation></author>
      <author><first>Yang</first><last>Wei</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Yuchen</first><last>Zhang</last><affiliation>ByteDance Research</affiliation></author>
      <author><first>Tao</first><last>Kong</last><affiliation>Bytedance</affiliation></author>
      <author><first>Ruihua</first><last>Song</last><affiliation>Renmin University of China</affiliation></author>
      <pages>7916-7943</pages>
      <abstract>Recent advancements in GPT-4V have displayed remarkable multi-modal capabilities in processing image inputs and following open-ended instructions. Despite these advancements, there is considerable scope for enhancing open-source multi-modal LLMs, especially in terms of multi-modal understanding accuracy and instruction-following proficiency. In this paper, we conduct a comprehensive study on training GPT4-style models. We introduce Lynx a multi-modal LLM developed through a series of controlled experiments comparing various model variants. This process allowed us to identify and implement an optimal training strategy tailored for multi-modal LLMs. In addition to our model development, we propose a plug-and-play technique designed to augment the instruction-following capabilities of multi-modal LLMs. We have validated the performance of Lynx on multiple benchmarks. Results demonstrate that Lynx not only achieves strong image understanding accuracy but also excels in instruction-following tasks, paving the path for ongoing enhancements in multi-modal LLMs.</abstract>
      <url hash="ad93c7d4">2024.naacl-long.439</url>
    </paper>
    <paper id="440">
      <title>Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable <fixed-case>NLG</fixed-case> Evaluation</title>
      <author><first>Jie</first><last>Ruan</last></author>
      <author><first>WangWenqing</first><last>WangWenqing</last></author>
      <author><first>Xiaojun</first><last>Wan</last><affiliation>Peking University</affiliation></author>
      <pages>7944-7968</pages>
      <abstract>Human evaluation serves as the gold standard for assessing the quality of Natural Language Generation (NLG) systems. Nevertheless, the evaluation guideline, as a pivotal element ensuring reliable and reproducible human assessment, has received limited attention. Our investigation revealed that only 29.84% of recent papers involving human evaluation at top conferences release their evaluation guidelines, with vulnerabilities identified in 77.09% of these guidelines. Unreliable evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding the advancement of NLG in the right direction. To address these challenges, we take an initial step towards reliable evaluation guidelines and propose the first human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via Large Language Models (LLMs). We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines. Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation. The annotated human evaluation guideline dataset and code for the vulnerability detection method are publicly available online.</abstract>
      <url hash="9a0b012e">2024.naacl-long.440</url>
    </paper>
    <paper id="441">
      <title><fixed-case>MOSAIC</fixed-case>o: a Multilingual Open-text Semantically Annotated Interlinked Corpus</title>
      <author><first>Simone</first><last>Conia</last><affiliation>Sapienza University of Rome</affiliation></author>
      <author><first>Edoardo</first><last>Barba</last></author>
      <author><first>Abelardo Carlos</first><last>Martinez Lorenzo</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Pere-Lluís</first><last>Huguet Cabot</last></author>
      <author><first>Riccardo</first><last>Orlando</last></author>
      <author><first>Luigi</first><last>Procopio</last></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>7969-7983</pages>
      <abstract>Several Natural Language Understanding (NLU) tasks focus on linking text to explicit knowledge, including Word Sense Disambiguation, Semantic Role Labeling, Semantic Parsing, and Relation Extraction. In addition to the importance of connecting raw text with explicit knowledge bases, the integration of such carefully curated knowledge into deep learning models has been shown to be beneficial across a diverse range of applications, including Language Modeling and Machine Translation. Nevertheless, the scarcity of semantically-annotated corpora across various tasks and languages limits the potential advantages significantly. To address this issue, we put forward MOSAICo, the first endeavor aimed at equipping the research community with the key ingredients to model explicit semantic knowledge at a large scale, providing hundreds of millions of silver yet high-quality annotations for four NLU tasks across five languages. We describe the creation process of MOSAICo, demonstrate its quality and variety, and analyze the interplay between different types of semantic information. MOSAICo, available at https://github.com/SapienzaNLP/mosaico, aims to drop the requirement of closed, licensed datasets and represents a step towards a level playing field across languages and tasks in NLU.</abstract>
      <url hash="18203d54">2024.naacl-long.441</url>
    </paper>
    <paper id="442">
      <title><fixed-case>S</fixed-case>em<fixed-case>R</fixed-case>o<fixed-case>D</fixed-case>e: Macro Adversarial Training to Learn Representations that are Robust to Word-Level Attacks</title>
      <author><first>Brian</first><last>Formento</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Wenjie</first><last>Feng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Chuan-Sheng</first><last>Foo</last><affiliation>Centre for Frontier AI Research, A*STAR and Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>7984-8007</pages>
      <abstract>Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it would improve attack robustness. We align the domains by incorporating a new distance-based objective. With this, our model is able to learn more generalized representations by aligning the model’s high-level output features and therefore better handling unseen adversarial samples. This method can be generalized across word embeddings, even when they share minimal overlap at both vocabulary and word-substitution levels. To evaluate the effectiveness of our approach, we conduct experiments on BERT and RoBERTa models on three datasets. The results demonstrate promising state-of-the-art robustness.</abstract>
      <url hash="98b955c9">2024.naacl-long.442</url>
    </paper>
    <paper id="443">
      <title><fixed-case>BUST</fixed-case>: Benchmark for the evaluation of detectors of <fixed-case>LLM</fixed-case>-Generated Text</title>
      <author><first>Joseph</first><last>Cornelius</last><affiliation>SUPSI - University of Applied Sciences Southern Switzerland</affiliation></author>
      <author><first>Oscar</first><last>Lithgow-Serrano</last><affiliation>The Swiss AI Lab (IDSIA)</affiliation></author>
      <author><first>Sandra</first><last>Mitrovic</last><affiliation>IDSIA-USI/SUPSI</affiliation></author>
      <author><first>Ljiljana</first><last>Dolamic</last><affiliation>armasuisse</affiliation></author>
      <author><first>Fabio</first><last>Rinaldi</last><affiliation>IDSIA</affiliation></author>
      <pages>8008-8036</pages>
      <abstract>We introduce BUST, a comprehensive benchmark designed to evaluate detectors of texts generated by instruction-tuned large language models (LLMs). Unlike previous benchmarks, our focus lies on evaluating the performance of detector systems, acknowledging the inevitable influence of the underlying tasks and different LLM generators. Our benchmark dataset consists of 25K texts from humans and 7 LLMs responding to instructions across 10 tasks from 3 diverse sources. Using the benchmark, we evaluated 5 detectors and found substantial performance variance across tasks. A meta-analysis of the dataset characteristics was conducted to guide the examination of detector performance. The dataset was analyzed using diverse metrics assessing linguistic features like fluency and coherence, readability scores, and writer attitudes, such as emotions, convincingness, and persuasiveness. Features impacting detector performance were investigated with surrogate models, revealing emotional content in texts enhanced some detectors, yet the most effective detector demonstrated consistent performance, irrespective of writer’s attitudes and text styles. Our approach focused on investigating relationships between the detectors’ performance and two key factors: text characteristics and LLM generators. We believe BUST will provide valuable insights into selecting detectors tailored to specific text styles and tasks and facilitate a more practical and in-depth investigation of detection systems for LLM-generated text.</abstract>
      <url hash="839644aa">2024.naacl-long.443</url>
    </paper>
    <paper id="444">
      <title>Improving In-context Learning of Multilingual Generative Language Models with Cross-lingual Alignment</title>
      <author><first>Chong</first><last>Li</last><affiliation>Institute of automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>8037-8055</pages>
      <abstract>Multilingual generative models obtain remarkable cross-lingual in-context learning capabilities through pre-training on large-scale corpora. However, they still exhibit a performance bias toward high-resource languages and learn isolated distributions of multilingual sentence representations, which may hinder knowledge transfer across languages. To bridge this gap, we propose a simple yet effective cross-lingual alignment framework exploiting pairs of translation sentences. It aligns the internal sentence representations across different languages via multilingual contrastive learning and aligns outputs by following cross-lingual instructions in the target language. Experimental results show that even with less than 0.1<tex-math>{\textperthousand}</tex-math> of pre-training tokens, our alignment framework significantly boosts the cross-lingual abilities of generative language models and mitigates the performance gap. Further analyses reveal that it results in a better internal multilingual representation distribution of multilingual models.</abstract>
      <url hash="e19c2d61">2024.naacl-long.444</url>
    </paper>
    <paper id="445">
      <title><fixed-case>M</fixed-case>a<fixed-case>CSC</fixed-case>: Towards Multimodal-augmented Pre-trained Language Models via Conceptual Prototypes and Self-balancing Calibration</title>
      <author><first>Xianwei</first><last>Zhuang</last></author>
      <author><first>Zhichang</first><last>Wang</last></author>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Yuxin</first><last>Xie</last></author>
      <author><first>Liming</first><last>Liang</last></author>
      <author><first>Yuexian</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <pages>8056-8069</pages>
      <abstract>Pre-trained language models (PLMs) that rely solely on textual data may exhibit limitations in multimodal semantics comprehension. Existing solutions attempt to alleviate this issue by incorporating explicit image retrieval or generation techniques.However, these methods: (1) focus exclusively on the static image modality; (2) inevitably encounter modality gaps and noise; (3) indiscriminately treat all modalities.In this paper, we propose a novel multimodal-augmented framework termed MaCSC, which can infuse multimodal semantics into PLMs and facilitate a self-balancing calibration of information allocation.Specifically, MaCSC obtains modal-specific conceptual prototypes from contrastive pre-training models (e.g., CLIP),and aggregates the intra- and inter-modal semantics of the conceptual prototype to enhance PLMs.In addition, we utilize a novel self-balancing contrastive loss to achieve multi-scale self-balancing calibration of multimodal information during fine-tuning PLMs.Experimental results show that MaCSC consistently improves the performance of PLMs across various architectures and scales, and outperforms competitive baselines on multiple NLP tasks.</abstract>
      <url hash="ac61df30">2024.naacl-long.445</url>
    </paper>
    <paper id="446">
      <title>Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?</title>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Division of Information Science, Nara Institute of Science and Technology</affiliation></author>
      <author><first>Katsuhiko</first><last>Hayashi</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>8070-8085</pages>
      <abstract>Knowledge graphs (KGs) consist of links that describe relationships between entities. Due to the difficulty of manually enumerating all relationships between entities, automatically completing them is essential for KGs. Knowledge Graph Completion (KGC) is a task that infers unseen relationships between entities in a KG. Traditional embedding-based KGC methods (e.g. RESCAL, TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc.) infer missing links using only the knowledge from training data. In contrast, the recent Pre-trained Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training, which means it can estimate missing links between entities by reusing memorized knowledge from pre-training without inference. This part is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications. To address this issue, we analyze whether PLM-based KGC methods make inferences or merely access memorized knowledge. For this purpose, we propose a method for constructing synthetic datasets specified in this analysis and conclude that PLMs acquire the inference abilities required for KGC through pre-training, even though the performance improvements mostly come from textual information of entities and relations.</abstract>
      <url hash="d1277408">2024.naacl-long.446</url>
    </paper>
    <paper id="447">
      <title>Discovering Lobby-Parliamentarian Alignments through <fixed-case>NLP</fixed-case></title>
      <author><first>Aswin</first><last>Suresh</last><affiliation>Independent Consultant</affiliation></author>
      <author><first>Lazar</first><last>Radojević</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Francesco</first><last>Salvi</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Antoine</first><last>Magron</last></author>
      <author><first>Victor</first><last>Kristof</last></author>
      <author><first>Matthias</first><last>Grossglauser</last><affiliation>EPFL</affiliation></author>
      <pages>8086-8099</pages>
      <abstract>We discover alignments of views between interest groups (lobbies) and members of the European Parliament (MEPs) by automatically analyzing their texts. Specifically, we do so by collecting novel datasets of lobbies’ position papers and MEPs’ speeches, and comparing these texts on the basis of semantic similarity and entailment. In the absence of ground-truth, we perform an indirect validation by comparing the discovered alignments with a dataset, which we curate, of retweet links between MEPs and lobbies, and with the publicly disclosed meetings of MEPs. Our best method performs significantly better than several baselines. Moreover, an aggregate analysis of the discovered alignments, between groups of related lobbies and political groups of MEPs, correspond to the expectations from the ideology of the groups (e.g., groups on the political left are more aligned with humanitarian and environmental organisations). We believe that this work is a step towards enhancing the transparency of the intricate decision-making processes within democratic institutions.</abstract>
      <url hash="045464fd">2024.naacl-long.447</url>
    </paper>
    <paper id="448">
      <title><fixed-case>I</fixed-case>ter<fixed-case>CQR</fixed-case>: Iterative Conversational Query Reformulation with Retrieval Guidance</title>
      <author><first>Yunah</first><last>Jang</last></author>
      <author><first>Kang-il</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Hyunkyung</first><last>Bae</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Hwanhee</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Kyomin</first><last>Jung</last></author>
      <pages>8100-8117</pages>
      <abstract>Conversational search aims to retrieve passages containing essential information to answer queries in a multi-turn conversation. In conversational search, reformulating context-dependent conversational queries into stand-alone forms is imperative to effectively utilize off-the-shelf retrievers. Previous methodologies for conversational query reformulation frequently depend on human-annotated rewrites.However, these manually crafted queries often result in sub-optimal retrieval performance and require high collection costs.To address these challenges, we propose **Iter**ative **C**onversational **Q**uery **R**eformulation (**IterCQR**), a methodology that conducts query reformulation without relying on human rewrites. IterCQR iteratively trains the conversational query reformulation (CQR) model by directly leveraging information retrieval (IR) signals as a reward.Our IterCQR training guides the CQR model such that generated queries contain necessary information from the previous dialogue context.Our proposed method shows state-of-the-art performance on two widely-used datasets, demonstrating its effectiveness on both sparse and dense retrievers. Moreover, IterCQR exhibits superior performance in challenging settings such as generalization on unseen datasets and low-resource scenarios.</abstract>
      <url hash="36047308">2024.naacl-long.448</url>
    </paper>
    <paper id="449">
      <title><fixed-case>A</fixed-case>ce<fixed-case>GPT</fixed-case>, Localizing Large Language Models in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Huang</first><last>Huang</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Fei</first><last>Yu</last></author>
      <author><first>Jianqing</first><last>Zhu</last></author>
      <author><first>Xuening</first><last>Sun</last></author>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Song</first><last>Dingjie</last></author>
      <author><first>Zhihong</first><last>Chen</last><affiliation>Stanford University and THE CHINESE UNIVERSITY OF HONG KONG, SHENZHEN</affiliation></author>
      <author><first>Mosen</first><last>Alharthi</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Bang</first><last>An</last></author>
      <author><first>Juncai</first><last>He</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Ziche</first><last>Liu</last></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Jianquan</first><last>Li</last></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Lian</first><last>Zhang</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Ruoyu</first><last>Sun</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Xiang</first><last>Wan</last><affiliation>Shenzhen Research Institute of Big Data</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <author><first>Jinchao</first><last>Xu</last><affiliation>King Abdullah University of Science and Technology and Pennsylvania State University</affiliation></author>
      <pages>8118-8142</pages>
      <abstract>This paper is devoted to the development of a localized Large Language Model (LLM) specifically for Arabic, a language imbued with unique cultural characteristics inadequately addressed by current mainstream models. Significant concerns emerge when addressing cultural sensitivity and local values. To address this, the paper proposes a comprehensive solution that includes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT) utilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside Reinforcement Learning with AI Feedback (RLAIF) employing a reward model attuned to local culture and values. The goal is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of accommodating the diverse, application-specific needs of Arabic-speaking communities. Comprehensive evaluations reveal that the resulting model, dubbed ‘AceGPT’, sets the state-of-the-art standard for open Arabic LLMs across various benchmarks. Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.</abstract>
      <url hash="d8c13502">2024.naacl-long.449</url>
    </paper>
    <paper id="450">
      <title>Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model</title>
      <author><first>Zhiwei</first><last>He</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xing</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wenxiang</first><last>Jiao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>8143-8159</pages>
      <abstract>Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the reward scores of them. Experimental results show that the proposed QE-based feedback training achieves consistent and significant improvements across various settings, further verified through human preference studies. Our subsequent analysis demonstrates the high data efficiency of the proposed QE-based feedback training: it outperforms systems using larger parallel corpora by a small amount of monolingual data. Our code is available at: https://github.com/zwhe99/FeedbackMT</abstract>
      <url hash="eb25c964">2024.naacl-long.450</url>
    </paper>
    <paper id="451">
      <title>Depression Detection in Clinical Interviews with <fixed-case>LLM</fixed-case>-Empowered Structural Element Graph</title>
      <author><first>Zhuang</first><last>Chen</last></author>
      <author><first>Jiawen</first><last>Deng</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Jinfeng</first><last>Zhou</last></author>
      <author><first>Jincenzi</first><last>Wu</last></author>
      <author><first>Tieyun</first><last>Qian</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>8160-8173</pages>
      <abstract>Depression is a widespread mental health disorder affecting millions globally. Clinical interviews are the gold standard for assessing depression, but they heavily rely on scarce professional clinicians, highlighting the need for automated detection systems. However, existing methods only capture part of the relevant elements in clinical interviews, unable to incorporate all depressive cues. Moreover, the scarcity of participant data, due to privacy concerns and collection challenges, intrinsically constrains interview modeling. To address these limitations, in this paper, we propose a structural element graph (SEGA), which transforms the clinical interview into an expertise-inspired directed acyclic graph for comprehensive modeling. Additionally, we further empower SEGA by devising novel principle-guided data augmentation with large language models (LLMs) to supplement high-quality synthetic data and enable graph contrastive learning. Extensive evaluations on two real-world clinical datasets, in both English and Chinese, show that SEGA significantly outperforms baseline methods and powerful LLMs like GPT-3.5 and GPT-4.</abstract>
      <url hash="7c8fbd7c">2024.naacl-long.451</url>
    </paper>
    <paper id="452">
      <title><fixed-case>SQATIN</fixed-case>: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue <fixed-case>NLU</fixed-case></title>
      <author><first>Evgeniia</first><last>Razumovskaia</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Anna</first><last>Korhonen</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Ivan</first><last>Vulić</last><affiliation>University of Cambridge and PolyAI Limited</affiliation></author>
      <pages>8174-8190</pages>
      <abstract>Task-oriented dialogue (TOD) systems help users execute well-defined tasks across a variety of domains (e.g., <i>flight booking</i> or <i>food ordering</i>), with their Natural Language Understanding (NLU) components being dedicated to the analysis of user utterances, predicting users’ intents (<i>Intent Detection</i>, ID) and extracting values for informational slots (<i>Value Extraction</i>, VE). In most domains, labelled NLU data is scarce, making sample-efficient learning – enabled with effective transfer paradigms – paramount. In this work, we introduce SQATIN, a new framework for dialog NLU based on (i) instruction tuning and (ii) question-answering-based formulation of ID and VE tasks. According to the evaluation on established NLU benchmarks, SQATIN sets the new state of the art in dialogue NLU, substantially surpassing the performance of current models based on standard fine-tuning objectives in both in-domain training and cross-domain transfer, and it also surpasses off-the-shelf large language models for the same task, both in terms of performance and inference efficiency. Furthermore, SQATIN yields particularly large performance gains in cross-domain transfer, owing to the fact that our QA-based instruction tuning leverages similarities between natural language descriptions of classes (i.e., slots and intents) across domains.</abstract>
      <url hash="e05bc60e">2024.naacl-long.452</url>
    </paper>
    <paper id="453">
      <title>Enhancing Argument Summarization: Prioritizing Exhaustiveness in Key Point Generation and Introducing an Automatic Coverage Evaluation Metric</title>
      <author><first>Mohammad</first><last>Khosravani</last></author>
      <author><first>Chenyang</first><last>Huang</last></author>
      <author><first>Amine</first><last>Trabelsi</last><affiliation>Université de Sherbrooke</affiliation></author>
      <pages>8191-8203</pages>
      <abstract>The proliferation of social media platforms has given rise to the amount of online debates and arguments. Consequently, the need for automatic summarization methods for such debates is imperative, however this area of summarization is rather understudied. The Key Point Analysis (KPA) task formulates argument summarization as representing the summary of a large collection of arguments in the form of concise sentences in bullet-style format, called key points. A sub-task of KPA, called Key Point Generation (KPG), focuses on generating these key points given the arguments. This paper introduces a novel extractive approach for key point generation, that outperforms previous state-of-the-art methods for the task. Our method utilizes an extractive clustering based approach that offers concise, high quality generated key points with higher coverage of reference summaries, and less redundant outputs. In addition, we show that the existing evaluation metrics for summarization such as ROUGE are incapable of differentiating between generated key points of different qualities. To this end, we propose a new evaluation metric for assessing the generated key points by their coverage. Our code can be accessed online.</abstract>
      <url hash="c3f523d5">2024.naacl-long.453</url>
    </paper>
    <paper id="454">
      <title><fixed-case>ARM</fixed-case>: Alignment with Residual Energy-Based Model</title>
      <author><first>Bo</first><last>Pang</last><affiliation>SalesForce.com and University of California, Los Angeles</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Yingbo</first><last>Zhou</last><affiliation>Salesforce Research</affiliation></author>
      <pages>8204-8215</pages>
      <abstract>While large language models (LLMs) trained with large-scale unsupervised learning acquire a wide variety of world knowledge and skills, its behavior does not necessarily align with human preferences. RLHF methods achieve successes in aligning LLM responses with human preferences and improving the controllability of LLM behavior with human instruction. However, RLHF methods are considerably complicated to implement, computationally expensive to train, and notoriously tricky to tune. In this work, we propose Alignment with Residual Energy-Based Model (ARM), as a simple and flexible alternative to RLHF methods. Our method is driven by an observation that we can learn an aligned policy by minimizing a forward Kullback–Leibler (KL) divergence from a target policy (in the form of a residual energy-based model) to a parameteric policy (LLM), instead of a reverse KL as in RLHF methods. With samples from the energy-based target policy, we can leverage the power of DPO (or other offline methods) to learn an aligned policy efficiently. ARM is simple to implement and applicable in various data settings. Our extensive experiments demonstrate its strong performance across multiple datasets, compared to strong baselines like PPO, DPO.</abstract>
      <url hash="013df9d0">2024.naacl-long.454</url>
    </paper>
    <paper id="455">
      <title><fixed-case>H</fixed-case>uman<fixed-case>R</fixed-case>ank<fixed-case>E</fixed-case>val: Automatic Evaluation of <fixed-case>LM</fixed-case>s as Conversational Assistants</title>
      <author><first>Milan</first><last>Gritta</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ignacio</first><last>Iacobacci</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <pages>8216-8228</pages>
      <abstract>Language models (LMs) as conversational assistants recently became popular tools that help people accomplish a variety of tasks. These typically result from adapting LMs pretrained on general domain text sequences through further instruction-tuning and possibly preference optimisation methods. The evaluation of such LMs would ideally be performed using human judgement, however, this is not scalable. On the other hand, automatic evaluation featuring auxiliary LMs as judges and/or knowledge-based tasks is scalable but struggles with assessing conversational ability and adherence to instructions. To help accelerate the development of LMs as conversational assistants, we propose a novel automatic evaluation task: HumanRankEval (HRE). It consists of a large-scale, diverse and high-quality set of questions, each with several answers authored and scored by humans. To perform evaluation, HRE ranks these answers based on their log-likelihood under the LM’s distribution, and subsequently calculates their correlation with the corresponding human rankings. We support HRE’s efficacy by investigating how efficiently it separates pretrained and instruction-tuned LMs of various sizes. We show that HRE correlates well with human judgements and is particularly responsive to model changes following instruction-tuning.</abstract>
      <url hash="f478abb4">2024.naacl-long.455</url>
    </paper>
    <paper id="456">
      <title><fixed-case>FAM</fixed-case>u<fixed-case>S</fixed-case>: Frames Across Multiple Sources</title>
      <author><first>Siddharth</first><last>Vashishtha</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Alexander</first><last>Martin</last></author>
      <author><first>William</first><last>Gantt</last><affiliation>Department of Computer Science, University of Rochester</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Aaron</first><last>White</last><affiliation>University of Rochester</affiliation></author>
      <pages>8229-8252</pages>
      <abstract>Understanding event descriptions is a central aspect of language processing, but current approaches focus overwhelmingly on single sentences or documents. Aggregating information about an event across documents can offer a much richer understanding. To this end, we present FAMuS, a new corpus of Wikipedia passages that report on some event, paired with underlying, genre-diverse (non-Wikipedia) source articles for the same event. Events and (cross-sentence) arguments in both report and source are annotated against FrameNet, providing broad coverage of different event types. We present results on two key event understanding tasks enabled by FAMuS: source validation—determining whether a document is a valid source for a target report event—and cross-document argument extraction—full-document argument extraction for a target event from both its report and the correct source article.</abstract>
      <url hash="c2002c5d">2024.naacl-long.456</url>
    </paper>
    <paper id="457">
      <title>Rationale-based Opinion Summarization</title>
      <author><first>Haoyuan</first><last>Li</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last><affiliation>Department of Computer Science, University of North Carolina, Chapel Hill</affiliation></author>
      <pages>8253-8271</pages>
      <abstract>Opinion summarization aims to generate concise summaries that present popular opinions of a large group of reviews. However, these summaries can be too generic and lack supporting details. To address these issues, we propose a new paradigm for summarizing reviews, rationale-based opinion summarization. Rationale-based opinion summaries output the representative opinions as well as one or more corresponding rationales. To extract good rationales, we define four desirable properties: relatedness, specificity, popularity, and diversity and present a Gibbs-sampling-based method to extract rationales. Overall, we propose RATION, an unsupervised extractive system that has two components: an Opinion Extractor (to extract representative opinions) and Rationales Extractor (to extract corresponding rationales). We conduct automatic and human evaluations to show that rationales extracted by RATION have the proposed properties and its summaries are more useful than conventional summaries. The implementation of our work is available at https://github.com/leehaoyuan/RATION.</abstract>
      <url hash="4fa8fa91">2024.naacl-long.457</url>
    </paper>
    <paper id="458">
      <title>Mustango: Toward Controllable Text-to-Music Generation</title>
      <author><first>Jan</first><last>Melechovsky</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Zixun</first><last>Guo</last></author>
      <author><first>Deepanway</first><last>Ghosal</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Navonil</first><last>Majumder</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Dorien</first><last>Herremans</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>8272-8295</pages>
      <abstract>The quality of the text-to-music models has reached new heights due to recent advancements in diffusion models. The controllability of various musical aspects, however, has barely been explored. In this paper, we propose Mustango: a music-domain-knowledge-inspired text-to-music system based on diffusion. Mustango aims to control the generated music, not only with general text captions, but with more rich captions that can include specific instructions related to chords, beats, tempo, and key. At the core of Mustango is MuNet, a Music-Domain-Knowledge-Informed UNet guidance module that steers the generated music to include the music-specific conditions, which we predict from the text prompt, as well as the general text embedding, during the reverse diffusion process. To overcome the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract the music features which will then be appended to the existing descriptions in text format. We release the resulting MusicBench dataset which contains over 52K instances and includes music-theory-based descriptions in the caption text. Through extensive experiments, we show that the quality of the music generated by Mustango is state-of-the-art, and the controllability through music-specific text prompts greatly outperforms other models such as MusicGen and AudioLDM2.</abstract>
      <url hash="39e6ef37">2024.naacl-long.458</url>
    </paper>
    <paper id="459">
      <title>Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations</title>
      <author><first>Emilio</first><last>Cueva</last></author>
      <author><first>Adrian</first><last>Lopez Monroy</last></author>
      <author><first>Fernando</first><last>Sánchez-Vega</last><affiliation>Center for Research in Mathematics (CIMAT)</affiliation></author>
      <author><first>Thamar</first><last>Solorio</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of Houston</affiliation></author>
      <pages>8296-8314</pages>
      <abstract>Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning. Moreover, we show that when source-language data is limited, the fine-tuning framework employed for IC-XLT performs comparably to prompt-based fine-tuning with significantly more training data in the source language.</abstract>
      <url hash="f8f79a5f">2024.naacl-long.459</url>
    </paper>
    <paper id="460">
      <title><fixed-case>CNER</fixed-case>: Concept and Named Entity Recognition</title>
      <author><first>Giuliano</first><last>Martinelli</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Francesco</first><last>Molfese</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Simone</first><last>Tedeschi</last></author>
      <author><first>Alberte</first><last>Fernández-Castro</last></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>8315-8330</pages>
      <abstract>Named entities – typically expressed via proper nouns – play a key role in Natural Language Processing, as their identification and comprehension are crucial in tasks such as Relation Extraction, Coreference Resolution and Question Answering, among others. Tasks like these also often entail dealing with concepts – typically represented by common nouns – which, however, have not received as much attention. Indeed, the potential of their identification and understanding remains underexplored, as does the benefit of a synergistic formulation with named entities. To fill this gap, we introduce Concept and Named Entity Recognition (CNER), a new unified task that handles concepts and entities mentioned in unstructured texts seamlessly. We put forward a comprehensive set of categories that can be used to model concepts and named entities jointly, and propose new approaches for the creation of CNER datasets. We evaluate the benefits of performing CNER as a unified task extensively, showing that a CNER model gains up to +5.4 and +8 macro F1 points when compared to specialized named entity and concept recognition systems, respectively. Finally, to encourage the development of CNER systems, we release our datasets and models at https://github.com/Babelscape/cner.</abstract>
      <url hash="50a6deeb">2024.naacl-long.460</url>
    </paper>
    <paper id="461">
      <title>Branch-Solve-Merge Improves Large Language Model Evaluation and Generation</title>
      <author><first>Swarnadeep</first><last>Saha</last><affiliation>Department of Computer Science, University of North Carolina, Chapel Hill</affiliation></author>
      <author><first>Omer</first><last>Levy</last><affiliation>Facebook</affiliation></author>
      <author><first>Asli</first><last>Celikyilmaz</last><affiliation>FAIR</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Jason</first><last>Weston</last><affiliation>New York University and Facebook</affiliation></author>
      <author><first>Xian</first><last>Li</last><affiliation>Facebook AI</affiliation></author>
      <pages>8331-8349</pages>
      <abstract>Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model’s lack of coherence and inability to plan and decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%, and allowing LLaMA-2-chat to match or outperform GPT-4 on most domains. On a constraint story generation task, BSM improves the coherence of stories while also improving constraint satisfaction by 12%.</abstract>
      <url hash="5d15a4c6">2024.naacl-long.461</url>
    </paper>
    <paper id="462">
      <title><fixed-case>REPLUG</fixed-case>: Retrieval-Augmented Black-Box Language Models</title>
      <author><first>Weijia</first><last>Shi</last></author>
      <author><first>Sewon</first><last>Min</last><affiliation>Facebook and Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Michihiro</first><last>Yasunaga</last><affiliation>Stanford University</affiliation></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Richard</first><last>James</last><affiliation>Research, Facebook</affiliation></author>
      <author><first>Mike</first><last>Lewis</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Luke</first><last>Zettlemoyer</last><affiliation>University of Washington, Facebook and Meta</affiliation></author>
      <author><first>Wen-tau</first><last>Yih</last><affiliation>Meta Platforms, Inc.</affiliation></author>
      <pages>8350-8363</pages>
      <abstract>We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.</abstract>
      <url hash="6ce9b396">2024.naacl-long.462</url>
    </paper>
    <paper id="463">
      <title><fixed-case>D</fixed-case>avid helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion <fixed-case>LM</fixed-case>s</title>
      <author><first>Xiaochuang</first><last>Han</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Sachin</first><last>Kumar</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Marjan</first><last>Ghazvininejad</last><affiliation>Facebook AI Research</affiliation></author>
      <pages>8364-8379</pages>
      <abstract>Diffusion-based language models are emerging as a promising alternative to autoregressive LMs: they approach the competence of autoregressive LMs while offering nuanced controllability at inference time. While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies of diffusion LMs have been conducted on a smaller scale. Starting with a recently proposed diffusion model SSD-LM, in this work we first explore methods to scale it from 0.4B to 13B parameters, proposing techniques to improve its training and inference efficiency, and to finetune the model to follow instructions. Armed with a more powerful, general purpose diffusion LM, we introduce the primary contribution of this work – SSD-2 – an approach to easily ensemble at inference time a large general-purpose diffusion LM with smaller, but specialized and contextualized diffusion LMs. We show that SSD-2 facilitates novel ensembles with 100x smaller models that can be customized and deployed by individual users. We find that compared to autoregressive models, the collaboration between diffusion LMs is more effective, leading to higher-quality model responses due to their ability to dynamically incorporate bi-directional contexts.</abstract>
      <url hash="ae792545">2024.naacl-long.463</url>
    </paper>
    <paper id="464">
      <title>Efficient End-to-End Visual Document Understanding with Rationale Distillation</title>
      <author><first>Wang</first><last>Zhu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Alekh</first><last>Agarwal</last><affiliation>Google</affiliation></author>
      <author><first>Mandar</first><last>Joshi</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Robin</first><last>Jia</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jesse</first><last>Thomason</last><affiliation>University of Southern California and Amazon</affiliation></author>
      <author><first>Kristina</first><last>Toutanova</last><affiliation>Google</affiliation></author>
      <pages>8380-8403</pages>
      <abstract>Understanding visually situated language requires interpreting complex layouts of textual and visual elements. Pre-processing tools, such as optical character recognition (OCR), can map document image inputs to textual tokens, then large language models (LLMs) can reason over text.However, such methods have high computational and engineering complexity. Can small pretrained image-to-text models accurately understand visual documents through similar recognition and reasoning steps instead?We propose Rationale Distillation (RD), which incorporates the outputs of OCR tools, LLMs, and larger multimodal models as intermediate “rationales”, and trains a small student model to predict both rationales and answers. On three visual document understanding benchmarks representing infographics, scanned documents, and figures, our Pix2Struct (282M parameters) student model finetuned with RD outperforms the base model by 4-5% absolute accuracy with only 1% higher computational cost.</abstract>
      <url hash="12cd2c8d">2024.naacl-long.464</url>
    </paper>
    <paper id="465">
      <title>A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models</title>
      <author><first>Tiwalayo</first><last>Eisape</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Michael</first><last>Tessler</last><affiliation>DeepMind</affiliation></author>
      <author><first>Ishita</first><last>Dasgupta</last><affiliation>DeepMind</affiliation></author>
      <author><first>Fei</first><last>Sha</last></author>
      <author><first>Sjoerd</first><last>Steenkiste</last><affiliation>Google</affiliation></author>
      <author><first>Tal</first><last>Linzen</last><affiliation>New York University and Google</affiliation></author>
      <pages>8404-8423</pages>
      <abstract>A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans’ inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate such human biases, or are they able to overcome them? Focusing on the case of syllogisms—inferences from two simple premises—we show that, within the PaLM 2 family of transformer language models, larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases: they show sensitivity to the (irrelevant) ordering of the variables in the syllogism, and draw confident but incorrect inferences from particular syllogisms (syllogistic fallacies). Overall, we find that language models often mimic the human biases included in their training data, but are able to overcome them in some cases.</abstract>
      <url hash="a3e6759f">2024.naacl-long.465</url>
    </paper>
    <paper id="466">
      <title><fixed-case>A</fixed-case>nchor<fixed-case>AL</fixed-case>: Computationally Efficient Active Learning for Large and Imbalanced Datasets</title>
      <author><first>Pietro</first><last>Lesci</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <pages>8424-8443</pages>
      <abstract>Active learning for imbalanced classification tasks is challenging as the minority classes naturally occur rarely. Gathering a large pool of unlabelled data is thus essential to capture minority instances. Standard pool-based active learning is computationally expensive on large pools and often reaches low accuracy by overfitting the initial decision boundary, thus failing to explore the input space and find minority instances. To address these issues we propose AnchorAL. At each iteration, AnchorAL chooses class-specific instances from the labelled set, or *anchors*, and retrieves the most similar unlabelled instances from the pool. This resulting *subpool* is then used for active learning. Using a small, fixed-sized subpool AnchorAL allows scaling any active learning strategy to large pools. By dynamically selecting different anchors at each iteration it promotes class balance and prevents overfitting the initial decision boundary, thus promoting the discovery of new clusters of minority instances. Experiments across different classification tasks, active learning strategies, and model architectures AnchorAL is *(i)* faster, often reducing runtime from hours to minutes, *(ii)* trains more performant models, *(iii)* and returns more balanced datasets than competing methods.</abstract>
      <url hash="50a862d8">2024.naacl-long.466</url>
    </paper>
    <paper id="467">
      <title><fixed-case>ICLE</fixed-case>++: Modeling Fine-Grained Traits for Holistic Essay Scoring</title>
      <author><first>Shengjie</first><last>Li</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Vincent</first><last>Ng</last><affiliation>University of Texas at Dallas, Central China Normal University and State University of New York at Stony Brook</affiliation></author>
      <pages>8444-8464</pages>
      <abstract>The majority of the recently developed models for automated essay scoring (AES) are evaluated solely on the ASAP corpus. However, ASAP is not without its limitations. For instance, it is not clear whether models trained on ASAP can generalize well when evaluated on other corpora. In light of these limitations, we introduce ICLE++, a corpus of persuasive student essays annotated with both holistic scores and trait-specific scores. Not only can ICLE++ be used to test the generalizability of AES models trained on ASAP, but it can also facilitate the evaluation of models developed for newer AES problems such as multi-trait scoring and cross-prompt scoring. We believe that ICLE++, which represents a culmination of our long-term effort in annotating the essays in the ICLE corpus, contributes to the set of much-needed annotated corpora for AES research.</abstract>
      <url hash="171c2abf">2024.naacl-long.467</url>
    </paper>
    <paper id="468">
      <title><fixed-case>UN</fixed-case>commonsense Reasoning: Abductive Reasoning about Uncommon Situations</title>
      <author><first>Wenting</first><last>Zhao</last><affiliation>Cornell University</affiliation></author>
      <author><first>Justin</first><last>Chiu</last><affiliation>Cornell University</affiliation></author>
      <author><first>Jena</first><last>Hwang</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Faeze</first><last>Brahman</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Jack</first><last>Hessel</last><affiliation>Samaya AI</affiliation></author>
      <author><first>Sanjiban</first><last>Choudhury</last><affiliation>Cornell University</affiliation></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Alane</first><last>Suhr</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>8465-8483</pages>
      <abstract>Language technologies that accurately model the dynamics of events must perform commonsense reasoning. Existing work evaluating commonsense reasoning focuses on making inferences about common, everyday situations. To instead investigate the ability to model unusual, unexpected, and unlikely situations, we explore the task of uncommonsense abductive reasoning. Given a piece of context with an unexpected outcome, this task requires reasoning abductively to generate an explanation that makes the unexpected outcome more likely in the context. To this end, we curate and release a new English language corpus called UNcommonsense. We characterize the performance differences between human explainers and the best-performing large language models, finding that model-enhanced human-written explanations achieve the highest quality by trading off between specificity and diversity. Finally, we experiment with several imitation learning algorithms to train open and accessible language models on this task. When compared with the vanilla supervised fine-tuning approach, these methods consistently reduce lose rates on both common and uncommonsense abductive reasoning judged by human evaluators.</abstract>
      <url hash="09c590f3">2024.naacl-long.468</url>
    </paper>
    <paper id="469">
      <title>To Tell The Truth: Language of Deception and Language Models</title>
      <author><first>Sanchaita</first><last>Hazra</last></author>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>8484-8498</pages>
      <abstract>Text-based false information permeates online discourses, yet evidence of people’s ability to discern truth from such deceptive textual content is scarce. We analyze a novel TV game show data where conversations in a high-stake environment between individuals with conflicting objectives result in lies. We investigate the manifestation of potentially verifiable language cues of deception in the presence of objective truth, a distinguishing feature absent in previous text-based deception datasets. We show that there exists a class of detectors (algorithms) that have similar truth detection performance compared to human subjects, even when the former accesses only the language cues while the latter engages in conversations with complete access to all potential sources of cues (language and audio-visual). Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth, an act of reasoning in which human subjects often perform poorly, even with incentives. Our model detects novel but accurate language cues in many cases where humans failed to detect deception, opening up the possibility of humans collaborating with algorithms and ameliorating their ability to detect the truth.</abstract>
      <url hash="54c41cda">2024.naacl-long.469</url>
    </paper>
    <paper id="470">
      <title>Multilingual Models for <fixed-case>ASR</fixed-case> in Chibchan Languages</title>
      <author><first>Rolando</first><last>Coto-Solano</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Tai Wan</first><last>Kim</last></author>
      <author><first>Alexander</first><last>Jones</last></author>
      <author><first>Sharid</first><last>Loáiciga</last><affiliation>University of Gothenburg, Sweden</affiliation></author>
      <pages>8499-8513</pages>
      <abstract>We present experiments on Automatic Speech Recognition (ASR) for Bribri and Cabécar, two languages from the Chibchan family. We fine-tune four ASR algorithms (Wav2Vec2, Whisper, MMS &amp; WavLM) to create monolingual models, with the Wav2Vec2 model demonstrating the best performance. We then proceed to use Wav2Vec2 for (1) experiments on training joint and transfer learning models for both languages, and (2) an analysis of the errors, with a focus on the transcription of tone. Results show effective transfer learning for both Bribri and Cabécar, but especially for Bribri. A post-processing spell checking step further reduced character and word error rates. As for the errors, tone is where the Bribri models make the most errors, whereas the simpler tonal system of Cabécar is better transcribed by the model. Our work contributes to developing better ASR technology, an important tool that could facilitate transcription, one of the major bottlenecks in language documentation efforts. Our work also assesses how existing pre-trained models and algorithms perform for genuine extremely low resource-languages.</abstract>
      <url hash="e856595b">2024.naacl-long.470</url>
    </paper>
    <paper id="471">
      <title><fixed-case>L</fixed-case>egal<fixed-case>D</fixed-case>iscourse: Interpreting When Laws Apply and To Whom</title>
      <author><first>Alexander</first><last>Spangher</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Zihan</first><last>Xue</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Te-Lin</first><last>Wu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Mark</first><last>Hansen</last><affiliation>Columbia University</affiliation></author>
      <author><first>Jonathan</first><last>May</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>8514-8537</pages>
      <abstract>While legal AI has made strides in recent years, it still struggles with basic legal concepts: _when_ does a law apply? _Who_ does it applies to? _What_ does it do? We take a _discourse_ approach to addressing these problems and introduce a novel taxonomy for span-and-relation parsing of legal texts. We create a dataset, _LegalDiscourse_ of 602 state-level law paragraphs consisting of 3,715 discourse spans and 1,671 relations. Our trained annotators have an agreement-rate <tex-math>\kappa&gt;.8</tex-math>, yet few-shot GPT3.5 performs poorly at span identification and relation classification. Although fine-tuning improves performance, GPT3.5 still lags far below human level. We demonstrate the usefulness of our schema by creating a web application with journalists. We collect over 100,000 laws for 52 U.S. states and territories using 20 scrapers we built, and apply our trained models to 6,000 laws using U.S. Census population numbers. We describe two journalistic outputs stemming from this application: (1) an investigation into the increase in liquor licenses following population growth and (2) a decrease in applicable laws under different under-count projections.</abstract>
      <url hash="07fd1fd8">2024.naacl-long.471</url>
    </paper>
    <paper id="472">
      <title><fixed-case>X</fixed-case>-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects</title>
      <author><first>Minqian</first><last>Liu</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Zhiyang</first><last>Xu</last></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>Vaibhav</first><last>Kumar</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Reza</first><last>Ghanadan</last><affiliation>Amazon</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>8538-8557</pages>
      <abstract>Natural Language Generation (NLG) typically involves evaluating the generated text in various aspects (e.g., consistency and naturalness) to obtain a comprehensive assessment. However, multi-aspect evaluation remains challenging as it may require the evaluator to generalize to any given evaluation aspect even if it’s absent during training. In this paper, we introduce X-Eval, a two-stage instruction tuning framework to evaluate text in both seen and unseen aspects customized by end users. X-Eval consists of two learning stages: the vanilla instruction tuning stage that improves the model’s ability to follow evaluation instructions, and an enhanced instruction tuning stage that exploits the connections between fine-grained evaluation aspects to better assess text quality. To support the training of X-Eval, we collect AspectInstruct, the first instruction tuning dataset tailored for multi-aspect NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance task diversity, we devise an augmentation strategy that converts human rating annotations into diverse forms of NLG evaluation tasks, including scoring, comparison, ranking, and Boolean question answering. Extensive experiments across three essential categories of NLG tasks: dialogue generation, summarization, and data-to-text coupled with 21 aspects in meta-evaluation, demonstrate that X-Eval enables even a lightweight language model to achieve a comparable if not higher correlation with human judgments compared to the state-of-the-art NLG evaluators like GPT-4.</abstract>
      <url hash="c663387e">2024.naacl-long.472</url>
    </paper>
    <paper id="473">
      <title>Is Reference Necessary in the Evaluation of <fixed-case>NLG</fixed-case> Systems? When and Where?</title>
      <author><first>Shuqian</first><last>Sheng</last></author>
      <author><first>Yi</first><last>Xu</last></author>
      <author><first>Luoyi</first><last>Fu</last></author>
      <author><first>Jiaxin</first><last>Ding</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Lei</first><last>Zhou</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Xinbing</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Chenghu</first><last>Zhou</last><affiliation>IGSNRR, Chinese Academy of Sciences, Beijing, China</affiliation></author>
      <pages>8558-8574</pages>
      <abstract>The majority of automatic metrics for evaluating NLG systems are reference-based. However, the challenge of collecting human annotation results in a lack of reliable references in numerous application scenarios. Despite recent advancements in reference-free metrics, it has not been well understood when and where they can be used as an alternative to reference-based metrics. In this study, by employing diverse analytical approaches, we comprehensively assess the performance of both metrics across a wide range of NLG tasks, encompassing eight datasets and eight evaluation models. Based on solid experiments, the results show that reference-free metrics exhibit a higher correlation with human judgment and greater sensitivity to deficiencies in language quality. However, their effectiveness varies across tasks and is influenced by the quality of candidate texts. Therefore, it’s important to assess the performance of reference-free metrics before applying them to a new task, especially when inputs are in uncommon form or when the answer space is highly variable. Our study can provide insight into the appropriate application of automatic metrics and the impact of metric choice on evaluation performance.</abstract>
      <url hash="1da87715">2024.naacl-long.473</url>
    </paper>
    <paper id="474">
      <title>Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning</title>
      <author><first>Xin</first><last>Su</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Tiep</first><last>Le</last><affiliation>Intel</affiliation></author>
      <author><first>Steven</first><last>Bethard</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Phillip</first><last>Howard</last><affiliation>Intel</affiliation></author>
      <pages>8575-8591</pages>
      <abstract>An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model’s parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model’s parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning.</abstract>
      <url hash="5364de5c">2024.naacl-long.474</url>
    </paper>
    <paper id="475">
      <title>Evaluating the Deductive Competence of Large Language Models</title>
      <author><first>S</first><last>Seals</last></author>
      <author><first>Valerie</first><last>Shalin</last><affiliation>University of South Carolina and Wright State University</affiliation></author>
      <pages>8592-8608</pages>
      <abstract>The development of highly fluent large language models (LLMs) has prompted increased interest in assessing their reasoning and problem-solving capabilities. We investigate whether several LLMs can solve a classic type of deductive reasoning problem from the cognitive science literature. The tested LLMs have limited abilities to solve these problems in their conventional form. We performed follow up experiments to investigate if changes to the presentation format and content improve model performance. We do find performance differences between conditions; however, they do not improve overall performance. Moreover, we find that performance interacts with presentation format and content in unexpected ways that differ from human performance. Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance and the human-generated language corpora that informs them.</abstract>
      <url hash="c048c8ca">2024.naacl-long.475</url>
    </paper>
    <paper id="476">
      <title>Large Human Language Models: A Need and the Challenges</title>
      <author><first>Nikita</first><last>Soni</last></author>
      <author><first>H.</first><last>Schwartz</last><affiliation>Stony Brook University (SUNY)</affiliation></author>
      <author><first>João</first><last>Sedoc</last><affiliation>New York University</affiliation></author>
      <author><first>Niranjan</first><last>Balasubramanian</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>8609-8624</pages>
      <abstract>As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human context. We refer to relevant advances and present open challenges that need to be addressed and their possible solutions in realizing these goals.</abstract>
      <url hash="ea6132bd">2024.naacl-long.476</url>
    </paper>
    <paper id="477">
      <title>On Learning to Summarize with Large Language Models as References</title>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Kejian</first><last>Shi</last></author>
      <author><first>Katherine</first><last>He</last></author>
      <author><first>Longtian</first><last>Ye</last></author>
      <author><first>Alexander</first><last>Fabbri</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Dragomir</first><last>Radev</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>8625-8642</pages>
      <abstract>Recent studies have found that summaries generated by large language models (LLMs) are favored by human annotators over the original reference summaries in commonly used summarization datasets. Therefore, we study an LLM-as-reference learning setting for smaller text summarization models to investigate whether their performance can be substantially improved. To this end, we use LLMs as both oracle summary generators for standard supervised fine-tuning and oracle summary evaluators for efficient contrastive learning that leverages the LLMs’ supervision signals. We conduct comprehensive experiments with source news articles and find that (1) summarization models trained under the LLM-as-reference setting achieve significant performance improvement in both LLM and human evaluations; (2) contrastive learning outperforms standard supervised fine-tuning under both low and high resource settings. Our experimental results also enable a meta-analysis of LLMs’ summary evaluation capacities under a challenging setting, showing that LLMs are not well-aligned with human evaluators. Particularly, our expert human evaluation reveals remaining nuanced performance gaps between LLMs and our fine-tuned models, which LLMs fail to capture. Thus, we call for further studies into both the potential and challenges of using LLMs in summarization model development.</abstract>
      <url hash="e92ec114">2024.naacl-long.477</url>
    </paper>
    <paper id="478">
      <title>Hallucination Diversity-Aware Active Learning for Text Summarization</title>
      <author><first>Yu</first><last>Xia</last></author>
      <author><first>Xu</first><last>Liu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ryan</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Anup</first><last>Rao</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tung</first><last>Mai</last><affiliation>Adobe</affiliation></author>
      <author><first>Shuai</first><last>Li</last><affiliation>John Hopcroft Center, Shanghai Jiao Tong University</affiliation></author>
      <pages>8643-8655</pages>
      <abstract>Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning. Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating LLM hallucinations.</abstract>
      <url hash="e43ecc1a">2024.naacl-long.478</url>
    </paper>
    <paper id="479">
      <title><fixed-case>K</fixed-case>eep it <fixed-case>P</fixed-case>rivate: Unsupervised Privatization of Online Text</title>
      <author><first>Calvin</first><last>Bao</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>8656-8671</pages>
      <abstract>Authorship obfuscation techniques hold the promise of helping people protect their privacy in online communications by automatically rewriting text to hide the identity of the original author. However, obfuscation has been evaluated in narrow settings in the NLP literature and has primarily been addressed with superficial edit operations that can lead to unnatural outputs. In this work, we introduce an automatic text privatization framework that fine-tunes a large language model via reinforcement learning to produce rewrites that balance soundness, sense, and privacy. We evaluate it extensively on a large-scale test set of English Reddit posts by 68k authors composed of short-medium length texts. We study how the performance changes among evaluative conditions including authorial profile length and authorship detection strategy. Our method maintains high text quality according to both automated metrics and human evaluation, and successfully evades several automated authorship attacks.</abstract>
      <url hash="07598044">2024.naacl-long.479</url>
    </paper>
    <paper id="480">
      <title>Tied-<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Enhancing parameter efficiency of <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> with Weight Tying</title>
      <author><first>Adithya</first><last>Renduchintala</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Tugrul</first><last>Konuk</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Oleksii</first><last>Kuchaiev</last><affiliation>NVIDIA</affiliation></author>
      <pages>8672-8683</pages>
      <abstract>We introduce Tied-LoRA, a novel paradigm leveraging weight tying and selective training to enhance the parameter efficiency of Low-rank Adaptation (LoRA). Our exploration encompasses different plausible combinations of parameter training and freezing, coupled with weight tying, aimed at identifying the optimal trade-off between performance and the count of trainable parameters. Across 5 diverse tasks and two foundational language models with different parameter counts, our experiments provide comprehensive insights into the inherent trade-offs between efficiency and performance.Our findings reveal a specific Tied-LoRA configuration that distinguishes itself by showcasing comparable performance to LoRA across multiple tasks while utilizing only a fraction of the parameters employed by the standard LoRA method, particularly at elevated ranks. This underscores the efficacy of Tied-LoRA in achieving impressive results with significantly reduced model complexity.</abstract>
      <url hash="54db5f4e">2024.naacl-long.480</url>
    </paper>
    <paper id="481">
      <title>Investigating Data Contamination in Modern Benchmarks for Large Language Models</title>
      <author><first>Chunyuan</first><last>Deng</last></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Xiangru</first><last>Tang</last><affiliation>Yale University</affiliation></author>
      <author><first>Mark</first><last>Gerstein</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>8684-8697</pages>
      <abstract>Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named Testset Slot Guessing (TS-Guessing), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52% and 57%, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.</abstract>
      <url hash="a6c08a3b">2024.naacl-long.481</url>
    </paper>
    <paper id="482">
      <title>Pre-trained Language Models for Entity Blocking: A Reproducibility Study</title>
      <author><first>Runhui</first><last>Wang</last></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>8698-8708</pages>
      <abstract>Entity Resolution (ER) is an essential task in data integration and its goal is to find records that represent the same entity in a dataset. Deep learning models, especially large pre-trained language models, have achieved state-of-the-art results on this task. A typical ER pipeline consists of Entity Blocking and Entity Matching: Entity Blocking finds candidate record pairs that potentially match and Entity Matching determines if the pairs match. The goal of the entity blocking step is to include as many matching pairs as possible while including as few non-matching pairs as possible. On the other hand, the blocking task can also be considered as an Information Retrieval (IR) task. However, state-of-the-art neural IR models that are based on large language models have not been evaluated on the ER task. What’s more, the generalization ability of state-of-the-art methods for entity blocking is not well-studied but an import aspect in real-world applications. In this work, we evaluate state-of-the-art models for Entity Blocking along with neural IR models on a wide range of real-world datasets, and also study their in-distribution and out-of-distribution generalization abilities.</abstract>
      <url hash="1fd3f97c">2024.naacl-long.482</url>
    </paper>
    <paper id="483">
      <title><tex-math>RE^2</tex-math>: Region-Aware Relation Extraction from Visually Rich Documents</title>
      <author><first>Pritika</first><last>Ramu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Sijia</first><last>Wang</last></author>
      <author><first>Lalla</first><last>Mouatadid</last></author>
      <author><first>Joy</first><last>Rimchala</last></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>8709-8725</pages>
      <abstract>Current research in form understanding predominantly relies on large pre-trained language models, necessitating extensive data for pre-training. However, the importance of layout structure (i.e., the spatial relationship between the entity blocks in the visually rich document) to relation extraction has been overlooked. In this paper, we propose <tex-math>\textbf{RE}</tex-math>gion-Aware <tex-math>\textbf{R}</tex-math>elation <tex-math>\textbf{E}</tex-math>xtraction (<tex-math>\bf{RE^2}</tex-math>) that leverages region-level spatial structure among the entity blocks to improve their relation prediction. We design an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations. We also introduce a constraint objective to regularize the model towards consistency with the inherent constraints of the relation extraction task. To support the research on relation extraction from visually rich documents and demonstrate the generalizability of <tex-math>\bf{RE^2}</tex-math>, we build a new benchmark dataset, <tex-math>{DiverseForm}</tex-math>, that covers a wide range of domains. Extensive experiments on <tex-math>{DiverseForm}</tex-math> and several public benchmark datasets demonstrate significant superiority and transferability of <tex-math>\bf{RE^2}</tex-math> across various domains and languages, with up to 18.88% absolute F-score gain over all high-performing baselines</abstract>
      <url hash="dc2b2e51">2024.naacl-long.483</url>
    </paper>
    <paper id="484">
      <title>Mix-Initiative Response Generation with Dynamic Prefix Tuning</title>
      <author><first>Yuxiang</first><last>Nie</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Xian-Ling</first><last>Mao</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <pages>8726-8739</pages>
      <abstract>Mixed initiative serves as one of the key factors in controlling conversation directions. For a speaker, responding passively or leading proactively would result in rather different responses. However, most dialogue systems focus on training a holistic response generation model without any distinction among different initiatives. It leads to the cross-contamination problem, where the model confuses different initiatives and generates inappropriate responses. Moreover, obtaining plenty of human annotations for initiative labels can be expensive. To address this issue, we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to decouple different initiatives from the generation model, which learns initiative-aware prefixes in both supervised and unsupervised settings. Specifically, IDPT decouples initiative factors into different prefix parameters and uses the attention mechanism to adjust the selection of initiatives in guiding generation dynamically. The prefix parameters can be tuned towards accurate initiative prediction as well as mix-initiative response generation. Extensive experiments on two public dialogue datasets show that the proposed IDPT outperforms previous baselines on both automatic metrics and human evaluations. It also manages to generate appropriate responses with manipulated initiatives.</abstract>
      <url hash="174876da">2024.naacl-long.484</url>
    </paper>
    <paper id="485">
      <title>Value <fixed-case>FULCRA</fixed-case>: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Value</title>
      <author><first>Jing</first><last>Yao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xiaoyuan</first><last>Yi</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Yifan</first><last>Gong</last></author>
      <author><first>Xiting</first><last>Wang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft</affiliation></author>
      <pages>8740-8763</pages>
      <abstract>Value alignment is crucial for the responsible development of Large Language Models (LLMs). However, how to define values in this context remains largely unexplored. Existing work mainly specifies values as risk criteria formulated in the AI community, e.g., fairness and privacy protection, suffering from poor clarity, adaptability and transparency. Leveraging basic values established in humanity and social science that are compatible with values across cultures, this paper introduces a novel value space spanned by multiple basic value dimensions and proposes BaseAlign, a corresponding value alignment paradigm. Applying the representative Schwartz’s Theory of Basic Values as an instantiation, we construct FULCRA, a dataset consisting of 20k <tex-math>(</tex-math>LLM output, value vector<tex-math>)</tex-math> pairs. LLMs’ outputs are mapped into the <tex-math>K</tex-math>-dim value space beyond simple binary labels, by identifying their underlying priorities for these value dimensions. Extensive analysis and experiments on FULCRA: (1) reveal the essential relation between basic values and LLMs’ behaviors, (2) demonstrate that our paradigm with basic values not only covers existing risks but also anticipates the unidentified ones, and (3) manifest BaseAlign’s superiority in alignment performance with less data, paving the way for addressing the above three challenges.</abstract>
      <url hash="d92872e0">2024.naacl-long.485</url>
    </paper>
    <paper id="486">
      <title><fixed-case>I</fixed-case>ndi<fixed-case>B</fixed-case>ias: A Benchmark Dataset to Measure Social Biases in Language Models for <fixed-case>I</fixed-case>ndian Context</title>
      <author><first>Nihar</first><last>Sahoo</last></author>
      <author><first>Pranamya</first><last>Kulkarni</last></author>
      <author><first>Arif</first><last>Ahmad</last></author>
      <author><first>Tanu</first><last>Goyal</last></author>
      <author><first>Narjis</first><last>Asad</last></author>
      <author><first>Aparna</first><last>Garimella</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>8764-8784</pages>
      <abstract>The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India’s unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersectional biases along three intersectional dimensions. Our dataset contains 800 sentence pairs and 300 tuples for bias measurement across different demographics. The dataset is available in English and Hindi, providing a size comparable to existing benchmark datasets. Furthermore, using IndiBias we compare ten different language models on multiple bias measurement metrics. We observed that the language models exhibit more bias across a majority of the intersectional groups. All the scripts utilized and datasets created in this study are publicly available.</abstract>
      <url hash="cd418f93">2024.naacl-long.486</url>
    </paper>
  </volume>
</collection>
