<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.wanlp">
  <volume id="1" ingest-date="2021-04-19" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Sixth Arabic Natural Language Processing Workshop</booktitle>
      <editor><first>Nizar</first><last>Habash</last></editor>
      <editor><first>Houda</first><last>Bouamor</last></editor>
      <editor><first>Hazem</first><last>Hajj</last></editor>
      <editor><first>Walid</first><last>Magdy</last></editor>
      <editor><first>Wajdi</first><last>Zaghouani</last></editor>
      <editor><first>Fethi</first><last>Bougares</last></editor>
      <editor><first>Nadi</first><last>Tomeh</last></editor>
      <editor><first>Ibrahim</first><last>Abu Farha</last></editor>
      <editor><first>Samia</first><last>Touileb</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Kyiv, Ukraine (Virtual)</address>
      <month>April</month>
      <year>2021</year>
      <venue>wanlp</venue>
    </meta>
    <frontmatter>
      <url hash="c61d72ad">2021.wanlp-1.0</url>
      <bibkey>wanlp-2021-arabic</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>QADI</fixed-case>: <fixed-case>A</fixed-case>rabic Dialect Identification in the Wild</title>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <pages>1–10</pages>
      <abstract>Proper dialect identification is important for a variety of Arabic NLP applications. In this paper, we present a method for rapidly constructing a tweet dataset containing a wide range of country-level Arabic dialects —covering 18 different countries in the Middle East and North Africa region. Our method relies on applying multiple filters to identify users who belong to different countries based on their account descriptions and to eliminate tweets that either write mainly in Modern Standard Arabic or mostly use vulgar language. The resultant dataset contains 540k tweets from 2,525 users who are evenly distributed across 18 Arab countries. Using intrinsic evaluation, we show that the labels of a set of randomly selected tweets are 91.5% accurate. For extrinsic evaluation, we are able to build effective country level dialect identification on tweets with a macro-averaged F1-score of 60.6% across 18 classes.</abstract>
      <url hash="db4f2ab9">2021.wanlp-1.1</url>
      <bibkey>abdelali-etal-2021-qadi</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>D</fixed-case>ia<fixed-case>L</fixed-case>ex: A Benchmark for Evaluating Multidialectal <fixed-case>A</fixed-case>rabic Word Embeddings</title>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Shady</first><last>Elbassuoni</last></author>
      <author><first>Jad</first><last>Doughman</last></author>
      <author><first>AbdelRahim</first><last>Elmadany</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
      <author><first>Yorgo</first><last>Zoughby</last></author>
      <author><first>Ahmad</first><last>Shaher</last></author>
      <author><first>Iskander</first><last>Gaba</last></author>
      <author><first>Ahmed</first><last>Helal</last></author>
      <author><first>Mohammed</first><last>El-Razzaz</last></author>
      <pages>11–20</pages>
      <abstract>Word embeddings are a core component of modern natural language processing systems, making the ability to thoroughly evaluate them a vital task. We describe DiaLex, a benchmark for intrinsic evaluation of dialectal Arabic word embeddings. DiaLex covers five important Arabic dialects: Algerian, Egyptian, Lebanese, Syrian, and Tunisian. Across these dialects, DiaLex provides a testbank for six syntactic and semantic relations, namely male to female, singular to dual, singular to plural, antonym, comparative, and genitive to past tense. DiaLex thus consists of a collection of word pairs representing each of the six relations in each of the five dialects. To demonstrate the utility of DiaLex, we use it to evaluate a set of existing and new Arabic word embeddings that we developed. Beyond evaluation of word embeddings, DiaLex supports efforts to integrate dialects into the Arabic language curriculum. It can be easily translated into Modern Standard Arabic and English, which can be useful for evaluating word translation. Our benchmark, evaluation code, and new word embedding models will be publicly available.</abstract>
      <url hash="7e391a13">2021.wanlp-1.2</url>
      <bibkey>abdul-mageed-etal-2021-dialex</bibkey>
      <pwccode url="https://github.com/UBC-NLP/dialex" additional="false">UBC-NLP/dialex</pwccode>
    </paper>
    <paper id="3">
      <title>Benchmarking Transformer-based Language Models for <fixed-case>A</fixed-case>rabic Sentiment and Sarcasm Detection</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>21–31</pages>
      <abstract>The introduction of transformer-based language models has been a revolutionary step for natural language processing (NLP) research. These models, such as BERT, GPT and ELECTRA, led to state-of-the-art performance in many NLP tasks. Most of these models were initially developed for English and other languages followed later. Recently, several Arabic-specific models started emerging. However, there are limited direct comparisons between these models. In this paper, we evaluate the performance of 24 of these models on Arabic sentiment and sarcasm detection. Our results show that the models achieving the best performance are those that are trained on only Arabic data, including dialectal Arabic, and use a larger number of parameters, such as the recently released MARBERT. However, we noticed that AraELECTRA is one of the top performing models while being much more efficient in its computational cost. Finally, the experiments on AraGPT2 variants showed low performance compared to BERT models, which indicates that it might not be suitable for classification tasks.</abstract>
      <url hash="90971353">2021.wanlp-1.3</url>
      <bibkey>abu-farha-magdy-2021-benchmarking</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm">ArSarcasm</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
    </paper>
    <paper id="4">
      <title>What does <fixed-case>BERT</fixed-case> Learn from <fixed-case>A</fixed-case>rabic Machine Reading Comprehension Datasets?</title>
      <author><first>Eman</first><last>Albilali</last></author>
      <author><first>Nora</first><last>Altwairesh</last></author>
      <author><first>Manar</first><last>Hosny</last></author>
      <pages>32–41</pages>
      <abstract>In machine reading comprehension tasks, a model must extract an answer from the available context given a question and a passage. Recently, transformer-based pre-trained language models have achieved state-of-the-art performance in several natural language processing tasks. However, it is unclear whether such performance reflects true language understanding. In this paper, we propose adversarial examples to probe an Arabic pre-trained language model (AraBERT), leading to a significant performance drop over four Arabic machine reading comprehension datasets. We present a layer-wise analysis for the transformer’s hidden states to offer insights into how AraBERT reasons to derive an answer. The experiments indicate that AraBERT relies on superficial cues and keyword matching rather than text understanding. Furthermore, hidden state visualization demonstrates that prediction errors can be recognized from vector representations in earlier layers.</abstract>
      <url hash="1a9fc39f">2021.wanlp-1.4</url>
      <bibkey>albilali-etal-2021-bert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="5">
      <title>Kawarith: an <fixed-case>A</fixed-case>rabic <fixed-case>T</fixed-case>witter Corpus for Crisis Events</title>
      <author><first>Alaa</first><last>Alharbi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>42–52</pages>
      <abstract>Social media (SM) platforms such as Twitter provide large quantities of real-time data that can be leveraged during mass emergencies. Developing tools to support crisis-affected communities requires available datasets, which often do not exist for low resource languages. This paper introduces Kawarith a multi-dialect Arabic Twitter corpus for crisis events, comprising more than a million Arabic tweets collected during 22 crises that occurred between 2018 and 2020 and involved several types of hazard. Exploration of this content revealed the most discussed topics and information types, and the paper presents a labelled dataset from seven emergency events that serves as a gold standard for several tasks in crisis informatics research. Using annotated data from the same event, a BERT model is fine-tuned to classify tweets into different categories in the multi- label setting. Results show that BERT-based models yield good performance on this task even with small amounts of task-specific training data.</abstract>
      <url hash="210f6219">2021.wanlp-1.5</url>
      <bibkey>alharbi-lee-2021-kawarith</bibkey>
      <pwccode url="https://github.com/alaa-a-a/multi-dialect-arabic-stop-words" additional="true">alaa-a-a/multi-dialect-arabic-stop-words</pwccode>
    </paper>
    <paper id="6">
      <title><fixed-case>A</fixed-case>rabic Compact Language Modelling for Resource Limited Devices</title>
      <author><first>Zaid</first><last>Alyafeai</last></author>
      <author><first>Irfan</first><last>Ahmad</last></author>
      <pages>53–59</pages>
      <abstract>Natural language modelling has gained a lot of interest recently. The current state-of-the-art results are achieved by first training a very large language model and then fine-tuning it on multiple tasks. However, there is little work on smaller more compact language models for resource-limited devices or applications. Not to mention, how to efficiently train such models for a low-resource language like Arabic. In this paper, we investigate how such models can be trained in a compact way for Arabic. We also show how distillation and quantization can be applied to create even smaller models. Our experiments show that our largest model which is 2x smaller than the baseline can achieve better results on multiple tasks with 2x less data for pretraining.</abstract>
      <url hash="ecc21586">2021.wanlp-1.6</url>
      <bibkey>alyafeai-ahmad-2021-arabic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arcd">ARCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/labr">LABR</pwcdataset>
    </paper>
    <paper id="7">
      <title><fixed-case>A</fixed-case>rabic Emoji Sentiment Lexicon (<fixed-case>A</fixed-case>rab-<fixed-case>ESL</fixed-case>): A Comparison between <fixed-case>A</fixed-case>rabic and <fixed-case>E</fixed-case>uropean Emoji Sentiment Lexicons</title>
      <author><first>Shatha Ali A.</first><last>Hakami</last></author>
      <author><first>Robert</first><last>Hendley</last></author>
      <author><first>Phillip</first><last>Smith</last></author>
      <pages>60–71</pages>
      <abstract>Emoji (the popular digital pictograms) are sometimes seen as a new kind of artificial and universally usable and consistent writing code. In spite of their assumed universality, there is some evidence that the sense of an emoji, specifically in regard to sentiment, may change from language to language and culture to culture. This paper investigates whether contextual emoji sentiment analysis is consistent across Arabic and European languages. To conduct this investigation, we, first, created the Arabic emoji sentiment lexicon (Arab-ESL). Then, we exploited an existing European emoji sentiment lexicon to compare the sentiment conveyed in each of the two families of language and culture (Arabic and European). The results show that the pairwise correlation between the two lexicons is consistent for emoji that represent, for instance, hearts, facial expressions, and body language. However, for a subset of emoji (those that represent objects, nature, symbols, and some human activities), there are large differences in the sentiment conveyed. More interestingly, an extremely high level of inconsistency has been shown with food emoji.</abstract>
      <url hash="29073b20">2021.wanlp-1.7</url>
      <bibkey>hakami-etal-2021-arabic</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>A</fixed-case>r<fixed-case>COV</fixed-case>19-Rumors: <fixed-case>A</fixed-case>rabic <fixed-case>COVID</fixed-case>-19 <fixed-case>T</fixed-case>witter Dataset for Misinformation Detection</title>
      <author><first>Fatima</first><last>Haouari</last></author>
      <author><first>Maram</first><last>Hasanain</last></author>
      <author><first>Reem</first><last>Suwaileh</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <pages>72–81</pages>
      <abstract>In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset for misinformation detection composed of tweets containing claims from 27th January till the end of April 2020. We collected 138 verified claims, mostly from popular fact-checking websites, and identified 9.4K relevant tweets to those claims. Tweets were manually-annotated by veracity to support research on misinformation detection, which is one of the major problems faced during a pandemic. ArCOV19-Rumors supports two levels of misinformation detection over Twitter: verifying free-text claims (called claim-level verification) and verifying claims expressed in tweets (called tweet-level verification). Our dataset covers, in addition to health, claims related to other topical categories that were influenced by COVID-19, namely, social, politics, sports, entertainment, and religious. Moreover, we present benchmarking results for tweet-level verification on the dataset. We experimented with SOTA models of versatile approaches that either exploit content, user profiles features, temporal features and propagation structure of the conversational threads for tweet verification.</abstract>
      <url hash="508bd53f">2021.wanlp-1.8</url>
      <bibkey>haouari-etal-2021-arcov19</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arcov19-rumors">ArCOV19-Rumors</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arcov-19">ArCOV-19</pwcdataset>
    </paper>
    <paper id="9">
      <title><fixed-case>A</fixed-case>r<fixed-case>COV</fixed-case>-19: The First <fixed-case>A</fixed-case>rabic <fixed-case>COVID</fixed-case>-19 <fixed-case>T</fixed-case>witter Dataset with Propagation Networks</title>
      <author><first>Fatima</first><last>Haouari</last></author>
      <author><first>Maram</first><last>Hasanain</last></author>
      <author><first>Reem</first><last>Suwaileh</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <pages>82–91</pages>
      <abstract>In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that spans one year, covering the period from 27th of January 2020 till 31st of January 2021. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes about 2.7M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and -liked). The propagation networks include both retweetsand conversational threads (i.e., threads of replies). ArCOV-19 is designed to enable research under several domains including natural language processing, information retrieval, and social computing. Preliminary analysis shows that ArCOV-19 captures rising discussions associated with the first reported cases of the disease as they appeared in the Arab world. In addition to the source tweets and the propagation networks, we also release the search queries and the language-independent crawler used to collect the tweets to encourage the curation of similar datasets.</abstract>
      <url hash="00cd32be">2021.wanlp-1.9</url>
      <bibkey>haouari-etal-2021-arcov</bibkey>
      <pwccode url="https://gitlab.com/bigirqu/ArCOV-19" additional="false">bigirqu/ArCOV-19</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arcov-19">ArCOV-19</pwcdataset>
    </paper>
    <paper id="10">
      <title>The Interplay of Variant, Size, and Task Type in <fixed-case>A</fixed-case>rabic Pre-trained Language Models</title>
      <author><first>Go</first><last>Inoue</last></author>
      <author><first>Bashar</first><last>Alhafni</last></author>
      <author><first>Nurpeiis</first><last>Baimukan</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>92–104</pages>
      <abstract>In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.</abstract>
      <url hash="ea853137">2021.wanlp-1.10</url>
      <bibkey>inoue-etal-2021-interplay</bibkey>
      <revision id="1" href="2021.wanlp-1.10v1" hash="e6d34c34"/>
      <revision id="2" href="2021.wanlp-1.10v2" hash="ea853137" date="2021-09-21">updated results according to code bug fix</revision>
      <pwccode url="https://github.com/CAMeL-Lab/CAMeLBERT" additional="false">CAMeL-Lab/CAMeLBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
    </paper>
    <paper id="11">
      <title>Automatic Difficulty Classification of <fixed-case>A</fixed-case>rabic Sentences</title>
      <author><first>Nouran</first><last>Khallaf</last></author>
      <author><first>Serge</first><last>Sharoff</last></author>
      <pages>105–114</pages>
      <abstract>In this paper, we present a Modern Standard Arabic (MSA) Sentence difficulty classifier, which predicts the difficulty of sentences for language learners using either the CEFR proficiency levels or the binary classification as simple or complex. We compare the use of sentence embeddings of different kinds (fastText, mBERT , XLM-R and Arabic-BERT), as well as traditional language features such as POS tags, dependency trees, readability scores and frequency lists for language learners. Our best results have been achieved using fined-tuned Arabic-BERT. The accuracy of our 3-way CEFR classification is F-1 of 0.80 and 0.75 for Arabic-Bert and XLM-R classification respectively and 0.71 Spearman correlation for regression. Our binary difficulty classifier reaches F-1 0.94 and F-1 0.98 for sentence-pair semantic similarity classifier.</abstract>
      <url hash="1dcb9e22">2021.wanlp-1.11</url>
      <bibkey>khallaf-sharoff-2021-automatic</bibkey>
    </paper>
    <paper id="12">
      <title>Dynamic Ensembles in Named Entity Recognition for Historical <fixed-case>A</fixed-case>rabic Texts</title>
      <author><first>Muhammad</first><last>Majadly</last></author>
      <author><first>Tomer</first><last>Sagi</last></author>
      <pages>115–125</pages>
      <abstract>The use of Named Entity Recognition (NER) over archaic Arabic texts is steadily increasing. However, most tools have been either developed for modern English or trained over English language documents and are limited over historical Arabic text. Even Arabic NER tools are often trained on modern web-sourced text, making their fit for a historical task questionable. To mitigate historic Arabic NER resource scarcity, we propose a dynamic ensemble model utilizing several learners. The dynamic aspect is achieved by utilizing predictors and features over NER algorithm results that identify which have performed better on a specific task in real-time. We evaluate our approach against state-of-the-art Arabic NER and static ensemble methods over a novel historical Arabic NER task we have created. Our results show that our approach improves upon the state-of-the-art and reaches a 0.8 F-score on this challenging task.</abstract>
      <url hash="910ad0b2">2021.wanlp-1.12</url>
      <bibkey>majadly-sagi-2021-dynamic</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>A</fixed-case>rabic Offensive Language on <fixed-case>T</fixed-case>witter: Analysis and Experiments</title>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Ammar</first><last>Rashed</last></author>
      <author><first>Kareem</first><last>Darwish</last></author>
      <author><first>Younes</first><last>Samih</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <pages>126–135</pages>
      <abstract>Detecting offensive language on Twitter has many applications ranging from detecting/predicting bullying to measuring polarization. In this paper, we focus on building a large Arabic offensive tweet dataset. We introduce a method for building a dataset that is not biased by topic, dialect, or target. We produce the largest Arabic dataset to date with special tags for vulgarity and hate speech. We thoroughly analyze the dataset to determine which topics, dialects, and gender are most associated with offensive tweets and how Arabic speakers useoffensive language. Lastly, we conduct many experiments to produce strong results (F1 =83.2) on the dataset using SOTA techniques.</abstract>
      <url hash="9f47ef77">2021.wanlp-1.13</url>
      <bibkey>mubarak-etal-2021-arabic</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="14">
      <title>Adult Content Detection on <fixed-case>A</fixed-case>rabic <fixed-case>T</fixed-case>witter: Analysis and Experiments</title>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <author><first>Ahmed</first><last>Abdelali</last></author>
      <pages>136–144</pages>
      <abstract>With Twitter being one of the most popular social media platforms in the Arab region, it is not surprising to find accounts that post adult content in Arabic tweets; despite the fact that these platforms dissuade users from such content. In this paper, we present a dataset of Twitter accounts that post adult content. We perform an in-depth analysis of the nature of this data and contrast it with normal tweet content. Additionally, we present extensive experiments with traditional machine learning models, deep neural networks and contextual embeddings to identify such accounts. We show that from user information alone, we can identify such accounts with F1 score of 94.7% (macro average). With the addition of only one tweet as input, the F1 score rises to 96.8%.</abstract>
      <url hash="a8b2920a">2021.wanlp-1.14</url>
      <bibkey>mubarak-etal-2021-adult</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>UL</fixed-case>2<fixed-case>C</fixed-case>: Mapping User Locations to Countries on <fixed-case>A</fixed-case>rabic <fixed-case>T</fixed-case>witter</title>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Sabit</first><last>Hassan</last></author>
      <pages>145–153</pages>
      <abstract>Mapping user locations to countries can be useful for many applications such as dialect identification, author profiling, recommendation system, etc. Twitter allows users to declare their locations as free text, and these user-declared locations are often noisy and hard to decipher automatically. In this paper, we present the largest manually labeled dataset for mapping user locations on Arabic Twitter to their corresponding countries. We build effective machine learning models that can automate this mapping with significantly better efficiency compared to libraries such as geopy. We also show that our dataset is more effective than data extracted from GeoNames geographical database in this task as the latter covers only locations written in formal ways.</abstract>
      <url hash="3b228bd6">2021.wanlp-1.15</url>
      <bibkey>mubarak-hassan-2021-ul2c</bibkey>
    </paper>
    <paper id="16">
      <title>Let-Mi: An <fixed-case>A</fixed-case>rabic <fixed-case>L</fixed-case>evantine <fixed-case>T</fixed-case>witter Dataset for Misogynistic Language</title>
      <author><first>Hala</first><last>Mulki</last></author>
      <author><first>Bilal</first><last>Ghanem</last></author>
      <pages>154–163</pages>
      <abstract>Online misogyny has become an increasing worry for Arab women who experience gender-based online abuse on a daily basis. Misogyny automatic detection systems can assist in the prohibition of anti-women Arabic toxic content. Developing such systems is hindered by the lack of the Arabic misogyny benchmark datasets. In this paper, we introduce an Arabic Levantine Twitter dataset for Misogynistic language (LeT-Mi) to be the first benchmark dataset for Arabic misogyny. We further provide a detailed review of the dataset creation and annotation phases. The consistency of the annotations for the proposed dataset was emphasized through inter-rater agreement evaluation measures. Moreover, Let-Mi was used as an evaluation dataset through binary/multi-/target classification tasks conducted by several state-of-the-art machine learning systems along with Multi-Task Learning (MTL) configuration. The obtained results indicated that the performances achieved by the used systems are consistent with state-of-the-art results for languages other than Arabic, while employing MTL improved the performance of the misogyny/target classification tasks.</abstract>
      <url hash="f933dfff">2021.wanlp-1.16</url>
      <bibkey>mulki-ghanem-2021-mi</bibkey>
      <pwccode url="https://github.com/bilalghanem/let-mi" additional="false">bilalghanem/let-mi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/let-mi">LeT-Mi</pwcdataset>
    </paper>
    <paper id="17">
      <title>Empathetic <fixed-case>BERT</fixed-case>2<fixed-case>BERT</fixed-case> Conversational Model: Learning <fixed-case>A</fixed-case>rabic Language Generation with Little Data</title>
      <author><first>Tarek</first><last>Naous</last></author>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Reem</first><last>Mahmoud</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>164–172</pages>
      <abstract>Enabling empathetic behavior in Arabic dialogue agents is an important aspect of building human-like conversational models. While Arabic Natural Language Processing has seen significant advances in Natural Language Understanding (NLU) with language models such as AraBERT, Natural Language Generation (NLG) remains a challenge. The shortcomings of NLG encoder-decoder models are primarily due to the lack of Arabic datasets suitable to train NLG models such as conversational agents. To overcome this issue, we propose a transformer-based encoder-decoder initialized with AraBERT parameters. By initializing the weights of the encoder and decoder with AraBERT pre-trained weights, our model was able to leverage knowledge transfer and boost performance in response generation. To enable empathy in our conversational model, we train it using the ArabicEmpatheticDialogues dataset and achieve high performance in empathetic response generation. Specifically, our model achieved a low perplexity value of 17.0 and an increase in 5 BLEU points compared to the previous state-of-the-art model. Also, our proposed model was rated highly by 85 human evaluators, validating its high capability in exhibiting empathy while generating relevant and fluent responses in open-domain settings.</abstract>
      <url hash="8fd6d3f9">2021.wanlp-1.17</url>
      <bibkey>naous-etal-2021-empathetic</bibkey>
      <pwccode url="https://github.com/aub-mind/Arabic-Empathetic-Chatbot" additional="false">aub-mind/Arabic-Empathetic-Chatbot</pwccode>
    </paper>
    <paper id="18">
      <title><fixed-case>ALUE</fixed-case>: <fixed-case>A</fixed-case>rabic Language Understanding Evaluation</title>
      <author><first>Haitham</first><last>Seelawi</last></author>
      <author><first>Ibraheem</first><last>Tuffaha</last></author>
      <author><first>Mahmoud</first><last>Gzawi</last></author>
      <author><first>Wael</first><last>Farhan</last></author>
      <author><first>Bashar</first><last>Talafha</last></author>
      <author><first>Riham</first><last>Badawi</last></author>
      <author><first>Zyad</first><last>Sober</last></author>
      <author><first>Oday</first><last>Al-Dweik</last></author>
      <author><first>Abed Alhakim</first><last>Freihat</last></author>
      <author><first>Hussein</first><last>Al-Natsheh</last></author>
      <pages>173–184</pages>
      <abstract>The emergence of Multi-task learning (MTL)models in recent years has helped push thestate of the art in Natural Language Un-derstanding (NLU). We strongly believe thatmany NLU problems in Arabic are especiallypoised to reap the benefits of such models. Tothis end we propose the Arabic Language Un-derstanding Evaluation Benchmark (ALUE),based on 8 carefully selected and previouslypublished tasks. For five of these, we providenew privately held evaluation datasets to en-sure the fairness and validity of our benchmark. We also provide a diagnostic dataset to helpresearchers probe the inner workings of theirmodels.Our initial experiments show thatMTL models outperform their singly trainedcounterparts on most tasks. But in order to en-tice participation from the wider community,we stick to publishing singly trained baselinesonly. Nonetheless, our analysis reveals thatthere is plenty of room for improvement inArabic NLU. We hope that ALUE will playa part in helping our community realize someof these improvements. Interested researchersare invited to submit their results to our online,and publicly accessible leaderboard.</abstract>
      <url hash="7089ea37">2021.wanlp-1.18</url>
      <bibkey>seelawi-etal-2021-alue</bibkey>
      <pwccode url="https://github.com/Alue-Benchmark/alue_baselines" additional="false">Alue-Benchmark/alue_baselines</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-question-similarity-in-arabic">Semantic Question Similarity in Arabic</pwcdataset>
    </paper>
    <paper id="19">
      <title>Quranic Verses Semantic Relatedness Using <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case></title>
      <author><first>Abdullah</first><last>Alsaleh</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <author><first>Abdulrahman</first><last>Altahhan</last></author>
      <pages>185–190</pages>
      <abstract>Bidirectional Encoder Representations from Transformers (BERT) has gained popularity in recent years producing state-of-the-art performances across Natural Language Processing tasks. In this paper, we used AraBERT language model to classify pairs of verses provided by the QurSim dataset to either be semantically related or not. We have pre-processed The QurSim dataset and formed three datasets for comparisons. Also, we have used both versions of AraBERT, which are AraBERTv02 and AraBERTv2, to recognise which version performs the best with the given datasets. The best results was AraBERTv02 with 92% accuracy score using a dataset comprised of label ‘2’ and label '-1’, the latter was generated outside of QurSim dataset.</abstract>
      <url hash="1f3118ee">2021.wanlp-1.19</url>
      <bibkey>alsaleh-etal-2021-quranic</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>A</fixed-case>ra<fixed-case>ELECTRA</fixed-case>: Pre-Training Text Discriminators for <fixed-case>A</fixed-case>rabic Language Understanding</title>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>191–195</pages>
      <abstract>Advances in English language representation enabled a more sample-efficient pre-training task by Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA). Which, instead of training a model to recover masked tokens, it trains a discriminator model to distinguish true input tokens from corrupted tokens that were replaced by a generator network. On the other hand, current Arabic language representation approaches rely only on pretraining via masked language modeling. In this paper, we develop an Arabic language representation model, which we name AraELECTRA. Our model is pretrained using the replaced token detection objective on large Arabic text corpora. We evaluate our model on multiple Arabic NLP tasks, including reading comprehension, sentiment analysis, and named-entity recognition and we show that AraELECTRA outperforms current state-of-the-art Arabic language representation models, given the same pretraining data and with even a smaller model size.</abstract>
      <url hash="b1925ff0">2021.wanlp-1.20</url>
      <bibkey>antoun-etal-2021-araelectra</bibkey>
      <pwccode url="https://github.com/aub-mind/araBERT" additional="false">aub-mind/araBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arcd">ARCD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsentd-lev">ArSentD-LEV</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDiQA</pwcdataset>
    </paper>
    <paper id="21">
      <title><fixed-case>A</fixed-case>ra<fixed-case>GPT</fixed-case>2: Pre-Trained Transformer for <fixed-case>A</fixed-case>rabic Language Generation</title>
      <author><first>Wissam</first><last>Antoun</last></author>
      <author><first>Fady</first><last>Baly</last></author>
      <author><first>Hazem</first><last>Hajj</last></author>
      <pages>196–207</pages>
      <abstract>Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.</abstract>
      <url hash="1c4d395f">2021.wanlp-1.21</url>
      <bibkey>antoun-etal-2021-aragpt2</bibkey>
      <pwccode url="https://github.com/aub-mind/araBERT" additional="false">aub-mind/araBERT</pwccode>
    </paper>
    <paper id="22">
      <title><fixed-case>Q</fixed-case>uran<fixed-case>T</fixed-case>ree.jl: A <fixed-case>J</fixed-case>ulia Package for <fixed-case>Q</fixed-case>uranic <fixed-case>A</fixed-case>rabic Corpus</title>
      <author><first>Al-Ahmadgaid</first><last>Asaad</last></author>
      <pages>208–212</pages>
      <abstract>QuranTree.jl is an open-source package for working with the Quranic Arabic Corpus (Dukes and Habash, 2010). It aims to provide Julia APIs as an alternative to the Java APIs of JQuranTree. QuranTree.jl currently offers functionalities for intuitive indexing of chapters, verses, words and parts of words of the Qur’an; for creating custom transliteration; for character dediacritization and normalization; and, for handling the morphological features. Lastly, it can work well with Julia’s TextAnalysis.jl and Python’s CAMeL Tools.</abstract>
      <url hash="13100d5a">2021.wanlp-1.22</url>
      <bibkey>asaad-2021-qurantree</bibkey>
    </paper>
    <paper id="23">
      <title>Automatic <fixed-case>R</fixed-case>omanization of <fixed-case>A</fixed-case>rabic Bibliographic Records</title>
      <author><first>Fadhl</first><last>Eryani</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>213–218</pages>
      <abstract>International library standards require cataloguers to tediously input Romanization of their catalogue records for the benefit of library users without specific language expertise. In this paper, we present the first reported results on the task of automatic Romanization of <i>undiacritized</i> Arabic bibliographic entries. This complex task requires the modeling of Arabic phonology, morphology, and even semantics. We collected a 2.5M word corpus of parallel Arabic and Romanized bibliographic entries, and benchmarked a number of models that vary in terms of complexity and resource dependence. Our best system reaches 89.3% exact word Romanization on a blind test set. We make our data and code publicly available.</abstract>
      <url hash="64f0a70b">2021.wanlp-1.23</url>
      <bibkey>eryani-habash-2021-automatic</bibkey>
      <pwccode url="https://github.com/CAMeL-Lab/Arabic_ALA-LC_Romanization" additional="false">CAMeL-Lab/Arabic_ALA-LC_Romanization</pwccode>
    </paper>
    <paper id="24">
      <title><fixed-case>SERAG</fixed-case>: Semantic Entity Retrieval from <fixed-case>A</fixed-case>rabic Knowledge Graphs</title>
      <author><first>Saher</first><last>Esmeir</last></author>
      <pages>219–225</pages>
      <abstract>Knowledge graphs (KGs) are widely used to store and access information about entities and their relationships. Given a query, the task of entity retrieval from a KG aims at presenting a ranked list of entities relevant to the query. Lately, an increasing number of models for entity retrieval have shown a significant improvement over traditional methods. These models, however, were developed for English KGs. In this work, we build on one such system, named KEWER, to propose SERAG (Semantic Entity Retrieval from Arabic knowledge Graphs). Like KEWER, SERAG uses random walks to generate entity embeddings. DBpedia-Entity v2 is considered the standard test collection for entity retrieval. We discuss the challenges of using it for non-English languages in general and Arabic in particular. We provide an Arabic version of this standard collection, and use it to evaluate SERAG. SERAG is shown to significantly outperform the popular BM25 model thanks to its multi-hop reasoning.</abstract>
      <url hash="4e1cea8e">2021.wanlp-1.24</url>
      <bibkey>esmeir-2021-serag</bibkey>
    </paper>
    <paper id="25">
      <title>Introducing A large <fixed-case>T</fixed-case>unisian <fixed-case>A</fixed-case>rabizi Dialectal Dataset for Sentiment Analysis</title>
      <author><first>Chayma</first><last>Fourati</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <author><first>Abir</first><last>Messaoudi</last></author>
      <author><first>Moez</first><last>BenHajhmida</last></author>
      <author><first>Aymen</first><last>Ben Elhaj Mabrouk</last></author>
      <author><first>Malek</first><last>Naski</last></author>
      <pages>226–230</pages>
      <abstract>On various Social Media platforms, people, tend to use the informal way to communicate, or write posts and comments: their local dialects. In Africa, more than 1500 dialects and languages exist. Particularly, Tunisians talk and write informally using Latin letters and numbers rather than Arabic ones. In this paper, we introduce a large common-crawl-based Tunisian Arabizi dialectal dataset dedicated for Sentiment Analysis. The dataset consists of a total of 100k comments (about movies, politic, sport, etc.) annotated manually by Tunisian native speakers as Positive, negative and Neutral. We evaluate our dataset on sentiment analysis task using the Bidirectional Encoder Representations from Transformers (BERT) as a contextual language model in its multilingual version (mBERT) as an embedding technique then combining mBERT with Convolutional Neural Network (CNN) as classifier. The dataset is publicly available.</abstract>
      <url hash="80118069">2021.wanlp-1.25</url>
      <bibkey>fourati-etal-2021-introducing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/tsac">TSAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tunizi">TUNIZI</pwcdataset>
    </paper>
    <paper id="26">
      <title><fixed-case>A</fixed-case>ra<fixed-case>F</fixed-case>acts: The First Large <fixed-case>A</fixed-case>rabic Dataset of Naturally Occurring Claims</title>
      <author><first>Zien</first><last>Sheikh Ali</last></author>
      <author><first>Watheq</first><last>Mansour</last></author>
      <author><first>Tamer</first><last>Elsayed</last></author>
      <author><first>Abdulaziz</first><last>Al‐Ali</last></author>
      <pages>231–236</pages>
      <abstract>We introduce AraFacts, the first large Arabic dataset of naturally occurring claims collected from 5 Arabic fact-checking websites, e.g., Fatabyyano and Misbar, and covering claims since 2016. Our dataset consists of 6,121 claims along with their factual labels and additional metadata, such as fact-checking article content, topical category, and links to posts or Web pages spreading the claim. Since the data is obtained from various fact-checking websites, we standardize the original claim labels to provide a unified label rating for all claims. Moreover, we provide revealing dataset statistics and motivate its use by suggesting possible research applications. The dataset is made publicly available for the research community.</abstract>
      <url hash="08dd3237">2021.wanlp-1.26</url>
      <bibkey>sheikh-ali-etal-2021-arafacts</bibkey>
      <pwccode url="https://gitlab.com/bigirqu/arafacts" additional="false">bigirqu/arafacts</pwccode>
    </paper>
    <paper id="27">
      <title>Improving Cross-Lingual Transfer for Event Argument Extraction with Language-Universal Sentence Structures</title>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>237–243</pages>
      <abstract>We study the problem of Cross-lingual Event Argument Extraction (CEAE). The task aims to predict argument roles of entity mentions for events in text, whose language is different from the language that a predictive model has been trained on. Previous work on CEAE has shown the cross-lingual benefits of universal dependency trees in capturing shared syntactic structures of sentences across languages. In particular, this work exploits the existence of the syntactic connections between the words in the dependency trees as the anchor knowledge to transfer the representation learning across languages for CEAE models (i.e., via graph convolutional neural networks – GCNs). In this paper, we introduce two novel sources of language-independent information for CEAE models based on the semantic similarity and the universal dependency relations of the word pairs in different languages. We propose to use the two sources of information to produce shared sentence structures to bridge the gap between languages and improve the cross-lingual performance of the CEAE models. Extensive experiments are conducted with Arabic, Chinese, and English to demonstrate the effectiveness of the proposed method for CEAE.</abstract>
      <url hash="16e675d6">2021.wanlp-1.27</url>
      <bibkey>nguyen-nguyen-2021-improving</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>NADI</fixed-case> 2021: The Second Nuanced <fixed-case>A</fixed-case>rabic Dialect Identification Shared Task</title>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Chiyu</first><last>Zhang</last></author>
      <author><first>AbdelRahim</first><last>Elmadany</last></author>
      <author><first>Houda</first><last>Bouamor</last></author>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>244–259</pages>
      <abstract>We present the findings and results of theSecond Nuanced Arabic Dialect IdentificationShared Task (NADI 2021). This Shared Taskincludes four subtasks: country-level ModernStandard Arabic (MSA) identification (Subtask1.1), country-level dialect identification (Subtask1.2), province-level MSA identification (Subtask2.1), and province-level sub-dialect identifica-tion (Subtask 2.2). The shared task dataset cov-ers a total of 100 provinces from 21 Arab coun-tries, collected from the Twitter domain. A totalof 53 teams from 23 countries registered to par-ticipate in the tasks, thus reflecting the interestof the community in this area. We received 16submissions for Subtask 1.1 from five teams, 27submissions for Subtask 1.2 from eight teams,12 submissions for Subtask 2.1 from four teams,and 13 Submissions for subtask 2.2 from fourteams.</abstract>
      <url hash="7eab317e">2021.wanlp-1.28</url>
      <bibkey>abdul-mageed-etal-2021-nadi</bibkey>
      <pwccode url="https://github.com/UBC-NLP/nadi" additional="false">UBC-NLP/nadi</pwccode>
    </paper>
    <paper id="29">
      <title>Adapting <fixed-case>MARBERT</fixed-case> for Improved <fixed-case>A</fixed-case>rabic Dialect Identification: Submission to the <fixed-case>NADI</fixed-case> 2021 Shared Task</title>
      <author><first>Badr</first><last>AlKhamissi</last></author>
      <author><first>Mohamed</first><last>Gabr</last></author>
      <author><first>Muhammad</first><last>ElNokrashy</last></author>
      <author><first>Khaled</first><last>Essam</last></author>
      <pages>260–264</pages>
      <abstract>In this paper, we tackle the Nuanced Arabic Dialect Identification (NADI) shared task (Abdul-Mageed et al., 2021) and demonstrate state-of-the-art results on all of its four subtasks. Tasks are to identify the geographic origin of short Dialectal (DA) and Modern Standard Arabic (MSA) utterances at the levels of both country and province. Our final model is an ensemble of variants built on top of MARBERT that achieves an F1-score of 34.03% for DA at the country-level development set—an improvement of 7.63% from previous work.</abstract>
      <url hash="dc5e492e">2021.wanlp-1.29</url>
      <bibkey>alkhamissi-etal-2021-adapting</bibkey>
      <pwccode url="https://github.com/mohamedgabr96/NeuralDialectDetector" additional="false">mohamedgabr96/NeuralDialectDetector</pwccode>
    </paper>
    <paper id="30">
      <title>Country-level <fixed-case>A</fixed-case>rabic Dialect Identification Using Small Datasets with Integrated Machine Learning Techniques and Deep Learning Models</title>
      <author><first>Maha J.</first><last>Althobaiti</last></author>
      <pages>265–270</pages>
      <abstract>Arabic is characterised by a considerable number of varieties including spoken dialects. In this paper, we presented our models developed to participate in the NADI subtask 1.2 that requires building a system to distinguish between 21 country-level dialects. We investigated several classical machine learning approaches and deep learning models using small datasets. We examined an integration technique between two machine learning approaches. Additionally, we created dictionaries automatically based on Pointwise Mutual Information and labelled datasets, which enriched the feature space when training models. A semi-supervised learning approach was also examined and compared to other methods that exploit large unlabelled datasets, such as building pre-trained word embeddings. Our winning model was the Support Vector Machine with dictionary-based features and Pointwise Mutual Information values, achieving an 18.94% macros-average F1-score.</abstract>
      <url hash="169b577e">2021.wanlp-1.30</url>
      <bibkey>althobaiti-2021-country</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>BERT</fixed-case>-based Multi-Task Model for Country and Province Level <fixed-case>MSA</fixed-case> and Dialectal <fixed-case>A</fixed-case>rabic Identification</title>
      <author><first>Abdellah</first><last>El Mekki</last></author>
      <author><first>Abdelkader</first><last>El Mahdaouy</last></author>
      <author><first>Kabil</first><last>Essefar</last></author>
      <author><first>Nabil</first><last>El Mamoun</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <author><first>Ahmed</first><last>Khoumsi</last></author>
      <pages>271–275</pages>
      <abstract>Dialect and standard language identification are crucial tasks for many Arabic natural language processing applications. In this paper, we present our deep learning-based system, submitted to the second NADI shared task for country-level and province-level identification of Modern Standard Arabic (MSA) and Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task Learning (MTL) model to tackle both country-level and province-level MSA/DA identification. The latter MTL model consists of a shared Bidirectional Encoder Representation Transformers (BERT) encoder, two task-specific attention layers, and two classifiers. Our key idea is to leverage both the task-discriminative and the inter-task shared features for country and province MSA/DA identification. The obtained results show that our MTL model outperforms single-task models on most subtasks.</abstract>
      <url hash="9776ec6e">2021.wanlp-1.31</url>
      <bibkey>el-mekki-etal-2021-bert</bibkey>
    </paper>
    <paper id="32">
      <title>Country-level <fixed-case>A</fixed-case>rabic Dialect Identification using <fixed-case>RNN</fixed-case>s with and without Linguistic Features</title>
      <author><first>Elsayed</first><last>Issa</last></author>
      <author><first>Mohammed</first><last>AlShakhori1</last></author>
      <author><first>Reda</first><last>Al-Bahrani</last></author>
      <author><first>Gus</first><last>Hahn-Powell</last></author>
      <pages>276–281</pages>
      <abstract>This work investigates the value of augmenting recurrent neural networks with feature engineering for the Second Nuanced Arabic Dialect Identification (NADI) Subtask 1.2: Country-level DA identification. We compare the performance of a simple word-level LSTM using pretrained embeddings with one enhanced using feature embeddings for engineered linguistic features. Our results show that the addition of explicit features to the LSTM is detrimental to performance. We attribute this performance loss to the bivalency of some linguistic items in some text, ubiquity of topics, and participant mobility.</abstract>
      <url hash="56b4a547">2021.wanlp-1.32</url>
      <bibkey>issa-etal-2021-country</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>A</fixed-case>rabic Dialect Identification based on a Weighted Concatenation of <fixed-case>TF</fixed-case>-<fixed-case>IDF</fixed-case> Features</title>
      <author><first>Mohamed</first><last>Lichouri</last></author>
      <author><first>Mourad</first><last>Abbas</last></author>
      <author><first>Khaled</first><last>Lounnas</last></author>
      <author><first>Besma</first><last>Benaziz</last></author>
      <author><first>Aicha</first><last>Zitouni</last></author>
      <pages>282–286</pages>
      <abstract>In this paper, we analyze the impact of the weighted concatenation of TF-IDF features for the Arabic Dialect Identification task while we participated in the NADI2021 shared task. This study is performed for two subtasks: subtask 1.1 (country-level MSA) and subtask 1.2 (country-level DA) identification. The classifiers supporting our comparative study are Linear Support Vector Classification (LSVC), Linear Regression (LR), Perceptron, Stochastic Gradient Descent (SGD), Passive Aggressive (PA), Complement Naive Bayes (CNB), MutliLayer Perceptron (MLP), and RidgeClassifier. In the evaluation phase, our system gives F1 scores of 14.87% and 21.49%, for country-level MSA and DA identification respectively, which is very close to the average F1 scores achieved by the submitted systems and recorded for both subtasks (18.70% and 24.23%).</abstract>
      <url hash="7a48b49f">2021.wanlp-1.33</url>
      <bibkey>lichouri-etal-2021-arabic</bibkey>
    </paper>
    <paper id="34">
      <title>Machine Learning-Based Approach for <fixed-case>A</fixed-case>rabic Dialect Identification</title>
      <author><first>Hamada</first><last>Nayel</last></author>
      <author><first>Ahmed</first><last>Hassan</last></author>
      <author><first>Mahmoud</first><last>Sobhi</last></author>
      <author><first>Ahmed</first><last>El-Sawy</last></author>
      <pages>287–290</pages>
      <abstract>This paper describes our systems submitted to the Second Nuanced Arabic Dialect Identification Shared Task (NADI 2021). Dialect identification is the task of automatically detecting the source variety of a given text or speech segment. There are four subtasks, two subtasks for country-level identification and the other two subtasks for province-level identification. The data in this task covers a total of 100 provinces from all 21 Arab countries and come from the Twitter domain. The proposed systems depend on five machine-learning approaches namely Complement Naïve Bayes, Support Vector Machine, Decision Tree, Logistic Regression and Random Forest Classifiers. F1 macro-averaged score of Naïve Bayes classifier outperformed all other classifiers for development and test data.</abstract>
      <url hash="00520d27">2021.wanlp-1.34</url>
      <bibkey>nayel-etal-2021-machine</bibkey>
    </paper>
    <paper id="35">
      <title>Dialect Identification in Nuanced <fixed-case>A</fixed-case>rabic Tweets Using Farasa Segmentation and <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case></title>
      <author><first>Anshul</first><last>Wadhawan</last></author>
      <pages>291–295</pages>
      <abstract>This paper presents our approach to address the EACL WANLP-2021 Shared Task 1: Nuanced Arabic Dialect Identification (NADI). The task is aimed at developing a system that identifies the geographical location(country/province) from where an Arabic tweet in the form of modern standard Arabic or dialect comes from. We solve the task in two parts. The first part involves pre-processing the provided dataset by cleaning, adding and segmenting various parts of the text. This is followed by carrying out experiments with different versions of two Transformer based models, AraBERT and AraELECTRA. Our final approach achieved macro F1-scores of 0.216, 0.235, 0.054, and 0.043 in the four subtasks, and we were ranked second in MSA identification subtasks and fourth in DA identification subtasks.</abstract>
      <url hash="b1fb2962">2021.wanlp-1.35</url>
      <bibkey>wadhawan-2021-dialect</bibkey>
    </paper>
    <paper id="36">
      <title>Overview of the <fixed-case>WANLP</fixed-case> 2021 Shared Task on Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Ibrahim</first><last>Abu Farha</last></author>
      <author><first>Wajdi</first><last>Zaghouani</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>296–305</pages>
      <abstract>This paper provides an overview of the WANLP 2021 shared task on sarcasm and sentiment detection in Arabic. The shared task has two subtasks: sarcasm detection (subtask 1) and sentiment analysis (subtask 2). This shared task aims to promote and bring attention to Arabic sarcasm detection, which is crucial to improve the performance in other tasks such as sentiment analysis. The dataset used in this shared task, namely ArSarcasm-v2, consists of 15,548 tweets labelled for sarcasm, sentiment and dialect. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 F1-score and 0.748 F1-PN respectively.</abstract>
      <url hash="b17dc947">2021.wanlp-1.36</url>
      <bibkey>abu-farha-etal-2021-overview</bibkey>
      <pwccode url="https://github.com/iabufarha/arsarcasm-v2" additional="false">iabufarha/arsarcasm-v2</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm">ArSarcasm</pwcdataset>
    </paper>
    <paper id="37">
      <title><fixed-case>WANLP</fixed-case> 2021 Shared-Task: Towards Irony and Sentiment Detection in <fixed-case>A</fixed-case>rabic Tweets using Multi-headed-<fixed-case>LSTM</fixed-case>-<fixed-case>CNN</fixed-case>-<fixed-case>GRU</fixed-case> and <fixed-case>M</fixed-case>a<fixed-case>RBERT</fixed-case></title>
      <author><first>Reem</first><last>Abdel-Salam</last></author>
      <pages>306–311</pages>
      <abstract>Irony and Sentiment detection is important to understand people’s behavior and thoughts. Thus it has become a popular task in natural language processing (NLP). This paper presents results and main findings in WANLP 2021 shared tasks one and two. The task was based on the ArSarcasm-v2 dataset (Abu Farha et al., 2021). In this paper, we describe our system Multi-headed-LSTM-CNN-GRU and also MARBERT (Abdul-Mageed et al., 2021) submitted for the shared task, ranked 10 out of 27 in shared task one achieving 0.5662 F1-Sarcasm and ranked 3 out of 22 in shared task two achieving 0.7321 F1-PN under CodaLab username “rematchka”. We experimented with various models and the two best performing models are a Multi-headed CNN-LSTM-GRU in which we used prepossessed text and emoji presented from tweets and MARBERT.</abstract>
      <url hash="58610b7b">2021.wanlp-1.37</url>
      <bibkey>abdel-salam-2021-wanlp</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
    </paper>
    <paper id="38">
      <title>Sarcasm and Sentiment Detection In <fixed-case>A</fixed-case>rabic Tweets Using <fixed-case>BERT</fixed-case>-based Models and Data Augmentation</title>
      <author><first>Abeer</first><last>Abuzayed</last></author>
      <author><first>Hend</first><last>Al-Khalifa</last></author>
      <pages>312–317</pages>
      <abstract>In this paper, we describe our efforts on the shared task of sarcasm and sentiment detection in Arabic (Abu Farha et al., 2021). The shared task consists of two sub-tasks: Sarcasm Detection (Subtask 1) and Sentiment Analysis (Subtask 2). Our experiments were based on fine-tuning seven BERT-based models with data augmentation to solve the imbalanced data problem. For both tasks, the MARBERT BERT-based model with data augmentation outperformed other models with an increase of the F-score by 15% for both tasks which shows the effectiveness of our approach.</abstract>
      <url hash="0d407008">2021.wanlp-1.38</url>
      <bibkey>abuzayed-al-khalifa-2021-sarcasm</bibkey>
    </paper>
    <paper id="39">
      <title>Multi-task Learning Using a Combination of Contextualised and Static Word Embeddings for <fixed-case>A</fixed-case>rabic Sarcasm Detection and Sentiment Analysis</title>
      <author><first>Abdullah I.</first><last>Alharbi</last></author>
      <author><first>Mark</first><last>Lee</last></author>
      <pages>318–322</pages>
      <abstract>Sarcasm detection and sentiment analysis are important tasks in Natural Language Understanding. Sarcasm is a type of expression where the sentiment polarity is flipped by an interfering factor. In this study, we exploited this relationship to enhance both tasks by proposing a multi-task learning approach using a combination of static and contextualised embeddings. Our proposed system achieved the best result in the sarcasm detection subtask.</abstract>
      <url hash="b3a69c39">2021.wanlp-1.39</url>
      <bibkey>alharbi-lee-2021-multi</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>A</fixed-case>r<fixed-case>S</fixed-case>arcasm Shared Task: An Ensemble <fixed-case>BERT</fixed-case> Model for <fixed-case>S</fixed-case>arcasm<fixed-case>D</fixed-case>etection in <fixed-case>A</fixed-case>rabic Tweets</title>
      <author><first>Laila</first><last>Bashmal</last></author>
      <author><first>Daliyah</first><last>AlZeer</last></author>
      <pages>323–328</pages>
      <abstract>Detecting Sarcasm has never been easy for machines to process. In this work, we present our submission of the sub-task1 of the shared task on sarcasm and sentiment detection in Arabic organized by the 6th Workshop for Arabic Natural Language Processing. In this work, we explored different approaches based on BERT models. First, we fine-tuned the AraBERTv02 model for the sarcasm detection task. Then, we used the Sentence-BERT model trained with contrastive learning to extract representative tweet embeddings. Finally, inspired by how the human brain comprehends the surface and the implicit meanings of sarcastic tweets, we combined the sentence embedding with the fine-tuned AraBERTv02 to further boost the performance of the model. Through the ensemble of the two models, our team ranked 5th out of 27 teams on the shared task of sarcasm detection in Arabic, with an F1-score of %59.89 on the official test data. The obtained result is %2.36 lower than the 1st place which confirms the capabilities of the employed combined model in detecting sarcasm.</abstract>
      <url hash="ddf3d376">2021.wanlp-1.40</url>
      <bibkey>bashmal-alzeer-2021-arsarcasm</bibkey>
    </paper>
    <paper id="41">
      <title>Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic: investigating the interest of character-level features</title>
      <author><first>Dhaou</first><last>Ghoul</last></author>
      <author><first>Gaël</first><last>Lejeune</last></author>
      <pages>329–333</pages>
      <abstract>We present three methods developed for the Shared Task on Sarcasm and Sentiment Detection in Arabic. We present a baseline that uses character n-gram features. We also propose two more sophisticated methods: a recurrent neural network with a word level representation and an ensemble classifier relying on word and character-level features. We chose to present results from an ensemble classifier but it was not very successful as compared to the best systems : 22th/37 on sarcasm detection and 15th/22 on sentiment detection. It finally appeared that our baseline could have been improved and beat those results.</abstract>
      <url hash="7ce24f47">2021.wanlp-1.41</url>
      <attachment type="Dataset" hash="f170ae85">2021.wanlp-1.41.Dataset.pdf</attachment>
      <attachment type="Software" hash="595b041b">2021.wanlp-1.41.Software.zip</attachment>
      <bibkey>ghoul-lejeune-2021-sarcasm</bibkey>
    </paper>
    <paper id="42">
      <title>Deep Multi-Task Model for Sarcasm Detection and Sentiment Analysis in <fixed-case>A</fixed-case>rabic Language</title>
      <author><first>Abdelkader</first><last>El Mahdaouy</last></author>
      <author><first>Abdellah</first><last>El Mekki</last></author>
      <author><first>Kabil</first><last>Essefar</last></author>
      <author><first>Nabil</first><last>El Mamoun</last></author>
      <author><first>Ismail</first><last>Berrada</last></author>
      <author><first>Ahmed</first><last>Khoumsi</last></author>
      <pages>334–339</pages>
      <abstract>The prominence of figurative language devices, such as sarcasm and irony, poses serious challenges for Arabic Sentiment Analysis (SA). While previous research works tackle SA and sarcasm detection separately, this paper introduces an end-to-end deep Multi-Task Learning (MTL) model, allowing knowledge interaction between the two tasks. Our MTL model’s architecture consists of a Bidirectional Encoder Representation from Transformers (BERT) model, a multi-task attention interaction module, and two task classifiers. The overall obtained results show that our proposed model outperforms its single-task and MTL counterparts on both sarcasm and sentiment detection subtasks.</abstract>
      <url hash="59195e38">2021.wanlp-1.42</url>
      <bibkey>el-mahdaouy-etal-2021-deep</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
    </paper>
    <paper id="43">
      <title>A Contextual Word Embedding for <fixed-case>A</fixed-case>rabic Sarcasm Detection with Random Forests</title>
      <author><first>Hazem</first><last>Elgabry</last></author>
      <author><first>Shimaa</first><last>Attia</last></author>
      <author><first>Ahmed</first><last>Abdel-Rahman</last></author>
      <author><first>Ahmed</first><last>Abdel-Ate</last></author>
      <author><first>Sandra</first><last>Girgis</last></author>
      <pages>340–344</pages>
      <abstract>Sarcasm detection is of great importance in understanding people’s true sentiments and opinions. Many online feedbacks, reviews, social media comments, etc. are sarcastic. Several researches have already been done in this field, but most researchers studied the English sarcasm analysis compared to the researches are done in Arabic sarcasm analysis because of the Arabic language challenges. In this paper, we propose a new approach for improving Arabic sarcasm detection. Our approach is using data augmentation, contextual word embedding and random forests model to get the best results. Our accuracy in the shared task on sarcasm and sentiment detection in Arabic was 0.5189 for F1-sarcastic as the official metric using the shared dataset ArSarcasmV2 (Abu Farha, et al., 2021).</abstract>
      <url hash="55bdb51e">2021.wanlp-1.43</url>
      <bibkey>elgabry-etal-2021-contextual</bibkey>
    </paper>
    <paper id="44">
      <title><fixed-case>S</fixed-case>arcasm<fixed-case>D</fixed-case>et at Sarcasm Detection Task 2021 in <fixed-case>A</fixed-case>rabic using <fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> Pretrained Model</title>
      <author><first>Dalya</first><last>Faraj</last></author>
      <author><first>Dalya</first><last>Faraj</last></author>
      <author><first>Malak</first><last>Abdullah</last></author>
      <pages>345–350</pages>
      <abstract>This paper presents one of the top five winning solutions for the Shared Task on Sarcasm and Sentiment Detection in Arabic (Subtask-1 Sarcasm Detection). The goal of the task is to identify whether a tweet is sarcastic or not. Our solution has been developed using ensemble technique with AraBERT pre-trained model. We describe the architecture of the submitted solution in the shared task. We also provide the experiments and the hyperparameter tuning that lead to this result. Besides, we discuss and analyze the results by comparing all the models that we trained or tested to achieve a better score in a table design. Our model is ranked fifth out of 27 teams with an F1 score of 0.5985. It is worth mentioning that our model achieved the highest accuracy score of 0.7830</abstract>
      <url hash="6e3bf51c">2021.wanlp-1.44</url>
      <bibkey>faraj-etal-2021-sarcasmdet</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
    </paper>
    <paper id="45">
      <title>Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic language A Hybrid Approach Combining Embeddings and Rule-based Features</title>
      <author><first>Kamel</first><last>Gaanoun</last></author>
      <author><first>Imade</first><last>Benelallam</last></author>
      <pages>351–356</pages>
      <abstract>This paper presents the ArabicProcessors team’s system designed for sarcasm (subtask 1) and sentiment (subtask 2) detection shared task. We created a hybrid system by combining rule-based features and both static and dynamic embeddings using transformers and deep learning. The system’s architecture is an ensemble of Naive bayes, MarBERT and Mazajak embedding. This process scored an F1-score of 51% on sarcasm and 71% for sentiment detection.</abstract>
      <url hash="63112aaf">2021.wanlp-1.45</url>
      <bibkey>gaanoun-benelallam-2021-sarcasm</bibkey>
    </paper>
    <paper id="46">
      <title>Combining Context-Free and Contextualized Representations for <fixed-case>A</fixed-case>rabic Sarcasm Detection and Sentiment Identification</title>
      <author><first>Amey</first><last>Hengle</last></author>
      <author><first>Atharva</first><last>Kshirsagar</last></author>
      <author><first>Shaily</first><last>Desai</last></author>
      <author><first>Manisha</first><last>Marathe</last></author>
      <pages>357–363</pages>
      <abstract>Since their inception, transformer-based language models have led to impressive performance gains across multiple natural language processing tasks. For Arabic, the current state-of-the-art results on most datasets are achieved by the AraBERT language model. Notwithstanding these recent advancements, sarcasm and sentiment detection persist to be challenging tasks in Arabic, given the language’s rich morphology, linguistic disparity and dialectal variations. This paper proffers team SPPU-AASM’s submission for the WANLP ArSarcasm shared-task 2021, which centers around the sarcasm and sentiment polarity detection of Arabic tweets. The study proposes a hybrid model, combining sentence representations from AraBERT with static word vectors trained on Arabic social media corpora. The proposed system achieves a F1-sarcastic score of 0.62 and a F-PN score of 0.715 for the sarcasm and sentiment detection tasks, respectively. Simulation results show that the proposed system outperforms multiple existing approaches for both the tasks, suggesting that the amalgamation of context-free and context-dependent text representations can help capture complementary facets of word meaning in Arabic. The system ranked second and tenth in the respective sub-tasks of sarcasm detection and sentiment identification.</abstract>
      <url hash="079de456">2021.wanlp-1.46</url>
      <bibkey>hengle-etal-2021-combining</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm">ArSarcasm</pwcdataset>
    </paper>
    <paper id="47">
      <title>Leveraging Offensive Language for Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Fatemah</first><last>Husain</last></author>
      <author><first>Ozlem</first><last>Uzuner</last></author>
      <pages>364–369</pages>
      <abstract>Sarcasm detection is one of the top challenging tasks in text classification, particularly for informal Arabic with high syntactic and semantic ambiguity. We propose two systems that harness knowledge from multiple tasks to improve the performance of the classifier. This paper presents the systems used in our participation to the two sub-tasks of the Sixth Arabic Natural Language Processing Workshop (WANLP); Sarcasm Detection and Sentiment Analysis. Our methodology is driven by the hypothesis that tweets with negative sentiment and tweets with sarcasm content are more likely to have offensive content, thus, fine-tuning the classification model using large corpus of offensive language, supports the learning process of the model to effectively detect sentiment and sarcasm contents. Results demonstrate the effectiveness of our approach for sarcasm detection task over sentiment analysis task.</abstract>
      <url hash="91852a85">2021.wanlp-1.47</url>
      <bibkey>husain-uzuner-2021-leveraging</bibkey>
    </paper>
    <paper id="48">
      <title>The <fixed-case>IDC</fixed-case> System for Sentiment Classification and Sarcasm Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Abraham</first><last>Israeli</last></author>
      <author><first>Yotam</first><last>Nahum</last></author>
      <author><first>Shai</first><last>Fine</last></author>
      <author><first>Kfir</first><last>Bar</last></author>
      <pages>370–375</pages>
      <abstract>Sentiment classification and sarcasm detection attract a lot of attention by the NLP research community. However, solving these two problems in Arabic and on the basis of social network data (i.e., Twitter) is still of lower interest. In this paper we present designated solutions for sentiment classification and sarcasm detection tasks that were introduced as part of a shared task by Abu Farha et al. (2021). We adjust the existing state-of-the-art transformer pretrained models for our needs. In addition, we use a variety of machine-learning techniques such as down-sampling, augmentation, bagging, and usage of meta-features to improve the models performance. We achieve an F1-score of 0.75 over the sentiment classification problem where the F1-score is calculated over the positive and negative classes (the neutral class is not taken into account). We achieve an F1-score of 0.66 over the sarcasm detection problem where the F1-score is calculated over the sarcastic class only. In both cases, the above reported results are evaluated over the ArSarcasm-v2–an extended dataset of the ArSarcasm (Farha and Magdy, 2020) that was introduced as part of the shared task. This reflects an improvement to the state-of-the-art results in both tasks.</abstract>
      <url hash="d899a487">2021.wanlp-1.48</url>
      <bibkey>israeli-etal-2021-idc</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm">ArSarcasm</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
    </paper>
    <paper id="49">
      <title>Preprocessing Solutions for Detection of Sarcasm and Sentiment for <fixed-case>A</fixed-case>rabic</title>
      <author><first>Mohamed</first><last>Lichouri</last></author>
      <author><first>Mourad</first><last>Abbas</last></author>
      <author><first>Besma</first><last>Benaziz</last></author>
      <author><first>Aicha</first><last>Zitouni</last></author>
      <author><first>Khaled</first><last>Lounnas</last></author>
      <pages>376–380</pages>
      <abstract>This paper describes our approach to detecting Sentiment and Sarcasm for Arabic in the ArSarcasm 2021 shared task. Data preprocessing is a crucial task for a successful learning, that is why we applied a set of preprocessing steps to the dataset before training two classifiers, namely Linear Support Vector Classifier (LSVC) and Bidirectional Long Short Term Memory (BiLSTM). The findings show that despite the simplicity of the proposed approach, using the LSVC model with a normalizing Arabic (NA) preprocessing and the BiLSTM architecture with an Embedding layer as input have yielded an encouraging F1score of 33.71% and 57.80% for sarcasm and sentiment detection, respectively.</abstract>
      <url hash="96db8111">2021.wanlp-1.49</url>
      <bibkey>lichouri-etal-2021-preprocessing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/astd">ASTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm">ArSarcasm</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
    </paper>
    <paper id="50">
      <title>i<fixed-case>C</fixed-case>ompass at Shared Task on Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Malek</first><last>Naski</last></author>
      <author><first>Abir</first><last>Messaoudi</last></author>
      <author><first>Hatem</first><last>Haddad</last></author>
      <author><first>Moez</first><last>BenHajhmida</last></author>
      <author><first>Chayma</first><last>Fourati</last></author>
      <author><first>Aymen</first><last>Ben Elhaj Mabrouk</last></author>
      <pages>381–385</pages>
      <abstract>We describe our submitted system to the 2021 Shared Task on Sarcasm and Sentiment Detection in Arabic (Abu Farha et al., 2021). We tackled both subtasks, namely Sarcasm Detection (Subtask 1) and Sentiment Analysis (Subtask 2). We used state-of-the-art pretrained contextualized text representation models and fine-tuned them according to the downstream task in hand. As a first approach, we used Google’s multilingual BERT and then other Arabic variants: AraBERT, ARBERT and MARBERT. The results found show that MARBERT outperforms all of the previously mentioned models overall, either on Subtask 1 or Subtask 2.</abstract>
      <url hash="180c2655">2021.wanlp-1.50</url>
      <bibkey>naski-etal-2021-icompass</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
    </paper>
    <paper id="51">
      <title>Machine Learning-Based Model for Sentiment and Sarcasm Detection</title>
      <author><first>Hamada</first><last>Nayel</last></author>
      <author><first>Eslam</first><last>Amer</last></author>
      <author><first>Aya</first><last>Allam</last></author>
      <author><first>Hanya</first><last>Abdallah</last></author>
      <pages>386–389</pages>
      <abstract>Within the last few years, the number of Arabic internet users and Arabic online content is in exponential growth. Dealing with Arabic datasets and the usage of non-explicit sentences to express an opinion are considered to be the major challenges in the field of natural language processing. Hence, sarcasm and sentiment analysis has gained a major interest from the research community, especially in this language. Automatic sarcasm detection and sentiment analysis can be applied using three approaches, namely supervised, unsupervised and hybrid approach. In this paper, a model based on a supervised machine learning algorithm called Support Vector Machine (SVM) has been used for this process. The proposed model has been evaluated using ArSarcasm-v2 dataset. The performance of the proposed model has been compared with other models submitted to sentiment analysis and sarcasm detection shared task.</abstract>
      <url hash="6d4b826c">2021.wanlp-1.51</url>
      <bibkey>nayel-etal-2021-machine-learning</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
    </paper>
    <paper id="52">
      <title><fixed-case>D</fixed-case>eep<fixed-case>B</fixed-case>lue<fixed-case>AI</fixed-case> at <fixed-case>WANLP</fixed-case>-<fixed-case>EACL</fixed-case>2021 task 2: A Deep Ensemble-based Method for Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Bingyan</first><last>Song</last></author>
      <author><first>Chunguang</first><last>Pan</last></author>
      <author><first>Shengguang</first><last>Wang</last></author>
      <author><first>Zhipeng</first><last>Luo</last></author>
      <pages>390–394</pages>
      <abstract>Sarcasm is one of the main challenges for sentiment analysis systems due to using implicit indirect phrasing for expressing opinions, especially in Arabic. This paper presents the system we submitted to the Sarcasm and Sentiment Detection task of WANLP-2021 that is capable of dealing with both two subtasks. We first perform fine-tuning on two kinds of pre-trained language models (PLMs) with different training strategies. Then an effective stacking mechanism is applied on top of the fine-tuned PLMs to obtain the final prediction. Experimental results on ArSarcasm-v2 dataset show the effectiveness of our method and we rank third and second for subtask 1 and 2.</abstract>
      <url hash="76a43991">2021.wanlp-1.52</url>
      <bibkey>song-etal-2021-deepblueai</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
    </paper>
    <paper id="53">
      <title><fixed-case>A</fixed-case>ra<fixed-case>BERT</fixed-case> and Farasa Segmentation Based Approach For Sarcasm and Sentiment Detection in <fixed-case>A</fixed-case>rabic Tweets</title>
      <author><first>Anshul</first><last>Wadhawan</last></author>
      <pages>395–400</pages>
      <abstract>This paper presents our strategy to tackle the EACL WANLP-2021 Shared Task 2: Sarcasm and Sentiment Detection. One of the subtasks aims at developing a system that identifies whether a given Arabic tweet is sarcastic in nature or not, while the other aims to identify the sentiment of the Arabic tweet. We approach the task in two steps. The first step involves pre processing the provided dataset by performing insertions, deletions and segmentation operations on various parts of the text. The second step involves experimenting with multiple variants of two transformer based models, AraELECTRA and AraBERT. Our final approach was ranked seventh and fourth in the Sarcasm and Sentiment Detection subtasks respectively.</abstract>
      <url hash="9d5e7f8b">2021.wanlp-1.53</url>
      <bibkey>wadhawan-2021-arabert</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/arsarcasm-v2">ArSarcasm-v2</pwcdataset>
    </paper>
  </volume>
</collection>
