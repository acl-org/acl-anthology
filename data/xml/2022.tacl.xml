<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.tacl">
  <volume id="1">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 10</booktitle>
      <editor><last>Roark</last><first>Brian</first></editor>
      <editor><last>Nenkova</last><first>Ani</first></editor>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2022</year>
    </meta>
    <paper id="1">
      <title>Word Acquisition in Neural Language Models</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Benjamin K.</first><last>Bergen</last></author>
      <doi>10.1162/tacl_a_00444</doi>
      <abstract>We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words’ ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.</abstract>
      <pages>1–16</pages>
      <url hash="49b3ffae">2022.tacl-1.1</url>
      <bibkey>chang-bergen-2022-word</bibkey>
    </paper>
    <paper id="2">
      <title>Decomposing and Recomposing Event Structure</title>
      <author><first>William</first><last>Gantt</last></author>
      <author><first>Lelia</first><last>Glass</last></author>
      <author><first>Aaron Steven</first><last>White</last></author>
      <doi>10.1162/tacl_a_00445</doi>
      <abstract>We present an event structure classification empirically derived from inferential properties annotated on sentence- and document-level Universal Decompositional Semantics (UDS) graphs. We induce this classification jointly with semantic role, entity, and event-event relation classifications using a document-level generative model structured by these graphs. To support this induction, we augment existing annotations found in the UDS1.0 dataset, which covers the entirety of the English Web Treebank, with an array of inferential properties capturing fine-grained aspects of the temporal and aspectual structure of events. The resulting dataset (available at decomp.io) is the largest annotation of event structure and (partial) event coreference to date.</abstract>
      <pages>17–34</pages>
      <url hash="161c4cb9">2022.tacl-1.2</url>
      <bibkey>gantt-etal-2022-decomposing</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>F</fixed-case>e<fixed-case>T</fixed-case>a<fixed-case>QA</fixed-case>: Free-form Table Question Answering</title>
      <author><first>Linyong</first><last>Nan</last></author>
      <author><first>Chiachun</first><last>Hsieh</last></author>
      <author><first>Ziming</first><last>Mao</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <author><first>Neha</first><last>Verma</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Wojciech</first><last>Kryściński</last></author>
      <author><first>Hailey</first><last>Schoelkopf</last></author>
      <author><first>Riley</first><last>Kong</last></author>
      <author><first>Xiangru</first><last>Tang</last></author>
      <author><first>Mutethia</first><last>Mutuma</last></author>
      <author><first>Ben</first><last>Rosand</last></author>
      <author><first>Isabel</first><last>Trindade</last></author>
      <author><first>Renusree</first><last>Bandaru</last></author>
      <author><first>Jacob</first><last>Cunningham</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <doi>10.1162/tacl_a_00446</doi>
      <abstract>Existing table question answering datasets contain abundant factual questions that primarily evaluate a QA system’s comprehension of query and tabular data. However, restricted by their short-form answers, these datasets fail to include question–answer interactions that represent more advanced and naturally occurring information needs: questions that ask for reasoning and integration of information pieces retrieved from a structured knowledge source. To complement the existing datasets and to reveal the challenging nature of the table-based question answering task, we introduce FeTaQA, a new dataset with 10K Wikipedia-based table, question, free-form answer, supporting table cells pairs. FeTaQA is collected from noteworthy descriptions of Wikipedia tables that contain information people tend to seek; generation of these descriptions requires advanced processing that humans perform on a daily basis: Understand the question and table, retrieve, integrate, infer, and conduct text planning and surface realization to generate an answer. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based QA systems and an end-to-end method based on large pretrained text generation models, and show that FeTaQA poses a challenge for both methods.</abstract>
      <pages>35–49</pages>
      <url hash="3e308fa9">2022.tacl-1.3</url>
      <bibkey>nan-etal-2022-fetaqa</bibkey>
    </paper>
    <paper id="4">
      <title>Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets</title>
      <author><first>Julia</first><last>Kreutzer</last></author>
      <author><first>Isaac</first><last>Caswell</last></author>
      <author><first>Lisa</first><last>Wang</last></author>
      <author><first>Ahsan</first><last>Wahab</last></author>
      <author><first>Daan</first><last>van Esch</last></author>
      <author><first>Nasanbayar</first><last>Ulzii-Orshikh</last></author>
      <author><first>Allahsera</first><last>Tapo</last></author>
      <author><first>Nishant</first><last>Subramani</last></author>
      <author><first>Artem</first><last>Sokolov</last></author>
      <author><first>Claytone</first><last>Sikasote</last></author>
      <author><first>Monang</first><last>Setyawan</last></author>
      <author><first>Supheakmungkol</first><last>Sarin</last></author>
      <author><first>Sokhar</first><last>Samb</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Clara</first><last>Rivera</last></author>
      <author><first>Annette</first><last>Rios</last></author>
      <author><first>Isabel</first><last>Papadimitriou</last></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Pedro Ortiz</first><last>Suarez</last></author>
      <author><first>Iroro</first><last>Orife</last></author>
      <author><first>Kelechi</first><last>Ogueji</last></author>
      <author><first>Andre Niyongabo</first><last>Rubungo</last></author>
      <author><first>Toan Q.</first><last>Nguyen</last></author>
      <author><first>Mathias</first><last>Müller</last></author>
      <author><first>André</first><last>Müller</last></author>
      <author><first>Shamsuddeen Hassan</first><last>Muhammad</last></author>
      <author><first>Nanda</first><last>Muhammad</last></author>
      <author><first>Ayanda</first><last>Mnyakeni</last></author>
      <author><first>Jamshidbek</first><last>Mirzakhalov</last></author>
      <author><first>Tapiwanashe</first><last>Matangira</last></author>
      <author><first>Colin</first><last>Leong</last></author>
      <author><first>Nze</first><last>Lawson</last></author>
      <author><first>Sneha</first><last>Kudugunta</last></author>
      <author><first>Yacine</first><last>Jernite</last></author>
      <author><first>Mathias</first><last>Jenny</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Bonaventure F. P.</first><last>Dossou</last></author>
      <author><first>Sakhile</first><last>Dlamini</last></author>
      <author><first>Nisansa</first><last>de Silva</last></author>
      <author><first>Sakine</first><last>Çabuk Ballı</last></author>
      <author><first>Stella</first><last>Biderman</last></author>
      <author><first>Alessia</first><last>Battisti</last></author>
      <author><first>Ahmed</first><last>Baruwa</last></author>
      <author><first>Ankur</first><last>Bapna</last></author>
      <author><first>Pallavi</first><last>Baljekar</last></author>
      <author><first>Israel Abebe</first><last>Azime</last></author>
      <author><first>Ayodele</first><last>Awokoya</last></author>
      <author><first>Duygu</first><last>Ataman</last></author>
      <author><first>Orevaoghene</first><last>Ahia</last></author>
      <author><first>Oghenefego</first><last>Ahia</last></author>
      <author><first>Sweta</first><last>Agrawal</last></author>
      <author><first>Mofetoluwa</first><last>Adeyemi</last></author>
      <doi>10.1162/tacl_a_00447</doi>
      <abstract>With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.</abstract>
      <pages>50–72</pages>
      <url hash="db342649">2022.tacl-1.4</url>
      <bibkey>kreutzer-etal-2022-quality</bibkey>
    </paper>
    <paper id="5">
      <title>Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation</title>
      <author><first>Jonathan H.</first><last>Clark</last></author>
      <author><first>Dan</first><last>Garrette</last></author>
      <author><first>Iulia</first><last>Turc</last></author>
      <author><first>John</first><last>Wieting</last></author>
      <doi>10.1162/tacl_a_00448</doi>
      <abstract>Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model’s ability to adapt. In this paper, we present Canine, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. Canine outperforms a comparable mBert model by 5.7 F1 on TyDi QA, a challenging multilingual benchmark, despite having fewer model parameters.</abstract>
      <pages>73–91</pages>
      <url hash="0de3f9e2">2022.tacl-1.5</url>
      <bibkey>clark-etal-2022-canine</bibkey>
    </paper>
    <paper id="6">
      <title>Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations</title>
      <author><first>Aida Mostafazadeh</first><last>Davani</last></author>
      <author><first>Mark</first><last>Díaz</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <doi>10.1162/tacl_a_00449</doi>
      <abstract>Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators’ judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important.</abstract>
      <pages>92–110</pages>
      <url hash="ab44fbfa">2022.tacl-1.6</url>
      <bibkey>davani-etal-2022-dealing</bibkey>
    </paper>
    <paper id="7">
      <title>Break, Perturb, Build: Automatic Perturbation of Reasoning Paths Through Question Decomposition</title>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Tomer</first><last>Wolfson</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <doi>10.1162/tacl_a_00450</doi>
      <abstract>Recent efforts to create challenge benchmarks that test the abilities of natural language understanding models have largely depended on human annotations. In this work, we introduce the “Break, Perturb, Build” (BPB) framework for automatic reasoning-oriented perturbation of question-answer pairs. BPB represents a question by decomposing it into the reasoning steps that are required to answer it, symbolically perturbs the decomposition, and then generates new question-answer pairs. We demonstrate the effectiveness of BPB by creating evaluation sets for three reading comprehension (RC) benchmarks, generating thousands of high-quality examples without human intervention. We evaluate a range of RC models on our evaluation sets, which reveals large performance gaps on generated examples compared to the original data. Moreover, symbolic perturbations enable fine-grained analysis of the strengths and limitations of models. Last, augmenting the training data with examples generated by BPB helps close the performance gaps, without any drop on the original data distribution.</abstract>
      <pages>111–126</pages>
      <url hash="13f13e39">2022.tacl-1.7</url>
      <bibkey>geva-etal-2022-break</bibkey>
    </paper>
    <paper id="8">
      <title>Out-of-Domain Discourse Dependency Parsing via Bootstrapping: An Empirical Analysis on Its Effectiveness and Limitation</title>
      <author><first>Noriki</first><last>Nishida</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <doi>10.1162/tacl_a_00451</doi>
      <abstract>Discourse parsing has been studied for decades. However, it still remains challenging to utilize discourse parsing for real-world applications because the parsing accuracy degrades significantly on out-of-domain text. In this paper, we report and discuss the effectiveness and limitations of bootstrapping methods for adapting modern BERT-based discourse dependency parsers to out-of-domain text without relying on additional human supervision. Specifically, we investigate self-training, co-training, tri-training, and asymmetric tri-training of graph-based and transition-based discourse dependency parsing models, as well as confidence measures and sample selection criteria in two adaptation scenarios: monologue adaptation between scientific disciplines and dialogue genre adaptation. We also release COVID-19 Discourse Dependency Treebank (COVID19-DTB), a new manually annotated resource for discourse dependency parsing of biomedical paper abstracts. The experimental results show that bootstrapping is significantly and consistently effective for unsupervised domain adaptation of discourse dependency parsing, but the low coverage of accurately predicted pseudo labels is a bottleneck for further improvement. We show that active learning can mitigate this limitation.</abstract>
      <pages>127–144</pages>
      <url hash="9822021e">2022.tacl-1.8</url>
      <bibkey>nishida-matsumoto-2022-domain</bibkey>
    </paper>
    <paper id="9">
      <title>Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages</title>
      <author><first>Gowtham</first><last>Ramesh</last></author>
      <author><first>Sumanth</first><last>Doddapaneni</last></author>
      <author><first>Aravinth</first><last>Bheemaraj</last></author>
      <author><first>Mayank</first><last>Jobanputra</last></author>
      <author><first>Raghavan</first><last>AK</last></author>
      <author><first>Ajitesh</first><last>Sharma</last></author>
      <author><first>Sujit</first><last>Sahoo</last></author>
      <author><first>Harshita</first><last>Diddee</last></author>
      <author><first>Mahalakshmi</first><last>J</last></author>
      <author><first>Divyanshu</first><last>Kakwani</last></author>
      <author><first>Navneet</first><last>Kumar</last></author>
      <author><first>Aswin</first><last>Pradeep</last></author>
      <author><first>Srihari</first><last>Nagaraj</last></author>
      <author><first>Kumar</first><last>Deepak</last></author>
      <author><first>Vivek</first><last>Raghavan</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Pratyush</first><last>Kumar</last></author>
      <author><first>Mitesh Shantadevi</first><last>Khapra</last></author>
      <doi>10.1162/tacl_a_00452</doi>
      <abstract>We present Samanantar, the largest publicly available parallel corpora collection for Indic languages. The collection contains a total of 49.7 million sentence pairs between English and 11 Indic languages (from two language families). Specifically, we compile 12.4 million sentence pairs from existing, publicly available parallel corpora, and additionally mine 37.4 million sentence pairs from the Web, resulting in a 4× increase. We mine the parallel sentences from the Web by combining many corpora, tools, and methods: (a) Web-crawled monolingual corpora, (b) document OCR for extracting sentences from scanned documents, (c) multilingual representation models for aligning sentences, and (d) approximate nearest neighbor search for searching in a large collection of sentences. Human evaluation of samples from the newly mined corpora validate the high quality of the parallel sentences across 11 languages. Further, we extract 83.4 million sentence pairs between all 55 Indic language pairs from the English-centric parallel corpus using English as the pivot language. We trained multilingual NMT models spanning all these languages on Samanantar which outperform existing models and baselines on publicly available benchmarks, such as FLORES, establishing the utility of Samanantar. Our data and models are available publicly at Samanantar and we hope they will help advance research in NMT and multilingual NLP for Indic languages.</abstract>
      <pages>145–162</pages>
      <url hash="87b5fb09">2022.tacl-1.9</url>
      <bibkey>ramesh-etal-2022-samanantar</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>S</fixed-case>umma<fixed-case>C</fixed-case>: Re-Visiting <fixed-case>NLI</fixed-case>-based Models for Inconsistency Detection in Summarization</title>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Tobias</first><last>Schnabel</last></author>
      <author><first>Paul N.</first><last>Bennett</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <doi>10.1162/tacl_a_00453</doi>
      <abstract>In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work.</abstract>
      <pages>163–177</pages>
      <url hash="868cc089">2022.tacl-1.10</url>
      <bibkey>laban-etal-2022-summac</bibkey>
    </paper>
    <paper id="11">
      <title>A Survey on Automated Fact-Checking</title>
      <author><first>Zhijiang</first><last>Guo</last></author>
      <author><first>Michael</first><last>Schlichtkrull</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <doi>10.1162/tacl_a_00454</doi>
      <abstract>Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.</abstract>
      <pages>178–206</pages>
      <url hash="9bd6cc19">2022.tacl-1.11</url>
      <bibkey>guo-etal-2022-survey</bibkey>
    </paper>
    <paper id="12">
      <title>Predicting Document Coverage for Relation Extraction</title>
      <author><first>Sneha</first><last>Singhania</last></author>
      <author><first>Simon</first><last>Razniewski</last></author>
      <author><first>Gerhard</first><last>Weikum</last></author>
      <doi>10.1162/tacl_a_00456</doi>
      <abstract>This paper presents a new task of predicting the coverage of a text document for relation extraction (RE): Does the document contain many relational tuples for a given entity? Coverage predictions are useful in selecting the best documents for knowledge base construction with large input corpora. To study this problem, we present a dataset of 31,366 diverse documents for 520 entities. We analyze the correlation of document coverage with features like length, entity mention frequency, Alexa rank, language complexity, and information retrieval scores. Each of these features has only moderate predictive power. We employ methods combining features with statistical models like TF-IDF and language models like BERT. The model combining features and BERT, HERB, achieves an F1 score of up to 46%. We demonstrate the utility of coverage predictions on two use cases: KB construction and claim refutation.</abstract>
      <pages>207–223</pages>
      <url hash="bf658a66">2022.tacl-1.12</url>
      <bibkey>singhania-etal-2022-predicting</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>ABNIRML</fixed-case>: Analyzing the Behavior of Neural <fixed-case>IR</fixed-case> Models</title>
      <author><first>Sean</first><last>MacAvaney</last></author>
      <author><first>Sergey</first><last>Feldman</last></author>
      <author><first>Nazli</first><last>Goharian</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <doi>10.1162/tacl_a_00457</doi>
      <abstract>Pretrained contextualized language models such as BERT and T5 have established a new state-of-the-art for ad-hoc search. However, it is not yet well understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have. We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs (ABNIRML), which includes new types of diagnostic probes that allow us to test several characteristics—such as writing styles, factuality, sensitivity to paraphrasing and word order—that are not addressed by previous techniques. To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model’s gains, and identify potential unintended biases the models exhibit. Some of our results confirm conventional wisdom, for example, that recent neural ranking models rely less on exact term overlap with the query, and instead leverage richer linguistic information, evidenced by their higher sensitivity to word and sentence order. Other results are more surprising, such as that some models (e.g., T5 and ColBERT) are biased towards factually correct (rather than simply relevant) texts. Further, some characteristics vary even for the same base language model, and other characteristics can appear due to random variations during model training.1</abstract>
      <pages>224–239</pages>
      <url hash="4c991d03">2022.tacl-1.13</url>
      <bibkey>macavaney-etal-2022-abnirml</bibkey>
    </paper>
    <paper id="14">
      <title>Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference</title>
      <author><first>Yufei</first><last>Feng</last></author>
      <author><first>Xiaoyu</first><last>Yang</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Greenspan</last></author>
      <doi>10.1162/tacl_a_00458</doi>
      <abstract>We introduce a neuro-symbolic natural logic framework based on reinforcement learning with introspective revision. The model samples and rewards specific reasoning paths through policy gradient, in which the introspective revision algorithm modifies intermediate symbolic reasoning steps to discover reward-earning operations as well as leverages external knowledge to alleviate spurious reasoning and training inefficiency. The framework is supported by properly designed local relation models to avoid input entangling, which helps ensure the interpretability of the proof paths. The proposed model has built-in interpretability and shows superior capability in monotonicity inference, systematic generalization, and interpretability, compared with previous models on the existing datasets.</abstract>
      <pages>240–256</pages>
      <url hash="50f2e166">2022.tacl-1.14</url>
      <bibkey>feng-etal-2022-neuro</bibkey>
    </paper>
    <paper id="15">
      <title>Time-Aware Language Models as Temporal Knowledge Bases</title>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Jeremy R.</first><last>Cole</last></author>
      <author><first>Julian Martin</first><last>Eisenschlos</last></author>
      <author><first>Daniel</first><last>Gillick</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <author><first>William W.</first><last>Cohen</last></author>
      <doi>10.1162/tacl_a_00459</doi>
      <abstract>Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum—those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently “refreshed” as new data arrives, without the need for retraining from scratch.</abstract>
      <pages>257–273</pages>
      <url hash="fa836992">2022.tacl-1.15</url>
      <bibkey>dhingra-etal-2022-time</bibkey>
    </paper>
    <paper id="16">
      <title>Multilingual Autoregressive Entity Linking</title>
      <author><first>Nicola</first><last>De Cao</last></author>
      <author><first>Ledell</first><last>Wu</last></author>
      <author><first>Kashyap</first><last>Popat</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Mikhail</first><last>Plekhanov</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Nicola</first><last>Cancedda</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <doi>10.1162/tacl_a_00460</doi>
      <abstract>We present mGENRE, a sequence-to- sequence system for the Multilingual Entity Linking (MEL) problem—the task of resolving language-specific mentions to a multilingual Knowledge Base (KB). For a mention in a given language, mGENRE predicts the name of the target entity left-to-right, token-by-token in an autoregressive fashion. The autoregressive formulation allows us to effectively cross-encode mention string and entity names to capture more interactions than the standard dot product between mention and entity vectors. It also enables fast search within a large KB even for mentions that do not appear in mention tables and with no need for large-scale vector indices. While prior MEL works use a single representation for each entity, we match against entity names of as many languages as possible, which allows exploiting language connections between source input and target name. Moreover, in a zero-shot setting on languages with no training data at all, mGENRE treats the target language as a latent variable that is marginalized at prediction time. This leads to over 50% improvements in average accuracy. We show the efficacy of our approach through extensive evaluation including experiments on three popular MEL benchmarks where we establish new state-of-the-art results. Source code available at https://github.com/facebookresearch/GENRE.</abstract>
      <pages>274–290</pages>
      <url hash="376b497c">2022.tacl-1.16</url>
      <bibkey>de-cao-etal-2022-multilingual</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>B</fixed-case>y<fixed-case>T</fixed-case>5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models</title>
      <author><first>Linting</first><last>Xue</last></author>
      <author><first>Aditya</first><last>Barua</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <author><first>Rami</first><last>Al-Rfou</last></author>
      <author><first>Sharan</first><last>Narang</last></author>
      <author><first>Mihir</first><last>Kale</last></author>
      <author><first>Adam</first><last>Roberts</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <doi>10.1162/tacl_a_00461</doi>
      <abstract>Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1</abstract>
      <pages>291–306</pages>
      <url hash="3db6b62d">2022.tacl-1.17</url>
      <bibkey>xue-etal-2022-byt5</bibkey>
    </paper>
    <paper id="18">
      <title>Designing an Automatic Agent for Repeated Language–based Persuasion Games</title>
      <author><first>Maya</first><last>Raifer</last></author>
      <author><first>Guy</first><last>Rotman</last></author>
      <author><first>Reut</first><last>Apel</last></author>
      <author><first>Moshe</first><last>Tennenholtz</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <doi>10.1162/tacl_a_00462</doi>
      <abstract>Persuasion games are fundamental in economics and AI research and serve as the basis for important applications. However, work on this setup assumes communication with stylized messages that do not consist of rich human language. In this paper we consider a repeated sender (expert) – receiver (decision maker) game, where the sender is fully informed about the state of the world and aims to persuade the receiver to accept a deal by sending one of several possible natural language reviews. We design an automatic expert that plays this repeated game, aiming to achieve the maximal payoff. Our expert is implemented within the Monte Carlo Tree Search (MCTS) algorithm, with deep learning models that exploit behavioral and linguistic signals in order to predict the next action of the decision maker, and the future payoff of the expert given the state of the game and a candidate review. We demonstrate the superiority of our expert over strong baselines and its adaptability to different decision makers and potential proposed deals.1</abstract>
      <pages>307–324</pages>
      <url hash="5bcc954f">2022.tacl-1.18</url>
      <bibkey>raifer-etal-2022-designing</bibkey>
    </paper>
    <paper id="19">
      <title>Towards General Natural Language Understanding with Probabilistic Worldbuilding</title>
      <author><first>Abulhair</first><last>Saparov</last></author>
      <author><first>Tom M.</first><last>Mitchell</last></author>
      <doi>10.1162/tacl_a_00463</doi>
      <abstract>We introduce the Probabilistic Worldbuilding Model (PWM), a new fully symbolic Bayesian model of semantic parsing and reasoning, as a first step in a research program toward more domain- and task-general NLU and AI. Humans create internal mental models of their observations that greatly aid in their ability to understand and reason about a large variety of problems. In PWM, the meanings of sentences, acquired facts about the world, and intermediate steps in reasoning are all expressed in a human-readable formal language, with the design goal of interpretability. PWM is Bayesian, designed specifically to be able to generalize to new domains and new tasks. We derive and implement an inference algorithm that reads sentences by parsing and abducing updates to its latent world model that capture the semantics of those sentences, and evaluate it on two out-of-domain question-answering datasets: (1) ProofWriter and (2) a new dataset we call FictionalGeoQA, designed to be more representative of real language but still simple enough to focus on evaluating reasoning ability, while being robust against heuristics. Our method outperforms baselines on both, thereby demonstrating its value as a proof-of-concept.</abstract>
      <pages>325–342</pages>
      <url hash="67da10c5">2022.tacl-1.19</url>
      <bibkey>saparov-mitchell-2022-towards</bibkey>
    </paper>
    <paper id="20">
      <title>A Multi-Level Optimization Framework for End-to-End Text Augmentation</title>
      <author><first>Sai Ashish</first><last>Somayajula</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Pengtao</first><last>Xie</last></author>
      <doi>10.1162/tacl_a_00464</doi>
      <abstract>Text augmentation is an effective technique in alleviating overfitting in NLP tasks. In existing methods, text augmentation and downstream tasks are mostly performed separately. As a result, the augmented texts may not be optimal to train the downstream model. To address this problem, we propose a three-level optimization framework to perform text augmentation and the downstream task end-to- end. The augmentation model is trained in a way tailored to the downstream task. Our framework consists of three learning stages. A text summarization model is trained to perform data augmentation at the first stage. Each summarization example is associated with a weight to account for its domain difference with the text classification data. At the second stage, we use the model trained at the first stage to perform text augmentation and train a text classification model on the augmented texts. At the third stage, we evaluate the text classification model trained at the second stage and update weights of summarization examples by minimizing the validation loss. These three stages are performed end-to-end. We evaluate our method on several text classification datasets where the results demonstrate the effectiveness of our method. Code is available at https://github.com/Sai-Ashish/End-to-End-Text-Augmentation.</abstract>
      <pages>343–358</pages>
      <url hash="e250d290">2022.tacl-1.20</url>
      <bibkey>somayajula-etal-2022-multi</bibkey>
    </paper>
    <paper id="21">
      <title>Evaluating Explanations: How Much Do Explanations from the Teacher Aid Students?</title>
      <author><first>Danish</first><last>Pruthi</last></author>
      <author><first>Rachit</first><last>Bansal</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Livio Baldini</first><last>Soares</last></author>
      <author><first>Michael</first><last>Collins</last></author>
      <author><first>Zachary C.</first><last>Lipton</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>William W.</first><last>Cohen</last></author>
      <doi>10.1162/tacl_a_00465</doi>
      <abstract>While many methods purport to explain predictions by highlighting salient features, what aims these explanations serve and how they ought to be evaluated often go unstated. In this work, we introduce a framework to quantify the value of explanations via the accuracy gains that they confer on a student model trained to simulate a teacher model. Crucially, the explanations are available to the student during training, but are not available at test time. Compared with prior proposals, our approach is less easily gamed, enabling principled, automatic, model-agnostic evaluation of attributions. Using our framework, we compare numerous attribution methods for text classification and question answering, and observe quantitative differences that are consistent (to a moderate to high degree) across different student model architectures and learning strategies.1</abstract>
      <pages>359–375</pages>
      <url hash="85ed9ebf">2022.tacl-1.21</url>
      <bibkey>pruthi-etal-2022-evaluating</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>VILA</fixed-case>: Improving Structured Content Extraction from Scientific <fixed-case>PDF</fixed-case>s Using Visual Layout Groups</title>
      <author><first>Zejiang</first><last>Shen</last></author>
      <author><first>Kyle</first><last>Lo</last></author>
      <author><first>Lucy Lu</first><last>Wang</last></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Daniel S.</first><last>Weld</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <doi>10.1162/tacl_a_00466</doi>
      <abstract>Accurately extracting structured content from PDFs is a critical first step for NLP over scientific papers. Recent work has improved extraction accuracy by incorporating elementary layout information, for example, each token’s 2D position on the page, into language model pretraining. We introduce new methods that explicitly model VIsual LAyout (VILA) groups, that is, text lines or text blocks, to further improve performance. In our I-VILA approach, we show that simply inserting special tokens denoting layout group boundaries into model inputs can lead to a 1.9% Macro F1 improvement in token classification. In the H-VILA approach, we show that hierarchical encoding of layout-groups can result in up to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike prior layout-aware approaches, our methods do not require expensive additional pretraining, only fine-tuning, which we show can reduce training cost by up to 95%. Experiments are conducted on a newly curated evaluation suite, S2-VLUE, that unifies existing automatically labeled datasets and includes a new dataset of manual annotations covering diverse papers from 19 scientific disciplines. Pre-trained weights, benchmark datasets, and source code are available at https://github.com/allenai/VILA.</abstract>
      <pages>376–392</pages>
      <url hash="7766e7cf">2022.tacl-1.22</url>
      <bibkey>shen-etal-2022-vila</bibkey>
    </paper>
    <paper id="23">
      <title>Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation</title>
      <author><first>Zoey</first><last>Liu</last></author>
      <author><first>Emily</first><last>Prud’hommeaux</last></author>
      <doi>10.1162/tacl_a_00467</doi>
      <abstract>Common designs of model evaluation typically focus on monolingual settings, where different models are compared according to their performance on a single data set that is assumed to be representative of all possible data for the task at hand. While this may be reasonable for a large data set, this assumption is difficult to maintain in low-resource scenarios, where artifacts of the data collection can yield data sets that are outliers, potentially making conclusions about model performance coincidental. To address these concerns, we investigate model generalizability in crosslinguistic low-resource scenarios. Using morphological segmentation as the test case, we compare three broad classes of models with different parameterizations, taking data from 11 languages across 6 language families. In each experimental setting, we evaluate all models on a first data set, then examine their performance consistency when introducing new randomly sampled data sets with the same size and when applying the trained models to unseen test sets of varying sizes. The results demonstrate that the extent of model generalization depends on the characteristics of the data set, and does not necessarily rely heavily on the data set size. Among the characteristics that we studied, the ratio of morpheme overlap and that of the average number of morphemes per word between the training and test sets are the two most prominent factors. Our findings suggest that future work should adopt random sampling to construct data sets with different sizes in order to make more responsible claims about model evaluation.</abstract>
      <pages>393–413</pages>
      <url hash="b898ee8e">2022.tacl-1.23</url>
      <bibkey>liu-prudhommeaux-2022-data</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>PADA</fixed-case>: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains</title>
      <author><first>Eyal</first><last>Ben-David</last></author>
      <author><first>Nadav</first><last>Oved</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <doi>10.1162/tacl_a_00468</doi>
      <abstract>Natural Language Processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples. We address a challenging and underexplored version of this domain adaptation problem, where an algorithm is trained on several source domains, and then applied to examples from unseen domains that are unknown at training time. Particularly, no examples, labeled or unlabeled, or any other knowledge about the target domain are available to the algorithm at training time. We present PADA: An example-based autoregressive Prompt learning algorithm for on-the-fly Any-Domain Adaptation, based on the T5 language model. Given a test example, PADA first generates a unique prompt for it and then, conditioned on this prompt, labels the example with respect to the NLP prediction task. PADA is trained to generate a prompt that is a token sequence of unrestricted length, consisting of Domain Related Features (DRFs) that characterize each of the source domains. Intuitively, the generated prompt is a unique signature that maps the test example to a semantic space spanned by the source domains. In experiments with 3 tasks (text classification and sequence tagging), for a total of 14 multi-source adaptation scenarios, PADA substantially outperforms strong baselines.1</abstract>
      <pages>414–433</pages>
      <url hash="755969f8">2022.tacl-1.24</url>
      <bibkey>ben-david-etal-2022-pada</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>LOT</fixed-case>: A Story-Centric Benchmark for Evaluating <fixed-case>C</fixed-case>hinese Long Text Understanding and Generation</title>
      <author><first>Jian</first><last>Guan</last></author>
      <author><first>Zhuoer</first><last>Feng</last></author>
      <author><first>Yamei</first><last>Chen</last></author>
      <author><first>Ruilin</first><last>He</last></author>
      <author><first>Xiaoxi</first><last>Mao</last></author>
      <author><first>Changjie</first><last>Fan</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <doi>10.1162/tacl_a_00469</doi>
      <abstract>Standard multi-task benchmarks are essential for developing pretraining models that can generalize to various downstream tasks. Existing benchmarks for natural language processing (NLP) usually focus only on understanding or generating short texts. However, long text modeling requires many distinct abilities in contrast to short texts, such as the modeling of long-range discourse and commonsense relations, and the coherence and controllability of generation. The lack of standardized benchmarks makes it difficult to assess these abilities of a model and fairly compare different models, especially Chinese models. Therefore, we propose a story-centric benchmark named LOT for evaluating Chinese long text modeling, which aggregates two understanding tasks and two generation tasks. We construct new datasets for these tasks based on human-written Chinese stories with hundreds of words. Furthermore, we release an encoder-decoder-based Chinese long text pretraining model named LongLM with up to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two generative tasks including text infilling and conditional continuation. Extensive experiments show that LongLM outperforms similar-sized pretraining models substantially on both the understanding and generation tasks in LOT.</abstract>
      <pages>434–451</pages>
      <url hash="efd10361">2022.tacl-1.25</url>
      <bibkey>guan-etal-2022-lot</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>C</fixed-case>zech Grammar Error Correction with a Large and Diverse Corpus</title>
      <author><first>Jakub</first><last>Náplava</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Jana</first><last>Straková</last></author>
      <author><first>Alexandr</first><last>Rosen</last></author>
      <doi>10.1162/tacl_a_00470</doi>
      <abstract>We introduce a large and diverse Czech corpus annotated for grammatical error correction (GEC) with the aim to contribute to the still scarce data resources in this domain for languages other than English. The Grammar Error Correction Corpus for Czech (GECCC) offers a variety of four domains, covering error distributions ranging from high error density essays written by non-native speakers, to website texts, where errors are expected to be much less common. We compare several Czech GEC systems, including several Transformer-based ones, setting a strong baseline to future research. Finally, we meta-evaluate common GEC metrics against human judgments on our data. We make the new Czech GEC corpus publicly available under the CC BY-SA 4.0 license at http://hdl.handle.net/11234/1-4639.</abstract>
      <pages>452–467</pages>
      <url hash="720b2c4e">2022.tacl-1.26</url>
      <bibkey>naplava-etal-2022-czech</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>T</fixed-case>opi<fixed-case>OCQA</fixed-case>: Open-domain Conversational Question Answering with Topic Switching</title>
      <author><first>Vaibhav</first><last>Adlakha</last></author>
      <author><first>Shehzaad</first><last>Dhuliawala</last></author>
      <author><first>Kaheer</first><last>Suleman</last></author>
      <author><first>Harm</first><last>de Vries</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <doi>10.1162/tacl_a_00471</doi>
      <abstract>In a conversational question answering scenario, a questioner seeks to extract information about a topic through a series of interdependent questions and answers. As the conversation progresses, they may switch to related topics, a phenomenon commonly observed in information-seeking search sessions. However, current datasets for conversational question answering are limiting in two ways: 1) they do not contain topic switches; and 2) they assume the reference text for the conversation is given, that is, the setting is not open-domain. We introduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset with topic switches based on Wikipedia. TopiOCQA contains 3,920 conversations with information-seeking questions and free-form answers. On average, a conversation in our dataset spans 13 question-answer turns and involves four topics (documents). TopiOCQA poses a challenging test-bed for models, where efficient retrieval is required on multiple turns of the same conversation, in conjunction with constructing valid responses using conversational history. We evaluate several baselines, by combining state-of-the-art document retrieval methods with neural reader models. Our best model achieves F1 of 55.8, falling short of human performance by 14.2 points, indicating the difficulty of our dataset. Our dataset and code are available at https://mcgill-nlp.github.io/topiocqa.</abstract>
      <pages>468–483</pages>
      <url hash="78eee5a1">2022.tacl-1.27</url>
      <bibkey>adlakha-etal-2022-topiocqa</bibkey>
    </paper>
    <paper id="28">
      <title>A Neighborhood Framework for Resource-Lean Content Flagging</title>
      <author><first>Sheikh Muhammad</first><last>Sarwar</last></author>
      <author><first>Dimitrina</first><last>Zlatkova</last></author>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Yoan</first><last>Dinkov</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <doi>10.1162/tacl_a_00472</doi>
      <abstract>We propose a novel framework for cross- lingual content flagging with limited target- language data, which significantly outperforms prior work in terms of predictive performance. The framework is based on a nearest-neighbor architecture. It is a modern instantiation of the vanilla k-nearest neighbor model, as we use Transformer representations in all its components. Our framework can adapt to new source- language instances, without the need to be retrained from scratch. Unlike prior work on neighborhood-based approaches, we encode the neighborhood information based on query– neighbor interactions. We propose two encoding schemes and we show their effectiveness using both qualitative and quantitative analysis. Our evaluation results on eight languages from two different datasets for abusive language detection show sizable improvements of up to 9.5 F1 points absolute (for Italian) over strong baselines. On average, we achieve 3.6 absolute F1 points of improvement for the three languages in the Jigsaw Multilingual dataset and 2.14 points for the WUL dataset.</abstract>
      <pages>484–502</pages>
      <url hash="405881b7">2022.tacl-1.28</url>
      <bibkey>sarwar-etal-2022-neighborhood</bibkey>
    </paper>
    <paper id="29">
      <title>Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval</title>
      <author><first>Gregor</first><last>Geigle</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/tacl_a_00473</doi>
      <abstract>Current state-of-the-art approaches to cross- modal retrieval process text and visual input jointly, relying on Transformer-based architectures with cross-attention mechanisms that attend over all words and objects in an image. While offering unmatched retrieval performance, such models: 1) are typically pretrained from scratch and thus less scalable, 2) suffer from huge retrieval latency and inefficiency issues, which makes them impractical in realistic applications. To address these crucial gaps towards both improved and efficient cross- modal retrieval, we propose a novel fine-tuning framework that turns any pretrained text-image multi-modal model into an efficient retrieval model. The framework is based on a cooperative retrieve-and-rerank approach that combines: 1) twin networks (i.e., a bi-encoder) to separately encode all items of a corpus, enabling efficient initial retrieval, and 2) a cross-encoder component for a more nuanced (i.e., smarter) ranking of the retrieved small set of items. We also propose to jointly fine- tune the two components with shared weights, yielding a more parameter-efficient model. Our experiments on a series of standard cross-modal retrieval benchmarks in monolingual, multilingual, and zero-shot setups, demonstrate improved accuracy and huge efficiency benefits over the state-of-the-art cross- encoders.1</abstract>
      <pages>503–521</pages>
      <url hash="531eb1e6">2022.tacl-1.29</url>
      <bibkey>geigle-etal-2022-retrieve</bibkey>
    </paper>
    <paper id="30">
      <title>The <fixed-case>F</fixed-case>lores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation</title>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Cynthia</first><last>Gao</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Peng-Jen</first><last>Chen</last></author>
      <author><first>Guillaume</first><last>Wenzek</last></author>
      <author><first>Da</first><last>Ju</last></author>
      <author><first>Sanjana</first><last>Krishnan</last></author>
      <author><first>Marc’Aurelio</first><last>Ranzato</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <doi>10.1162/tacl_a_00474</doi>
      <abstract>One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are fully aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.</abstract>
      <pages>522–538</pages>
      <url hash="68f79ffa">2022.tacl-1.30</url>
      <bibkey>goyal-etal-2022-flores</bibkey>
    </paper>
    <paper id="31">
      <title>♫ <fixed-case>M</fixed-case>u<fixed-case>S</fixed-case>i<fixed-case>Q</fixed-case>ue: Multihop Questions via Single-hop Question Composition</title>
      <author><first>Harsh</first><last>Trivedi</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <doi>10.1162/tacl_a_00475</doi>
      <abstract>Multihop reasoning remains an elusive goal as existing multihop benchmarks are known to be largely solvable via shortcuts. Can we create a question answering (QA) dataset that, by construction, requires proper multihop reasoning? To this end, we introduce a bottom–up approach that systematically selects composable pairs of single-hop questions that are connected, that is, where one reasoning step critically relies on information from another. This bottom–up methodology lets us explore a vast space of questions and add stringent filters as well as other mechanisms targeting connected reasoning. It provides fine-grained control over the construction process and the properties of the resulting k-hop questions. We use this methodology to create MuSiQue-Ans, a new multihop QA dataset with 25K 2–4 hop questions. Relative to existing datasets, MuSiQue-Ans is more difficult overall (3× increase in human–machine gap), and harder to cheat via disconnected reasoning (e.g., a single-hop model has a 30-point drop in F1). We further add unanswerable contrast questions to produce a more stringent dataset, MuSiQue-Full. We hope our datasets will help the NLP community develop models that perform genuine multihop reasoning.1</abstract>
      <pages>539–554</pages>
      <url hash="9cd58cee">2022.tacl-1.31</url>
      <bibkey>trivedi-etal-2022-musique</bibkey>
    </paper>
    <paper id="32">
      <title>Relational Memory-Augmented Language Models</title>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Dani</first><last>Yogatama</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <doi>10.1162/tacl_a_00476</doi>
      <abstract>We present a memory-augmented approach to condition an autoregressive language model on a knowledge graph. We represent the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation. Experiments on WikiText-103, WMT19, and enwik8 English datasets demonstrate that our approach produces a better language model in terms of perplexity and bits per character. We also show that relational memory improves coherence, is complementary to token-based memory, and enables causal interventions. Our model provides a simple yet effective way to combine an autoregressive language model and a knowledge graph for more coherent and logical generation.</abstract>
      <pages>555–572</pages>
      <url hash="52df8e80">2022.tacl-1.32</url>
      <bibkey>liu-etal-2022-relational</bibkey>
    </paper>
    <paper id="33">
      <title>Sentence Similarity Based on Contexts</title>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Tianwei</first><last>Zhang</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <author><first>Chun</first><last>Fan</last></author>
      <doi>10.1162/tacl_a_00477</doi>
      <abstract>Existing methods to measure sentence similarity are faced with two challenges: (1) labeled datasets are usually limited in size, making them insufficient to train supervised neural models; and (2) there is a training-test gap for unsupervised language modeling (LM) based models to compute semantic scores between sentences, since sentence-level semantics are not explicitly modeled at training. This results in inferior performances in this task. In this work, we propose a new framework to address these two issues. The proposed framework is based on the core idea that the meaning of a sentence should be defined by its contexts, and that sentence similarity can be measured by comparing the probabilities of generating two sentences given the same context. The proposed framework is able to generate high-quality, large-scale dataset with semantic similarity scores between two sentences in an unsupervised manner, with which the train-test gap can be largely bridged. Extensive experiments show that the proposed framework achieves significant performance boosts over existing baselines under both the supervised and unsupervised settings across different datasets.</abstract>
      <pages>573–588</pages>
      <url hash="c4aaec3f">2022.tacl-1.33</url>
      <bibkey>sun-etal-2022-sentence</bibkey>
    </paper>
    <paper id="34">
      <title>It’s not Rocket Science: Interpreting Figurative Language in Narratives</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <doi>10.1162/tacl_a_00478</doi>
      <abstract>Figurative language is ubiquitous in English. Yet, the vast majority of NLP research focuses on literal language. Existing text representations by design rely on compositionality, while figurative language is often non- compositional. In this paper, we study the interpretation of two non-compositional figurative languages (idioms and similes). We collected datasets of fictional narratives containing a figurative expression along with crowd-sourced plausible and implausible continuations relying on the correct interpretation of the expression. We then trained models to choose or generate the plausible continuation. Our experiments show that models based solely on pre-trained language models perform substantially worse than humans on these tasks. We additionally propose knowledge-enhanced models, adopting human strategies for interpreting figurative language types: inferring meaning from the context and relying on the constituent words’ literal meanings. The knowledge-enhanced models improve the performance on both the discriminative and generative tasks, further bridging the gap from human performance.</abstract>
      <pages>589–606</pages>
      <url hash="4737650b">2022.tacl-1.34</url>
      <bibkey>chakrabarty-etal-2022-rocket</bibkey>
    </paper>
    <paper id="35">
      <title>Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference</title>
      <author><first>Bangzheng</first><last>Li</last></author>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <doi>10.1162/tacl_a_00479</doi>
      <abstract>The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large number of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics because types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE🍻, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.1</abstract>
      <pages>607–622</pages>
      <url hash="505a8910">2022.tacl-1.35</url>
      <bibkey>li-etal-2022-ultra</bibkey>
    </paper>
    <paper id="36">
      <title>Document Summarization with Latent Queries</title>
      <author><first>Yumo</first><last>Xu</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/tacl_a_00480</doi>
      <abstract>The availability of large-scale datasets has driven the development of neural models that create generic summaries for single or multiple documents. For query-focused summarization (QFS), labeled training data in the form of queries, documents, and summaries is not readily available. We provide a unified modeling framework for any kind of summarization, under the assumption that all summaries are a response to a query, which is observed in the case of QFS and latent in the case of generic summarization. We model queries as discrete latent variables over document tokens, and learn representations compatible with observed and unobserved query verbalizations. Our framework formulates summarization as a generative process, and jointly optimizes a latent query model and a conditional language model. Despite learning from generic summarization data only, our approach outperforms strong comparison systems across benchmarks, query types, document settings, and target domains.1</abstract>
      <pages>623–638</pages>
      <url hash="c13652c6">2022.tacl-1.36</url>
      <bibkey>xu-lapata-2022-document</bibkey>
    </paper>
    <paper id="37">
      <title>End-to-end Argument Mining with Cross-corpora Multi-task Learning</title>
      <author><first>Gaku</first><last>Morio</last></author>
      <author><first>Hiroaki</first><last>Ozaki</last></author>
      <author><first>Terufumi</first><last>Morishita</last></author>
      <author><first>Kohsuke</first><last>Yanai</last></author>
      <doi>10.1162/tacl_a_00481</doi>
      <abstract>Mining an argument structure from text is an important step for tasks such as argument search and summarization. While studies on argument(ation) mining have proposed promising neural network models, they usually suffer from a shortage of training data. To address this issue, we expand the training data with various auxiliary argument mining corpora and propose an end-to-end cross-corpus training method called Multi-Task Argument Mining (MT-AM). To evaluate our approach, we conducted experiments for the main argument mining tasks on several well-established argument mining corpora. The results demonstrate that MT-AM generally outperformed the models trained on a single corpus. Also, the smaller the target corpus was, the better the MT-AM performed. Our extensive analyses suggest that the improvement of MT-AM depends on several factors of transferability among auxiliary and target corpora.</abstract>
      <pages>639–658</pages>
      <url hash="4a308122">2022.tacl-1.37</url>
      <bibkey>morio-etal-2022-end</bibkey>
    </paper>
  </volume>
</collection>
