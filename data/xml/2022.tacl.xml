<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.tacl">
  <volume id="1" type="journal">
    <meta>
      <booktitle>Transactions of the Association for Computational Linguistics, Volume 10</booktitle>
      <editor><last>Roark</last><first>Brian</first></editor>
      <editor><last>Nenkova</last><first>Ani</first></editor>
      <publisher>MIT Press</publisher>
      <address>Cambridge, MA</address>
      <year>2022</year>
      <venue>tacl</venue>
      <journal-volume>10</journal-volume>
    </meta>
    <paper id="1">
      <title>Word Acquisition in Neural Language Models</title>
      <author><first>Tyler A.</first><last>Chang</last></author>
      <author><first>Benjamin K.</first><last>Bergen</last></author>
      <doi>10.1162/tacl_a_00444</doi>
      <abstract>We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words’ ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.</abstract>
      <pages>1–16</pages>
      <url hash="49b3ffae">2022.tacl-1.1</url>
      <bibkey>chang-bergen-2022-word</bibkey>
      <video href="2022.tacl-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Decomposing and Recomposing Event Structure</title>
      <author><first>William</first><last>Gantt</last></author>
      <author><first>Lelia</first><last>Glass</last></author>
      <author><first>Aaron Steven</first><last>White</last></author>
      <doi>10.1162/tacl_a_00445</doi>
      <abstract>We present an event structure classification empirically derived from inferential properties annotated on sentence- and document-level Universal Decompositional Semantics (UDS) graphs. We induce this classification jointly with semantic role, entity, and event-event relation classifications using a document-level generative model structured by these graphs. To support this induction, we augment existing annotations found in the UDS1.0 dataset, which covers the entirety of the English Web Treebank, with an array of inferential properties capturing fine-grained aspects of the temporal and aspectual structure of events. The resulting dataset (available at decomp.io) is the largest annotation of event structure and (partial) event coreference to date.</abstract>
      <pages>17–34</pages>
      <url hash="161c4cb9">2022.tacl-1.2</url>
      <bibkey>gantt-etal-2022-decomposing</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>F</fixed-case>e<fixed-case>T</fixed-case>a<fixed-case>QA</fixed-case>: Free-form Table Question Answering</title>
      <author><first>Linyong</first><last>Nan</last></author>
      <author><first>Chiachun</first><last>Hsieh</last></author>
      <author><first>Ziming</first><last>Mao</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <author><first>Neha</first><last>Verma</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Wojciech</first><last>Kryściński</last></author>
      <author><first>Hailey</first><last>Schoelkopf</last></author>
      <author><first>Riley</first><last>Kong</last></author>
      <author><first>Xiangru</first><last>Tang</last></author>
      <author><first>Mutethia</first><last>Mutuma</last></author>
      <author><first>Ben</first><last>Rosand</last></author>
      <author><first>Isabel</first><last>Trindade</last></author>
      <author><first>Renusree</first><last>Bandaru</last></author>
      <author><first>Jacob</first><last>Cunningham</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <doi>10.1162/tacl_a_00446</doi>
      <abstract>Existing table question answering datasets contain abundant factual questions that primarily evaluate a QA system’s comprehension of query and tabular data. However, restricted by their short-form answers, these datasets fail to include question–answer interactions that represent more advanced and naturally occurring information needs: questions that ask for reasoning and integration of information pieces retrieved from a structured knowledge source. To complement the existing datasets and to reveal the challenging nature of the table-based question answering task, we introduce FeTaQA, a new dataset with 10K Wikipedia-based table, question, free-form answer, supporting table cells pairs. FeTaQA is collected from noteworthy descriptions of Wikipedia tables that contain information people tend to seek; generation of these descriptions requires advanced processing that humans perform on a daily basis: Understand the question and table, retrieve, integrate, infer, and conduct text planning and surface realization to generate an answer. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based QA systems and an end-to-end method based on large pretrained text generation models, and show that FeTaQA poses a challenge for both methods.</abstract>
      <pages>35–49</pages>
      <url hash="3e308fa9">2022.tacl-1.3</url>
      <bibkey>nan-etal-2022-fetaqa</bibkey>
    </paper>
    <paper id="4">
      <title>Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets</title>
      <author><first>Julia</first><last>Kreutzer</last></author>
      <author><first>Isaac</first><last>Caswell</last></author>
      <author><first>Lisa</first><last>Wang</last></author>
      <author><first>Ahsan</first><last>Wahab</last></author>
      <author><first>Daan</first><last>van Esch</last></author>
      <author><first>Nasanbayar</first><last>Ulzii-Orshikh</last></author>
      <author><first>Allahsera</first><last>Tapo</last></author>
      <author><first>Nishant</first><last>Subramani</last></author>
      <author><first>Artem</first><last>Sokolov</last></author>
      <author><first>Claytone</first><last>Sikasote</last></author>
      <author><first>Monang</first><last>Setyawan</last></author>
      <author><first>Supheakmungkol</first><last>Sarin</last></author>
      <author><first>Sokhar</first><last>Samb</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Clara</first><last>Rivera</last></author>
      <author><first>Annette</first><last>Rios</last></author>
      <author><first>Isabel</first><last>Papadimitriou</last></author>
      <author><first>Salomey</first><last>Osei</last></author>
      <author><first>Pedro Ortiz</first><last>Suarez</last></author>
      <author><first>Iroro</first><last>Orife</last></author>
      <author><first>Kelechi</first><last>Ogueji</last></author>
      <author><first>Andre Niyongabo</first><last>Rubungo</last></author>
      <author><first>Toan Q.</first><last>Nguyen</last></author>
      <author><first>Mathias</first><last>Müller</last></author>
      <author><first>André</first><last>Müller</last></author>
      <author><first>Shamsuddeen Hassan</first><last>Muhammad</last></author>
      <author><first>Nanda</first><last>Muhammad</last></author>
      <author><first>Ayanda</first><last>Mnyakeni</last></author>
      <author><first>Jamshidbek</first><last>Mirzakhalov</last></author>
      <author><first>Tapiwanashe</first><last>Matangira</last></author>
      <author><first>Colin</first><last>Leong</last></author>
      <author><first>Nze</first><last>Lawson</last></author>
      <author><first>Sneha</first><last>Kudugunta</last></author>
      <author><first>Yacine</first><last>Jernite</last></author>
      <author><first>Mathias</first><last>Jenny</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Bonaventure F. P.</first><last>Dossou</last></author>
      <author><first>Sakhile</first><last>Dlamini</last></author>
      <author><first>Nisansa</first><last>de Silva</last></author>
      <author><first>Sakine</first><last>Çabuk Ballı</last></author>
      <author><first>Stella</first><last>Biderman</last></author>
      <author><first>Alessia</first><last>Battisti</last></author>
      <author><first>Ahmed</first><last>Baruwa</last></author>
      <author><first>Ankur</first><last>Bapna</last></author>
      <author><first>Pallavi</first><last>Baljekar</last></author>
      <author><first>Israel Abebe</first><last>Azime</last></author>
      <author><first>Ayodele</first><last>Awokoya</last></author>
      <author><first>Duygu</first><last>Ataman</last></author>
      <author><first>Orevaoghene</first><last>Ahia</last></author>
      <author><first>Oghenefego</first><last>Ahia</last></author>
      <author><first>Sweta</first><last>Agrawal</last></author>
      <author><first>Mofetoluwa</first><last>Adeyemi</last></author>
      <doi>10.1162/tacl_a_00447</doi>
      <abstract>With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.</abstract>
      <pages>50–72</pages>
      <url hash="db342649">2022.tacl-1.4</url>
      <bibkey>kreutzer-etal-2022-quality</bibkey>
      <video href="2022.tacl-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation</title>
      <author><first>Jonathan H.</first><last>Clark</last></author>
      <author><first>Dan</first><last>Garrette</last></author>
      <author><first>Iulia</first><last>Turc</last></author>
      <author><first>John</first><last>Wieting</last></author>
      <doi>10.1162/tacl_a_00448</doi>
      <abstract>Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model’s ability to adapt. In this paper, we present Canine, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. Canine outperforms a comparable mBert model by 5.7 F1 on TyDi QA, a challenging multilingual benchmark, despite having fewer model parameters.</abstract>
      <pages>73–91</pages>
      <url hash="0de3f9e2">2022.tacl-1.5</url>
      <bibkey>clark-etal-2022-canine</bibkey>
      <video href="2022.tacl-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations</title>
      <author><first>Aida</first><last>Mostafazadeh Davani</last></author>
      <author><first>Mark</first><last>Díaz</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <doi>10.1162/tacl_a_00449</doi>
      <abstract>Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators’ judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important.</abstract>
      <pages>92–110</pages>
      <url hash="ab44fbfa">2022.tacl-1.6</url>
      <bibkey>davani-etal-2022-dealing</bibkey>
      <video href="2022.tacl-1.6.mp4"/>
    </paper>
    <paper id="7">
      <title>Break, Perturb, Build: Automatic Perturbation of Reasoning Paths Through Question Decomposition</title>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Tomer</first><last>Wolfson</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <doi>10.1162/tacl_a_00450</doi>
      <abstract>Recent efforts to create challenge benchmarks that test the abilities of natural language understanding models have largely depended on human annotations. In this work, we introduce the “Break, Perturb, Build” (BPB) framework for automatic reasoning-oriented perturbation of question-answer pairs. BPB represents a question by decomposing it into the reasoning steps that are required to answer it, symbolically perturbs the decomposition, and then generates new question-answer pairs. We demonstrate the effectiveness of BPB by creating evaluation sets for three reading comprehension (RC) benchmarks, generating thousands of high-quality examples without human intervention. We evaluate a range of RC models on our evaluation sets, which reveals large performance gaps on generated examples compared to the original data. Moreover, symbolic perturbations enable fine-grained analysis of the strengths and limitations of models. Last, augmenting the training data with examples generated by BPB helps close the performance gaps, without any drop on the original data distribution.</abstract>
      <pages>111–126</pages>
      <url hash="13f13e39">2022.tacl-1.7</url>
      <bibkey>geva-etal-2022-break</bibkey>
      <video href="2022.tacl-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Out-of-Domain Discourse Dependency Parsing via Bootstrapping: An Empirical Analysis on Its Effectiveness and Limitation</title>
      <author><first>Noriki</first><last>Nishida</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <doi>10.1162/tacl_a_00451</doi>
      <abstract>Discourse parsing has been studied for decades. However, it still remains challenging to utilize discourse parsing for real-world applications because the parsing accuracy degrades significantly on out-of-domain text. In this paper, we report and discuss the effectiveness and limitations of bootstrapping methods for adapting modern BERT-based discourse dependency parsers to out-of-domain text without relying on additional human supervision. Specifically, we investigate self-training, co-training, tri-training, and asymmetric tri-training of graph-based and transition-based discourse dependency parsing models, as well as confidence measures and sample selection criteria in two adaptation scenarios: monologue adaptation between scientific disciplines and dialogue genre adaptation. We also release COVID-19 Discourse Dependency Treebank (COVID19-DTB), a new manually annotated resource for discourse dependency parsing of biomedical paper abstracts. The experimental results show that bootstrapping is significantly and consistently effective for unsupervised domain adaptation of discourse dependency parsing, but the low coverage of accurately predicted pseudo labels is a bottleneck for further improvement. We show that active learning can mitigate this limitation.</abstract>
      <pages>127–144</pages>
      <url hash="9822021e">2022.tacl-1.8</url>
      <bibkey>nishida-matsumoto-2022-domain</bibkey>
      <video href="2022.tacl-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title>Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Gowtham</first><last>Ramesh</last></author>
      <author><first>Sumanth</first><last>Doddapaneni</last></author>
      <author><first>Aravinth</first><last>Bheemaraj</last></author>
      <author><first>Mayank</first><last>Jobanputra</last></author>
      <author><first>Raghavan</first><last>AK</last></author>
      <author><first>Ajitesh</first><last>Sharma</last></author>
      <author><first>Sujit</first><last>Sahoo</last></author>
      <author><first>Harshita</first><last>Diddee</last></author>
      <author><first>Mahalakshmi</first><last>J</last></author>
      <author><first>Divyanshu</first><last>Kakwani</last></author>
      <author><first>Navneet</first><last>Kumar</last></author>
      <author><first>Aswin</first><last>Pradeep</last></author>
      <author><first>Srihari</first><last>Nagaraj</last></author>
      <author><first>Kumar</first><last>Deepak</last></author>
      <author><first>Vivek</first><last>Raghavan</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Pratyush</first><last>Kumar</last></author>
      <author><first>Mitesh Shantadevi</first><last>Khapra</last></author>
      <doi>10.1162/tacl_a_00452</doi>
      <abstract>We present Samanantar, the largest publicly available parallel corpora collection for Indic languages. The collection contains a total of 49.7 million sentence pairs between English and 11 Indic languages (from two language families). Specifically, we compile 12.4 million sentence pairs from existing, publicly available parallel corpora, and additionally mine 37.4 million sentence pairs from the Web, resulting in a 4× increase. We mine the parallel sentences from the Web by combining many corpora, tools, and methods: (a) Web-crawled monolingual corpora, (b) document OCR for extracting sentences from scanned documents, (c) multilingual representation models for aligning sentences, and (d) approximate nearest neighbor search for searching in a large collection of sentences. Human evaluation of samples from the newly mined corpora validate the high quality of the parallel sentences across 11 languages. Further, we extract 83.4 million sentence pairs between all 55 Indic language pairs from the English-centric parallel corpus using English as the pivot language. We trained multilingual NMT models spanning all these languages on Samanantar which outperform existing models and baselines on publicly available benchmarks, such as FLORES, establishing the utility of Samanantar. Our data and models are available publicly at Samanantar and we hope they will help advance research in NMT and multilingual NLP for Indic languages.</abstract>
      <pages>145–162</pages>
      <url hash="87b5fb09">2022.tacl-1.9</url>
      <bibkey>ramesh-etal-2022-samanantar</bibkey>
      <video href="2022.tacl-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title><fixed-case>S</fixed-case>umma<fixed-case>C</fixed-case>: Re-Visiting <fixed-case>NLI</fixed-case>-based Models for Inconsistency Detection in Summarization</title>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Tobias</first><last>Schnabel</last></author>
      <author><first>Paul N.</first><last>Bennett</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <doi>10.1162/tacl_a_00453</doi>
      <abstract>In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work.</abstract>
      <pages>163–177</pages>
      <url hash="868cc089">2022.tacl-1.10</url>
      <bibkey>laban-etal-2022-summac</bibkey>
      <video href="2022.tacl-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>A Survey on Automated Fact-Checking</title>
      <author><first>Zhijiang</first><last>Guo</last></author>
      <author><first>Michael</first><last>Schlichtkrull</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <doi>10.1162/tacl_a_00454</doi>
      <abstract>Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.</abstract>
      <pages>178–206</pages>
      <url hash="9bd6cc19">2022.tacl-1.11</url>
      <bibkey>guo-etal-2022-survey</bibkey>
      <video href="2022.tacl-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Predicting Document Coverage for Relation Extraction</title>
      <author><first>Sneha</first><last>Singhania</last></author>
      <author><first>Simon</first><last>Razniewski</last></author>
      <author><first>Gerhard</first><last>Weikum</last></author>
      <doi>10.1162/tacl_a_00456</doi>
      <abstract>This paper presents a new task of predicting the coverage of a text document for relation extraction (RE): Does the document contain many relational tuples for a given entity? Coverage predictions are useful in selecting the best documents for knowledge base construction with large input corpora. To study this problem, we present a dataset of 31,366 diverse documents for 520 entities. We analyze the correlation of document coverage with features like length, entity mention frequency, Alexa rank, language complexity, and information retrieval scores. Each of these features has only moderate predictive power. We employ methods combining features with statistical models like TF-IDF and language models like BERT. The model combining features and BERT, HERB, achieves an F1 score of up to 46%. We demonstrate the utility of coverage predictions on two use cases: KB construction and claim refutation.</abstract>
      <pages>207–223</pages>
      <url hash="bf658a66">2022.tacl-1.12</url>
      <bibkey>singhania-etal-2022-predicting</bibkey>
      <video href="2022.tacl-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title><fixed-case>ABNIRML</fixed-case>: Analyzing the Behavior of Neural <fixed-case>IR</fixed-case> Models</title>
      <author><first>Sean</first><last>MacAvaney</last></author>
      <author><first>Sergey</first><last>Feldman</last></author>
      <author><first>Nazli</first><last>Goharian</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <author><first>Arman</first><last>Cohan</last></author>
      <doi>10.1162/tacl_a_00457</doi>
      <abstract>Pretrained contextualized language models such as BERT and T5 have established a new state-of-the-art for ad-hoc search. However, it is not yet well understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have. We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs (ABNIRML), which includes new types of diagnostic probes that allow us to test several characteristics—such as writing styles, factuality, sensitivity to paraphrasing and word order—that are not addressed by previous techniques. To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model’s gains, and identify potential unintended biases the models exhibit. Some of our results confirm conventional wisdom, for example, that recent neural ranking models rely less on exact term overlap with the query, and instead leverage richer linguistic information, evidenced by their higher sensitivity to word and sentence order. Other results are more surprising, such as that some models (e.g., T5 and ColBERT) are biased towards factually correct (rather than simply relevant) texts. Further, some characteristics vary even for the same base language model, and other characteristics can appear due to random variations during model training.1</abstract>
      <pages>224–239</pages>
      <url hash="4c991d03">2022.tacl-1.13</url>
      <bibkey>macavaney-etal-2022-abnirml</bibkey>
      <video href="2022.tacl-1.13.mp4"/>
    </paper>
    <paper id="14">
      <title>Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference</title>
      <author><first>Yufei</first><last>Feng</last></author>
      <author><first>Xiaoyu</first><last>Yang</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Greenspan</last></author>
      <doi>10.1162/tacl_a_00458</doi>
      <abstract>We introduce a neuro-symbolic natural logic framework based on reinforcement learning with introspective revision. The model samples and rewards specific reasoning paths through policy gradient, in which the introspective revision algorithm modifies intermediate symbolic reasoning steps to discover reward-earning operations as well as leverages external knowledge to alleviate spurious reasoning and training inefficiency. The framework is supported by properly designed local relation models to avoid input entangling, which helps ensure the interpretability of the proof paths. The proposed model has built-in interpretability and shows superior capability in monotonicity inference, systematic generalization, and interpretability, compared with previous models on the existing datasets.</abstract>
      <pages>240–256</pages>
      <url hash="50f2e166">2022.tacl-1.14</url>
      <bibkey>feng-etal-2022-neuro</bibkey>
    </paper>
    <paper id="15">
      <title>Time-Aware Language Models as Temporal Knowledge Bases</title>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Jeremy R.</first><last>Cole</last></author>
      <author><first>Julian Martin</first><last>Eisenschlos</last></author>
      <author><first>Daniel</first><last>Gillick</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <author><first>William W.</first><last>Cohen</last></author>
      <doi>10.1162/tacl_a_00459</doi>
      <abstract>Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum—those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently “refreshed” as new data arrives, without the need for retraining from scratch.</abstract>
      <pages>257–273</pages>
      <url hash="fa836992">2022.tacl-1.15</url>
      <bibkey>dhingra-etal-2022-time</bibkey>
      <video href="2022.tacl-1.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Multilingual Autoregressive Entity Linking</title>
      <author><first>Nicola</first><last>De Cao</last></author>
      <author><first>Ledell</first><last>Wu</last></author>
      <author><first>Kashyap</first><last>Popat</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Mikhail</first><last>Plekhanov</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Nicola</first><last>Cancedda</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <doi>10.1162/tacl_a_00460</doi>
      <abstract>We present mGENRE, a sequence-to- sequence system for the Multilingual Entity Linking (MEL) problem—the task of resolving language-specific mentions to a multilingual Knowledge Base (KB). For a mention in a given language, mGENRE predicts the name of the target entity left-to-right, token-by-token in an autoregressive fashion. The autoregressive formulation allows us to effectively cross-encode mention string and entity names to capture more interactions than the standard dot product between mention and entity vectors. It also enables fast search within a large KB even for mentions that do not appear in mention tables and with no need for large-scale vector indices. While prior MEL works use a single representation for each entity, we match against entity names of as many languages as possible, which allows exploiting language connections between source input and target name. Moreover, in a zero-shot setting on languages with no training data at all, mGENRE treats the target language as a latent variable that is marginalized at prediction time. This leads to over 50% improvements in average accuracy. We show the efficacy of our approach through extensive evaluation including experiments on three popular MEL benchmarks where we establish new state-of-the-art results. Source code available at <url>https://github.com/facebookresearch/GENRE</url>.</abstract>
      <pages>274–290</pages>
      <url hash="376b497c">2022.tacl-1.16</url>
      <bibkey>de-cao-etal-2022-multilingual</bibkey>
      <video href="2022.tacl-1.16.mp4"/>
    </paper>
    <paper id="17">
      <title><fixed-case>B</fixed-case>y<fixed-case>T</fixed-case>5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models</title>
      <author><first>Linting</first><last>Xue</last></author>
      <author><first>Aditya</first><last>Barua</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <author><first>Rami</first><last>Al-Rfou</last></author>
      <author><first>Sharan</first><last>Narang</last></author>
      <author><first>Mihir</first><last>Kale</last></author>
      <author><first>Adam</first><last>Roberts</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <doi>10.1162/tacl_a_00461</doi>
      <abstract>Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1</abstract>
      <pages>291–306</pages>
      <url hash="3db6b62d">2022.tacl-1.17</url>
      <bibkey>xue-etal-2022-byt5</bibkey>
      <video href="2022.tacl-1.17.mp4"/>
    </paper>
    <paper id="18">
      <title>Designing an Automatic Agent for Repeated Language–based Persuasion Games</title>
      <author><first>Maya</first><last>Raifer</last></author>
      <author><first>Guy</first><last>Rotman</last></author>
      <author><first>Reut</first><last>Apel</last></author>
      <author><first>Moshe</first><last>Tennenholtz</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <doi>10.1162/tacl_a_00462</doi>
      <abstract>Persuasion games are fundamental in economics and AI research and serve as the basis for important applications. However, work on this setup assumes communication with stylized messages that do not consist of rich human language. In this paper we consider a repeated sender (expert) – receiver (decision maker) game, where the sender is fully informed about the state of the world and aims to persuade the receiver to accept a deal by sending one of several possible natural language reviews. We design an automatic expert that plays this repeated game, aiming to achieve the maximal payoff. Our expert is implemented within the Monte Carlo Tree Search (MCTS) algorithm, with deep learning models that exploit behavioral and linguistic signals in order to predict the next action of the decision maker, and the future payoff of the expert given the state of the game and a candidate review. We demonstrate the superiority of our expert over strong baselines and its adaptability to different decision makers and potential proposed deals.1</abstract>
      <pages>307–324</pages>
      <url hash="5bcc954f">2022.tacl-1.18</url>
      <bibkey>raifer-etal-2022-designing</bibkey>
    </paper>
    <paper id="19">
      <title>Towards General Natural Language Understanding with Probabilistic Worldbuilding</title>
      <author><first>Abulhair</first><last>Saparov</last></author>
      <author><first>Tom M.</first><last>Mitchell</last></author>
      <doi>10.1162/tacl_a_00463</doi>
      <abstract>We introduce the Probabilistic Worldbuilding Model (PWM), a new fully symbolic Bayesian model of semantic parsing and reasoning, as a first step in a research program toward more domain- and task-general NLU and AI. Humans create internal mental models of their observations that greatly aid in their ability to understand and reason about a large variety of problems. In PWM, the meanings of sentences, acquired facts about the world, and intermediate steps in reasoning are all expressed in a human-readable formal language, with the design goal of interpretability. PWM is Bayesian, designed specifically to be able to generalize to new domains and new tasks. We derive and implement an inference algorithm that reads sentences by parsing and abducing updates to its latent world model that capture the semantics of those sentences, and evaluate it on two out-of-domain question-answering datasets: (1) ProofWriter and (2) a new dataset we call FictionalGeoQA, designed to be more representative of real language but still simple enough to focus on evaluating reasoning ability, while being robust against heuristics. Our method outperforms baselines on both, thereby demonstrating its value as a proof-of-concept.</abstract>
      <pages>325–342</pages>
      <url hash="67da10c5">2022.tacl-1.19</url>
      <bibkey>saparov-mitchell-2022-towards</bibkey>
      <video href="2022.tacl-1.19.mp4"/>
    </paper>
    <paper id="20">
      <title>A Multi-Level Optimization Framework for End-to-End Text Augmentation</title>
      <author><first>Sai Ashish</first><last>Somayajula</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Pengtao</first><last>Xie</last></author>
      <doi>10.1162/tacl_a_00464</doi>
      <abstract>Text augmentation is an effective technique in alleviating overfitting in NLP tasks. In existing methods, text augmentation and downstream tasks are mostly performed separately. As a result, the augmented texts may not be optimal to train the downstream model. To address this problem, we propose a three-level optimization framework to perform text augmentation and the downstream task end-to- end. The augmentation model is trained in a way tailored to the downstream task. Our framework consists of three learning stages. A text summarization model is trained to perform data augmentation at the first stage. Each summarization example is associated with a weight to account for its domain difference with the text classification data. At the second stage, we use the model trained at the first stage to perform text augmentation and train a text classification model on the augmented texts. At the third stage, we evaluate the text classification model trained at the second stage and update weights of summarization examples by minimizing the validation loss. These three stages are performed end-to-end. We evaluate our method on several text classification datasets where the results demonstrate the effectiveness of our method. Code is available at <url>https://github.com/Sai-Ashish/End-to-End-Text-Augmentation</url>.</abstract>
      <pages>343–358</pages>
      <url hash="e250d290">2022.tacl-1.20</url>
      <bibkey>somayajula-etal-2022-multi</bibkey>
      <video href="2022.tacl-1.20.mp4"/>
    </paper>
    <paper id="21">
      <title>Evaluating Explanations: How Much Do Explanations from the Teacher Aid Students?</title>
      <author><first>Danish</first><last>Pruthi</last></author>
      <author><first>Rachit</first><last>Bansal</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Livio</first><last>Baldini Soares</last></author>
      <author><first>Michael</first><last>Collins</last></author>
      <author><first>Zachary C.</first><last>Lipton</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>William W.</first><last>Cohen</last></author>
      <doi>10.1162/tacl_a_00465</doi>
      <abstract>While many methods purport to explain predictions by highlighting salient features, what aims these explanations serve and how they ought to be evaluated often go unstated. In this work, we introduce a framework to quantify the value of explanations via the accuracy gains that they confer on a student model trained to simulate a teacher model. Crucially, the explanations are available to the student during training, but are not available at test time. Compared with prior proposals, our approach is less easily gamed, enabling principled, automatic, model-agnostic evaluation of attributions. Using our framework, we compare numerous attribution methods for text classification and question answering, and observe quantitative differences that are consistent (to a moderate to high degree) across different student model architectures and learning strategies.1</abstract>
      <pages>359–375</pages>
      <url hash="85ed9ebf">2022.tacl-1.21</url>
      <bibkey>pruthi-etal-2022-evaluating</bibkey>
      <video href="2022.tacl-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title><fixed-case>VILA</fixed-case>: Improving Structured Content Extraction from Scientific <fixed-case>PDF</fixed-case>s Using Visual Layout Groups</title>
      <author><first>Zejiang</first><last>Shen</last></author>
      <author><first>Kyle</first><last>Lo</last></author>
      <author><first>Lucy Lu</first><last>Wang</last></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Daniel S.</first><last>Weld</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <doi>10.1162/tacl_a_00466</doi>
      <abstract>Accurately extracting structured content from PDFs is a critical first step for NLP over scientific papers. Recent work has improved extraction accuracy by incorporating elementary layout information, for example, each token’s 2D position on the page, into language model pretraining. We introduce new methods that explicitly model VIsual LAyout (VILA) groups, that is, text lines or text blocks, to further improve performance. In our I-VILA approach, we show that simply inserting special tokens denoting layout group boundaries into model inputs can lead to a 1.9% Macro F1 improvement in token classification. In the H-VILA approach, we show that hierarchical encoding of layout-groups can result in up to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike prior layout-aware approaches, our methods do not require expensive additional pretraining, only fine-tuning, which we show can reduce training cost by up to 95%. Experiments are conducted on a newly curated evaluation suite, S2-VLUE, that unifies existing automatically labeled datasets and includes a new dataset of manual annotations covering diverse papers from 19 scientific disciplines. Pre-trained weights, benchmark datasets, and source code are available at <url>https://github.com/allenai/VILA</url>.</abstract>
      <pages>376–392</pages>
      <url hash="7766e7cf">2022.tacl-1.22</url>
      <bibkey>shen-etal-2022-vila</bibkey>
      <video href="2022.tacl-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation</title>
      <author><first>Zoey</first><last>Liu</last></author>
      <author><first>Emily</first><last>Prud’hommeaux</last></author>
      <doi>10.1162/tacl_a_00467</doi>
      <abstract>Common designs of model evaluation typically focus on monolingual settings, where different models are compared according to their performance on a single data set that is assumed to be representative of all possible data for the task at hand. While this may be reasonable for a large data set, this assumption is difficult to maintain in low-resource scenarios, where artifacts of the data collection can yield data sets that are outliers, potentially making conclusions about model performance coincidental. To address these concerns, we investigate model generalizability in crosslinguistic low-resource scenarios. Using morphological segmentation as the test case, we compare three broad classes of models with different parameterizations, taking data from 11 languages across 6 language families. In each experimental setting, we evaluate all models on a first data set, then examine their performance consistency when introducing new randomly sampled data sets with the same size and when applying the trained models to unseen test sets of varying sizes. The results demonstrate that the extent of model generalization depends on the characteristics of the data set, and does not necessarily rely heavily on the data set size. Among the characteristics that we studied, the ratio of morpheme overlap and that of the average number of morphemes per word between the training and test sets are the two most prominent factors. Our findings suggest that future work should adopt random sampling to construct data sets with different sizes in order to make more responsible claims about model evaluation.</abstract>
      <pages>393–413</pages>
      <url hash="b898ee8e">2022.tacl-1.23</url>
      <bibkey>liu-prudhommeaux-2022-data</bibkey>
      <video href="2022.tacl-1.23.mp4"/>
    </paper>
    <paper id="24">
      <title><fixed-case>PADA</fixed-case>: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains</title>
      <author><first>Eyal</first><last>Ben-David</last></author>
      <author><first>Nadav</first><last>Oved</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <doi>10.1162/tacl_a_00468</doi>
      <abstract>Natural Language Processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples. We address a challenging and underexplored version of this domain adaptation problem, where an algorithm is trained on several source domains, and then applied to examples from unseen domains that are unknown at training time. Particularly, no examples, labeled or unlabeled, or any other knowledge about the target domain are available to the algorithm at training time. We present PADA: An example-based autoregressive Prompt learning algorithm for on-the-fly Any-Domain Adaptation, based on the T5 language model. Given a test example, PADA first generates a unique prompt for it and then, conditioned on this prompt, labels the example with respect to the NLP prediction task. PADA is trained to generate a prompt that is a token sequence of unrestricted length, consisting of Domain Related Features (DRFs) that characterize each of the source domains. Intuitively, the generated prompt is a unique signature that maps the test example to a semantic space spanned by the source domains. In experiments with 3 tasks (text classification and sequence tagging), for a total of 14 multi-source adaptation scenarios, PADA substantially outperforms strong baselines.1</abstract>
      <pages>414–433</pages>
      <url hash="755969f8">2022.tacl-1.24</url>
      <bibkey>ben-david-etal-2022-pada</bibkey>
      <video href="2022.tacl-1.24.mp4"/>
    </paper>
    <paper id="25">
      <title><fixed-case>LOT</fixed-case>: A Story-Centric Benchmark for Evaluating <fixed-case>C</fixed-case>hinese Long Text Understanding and Generation</title>
      <author><first>Jian</first><last>Guan</last></author>
      <author><first>Zhuoer</first><last>Feng</last></author>
      <author><first>Yamei</first><last>Chen</last></author>
      <author><first>Ruilin</first><last>He</last></author>
      <author><first>Xiaoxi</first><last>Mao</last></author>
      <author><first>Changjie</first><last>Fan</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <doi>10.1162/tacl_a_00469</doi>
      <abstract>Standard multi-task benchmarks are essential for developing pretraining models that can generalize to various downstream tasks. Existing benchmarks for natural language processing (NLP) usually focus only on understanding or generating short texts. However, long text modeling requires many distinct abilities in contrast to short texts, such as the modeling of long-range discourse and commonsense relations, and the coherence and controllability of generation. The lack of standardized benchmarks makes it difficult to assess these abilities of a model and fairly compare different models, especially Chinese models. Therefore, we propose a story-centric benchmark named LOT for evaluating Chinese long text modeling, which aggregates two understanding tasks and two generation tasks. We construct new datasets for these tasks based on human-written Chinese stories with hundreds of words. Furthermore, we release an encoder-decoder-based Chinese long text pretraining model named LongLM with up to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two generative tasks including text infilling and conditional continuation. Extensive experiments show that LongLM outperforms similar-sized pretraining models substantially on both the understanding and generation tasks in LOT.</abstract>
      <pages>434–451</pages>
      <url hash="efd10361">2022.tacl-1.25</url>
      <bibkey>guan-etal-2022-lot</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>C</fixed-case>zech Grammar Error Correction with a Large and Diverse Corpus</title>
      <author><first>Jakub</first><last>Náplava</last></author>
      <author><first>Milan</first><last>Straka</last></author>
      <author><first>Jana</first><last>Straková</last></author>
      <author><first>Alexandr</first><last>Rosen</last></author>
      <doi>10.1162/tacl_a_00470</doi>
      <abstract>We introduce a large and diverse Czech corpus annotated for grammatical error correction (GEC) with the aim to contribute to the still scarce data resources in this domain for languages other than English. The Grammar Error Correction Corpus for Czech (GECCC) offers a variety of four domains, covering error distributions ranging from high error density essays written by non-native speakers, to website texts, where errors are expected to be much less common. We compare several Czech GEC systems, including several Transformer-based ones, setting a strong baseline to future research. Finally, we meta-evaluate common GEC metrics against human judgments on our data. We make the new Czech GEC corpus publicly available under the CC BY-SA 4.0 license at <url>http://hdl.handle.net/11234/1-4639</url>.</abstract>
      <pages>452–467</pages>
      <url hash="720b2c4e">2022.tacl-1.26</url>
      <bibkey>naplava-etal-2022-czech</bibkey>
      <video href="2022.tacl-1.26.mp4"/>
    </paper>
    <paper id="27">
      <title><fixed-case>T</fixed-case>opi<fixed-case>OCQA</fixed-case>: Open-domain Conversational Question Answering with Topic Switching</title>
      <author><first>Vaibhav</first><last>Adlakha</last></author>
      <author><first>Shehzaad</first><last>Dhuliawala</last></author>
      <author><first>Kaheer</first><last>Suleman</last></author>
      <author><first>Harm</first><last>de Vries</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <doi>10.1162/tacl_a_00471</doi>
      <abstract>In a conversational question answering scenario, a questioner seeks to extract information about a topic through a series of interdependent questions and answers. As the conversation progresses, they may switch to related topics, a phenomenon commonly observed in information-seeking search sessions. However, current datasets for conversational question answering are limiting in two ways: 1) they do not contain topic switches; and 2) they assume the reference text for the conversation is given, that is, the setting is not open-domain. We introduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset with topic switches based on Wikipedia. TopiOCQA contains 3,920 conversations with information-seeking questions and free-form answers. On average, a conversation in our dataset spans 13 question-answer turns and involves four topics (documents). TopiOCQA poses a challenging test-bed for models, where efficient retrieval is required on multiple turns of the same conversation, in conjunction with constructing valid responses using conversational history. We evaluate several baselines, by combining state-of-the-art document retrieval methods with neural reader models. Our best model achieves F1 of 55.8, falling short of human performance by 14.2 points, indicating the difficulty of our dataset. Our dataset and code are available at <url>https://mcgill-nlp.github.io/topiocqa</url>.</abstract>
      <pages>468–483</pages>
      <url hash="78eee5a1">2022.tacl-1.27</url>
      <bibkey>adlakha-etal-2022-topiocqa</bibkey>
      <video href="2022.tacl-1.27.mp4"/>
    </paper>
    <paper id="28">
      <title>A Neighborhood Framework for Resource-Lean Content Flagging</title>
      <author><first>Sheikh Muhammad</first><last>Sarwar</last></author>
      <author><first>Dimitrina</first><last>Zlatkova</last></author>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Yoan</first><last>Dinkov</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <doi>10.1162/tacl_a_00472</doi>
      <abstract>We propose a novel framework for cross- lingual content flagging with limited target- language data, which significantly outperforms prior work in terms of predictive performance. The framework is based on a nearest-neighbor architecture. It is a modern instantiation of the vanilla k-nearest neighbor model, as we use Transformer representations in all its components. Our framework can adapt to new source- language instances, without the need to be retrained from scratch. Unlike prior work on neighborhood-based approaches, we encode the neighborhood information based on query– neighbor interactions. We propose two encoding schemes and we show their effectiveness using both qualitative and quantitative analysis. Our evaluation results on eight languages from two different datasets for abusive language detection show sizable improvements of up to 9.5 F1 points absolute (for Italian) over strong baselines. On average, we achieve 3.6 absolute F1 points of improvement for the three languages in the Jigsaw Multilingual dataset and 2.14 points for the WUL dataset.</abstract>
      <pages>484–502</pages>
      <url hash="405881b7">2022.tacl-1.28</url>
      <bibkey>sarwar-etal-2022-neighborhood</bibkey>
    </paper>
    <paper id="29">
      <title>Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval</title>
      <author><first>Gregor</first><last>Geigle</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <doi>10.1162/tacl_a_00473</doi>
      <abstract>Current state-of-the-art approaches to cross- modal retrieval process text and visual input jointly, relying on Transformer-based architectures with cross-attention mechanisms that attend over all words and objects in an image. While offering unmatched retrieval performance, such models: 1) are typically pretrained from scratch and thus less scalable, 2) suffer from huge retrieval latency and inefficiency issues, which makes them impractical in realistic applications. To address these crucial gaps towards both improved and efficient cross- modal retrieval, we propose a novel fine-tuning framework that turns any pretrained text-image multi-modal model into an efficient retrieval model. The framework is based on a cooperative retrieve-and-rerank approach that combines: 1) twin networks (i.e., a bi-encoder) to separately encode all items of a corpus, enabling efficient initial retrieval, and 2) a cross-encoder component for a more nuanced (i.e., smarter) ranking of the retrieved small set of items. We also propose to jointly fine- tune the two components with shared weights, yielding a more parameter-efficient model. Our experiments on a series of standard cross-modal retrieval benchmarks in monolingual, multilingual, and zero-shot setups, demonstrate improved accuracy and huge efficiency benefits over the state-of-the-art cross- encoders.1</abstract>
      <pages>503–521</pages>
      <url hash="531eb1e6">2022.tacl-1.29</url>
      <bibkey>geigle-etal-2022-retrieve</bibkey>
      <video href="2022.tacl-1.29.mp4"/>
    </paper>
    <paper id="30">
      <title>The <fixed-case>F</fixed-case>lores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation</title>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Cynthia</first><last>Gao</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Peng-Jen</first><last>Chen</last></author>
      <author><first>Guillaume</first><last>Wenzek</last></author>
      <author><first>Da</first><last>Ju</last></author>
      <author><first>Sanjana</first><last>Krishnan</last></author>
      <author><first>Marc’Aurelio</first><last>Ranzato</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Angela</first><last>Fan</last></author>
      <doi>10.1162/tacl_a_00474</doi>
      <abstract>One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the Flores-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are fully aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.</abstract>
      <pages>522–538</pages>
      <url hash="68f79ffa">2022.tacl-1.30</url>
      <bibkey>goyal-etal-2022-flores</bibkey>
      <video href="2022.tacl-1.30.mp4"/>
    </paper>
    <paper id="31">
      <title>♫ <fixed-case>M</fixed-case>u<fixed-case>S</fixed-case>i<fixed-case>Q</fixed-case>ue: Multihop Questions via Single-hop Question Composition</title>
      <author><first>Harsh</first><last>Trivedi</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <doi>10.1162/tacl_a_00475</doi>
      <abstract>Multihop reasoning remains an elusive goal as existing multihop benchmarks are known to be largely solvable via shortcuts. Can we create a question answering (QA) dataset that, by construction, requires proper multihop reasoning? To this end, we introduce a bottom–up approach that systematically selects composable pairs of single-hop questions that are connected, that is, where one reasoning step critically relies on information from another. This bottom–up methodology lets us explore a vast space of questions and add stringent filters as well as other mechanisms targeting connected reasoning. It provides fine-grained control over the construction process and the properties of the resulting k-hop questions. We use this methodology to create MuSiQue-Ans, a new multihop QA dataset with 25K 2–4 hop questions. Relative to existing datasets, MuSiQue-Ans is more difficult overall (3× increase in human–machine gap), and harder to cheat via disconnected reasoning (e.g., a single-hop model has a 30-point drop in F1). We further add unanswerable contrast questions to produce a more stringent dataset, MuSiQue-Full. We hope our datasets will help the NLP community develop models that perform genuine multihop reasoning.1</abstract>
      <pages>539–554</pages>
      <url hash="9cd58cee">2022.tacl-1.31</url>
      <bibkey>trivedi-etal-2022-musique</bibkey>
      <video href="2022.tacl-1.31.mp4"/>
    </paper>
    <paper id="32">
      <title>Relational Memory-Augmented Language Models</title>
      <author><first>Qi</first><last>Liu</last></author>
      <author><first>Dani</first><last>Yogatama</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <doi>10.1162/tacl_a_00476</doi>
      <abstract>We present a memory-augmented approach to condition an autoregressive language model on a knowledge graph. We represent the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation. Experiments on WikiText-103, WMT19, and enwik8 English datasets demonstrate that our approach produces a better language model in terms of perplexity and bits per character. We also show that relational memory improves coherence, is complementary to token-based memory, and enables causal interventions. Our model provides a simple yet effective way to combine an autoregressive language model and a knowledge graph for more coherent and logical generation.</abstract>
      <pages>555–572</pages>
      <url hash="52df8e80">2022.tacl-1.32</url>
      <bibkey>liu-etal-2022-relational</bibkey>
    </paper>
    <paper id="33">
      <title>Sentence Similarity Based on Contexts</title>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Tianwei</first><last>Zhang</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <author><first>Chun</first><last>Fan</last></author>
      <doi>10.1162/tacl_a_00477</doi>
      <abstract>Existing methods to measure sentence similarity are faced with two challenges: (1) labeled datasets are usually limited in size, making them insufficient to train supervised neural models; and (2) there is a training-test gap for unsupervised language modeling (LM) based models to compute semantic scores between sentences, since sentence-level semantics are not explicitly modeled at training. This results in inferior performances in this task. In this work, we propose a new framework to address these two issues. The proposed framework is based on the core idea that the meaning of a sentence should be defined by its contexts, and that sentence similarity can be measured by comparing the probabilities of generating two sentences given the same context. The proposed framework is able to generate high-quality, large-scale dataset with semantic similarity scores between two sentences in an unsupervised manner, with which the train-test gap can be largely bridged. Extensive experiments show that the proposed framework achieves significant performance boosts over existing baselines under both the supervised and unsupervised settings across different datasets.</abstract>
      <pages>573–588</pages>
      <url hash="c4aaec3f">2022.tacl-1.33</url>
      <bibkey>sun-etal-2022-sentence</bibkey>
    </paper>
    <paper id="34">
      <title>It’s not Rocket Science: Interpreting Figurative Language in Narratives</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <doi>10.1162/tacl_a_00478</doi>
      <abstract>Figurative language is ubiquitous in English. Yet, the vast majority of NLP research focuses on literal language. Existing text representations by design rely on compositionality, while figurative language is often non- compositional. In this paper, we study the interpretation of two non-compositional figurative languages (idioms and similes). We collected datasets of fictional narratives containing a figurative expression along with crowd-sourced plausible and implausible continuations relying on the correct interpretation of the expression. We then trained models to choose or generate the plausible continuation. Our experiments show that models based solely on pre-trained language models perform substantially worse than humans on these tasks. We additionally propose knowledge-enhanced models, adopting human strategies for interpreting figurative language types: inferring meaning from the context and relying on the constituent words’ literal meanings. The knowledge-enhanced models improve the performance on both the discriminative and generative tasks, further bridging the gap from human performance.</abstract>
      <pages>589–606</pages>
      <url hash="4737650b">2022.tacl-1.34</url>
      <bibkey>chakrabarty-etal-2022-rocket</bibkey>
      <video href="2022.tacl-1.34.mp4"/>
    </paper>
    <paper id="35">
      <title>Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference</title>
      <author><first>Bangzheng</first><last>Li</last></author>
      <author><first>Wenpeng</first><last>Yin</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <doi>10.1162/tacl_a_00479</doi>
      <abstract>The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large number of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics because types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE🍻, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.1</abstract>
      <pages>607–622</pages>
      <url hash="505a8910">2022.tacl-1.35</url>
      <bibkey>li-etal-2022-ultra</bibkey>
      <video href="2022.tacl-1.35.mp4"/>
    </paper>
    <paper id="36">
      <title>Document Summarization with Latent Queries</title>
      <author><first>Yumo</first><last>Xu</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/tacl_a_00480</doi>
      <abstract>The availability of large-scale datasets has driven the development of neural models that create generic summaries for single or multiple documents. For query-focused summarization (QFS), labeled training data in the form of queries, documents, and summaries is not readily available. We provide a unified modeling framework for any kind of summarization, under the assumption that all summaries are a response to a query, which is observed in the case of QFS and latent in the case of generic summarization. We model queries as discrete latent variables over document tokens, and learn representations compatible with observed and unobserved query verbalizations. Our framework formulates summarization as a generative process, and jointly optimizes a latent query model and a conditional language model. Despite learning from generic summarization data only, our approach outperforms strong comparison systems across benchmarks, query types, document settings, and target domains.1</abstract>
      <pages>623–638</pages>
      <url hash="c13652c6">2022.tacl-1.36</url>
      <bibkey>xu-lapata-2022-document</bibkey>
      <video href="2022.tacl-1.36.mp4"/>
    </paper>
    <paper id="37">
      <title>End-to-end Argument Mining with Cross-corpora Multi-task Learning</title>
      <author><first>Gaku</first><last>Morio</last></author>
      <author><first>Hiroaki</first><last>Ozaki</last></author>
      <author><first>Terufumi</first><last>Morishita</last></author>
      <author><first>Kohsuke</first><last>Yanai</last></author>
      <doi>10.1162/tacl_a_00481</doi>
      <abstract>Mining an argument structure from text is an important step for tasks such as argument search and summarization. While studies on argument(ation) mining have proposed promising neural network models, they usually suffer from a shortage of training data. To address this issue, we expand the training data with various auxiliary argument mining corpora and propose an end-to-end cross-corpus training method called Multi-Task Argument Mining (MT-AM). To evaluate our approach, we conducted experiments for the main argument mining tasks on several well-established argument mining corpora. The results demonstrate that MT-AM generally outperformed the models trained on a single corpus. Also, the smaller the target corpus was, the better the MT-AM performed. Our extensive analyses suggest that the improvement of MT-AM depends on several factors of transferability among auxiliary and target corpora.</abstract>
      <pages>639–658</pages>
      <url hash="4a308122">2022.tacl-1.37</url>
      <bibkey>morio-etal-2022-end</bibkey>
    </paper>
    <paper id="38">
      <title>Is My Model Using the Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning</title>
      <author><first>Vivek</first><last>Gupta</last></author>
      <author><first>Riyaz A.</first><last>Bhat</last></author>
      <author><first>Atreya</first><last>Ghosal</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Maneesh</first><last>Singh</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <doi>10.1162/tacl_a_00482</doi>
      <abstract>Neural models command state-of-the-art performance across NLP tasks, including ones involving “reasoning”. Models claiming to reason about the evidence presented to them should attend to the correct parts of the input while avoiding spurious patterns therein, be self-consistent in their predictions across inputs, and be immune to biases derived from their pre-training in a nuanced, context- sensitive fashion. Do the prevalent *BERT- family of models do so? In this paper, we study this question using the problem of reasoning on tabular data. Tabular inputs are especially well-suited for the study—they admit systematic probes targeting the properties listed above. Our experiments demonstrate that a RoBERTa-based model, representative of the current state-of-the-art, fails at reasoning on the following counts: it (a) ignores relevant parts of the evidence, (b) is over- sensitive to annotation artifacts, and (c) relies on the knowledge encoded in the pre-trained language model rather than the evidence presented in its tabular inputs. Finally, through inoculation experiments, we show that fine- tuning the model on perturbed data does not help it overcome the above challenges.</abstract>
      <pages>659–679</pages>
      <url hash="a4f5040f">2022.tacl-1.38</url>
      <bibkey>gupta-etal-2022-model</bibkey>
      <video href="2022.tacl-1.38.mp4"/>
    </paper>
    <paper id="39">
      <title>Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Daniel</first><last>Beck</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Karin</first><last>Verspoor</last></author>
      <doi>10.1162/tacl_a_00483</doi>
      <abstract>State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization.</abstract>
      <pages>680–696</pages>
      <url hash="32b3b824">2022.tacl-1.39</url>
      <bibkey>wang-etal-2022-uncertainty</bibkey>
      <video href="2022.tacl-1.39.mp4"/>
    </paper>
    <paper id="40">
      <title>Data-to-text Generation with Variational Sequential Planning</title>
      <author><first>Ratish</first><last>Puduppully</last></author>
      <author><first>Yao</first><last>Fu</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/tacl_a_00484</doi>
      <abstract>We consider the task of data-to-text generation, which aims to create textual output from non-linguistic input. We focus on generating long-form text, that is, documents with multiple paragraphs, and propose a neural model enhanced with a planning component responsible for organizing high-level information in a coherent and meaningful way. We infer latent plans sequentially with a structured variational model, while interleaving the steps of planning and generation. Text is generated by conditioning on previous variational decisions and previously generated text. Experiments on two data-to-text benchmarks (RotoWire and MLB) show that our model outperforms strong baselines and is sample-efficient in the face of limited training data (e.g., a few hundred instances).</abstract>
      <pages>697–715</pages>
      <url hash="367d5927">2022.tacl-1.40</url>
      <bibkey>puduppully-etal-2022-data</bibkey>
    </paper>
    <paper id="41">
      <title>True Few-Shot Learning with <fixed-case>P</fixed-case>rompts—<fixed-case>A</fixed-case> Real-World Perspective</title>
      <author><first>Timo</first><last>Schick</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <doi>10.1162/tacl_a_00485</doi>
      <abstract>Prompt-based approaches excel at few-shot learning. However, Perez et al. (2021) recently cast doubt on their performance as they had difficulty getting good results in a “true” few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of Pet, a method that combines textual instructions with example-based finetuning. We show that, if correctly configured, Pet performs strongly in true few-shot settings without a dev set. Crucial for this strong performance is a number of design choices, including Pet’s ability to intelligently handle multiple prompts. We put our findings to a real-world test by running Pet on RAFT, a benchmark of tasks taken from realistic NLP applications for which no labeled dev or test sets are available. Pet achieves a new state of the art on RAFT and performs close to non-expert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners can successfully be applied in true few-shot settings and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities.</abstract>
      <pages>716–731</pages>
      <url hash="8be61109">2022.tacl-1.41</url>
      <bibkey>schick-schutze-2022-true</bibkey>
      <video href="2022.tacl-1.41.mp4"/>
    </paper>
    <paper id="42">
      <title>Heterogeneous Supervised Topic Models</title>
      <author><first>Dhanya</first><last>Sridhar</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <author><first>David</first><last>Blei</last></author>
      <doi>10.1162/tacl_a_00487</doi>
      <abstract>Researchers in the social sciences are often interested in the relationship between text and an outcome of interest, where the goal is to both uncover latent patterns in the text and predict outcomes for unseen texts. To this end, this paper develops the heterogeneous supervised topic model (HSTM), a probabilistic approach to text analysis and prediction. HSTMs posit a joint model of text and outcomes to find heterogeneous patterns that help with both text analysis and prediction. The main benefit of HSTMs is that they capture heterogeneity in the relationship between text and the outcome across latent topics. To fit HSTMs, we develop a variational inference algorithm based on the auto-encoding variational Bayes framework. We study the performance of HSTMs on eight datasets and find that they consistently outperform related methods, including fine-tuned black-box models. Finally, we apply HSTMs to analyze news articles labeled with pro- or anti-tone. We find evidence of differing language used to signal a pro- and anti-tone.</abstract>
      <pages>732–745</pages>
      <url hash="3246dc67">2022.tacl-1.42</url>
      <bibkey>sridhar-etal-2022-heterogeneous</bibkey>
      <video href="2022.tacl-1.42.mp4"/>
    </paper>
    <paper id="43">
      <title>Fact Checking with Insufficient Evidence</title>
      <author><first>Pepa</first><last>Atanasova</last></author>
      <author><first>Jakob Grue</first><last>Simonsen</last></author>
      <author><first>Christina</first><last>Lioma</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <doi>10.1162/tacl_a_00486</doi>
      <abstract>Automating the fact checking (FC) process relies on information obtained from external sources. In this work, we posit that it is crucial for FC models to make veracity predictions only when there is sufficient evidence and otherwise indicate when it is not enough. To this end, we are the first to study what information FC models consider sufficient by introducing a novel task and advancing it with three main contributions. First, we conduct an in-depth empirical analysis of the task with a new fluency-preserving method for omitting information from the evidence at the constituent and sentence level. We identify when models consider the remaining evidence (in)sufficient for FC, based on three trained models with different Transformer architectures and three FC datasets. Second, we ask annotators whether the omitted evidence was important for FC, resulting in a novel diagnostic dataset, SufficientFacts1, for FC with omitted evidence. We find that models are least successful in detecting missing evidence when adverbial modifiers are omitted (21% accuracy), whereas it is easiest for omitted date modifiers (63% accuracy). Finally, we propose a novel data augmentation strategy for contrastive self-learning of missing evidence by employing the proposed omission method combined with tri-training. It improves performance for Evidence Sufficiency Prediction by up to 17.8 F1 score, which in turn improves FC performance by up to 2.6 F1 score.</abstract>
      <pages>746–763</pages>
      <url hash="5b4de1c4">2022.tacl-1.43</url>
      <bibkey>atanasova-etal-2022-fact</bibkey>
      <video href="2022.tacl-1.43.mp4"/>
    </paper>
    <paper id="44">
      <title>Text-based <fixed-case>NP</fixed-case> Enrichment</title>
      <author><first>Yanai</first><last>Elazar</last></author>
      <author><first>Victoria</first><last>Basmov</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <doi>10.1162/tacl_a_00488</doi>
      <abstract>Understanding the relations between entities denoted by NPs in a text is a critical part of human-like natural language understanding. However, only a fraction of such relations is covered by standard NLP tasks and benchmarks nowadays. In this work, we propose a novel task termed text-based NP enrichment (TNE), in which we aim to enrich each NP in a text with all the preposition-mediated relations—either explicit or implicit—that hold between it and other NPs in the text. The relations are represented as triplets, each denoted by two NPs related via a preposition. Humans recover such relations seamlessly, while current state-of-the-art models struggle with them due to the implicit nature of the problem. We build the first large-scale dataset for the problem, provide the formal framing and scope of annotation, analyze the data, and report the results of fine-tuned language models on the task, demonstrating the challenge it poses to current technology. A webpage with a data-exploration UI, a demo, and links to the code, models, and leaderboard, to foster further research into this challenging problem can be found at: yanaiela.github.io/TNE/.</abstract>
      <pages>764–784</pages>
      <url hash="4a755db6">2022.tacl-1.44</url>
      <bibkey>elazar-etal-2022-text</bibkey>
      <video href="2022.tacl-1.44.mp4"/>
    </paper>
    <paper id="45">
      <title>Minimum Description Length Recurrent Neural Networks</title>
      <author><first>Nur</first><last>Lan</last></author>
      <author><first>Michal</first><last>Geyer</last></author>
      <author><first>Emmanuel</first><last>Chemla</last></author>
      <author><first>Roni</first><last>Katzir</last></author>
      <doi>10.1162/tacl_a_00489</doi>
      <abstract>We train neural networks to optimize a Minimum Description Length score, that is, to balance between the complexity of the network and its accuracy at a task. We show that networks optimizing this objective function master tasks involving memory challenges and go beyond context-free languages. These learners master languages such as anbn, anbncn, anb2n, anbmcn +m, and they perform addition. Moreover, they often do so with 100% accuracy. The networks are small, and their inner workings are transparent. We thus provide formal proofs that their perfect accuracy holds not only on a given test set, but for any input sequence. To our knowledge, no other connectionist model has been shown to capture the underlying grammars for these languages in full generality.</abstract>
      <pages>785–799</pages>
      <url hash="5aba475d">2022.tacl-1.45</url>
      <bibkey>lan-etal-2022-minimum</bibkey>
    </paper>
    <paper id="46">
      <title>Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity</title>
      <author><first>Yiding</first><last>Hao</last></author>
      <author><first>Dana</first><last>Angluin</last></author>
      <author><first>Robert</first><last>Frank</last></author>
      <doi>10.1162/tacl_a_00490</doi>
      <abstract>This paper analyzes three formal models of Transformer encoders that differ in the form of their self-attention mechanism: unique hard attention (UHAT); generalized unique hard attention (GUHAT), which generalizes UHAT; and averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers, viewed as string acceptors, can only recognize formal languages in the complexity class AC0, the class of languages recognizable by families of Boolean circuits of constant depth and polynomial size. This upper bound subsumes Hahn’s (2020) results that GUHAT cannot recognize the DYCK languages or the PARITY language, since those languages are outside AC0 (Furst et al., 1984). In contrast, the non-AC0 languages MAJORITY and DYCK-1 are recognizable by AHAT networks, implying that AHAT can recognize languages that UHAT and GUHAT cannot.</abstract>
      <pages>800–810</pages>
      <url hash="ac9625e6">2022.tacl-1.46</url>
      <bibkey>hao-etal-2022-formal</bibkey>
      <video href="2022.tacl-1.46.mp4"/>
    </paper>
    <paper id="47">
      <title>High Quality Rather than High Model Probability: Minimum <fixed-case>B</fixed-case>ayes Risk Decoding with Neural Metrics</title>
      <author><first>Markus</first><last>Freitag</last></author>
      <author><first>David</first><last>Grangier</last></author>
      <author><first>Qijun</first><last>Tan</last></author>
      <author><first>Bowen</first><last>Liang</last></author>
      <doi>10.1162/tacl_a_00491</doi>
      <abstract>In Neural Machine Translation, it is typically assumed that the sentence with the highest estimated probability should also be the translation with the highest quality as measured by humans. In this work, we question this assumption and show that model estimates and translation quality only vaguely correlate. We apply Minimum Bayes Risk (MBR) decoding on unbiased samples to optimize diverse automated metrics of translation quality as an alternative inference strategy to beam search. Instead of targeting the hypotheses with the highest model probability, MBR decoding extracts the hypotheses with the highest estimated quality. Our experiments show that the combination of a neural translation model with a neural reference-based metric, Bleurt, results in significant improvement in human evaluations. This improvement is obtained with translations different from classical beam-search output: These translations have much lower model likelihood and are less favored by surface metrics like Bleu.</abstract>
      <pages>811–825</pages>
      <url hash="68bc071c">2022.tacl-1.47</url>
      <bibkey>freitag-etal-2022-high</bibkey>
      <video href="2022.tacl-1.47.mp4"/>
    </paper>
    <paper id="48">
      <title>Generate, Annotate, and Learn: <fixed-case>NLP</fixed-case> with Synthetic Text</title>
      <author><first>Xuanli</first><last>He</last></author>
      <author><first>Islam</first><last>Nassar</last></author>
      <author><first>Jamie</first><last>Kiros</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Mohammad</first><last>Norouzi</last></author>
      <doi>10.1162/tacl_a_00492</doi>
      <abstract>This paper studies the use of language models as a source of synthetic unlabeled text for NLP. We formulate a general framework called “generate, annotate, and learn (GAL)” to take advantage of synthetic text within knowledge distillation, self-training, and few-shot learning applications. To generate high-quality task-specific text, we either fine-tune LMs on inputs from the task of interest, or prompt large LMs with few examples. We use the best available classifier to annotate synthetic text with soft pseudo labels for knowledge distillation and self-training, and use LMs to obtain hard labels for few-shot learning. We train new supervised models on the combination of labeled and pseudo-labeled data, which results in significant gains across several applications. We investigate key components of GAL and present theoretical and empirical arguments against the use of class-conditional LMs to generate synthetic labeled text instead of unlabeled text. GAL achieves new state-of-the-art knowledge distillation results for 6-layer transformers on the GLUE leaderboard.</abstract>
      <pages>826–842</pages>
      <url hash="97c89005">2022.tacl-1.48</url>
      <bibkey>he-etal-2022-generate</bibkey>
      <video href="2022.tacl-1.48.mp4"/>
    </paper>
    <paper id="49">
      <title>Saturated Transformers are Constant-Depth Threshold Circuits</title>
      <author><first>William</first><last>Merrill</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <doi>10.1162/tacl_a_00493</doi>
      <abstract>Transformers have become a standard neural network architecture for many NLP problems, motivating theoretical analysis of their power in terms of formal languages. Recent work has shown that transformers with hard attention are quite limited in power (Hahn, 2020), as they can be simulated by constant-depth AND/OR circuits (Hao et al., 2022). However, hard attention is a strong assumption, which may complicate the relevance of these results in practice. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We first show that saturated transformers transcend the known limitations of hard-attention transformers. We then prove saturated transformers with floating-point values can be simulated by constant-depth threshold circuits, giving the class TC0 as an upper bound on the formal languages they recognize.</abstract>
      <pages>843–856</pages>
      <url hash="fb928cf5">2022.tacl-1.49</url>
      <bibkey>merrill-etal-2022-saturated</bibkey>
      <video href="2022.tacl-1.49.mp4"/>
    </paper>
    <paper id="50">
      <title>Reducing Conversational Agents’ Overconfidence Through Linguistic Calibration</title>
      <author><first>Sabrina J.</first><last>Mielke</last></author>
      <author><first>Arthur</first><last>Szlam</last></author>
      <author><first>Emily</first><last>Dinan</last></author>
      <author><first>Y-Lan</first><last>Boureau</last></author>
      <doi>10.1162/tacl_a_00494</doi>
      <abstract>While improving neural dialogue agents’ factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model’s responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration.</abstract>
      <pages>857–872</pages>
      <url hash="bf6e919f">2022.tacl-1.50</url>
      <bibkey>mielke-etal-2022-reducing</bibkey>
      <video href="2022.tacl-1.50.mp4"/>
    </paper>
    <paper id="51">
      <title>A Survey of Text Games for Reinforcement Learning Informed by Natural Language</title>
      <author><first>Philip</first><last>Osborne</last></author>
      <author><first>Heido</first><last>Nõmm</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <doi>10.1162/tacl_a_00495</doi>
      <abstract>Reinforcement Learning has shown success in a number of complex virtual environments. However, many challenges still exist towards solving problems with natural language as a core component. Interactive Fiction Games (or Text Games) are one such problem type that offer a set of safe, partially observable environments where natural language is required as part of the Reinforcement Learning solution. Therefore, this survey’s aim is to assist in the development of new Text Game problem settings and solutions for Reinforcement Learning informed by natural language. Specifically, this survey: 1) introduces the challenges in Text Game Reinforcement Learning problems, 2) outlines the generation tools for rendering Text Games and the subsequent environments generated, and 3) compares the agent architectures currently applied to provide a systematic review of benchmark methodologies and opportunities for future researchers.</abstract>
      <pages>873–887</pages>
      <url hash="a29c0f0a">2022.tacl-1.51</url>
      <bibkey>osborne-etal-2022-survey</bibkey>
      <video href="2022.tacl-1.51.mp4"/>
    </paper>
    <paper id="52">
      <title>Dependency Parsing with Backtracking using Deep Reinforcement Learning</title>
      <author><first>Franck</first><last>Dary</last></author>
      <author><first>Maxime</first><last>Petit</last></author>
      <author><first>Alexis</first><last>Nasr</last></author>
      <doi>10.1162/tacl_a_00496</doi>
      <abstract>Greedy algorithms for NLP such as transition-based parsing are prone to error propagation. One way to overcome this problem is to allow the algorithm to backtrack and explore an alternative solution in cases where new evidence contradicts the solution explored so far. In order to implement such a behavior, we use reinforcement learning and let the algorithm backtrack in cases where such an action gets a better reward than continuing to explore the current solution. We test this idea on both POS tagging and dependency parsing and show that backtracking is an effective means to fight against error propagation.</abstract>
      <pages>888–903</pages>
      <url hash="0a79e150">2022.tacl-1.52</url>
      <bibkey>dary-etal-2022-dependency</bibkey>
    </paper>
    <paper id="53">
      <title>Temporal Effects on Pre-trained Models for Language Processing Tasks</title>
      <author><first>Oshin</first><last>Agarwal</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <doi>10.1162/tacl_a_00497</doi>
      <abstract>Keeping the performance of language technologies optimal as time passes is of great practical interest. We study temporal effects on model performance on downstream language tasks, establishing a nuanced terminology for such discussion and identifying factors essential to conduct a robust study. We present experiments for several tasks in English where the label correctness is not dependent on time and demonstrate the importance of distinguishing between temporal model deterioration and temporal domain adaptation for systems using pre-trained representations. We find that, depending on the task, temporal model deterioration is not necessarily a concern. Temporal domain adaptation, however, is beneficial in all cases, with better performance for a given time period possible when the system is trained on temporally more recent data. Therefore, we also examine the efficacy of two approaches for temporal domain adaptation without human annotations on new data. Self-labeling shows consistent improvement and notably, for named entity recognition, leads to better temporal adaptation than even human annotations.</abstract>
      <pages>904–921</pages>
      <url hash="89ccf909">2022.tacl-1.53</url>
      <bibkey>agarwal-nenkova-2022-temporal</bibkey>
    </paper>
    <paper id="54">
      <title>Learning <fixed-case>E</fixed-case>nglish with <fixed-case>P</fixed-case>eppa <fixed-case>P</fixed-case>ig</title>
      <author><first>Mitja</first><last>Nikolaus</last></author>
      <author><first>Afra</first><last>Alishahi</last></author>
      <author><first>Grzegorz</first><last>Chrupała</last></author>
      <doi>10.1162/tacl_a_00498</doi>
      <abstract>Recent computational models of the acquisition of spoken language via grounding in perception exploit associations between spoken and visual modalities and learn to represent speech and visual data in a joint vector space. A major unresolved issue from the point of ecological validity is the training data, typically consisting of images or videos paired with spoken descriptions of what is depicted. Such a setup guarantees an unrealistically strong correlation between speech and the visual data. In the real world the coupling between the linguistic and the visual modality is loose, and often confounded by correlations with non-semantic aspects of the speech signal. Here we address this shortcoming by using a dataset based on the children’s cartoon Peppa Pig. We train a simple bi-modal architecture on the portion of the data consisting of dialog between characters, and evaluate on segments containing descriptive narrations. Despite the weak and confounded signal in this training data, our model succeeds at learning aspects of the visual semantics of spoken language.</abstract>
      <pages>922–936</pages>
      <url hash="d76ef062">2022.tacl-1.54</url>
      <bibkey>nikolaus-etal-2022-learning</bibkey>
      <video href="2022.tacl-1.54.mp4"/>
    </paper>
    <paper id="55">
      <title>Compositional Generalization in Multilingual Semantic Parsing over <fixed-case>W</fixed-case>ikidata</title>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Rahul</first><last>Aralikatte</last></author>
      <author><first>Heather</first><last>Lent</last></author>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <doi>10.1162/tacl_a_00499</doi>
      <abstract>Semantic parsing (SP) allows humans to leverage vast knowledge resources through natural interaction. However, parsers are mostly designed for and evaluated on English resources, such as CFQ (Keysers et al., 2020), the current standard benchmark based on English data generated from grammar rules and oriented towards Freebase, an outdated knowledge base. We propose a method for creating a multilingual, parallel dataset of question-query pairs, grounded in Wikidata. We introduce such a dataset, which we call Multilingual Compositional Wikidata Questions (MCWQ), and use it to analyze the compositional generalization of semantic parsers in Hebrew, Kannada, Chinese, and English. While within- language generalization is comparable across languages, experiments on zero-shot cross- lingual transfer demonstrate that cross-lingual compositional generalization fails, even with state-of-the-art pretrained multilingual encoders. Furthermore, our methodology, dataset, and results will facilitate future research on SP in more realistic and diverse settings than has been possible with existing resources.</abstract>
      <pages>937–955</pages>
      <url hash="2daa1bd5">2022.tacl-1.55</url>
      <bibkey>cui-etal-2022-compositional</bibkey>
      <video href="2022.tacl-1.55.mp4"/>
    </paper>
    <paper id="56">
      <title>Adapting to the Long Tail: A Meta-Analysis of Transfer Learning Research for Language Understanding Tasks</title>
      <author><first>Aakanksha</first><last>Naik</last></author>
      <author><first>Jill</first><last>Lehman</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <doi>10.1162/tacl_a_00500</doi>
      <abstract>Natural language understanding (NLU) has made massive progress driven by large benchmarks, but benchmarks often leave a long tail of infrequent phenomena underrepresented. We reflect on the question: Have transfer learning methods sufficiently addressed the poor performance of benchmark-trained models on the long tail? We conceptualize the long tail using macro-level dimensions (underrepresented genres, topics, etc.), and perform a qualitative meta-analysis of 100 representative papers on transfer learning research for NLU. Our analysis asks three questions: (i) Which long tail dimensions do transfer learning studies target? (ii) Which properties of adaptation methods help improve performance on the long tail? (iii) Which methodological gaps have greatest negative impact on long tail performance? Our answers highlight major avenues for future research in transfer learning for the long tail. Lastly, using our meta-analysis framework, we perform a case study comparing the performance of various adaptation methods on clinical narratives, which provides interesting insights that may enable us to make progress along these future avenues.</abstract>
      <pages>956–980</pages>
      <url hash="c3e6a886">2022.tacl-1.56</url>
      <bibkey>naik-etal-2022-adapting</bibkey>
    </paper>
    <paper id="57">
      <title>How to Dissect a <fixed-case>M</fixed-case>uppet: The Structure of Transformer Embedding Spaces</title>
      <author><first>Timothee</first><last>Mickus</last></author>
      <author><first>Denis</first><last>Paperno</last></author>
      <author><first>Mathieu</first><last>Constant</last></author>
      <doi>10.1162/tacl_a_00501</doi>
      <abstract>Pretrained embeddings based on the Transformer architecture have taken the NLP community by storm. We show that they can mathematically be reframed as a sum of vector factors and showcase how to use this reframing to study the impact of each component. We provide evidence that multi-head attentions and feed-forwards are not equally useful in all downstream applications, as well as a quantitative overview of the effects of finetuning on the overall embedding space. This approach allows us to draw connections to a wide range of previous studies, from vector space anisotropy to attention weights.</abstract>
      <pages>981–996</pages>
      <url hash="57f74758">2022.tacl-1.57</url>
      <bibkey>mickus-etal-2022-dissect</bibkey>
      <video href="2022.tacl-1.57.mp4"/>
    </paper>
    <paper id="58">
      <title>On Decoding Strategies for Neural Text Generators</title>
      <author><first>Gian</first><last>Wiher</last></author>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <doi>10.1162/tacl_a_00502</doi>
      <abstract>When generating text from probabilistic models, the chosen decoding strategy has a profound effect on the resulting text. Yet the properties elicited by various decoding strategies do not always transfer across natural language generation tasks. For example, while mode-seeking methods like beam search perform remarkably well for machine translation, they have been observed to lead to incoherent and repetitive text in story generation. Despite such observations, the effectiveness of decoding strategies is often assessed on only a single task. This work—in contrast—provides a comprehensive analysis of the interaction between language generation tasks and decoding strategies. Specifically, we measure changes in attributes of generated text as a function of both decoding strategy and task using human and automatic evaluation. Our results reveal both previously observed and novel findings. For example, the nature of the diversity–quality trade-off in language generation is very task-specific; the length bias often attributed to beam search is not constant across tasks. <url>https://github.com/gianwiher/decoding-NLG</url></abstract>
      <pages>997–1012</pages>
      <url hash="8fb7789c">2022.tacl-1.58</url>
      <bibkey>wiher-etal-2022-decoding</bibkey>
    </paper>
    <paper id="59">
      <title><fixed-case>P</fixed-case>roo<fixed-case>FV</fixed-case>er: Natural Logic Theorem Proving for Fact Verification</title>
      <author><first>Amrith</first><last>Krishna</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <doi>10.1162/tacl_a_00503</doi>
      <abstract>Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best score in the FEVER leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.1</abstract>
      <pages>1013–1030</pages>
      <url hash="683a8870">2022.tacl-1.59</url>
      <bibkey>krishna-etal-2022-proofver</bibkey>
      <video href="2022.tacl-1.59.mp4"/>
    </paper>
    <paper id="60">
      <title>Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations</title>
      <author><first>Arabella</first><last>Sinclair</last></author>
      <author><first>Jaap</first><last>Jumelet</last></author>
      <author><first>Willem</first><last>Zuidema</last></author>
      <author><first>Raquel</first><last>Fernández</last></author>
      <doi>10.1162/tacl_a_00504</doi>
      <abstract>We investigate the extent to which modern neural language models are susceptible to structural priming, the phenomenon whereby the structure of a sentence makes the same structure more probable in a follow-up sentence. We explore how priming can be used to study the potential of these models to learn abstract structural information, which is a prerequisite for good performance on tasks that require natural language understanding skills. We introduce a novel metric and release Prime-LM, a large corpus where we control for various linguistic factors that interact with priming strength. We find that Transformer models indeed show evidence of structural priming, but also that the generalizations they learned are to some extent modulated by semantic information. Our experiments also show that the representations acquired by the models may not only encode abstract sequential structure but involve certain level of hierarchical syntactic information. More generally, our study shows that the priming paradigm is a useful, additional tool for gaining insights into the capacities of language models and opens the door to future priming-based investigations that probe the model’s internal states.1</abstract>
      <pages>1031–1050</pages>
      <url hash="ae75e122">2022.tacl-1.60</url>
      <bibkey>sinclair-etal-2022-structural</bibkey>
      <video href="2022.tacl-1.60.mp4"/>
    </paper>
    <paper id="61">
      <title><fixed-case>DP</fixed-case>-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon</title>
      <author><first>Robin</first><last>Algayres</last></author>
      <author><first>Tristan</first><last>Ricoul</last></author>
      <author><first>Julien</first><last>Karadayi</last></author>
      <author><first>Hugo</first><last>Laurençon</last></author>
      <author><first>Salah</first><last>Zaiem</last></author>
      <author><first>Abdelrahman</first><last>Mohamed</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <doi>10.1162/tacl_a_00505</doi>
      <abstract>Finding word boundaries in continuous speech is challenging as there is little or no equivalent of a ‘space’ delimiter between words. Popular Bayesian non-parametric models for text segmentation (Goldwater et al., 2006, 2009) use a Dirichlet process to jointly segment sentences and build a lexicon of word types. We introduce DP-Parse, which uses similar principles but only relies on an instance lexicon of word tokens, avoiding the clustering errors that arise with a lexicon of word types. On the Zero Resource Speech Benchmark 2017, our model sets a new speech segmentation state-of-the-art in 5 languages. The algorithm monotonically improves with better input representations, achieving yet higher scores when fed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can be pipelined to a language model and learn semantic and syntactic representations as assessed by a new spoken word embedding benchmark. 1</abstract>
      <pages>1051–1065</pages>
      <url hash="349f7b43">2022.tacl-1.61</url>
      <bibkey>algayres-etal-2022-dp</bibkey>
      <video href="2022.tacl-1.61.mp4"/>
    </paper>
    <paper id="62">
      <title>Evaluating Attribution in Dialogue Systems: The <fixed-case>BEGIN</fixed-case> Benchmark</title>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Hannah</first><last>Rashkin</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <author><first>David</first><last>Reitter</last></author>
      <doi>10.1162/tacl_a_00506</doi>
      <abstract>Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (Begin), comprising 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora. We collect human annotations assessing the extent to which the models’ responses can be attributed to the given background information. We then use Begin to analyze eight evaluation metrics. We find that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer. Our findings underscore the need for more sophisticated and robust evaluation metrics for knowledge-grounded dialogue. We make Begin publicly available at <url>https://github.com/google/BEGIN-dataset</url>.</abstract>
      <pages>1066–1083</pages>
      <url hash="c657c8d6">2022.tacl-1.62</url>
      <bibkey>dziri-etal-2022-evaluating</bibkey>
      <video href="2022.tacl-1.62.mp4"/>
    </paper>
    <paper id="63">
      <title>Modeling Non-Cooperative Dialogue: Theoretical and Empirical Insights</title>
      <author><first>Anthony</first><last>Sicilia</last></author>
      <author><first>Tristan</first><last>Maidment</last></author>
      <author><first>Pat</first><last>Healy</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <doi>10.1162/tacl_a_00507</doi>
      <abstract>Investigating cooperativity of interlocutors is central in studying pragmatics of dialogue. Models of conversation that only assume cooperative agents fail to explain the dynamics of strategic conversations. Thus, we investigate the ability of agents to identify non-cooperative interlocutors while completing a concurrent visual-dialogue task. Within this novel setting, we study the optimality of communication strategies for achieving this multi-task objective. We use the tools of learning theory to develop a theoretical model for identifying non-cooperative interlocutors and apply this theory to analyze different communication strategies. We also introduce a corpus of non-cooperative conversations about images in the GuessWhat?! dataset proposed by De Vries et al. (2017). We use reinforcement learning to implement multiple communication strategies in this context and find that empirical results validate our theory.</abstract>
      <pages>1084–1102</pages>
      <url hash="abe04ea7">2022.tacl-1.63</url>
      <bibkey>sicilia-etal-2022-modeling</bibkey>
      <video href="2022.tacl-1.63.mp4"/>
    </paper>
    <paper id="64">
      <title>Diff-Explainer: Differentiable Convex Optimization for Explainable Multi-hop Inference</title>
      <author><first>Mokanarangan</first><last>Thayaparan</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Deborah</first><last>Ferreira</last></author>
      <author><first>Julia</first><last>Rozanova</last></author>
      <author><first>André</first><last>Freitas</last></author>
      <doi>10.1162/tacl_a_00508</doi>
      <abstract>This paper presents Diff-Explainer, the first hybrid framework for explainable multi-hop inference that integrates explicit constraints with neural architectures through differentiable convex optimization. Specifically, Diff- Explainer allows for the fine-tuning of neural representations within a constrained optimization framework to answer and explain multi-hop questions in natural language. To demonstrate the efficacy of the hybrid framework, we combine existing ILP-based solvers for multi-hop Question Answering (QA) with Transformer-based representations. An extensive empirical evaluation on scientific and commonsense QA tasks demonstrates that the integration of explicit constraints in a end-to-end differentiable framework can significantly improve the performance of non- differentiable ILP solvers (8.91%–13.3%). Moreover, additional analysis reveals that Diff-Explainer is able to achieve strong performance when compared to standalone Transformers and previous multi-hop approaches while still providing structured explanations in support of its predictions.</abstract>
      <pages>1103–1119</pages>
      <url hash="413ee561">2022.tacl-1.64</url>
      <bibkey>thayaparan-etal-2022-diff</bibkey>
      <video href="2022.tacl-1.64.mp4"/>
    </paper>
    <paper id="65">
      <title>Getting <fixed-case>BART</fixed-case> to Ride the Idiomatic Train: Learning to Represent Idiomatic Expressions</title>
      <author><first>Ziheng</first><last>Zeng</last></author>
      <author><first>Suma</first><last>Bhat</last></author>
      <doi>10.1162/tacl_a_00510</doi>
      <abstract>Idiomatic expressions (IEs), characterized by their non-compositionality, are an important part of natural language. They have been a classical challenge to NLP, including pre-trained language models that drive today’s state-of-the-art. Prior work has identified deficiencies in their contextualized representation stemming from the underlying compositional paradigm of representation. In this work, we take a first-principles approach to build idiomaticity into BART using an adapter as a lightweight non-compositional language expert trained on idiomatic sentences. The improved capability over baselines (e.g., BART) is seen via intrinsic and extrinsic methods, where idiom embeddings score 0.19 points higher in homogeneity score for embedding clustering, and up to 25% higher sequence accuracy on the idiom processing tasks of IE sense disambiguation and span detection.</abstract>
      <pages>1120–1137</pages>
      <url hash="122991d4">2022.tacl-1.65</url>
      <bibkey>zeng-bhat-2022-getting</bibkey>
    </paper>
    <paper id="66">
      <title>Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond</title>
      <author><first>Amir</first><last>Feder</last></author>
      <author><first>Katherine A.</first><last>Keith</last></author>
      <author><first>Emaad</first><last>Manzoor</last></author>
      <author><first>Reid</first><last>Pryzant</last></author>
      <author><first>Dhanya</first><last>Sridhar</last></author>
      <author><first>Zach</first><last>Wood-Doughty</last></author>
      <author><first>Jacob</first><last>Eisenstein</last></author>
      <author><first>Justin</first><last>Grimmer</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <author><first>Margaret E.</first><last>Roberts</last></author>
      <author><first>Brandon M.</first><last>Stewart</last></author>
      <author><first>Victor</first><last>Veitch</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <doi>10.1162/tacl_a_00511</doi>
      <abstract>A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.1</abstract>
      <pages>1138–1158</pages>
      <url hash="12fe2cad">2022.tacl-1.66</url>
      <bibkey>feder-etal-2022-causal</bibkey>
      <video href="2022.tacl-1.66.mp4"/>
    </paper>
    <paper id="67">
      <title>Learning Fair Representations via Rate-Distortion Maximization</title>
      <author><first>Somnath Basu Roy</first><last>Chowdhury</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <doi>10.1162/tacl_a_00512</doi>
      <abstract>Text representations learned by machine learning models often encode undesirable demographic information of the user. Predictive models based on these representations can rely on such information, resulting in biased decisions. We present a novel debiasing technique, Fairness-aware Rate Maximization (FaRM), that removes protected information by making representations of instances belonging to the same protected attribute class uncorrelated, using the rate-distortion function. FaRM is able to debias representations with or without a target task at hand. FaRM can also be adapted to remove information about multiple protected attributes simultaneously. Empirical evaluations show that FaRM achieves state-of-the-art performance on several datasets, and learned representations leak significantly less protected attribute information against an attack by a non-linear probing network.</abstract>
      <pages>1159–1174</pages>
      <url hash="df9e97d1">2022.tacl-1.67</url>
      <bibkey>chowdhury-chaturvedi-2022-learning</bibkey>
      <video href="2022.tacl-1.67.mp4"/>
    </paper>
    <paper id="68">
      <title>Robust Dialogue State Tracking with Weak Supervision and Sparse Data</title>
      <author><first>Michael</first><last>Heck</last></author>
      <author><first>Nurul</first><last>Lubis</last></author>
      <author><first>Carel</first><last>van Niekerk</last></author>
      <author><first>Shutong</first><last>Feng</last></author>
      <author><first>Christian</first><last>Geishauser</last></author>
      <author><first>Hsien-Chin</first><last>Lin</last></author>
      <author><first>Milica</first><last>Gašić</last></author>
      <doi>10.1162/tacl_a_00513</doi>
      <abstract>Generalizing dialogue state tracking (DST) to new data is especially challenging due to the strong reliance on abundant and fine-grained supervision during training. Sample sparsity, distributional shift, and the occurrence of new concepts and topics frequently lead to severe performance degradation during inference. In this paper we propose a training strategy to build extractive DST models without the need for fine-grained manual span labels. Two novel input-level dropout methods mitigate the negative impact of sample sparsity. We propose a new model architecture with a unified encoder that supports value as well as slot independence by leveraging the attention mechanism. We combine the strengths of triple copy strategy DST and value matching to benefit from complementary predictions without violating the principle of ontology independence. Our experiments demonstrate that an extractive DST model can be trained without manual span labels. Our architecture and training strategies improve robustness towards sample sparsity, new concepts, and topics, leading to state-of-the-art performance on a range of benchmarks. We further highlight our model’s ability to effectively learn from non-dialogue data.</abstract>
      <pages>1175–1192</pages>
      <url hash="d9f91028">2022.tacl-1.68</url>
      <bibkey>heck-etal-2022-robust</bibkey>
    </paper>
    <paper id="69">
      <title>Unit Testing for Concepts in Neural Networks</title>
      <author><first>Charles</first><last>Lovering</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <doi>10.1162/tacl_a_00514</doi>
      <abstract>Many complex problems are naturally understood in terms of symbolic concepts. For example, our concept of “cat” is related to our concepts of “ears” and “whiskers” in a non-arbitrary way. Fodor (1998) proposes one theory of concepts, which emphasizes symbolic representations related via constituency structures. Whether neural networks are consistent with such a theory is open for debate. We propose unit tests for evaluating whether a system’s behavior is consistent with several key aspects of Fodor’s criteria. Using a simple visual concept learning task, we evaluate several modern neural architectures against this specification. We find that models succeed on tests of groundedness, modularity, and reusability of concepts, but that important questions about causality remain open. Resolving these will require new methods for analyzing models’ internal states.</abstract>
      <pages>1193–1208</pages>
      <url hash="5f5e676b">2022.tacl-1.69</url>
      <bibkey>lovering-pavlick-2022-unit</bibkey>
      <video href="2022.tacl-1.69.mp4"/>
    </paper>
    <paper id="70">
      <title>Multi-task Active Learning for Pre-trained Transformer-based Models</title>
      <author><first>Guy</first><last>Rotman</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <doi>10.1162/tacl_a_00515</doi>
      <abstract>Multi-task learning, in which several tasks are jointly learned by a single model, allows NLP models to share information from multiple annotations and may facilitate better predictions when the tasks are inter-related. This technique, however, requires annotating the same text with multiple annotation schemes, which may be costly and laborious. Active learning (AL) has been demonstrated to optimize annotation processes by iteratively selecting unlabeled examples whose annotation is most valuable for the NLP model. Yet, multi-task active learning (MT-AL) has not been applied to state-of-the-art pre-trained Transformer-based NLP models. This paper aims to close this gap. We explore various multi-task selection criteria in three realistic multi-task scenarios, reflecting different relations between the participating tasks, and demonstrate the effectiveness of multi-task compared to single-task selection. Our results suggest that MT-AL can be effectively used in order to minimize annotation efforts for multi-task NLP models.1</abstract>
      <pages>1209–1228</pages>
      <url hash="71d079fe">2022.tacl-1.70</url>
      <bibkey>rotman-reichart-2022-multi</bibkey>
      <video href="2022.tacl-1.70.mp4"/>
    </paper>
    <paper id="71">
      <title>Template-based Abstractive Microblog Opinion Summarization</title>
      <author><first>Iman Munire</first><last>Bilal</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Adam</first><last>Tsakalidis</last></author>
      <author><first>Dong</first><last>Nguyen</last></author>
      <author><first>Rob</first><last>Procter</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <doi>10.1162/tacl_a_00516</doi>
      <abstract>We introduce the task of microblog opinion summarization (MOS) and share a dataset of 3100 gold-standard opinion summaries to facilitate research in this domain. The dataset contains summaries of tweets spanning a 2-year period and covers more topics than any other public Twitter summarization dataset. Summaries are abstractive in nature and have been created by journalists skilled in summarizing news articles following a template separating factual information (main story) from author opinions. Our method differs from previous work on generating gold-standard summaries from social media, which usually involves selecting representative posts and thus favors extractive summarization models. To showcase the dataset’s utility and challenges, we benchmark a range of abstractive and extractive state-of-the-art summarization models and achieve good performance, with the former outperforming the latter. We also show that fine-tuning is necessary to improve performance and investigate the benefits of using different sample sizes.</abstract>
      <pages>1229–1248</pages>
      <url hash="eccbf65e">2022.tacl-1.71</url>
      <bibkey>bilal-etal-2022-template</bibkey>
      <video href="2022.tacl-1.71.mp4"/>
    </paper>
    <paper id="72">
      <title>Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation</title>
      <author><first>Zejiang</first><last>Hou</last></author>
      <author><first>Julian</first><last>Salazar</last></author>
      <author><first>George</first><last>Polovets</last></author>
      <doi>10.1162/tacl_a_00517</doi>
      <abstract>Large pretrained language models (PLMs) are often domain- or task-adapted via finetuning or prompting. Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance. Instead, we prepare PLMs for data- and parameter-efficient adaptation by learning to learn the difference between general and adapted PLMs. This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller. Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining. Ablations show our task-adaptive reparameterization (TARP) and model search (TAMS) components individually improve on other parameter-efficient transfer like adapters and structure-learning methods like learned sparsification.</abstract>
      <pages>1249–1265</pages>
      <url hash="c0d0826b">2022.tacl-1.72</url>
      <bibkey>hou-etal-2022-meta</bibkey>
      <video href="2022.tacl-1.72.mp4"/>
    </paper>
    <paper id="73">
      <title>Compositional Evaluation on <fixed-case>J</fixed-case>apanese Textual Entailment and Similarity</title>
      <author><first>Hitomi</first><last>Yanaka</last></author>
      <author><first>Koji</first><last>Mineshima</last></author>
      <doi>10.1162/tacl_a_00518</doi>
      <abstract>Natural Language Inference (NLI) and Semantic Textual Similarity (STS) are widely used benchmark tasks for compositional evaluation of pre-trained language models. Despite growing interest in linguistic universals, most NLI/STS studies have focused almost exclusively on English. In particular, there are no available multilingual NLI/STS datasets in Japanese, which is typologically different from English and can shed light on the currently controversial behavior of language models in matters such as sensitivity to word order and case particles. Against this background, we introduce JSICK, a Japanese NLI/STS dataset that was manually translated from the English dataset SICK. We also present a stress-test dataset for compositional inference, created by transforming syntactic structures of sentences in JSICK to investigate whether language models are sensitive to word order and case particles. We conduct baseline experiments on different pre-trained language models and compare the performance of multilingual models when applied to Japanese and other languages. The results of the stress-test experiments suggest that the current pre-trained language models are insensitive to word order and case marking.</abstract>
      <pages>1266–1284</pages>
      <url hash="7235c565">2022.tacl-1.73</url>
      <bibkey>yanaka-mineshima-2022-compositional</bibkey>
      <video href="2022.tacl-1.73.mp4"/>
    </paper>
    <paper id="74">
      <title>Neuron-level Interpretation of Deep <fixed-case>NLP</fixed-case> Models: A Survey</title>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Fahim</first><last>Dalvi</last></author>
      <doi>10.1162/tacl_a_00519</doi>
      <abstract>The proliferation of Deep Neural Networks in various domains has seen an increased need for interpretability of these models. Preliminary work done along this line, and papers that surveyed such, are focused on high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level of analyzing neurons within these models. In this paper, we survey the work done on neuron analysis including: i) methods to discover and understand neurons in a network; ii) evaluation methods; iii) major findings including cross architectural comparisons that neuron analysis has unraveled; iv) applications of neuron probing such as: controlling the model, domain adaptation, and so forth; and v) a discussion on open issues and future research directions.</abstract>
      <pages>1285–1303</pages>
      <url hash="87add379">2022.tacl-1.74</url>
      <bibkey>sajjad-etal-2022-neuron</bibkey>
      <video href="2022.tacl-1.74.mp4"/>
    </paper>
    <paper id="75">
      <title>A Survey on Cross-Lingual Summarization</title>
      <author><first>Jiaan</first><last>Wang</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Duo</first><last>Zheng</last></author>
      <author><first>Yunlong</first><last>Liang</last></author>
      <author><first>Zhixu</first><last>Li</last></author>
      <author><first>Jianfeng</first><last>Qu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <doi>10.1162/tacl_a_00520</doi>
      <abstract>Cross-lingual summarization is the task of generating a summary in one language (e.g., English) for the given document(s) in a different language (e.g., Chinese). Under the globalization background, this task has attracted increasing attention of the computational linguistics community. Nevertheless, there still remains a lack of comprehensive review for this task. Therefore, we present the first systematic critical review on the datasets, approaches, and challenges in this field. Specifically, we carefully organize existing datasets and approaches according to different construction methods and solution paradigms, respectively. For each type of dataset or approach, we thoroughly introduce and summarize previous efforts and further compare them with each other to provide deeper analyses. In the end, we also discuss promising directions and offer our thoughts to facilitate future research. This survey is for both beginners and experts in cross-lingual summarization, and we hope it will serve as a starting point as well as a source of new ideas for researchers and engineers interested in this area.</abstract>
      <pages>1304–1323</pages>
      <url hash="38632f3e">2022.tacl-1.75</url>
      <bibkey>wang-etal-2022-survey</bibkey>
    </paper>
    <paper id="76">
      <title>An End-to-End Contrastive Self-Supervised Learning Framework for Language Understanding</title>
      <author><first>Hongchao</first><last>Fang</last></author>
      <author><first>Pengtao</first><last>Xie</last></author>
      <doi>10.1162/tacl_a_00521</doi>
      <abstract>Self-supervised learning (SSL) methods such as Word2vec, BERT, and GPT have shown great effectiveness in language understanding. Contrastive learning, as a recent SSL approach, has attracted increasing attention in NLP. Contrastive learning learns data representations by predicting whether two augmented data instances are generated from the same original data example. Previous contrastive learning methods perform data augmentation and contrastive learning separately. As a result, the augmented data may not be optimal for contrastive learning. To address this problem, we propose a four-level optimization framework that performs data augmentation and contrastive learning end-to-end, to enable the augmented data to be tailored to the contrastive learning task. This framework consists of four learning stages, including training machine translation models for sentence augmentation, pretraining a text encoder using contrastive learning, finetuning a text classification model, and updating weights of translation data by minimizing the validation loss of the classification model, which are performed in a unified way. Experiments on datasets in the GLUE benchmark (Wang et al., 2018a) and on datasets used in Gururangan et al. (2020) demonstrate the effectiveness of our method.</abstract>
      <pages>1324–1340</pages>
      <url hash="91ec4878">2022.tacl-1.76</url>
      <bibkey>fang-xie-2022-end</bibkey>
    </paper>
    <paper id="77">
      <title>Draw Me a Flower: Processing and Grounding Abstraction in Natural Language</title>
      <author><first>Royi</first><last>Lachmy</last></author>
      <author><first>Valentina</first><last>Pyatkin</last></author>
      <author><first>Avshalom</first><last>Manevich</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <doi>10.1162/tacl_a_00522</doi>
      <abstract>Abstraction is a core tenet of human cognition and communication. When composing natural language instructions, humans naturally evoke abstraction to convey complex procedures in an efficient and concise way. Yet, interpreting and grounding abstraction expressed in NL has not yet been systematically studied in NLP, with no accepted benchmarks specifically eliciting abstraction in NL. In this work, we set the foundation for a systematic study of processing and grounding abstraction in NLP. First, we deliver a novel abstraction elicitation method and present Hexagons, a 2D instruction-following game. Using Hexagons we collected over 4k naturally occurring visually-grounded instructions rich with diverse types of abstractions. From these data, we derive an instruction-to-execution task and assess different types of neural models. Our results show that contemporary models and modeling practices are substantially inferior to human performance, and that model performance is inversely correlated with the level of abstraction, showing less satisfying performance on higher levels of abstraction. These findings are consistent across models and setups, confirming that abstraction is a challenging phenomenon deserving further attention and study in NLP/AI research.</abstract>
      <pages>1341–1356</pages>
      <url hash="5da10618">2022.tacl-1.77</url>
      <bibkey>lachmy-etal-2022-draw</bibkey>
    </paper>
    <paper id="78">
      <title>Investigating Reasons for Disagreement in Natural Language Inference</title>
      <author><first>Nan-Jiang</first><last>Jiang</last></author>
      <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
      <doi>10.1162/tacl_a_00523</doi>
      <abstract>We investigate how disagreement in natural language inference (NLI) annotation arises. We developed a taxonomy of disagreement sources with 10 categories spanning 3 high- level classes. We found that some disagreements are due to uncertainty in the sentence meaning, others to annotator biases and task artifacts, leading to different interpretations of the label distribution. We explore two modeling approaches for detecting items with potential disagreement: a 4-way classification with a “Complicated” label in addition to the three standard NLI labels, and a multilabel classification approach. We found that the multilabel classification is more expressive and gives better recall of the possible interpretations in the data.</abstract>
      <pages>1357–1374</pages>
      <url hash="99c5b524">2022.tacl-1.78</url>
      <bibkey>jiang-marneffe-2022-investigating</bibkey>
    </paper>
    <paper id="79">
      <title>The Emergence of Argument Structure in Artificial Languages</title>
      <author><first>Tom</first><last>Bosc</last></author>
      <author><first>Pascal</first><last>Vincent</last></author>
      <doi>10.1162/tacl_a_00524</doi>
      <abstract>Computational approaches to the study of language emergence can help us understand how natural languages are shaped by cognitive and sociocultural factors. Previous work focused on tasks where agents refer to a single entity. In contrast, we study how agents predicate, that is, how they express that some relation holds between several entities. We introduce a setup where agents talk about a variable number of entities that can be partially observed by the listener. In the presence of a least-effort pressure, they tend to discuss only entities that are not observed by the listener. Thus we can obtain artificial phrases that denote a single entity, as well as artificial sentences that denote several entities. In natural languages, if we ignore the verb, phrases are usually concatenated, either in a specific order or by adding case markers to form sentences. Our setup allows us to quantify how much this holds in emergent languages using a metric we call concatenability. We also measure transitivity, which quantifies the importance of word order. We demonstrate the usefulness of this new setup and metrics for studying factors that influence argument structure. We compare agents having access to input representations structured into pre-segmented objects with properties, versus unstructured representations. Our results indicate that the awareness of object structure yields a more natural sentence organization.</abstract>
      <pages>1375–1391</pages>
      <url hash="e1cc7104">2022.tacl-1.79</url>
      <bibkey>bosc-vincent-2022-emergence</bibkey>
    </paper>
    <paper id="80">
      <title>Scientia Potentia <fixed-case>E</fixed-case>st—<fixed-case>O</fixed-case>n the Role of Knowledge in Computational Argumentation</title>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <doi>10.1162/tacl_a_00525</doi>
      <abstract>Despite extensive research efforts in recent years, computational argumentation (CA) remains one of the most challenging areas of natural language processing. The reason for this is the inherent complexity of the cognitive processes behind human argumentation, which integrate a plethora of different types of knowledge, ranging from topic-specific facts and common sense to rhetorical knowledge. The integration of knowledge from such a wide range in CA requires modeling capabilities far beyond many other natural language understanding tasks. Existing research on mining, assessing, reasoning over, and generating arguments largely acknowledges that much more knowledge is needed to accurately model argumentation computationally. However, a systematic overview of the types of knowledge introduced in existing CA models is missing, hindering targeted progress in the field. Adopting the operational definition of knowledge as any task-relevant normative information not provided as input, the survey paper at hand fills this gap by (1) proposing a taxonomy of types of knowledge required in CA tasks, (2) systematizing the large body of CA work according to the reliance on and exploitation of these knowledge types for the four main research areas in CA, and (3) outlining and discussing directions for future research efforts in CA.</abstract>
      <pages>1392–1422</pages>
      <url hash="6707ec38">2022.tacl-1.80</url>
      <bibkey>lauscher-etal-2022-scientia</bibkey>
    </paper>
    <paper id="81">
      <title>Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale</title>
      <author><first>Laurent</first><last>Sartran</last></author>
      <author><first>Samuel</first><last>Barrett</last></author>
      <author><first>Adhiguna</first><last>Kuncoro</last></author>
      <author><first>Miloš</first><last>Stanojević</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <author><first>Chris</first><last>Dyer</last></author>
      <doi>10.1162/tacl_a_00526</doi>
      <abstract>We introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism—one that is independent of composed syntactic representations—plays an important role in current successful models of long text.</abstract>
      <pages>1423–1439</pages>
      <url hash="c5467521">2022.tacl-1.81</url>
      <bibkey>sartran-etal-2022-transformer</bibkey>
    </paper>
    <paper id="82">
      <title>Explainable Abuse Detection as Intent Classification and Slot Filling</title>
      <author><first>Agostina</first><last>Calabrese</last></author>
      <author><first>Björn</first><last>Ross</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <doi>10.1162/tacl_a_00527</doi>
      <abstract>To proactively offer social media users a safe online experience, there is a need for systems that can detect harmful posts and promptly alert platform moderators. In order to guarantee the enforcement of a consistent policy, moderators are provided with detailed guidelines. In contrast, most state-of-the-art models learn what abuse is from labeled examples and as a result base their predictions on spurious cues, such as the presence of group identifiers, which can be unreliable. In this work we introduce the concept of policy-aware abuse detection, abandoning the unrealistic expectation that systems can reliably learn which phenomena constitute abuse from inspecting the data alone. We propose a machine-friendly representation of the policy that moderators wish to enforce, by breaking it down into a collection of intents and slots. We collect and annotate a dataset of 3,535 English posts with such slots, and show how architectures for intent classification and slot filling can be used for abuse detection, while providing a rationale for model decisions.1</abstract>
      <pages>1440–1454</pages>
      <url hash="e169bbaf">2022.tacl-1.82</url>
      <bibkey>calabrese-etal-2022-explainable</bibkey>
    </paper>
    <paper id="83">
      <title>Morphology Without Borders: Clause-Level Morphology</title>
      <author><first>Omer</first><last>Goldman</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <doi>10.1162/tacl_a_00528</doi>
      <abstract>Morphological tasks use large multi-lingual datasets that organize words into inflection tables, which then serve as training and evaluation data for various tasks. However, a closer inspection of these data reveals profound cross-linguistic inconsistencies, which arise from the lack of a clear linguistic and operational definition of what is a word, and which severely impair the universality of the derived tasks. To overcome this deficiency, we propose to view morphology as a clause-level phenomenon, rather than word-level. It is anchored in a fixed yet inclusive set of features, that encapsulates all functions realized in a saturated clause. We deliver MightyMorph, a novel dataset for clause-level morphology covering 4 typologically different languages: English, German, Turkish, and Hebrew. We use this dataset to derive 3 clause-level morphological tasks: inflection, reinflection and analysis. Our experiments show that the clause-level tasks are substantially harder than the respective word-level tasks, while having comparable complexity across languages. Furthermore, redefining morphology to the clause-level provides a neat interface with contextualized language models (LMs) and allows assessing the morphological knowledge encoded in these models and their usability for morphological tasks. Taken together, this work opens up new horizons in the study of computational morphology, leaving ample space for studying neural morphology cross-linguistically.</abstract>
      <pages>1455–1472</pages>
      <url hash="e5c88236">2022.tacl-1.83</url>
      <bibkey>goldman-tsarfaty-2022-morphology</bibkey>
    </paper>
    <paper id="84">
      <title><fixed-case>F</fixed-case>aith<fixed-case>D</fixed-case>ial: A Faithful Benchmark for Information-Seeking Dialogue</title>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Ehsan</first><last>Kamalloo</last></author>
      <author><first>Sivan</first><last>Milton</last></author>
      <author><first>Osmar</first><last>Zaiane</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Edoardo M.</first><last>Ponti</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <doi>10.1162/tacl_a_00529</doi>
      <abstract>The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 12.8 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.</abstract>
      <pages>1473–1490</pages>
      <url hash="483de8d1">2022.tacl-1.84</url>
      <bibkey>dziri-etal-2022-faithdial</bibkey>
      <video href="2022.tacl-1.84.mp4"/>
    </paper>
  </volume>
</collection>
