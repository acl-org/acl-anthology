<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.luhme">
  <volume id="1" ingest-date="2025-11-14" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 2nd LUHME Workshop</booktitle>
      <editor><first>Henrique Lopes</first><last>Cardoso</last></editor>
      <editor><first>Rui</first><last>Sousa-Silva</last></editor>
      <editor><first>Maarit</first><last>Koponen</last></editor>
      <editor><first>Antonio</first><last>Pareja-Lora</last></editor>
      <publisher>UP - Universidade do Porto (https://doi.org/10.21747/978-989-9193-73-4/lan2), LIACC - Laboratório de Inteligência Artificial e Ciência de Computadores da Universidade do Porto, CLUP - Centro de Linguística da Universidade do Porto, UEF - The University of Eastern Finland and UAH - Universidad de Alcalá.</publisher>
      <address>Bologna, Italy</address>
      <month>October</month>
      <year>2025</year>
      <url hash="e3ee30ee">2025.luhme-1</url>
      <venue>luhme</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="2a22eaa4">2025.luhme-1.0</url>
      <bibkey>luhme-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Understanding Social Interactions in the Era of <fixed-case>LLM</fixed-case>s – the Challenges of Transparency</title>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>2–2</pages>
      <abstract>Research on AI and social interaction is not entirely new — it falls within the field of social and affective computing, which emerged in the late 1990s. To understand social interactions, the research community has long drawn on both artificial intelligence and social science. In recent years, however, the field has shifted toward a dominant focus on generative large language models (LLMs). These models are undeniably powerful but often opaque. In this talk, I will present our current work on developing machine learning approaches — from classical methods to LLMs — for modeling the socio-emotional layer of interaction, with a particular focus on improving model transparency. I will also briefly present some of the applications we are developing to support human skill development, particularly in the fields of education and health.</abstract>
      <url hash="923797b7">2025.luhme-1.1</url>
      <bibkey>clavel-2025-understanding</bibkey>
    </paper>
    <paper id="2">
      <title>Building Common Ground in Dialogue: A Survey</title>
      <author><first>Tatiana</first><last>Anikina</last></author>
      <author><first>Alina</first><last>Leippert</last></author>
      <author><first>Simon</first><last>Ostermann</last></author>
      <pages>3–28</pages>
      <abstract>Common ground plays a crucial role in human communication and the grounding process helps to establish shared knowledge. However, common ground is also a heavily loaded term that may be interpreted in different ways depending on the context. The scope of common ground ranges from domain-specific and personal shared experiences to common sense knowledge. Representationally, common ground can be uni- or multi-modal, and static or dynamic. In this survey, we attempt to systematize different facets of common ground in dialogue and position it within the current landscape of NLP research that often relies on the usage of language models (LMs) and task-specific short-term interactions. We outline different dimensions of common ground and describe modeling approaches for several grounding tasks, discuss issues caused by the lack of common ground in human-LM interactions, and suggest future research directions. This survey serves as a roadmap of what to pay attention to when equipping a dialogue system with grounding capabilities and provides a summary of current research on grounding in dialogue, categorizing 448 papers and compiling a list of the available datasets.</abstract>
      <url hash="2ae6c557">2025.luhme-1.2</url>
      <bibkey>anikina-etal-2025-building</bibkey>
    </paper>
    <paper id="3">
      <title>Do Large Language Models Understand Morality Across Cultures?</title>
      <author><first>Hadi</first><last>Mohammadi</last></author>
      <author><first>Yasmeen F. S. S.</first><last>Meijer</last></author>
      <author><first>Efthymia</first><last>Papadopoulou</last></author>
      <author><first>Ayoub</first><last>Bagheri</last></author>
      <pages>30–39</pages>
      <abstract>Recent advancements in large language models (LLMs) have established them as powerful tools across numerous domains. However, persistent concerns about embedded biases, such as gender, racial, and cultural biases arising from their training data, raise significant questions about the ethical use and societal consequences of these technologies. This study investigates the extent to which LLMs capture cross-cultural differences and similarities in moral perspectives. Specifically, we examine whether LLM outputs align with patterns observed in international survey data on moral attitudes. To this end, we employ three complementary methods: (1) comparing variances in moral scores produced by models versus those reported in surveys, (2) conducting cluster alignment analyses to assess correspondence between country groupings derived from LLM outputs and survey data, and (3) directly probing models with comparative prompts using systematically chosen token pairs. Our results reveal that current LLMs often fail to reproduce the full spectrum of cross-cultural moral variation, tending to compress differences and exhibit low alignment with empirical survey patterns. These findings highlight a pressing need for more robust approaches to mitigate biases and improve cultural representativeness in LLMs. We conclude by discussing the implications for the responsible development and global deployment of LLMs, emphasizing fairness and ethical alignment.</abstract>
      <url hash="93b8d944">2025.luhme-1.3</url>
      <bibkey>mohammadi-etal-2025-large</bibkey>
    </paper>
    <paper id="4">
      <title>A Nightmare on <fixed-case>LLM</fixed-case>s Street: On the Importance of Cultural Awareness in Text Adaptation for <fixed-case>LRL</fixed-case>s</title>
      <author><first>David C. T.</first><last>Freitas</last></author>
      <author><first>Henrique Lopes</first><last>Cardoso</last></author>
      <pages>40–48</pages>
      <abstract>Large Language Models (LLMs) have revolutionized how we generate, interact with, and process language. Still, these models are biased toward WEIRD (Western, Educated, Industrialized, Rich, and Democratic) values. This bias is not merely linguistic but also cultural. Sociocultural contexts influence how people express ideas, interpret meaning, and communicate. In low-resource language settings, where data and cultural representation are limited, this issue becomes even more pronounced when models are applied without cultural adaptation, often leading to outputs that are irrelevant, inaccessible, or even harmful. In this paper, we argue for the importance of incorporating sociocultural context into LLMs. We review existing frameworks that explore culture in Natural Language Processing (NLP), and examine some work aimed at culturally aligning language models. As an illustrative scenario, we analyze the case of Guinea-Bissau. In this linguistically and culturally diverse country, Portuguese is the official language but not the primary means of communication for most of the population, highlighting the urgent need to adapt educational materials to the local sociocultural context. Finally, we propose a revised framework to address the challenge of adapting educational materials to diverse contexts, aiming to improve both the relevance and pedagogical impact of text adaptation.</abstract>
      <url hash="890e60cb">2025.luhme-1.4</url>
      <bibkey>freitas-cardoso-2025-nightmare</bibkey>
    </paper>
    <paper id="5">
      <title>Terminologists as Stewards of Meaning in the Age of <fixed-case>LLM</fixed-case>s: A Digital Humanism Perspective</title>
      <author><first>Barbara</first><last>Heinisch</last></author>
      <pages>49–56</pages>
      <abstract>Digital Humanism calls for a reconfiguration of the development of digital technologies that embeds interdisciplinary collaboration, ethical reflexivity and critical scrutiny into both the design and evaluation of these systems. From a Digital Humanism perspective, terminologists play a vital role in safeguarding language understanding in specialized domains where clarity and consistency are critical (in both monolingual and multilingual contexts). This conceptual paper, therefore, examines the role of terminologists (and terminology) in the era of LLMs, with a focus on their function as stewards of meaning in specialized communication. The study draws on the principles of Digital Humanism to critically assess how terminologists can counteract various ethically and epistemologically problematic features characterizing current LLM development and deployment. In this regard, terminologists can ensure terminological precision, help preserve linguistic diversity and knowledge excluded in LLMs. They may also support inclusive, transparent and accountable digital infrastructures. By documenting system and variety-specific terms, they counteract the homogenizing tendencies of LLMs and challenge epistemic monopolies. Their expertise bridges disciplines and reinforces that language is not neutral, but culturally and institutionally embedded. As educators and stewards of meaning, terminologists empower users to critically engage with LLM outputs, ensuring that language technologies remain ethically grounded and responsive to human contexts and values.</abstract>
      <url hash="b5ba667f">2025.luhme-1.5</url>
      <bibkey>heinisch-2025-terminologists</bibkey>
    </paper>
    <paper id="6">
      <title>A Toolbox for Improving Evolutionary Prompt Search</title>
      <author><first>Daniel</first><last>Grieβhaber</last></author>
      <author><first>Maximilian</first><last>Kimmich</last></author>
      <author><first>Johannes</first><last>Maucher</last></author>
      <author><first>Thang</first><last>Vu</last></author>
      <pages>58–66</pages>
      <abstract>Evolutionary prompt optimization has demonstrated effectiveness in refining prompts for LLMs. However, existing approaches lack robust operators and efficient evaluation mechanisms. In this work, we propose several key improvements to evolutionary prompt optimization that can partially generalize to prompt optimization in general: 1) decomposing evolution into distinct steps to enhance the evolution and its control, 2) introducing an LLM-based judge to verify the evolutions, 3) integrating human feedback to refine the evolutionary operator, and 4) developing more efficient evaluation strategies that maintain performance while reducing computational overhead. Our approach improves both optimization quality and efficiency. We release our code, enabling prompt optimization on new tasks and facilitating further research in this area.</abstract>
      <url hash="8131ce2d">2025.luhme-1.6</url>
      <bibkey>griebhaber-etal-2025-toolbox</bibkey>
    </paper>
    <paper id="7">
      <title>Improving <fixed-case>LLM</fixed-case>s for Machine Translation Using Synthetic Preference Data</title>
      <author><first>Dario</first><last>Vajda</last></author>
      <author><first>Domen</first><last>Vreš</last></author>
      <author><first>Marko Robnik</first><last>Šikonja</last></author>
      <pages>67–73</pages>
      <abstract>Large language models have emerged as effective machine translation systems. In this paper, we explore how a general instruction-tuned large language model can be improved for machine translation using relatively few easily produced data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct model using Direct Preference Optimization (DPO) training on a programmatically curated and enhanced subset of a public dataset. As DPO requires pairs of quality-ranked instances, we generated its training dataset by translating English Wikipedia articles using two LLMs, GaMS-9B-Instruct and EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics coupled with automatic evaluation metrics such as COMET. The evaluation shows that our fine-tuned model outperforms both models involved in the dataset generation. In comparison to the baseline models, the fine-tuned model achieved a COMET score gain of around 0.04 and 0.02, respectively, on translating Wikipedia articles. It also more consistently avoids language and formatting errors.</abstract>
      <url hash="778a9cf3">2025.luhme-1.7</url>
      <bibkey>vajda-etal-2025-improving</bibkey>
    </paper>
    <paper id="8">
      <title>Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls</title>
      <author><first>Elena</first><last>Pitta</last></author>
      <author><first>Tom</first><last>Kouwenhoven</last></author>
      <author><first>Tessa</first><last>Verhoef</last></author>
      <pages>74–83</pages>
      <abstract>This study investigates the extent to which the Visual Entailment (VE) task serves as a reliable probe of vision-language understanding in multimodal language models, using the LLaMA 3.2 11B Vision model as a test case. Beyond reporting performance metrics, we aim to interpret what these results reveal about the underlying possibilities and limitations of the VE task. We conduct a series of experiments across zero-shot, few-shot, and fine-tuning settings, exploring how factors such as prompt design, the number and order of in-context examples and access to visual information might affect VE performance. To further probe the reasoning processes of the model, we used explanation-based evaluations. Results indicate that three-shot inference outperforms the zero-shot baselines. However, additional examples introduce more noise than they provide benefits. Additionally, the order of the labels in the prompt is a critical factor that influences the predictions. In the absence of visual information, the model has a strong tendency to hallucinate and imagine content, raising questions about the model’s over-reliance on linguistic priors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on the e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model. Additionally, the explanation evaluation demonstrates that the fine-tuned model provides semantically meaningful explanations similar to those of humans, with a BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore results in experiments with limited vision, questioning the visual grounding of this task. Overall, our results highlight both the utility and limitations of VE as a diagnostic task for vision-language understanding and point to directions for refining multimodal evaluation methods.</abstract>
      <url hash="49598952">2025.luhme-1.8</url>
      <bibkey>pitta-etal-2025-probing</bibkey>
    </paper>
    <paper id="9">
      <title>Do Large Language Models understand how to be judges?</title>
      <author><first>Nicolò</first><last>Donati</last></author>
      <author><first>Paolo</first><last>Torroni</last></author>
      <author><first>Giuseppe</first><last>Savino</last></author>
      <pages>85–102</pages>
      <abstract>This paper investigates whether Large Language Models (LLMs) can effectively act as judges for evaluating open-ended text generation tasks, such as summarization, by interpreting nuanced editorial criteria. Traditional metrics like ROUGE and BLEU rely on surface-level overlap, while human evaluations remain costly and inconsistent. To address this, we propose a structured rubric with five dimensions: coherence, consistency, fluency, relevance, and ordering, each defined with explicit sub-criteria to guide LLMs in assessing semantic fidelity and structural quality. Using a purpose-built dataset of Italian news summaries generated by GPT-4o, each tailored to isolate specific criteria, we evaluate LLMs’ ability to assign scores and rationales aligned with expert human judgments. Results show moderate alignment (Spearman’s ρ = 0.6–0.7) for criteria like relevance but reveal systematic biases, such as overestimating fluency and coherence, likely due to training data biases. We identify challenges in rubric interpretation, particularly for hierarchical or abstract criteria, and highlight limitations in cross-genre generalization. The study underscores the potential of LLMs as scalable evaluators but emphasizes the need for fine-tuning, diverse benchmarks, and refined rubrics to mitigate biases and enhance reliability. Future directions include expanding to multilingual and multi-genre contexts and exploring task-specific instruction tuning to improve alignment with human editorial standards.</abstract>
      <url hash="2fe1dd85">2025.luhme-1.9</url>
      <bibkey>donati-etal-2025-large</bibkey>
    </paper>
    <paper id="10">
      <title>Cross-Genre Native Language Identification with Open-Source Large Language Models</title>
      <author><first>Robin</first><last>Nicholls</last></author>
      <author><first>Kenneth</first><last>Alperin</last></author>
      <pages>103–108</pages>
      <abstract>Native Language Identification (NLI) is a crucial area within computational linguistics, aimed at determining an author’s first language (L1) based on their proficiency in a second language (L2). Recent studies have shown remarkable improvements in NLI accuracy due to advancements in large language models (LLMs). This paper investigates the performance of open-source LLMs on short-form comments from the Reddit-L2 corpus compared to their performance on the TOEFL11 corpus of non-native English essays. Our experiments revealed that fine-tuning on TOEFL11 significantly improved accuracy on Reddit-L2, demonstrating the transferability of linguistic features across different text genres. Conversely, models fine-tuned on Reddit-L2 also generalised well to TOEFL11, achieving over 90% accuracy and F1 scores for the native languages that appear in both corpora. This shows the strong transfer performance from long-form to short-form text and vice versa. Additionally, we explored the task of classifying authors as native or non-native English speakers, where fine-tuned models achieve near-perfect accu- racy on the Reddit-L2 dataset. Our findings emphasize the impact of document length on model performance, with optimal results observed up to approximately 1200 tokens. This study highlights the effectiveness of open-source LLMs in NLI tasks across diverse linguistic contexts, suggesting their potential for broader applications in real-world scenarios.</abstract>
      <url hash="f77ed648">2025.luhme-1.10</url>
      <bibkey>nicholls-alperin-2025-cross</bibkey>
    </paper>
    <paper id="11">
      <title>Climate Change Discourse Over Time: A Topic-Sentiment Perspective</title>
      <author><first>Chaya</first><last>Liebeskind</last></author>
      <author><first>Barbara</first><last>Lewandowska-Tomaszczyk</last></author>
      <pages>109–116</pages>
      <abstract>The present paper focuses on the study of opinion dynamics and opinion shifts in social media in the context of climate change discourse in terms of the quantitative NLP analysis, supported by a linguistic outlook. The research draws on two comparable collections of climate-related social media data from different time periods, each based on trending climate-related hashtags and annotated for relevant sentiment values. The quantitative computer-based research methodology has been supported by a language-based perspective in the pragma-linguistic form. The research shows that the latter data source, for the majority of identified topics, exhibits a significant reduction in negative sentiment and a dominance of positive sentiment, i.e., a potential temporal evolution in public sentiment toward climate change. To achieve this, we used a BERT-based clustering approach to identify dominant themes within a combined dataset of tweets from both periods. Subsequently, a unified sentiment classification framework using a Large Language Model (LLM) was applied to reclassify all tweets, ensuring consistent and climate-specific sentiment analysis across both datasets. This methodology allowed for a coherent comparison of public attitudes and their evolution in different time periods and thematic structures.</abstract>
      <url hash="e595cd14">2025.luhme-1.11</url>
      <bibkey>liebeskind-lewandowska-tomaszczyk-2025-climate</bibkey>
    </paper>
  </volume>
</collection>
