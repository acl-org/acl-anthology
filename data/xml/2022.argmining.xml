<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.argmining">
  <volume id="1" ingest-date="2022-10-09">
    <meta>
      <booktitle>Proceedings of the 9th Workshop on Argument Mining</booktitle>
      <editor><first>Gabriella</first><last>Lapesa</last></editor>
      <editor><first>Jodi</first><last>Schneider</last></editor>
      <editor><first>Yohan</first><last>Jo</last></editor>
      <editor><first>Sougata</first><last>Saha</last></editor>
      <publisher>International Conference on Computational Linguistics</publisher>
      <address>Online and in Gyeongju, Republic of Korea</address>
      <month>October</month>
      <year>2022</year>
      <url hash="f06932e6">2022.argmining-1</url>
      <venue>argmining</venue>
    </meta>
    <frontmatter>
      <url hash="6218beaa">2022.argmining-1.0</url>
      <bibkey>argmining-2022-argument</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>I</fixed-case>mage<fixed-case>A</fixed-case>rg: A Multi-modal Tweet Dataset for Image Persuasiveness Mining</title>
      <author><first>Zhexiong</first><last>Liu</last></author>
      <author><first>Meiqi</first><last>Guo</last></author>
      <author><first>Yue</first><last>Dai</last></author>
      <author><first>Diane</first><last>Litman</last></author>
      <pages>1–18</pages>
      <abstract>The growing interest in developing corpora of persuasive texts has promoted applications in automated systems, e.g., debating and essay scoring systems; however, there is little prior work mining image persuasiveness from an argumentative perspective. To expand persuasiveness mining into a multi-modal realm, we present a multi-modal dataset, ImageArg, consisting of annotations of image persuasiveness in tweets. The annotations are based on a persuasion taxonomy we developed to explore image functionalities and the means of persuasion. We benchmark image persuasiveness tasks on ImageArg using widely-used multi-modal learning methods. The experimental results show that our dataset offers a useful resource for this rich and challenging topic, and there is ample room for modeling improvement.</abstract>
      <url hash="cd65473c">2022.argmining-1.1</url>
      <bibkey>liu-etal-2022-imagearg</bibkey>
      <pwccode url="https://github.com/meiqiguo/argmining2022-imagearg" additional="false">meiqiguo/argmining2022-imagearg</pwccode>
    </paper>
    <paper id="2">
      <title>Data Augmentation for Improving the Prediction of Validity and Novelty of Argumentative Conclusions</title>
      <author><first>Philipp</first><last>Heinisch</last></author>
      <author><first>Moritz</first><last>Plenz</last></author>
      <author><first>Juri</first><last>Opitz</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>19–33</pages>
      <abstract>We address the problem of automatically predicting the quality of a conclusion given a set of (textual) premises of an argument, focusing in particular on the task of predicting the validity and novelty of the argumentative conclusion. We propose a multi-task approach that jointly predicts the validity and novelty of the textual conclusion, relying on pre-trained language models fine-tuned on the task. As training data for this task is scarce and costly to obtain, we experimentally investigate the impact of data augmentation approaches for improving the accuracy of prediction compared to a baseline that relies on task-specific data only. We consider the generation of synthetic data as well as the integration of datasets from related argument tasks. We show that especially our synthetic data, combined with class-balancing and instance-specific learning rates, substantially improves classification results (+15.1 points in <tex-math>F_1</tex-math>-score). Using only training data retrieved from related datasets by automatically labeling them for validity and novelty, combined with synthetic data, outperforms the baseline by 11.5 points in <tex-math>F_1</tex-math>-score.</abstract>
      <url hash="c53c9a18">2022.argmining-1.2</url>
      <bibkey>heinisch-etal-2022-data</bibkey>
    </paper>
    <paper id="3">
      <title>Do Discourse Indicators Reflect the Main Arguments in Scientific Papers?</title>
      <author><first>Yingqiang</first><last>Gao</last></author>
      <author><first>Nianlong</first><last>Gu</last></author>
      <author><first>Jessica</first><last>Lam</last></author>
      <author><first>Richard H.R.</first><last>Hahnloser</last></author>
      <pages>34–50</pages>
      <abstract>In scientific papers, arguments are essential for explaining authors’ findings. As substrates of the reasoning process, arguments are often decorated with discourse indicators such as “which shows that” or “suggesting that”. However, it remains understudied whether discourse indicators by themselves can be used as an effective marker of the local argument components (LACs) in the body text that support the main claim in the abstract, i.e., the global argument. In this work, we investigate whether discourse indicators reflect the global premise and conclusion. We construct a set of regular expressions for over 100 word- and phrase-level discourse indicators and measure the alignment of LACs extracted by discourse indicators with the global arguments. We find a positive correlation between the alignment of local premises and local conclusions. However, compared to a simple textual intersection baseline, discourse indicators achieve lower ROUGE recall and have limited capability of extracting LACs relevant to the global argument; thus their role in scientific reasoning is less salient as expected.</abstract>
      <url hash="336e1609">2022.argmining-1.3</url>
      <bibkey>gao-etal-2022-discourse</bibkey>
      <pwccode url="https://github.com/charizardacademy/discourse-indicator" additional="false">charizardacademy/discourse-indicator</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="4">
      <title>Analyzing Culture-Specific Argument Structures in Learner Essays</title>
      <author><first>Wei-Fan</first><last>Chen</last></author>
      <author><first>Mei-Hua</first><last>Chen</last></author>
      <author><first>Garima</first><last>Mudgal</last></author>
      <author><first>Henning</first><last>Wachsmuth</last></author>
      <pages>51–61</pages>
      <abstract>Language education has been shown to benefit from computational argumentation, for example, from methods that assess quality dimensions of language learners’ argumentative essays, such as their organization and argument strength. So far, however, little attention has been paid to cultural differences in learners’ argument structures originating from different origins and language capabilities. This paper extends prior studies of learner argumentation by analyzing differences in the argument structure of essays from culturally diverse learners. Based on the ICLE corpus containing essays written by English learners of 16 different mother tongues, we train natural language processing models to mine argumentative discourse units (ADUs) as well as to assess the essays’ quality in terms of organization and argument strength. The extracted ADUs and the predicted quality scores enable us to look into the similarities and differences of essay argumentation across different English learners. In particular, we analyze the ADUs from learners with different mother tongues, different levels of arguing proficiency, and different context cultures.</abstract>
      <url hash="4ea29ca5">2022.argmining-1.4</url>
      <bibkey>chen-etal-2022-analyzing</bibkey>
      <pwccode url="https://github.com/webis-de/argmining22-culture-arg" additional="false">webis-de/argmining22-culture-arg</pwccode>
    </paper>
    <paper id="5">
      <title>Perturbations and Subpopulations for Testing Robustness in Token-Based Argument Unit Recognition</title>
      <author><first>Jonathan</first><last>Kamp</last></author>
      <author><first>Lisa</first><last>Beinborn</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <pages>62–73</pages>
      <abstract>Argument Unit Recognition and Classification aims at identifying argument units from text and classifying them as pro or against. One of the design choices that need to be made when developing systems for this task is what the unit of classification should be: segments of tokens or full sentences. Previous research suggests that fine-tuning language models on the token-level yields more robust results for classifying sentences compared to training on sentences directly. We reproduce the study that originally made this claim and further investigate what exactly token-based systems learned better compared to sentence-based ones. We develop systematic tests for analysing the behavioural differences between the token-based and the sentence-based system. Our results show that token-based models are generally more robust than sentence-based models both on manually perturbed examples and on specific subpopulations of the data.</abstract>
      <url hash="7b07b750">2022.argmining-1.5</url>
      <bibkey>kamp-etal-2022-perturbations</bibkey>
      <pwccode url="https://github.com/jbkamp/repo-rob-token-aur" additional="false">jbkamp/repo-rob-token-aur</pwccode>
    </paper>
    <paper id="6">
      <title>A Unified Representation and a Decoupled Deep Learning Architecture for Argumentation Mining of Students’ Persuasive Essays</title>
      <author><first>Muhammad Tawsif</first><last>Sazid</last></author>
      <author><first>Robert E.</first><last>Mercer</last></author>
      <pages>74–83</pages>
      <abstract>We develop a novel unified representation for the argumentation mining task facilitating the extracting from text and the labelling of the non-argumentative units and argumentation components—premises, claims, and major claims—and the argumentative relations—premise to claim or premise in a support or attack relation, and claim to major-claim in a for or against relation—in an end-to-end machine learning pipeline. This tightly integrated representation combines the component and relation identification sub-problems and enables a unitary solution for detecting argumentation structures. This new representation together with a new deep learning architecture composed of a mixed embedding method, a multi-head attention layer, two biLSTM layers, and a final linear layer obtain state-of-the-art accuracy on the Persuasive Essays dataset. Also, we have introduced a decoupled solution to identify the entities and relations first, and on top of that, a second model is used to detect distance between the detected related components. An augmentation of the corpus (paragraph version) by including copies of major claims has further increased the performance.</abstract>
      <url hash="0bd37971">2022.argmining-1.6</url>
      <bibkey>sazid-mercer-2022-unified</bibkey>
      <pwccode url="https://github.com/tawsifsazid/unified-representation-for-argumentation-mining" additional="false">tawsifsazid/unified-representation-for-argumentation-mining</pwccode>
    </paper>
    <paper id="7">
      <title>Overview of the 2022 Validity and Novelty Prediction Shared Task</title>
      <author><first>Philipp</first><last>Heinisch</last></author>
      <author><first>Anette</first><last>Frank</last></author>
      <author><first>Juri</first><last>Opitz</last></author>
      <author><first>Moritz</first><last>Plenz</last></author>
      <author><first>Philipp</first><last>Cimiano</last></author>
      <pages>84–94</pages>
      <abstract>This paper provides an overview of the Argument Validity and Novelty Prediction Shared Task that was organized as part of the 9th Workshop on Argument Mining (ArgMining 2022). The task focused on the prediction of the validity and novelty of a conclusion given a textual premise. Validity is defined as the degree to which the conclusion is justified with respect to the given premise. Novelty defines the degree to which the conclusion contains content that is new in relation to the premise. Six groups participated in the task, submitting overall 13 system runs for the subtask of binary classification and 2 system runs for the subtask of relative classification. The results reveal that the task is challenging, with best results obtained for Validity prediction in the range of 75% F1 score, for Novelty prediction of 70% F1 score and for correctly predicting both Validity and Novelty of 45% F1 score. In this paper we summarize the task definition and dataset. We give an overview of the results obtained by the participating systems, as well as insights to be gained from the diverse contributions.</abstract>
      <url hash="c4e1e111">2022.argmining-1.7</url>
      <bibkey>heinisch-etal-2022-overview</bibkey>
      <pwccode url="https://github.com/phhei/argsvalidnovel" additional="false">phhei/argsvalidnovel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/valnov-subtask-a">ValNov Subtask A</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/valnov-subtask-b">ValNov Subtask B</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="8">
      <title>Will It Blend? Mixing Training Paradigms &amp; Prompting for Argument Quality Prediction</title>
      <author><first>Michiel</first><last>van der Meer</last></author>
      <author><first>Myrthe</first><last>Reuver</last></author>
      <author><first>Urja</first><last>Khurana</last></author>
      <author><first>Lea</first><last>Krause</last></author>
      <author><first>Selene</first><last>Baez Santamaria</last></author>
      <pages>95–103</pages>
      <abstract>This paper describes our contributions to the Shared Task of the 9th Workshop on Argument Mining (2022). Our approach uses Large Language Models for the task of Argument Quality Prediction. We perform prompt engineering using GPT-3, and also investigate the training paradigms multi-task learning, contrastive learning, and intermediate-task training. We find that a mixed prediction setup outperforms single models. Prompting GPT-3 works best for predicting argument validity, and argument novelty is best estimated by a model trained using all three training paradigms.</abstract>
      <url hash="e672898a">2022.argmining-1.8</url>
      <bibkey>van-der-meer-etal-2022-will</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="9">
      <title><fixed-case>KEV</fixed-case>i<fixed-case>N</fixed-case>: A Knowledge Enhanced Validity and Novelty Classifier for Arguments</title>
      <author><first>Ameer</first><last>Saadat-Yazdi</last></author>
      <author><first>Xue</first><last>Li</last></author>
      <author><first>Sandrine</first><last>Chausson</last></author>
      <author><first>Vaishak</first><last>Belle</last></author>
      <author><first>Björn</first><last>Ross</last></author>
      <author><first>Jeff Z.</first><last>Pan</last></author>
      <author><first>Nadin</first><last>Kökciyan</last></author>
      <pages>104–110</pages>
      <abstract>The ArgMining 2022 Shared Task is concerned with predicting the validity and novelty of an inference for a given premise and conclusion pair. We propose two feed-forward network based models (KEViN1 and KEViN2), which combine features generated from several pretrained transformers and the WikiData knowledge graph. The transformers are used to predict entailment and semantic similarity, while WikiData is used to provide a semantic measure between concepts in the premise-conclusion pair. Our proposed models show significant improvement over RoBERTa, with KEViN1 outperforming KEViN2 and obtaining second rank on both subtasks (A and B) of the ArgMining 2022 Shared Task.</abstract>
      <url hash="97298fea">2022.argmining-1.9</url>
      <bibkey>saadat-yazdi-etal-2022-kevin</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/valnov-subtask-a">ValNov Subtask A</pwcdataset>
    </paper>
    <paper id="10">
      <title>Argument Novelty and Validity Assessment via Multitask and Transfer Learning</title>
      <author><first>Milad</first><last>Alshomary</last></author>
      <author><first>Maja</first><last>Stahl</last></author>
      <pages>111–114</pages>
      <abstract>An argument is a constellation of premises reasoning towards a certain conclusion. The automatic generation of conclusions is becoming a very prominent task, raising the need for automatic measures to assess the quality of these generated conclusions. The SharedTask at the 9th Workshop on Argument Mining proposes a new task to assess the novelty and validity of a conclusion given a set of premises. In this paper, we present a multitask learning approach that transfers the knowledge learned from the natural language inference task to the tasks at hand. Evaluation results indicate the importance of both knowledge transfer and joint learning, placing our approach in the fifth place with strong results compared to baselines.</abstract>
      <url hash="98fef1ad">2022.argmining-1.10</url>
      <bibkey>alshomary-stahl-2022-argument</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="11">
      <title>Is Your Perspective Also My Perspective? Enriching Prediction with Subjectivity</title>
      <author><first>Julia</first><last>Romberg</last></author>
      <pages>115–125</pages>
      <abstract>Although argumentation can be highly subjective, the common practice with supervised machine learning is to construct and learn from an aggregated ground truth formed from individual judgments by majority voting, averaging, or adjudication. This approach leads to a neglect of individual, but potentially important perspectives and in many cases cannot do justice to the subjective character of the tasks. One solution to this shortcoming are multi-perspective approaches, which have received very little attention in the field of argument mining so far. In this work we present PerspectifyMe, a method to incorporate perspectivism by enriching a task with subjectivity information from the data annotation process. We exemplify our approach with the use case of classifying argument concreteness, and provide first promising results for the recently published CIMT PartEval Argument Concreteness Corpus.</abstract>
      <url hash="eed54443">2022.argmining-1.11</url>
      <bibkey>romberg-2022-perspective</bibkey>
      <pwccode url="https://github.com/juliaromberg/argmining2022" additional="false">juliaromberg/argmining2022</pwccode>
    </paper>
    <paper id="12">
      <title>Boundary Detection and Categorization of Argument Aspects via Supervised Learning</title>
      <author><first>Mattes</first><last>Ruckdeschel</last></author>
      <author><first>Gregor</first><last>Wiedemann</last></author>
      <pages>126–136</pages>
      <abstract>Aspect-based argument mining (ABAM) is the task of automatic _detection_ and _categorization_ of argument aspects, i.e. the parts of an argumentative text that contain the issue-specific key rationale for its conclusion. From empirical data, overlapping but not congruent sets of aspect categories can be derived for different topics. So far, two supervised approaches to detect aspect boundaries, and a smaller number of unsupervised clustering approaches to categorize groups of similar aspects have been proposed. With this paper, we introduce the Argument Aspect Corpus (AAC) that contains token-level annotations of aspects in 3,547 argumentative sentences from three highly debated topics. This dataset enables both the supervised learning of boundaries and categorization of argument aspects. During the design of our annotation process, we noticed that it is not clear from the outset at which contextual unit aspects should be coded. We, thus, experiment with classification at the token, chunk, and sentence level granularity. Our finding is that the chunk level provides the most useful information for applications. At the same time, it produces the best performing results in our tested supervised learning setups.</abstract>
      <url hash="a345671e">2022.argmining-1.12</url>
      <bibkey>ruckdeschel-wiedemann-2022-boundary</bibkey>
      <pwccode url="https://github.com/leibniz-hbi/argument-aspect-corpus-v1" additional="false">leibniz-hbi/argument-aspect-corpus-v1</pwccode>
    </paper>
    <paper id="13">
      <title>Predicting the Presence of Reasoning Markers in Argumentative Text</title>
      <author><first>Jonathan</first><last>Clayton</last></author>
      <author><first>Rob</first><last>Gaizauskas</last></author>
      <pages>137–142</pages>
      <abstract>This paper proposes a novel task in Argument Mining, which we will refer to as Reasoning Marker Prediction. We reuse the popular Persuasive Essays Corpus (Stab and Gurevych, 2014). Instead of using this corpus for Argument Structure Parsing, we use a simple heuristic method to identify text spans which we can identify as reasoning markers. We propose baseline methods for predicting the presence of these reasoning markers automatically, and make a script to generate the data for the task publicly available.</abstract>
      <url hash="8fb414d1">2022.argmining-1.13</url>
      <bibkey>clayton-gaizauskas-2022-predicting</bibkey>
    </paper>
    <paper id="14">
      <title>Detecting Arguments in <fixed-case>CJEU</fixed-case> Decisions on Fiscal State Aid</title>
      <author><first>Giulia</first><last>Grundler</last></author>
      <author><first>Piera</first><last>Santin</last></author>
      <author><first>Andrea</first><last>Galassi</last></author>
      <author><first>Federico</first><last>Galli</last></author>
      <author><first>Francesco</first><last>Godano</last></author>
      <author><first>Francesca</first><last>Lagioia</last></author>
      <author><first>Elena</first><last>Palmieri</last></author>
      <author><first>Federico</first><last>Ruggeri</last></author>
      <author><first>Giovanni</first><last>Sartor</last></author>
      <author><first>Paolo</first><last>Torroni</last></author>
      <pages>143–157</pages>
      <abstract>The successful application of argument mining in the legal domain can dramatically impact many disciplines related to law. For this purpose, we present Demosthenes, a novel corpus for argument mining in legal documents, composed of 40 decisions of the Court of Justice of the European Union on matters of fiscal state aid. The annotation specifies three hierarchical levels of information: the argumentative elements, their types, and their argument schemes. In our experimental evaluation, we address 4 different classification tasks, combining advanced language models and traditional classifiers.</abstract>
      <url hash="eb46b184">2022.argmining-1.14</url>
      <bibkey>grundler-etal-2022-detecting</bibkey>
      <pwccode url="https://github.com/adele-project/demosthenes" additional="false">adele-project/demosthenes</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/demosthenes">Demosthenes</pwcdataset>
    </paper>
    <paper id="15">
      <title>Multimodal Argument Mining: A Case Study in Political Debates</title>
      <author><first>Eleonora</first><last>Mancini</last></author>
      <author><first>Federico</first><last>Ruggeri</last></author>
      <author><first>Andrea</first><last>Galassi</last></author>
      <author><first>Paolo</first><last>Torroni</last></author>
      <pages>158–170</pages>
      <abstract>We propose a study on multimodal argument mining in the domain of political debates. We collate and extend existing corpora and provide an initial empirical study on multimodal architectures, with a special emphasis on input encoding methods. Our results provide interesting indications about future directions in this important domain.</abstract>
      <url hash="1f3bd4b2">2022.argmining-1.15</url>
      <bibkey>mancini-etal-2022-multimodal</bibkey>
      <pwccode url="https://github.com/federicoruggeri/multimodal-am" additional="false">federicoruggeri/multimodal-am</pwccode>
    </paper>
    <paper id="16">
      <title>A Robustness Evaluation Framework for Argument Mining</title>
      <author><first>Mehmet</first><last>Sofi</last></author>
      <author><first>Matteo</first><last>Fortier</last></author>
      <author><first>Oana</first><last>Cocarascu</last></author>
      <pages>171–180</pages>
      <abstract>Standard practice for evaluating the performance of machine learning models for argument mining is to report different metrics such as accuracy or F1. However, little is usually known about the model’s stability and consistency when deployed in real-world settings. In this paper, we propose a robustness evaluation framework to guide the design of rigorous argument mining models. As part of the framework, we introduce several novel robustness tests tailored specifically to argument mining tasks. Additionally, we integrate existing robustness tests designed for other natural language processing tasks and re-purpose them for argument mining. Finally, we illustrate the utility of our framework on two widely used argument mining corpora, UKP topic-sentences and IBM Debater Evidence Sentence. We argue that our framework should be used in conjunction with standard performance evaluation techniques as a measure of model stability.</abstract>
      <url hash="5a4dce03">2022.argmining-1.16</url>
      <bibkey>sofi-etal-2022-robustness</bibkey>
    </paper>
    <paper id="17">
      <title>On Selecting Training Corpora for Cross-Domain Claim Detection</title>
      <author><first>Robin</first><last>Schaefer</last></author>
      <author><first>René</first><last>Knaebel</last></author>
      <author><first>Manfred</first><last>Stede</last></author>
      <pages>181–186</pages>
      <abstract>Identifying claims in text is a crucial first step in argument mining. In this paper, we investigate factors for the composition of training corpora to improve cross-domain claim detection. To this end, we use four recent argumentation corpora annotated with claims and submit them to several experimental scenarios. Our results indicate that the “ideal” composition of training corpora is characterized by a large corpus size, homogeneous claim proportions, and less formal text domains.</abstract>
      <url hash="5f7bf0b3">2022.argmining-1.17</url>
      <bibkey>schaefer-etal-2022-selecting</bibkey>
    </paper>
    <paper id="18">
      <title>Entity-based Claim Representation Improves Fact-Checking of Medical Content in Tweets</title>
      <author><first>Amelie</first><last>Wührl</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <pages>187–198</pages>
      <abstract>False medical information on social media poses harm to people’s health. While the need for biomedical fact-checking has been recognized in recent years, user-generated medical content has received comparably little attention. At the same time, models for other text genres might not be reusable, because the claims they have been trained with are substantially different. For instance, claims in the SciFact dataset are short and focused: “Side effects associated with antidepressants increases risk of stroke”. In contrast, social media holds naturally-occurring claims, often embedded in additional context: "‘If you take antidepressants like SSRIs, you could be at risk of a condition called serotonin syndrome’ Serotonin syndrome nearly killed me in 2010. Had symptoms of stroke and seizure.” This showcases the mismatch between real-world medical claims and the input that existing fact-checking systems expect. To make user-generated content checkable by existing models, we propose to reformulate the social-media input in such a way that the resulting claim mimics the claim characteristics in established datasets. To accomplish this, our method condenses the claim with the help of relational entity information and either compiles the claim out of an entity-relation-entity triple or extracts the shortest phrase that contains these elements. We show that the reformulated input improves the performance of various fact-checking models as opposed to checking the tweet text in its entirety.</abstract>
      <url hash="d8f1bc1d">2022.argmining-1.18</url>
      <bibkey>wuhrl-klinger-2022-entity</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/covid-fact">COVID-Fact</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubhealth">PUBHEALTH</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scifact">SciFact</pwcdataset>
    </paper>
    <paper id="19">
      <title><fixed-case>Q</fixed-case>uali<fixed-case>A</fixed-case>ssistant: Extracting Qualia Structures from Texts</title>
      <author><first>Manuel</first><last>Biertz</last></author>
      <author><first>Lorik</first><last>Dumani</last></author>
      <author><first>Markus</first><last>Nilles</last></author>
      <author><first>Björn</first><last>Metzler</last></author>
      <author><first>Ralf</first><last>Schenkel</last></author>
      <pages>199–208</pages>
      <abstract>In this paper, we present QualiAssistant, a free and open-source system written in Java for identification and extraction of Qualia structures from any natural language texts having many application scenarios such as argument mining or creating dictionaries. It answers the call for a Qualia bootstrapping tool with a ready-to-use system that can be gradually filled by the community with patterns in multiple languages. Qualia structures express the meaning of lexical items. They describe, e.g., of what kind the item is (formal role), what it includes (constitutive role), how it is brought about (agentive role), and what it is used for (telic role). They are also valuable for various Information Retrieval and NLP tasks. Our application requires search patterns for Qualia structures consisting of POS tag sequences as well as the dataset the user wants to search for Qualias. Samples for both are provided alongside this paper. While samples are in German, QualiAssistant can process all languages for which constituency trees can be generated and patterns are available. Our provided patterns follow a high-precision low-recall design aiming to generate automatic annotations for text mining but can be exchanged easily for other purposes. Our evaluation shows that QualiAssistant is a valuable and reliable tool for finding Qualia structures in unstructured texts.</abstract>
      <url hash="67099557">2022.argmining-1.19</url>
      <bibkey>biertz-etal-2022-qualiassistant</bibkey>
    </paper>
  </volume>
</collection>
