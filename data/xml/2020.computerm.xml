<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.computerm">
  <volume id="1">
    <meta>
      <booktitle>Proceedings of the 6th International Workshop on Computational Terminology</booktitle>
      <editor><first>Béatrice</first><last>Daille</last></editor>
      <editor><first>Kyo</first><last>Kageura</last></editor>
      <editor><first>Ayla Rigouts</first><last>Terryn</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>May</month>
      <year>2020</year>
      <isbn>979-10-95546-57-3</isbn>
    </meta>
    <frontmatter>
      <url hash="4ff69a39">2020.computerm-1.0</url>
    </frontmatter>
    <paper id="1">
      <title>Automatic Term Extraction from Newspaper Corpora: Making the Most of Specificity and Common Features</title>
      <author><first>Patrick</first><last>Drouin</last></author>
      <author><first>Jean-Benoît</first><last>Morel</last></author>
      <author><first>Marie-Claude</first><last>L’ Homme</last></author>
      <pages>1–7</pages>
      <abstract>The first step of any terminological work is to setup a reliable, specialized corpus composed of documents written by specialists and then to apply automatic term extraction (ATE) methods to this corpus in order to retrieve a first list of potential terms. In this paper, the experiment we describe differs quite drastically from this usual process since we are applying ATE to unspecialized corpora. The corpus used for this study was built from newspaper articles retrieved from the Web using a short list of keywords. The general intuition on which this research is based is that ATE based corpus comparison techniques can be used to capture both similarities and dissimilarities between corpora. The former are exploited through a termhood measure and the latter through word embeddings. Our initial results were validated manually and show that combining a traditional ATE method that focuses on dissimilarities between corpora to newer methods that exploit similarities (more specifically distributional features of candidates) leads to promising results.</abstract>
      <url hash="810d1840">2020.computerm-1.1</url>
      <language>eng</language>
    </paper>
    <paper id="2">
      <title><fixed-case>T</fixed-case>erm<fixed-case>P</fixed-case>ortal: A Workbench for Automatic Term Extraction from <fixed-case>I</fixed-case>celandic Texts</title>
      <author><first>Steinþór</first><last>Steingrímsson</last></author>
      <author><first>Ágústa</first><last>Þorbergsdóttir</last></author>
      <author><first>Hjalti</first><last>Danielsson</last></author>
      <author><first>Gunnar Thor</first><last>Ornolfsson</last></author>
      <pages>8–16</pages>
      <abstract>Automatic term extraction (ATE) from texts is critical for effective terminology work in small speech communities. We present TermPortal, a workbench for terminology work in Iceland, featuring the first ATE system for Icelandic. The tool facilitates standardization in terminology work in Iceland, as it exports data in standard formats in order to streamline gathering and distribution of the material. In the project we focus on the domain of finance in order to do be able to fulfill the needs of an important and large field. We present a comprehensive survey amongst the most prominent organizations in that field, the results of which emphasize the need for a good, up-to-date and accessible termbank and the willingness to use terms in Icelandic. Furthermore we present the ATE tool for Icelandic, which uses a variety of methods and shows great potential with a recall rate of up to 95% and a high C-value, indicating that it competently finds term candidates that are important to the input text.</abstract>
      <url hash="5a2f1417">2020.computerm-1.2</url>
      <language>eng</language>
    </paper>
    <paper id="3">
      <title>Translating Knowledge Representations with Monolingual Word Embeddings: the Case of a Thesaurus on Corporate Non-Financial Reporting</title>
      <author><first>Martín</first><last>Quesada Zaragoza</last></author>
      <author><first>Lianet</first><last>Sepúlveda Torres</last></author>
      <author><first>Jérôme</first><last>Basdevant</last></author>
      <pages>17–25</pages>
      <abstract>A common method of structuring information extracted from textual data is using a knowledge model (e.g. a thesaurus) to organise the information semantically. Creating and managing a knowledge model is already a costly task in terms of human effort, not to mention making it multilingual. Multilingual knowledge modelling is a common problem for both transnational organisations and organisations providing text analytics that want to analyse information in more than one language. Many organisations tend to develop their language resources first in one language (often English). When it comes to analysing data sources in other languages, either a lot of effort has to be invested in recreating the same knowledge base in a different language or the data itself has to be translated into the language of the knowledge model. In this paper, we propose an unsupervised method to automatically induce a given thesaurus into another language using only comparable monolingual corpora. The aim of this proposal is to employ cross-lingual word embeddings to map the set of topics in an already-existing English thesaurus into Spanish. With this in mind, we describe different approaches to generate the Spanish thesaurus terms and offer an extrinsic evaluation by using the obtained thesaurus, which covers non-financial topics in a multi-label document classification task, and we compare the results across these approaches.</abstract>
      <url hash="07b7ea85">2020.computerm-1.3</url>
      <language>eng</language>
    </paper>
    <paper id="4">
      <title>Which Dependency Parser to Use for Distributional Semantics in a Specialized Domain?</title>
      <author><first>Pauline</first><last>Brunet</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <author><first>Ludovic</first><last>Tanguy</last></author>
      <pages>26–36</pages>
      <abstract>We present a study whose objective is to compare several dependency parsers for English applied to a specialized corpus for building distributional count-based models from syntactic dependencies. One of the particularities of this study is to focus on the concepts of the target domain, which mainly occur in documents as multi-terms and must be aligned with the outputs of the parsers. We compare a set of ten parsers in terms of syntactic triplets but also in terms of distributional neighbors extracted from the models built from these triplets, both with and without an external reference concerning the semantic relations between concepts. We show more particularly that some patterns of proximity between these parsers can be observed across our different evaluations, which could give insights for anticipating the performance of a parser for building distributional models from a given corpus</abstract>
      <url hash="dedcaf63">2020.computerm-1.4</url>
      <language>eng</language>
    </paper>
    <paper id="5">
      <title>Leveraging the Inherent Hierarchy of Vacancy Titles for Automated Job Ontology Expansion</title>
      <author><first>Jeroen</first><last>Van Hautte</last></author>
      <author><first>Vincent</first><last>Schelstraete</last></author>
      <author><first>Mikaël</first><last>Wornoo</last></author>
      <pages>37–42</pages>
      <abstract>Machine learning plays an ever-bigger part in online recruitment, powering intelligent matchmaking and job recommendations across many of the world’s largest job platforms. However, the main text is rarely enough to fully understand a job posting: more often than not, much of the required information is condensed into the job title. Several organised efforts have been made to map job titles onto a hand-made knowledge base as to provide this information, but these only cover around 60% of online vacancies. We introduce a novel, purely data-driven approach towards the detection of new job titles. Our method is conceptually simple, extremely efficient and competitive with traditional NER-based approaches. Although the standalone application of our method does not outperform a finetuned BERT model, it can be applied as a preprocessing step as well, substantially boosting accuracy across several architectures.</abstract>
      <url hash="d4feb79d">2020.computerm-1.5</url>
      <language>eng</language>
    </paper>
    <paper id="6">
      <title>Terminology in Written Medical Reports: A Proposal of Text Enrichment to Favour its Comprehension by the Patient</title>
      <author><first>Rosa</first><last>Estopà</last></author>
      <author><first>Alejandra</first><last>López-Fuentes</last></author>
      <author><first>Jorge M.</first><last>Porras-Garzon</last></author>
      <pages>43–49</pages>
      <abstract>The empowerment of the population and the democratisation of information regarding healthcare have revealed that there is a communication gap between health professionals and patients. The latter are constantly receiving more and more written information about their healthcare visits and treatments, but that does not mean they understand it. In this paper we focus on the patient’s lack of comprehension of medical reports. After linguistically characterising the medical report, we present the results of a survey that showed that patients have serious comprehension difficulties concerning the medical reports they receive, specifically problems regarding the medical terminology used in these texts, specifically in Spanish and Catalan. To favour the understanding of medical reports, we propose an automatic text enrichment strategy that generates linguistically and cognitively enriched medical reports which are more comprehensible to the patient, and which focus on the parts of the medical report that most interest the patient: the diagnosis and treatment sections.</abstract>
      <url hash="8684e7cc">2020.computerm-1.6</url>
      <language>eng</language>
    </paper>
    <paper id="7">
      <title>A study of semantic projection from single word terms to multi-word terms in the environment domain</title>
      <author><first>Yizhe</first><last>Wang</last></author>
      <author><first>Beatrice</first><last>Daille</last></author>
      <author><first>Nabil</first><last>Hathout</last></author>
      <pages>50–54</pages>
      <abstract>The semantic projection method is often used in terminology structuring to infer semantic relations between terms. Semantic projection relies upon the assumption of semantic compositionality: the relation that links simple term pairs remains valid in pairs of complex terms built from these simple terms. This paper proposes to investigate whether this assumption commonly adopted in natural language processing is actually valid. First, we describe the process of constructing a list of semantically linked multi-word terms (MWTs) related to the environmental field through the extraction of semantic variants. Second, we present our analysis of the results from the semantic projection. We find that contexts play an essential role in defining the relations between MWTs.</abstract>
      <url hash="521b2431">2020.computerm-1.7</url>
      <language>eng</language>
    </paper>
    <paper id="8">
      <title>The <fixed-case>N</fixed-case>et<fixed-case>V</fixed-case>iz terminology visualization tool and the use cases in karstology domain modeling</title>
      <author><first>Senja</first><last>Pollak</last></author>
      <author><first>Vid</first><last>Podpečan</last></author>
      <author><first>Dragana</first><last>Miljkovic</last></author>
      <author><first>Uroš</first><last>Stepišnik</last></author>
      <author><first>Špela</first><last>Vintar</last></author>
      <pages>55–61</pages>
      <abstract>We present the NetViz terminology visualization tool and apply it to the domain modeling of karstology, a subfield of geography studying karst phenomena. The developed tool allows for high-performance online network visualization where the user can upload the terminological data in a simple CSV format, define the nodes (terms, categories), edges (relations) and their properties (by assigning different node colors), and then edit and interactively explore domain knowledge in the form of a network. We showcase the usefulness of the tool on examples from the karstology domain, where in the first use case we visualize the domain knowledge as represented in a manually annotated corpus of domain definitions, while in the second use case we show the power of visualization for domain understanding by visualizing automatically extracted knowledge in the form of triplets extracted from the karstology domain corpus. The application is entirely web-based without any need for downloading or special configuration. The source code of the web application is also available under the permissive MIT license, allowing future extensions for developing new terminological applications.</abstract>
      <url hash="4ac24597">2020.computerm-1.8</url>
      <language>eng</language>
    </paper>
    <paper id="9">
      <title>Towards Automatic Thesaurus Construction and Enrichment.</title>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Beatrice</first><last>Daille</last></author>
      <author><first>Lanza</first><last>Claudia</last></author>
      <pages>62–71</pages>
      <abstract>Thesaurus construction with minimum human efforts often relies on automatic methods to discover terms and their relations. Hence, the quality of a thesaurus heavily depends on the chosen methodologies for: (i) building its content (terminology extraction task) and (ii) designing its structure (semantic similarity task). The performance of the existing methods on automatic thesaurus construction is still less accurate than the handcrafted ones of which is important to highlight the drawbacks to let new strategies build more accurate thesauri models. In this paper, we will provide a systematic analysis of existing methods for both tasks and discuss their feasibility based on an Italian Cybersecurity corpus. In particular, we will provide a detailed analysis on how the semantic relationships network of a thesaurus can be automatically built, and investigate the ways to enrich the terminological scope of a thesaurus by taking into account the information contained in external domain-oriented semantic sets.</abstract>
      <url hash="d3ec1b36">2020.computerm-1.9</url>
      <language>eng</language>
    </paper>
    <paper id="10">
      <title>Supporting terminology extraction with dependency parses</title>
      <author><first>Malgorzata</first><last>Marciniak</last></author>
      <author><first>Piotr</first><last>Rychlik</last></author>
      <author><first>Agnieszka</first><last>Mykowiecka</last></author>
      <pages>72–79</pages>
      <abstract>Terminology extraction procedure usually consists of selecting candidates for terms and ordering them according to their importance for the given text or set of texts. Depending on the method used, a list of candidates contains different fractions of grammatically incorrect, semantically odd and irrelevant sequences. The aim of this work was to improve term candidate selection by reducing the number of incorrect sequences using a dependency parser for Polish.</abstract>
      <url hash="50bbc3fe">2020.computerm-1.10</url>
      <language>eng</language>
    </paper>
    <paper id="11">
      <title>Computational Aspects of Frame-based Meaning Representation in Terminology</title>
      <author><first>Laura</first><last>Giacomini</last></author>
      <author><first>Johannes</first><last>Schäfer</last></author>
      <pages>80–84</pages>
      <abstract>Our contribution is part of a wider research project on term variation in German and concentrates on the computational aspects of a frame-based model for term meaning representation in the technical field. We focus on the role of frames (in the sense of Frame-Based Terminology) as the semantic interface between concepts covered by a domain ontology and domain-specific terminology. In particular, we describe methods for performing frame-based corpus annotation and frame-based term extraction. The aim of the contribution is to discuss the capacity of the model to automatically acquire semantic knowledge suitable for terminographic information tools such as specialised dictionaries, and its applicability to further specialised languages.</abstract>
      <url hash="406f6abf">2020.computerm-1.11</url>
      <language>eng</language>
    </paper>
    <paper id="12">
      <title><fixed-case>T</fixed-case>erm<fixed-case>E</fixed-case>val 2020: Shared Task on Automatic Term Extraction Using the Annotated Corpora for Term Extraction Research (<fixed-case>ACTER</fixed-case>) Dataset</title>
      <author><first>Ayla</first><last>Rigouts Terryn</last></author>
      <author><first>Veronique</first><last>Hoste</last></author>
      <author><first>Patrick</first><last>Drouin</last></author>
      <author><first>Els</first><last>Lefever</last></author>
      <pages>85–94</pages>
      <abstract>The TermEval 2020 shared task provided a platform for researchers to work on automatic term extraction (ATE) with the same dataset: the Annotated Corpora for Term Extraction Research (ACTER). The dataset covers three languages (English, French, and Dutch) and four domains, of which the domain of <i>heart failure</i> was kept as a held-out test set on which final f1-scores were calculated. The aim was to provide a large, transparent, qualitatively annotated, and diverse dataset to the ATE research community, with the goal of promoting comparative research and thus identifying strengths and weaknesses of various state-of-the-art methodologies. The results show a lot of variation between different systems and illustrate how some methodologies reach higher precision or recall, how different systems extract different types of terms, how some are exceptionally good at finding rare terms, or are less impacted by term length. The current contribution offers an overview of the shared task with a comparative evaluation, which complements the individual papers by all participants.</abstract>
      <url hash="3abd0381">2020.computerm-1.12</url>
      <language>eng</language>
    </paper>
    <paper id="13">
      <title><fixed-case>T</fixed-case>erm<fixed-case>E</fixed-case>val 2020: <fixed-case>TALN</fixed-case>-<fixed-case>LS</fixed-case>2<fixed-case>N</fixed-case> System for Automatic Term Extraction</title>
      <author><first>Amir</first><last>Hazem</last></author>
      <author><first>Mérieme</first><last>Bouhandi</last></author>
      <author><first>Florian</first><last>Boudin</last></author>
      <author><first>Beatrice</first><last>Daille</last></author>
      <pages>95–100</pages>
      <abstract>Automatic terminology extraction is a notoriously difficult task aiming to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. The main ways that addressed this task can be ranged in four main categories: (i) rule-based approaches, (ii) feature-based approaches, (iii) context-based approaches, and (iv) hybrid approaches. For this first TermEval shared task, we explore a feature-based approach, and a deep neural network multitask approach -BERT- that we fine-tune for term extraction. We show that BERT models (RoBERTa for English and CamemBERT for French) outperform other systems for French and English languages.</abstract>
      <url hash="7cb7ee34">2020.computerm-1.13</url>
      <language>eng</language>
    </paper>
    <paper id="14">
      <title><fixed-case>T</fixed-case>erm<fixed-case>E</fixed-case>val 2020: <fixed-case>RACAI</fixed-case>’s automatic term extraction system</title>
      <author><first>Vasile</first><last>Pais</last></author>
      <author><first>Radu</first><last>Ion</last></author>
      <pages>101–105</pages>
      <abstract>This paper describes RACAI’s automatic term extraction system, which participated in the TermEval 2020 shared task on English monolingual term extraction. We discuss the system architecture, some of the challenges that we faced as well as present our results in the English competition.</abstract>
      <url hash="f2079a99">2020.computerm-1.14</url>
      <language>eng</language>
    </paper>
    <paper id="15">
      <title><fixed-case>T</fixed-case>erm<fixed-case>E</fixed-case>val 2020: Using <fixed-case>TSR</fixed-case> Filtering Method to Improve Automatic Term Extraction</title>
      <author><first>Antoni</first><last>Oliver</last></author>
      <author><first>Mercè</first><last>Vàzquez</last></author>
      <pages>106–113</pages>
      <abstract>The identification of terms from domain-specific corpora using computational methods is a highly time-consuming task because terms has to be validated by specialists. In order to improve term candidate selection, we have developed the Token Slot Recognition (TSR) method, a filtering strategy based on terminological tokens which is used to rank extracted term candidates from domain-specific corpora. We have implemented this filtering strategy in TBXTools. In this paper we present the system we have used in the TermEval 2020 shared task on monolingual term extraction. We also present the evaluation results for the system for English, French and Dutch and for two corpora: corruption and heart failure. For English and French we have used a linguistic methodology based on POS patterns, and for Dutch we have used a statistical methodology based on n-grams calculation and filtering with stop-words. For all languages, TSR (Token Slot Recognition) filtering method has been applied. We have obtained competitive results, but there is still room for improvement of the system.</abstract>
      <url hash="c4372b61">2020.computerm-1.15</url>
      <language>eng</language>
    </paper>
  </volume>
</collection>
