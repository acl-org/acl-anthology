<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.spanlp">
  <volume id="1" ingest-date="2022-05-15" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 1st Workshop on Semiparametric Methods in NLP: Decoupling Logic from Knowledge</booktitle>
      <editor><first>Rajarshi</first><last>Das</last></editor>
      <editor><first>Patrick</first><last>Lewis</last></editor>
      <editor><first>Sewon</first><last>Min</last></editor>
      <editor><first>June</first><last>Thai</last></editor>
      <editor><first>Manzil</first><last>Zaheer</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland and Online</address>
      <month>May</month>
      <year>2022</year>
      <url hash="a9fd5539">2022.spanlp-1</url>
      <venue>spanlp</venue>
    </meta>
    <frontmatter>
      <url hash="2e49ac48">2022.spanlp-1.0</url>
      <bibkey>spanlp-2022-semiparametric</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Improving Discriminative Learning for Zero-Shot Relation Extraction</title>
      <author><first>Van-Hien</first><last>Tran</last></author>
      <author><first>Hiroki</first><last>Ouchi</last></author>
      <author><first>Taro</first><last>Watanabe</last></author>
      <author><first>Yuji</first><last>Matsumoto</last></author>
      <pages>1-6</pages>
      <abstract>Zero-shot relation extraction (ZSRE) aims to predict target relations that cannot be observed during training. While most previous studies have focused on fully supervised relation extraction and achieved considerably high performance, less effort has been made towards ZSRE. This study proposes a new model incorporating discriminative embedding learning for both sentences and semantic relations. In addition, a self-adaptive comparator network is used to judge whether the relationship between a sentence and a relation is consistent. Experimental results on two benchmark datasets showed that the proposed method significantly outperforms the state-of-the-art methods.</abstract>
      <url hash="742950af">2022.spanlp-1.1</url>
      <bibkey>tran-etal-2022-improving</bibkey>
      <doi>10.18653/v1/2022.spanlp-1.1</doi>
      <video href="2022.spanlp-1.1.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiki-zsl">Wiki-ZSL</pwcdataset>
    </paper>
    <paper id="2">
      <title>Choose Your <fixed-case>QA</fixed-case> Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering</title>
      <author><first>Man</first><last>Luo</last></author>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Zhiwei</first><last>Liu</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <pages>7-22</pages>
      <abstract>While both extractive and generative readers have been successfully applied to the Question Answering (QA) task, little attention has been paid toward the systematic comparison of them. Characterizing the strengths and weaknesses of the two readers is crucial not only for making a more informed reader selection in practice but also for developing a deeper understanding to foster further research on improving readers in a principled manner. Motivated by this goal, we make the first attempt to systematically study the comparison of extractive and generative readers for question answering. To be aligned with the state-of-the-art, we explore nine transformer-based large pre-trained language models (PrLMs) as backbone architectures. Furthermore, we organize our findings under two main categories: (1) keeping the architecture invariant, and (2) varying the underlying PrLMs. Among several interesting findings, it is important to highlight that (1) the generative readers perform better in long context QA, (2) the extractive readers perform better in short context while also showing better out-of-domain generalization, and (3) the encoder of encoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and outperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also study the effect of multi-task learning on the two types of readers varying the underlying PrLMs and perform qualitative and quantitative diagnosis to provide further insights into future directions in modeling better readers.</abstract>
      <url hash="50f4f437">2022.spanlp-1.2</url>
      <bibkey>luo-etal-2022-choose</bibkey>
      <doi>10.18653/v1/2022.spanlp-1.2</doi>
      <video href="2022.spanlp-1.2.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="3">
      <title>Efficient Machine Translation Domain Adaptation</title>
      <author><first>Pedro</first><last>Martins</last></author>
      <author><first>Zita</first><last>Marinho</last></author>
      <author><first>Andre</first><last>Martins</last></author>
      <pages>23-29</pages>
      <abstract>Machine translation models struggle when translating out-of-domain text, which makes domain adaptation a topic of critical importance. However, most domain adaptation methods focus on fine-tuning or training the entire or part of the model on every new domain, which can be costly. On the other hand, semi-parametric models have been shown to successfully perform domain adaptation by retrieving examples from an in-domain datastore (Khandelwal et al., 2021). A drawback of these retrieval-augmented models, however, is that they tend to be substantially slower. In this paper, we explore several approaches to speed up nearest neighbors machine translation. We adapt the methods recently proposed by He et al. (2021) for language modeling, and introduce a simple but effective caching strategy that avoids performing retrieval when similar contexts have been seen before. Translation quality and runtimes for several domains show the effectiveness of the proposed solutions.</abstract>
      <url hash="5884847a">2022.spanlp-1.3</url>
      <bibkey>martins-etal-2022-efficient</bibkey>
      <doi>10.18653/v1/2022.spanlp-1.3</doi>
      <video href="2022.spanlp-1.3.mp4"/>
      <pwccode url="https://github.com/deep-spin/efficient_knn_mt" additional="false">deep-spin/efficient_knn_mt</pwccode>
    </paper>
    <paper id="4">
      <title>Field Extraction from Forms with Unlabeled Data</title>
      <author><first>Mingfei</first><last>Gao</last></author>
      <author><first>Zeyuan</first><last>Chen</last></author>
      <author><first>Nikhil</first><last>Naik</last></author>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Ran</first><last>Xu</last></author>
      <pages>30-40</pages>
      <abstract>We propose a novel framework to conduct field extraction from forms with unlabeled data. To bootstrap the training process, we develop a rule-based method for mining noisy pseudo-labels from unlabeled forms. Using the supervisory signal from the pseudo-labels, we extract a discriminative token representation from a transformer-based model by modeling the interaction between text in the form. To prevent the model from overfitting to label noise, we introduce a refinement module based on a progressive pseudo-label ensemble. Experimental results demonstrate the effectiveness of our framework.</abstract>
      <url hash="5dd5ede6">2022.spanlp-1.4</url>
      <bibkey>gao-etal-2022-field</bibkey>
      <doi>10.18653/v1/2022.spanlp-1.4</doi>
      <video href="2022.spanlp-1.4.mp4"/>
      <pwccode url="https://github.com/salesforce/inv-cdip" additional="true">salesforce/inv-cdip</pwccode>
    </paper>
    <paper id="5">
      <title>Knowledge Base Index Compression via Dimensionality and Precision Reduction</title>
      <author><first>Vil√©m</first><last>Zouhar</last></author>
      <author><first>Marius</first><last>Mosbach</last></author>
      <author><first>Miaoran</first><last>Zhang</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>41-53</pages>
      <abstract>Recently neural network based approaches to knowledge-intensive NLP tasks, such as question answering, started to rely heavily on the combination of neural retrievers and readers. Retrieval is typically performed over a large textual knowledge base (KB) which requires significant memory and compute resources, especially when scaled up. On HotpotQA we systematically investigate reducing the size of the KB index by means of dimensionality (sparse random projections, PCA, autoencoders) and numerical precision reduction. Our results show that PCA is an easy solution that requires very little data and is only slightly worse than autoencoders, which are less stable. All methods are sensitive to pre- and post-processing and data should always be centered and normalized both before and after dimension reduction. Finally, we show that it is possible to combine PCA with using 1bit per dimension. Overall we achieve (1) 100<tex-math>\times</tex-math> compression with 75%, and (2) 24<tex-math>\times</tex-math> compression with 92% original retrieval performance.</abstract>
      <url hash="05382c23">2022.spanlp-1.5</url>
      <bibkey>zouhar-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.spanlp-1.5</doi>
      <video href="2022.spanlp-1.5.mp4"/>
      <revision id="1" href="2022.spanlp-1.5v1" hash="9baaafe5"/>
      <revision id="2" href="2022.spanlp-1.5v2" hash="05382c23" date="2024-03-27">The revision fixed malformed PDF.</revision>
      <pwccode url="https://github.com/zouharvi/kb-shrink" additional="false">zouharvi/kb-shrink</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
  </volume>
</collection>
