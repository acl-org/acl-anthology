<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.gebnlp">
  <volume id="1" ingest-date="2022-06-30" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)</booktitle>
      <editor><first>Christian</first><last>Hardmeier</last></editor>
      <editor><first>Christine</first><last>Basta</last></editor>
      <editor><first>Marta R.</first><last>Costa-jussà</last></editor>
      <editor><first>Gabriel</first><last>Stanovsky</last></editor>
      <editor><first>Hila</first><last>Gonen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Seattle, Washington</address>
      <month>July</month>
      <year>2022</year>
      <url hash="784128f7">2022.gebnlp-1</url>
      <venue>gebnlp</venue>
    </meta>
    <frontmatter>
      <url hash="f5d9b2bf">2022.gebnlp-1.0</url>
      <bibkey>gebnlp-2022-gender</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Analyzing Hate Speech Data along Racial, Gender and Intersectional Axes</title>
      <author><first>Antonis</first><last>Maronikolakis</last></author>
      <author><first>Philip</first><last>Baader</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>1-7</pages>
      <abstract>To tackle the rising phenomenon of hate speech, efforts have been made towards data curation and analysis. When it comes to analysis of bias, previous work has focused predominantly on race. In our work, we further investigate bias in hate speech datasets along racial, gender and intersectional axes. We identify strong bias against African American English (AAE), masculine and AAE+Masculine tweets, which are annotated as disproportionately more hateful and offensive than from other demographics. We provide evidence that BERT-based models propagate this bias and show that balancing the training data for these protected attributes can lead to fairer models with regards to gender, but not race.</abstract>
      <url hash="9e71cf77">2022.gebnlp-1.1</url>
      <bibkey>maronikolakis-etal-2022-analyzing</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Analysis of Gender Bias in Social Perception and Judgement Using <fixed-case>C</fixed-case>hinese Word Embeddings</title>
      <author><first>Jiali</first><last>Li</last></author>
      <author><first>Shucheng</first><last>Zhu</last></author>
      <author><first>Ying</first><last>Liu</last></author>
      <author><first>Pengyuan</first><last>Liu</last></author>
      <pages>8-16</pages>
      <abstract>Gender is a construction in line with social perception and judgment. An important means of this construction is through languages. When natural language processing tools, such as word embeddings, associate gender with the relevant categories of social perception and judgment, it is likely to cause bias and harm to those groups that do not conform to the mainstream social perception and judgment. Using 12,251 Chinese word embeddings as intermedium, this paper studies the relationship between social perception and judgment categories and gender. The results reveal that these grammatical gender-neutral Chinese word embeddings show a certain gender bias, which is consistent with the mainstream society’s perception and judgment of gender. Men are judged by their actions and perceived as bad, easily-disgusted, bad-tempered and rational roles while women are judged by their appearances and perceived as perfect, either happy or sad, and emotional roles.</abstract>
      <url hash="378e2cb9">2022.gebnlp-1.2</url>
      <bibkey>li-etal-2022-analysis</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.2</doi>
      <video href="2022.gebnlp-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Don’t Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information</title>
      <author><first>Tomasz</first><last>Limisiewicz</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <pages>17-29</pages>
      <abstract>The representations in large language models contain multiple types of gender information. We focus on two types of such signals in English texts: factual gender information, which is a grammatical or semantic property, and gender bias, which is the correlation between a word and specific gender. We can disentangle the model’s embeddings and identify components encoding both types of information with probing. We aim to diminish the stereotypical bias in the representations while preserving the factual gender signal. Our filtering method shows that it is possible to decrease the bias of gender-neutral profession names without significant deterioration of language modeling capabilities. The findings can be applied to language generation to mitigate reliance on stereotypes while preserving gender agreement in coreferences.</abstract>
      <url hash="e55899c0">2022.gebnlp-1.3</url>
      <bibkey>limisiewicz-marecek-2022-dont</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.3</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="4">
      <title>Uncertainty and Inclusivity in Gender Bias Annotation: An Annotation Taxonomy and Annotated Datasets of <fixed-case>B</fixed-case>ritish <fixed-case>E</fixed-case>nglish Text</title>
      <author><first>Lucy</first><last>Havens</last></author>
      <author><first>Melissa</first><last>Terras</last></author>
      <author><first>Benjamin</first><last>Bach</last></author>
      <author><first>Beatrice</first><last>Alex</last></author>
      <pages>30-57</pages>
      <abstract>Mitigating harms from gender biased language in Natural Language Processing (NLP) systems remains a challenge, and the situated nature of language means bias is inescapable in NLP data. Though efforts to mitigate gender bias in NLP are numerous, they often vaguely define gender and bias, only consider two genders, and do not incorporate uncertainty into models. To address these limitations, in this paper we present a taxonomy of gender biased language and apply it to create annotated datasets. We created the taxonomy and annotated data with the aim of making gender bias in language transparent. If biases are communicated clearly, varieties of biased language can be better identified and measured. Our taxonomy contains eleven types of gender biases inclusive of people whose gender expressions do not fit into the binary conceptions of woman and man, and whose gender differs from that they were assigned at birth, while also allowing annotators to document unknown gender information. The taxonomy and annotated data will, in future work, underpin analysis and more equitable language model development.</abstract>
      <url hash="e3f1ba8b">2022.gebnlp-1.4</url>
      <bibkey>havens-etal-2022-uncertainty</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.4</doi>
      <revision id="1" href="2022.gebnlp-1.4v1" hash="bd8064b8"/>
      <revision id="2" href="2022.gebnlp-1.4v2" hash="e3f1ba8b" date="2022-11-01">Corrected figures.</revision>
    </paper>
    <paper id="5">
      <title>Debiasing Neural Retrieval via In-batch Balancing Regularization</title>
      <author><first>Yuantong</first><last>Li</last></author>
      <author><first>Xiaokai</first><last>Wei</last></author>
      <author><first>Zijian</first><last>Wang</last></author>
      <author><first>Shen</first><last>Wang</last></author>
      <author><first>Parminder</first><last>Bhatia</last></author>
      <author><first>Xiaofei</first><last>Ma</last></author>
      <author><first>Andrew</first><last>Arnold</last></author>
      <pages>58-66</pages>
      <abstract>People frequently interact with information retrieval (IR) systems, however, IR models exhibit biases and discrimination towards various demographics. The in-processing fair ranking methods provides a trade-offs between accuracy and fairness through adding a fairness-related regularization term in the loss function. However, there haven’t been intuitive objective functions that depend on the click probability and user engagement to directly optimize towards this. In this work, we propose the <b>I</b>n-<b>B</b>atch <b>B</b>alancing <b>R</b>egularization (IBBR) to mitigate the ranking disparity among subgroups. In particular, we develop a differentiable <b>normed Pairwise Ranking Fairness</b> (nPRF) and leverage the T-statistics on top of nPRF over subgroups as a regularization to improve fairness. Empirical results with the BERT-based neural rankers on the MS MARCO Passage Retrieval dataset with the human-annotated non-gendered queries benchmark (CITATION) show that our IBBR method with nPRF achieves significantly less bias with minimal degradation in ranking performance compared with the baseline.</abstract>
      <url hash="2712ac2d">2022.gebnlp-1.5</url>
      <bibkey>li-etal-2022-debiasing</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.5</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="6">
      <title>Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning</title>
      <author><first>Przemyslaw</first><last>Joniak</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>67-73</pages>
      <abstract>Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue. We demonstrate a novel framework for inspecting bias in pre-trained transformer-based language models via movement pruning. Given a model and a debiasing objective, our framework finds a subset of the model containing less bias than the original model. We implement our framework by pruning the model while fine-tuning it on the debasing objective. Optimized are only the pruning scores – parameters coupled with the model’s weights that act as gates. We experiment with pruning attention heads, an important building block of transformers: we prune square blocks, as well as establish a new way of pruning the entire heads. Lastly, we demonstrate the usage of our framework using gender bias, and based on our findings, we propose an improvement to an existing debiasing method. Additionally, we re-discover a bias-performance trade-off: the better the model performs, the more bias it contains.</abstract>
      <url hash="e89343e3">2022.gebnlp-1.6</url>
      <bibkey>joniak-aizawa-2022-gender</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.6</doi>
      <video href="2022.gebnlp-1.6.mp4"/>
      <pwccode url="https://github.com/kainoj/pruning-bias" additional="false">kainoj/pruning-bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="7">
      <title>Gendered Language in Resumes and its Implications for Algorithmic Bias in Hiring</title>
      <author><first>Prasanna</first><last>Parasurama</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <pages>74-74</pages>
      <abstract>Despite growing concerns around gender bias in NLP models used in algorithmic hiring, there is little empirical work studying the extent and nature of gendered language in resumes. Using a corpus of 709k resumes from IT firms, we train a series of models to classify the gender of the applicant, thereby measuring the extent of gendered information encoded in resumes. We also investigate whether it is possible to obfuscate gender from resumes by removing gender identifiers, hobbies, gender sub-space in embedding models, etc. We find that there is a significant amount of gendered information in resumes even after obfuscation.A simple Tf-Idf model can learn to classify gender with AUROC=0.75, and more sophisticated transformer-based models achieve AUROC=0.8.We further find that gender predictive values have low correlation with gender direction of embeddings – meaning that, what is predictive of gender is much more than what is “gendered” in the masculine/feminine sense. We discuss the algorithmic bias and fairness implications of these findings in the hiring context.</abstract>
      <url hash="3f015bc0">2022.gebnlp-1.7</url>
      <bibkey>parasurama-sedoc-2022-gendered</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>The Birth of Bias: A case study on the evolution of gender bias in an <fixed-case>E</fixed-case>nglish language model</title>
      <author><first>Oskar</first><last>Van Der Wal</last></author>
      <author><first>Jaap</first><last>Jumelet</last></author>
      <author><first>Katrin</first><last>Schulz</last></author>
      <author><first>Willem</first><last>Zuidema</last></author>
      <pages>75-75</pages>
      <abstract>Detecting and mitigating harmful biases in modern language models are widely recognized as crucial, open problems. In this paper, we take a step back and investigate how language models come to be biased in the first place. We use a relatively small language model, using the LSTM architecture trained on an English Wikipedia corpus. With full access to the data and to the model parameters as they change during every step while training, we can map in detail how the representation of gender develops, what patterns in the dataset drive this, and how the model’s internal state relates to the bias in a downstream task (semantic textual similarity).We find that the representation of gender is dynamic and identify different phases during training. Furthermore, we show that gender information is represented increasingly locally in the input embeddings of the model and that, as a consequence, debiasing these can be effective in reducing the downstream bias. Monitoring the training dynamics, allows us to detect an asymmetry in how the female and male gender are represented in the input embeddings. This is important, as it may cause naive mitigation strategies to introduce new undesirable biases. We discuss the relevance of the findings for mitigation strategies more generally and the prospects of generalizing our methods to larger language models, the Transformer architecture, other languages and other undesirable biases.</abstract>
      <url hash="dd2b2174">2022.gebnlp-1.8</url>
      <bibkey>van-der-wal-etal-2022-birth</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.8</doi>
      <pwccode url="https://github.com/bias-barometer/birth-of-bias" additional="false">bias-barometer/birth-of-bias</pwccode>
    </paper>
    <paper id="9">
      <title>Challenges in Measuring Bias via Open-Ended Language Generation</title>
      <author><first>Afra Feyza</first><last>Akyürek</last></author>
      <author><first>Muhammed Yusuf</first><last>Kocyigit</last></author>
      <author><first>Sejin</first><last>Paik</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>76-76</pages>
      <abstract>Researchers have devised numerous ways to quantify social biases vested in pretrained language models. As some language models are capable of generating coherent completions given a set of textual prompts, several prompting datasets have been proposed to measure biases between social groups—posing language generation as a way of identifying biases. In this opinion paper, we analyze how specific choices of prompt sets, metrics, automatic tools and sampling strategies affect bias results. We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings. We additionally provide recommendations for reporting biases in open-ended language generation for a more complete outlook of biases exhibited by a given language model. Code to reproduce the results is released under <url>https://github.com/feyzaakyurek/bias-textgen</url>.</abstract>
      <url hash="61f2116c">2022.gebnlp-1.9</url>
      <bibkey>akyurek-etal-2022-challenges</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.9</doi>
      <video href="2022.gebnlp-1.9.mp4"/>
      <pwccode url="https://github.com/feyzaakyurek/bias-textgen" additional="false">feyzaakyurek/bias-textgen</pwccode>
    </paper>
    <paper id="10">
      <title>Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models</title>
      <author><first>Tejas</first><last>Srinivasan</last></author>
      <author><first>Yonatan</first><last>Bisk</last></author>
      <pages>77-85</pages>
      <abstract>Numerous works have analyzed biases in vision and pre-trained language models individually - however, less attention has been paid to how these biases interact in multimodal settings. This work extends text-based bias analysis methods to investigate multimodal language models, and analyzes intra- and inter-modality associations and biases learned by these models. Specifically, we demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often preferring to reinforce a stereotype over faithfully describing the visual scene. We demonstrate these findings on a controlled case-study and extend them for a larger set of stereotypically gendered entities.</abstract>
      <url hash="9931af56">2022.gebnlp-1.10</url>
      <bibkey>srinivasan-bisk-2022-worst</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.10</doi>
      <video href="2022.gebnlp-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback</title>
      <author><first>Emmy</first><last>Liu</last></author>
      <author><first>Michael Henry</first><last>Tessler</last></author>
      <author><first>Nicole</first><last>Dubosh</last></author>
      <author><first>Katherine</first><last>Hiller</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>86-93</pages>
      <abstract>Though approximately 50% of medical school graduates today are women, female physicians tend to be underrepresented in senior positions, make less money than their male counterparts and receive fewer promotions. There is a growing body of literature demonstrating gender bias in various forms of evaluation in medicine, but this work was mainly conducted by looking for specific words using fixed dictionaries such as LIWC and focused on global assessments of performance such as recommendation letters. We use a dataset of written and quantitative assessments of medical student performance on individual shifts of work, collected across multiple institutions, to investigate the extent to which gender bias exists in a day-to-day context for medical students. We investigate differences in the narrative comments given to male and female students by both male or female faculty assessors, using a fine-tuned BERT model. This allows us to examine whether groups are written about in systematically different ways, without relying on hand-crafted wordlists or topic models. We compare these results to results from the traditional LIWC method and find that, although we find no evidence of group-level gender bias in this dataset, terms related to family and children are used more in feedback given to women.</abstract>
      <url hash="35e03906">2022.gebnlp-1.11</url>
      <bibkey>liu-etal-2022-assessing</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>On the Dynamics of Gender Learning in Speech Translation</title>
      <author><first>Beatrice</first><last>Savoldi</last></author>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>94-111</pages>
      <abstract>Due to the complexity of bias and the opaque nature of current neural approaches, there is a rising interest in auditing language technologies. In this work, we contribute to such a line of inquiry by exploring the emergence of gender bias in Speech Translation (ST). As a new perspective, rather than focusing on the final systems only, we examine their evolution over the course of training. In this way, we are able to account for different variables related to the learning dynamics of gender translation, and investigate when and how gender divides emerge in ST. Accordingly, for three language pairs (en ? es, fr, it) we compare how ST systems behave for masculine and feminine translation at several levels of granularity. We find that masculine and feminine curves are dissimilar, with the feminine one being characterized by more erratic behaviour and late improvements over the course of training. Also, depending on the considered phenomena, their learning trends can be either antiphase or parallel. Overall, we show how such a progressive analysis can inform on the reliability and time-wise acquisition of gender, which is concealed by static evaluations and standard metrics.</abstract>
      <url hash="31a206c1">2022.gebnlp-1.12</url>
      <bibkey>savoldi-etal-2022-dynamics</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.12</doi>
      <video href="2022.gebnlp-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias</title>
      <author><first>Yarden</first><last>Tal</last></author>
      <author><first>Inbal</first><last>Magar</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <pages>112-120</pages>
      <abstract>The size of pretrained models is increasing, and so is their performance on a variety of NLP tasks. However, as their memorization capacity grows, they might pick up more social biases. In this work, we examine the connection between model size and its gender bias (specifically, occupational gender bias). We measure bias in three masked language model families (RoBERTa, DeBERTa, and T5) in two setups: directly using prompt based method, and using a downstream task (Winogender). We find on the one hand that larger models receive higher bias scores on the former task, but when evaluated on the latter, they make fewer gender errors. To examine these potentially conflicting results, we carefully investigate the behavior of the different models on Winogender. We find that while larger models outperform smaller ones, the probability that their mistakes are caused by gender bias is higher. Moreover, we find that the proportion of stereotypical errors compared to anti-stereotypical ones grows with the model size. Our findings highlight the potential risks that can arise from increasing model size.</abstract>
      <url hash="f940637e">2022.gebnlp-1.13</url>
      <bibkey>tal-etal-2022-fewer</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.13</doi>
      <video href="2022.gebnlp-1.13.mp4"/>
      <pwccode url="https://github.com/schwartz-lab-nlp/model_size_and_gender_bias" additional="false">schwartz-lab-nlp/model_size_and_gender_bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/stereoset">StereoSet</pwcdataset>
    </paper>
    <paper id="14">
      <title>Unsupervised Mitigating Gender Bias by Character Components: A Case Study of <fixed-case>C</fixed-case>hinese Word Embedding</title>
      <author><first>Xiuying</first><last>Chen</last></author>
      <author><first>Mingzhe</first><last>Li</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <author><first>Xin</first><last>Gao</last></author>
      <author><first>Xiangliang</first><last>Zhang</last></author>
      <pages>121-128</pages>
      <abstract>Word embeddings learned from massive text collections have demonstrated significant levels of discriminative biases. However, debias on the Chinese language, one of the most spoken languages, has been less explored. Meanwhile, existing literature relies on manually created supplementary data, which is time- and energy-consuming. In this work, we propose the first Chinese Gender-neutral word Embedding model (CGE) based on Word2vec, which learns gender-neutral word embeddings without any labeled data. Concretely, CGE utilizes and emphasizes the rich feminine and masculine information contained in radicals, i.e., a kind of component in Chinese characters, during the training procedure. This consequently alleviates discriminative gender biases. Experimental results on public benchmark datasets show that our unsupervised method outperforms the state-of-the-art supervised debiased word embedding models without sacrificing the functionality of the embedding model.</abstract>
      <url hash="dba14bca">2022.gebnlp-1.14</url>
      <bibkey>chen-etal-2022-unsupervised</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>An Empirical Study on the Fairness of Pre-trained Word Embeddings</title>
      <author><first>Emeralda</first><last>Sesari</last></author>
      <author><first>Max</first><last>Hort</last></author>
      <author><first>Federica</first><last>Sarro</last></author>
      <pages>129-144</pages>
      <abstract>Pre-trained word embedding models are easily distributed and applied, as they alleviate users from the effort to train models themselves. With widely distributed models, it is important to ensure that they do not exhibit undesired behaviour, such as biases against population groups. For this purpose, we carry out an empirical study on evaluating the bias of 15 publicly available, pre-trained word embeddings model based on three training algorithms (GloVe, word2vec, and fastText) with regard to four bias metrics (WEAT, SEMBIAS,DIRECT BIAS, and ECT). The choice of word embedding models and bias metrics is motivated by a literature survey over 37 publications which quantified bias on pre-trained word embeddings. Our results indicate that fastText is the least biased model (in 8 out of 12 cases) and small vector lengths lead to a higher bias.</abstract>
      <url hash="99bf4d0d">2022.gebnlp-1.15</url>
      <bibkey>sesari-etal-2022-empirical</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.15</doi>
      <video href="2022.gebnlp-1.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Mitigating Gender Stereotypes in <fixed-case>H</fixed-case>indi and <fixed-case>M</fixed-case>arathi</title>
      <author><first>Neeraja</first><last>Kirtane</last></author>
      <author><first>Tanvi</first><last>Anand</last></author>
      <pages>145-150</pages>
      <abstract>As the use of natural language processing increases in our day-to-day life, the need to address gender bias inherent in these systems also amplifies. This is because the inherent bias interferes with the semantic structure of the output of these systems while performing tasks in natural language processing. While research is being done in English to quantify and mitigate bias, debiasing methods in Indic Languages are either relatively nascent or absent for some Indic languages altogether. Most Indic languages are gendered, i.e., each noun is assigned a gender according to each language’s rules of grammar. As a consequence, evaluation differs from what is done in English. This paper evaluates the gender stereotypes in Hindi and Marathi languages. The methodologies will differ from the ones in the English language because there are masculine and feminine counterparts in the case of some words. We create a dataset of neutral and gendered occupation words, emotion words and measure bias with the help of Embedding Coherence Test (ECT) and Relative Norm Distance (RND). We also attempt to mitigate this bias from the embeddings. Experiments show that our proposed debiasing techniques reduce gender bias in these languages.</abstract>
      <url hash="305e9954">2022.gebnlp-1.16</url>
      <attachment type="dataset" hash="0fa48b36">2022.gebnlp-1.16.dataset.zip</attachment>
      <bibkey>kirtane-anand-2022-mitigating</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Choose Your Lenses: Flaws in Gender Bias Evaluation</title>
      <author><first>Hadas</first><last>Orgad</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <pages>151-167</pages>
      <abstract>Considerable efforts to measure and mitigate gender bias in recent years have led to the introduction of an abundance of tasks, datasets, and metrics used in this vein. In this position paper, we assess the current paradigm of gender bias evaluation and identify several flaws in it. First, we highlight the importance of extrinsic bias metrics that measure how a model’s performance on some task is affected by gender, as opposed to intrinsic evaluations of model representations, which are less strongly connected to specific harms to people interacting with systems. We find that only a few extrinsic metrics are measured in most studies, although more can be measured. Second, we find that datasets and metrics are often coupled, and discuss how their coupling hinders the ability to obtain reliable conclusions, and how one may decouple them. We then investigate how the choice of the dataset and its composition, as well as the choice of the metric, affect bias measurement, finding significant variations across each of them. Finally, we propose several guidelines for more reliable gender bias evaluation.</abstract>
      <url hash="72d9507a">2022.gebnlp-1.17</url>
      <bibkey>orgad-belinkov-2022-choose</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.17</doi>
      <video href="2022.gebnlp-1.17.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="18">
      <title>A Taxonomy of Bias-Causing Ambiguities in Machine Translation</title>
      <author><first>Michal</first><last>Měchura</last></author>
      <pages>168-173</pages>
      <abstract>This paper introduces a taxonomy of phenomena which cause bias in machine translation, covering gender bias (people being male and/or female), number bias (singular you versus plural you) and formality bias (informal you versus formal you). Our taxonomy is a formalism for describing situations in machine translation when the source text leaves some of these properties unspecified (eg. does not say whether doctor is male or female) but the target language requires the property to be specified (eg. because it does not have a gender-neutral word for doctor). The formalism described here is used internally by a web-based tool we have built for detecting and correcting bias in the output of any machine translator.</abstract>
      <url hash="f61f6167">2022.gebnlp-1.18</url>
      <bibkey>mechura-2022-taxonomy</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>On Gender Biases in Offensive Language Classification Models</title>
      <author><first>Sanjana</first><last>Marcé</last></author>
      <author><first>Adam</first><last>Poliak</last></author>
      <pages>174-183</pages>
      <abstract>We explore whether neural Natural Language Processing models trained to identify offensive language in tweets contain gender biases. We add historically gendered and gender ambiguous American names to an existing offensive language evaluation set to determine whether models? predictions are sensitive or robust to gendered names. While we see some evidence that these models might be prone to biased stereotypes that men use more offensive language than women, our results indicate that these models? binary predictions might not greatly change based upon gendered names.</abstract>
      <url hash="622072dd">2022.gebnlp-1.19</url>
      <bibkey>marce-poliak-2022-gender</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.19</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="20">
      <title>Gender Bias in <fixed-case>BERT</fixed-case> - Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task</title>
      <author><first>Sophie</first><last>Jentzsch</last></author>
      <author><first>Cigdem</first><last>Turan</last></author>
      <pages>184-199</pages>
      <abstract>Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT?s biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.</abstract>
      <url hash="18ad5dea">2022.gebnlp-1.20</url>
      <attachment type="note" hash="b5139377">2022.gebnlp-1.20.note.pdf</attachment>
      <bibkey>jentzsch-turan-2022-gender</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.20</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="21">
      <title>Occupational Biases in <fixed-case>N</fixed-case>orwegian and Multilingual Language Models</title>
      <author><first>Samia</first><last>Touileb</last></author>
      <author><first>Lilja</first><last>Øvrelid</last></author>
      <author><first>Erik</first><last>Velldal</last></author>
      <pages>200-211</pages>
      <abstract>In this paper we explore how a demographic distribution of occupations, along gender dimensions, is reflected in pre-trained language models. We give a descriptive assessment of the distribution of occupations, and investigate to what extent these are reflected in four Norwegian and two multilingual models. To this end, we introduce a set of simple bias probes, and perform five different tasks combining gendered pronouns, first names, and a set of occupations from the Norwegian statistics bureau. We show that language specific models obtain more accurate results, and are much closer to the real-world distribution of clearly gendered occupations. However, we see that none of the models have correct representations of the occupations that are demographically balanced between genders. We also discuss the importance of the training data on which the models were trained on, and argue that template-based bias probes can sometimes be fragile, and a simple alteration in a template can change a model’s behavior.</abstract>
      <url hash="9338aa10">2022.gebnlp-1.21</url>
      <bibkey>touileb-etal-2022-occupational</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.21</doi>
      <pwccode url="https://github.com/samiatouileb/biases-norwegian-multilingual-lms" additional="false">samiatouileb/biases-norwegian-multilingual-lms</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="22">
      <title>Looking for a Handsome Carpenter! Debiasing <fixed-case>GPT</fixed-case>-3 Job Advertisements</title>
      <author><first>Conrad</first><last>Borchers</last></author>
      <author><first>Dalia</first><last>Gala</last></author>
      <author><first>Benjamin</first><last>Gilburt</last></author>
      <author><first>Eduard</first><last>Oravkin</last></author>
      <author><first>Wilfried</first><last>Bounsi</last></author>
      <author><first>Yuki M</first><last>Asano</last></author>
      <author><first>Hannah</first><last>Kirk</last></author>
      <pages>212-224</pages>
      <abstract>The growing capability and availability of generative language models has enabled a wide range of new downstream tasks. Academic research has identified, quantified and mitigated biases present in language models but is rarely tailored to downstream tasks where wider impact on individuals and society can be felt. In this work, we leverage one popular generative language model, GPT-3, with the goal of writing unbiased and realistic job advertisements. We first assess the bias and realism of zero-shot generated advertisements and compare them to real-world advertisements. We then evaluate prompt-engineering and fine-tuning as debiasing methods. We find that prompt-engineering with diversity-encouraging prompts gives no significant improvement to bias, nor realism. Conversely, fine-tuning, especially on unbiased real advertisements, can improve realism and reduce bias.</abstract>
      <url hash="f985e0a2">2022.gebnlp-1.22</url>
      <bibkey>borchers-etal-2022-looking</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.22</doi>
      <video href="2022.gebnlp-1.22.mp4"/>
      <pwccode url="https://github.com/oxai/gpt3-jobadvert-bias" additional="false">oxai/gpt3-jobadvert-bias</pwccode>
    </paper>
    <paper id="23">
      <title><fixed-case>H</fixed-case>etero<fixed-case>C</fixed-case>orpus: A Corpus for Heteronormative Language Detection</title>
      <author><first>Juan</first><last>Vásquez</last></author>
      <author><first>Gemma</first><last>Bel-Enguix</last></author>
      <author><first>Scott Thomas</first><last>Andersen</last></author>
      <author><first>Sergio-Luis</first><last>Ojeda-Trueba</last></author>
      <pages>225-234</pages>
      <abstract>In recent years, plenty of work has been done by the NLP community regarding gender bias detection and mitigation in language systems. Yet, to our knowledge, no one has focused on the difficult task of heteronormative language detection and mitigation. We consider this an urgent issue, since language technologies are growing increasingly present in the world and, as it has been proven by various studies, NLP systems with biases can create real-life adverse consequences for women, gender minorities and racial minorities and queer people. For these reasons, we propose and evaluate HeteroCorpus; a corpus created specifically for studying heterononormative language in English. Additionally, we propose a baseline set of classification experiments on our corpus, in order to show the performance of our corpus in classification tasks.</abstract>
      <url hash="e344fc03">2022.gebnlp-1.23</url>
      <attachment type="dataset" hash="21dc9aac">2022.gebnlp-1.23.dataset.zip</attachment>
      <bibkey>vasquez-etal-2022-heterocorpus</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.23</doi>
      <video href="2022.gebnlp-1.23.mp4"/>
      <pwccode url="https://github.com/juanmvsa/heterocorpus" additional="false">juanmvsa/heterocorpus</pwccode>
    </paper>
    <paper id="24">
      <title>Evaluating Gender Bias Transfer from Film Data</title>
      <author><first>Amanda</first><last>Bertsch</last></author>
      <author><first>Ashley</first><last>Oh</last></author>
      <author><first>Sanika</first><last>Natu</last></author>
      <author><first>Swetha</first><last>Gangu</last></author>
      <author><first>Alan W.</first><last>Black</last></author>
      <author><first>Emma</first><last>Strubell</last></author>
      <pages>235-243</pages>
      <abstract>Films are a rich source of data for natural language processing. OpenSubtitles (Lison and Tiedemann, 2016) is a popular movie script dataset, used for training models for tasks such as machine translation and dialogue generation. However, movies often contain biases that reflect society at the time, and these biases may be introduced during pre-training and influence downstream models. We perform sentiment analysis on template infilling (Kurita et al., 2019) and the Sentence Embedding Association Test (May et al., 2019) to measure how BERT-based language models change after continued pre-training on OpenSubtitles. We consider gender bias as a primary motivating case for this analysis, while also measuring other social biases such as disability. We show that sentiment analysis on template infilling is not an effective measure of bias due to the rarity of disability and gender identifying tokens in the movie dialogue. We extend our analysis to a longitudinal study of bias in film dialogue over the last 110 years and find that continued pre-training on OpenSubtitles encodes additional bias into BERT. We show that BERT learns associations that reflect the biases and representation of each film era, suggesting that additional care must be taken when using historical data.</abstract>
      <url hash="64eec327">2022.gebnlp-1.24</url>
      <bibkey>bertsch-etal-2022-evaluating</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.24</doi>
      <video href="2022.gebnlp-1.24.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="25">
      <title>Indigenous Language Revitalization and the Dilemma of Gender Bias</title>
      <author><first>Oussama</first><last>Hansal</last></author>
      <author><first>Ngoc Tan</first><last>Le</last></author>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <pages>244-254</pages>
      <abstract>Natural Language Processing (NLP), through its several applications, has been considered as one of the most valuable field in interdisciplinary researches, as well as in computer science. However, it is not without its flaws. One of the most common flaws is bias. This paper examines the main linguistic challenges of Inuktitut, an indigenous language of Canada, and focuses on gender bias identification and mitigation. We explore the unique characteristics of this language to help us understand the right techniques that can be used to identify and mitigate implicit biases. We use some methods to quantify the gender bias existing in Inuktitut word embeddings; then we proceed to mitigate the bias and evaluate the performance of the debiased embeddings. Next, we explain how approaches for detecting and reducing bias in English embeddings may be transferred to Inuktitut embeddings by properly taking into account the language’s particular characteristics. Next, we compare the effect of the debiasing techniques on Inuktitut and English. Finally, we highlight some future research directions which will further help to push the boundaries.</abstract>
      <url hash="37d5f04e">2022.gebnlp-1.25</url>
      <bibkey>hansal-etal-2022-indigenous</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.25</doi>
    </paper>
    <paper id="26">
      <title>What changed? Investigating Debiasing Methods using Causal Mediation Analysis</title>
      <author><first>Sullam</first><last>Jeoung</last></author>
      <author><first>Jana</first><last>Diesner</last></author>
      <pages>255-265</pages>
      <abstract>Previous work has examined how debiasing language models affect downstream tasks, specifically, how debiasing techniques influence task performance and whether debiased models also make impartial predictions in downstream tasks or not. However, what we don’t understand well yet is why debiasing methods have varying impacts on downstream tasks and how debiasing techniques affect internal components of language models, i.e., neurons, layers, and attentions. In this paper, we decompose the internal mechanisms of debiasing language models with respect to gender by applying causal mediation analysis to understand the influence of debiasing methods on toxicity detection as a downstream task. Our findings suggest a need to test the effectiveness of debiasing methods with different bias metrics, and to focus on changes in the behavior of certain components of the models, e.g.,first two layers of language models, and attention heads.</abstract>
      <url hash="d41f990e">2022.gebnlp-1.26</url>
      <bibkey>jeoung-diesner-2022-changed</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.26</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="27">
      <title>Why Knowledge Distillation Amplifies Gender Bias and How to Mitigate from the Perspective of <fixed-case>D</fixed-case>istil<fixed-case>BERT</fixed-case></title>
      <author><first>Jaimeen</first><last>Ahn</last></author>
      <author><first>Hwaran</first><last>Lee</last></author>
      <author><first>Jinhwa</first><last>Kim</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>266-272</pages>
      <abstract>Knowledge distillation is widely used to transfer the language understanding of a large model to a smaller model. However, after knowledge distillation, it was found that the smaller model is more biased by gender compared to the source large model. This paper studies what causes gender bias to increase after the knowledge distillation process. Moreover, we suggest applying a variant of the mixup on knowledge distillation, which is used to increase generalizability during the distillation process, not for augmentation. By doing so, we can significantly reduce the gender bias amplification after knowledge distillation. We also conduct an experiment on the GLUE benchmark to demonstrate that even if the mixup is applied, it does not have a significant adverse effect on the model’s performance.</abstract>
      <url hash="3933cfd4">2022.gebnlp-1.27</url>
      <bibkey>ahn-etal-2022-knowledge</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.27</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="28">
      <title>Incorporating Subjectivity into Gendered Ambiguous Pronoun (<fixed-case>GAP</fixed-case>) Resolution using Style Transfer</title>
      <author><first>Kartikey</first><last>Pant</last></author>
      <author><first>Tanvi</first><last>Dadu</last></author>
      <pages>273-281</pages>
      <abstract>The GAP dataset is a Wikipedia-based evaluation dataset for gender bias detection in coreference resolution, containing mostly objective sentences. Since subjectivity is ubiquitous in our daily texts, it becomes necessary to evaluate models for both subjective and objective instances. In this work, we present a new evaluation dataset for gender bias in coreference resolution, GAP-Subjective, which increases the coverage of the original GAP dataset by including subjective sentences. We outline the methodology used to create this dataset. Firstly, we detect objective sentences and transfer them into their subjective variants using a sequence-to-sequence model. Secondly, we outline the thresholding techniques based on fluency and content preservation to maintain the quality of the sentences. Thirdly, we perform automated and human-based analysis of the style transfer and infer that the transferred sentences are of high quality. Finally, we benchmark both GAP and GAP-Subjective datasets using a BERT-based model and analyze its predictive performance and gender bias.</abstract>
      <url hash="c5574347">2022.gebnlp-1.28</url>
      <bibkey>pant-dadu-2022-incorporating</bibkey>
      <doi>10.18653/v1/2022.gebnlp-1.28</doi>
    </paper>
  </volume>
</collection>
