<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.in2writing">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Fourth Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2025)</booktitle>
      <editor><first>Vishakh</first><last>Padmakumar</last></editor>
      <editor><first>Katy</first><last>Gero</last></editor>
      <editor><first>Thiemo</first><last>Wambsganss</last></editor>
      <editor><first>Sarah</first><last>Sterman</last></editor>
      <editor id="ting-hao-huang"><first>Ting-Hao</first><last>Huang</last></editor>
      <editor><first>David</first><last>Zhou</last></editor>
      <editor><first>John</first><last>Chung</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico, US</address>
      <month>May</month>
      <year>2025</year>
      <url hash="2c1b9963">2025.in2writing-1</url>
      <venue>in2writing</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-239-8</isbn>
      <doi>10.18653/v1/2025.in2writing-1</doi>
    </meta>
    <frontmatter>
      <url hash="e7881c8a">2025.in2writing-1.0</url>
      <bibkey>in2writing-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Understanding Writing Assistants for Scientific Figure Captions: A Thematic Analysis</title>
      <author><first>Ho Yin Sam</first><last>Ng</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Ting-Yao</first><last>Hsu</last><affiliation>Pennsylvania State University</affiliation></author>
      <author id="jiyoo-min" orcid="0009-0002-2991-6875"><first>Jiyoo</first><last>Min</last><affiliation>Seoul City University</affiliation></author>
      <author id="sungchul-kim" orcid="0000-0003-3580-5290"><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author id="ryan-a-rossi" orcid="0000-0001-9758-0635"><first>Ryan A.</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author id="tong-yu" orcid="0000-0002-5991-2050"><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author id="hyunggu-jung" orcid="0000-0002-2967-4370"><first>Hyunggu</first><last>Jung</last><affiliation>Seoul National University</affiliation></author>
      <author id="ting-hao-huang" orcid="0000-0001-7021-4627"><first>Ting-Hao Kenneth</first><last>Huang</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>1-10</pages>
      <abstract>Scientific figure captions are essential for communicating complex data but are often overlooked, leading to unclear or redundant descriptions. While many studies focus on generating captions as an ‘output’, little attention has been given to the writer’s process of crafting captions for scientific figures. This study examines how researchers use AI-generated captions to support caption writing. Through thematic analysis of interviews and video recordings with 18 participants from diverse disciplines, we identified four key themes: (1) integrating captions with figures and text, (2) bridging gaps between language proficiency and domain expertise, (3) leveraging multiple AI-generated suggestions, and (4) adapting to diverse writing norms. These findings provide actionable design insights for developing AI writing assistants that better support researchers in creating effective scientific figure captions.</abstract>
      <url hash="09d7e338">2025.in2writing-1.1</url>
      <bibkey>ng-etal-2025-understanding</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>ARWI</fixed-case>: <fixed-case>A</fixed-case>rabic Write and Improve</title>
      <author id="kirill-chirkunov" orcid="0000-0002-2328-6035"><first>Kirill</first><last>Chirkunov</last></author>
      <author id="bashar-alhafni"><first>Bashar</first><last>Alhafni</last><affiliation>New York University</affiliation></author>
      <author><first>Chatrine</first><last>Qwaider</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Chalmers University of Technology</affiliation></author>
      <author id="nizar-habash" orcid="0000-0002-1831-3457"><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author id="ted-briscoe"><first>Ted</first><last>Briscoe</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>11-18</pages>
      <abstract>Although Arabic is spoken by over 400 million people, advanced Arabic writing assistance tools remain limited. To address this gap, we present ARWI, a new writing assistant that helps learners improve essay writing in Modern Standard Arabic. ARWI is the first publicly available Arabic writing assistant to include a prompt database for different proficiency levels, an Arabic text editor, state-of-the-art grammatical error detection and correction, and automated essay scoring aligned with the Common European Framework of Reference standards for language attainment (https://arwi.mbzuai.ac.ae/). Moreover, ARWI can be used to gather a growing auto-annotated corpus, facilitating further research on Arabic grammar correction and essay scoring, as well as profiling patterns of errors made by native speakers and non-native learners. A preliminary user study shows that ARWI provides actionable feedback, helping learners identify grammatical gaps, assess language proficiency, and guide improvement.</abstract>
      <url hash="b854702c">2025.in2writing-1.2</url>
      <bibkey>chirkunov-etal-2025-arwi</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>R</fixed-case>ead<fixed-case>C</fixed-case>trl: Personalizing text generation with readability-controlled instruction learning</title>
      <author><first>Hieu</first><last>Tran</last></author>
      <author id="zonghai-yao" orcid="0000-0002-5707-8410"><first>Zonghai</first><last>Yao</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Lingxi</first><last>Li</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>19-36</pages>
      <abstract>Content generation conditioning on users’ readability is an important application for personalization. In an era of large language models (LLMs), readability-controlled text generation based on LLMs has become increasingly important. This paper introduces a novel methodology called “Readability-Controlled Instruction Learning (ReadCtrl),” which aims to instruction-tune LLMs to tailor users’ readability levels. Unlike the traditional methods, which primarily focused on categorical readability adjustments—typically classified as high, medium, and low or expert and layperson levels—with limited success, ReadCtrl introduces a dynamic framework that enables LLMs to generate content at various (near continuous level) complexity levels, thereby enhancing their versatility across different applications. Our results show that the ReadCtrl-Mistral-7b models significantly outperformed strong baseline models such as GPT-4 and Claude-3, with a win rate of 52.1%:35.7% against GPT-4 in human evaluations. Furthermore, ReadCtrl has shown significant improvements in automatic evaluations, as evidenced by better readability metrics (e.g., FOG, FKGL) and generation quality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and Coherence). These results underscore ReadCtrl’s effectiveness and tenacity in producing high-quality, contextually appropriate outputs that closely align with targeted readability levels, marking a significant advancement in personalized content generation using LLMs.</abstract>
      <url hash="a3feee05">2025.in2writing-1.3</url>
      <bibkey>tran-etal-2025-readctrl</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.3</doi>
    </paper>
    <paper id="4">
      <title><fixed-case>AI</fixed-case> Writing Assistants in Tanzanian Universities: Adoption Trends, Challenges, and Opportunities</title>
      <author id="alfred-malengo-kondoro" orcid="0009-0000-6664-7526"><first>Alfred Malengo</first><last>Kondoro</last></author>
      <pages>37-46</pages>
      <abstract>This study examines the adoption, challenges, and impact of AI writing assistants in Tanzanian universities, with a focus on their role in supporting academic writing, enhancing accessibility, and accommodating low-resource languages such as Swahili. Through a structured survey of 1,005 university students, we analyze AI usage patterns, key barriers to adoption, and the improvements needed to make AI writing assistants more inclusive and effective. Findings reveal that limited Swahili integration, affordability constraints, and ethical concerns hinder AI adoption, disproportionately affecting students in resource-constrained settings. To address these challenges, we propose strategies for adapting AI models to diverse linguistic, academic, and infrastructural contexts, emphasizing Swahili-language support, AI literacy initiatives, and accessibility-focused AI development. By bridging these gaps, this study contributes to the development of AI-driven educational tools that are more equitable, contextually relevant, and effective for students in Tanzania and beyond.</abstract>
      <url hash="603ce636">2025.in2writing-1.4</url>
      <bibkey>kondoro-2025-ai</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.4</doi>
    </paper>
    <paper id="5">
      <title>From Crafting Text to Crafting Thought: Grounding Intelligent Writing Support to Writing Center Pedagogy</title>
      <author><first>Yijun</first><last>Liu</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Tal</first><last>August</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>47-61</pages>
      <abstract>Intelligent writing support tools have evolved from solving surface-level issues to collaborating and creating language with writers. Along with these new capabilities come concerns that generated fluent text can impact writers’ processes in unintended ways, especially for students. In this workshop paper, we look to a similar transition that writing centers experienced over the last century, which shifted focus from fixing surface-level issues to maintaining student writer voices. We interviewed 10 current writing tutors and grounded their described practices with ideas proposed in writing center literature. We employed these strategies in developing an intelligent writing tool prototype. We describe the design of our tool and discuss potential evaluations along with how to foster deeper relationships between writers and writing centers using intelligent writing tools.</abstract>
      <url hash="2642f768">2025.in2writing-1.5</url>
      <bibkey>liu-august-2025-crafting</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.5</doi>
    </paper>
    <paper id="6">
      <title>Interaction-Required Suggestions for Control, Ownership, and Awareness in Human-<fixed-case>AI</fixed-case> Co-Writing</title>
      <author><first>Kenneth C.</first><last>Arnold</last><affiliation>Calvin University</affiliation></author>
      <author id="jiho-kim" orcid="0000-0002-1434-4440"><first>Jiho</first><last>Kim</last><affiliation>Calvin University</affiliation></author>
      <pages>62-68</pages>
      <abstract>This paper explores interaction designs for generative AI interfaces that necessitate human involvement throughout the generation process. We argue that such interfaces can promote cognitive engagement, agency, and thoughtful decision-making. Through a case study in text revision, we present and analyze two interaction techniques: (1) using a predictive-text interaction to type the agent’s response to a revision request, and (2) highlighting potential edit opportunities in a document. Our implementations demonstrate how these approaches reveal the landscape of writing possibilities and enable fine-grained control. We discuss implications for human-AI writing partnerships and future interaction design directions.</abstract>
      <url hash="041ffa8b">2025.in2writing-1.6</url>
      <bibkey>arnold-kim-2025-interaction</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.6</doi>
    </paper>
    <paper id="7">
      <title>Voice Interaction With Conversational <fixed-case>AI</fixed-case> Could Facilitate Thoughtful Reflection and Substantive Revision in Writing</title>
      <author id="jiho-kim" orcid="0000-0002-1434-4440"><first>Jiho</first><last>Kim</last><affiliation>Calvin University</affiliation></author>
      <author><first>Philippe</first><last>Laban</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kenneth C.</first><last>Arnold</last><affiliation>Calvin University</affiliation></author>
      <pages>69-73</pages>
      <abstract>Writing well requires not only expressing ideas but also refining them through revision, a process facilitated by reflection. Prior research suggests that feedback delivered through dialogues, such as those in writing center tutoring sessions, can help writers reflect more thoughtfully on their work compared to static feedback. Recent advancements in multi-modal large language models (LLMs) now offer new possibilities for supporting interactive and expressive voice-based reflection in writing. In particular, we propose that LLM-generated static feedback can be repurposed as conversation starters, allowing writers to seek clarification, request examples, and ask follow-up questions, thereby fostering deeper reflection on their writing. We argue that voice-based interaction can naturally facilitate this conversational exchange, encouraging writers’ engagement with higher-order concerns, facilitating iterative refinement of their reflections, and reduce cognitive load compared to text-based interactions. To investigate these effects, we propose a formative study exploring how text vs. voice input influence writers’ reflection and subsequent revisions. Findings from this study will inform the design of intelligent and interactive writing tools, offering insights into how voice-based interactions with LLM-powered conversational agents can support reflection and revision.</abstract>
      <url hash="a0f4effb">2025.in2writing-1.7</url>
      <bibkey>kim-etal-2025-voice</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>RONA</fixed-case>: Pragmatically Diverse Image Captioning with Coherence Relations</title>
      <author><first>Aashish</first><last>Anantha Ramakrishnan</last><affiliation>Pennsylvania State University, Pennsylvania State University</affiliation></author>
      <author><first>Aadarsh Anantha</first><last>Ramakrishnan</last></author>
      <author id="dongwon-lee" orcid="0000-0001-8371-7629"><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>74-86</pages>
      <abstract>Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally generate diverse image captions by employing syntactic and semantic variations to describe image components. However, human-written captions prioritize conveying a central message alongside visual descriptions using pragmatic cues. To enhance caption diversity, it is essential to explore alternative ways of communicating these messages in conjunction with visual content. We propose RONA, a novel prompting strategy for Multi-modal Large Language Models (MLLM) that leverages Coherence Relations as a controllable axis for pragmatic variations. We demonstrate that RONA generates captions with better overall diversity and ground-truth alignment, compared to MLLM baselines across multiple domains. Our code is available at: https://github.com/aashish2000/RONA</abstract>
      <url hash="b7c199e3">2025.in2writing-1.8</url>
      <bibkey>anantha-ramakrishnan-etal-2025-rona</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.8</doi>
    </paper>
    <paper id="9">
      <title>Multi-Agent Based Character Simulation for Story Writing</title>
      <author><first>Tian</first><last>Yu</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Ken</first><last>Shi</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author id="zixin-zhao" orcid="0000-0002-8636-1987"><first>Zixin</first><last>Zhao</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author id="gerald-penn" orcid="0000-0003-3553-8305"><first>Gerald</first><last>Penn</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <pages>87-108</pages>
      <abstract>This work proposes a novel multi-agent story-generation system that writes stories from a narrative plan. Traditional approaches tend to generate a section of text directly from its outline. Our system, by contrast, divides this elaboration process into role-play and rewrite steps, where the former step enacts the story in chronological order with LLM-backed character agents, and the latter step refines the role-play result to align with a narrative plan. We show that the stories produced by our system are preferable to two other LLM-based story-generation approaches. We attribute this advancement to the benefits of incorporating a character-based simulation strategy.</abstract>
      <url hash="16f56c9f">2025.in2writing-1.9</url>
      <bibkey>yu-etal-2025-multi</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.9</doi>
    </paper>
    <paper id="10">
      <title>An Analysis of Scoring Methods for Reranking in Large Language Model Story Generation</title>
      <author><first>Megan</first><last>Deering</last><affiliation>University of Toronto</affiliation></author>
      <author id="gerald-penn" orcid="0000-0003-3553-8305"><first>Gerald</first><last>Penn</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <pages>109-120</pages>
      <abstract>Outline-conditioned story generation using Large Language Models (LLMs) offers a promising approach for automating narrative creation. Some outline-conditioned story generation methods use automatic scoring during the generation process in order to improve the story quality. However, current research has shown that automatic scoring is not ideal for assessing story quality. This paper evaluates three proposed automatic story-scoring methods to improve the reranking of outputs during the generation process. These scoring methods leverage different prompting strategies and fine-tuning techniques to enhance the accuracy and relevance of the assessments. By experimenting with these approaches within a beam search framework, we aim to identify the most effective methods for optimizing story-generation outcomes. While we have found no significant overall difference between these methods in terms of their agreement with human ratings during story generation, the overall story ratings by human evaluators are average. These findings motivate the need for improved automatic scoring techniques and datasets while also indicating that simpler, more easily implementable scoring methods for reranking perform comparably to more complex approaches.</abstract>
      <url hash="f8e23ef4">2025.in2writing-1.10</url>
      <bibkey>deering-penn-2025-analysis</bibkey>
      <doi>10.18653/v1/2025.in2writing-1.10</doi>
    </paper>
  </volume>
</collection>
