<?xml version='1.0' encoding='UTF-8'?>
<collection id="2026.iwsds">
  <volume id="1" type="proceedings" ingest-date="2026-02-18">
    <meta>
      <booktitle>Proceedings of the 16th International Workshop on Spoken Dialogue System Technology</booktitle>
      <editor><first>Giuseppe</first><last>Riccardi</last></editor>
      <editor><first/><last>Mousavi</last></editor>
      <editor><first>Maria</first><last>Torres</last></editor>
      <editor><first>Koichiro</first><last>Yoshino</last></editor>
      <editor><first>Zoraida</first><last>Callejas</last></editor>
      <editor><first>Shammur</first><last>Chowdhury</last></editor>
      <editor><first>Yun-Nung</first><last>Chen</last></editor>
      <editor><first>Frederic</first><last>Bechet</last></editor>
      <editor><first>Joakim</first><last>Gustafson</last></editor>
      <editor><first>Géraldine</first><last>Damnati</last></editor>
      <editor><first>Alex</first><last>Papangelis</last></editor>
      <editor><first>Luis</first><last>D'Haro</last></editor>
      <editor><first>John</first><last>Mendonça</last></editor>
      <editor><first>Raffaella</first><last>Bernardi</last></editor>
      <editor><first>Dilek</first><last>Hakkani-Tur</last></editor>
      <editor><first>Giuseppe</first><last>Di Fabbrizio</last></editor>
      <editor><first>Tatsuya</first><last>Kawahara</last></editor>
      <editor><first>Firoj</first><last>Alam</last></editor>
      <editor><first>Gokhan</first><last>Tur</last></editor>
      <editor><first>Michael</first><last>Johnston</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Trento, Italy</address>
      <month>February</month>
      <year>2026</year>
      <url hash="caaa5101">2026.iwsds-1</url>
      <venue>iwsds</venue>
    </meta>
    <frontmatter>
      <url hash="caaa5101">2026.iwsds-1</url>
      <bibkey>iwsds-2026-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>MAC</fixed-case>: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations</title>
      <author><first>Emre</first><last>Acikgoz</last></author>
      <author><first>Jinoh</first><last>Oh</last></author>
      <author><first>Joo</first><last>Jeon</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <author><first>Gokhan</first><last>Tur</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Chengyuan</first><last>Ma</last></author>
      <author><first>Xing</first><last>Fan</last></author>
      <pages>1–17</pages>
      <abstract>Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge—particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose <fixed-case>MAC</fixed-case> (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present <fixed-case>MAC</fixed-case> that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on <fixed-case>M</fixed-case>ulti<fixed-case>WOZ</fixed-case> 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8% (54.5 → 62.3) and reduces the average number of dialogue turns (6.53 → 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human–agent communication.</abstract>
      <url hash="5f621037">2026.iwsds-1.1</url>
      <bibkey>acikgoz-etal-2026-mac</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>F</fixed-case>low<fixed-case>S</fixed-case>witch: A State-Aware Framework for Workflow Transitions in Adaptive Dialogue Agents</title>
      <author><first>Wen</first><last>Chang</last></author>
      <author><first>Luning</first><last>Qiu</last></author>
      <author><first>Yi-Hung</first><last>Liu</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>18–33</pages>
      <abstract>To enhance large language models (<fixed-case>LLM</fixed-case>s) with real-world task-solving capabilities, integrating workflow knowledge into <fixed-case>LLM</fixed-case>s has emerged as a promising direction. However, real-world conversations are inherently dynamic—users often shift intents or request actions beyond the scope of the current workflow. Existing systems struggle to detect such transitions and to decide when to retrieve or switch to a new workflow. This paper presents <fixed-case>F</fixed-case>low<fixed-case>S</fixed-case>witch, a state-aware framework that learns when to search for relevant workflows and switch between them during multi-turn dialogues. A policy module determines whether to continue within the current workflow or transition to a new one based on contextual representations. When searching, a retriever identifies the most relevant workflow knowledge given the dialogue state. We conduct comprehensive experiments to explore the optimal configuration of <fixed-case>F</fixed-case>low<fixed-case>S</fixed-case>witch, including workflow format, retrieval input type, and retrieval method. Experimental results show that our framework, when using the agent’s self-generated search queries, achieves the highest Top-1 accuracy and Mean Average Precision (<fixed-case>MAP</fixed-case>). Moreover, <fixed-case>F</fixed-case>low<fixed-case>S</fixed-case>witch reduces nearly 50% of search operations, substantially lowering computational cost and response time.</abstract>
      <url hash="554b57d8">2026.iwsds-1.2</url>
      <bibkey>chang-etal-2026-flowswitch</bibkey>
    </paper>
    <paper id="3">
      <title>Personality Expression in Spoken Dialogue Systems: From Text to Speech</title>
      <author><first>Kenta</first><last>Yamamoto</last></author>
      <author><first>Kazunori</first><last>Komatani</last></author>
      <pages>34–42</pages>
      <abstract>A consistent personality in a spoken dialogue system enhances the naturalness and friendliness of interactions. However, users may not accurately perceive all the personality traits that the system attempts to express. This study aims to identify which traits are most reliably perceived by users. We first analyzed third-party personality ratings of a dialogue corpus using principal component and factor analyses to uncover the underlying dimensions of user perception. We then conducted experiments under both text-only and speech-based dialogue conditions to evaluate how effectively each trait could be perceived. Crowd-sourced ratings showed that a trait concerning Extraversion and Openness can be reliably perceived through text alone, whereas accurate perception of the other traits requires speech-related features such as speech rate, backchannels, fillers, and turn-taking pause duration. These findings suggest that, rather than attempting to express all Big Five traits, focusing on a subset aligned with users’ perceptual tendencies enables more effective and expressive personality design in spoken dialogue systems.</abstract>
      <url hash="9394d606">2026.iwsds-1.3</url>
      <bibkey>yamamoto-komatani-2026-personality</bibkey>
    </paper>
    <paper id="4">
      <title>Reproducing Proficiency-Conditioned Dialogue Features with Full-duplex Spoken Dialogue Models</title>
      <author><first>Takao</first><last>Obi</last></author>
      <author><first>Sadahiro</first><last>Yoshikawa</last></author>
      <author><first>Mao</first><last>Saeki</last></author>
      <author><first>Masaki</first><last>Eguchi</last></author>
      <author><first>Yoichi</first><last>Matsuyama</last></author>
      <pages>43–51</pages>
      <abstract>Real-time, human-centered conversational <fixed-case>AI</fixed-case> requires systems that handle spoken dialogue with overlap and rapid turn-taking. Although full-duplex models promise these capabilities, empirical work applying them to conversational <fixed-case>AI</fixed-case> is still nascent. To fill this gap, this study investigates whether the full-duplex model can reproduce the human dialogue features. We adapt a full-duplex spoken dialogue model to a large corpus of second-language (<fixed-case>L</fixed-case>2) learner interviews and train proficiency-conditioned models. We then conduct real-time interview sessions between these models and a spoken dialogue system designed to elicit spontaneous learner speech, and analyze reaction time, response frequency, and fluency metrics across aggregated <fixed-case>CEFR</fixed-case> levels (A/<fixed-case>B</fixed-case>/<fixed-case>C</fixed-case>). Our results show that proficiency-conditioned models partially reproduce levelwise trends and distributions observed in human interviews across multiple metrics. These findings suggest that full-duplex models can reproduce dialogue features of human dialogues and offer a promising foundation for conversational <fixed-case>AI</fixed-case> systems.</abstract>
      <url hash="d7860454">2026.iwsds-1.4</url>
      <bibkey>obi-etal-2026-reproducing</bibkey>
    </paper>
    <paper id="5">
      <title>Automatic Evaluation of Open-Domain Real Conversations: Combining Encoder-Based, Dialogue-Based Features and Large Language Models Ratings</title>
      <author><first>Cristina</first><last>Conforto López</last></author>
      <author><first>Marcos</first><last>Estecha-Goritagoitia</last></author>
      <author><first>Mario</first><last>Rodriguez-Cantelar</last></author>
      <author><first>Ricardo</first><last>Cordoba</last></author>
      <author><first>Luis</first><last>D'Haro</last></author>
      <pages>52–63</pages>
      <abstract>Conversational <fixed-case>AI</fixed-case> is a central application of <fixed-case>NLP</fixed-case>, yet ensuring high response quality remains challenging due to the inherently subjective nature of user satisfaction. Dialogue evaluation can be performed manually—through expert or user ratings—or automatically, using methods that aim to predict quality scores consistent with human judgment. In this work, we present a reference-free automatic dialogue evaluation system that predicts user ratings from a dataset of real human–chatbot interactions collected during the <fixed-case>A</fixed-case>lexa <fixed-case>P</fixed-case>rize Socialbot Grand Challenge 5, combining multiple complementary models to enhance correlation with human scores. Experimental results indicate that the model that achieves the highest <fixed-case>P</fixed-case>earson correlation with users’ ratings is an <fixed-case>XGB</fixed-case>oost regression model that combines different features such as conversation length, engineered flags capturing conversation characteristics, predictions from an Encoder-based Panel of Experts (<fixed-case>P</fixed-case>o<fixed-case>E</fixed-case>), and instruction-based outputs from a fine-tuned <fixed-case>LLM</fixed-case>. The overall <fixed-case>P</fixed-case>earson Correlation on the eval set is 0.404, which is competitive with prior work trained on an order of magnitude more dialogues, albeit using different datasets and system configurations.</abstract>
      <url hash="a61128f0">2026.iwsds-1.5</url>
      <bibkey>conforto-lopez-etal-2026-automatic</bibkey>
    </paper>
    <paper id="6">
      <title>Do audio and visual tokenizers capture backchannels?</title>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Auriane</first><last>Boudin</last></author>
      <pages>64–75</pages>
      <abstract>Audio and video tokenizers are autoencoders trained to represent the content of recordings as a sequence of vectors. They are prevalently used to interface large language models with non-textual modalities. While they allow advanced applications such as video generation, the envelope of their limitations is not known in the context of multimodal conversation. This work focuses on backchannels, which listeners use to signal to the speaker that they are listening. This feedback is essential to maintain the conversation flow. We evaluate whether a representative set of audio and video tokenizers encode backchannels using linear probing. Results show that although audio tokenizers capture the phenomenon relatively well, backchannels are not linearly separated by video tokenizers. However, joint representations resulting from concatenating representations in both modalities improve accuracy significantly over audio-only representations, suggesting to train multimodal tokenizers.</abstract>
      <url hash="ecc2d79c">2026.iwsds-1.6</url>
      <bibkey>favre-boudin-2026-audio</bibkey>
    </paper>
    <paper id="7">
      <title>The Context Trap: Why End-to-End Audio Language Models Fail Multi-turn Dialogues</title>
      <author><first>Zhi</first><last>Tam</last></author>
      <author><first>Wen</first><last>Chang</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>76–82</pages>
      <abstract>This study systematically compares end-to-end (<fixed-case>E</fixed-case>2<fixed-case>E</fixed-case>) audio language models (<fixed-case>A</fixed-case>udio<fixed-case>LM</fixed-case>s) against modular (<fixed-case>ASR</fixed-case>, <fixed-case>LLM</fixed-case>, <fixed-case>TTS</fixed-case>) systems for multi-phase task-oriented dialogues. We evaluate open-source models on key metrics: conversational naturalness and dialogue consistency. Our findings show that <fixed-case>E</fixed-case>2<fixed-case>E</fixed-case> configurations consistently underperform their modular counterparts, exhibiting severe degradation in dialogue quality across turns. Investigating this failure, our analysis reveals that the core issue lies in the <fixed-case>E</fixed-case>2<fixed-case>E</fixed-case> models’ dialogue modeling capabilities, specifically in context maintenance and topic tracking. This work highlights a critical gap between the purported low-latency benefit of <fixed-case>A</fixed-case>udio<fixed-case>LM</fixed-case>s and their practical ability to maintain coherence in complex, multi-turn dialogues, suggesting a need for focused architectural improvements.</abstract>
      <url hash="b90e28a0">2026.iwsds-1.7</url>
      <bibkey>tam-etal-2026-context</bibkey>
    </paper>
    <paper id="8">
      <title>Analysing Next Speaker Prediction in Multi-Party Conversation Using Multimodal Large Language Models</title>
      <author><first>Taiga</first><last>Mori</last></author>
      <author><first>Koji</first><last>Inoue</last></author>
      <author><first>Divesh</first><last>Lala</last></author>
      <author><first>Keiko</first><last>Ochi</last></author>
      <author><first>Tatsuya</first><last>Kawahara</last></author>
      <pages>83–94</pages>
      <abstract>This study analyses how state-of-the-art multimodal large language models (<fixed-case>MLLM</fixed-case>s) can predict the next speaker in multi-party conversations. Through experimental and qualitative analyses, we found that <fixed-case>MLLM</fixed-case>s are able to infer a plausible next speaker based solely on linguistic context and their internalized knowledge. However, even in cases where the next speaker is not uniquely determined, <fixed-case>MLLM</fixed-case>s exhibit a bias toward overpredicting a single participant as the next speaker. We further showed that this bias can be mitigated by explicitly providing knowledge of turn-taking rules. In addition, we observed that visual input can sometimes contribute to more accurate predictions, while in other cases it leads to erroneous judgments. Overall, however, no clear effect of visual input was observed.</abstract>
      <url hash="00461687">2026.iwsds-1.8</url>
      <bibkey>mori-etal-2026-analysing</bibkey>
    </paper>
    <paper id="9">
      <title>Exploring Emotional Nuances in Spoken Dialogue: Dataset Construction and Prediction of Emotional Dialogue Breakdown</title>
      <author><first>Hyuga</first><last>Nakaguro</last></author>
      <author><first>Koichiro</first><last>Yoshino</last></author>
      <pages>95–103</pages>
      <abstract>In spoken dialogue systems, even when the utterance text is the same, speaking style or tone differences can change its nuance. To respond appropriately in such cases, systems must accurately interpret paralinguistic information. Our study evaluates such a system’s ability using the "paraling-dial" dataset, which pairs a fixed utterance text with five distinct emotional expressions and their corresponding responses. We define a task using this dataset that detects mismatches—referred to as emotional dialogue breakdowns—between the expressed emotion of an utterance and the content of its response. We propose a breakdown detection system based on the Feature-wise Linear Modulation (<fixed-case>F</fixed-case>i<fixed-case>LM</fixed-case>) model, under the hypothesis that emotion dynamically controls text interpretation. Our experimental results show that the proposed model achieves 93.8% accuracy with gold emotion labels and 91.2% with predicted labels, demonstrating both its effectiveness and practicality. We also compare different types of control signals to identify the level of information required for such a breakdown detection task: emotion labels, emotion embeddings, and acoustic features. The results suggest that the appropriate level of abstraction, rather than simply richer information, is crucial for designing effective control signals.</abstract>
      <url hash="d91ee367">2026.iwsds-1.9</url>
      <bibkey>nakaguro-yoshino-2026-exploring</bibkey>
    </paper>
    <paper id="10">
      <title>Effects of Dialogue Corpora Properties on Fine-Tuning a Moshi-Based Spoken Dialogue Model</title>
      <author><first>Yuto</first><last>Abe</last></author>
      <author><first>Mao</first><last>Saeki</last></author>
      <author><first>Atsumoto</first><last>Ohashi</last></author>
      <author><first>Shinnosuke</first><last>Takamichi</last></author>
      <author><first>Shiyna</first><last>Fujie</last></author>
      <author><first>Tetsunori</first><last>Kobayashi</last></author>
      <author><first>Tetsuji</first><last>Ogawa</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>104–108</pages>
      <abstract>This study investigates how interactional characteristics of spoken dialogue corpora influence the learning process and resulting behavior of speech language models for full-duplex dialogue systems. While previous research has mainly focused on improving acoustic and linguistic quality, an effective dialogue system must also capture and reproduce task-dependent interactional dynamics such as conversational tempo and turn-taking patterns. To analyze these properties, we evaluated multiple dialogue corpora using <fixed-case>NISQA</fixed-case> for speech quality, <fixed-case>LLM</fixed-case>-as-a-Judge for linguistic and semantic appropriateness, and four timing-based indicators: inter-pausal units, pause, gap, and overlap. A curriculum learning strategy was applied to fine-tune a Moshi-based full-duplex dialogue model by incrementally combining corpora with different interactional characteristics. Experimental results on a dialogue continuation task showed that corpus-specific interactional patterns effectively shape model behavior. Chat-style corpora facilitated natural rhythms with moderate overlaps and gaps, whereas consultation-style corpora promoted more stable and deliberate timing. Fine-tuning with high-quality audio improved speech quality, while using task-mismatched data degraded linguistic coherence.</abstract>
      <url hash="f4d2b526">2026.iwsds-1.10</url>
      <bibkey>abe-etal-2026-effects</bibkey>
    </paper>
    <paper id="11">
      <title>Mixed-Initiative Dialogue Management for Human-Virtual Agents Interaction in Forum Theatre Inspired Training</title>
      <author><first>Samuel</first><last>Otofa</last></author>
      <author><first>Yacine</first><last>Zerenini</last></author>
      <author><first>Frederic</first><last>Bechet</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Jean-Marie</first><last>Pergandi</last></author>
      <author><first>Magalie</first><last>Ochs</last></author>
      <pages>109–113</pages>
      <abstract>This work presents a virtual reality (<fixed-case>VR</fixed-case>) training tool designed to raise awareness of social discrimination (ethnic and gender-based) and to train individuals to respond effectively when witnessing such situations. Inspired by Augusto Boal’s forum theatre, the system recreates interactive scenarios of discrimination using autonomous virtual agents. The user first observes a discriminatory scene, then analyzes it through an interaction with a virtual conversational agent, and finally replays the scene by embodying the discriminated character to explore alternative reactions. From a dialogue system perspective, the project introduces a hybrid dialogue management architecture combining state-based control with Large Language Model (<fixed-case>LLM</fixed-case>)-driven open dialogue. This mixed-initiative approach allows the system to manage structured training sequences while supporting flexible, context-aware interactions on sensitive topics. The demonstrator illustrates this approach through a case of ordinary sexism in a professional setting, highlighting the potential of spoken dialogue systems in <fixed-case>VR</fixed-case> for experiential learning and social behavior training.</abstract>
      <url hash="62a4fb03">2026.iwsds-1.11</url>
      <bibkey>otofa-etal-2026-mixed</bibkey>
    </paper>
    <paper id="12">
      <title>Analyzing Utterance Selection for Unnoticeable Topic Induction in Target-Guided Conversation Systems</title>
      <author><first>Kai</first><last>Yoshida</last></author>
      <author><first>Koichiro</first><last>Yoshino</last></author>
      <pages>114–122</pages>
      <abstract>Target-guided conversation systems conduct dialogues to achieve predefined conversation targets, such as recommending target goods or talking about target topics. In such systems, it is important to transition topics naturally toward the target without letting the user notice the intention behind the topic induction. In this study, we implement a surprisal-based framework that quantifies the sense of induction, target awareness, and naturalness of system utterances by computing surprisal using an external language model. Experimental results from dialogue sessions demonstrate that utterance selection based on the proposed surprisal-based evaluation reduces the perceived induction of system utterances. Furthermore, correlation analysis reveals that the proposed metric aligns with human perception of induction. We also observe that surprisal values with respect to the target gradually decrease as the conversation progresses, indicating that the model implicitly learns to approach the target more naturally over time.</abstract>
      <url hash="eae7fd3f">2026.iwsds-1.12</url>
      <bibkey>yoshida-yoshino-2026-analyzing</bibkey>
    </paper>
    <paper id="13">
      <title>Development of an Evaluation System for a Fan-Engagement Chat Application Using <fixed-case>LLM</fixed-case>-as-a-Judge</title>
      <author><first>Yuki</first><last>Fujita</last></author>
      <author><first>Yasunobu</first><last>Sasaki</last></author>
      <author><first>Ryota</first><last>Arashi</last></author>
      <author><first>Hokuto</first><last>Ototake</last></author>
      <author><first>Shinya</first><last>Takahashi</last></author>
      <pages>123–127</pages>
      <abstract>To address challenges in objectivity and efficiency in evaluating the quality of generative <fixed-case>AI</fixed-case> chatbots, we developed an automatic evaluation framework using the "<fixed-case>LLM</fixed-case>-as-a-judge" approach. A User Simulator, built with In-Context Learning and <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> tuning, was employed to generate pseudo-conversation logs of the fan-engagement application <fixed-case>OSHIAI</fixed-case>. These logs were then automatically evaluated by a Judge <fixed-case>LLM</fixed-case> across six dimensions, and the contribution of this method to quality management in real-world services was verified.</abstract>
      <url hash="30000d1d">2026.iwsds-1.13</url>
      <bibkey>fujita-etal-2026-development</bibkey>
    </paper>
    <paper id="14">
      <title>A Dialogue Agent to Let Users Experience and Gently Enhance the "Gyaru-Mind"</title>
      <author><first>Momoka</first><last>Ikegami</last></author>
      <author><first>Takuya</first><last>Kato</last></author>
      <author><first>Saizo</first><last>Aoyagi</last></author>
      <author><first>Tatsunori</first><last>Hirai</last></author>
      <pages>128–133</pages>
      <abstract>In <fixed-case>J</fixed-case>apan, the term "Gyaru-Mind" is commonly used to describe an upbeat mindset associated with gyaru culture, often linked to proactive positivity and strong self-affirmation. While it is widely regarded as beneficial, "Gyaru-Mind" lacks an academic operationalization and practical method for internalization. In this work, we define a quantitative index, "<fixed-case>GYARU</fixed-case>-<fixed-case>MIDX</fixed-case>", built from eight text-based factors, and implement a dialogue agent named <fixed-case>GYARU</fixed-case>-<fixed-case>AI</fixed-case> that uses this index in real time. During conversation, the agent estimates a user’s score and produces brief, context-appropriate replies by choosing between advice and empathy, so responses are not just positive all the time. A live "<fixed-case>GYARU</fixed-case>-<fixed-case>MIDX</fixed-case>" view provides real-time feedback for reflection and practice. The current system is <fixed-case>J</fixed-case>apanese-only because it is trained on <fixed-case>J</fixed-case>apanese "gyaru" style. We describe initial design and modeling results and outline limitations and next steps.</abstract>
      <url hash="acb6b13d">2026.iwsds-1.14</url>
      <bibkey>ikegami-etal-2026-dialogue</bibkey>
    </paper>
    <paper id="15">
      <title>Towards a proactive cooking companion for the elderly</title>
      <author><first>Katarina</first><last>Esteve</last></author>
      <author><first>Morgan</first><last>Fredriksson</last></author>
      <author><first>Joakim</first><last>Gustafson</last></author>
      <author><first>Dimosthenis</first><last>Kontogiorgos</last></author>
      <author><first>Timo</first><last>Mashiyi-Veikkola</last></author>
      <pages>134–141</pages>
      <abstract>Aging-in-place policies leave elderly populations vulnerable to declining nutrition and social isolation. This paper presents a voice-based cooking assistant designed as a companion, addressing both nutritional and social needs through intelligent kitchen interaction. Through <fixed-case>W</fixed-case>o<fixed-case>Z</fixed-case> experiments, we validated: social dialogue serves functional purposes, where "chatty" assistants transform cooking pauses into engaging interactions while instructional-only versions create frustrating dead air, despite identical timing.</abstract>
      <url hash="8addc2b0">2026.iwsds-1.15</url>
      <bibkey>esteve-etal-2026-towards</bibkey>
    </paper>
    <paper id="16">
      <title>Conversational <fixed-case>AI</fixed-case> for Virtual Standardized Patients using a Speech-to-Speech <fixed-case>LLM</fixed-case></title>
      <author><first>Andrew</first><last>Emerson</last></author>
      <author><first>Keelan</first><last>Evanini</last></author>
      <author><first>Su</first><last>Somay</last></author>
      <author><first>Kevin</first><last>Frome</last></author>
      <author><first>Le</first><last>Ha</last></author>
      <author><first>Polina</first><last>Harik</last></author>
      <pages>142–152</pages>
      <abstract>To develop clinical reasoning skills, medical students are often tasked with interacting with trained standardized patients (<fixed-case>SP</fixed-case>s). Human <fixed-case>SP</fixed-case>s enable real conversations that can resemble authentic clinical scenarios. However, human <fixed-case>SP</fixed-case>s require extensive training and are often limited in their accessibility and continual availability to medical students or residents. Virtual <fixed-case>SP</fixed-case>s offer the ability for medical students to practice clinical interviews in a lower-stakes setting across a broader set of clinical cases. This paper introduces a virtual <fixed-case>SP</fixed-case> (<fixed-case>VSP</fixed-case>) that leverages <fixed-case>A</fixed-case>mazon’s Nova Sonic, a speech-to-speech foundation model designed for human-like conversation. We investigated the ability of Nova Sonic to portray four distinct clinical cases in virtual doctor-patient encounters with 20 third-year medical students. The system’s realism, its perceived learning value, and user experience were all assessed via a survey administered to the students. Students were also asked to compare this experience to interactions with a human <fixed-case>SP</fixed-case>. Survey results and conversations were analyzed to derive insights for improving the Nova Sonic-based <fixed-case>VSP</fixed-case> system.</abstract>
      <url hash="ab8a6550">2026.iwsds-1.16</url>
      <bibkey>emerson-etal-2026-conversational</bibkey>
    </paper>
    <paper id="17">
      <title>Can Small-Scale <fixed-case>LLM</fixed-case>s Balance Content Accuracy and Speaker Faithfulness in Noisy <fixed-case>F</fixed-case>rench Dialogue Summarization?</title>
      <author><first>Rim</first><last>Abrougui</last></author>
      <author><first>Guillaume</first><last>Lechien</last></author>
      <author><first>Elisabeth</first><last>Savatier</last></author>
      <author><first>Benoît</first><last>Laurent</last></author>
      <pages>153–157</pages>
      <abstract>Summarizing domain-specific and multi-speaker conversations, such as political debates, remains challenging under noisy <fixed-case>ASR</fixed-case> conditions. In industrial contexts, large language models (<fixed-case>LLM</fixed-case>s) are often impractical due to resource and confidentiality constraints. This work evaluates whether smaller <fixed-case>LLM</fixed-case>s (up to 8<fixed-case>B</fixed-case> parameters) can produce reliable summaries in such settings. Experiments on <fixed-case>F</fixed-case>rench debates show that noise significantly degrades accuracy and readability, while fine-tuning on clean, domain-related data improves robustness and reduces hallucinations. We also analyze person-name mentions as indicators of speaker faithfulness, finding that fine-tuning can help identify all speakers in far more debates than chain-of-thought prompting. However, evaluations on limited industrial data show that fine-tuning still struggles to generalize to unseen speakers and topics.</abstract>
      <url hash="a6a0bcac">2026.iwsds-1.17</url>
      <bibkey>abrougui-etal-2026-small</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>ORCHESTRA</fixed-case>: <fixed-case>AI</fixed-case>-Driven Microservices Architecture to Create Personalized Experiences</title>
      <author><first>Jaime</first><last>Bellver</last></author>
      <author><first>Samuel</first><last>Ramos-Varela</last></author>
      <author><first>Anmol</first><last>Guragain</last></author>
      <author><first>Ricardo</first><last>Córdoba</last></author>
      <author><first>Luis</first><last>D'Haro</last></author>
      <pages>158–167</pages>
      <abstract>Industry stakeholders are willing to incorporate <fixed-case>AI</fixed-case> systems in their pipelines, therefore they want agentic flexibility without losing the guaranties and auditability of fixed pipelines. This paper describes <fixed-case>ORCHESTRA</fixed-case>, a portable and extensible microservice architecture for orchestrating customizable multimodal <fixed-case>AI</fixed-case> workflows across domains. It embeds Large Language Model (<fixed-case>LLM</fixed-case>) agents within a deterministic control flow, combining reliability with adaptive reasoning. A Dockerized Manager routes text, speech, and image requests through specialist workers for <fixed-case>ASR</fixed-case>, emotion analysis, retrieval, guardrails, and <fixed-case>TTS</fixed-case>, ensuring that multimodal processing, safety checks, logging, and memory updates are consistently executed, while scoped agent nodes adjust prompts and retrieval strategies dynamically. The system scales via container replication and exposes per-step observability through open-source dashboards. We ground the discussion in a concrete deployment: an interactive museum guide that handles speech and image queries, personalizes narratives with emotion cues, invokes tools, and enforces policy-compliant responses. From this application, we report actionable guidance: interface contracts for services, where to place pre/post safety passes, how to structure memory for <fixed-case>RAG</fixed-case>, and common failure modes with mitigations. We position the approach against fully agentic and pure pipeline baselines, outline trade-offs (determinism vs. flexibility, latency budget), and sketch near-term extensions such as sharded managers, adaptive sub-flows, and streaming inference. Our goal is to provide a reusable blueprint for safely deploying agent-enhanced, multimodal assistants in production, illustrated through the museums use case.</abstract>
      <url hash="96b4d24e">2026.iwsds-1.18</url>
      <bibkey>bellver-etal-2026-orchestra</bibkey>
    </paper>
    <paper id="19">
      <title>Benchmarking Multilingual Temporal Reasoning in <fixed-case>LLM</fixed-case>s: The Temporal Reasoning Dataset</title>
      <author><first>Vittorio</first><last>Mazzia</last></author>
      <author><first>Sandro</first><last>Pollastrini</last></author>
      <author><first>Davide</first><last>Bernardi</last></author>
      <author><first>Chiara</first><last>Rubagotti</last></author>
      <author><first>Daniele</first><last>Amberti</last></author>
      <pages>168–181</pages>
      <abstract>Time reasoning is a make-or-break capability for Large Language Models (<fixed-case>LLM</fixed-case>s) aspiring to act as reliable personal and enterprise assistants. This work introduces the Temporal Reasoning Dataset (<fixed-case>TRD</fixed-case>), a programmatically generated multilingual benchmark designed to evaluate temporal reasoning operational capabilities in <fixed-case>LLM</fixed-case>s across ten languages, with particular focus on basic operations relevant to conversational agents handling time-sensitive tasks. <fixed-case>TRD</fixed-case> utilizes human-curated carrier phrases to generate a resilient-to-overfitting dataset with diverse samples and controlled difficulty levels across five core task categories, each at five difficulty levels. Extensive experimentation shows consistent patterns in model performance across languages, with a strong linear decline in accuracy as task difficulty rises in reasoning-based tasks, while memorization-based tasks remain stable. Furthermore, reasoning tasks remain robust across temporal shifts, whereas memorization tasks show performance degradation. Additionally, contextual modifications to prompts influence model performance differently than human cognitive patterns.</abstract>
      <url hash="f1d2e102">2026.iwsds-1.19</url>
      <bibkey>mazzia-etal-2026-benchmarking</bibkey>
    </paper>
    <paper id="20">
      <title>Retrospective Speech Recognition for Spoken Dialogue System: Exploiting Subsequent Utterances to Enhance <fixed-case>ASR</fixed-case> Performance</title>
      <author><first>Ryu</first><last>Takeda</last></author>
      <author><first>Kazunori</first><last>Komatani</last></author>
      <pages>182–192</pages>
      <abstract>Spoken dialogue systems would benefit from the ability of self-correction, namely, –revising earlier recognition results once later utterances are available, as humans often do in dialogue. However, conventional automatic speech recognition (<fixed-case>ASR</fixed-case>) frameworks mainly process user utterances sequentially and rely only on the preceding context. To address this limitation, we propose Retrospective Speech Recognition (<fixed-case>RSR</fixed-case>), which refines past recognition results by exploiting its subsequent utterances. We formulate and implement an <fixed-case>RSR</fixed-case> model for a dialogue system situation where system utterances can also be utilized. Each past user utterance is processed with an interpretable syllabogram representation, which integrates preceding and subsequent utterances within a shared domain between the signal and text levels. This intermediate representation also helps reduce orthographic inconsistencies. Experimental results using real <fixed-case>J</fixed-case>apanese dialogue speech showed that utilizing the subsequent utterances improved the character error rate by 0.10 points, which demonstrates the utility of <fixed-case>RSR</fixed-case>. We also investigated the impact of other factors, such as utilization of system utterances.</abstract>
      <url hash="3037705e">2026.iwsds-1.20</url>
      <bibkey>takeda-komatani-2026-retrospective</bibkey>
    </paper>
    <paper id="21">
      <title>From Fact to Judgment: Investigating the Impact of Task Framing on <fixed-case>LLM</fixed-case> Conviction in Dialogue Systems</title>
      <author><first>Parisa</first><last>Rabbani</last></author>
      <author><first>Nimet</first><last>Bozdag</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>193–204</pages>
      <abstract><fixed-case>LLM</fixed-case>s are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such <fixed-case>LLM</fixed-case>-judges can reliably assess tasks that require social or conversational judgment. We investigate how an <fixed-case>LLM</fixed-case>’s conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model’s performance on direct factual queries with its assessment of a speaker’s correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?” to "Is this speaker correct?”. Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.”) to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like <fixed-case>GPT</fixed-case>-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8<fixed-case>B</fixed-case>-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in <fixed-case>LLM</fixed-case>-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.</abstract>
      <url hash="0b2ab58e">2026.iwsds-1.21</url>
      <bibkey>rabbani-etal-2026-fact</bibkey>
    </paper>
    <paper id="22">
      <title>Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction</title>
      <author><first>Galann</first><last>Pennec</last></author>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Nicholas</first><last>Asher</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>205–221</pages>
      <abstract>Vision-Language Models (<fixed-case>VLM</fixed-case>s) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by <fixed-case>VLM</fixed-case>s. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (<fixed-case>LLM</fixed-case>), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the <fixed-case>M</fixed-case>ovie<fixed-case>S</fixed-case>um dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in <fixed-case>M</fixed-case>ovie<fixed-case>S</fixed-case>um. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.</abstract>
      <url hash="4e547b9d">2026.iwsds-1.22</url>
      <bibkey>pennec-etal-2026-minimal</bibkey>
    </paper>
    <paper id="23">
      <title>Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study</title>
      <author><first>Koji</first><last>Inoue</last></author>
      <author><first>Mikey</first><last>Elmers</last></author>
      <author><first>Yahui</first><last>Fu</last></author>
      <author><first>Zi</first><last>Pang</last></author>
      <author><first>Taiga</first><last>Mori</last></author>
      <author><first>Divesh</first><last>Lala</last></author>
      <author><first>Keiko</first><last>Ochi</last></author>
      <author><first>Tatsuya</first><last>Kawahara</last></author>
      <pages>222–230</pages>
      <abstract>We present a multilingual, continuous backchannel prediction model for <fixed-case>J</fixed-case>apanese, <fixed-case>E</fixed-case>nglish, and <fixed-case>C</fixed-case>hinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: <fixed-case>J</fixed-case>apanese relies more on short-term linguistic information, whereas <fixed-case>E</fixed-case>nglish and <fixed-case>C</fixed-case>hinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in <fixed-case>C</fixed-case>hinese. A context-length study further shows that <fixed-case>J</fixed-case>apanese is relatively robust to shorter contexts, while <fixed-case>C</fixed-case>hinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating <fixed-case>CPU</fixed-case>-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.</abstract>
      <url hash="c792b266">2026.iwsds-1.23</url>
      <bibkey>inoue-etal-2026-multilingual</bibkey>
    </paper>
    <paper id="24">
      <title>Vanishing point of attention: A platform for adaptive driver dialogue experiments</title>
      <author><first>Morgan</first><last>Fredriksson</last></author>
      <author><first>Yanis</first><last>Yaici</last></author>
      <author><first>Kevin</first><last>Lam</last></author>
      <author><first>Jurgen</first><last>Konigsmann</last></author>
      <author><first>Jens</first><last>Edlund</last></author>
      <pages>231–238</pages>
      <abstract>Current in-vehicle conversational agents lack awareness of the driving situation, treating all dialogue alike regardless of cognitive demands. This paper presents a modular experimental platform that integrates the <fixed-case>CARLA</fixed-case> driving simulator with a real-time spatial-reasoning engine to support research on situation-aware dialogue. The system enables <fixed-case>W</fixed-case>izard-of-<fixed-case>O</fixed-case>z studies in which human operators control conversational agents informed by live spatial-semantic analysis of the traffic environment. As initial validation, a controlled study (n = 10) tested the platform’s sensitivity to conversational load effects, examining whether increasing conversational complexity produces a vanishing point of attention, a threshold where combined conversational and driving demands lead to a non-linear collapse in performance. Results revealed a sharp rise in collisions and missed hazard detections under high cognitive load, confirming the platform’s sensitivity to conversational strain. The platform provides a reproducible testbed for investigating how dialogue timing, content, and environmental demands interact, offering a foundation for designing adaptive, cognitively safe in-vehicle conversational systems.</abstract>
      <url hash="32d4e30b">2026.iwsds-1.24</url>
      <bibkey>fredriksson-etal-2026-vanishing</bibkey>
    </paper>
    <paper id="25">
      <title>When social robots see our sketches: evaluating human perception of a robot and a <fixed-case>VLM</fixed-case> model performance in a drawing task</title>
      <author><first>Viktoria</first><last>Daniilidou</last></author>
      <author><first>Nikolai</first><last>Ilinykh</last></author>
      <author><first>Vladislav</first><last>Maraev</last></author>
      <pages>239–252</pages>
      <abstract>We introduce a multimodal framework for interactive drawing in a robot-assisted second language learning scenario. In this scenario, humans are asked to draw objects and spatial relations between them, while a social robot that uses a vision-language model (<fixed-case>VLM</fixed-case>) to analyse whether the drawings are correct.<fixed-case>T</fixed-case>he correctness decision that is passed to the human is coming from a <fixed-case>W</fixed-case>izard-of-<fixed-case>O</fixed-case>z (<fixed-case>W</fixed-case>o<fixed-case>Z</fixed-case>) setup. Therefore, we use it to indirectly evaluate the quality of <fixed-case>VLM</fixed-case> predictions. We show that the task is very challenging for a <fixed-case>VLM</fixed-case> and approaching evaluation of <fixed-case>VLM</fixed-case> performance is important: focusing on the correctness of prediction of certain features (objects, relations) provides a different evaluation picture from when the model is evaluated on prediction of the content of the image as a whole. We also examine how the appearance of the social agent and the type of feedback influence perception of the agent by the participants through a questionnaire. The comparison of verbal feedback, generated by the large language models, against simple pattern-based feedback did not show any significant effects whereas the robot’s appearance change indicated significant difference in user ratings concerning naturalness of the agent and its social presence.</abstract>
      <url hash="99d95316">2026.iwsds-1.25</url>
      <bibkey>daniilidou-etal-2026-social</bibkey>
    </paper>
    <paper id="26">
      <title>Adding Determinism to a Dialogue Agent for a Robotic Environment</title>
      <author><first>Oihana</first><last>Garcia Anakabe</last></author>
      <author><first>Riccardo</first><last>Cocola</last></author>
      <author><first>Cristina</first><last>Aceta</last></author>
      <pages>253–261</pages>
      <abstract>Large Language Models (<fixed-case>LLM</fixed-case>s) have strong capabilities in natural dialogue, but their inherent indeterminacy presents challenges in robotic environments where safety and reliability are critical. In this study, we propose a dialogue agent that has been developed to guide and support human operators during robot demonstrations, following the Learning from Demonstration (<fixed-case>L</fixed-case>f<fixed-case>D</fixed-case>) paradigm, where the robot learns tasks from the operator’s actions. The agent presented in this work extends the standard prompt-based <fixed-case>LLM</fixed-case> setup by integrating state graphs that explicitly encode dialogue states and transitions. This structure ensures that user interactions follow the intended path, while still allowing users to communicate in a flexible and natural manner. The state graph agent is benchmarked against a monolithic prompt baseline in challenging dialogue scenarios involving ambiguity, incomplete actions, or operator errors. Despite the <fixed-case>LLM</fixed-case> prompt achieving good standalone performance, the state-controlled agent shows greater contextual understanding, reasoning capability, and advisory performance, leading to more intelligent and reliable interactions.</abstract>
      <url hash="f56174fd">2026.iwsds-1.26</url>
      <bibkey>garcia-anakabe-etal-2026-adding</bibkey>
    </paper>
    <paper id="27">
      <title>Context-Aware Language Understanding in Human-Robot Dialogue with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Svetlana</first><last>Stoyanchev</last></author>
      <author><first>Youmna</first><last>Farag</last></author>
      <author><first>Simon</first><last>Keizer</last></author>
      <author><first>Mohan</first><last>Li</last></author>
      <author><first>Rama</first><last>Doddipatla</last></author>
      <pages>262–274</pages>
      <abstract>In this work, we explore the use of large language models (<fixed-case>LLM</fixed-case>s) as interpreters of user utterances within a human-robot language interface. A user interacting with a robot that operates in a physical environment should be able to issue commands that interrupt the robot’s actions, for example, corrections or refinement of the task. This study addresses the context-aware interpretation of user utterances, including those issued while the robot is actively engaged in task execution, exploring whether <fixed-case>LLM</fixed-case>s, without fine-tuning, can translate user commands into corresponding sequences of robot actions. Using an interactive multimodal interface—combining text and video—for a virtual robot operating in simulated home environments, we collect a dataset of user utterances that guide the robot through various household tasks simultaneously capturing manual interpretation when the automatic one fails. Driven by practical considerations, the collected dataset is used to compare the interpretive performance of <fixed-case>GPT</fixed-case> models with smaller publicly available alternatives. Our findings reveal that action-interrupting utterances pose challenges for all models. While <fixed-case>GPT</fixed-case> consistently outperforms the smaller models, interpretation accuracy improves across the board when relevant dynamically selected in-context learning examples are included in the prompt.</abstract>
      <url hash="b14d9a86">2026.iwsds-1.27</url>
      <bibkey>stoyanchev-etal-2026-context</bibkey>
    </paper>
    <paper id="28">
      <title>Learning Vision-Language Alignment in Unified <fixed-case>LLM</fixed-case>s with 24 Text Tokens per Image</title>
      <author><first>Nicola</first><last>Irmiger</last></author>
      <author><first>Yixuan</first><last>Xu</last></author>
      <author><first>Raphael</first><last>Kreft</last></author>
      <author><first>Aram</first><last>Davtyan</last></author>
      <author><first>Manuel</first><last>Kaufmann</last></author>
      <author><first>Imanol</first><last>Schlag</last></author>
      <pages>275–287</pages>
      <abstract>We explore how to adapt a pre-trained large language model to understand and generate both visual and textual information. We use an image tokenizer to compress images into discrete tokens, and train the model using the next-token prediction paradigm with the standard cross-entropy loss. A two-stage pre-training approach is applied, first training on image-only data and then on a small amount of image-text data. We evaluate how different image-text token mixing ratios during continual pre-training affect the model’s ability to retain language skills while learning visual representations. The resulting model shows promising signs of flexible multimodal understanding, bridging vision and language in a single pre-trained model.</abstract>
      <url hash="33889365">2026.iwsds-1.28</url>
      <bibkey>irmiger-etal-2026-learning</bibkey>
    </paper>
    <paper id="29">
      <title>Incorporating Respect into <fixed-case>LLM</fixed-case>-Based Academic Feedback: A <fixed-case>BI</fixed-case>-<fixed-case>R</fixed-case> Framework for Instructing Students after <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case> Sessions</title>
      <author><first>Mayuko</first><last>Aiba</last></author>
      <author><first>Daisuke</first><last>Saito</last></author>
      <author><first>Nobuaki</first><last>Minematsu</last></author>
      <pages>288–301</pages>
      <abstract>In academic research, post-presentation <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case> sessions are crucial for deepening understanding and shaping research directions. Supervisors’ comments are particularly valuable when they highlight perspectives that students have not yet fully considered. Such comments typically arise from careful reasoning within dialogue, yet large language models (<fixed-case>LLM</fixed-case>s) still struggle to reason precisely about dialogue context and communicative intentions. Building on <fixed-case>LLM</fixed-case>s, this study proposes a feedback generation framework based on the Belief–Desire–Intention (<fixed-case>BDI</fixed-case>) model, which conceptualizes <fixed-case>Q</fixed-case>&amp;<fixed-case>A</fixed-case> sessions as cognitive interactions between presenters and questioners. We further extend this framework into <fixed-case>BI</fixed-case>-<fixed-case>R</fixed-case> by introducing Respect as an explicit dimension, ensuring that generated feedback is not only accurate but also pedagogically constructive. We evaluated the proposed framework (<fixed-case>BDI</fixed-case> and <fixed-case>BI</fixed-case>-<fixed-case>R</fixed-case>) through comparative experiments with master’s students and field experiments with doctoral students during pre-defense presentations. Results showed that while the <fixed-case>BDI</fixed-case> prompt did not outperform the baseline, the <fixed-case>BI</fixed-case>-<fixed-case>R</fixed-case> prompt was particularly effective when students did not fully grasp the broader context or background of the questions. When comparing <fixed-case>BDI</fixed-case> and <fixed-case>BI</fixed-case>-<fixed-case>R</fixed-case>, the inclusion of Respect improved the tone and pedagogical appropriateness of feedback. These findings highlight the potential of the proposed framework as a supportive tool for training students and early-career researchers.</abstract>
      <url hash="0eb6e550">2026.iwsds-1.29</url>
      <bibkey>aiba-etal-2026-incorporating</bibkey>
    </paper>
    <paper id="30">
      <title>The Complementary Role of Para-linguistic cues for Robust Pronunciation Assessment</title>
      <author><first>Yassine</first><last>El Kheir</last></author>
      <author><first>Shammur</first><last>Chowdhury</last></author>
      <author><first>Ahmed</first><last>Ali</last></author>
      <pages>302–306</pages>
      <abstract>Research on pronunciation assessment systems focuses on utilizing phonetic and phonological aspects of non-native (<fixed-case>L</fixed-case>2) speech, often neglecting the rich layer of information hidden within the para-linguistic cues. In this study, we proposed a novel pronunciation assessment framework, <b><fixed-case>I</fixed-case>ntra<fixed-case>V</fixed-case>erbal<fixed-case>PA</fixed-case></b>.[The source code will be available to the public upon acceptance.] The framework innovatively incorporates both fine-grained frame- and abstract utterance-level para-linguistic cues, alongside the raw speech and phoneme representations. Additionally, we introduce the “Goodness of phonemic-duration” metric to model phoneme duration distribution within the framework effectively. Our results validate the effectiveness of the proposed <fixed-case>I</fixed-case>ntra<fixed-case>V</fixed-case>erbal<fixed-case>PA</fixed-case> framework and its individual components, yielding performance that matches or outperforms existing research works.</abstract>
      <url hash="efa113b0">2026.iwsds-1.30</url>
      <bibkey>el-kheir-etal-2026-complementary</bibkey>
    </paper>
    <paper id="31">
      <title>Evaluating <fixed-case>LLM</fixed-case> Style Transfer Through Readability-Based Age Assessments</title>
      <author><first>Maria</first><last>Di Maro</last></author>
      <author><first>Antonio</first><last>Origlia</last></author>
      <author><first>Leonilda</first><last>Bilo</last></author>
      <author><first>Roberta</first><last>Meo</last></author>
      <author><first>Pietro</first><last>Maturi</last></author>
      <author><first>Francesca</first><last>Nappo</last></author>
      <pages>307–311</pages>
      <abstract>Adaptability to the audience is an important feature for conversational systems, especially in the healthcare dissemination field, where scientific concepts have to be delivered to a potentially wide range of users. This work presents an evaluation of the capability of <fixed-case>LLM</fixed-case>s to support style transfer according to the target user’s age group. Two complementary evaluation methods were employed: an automatic assessment based on the <fixed-case>ARI</fixed-case> readability index, and a human experts evaluation focusing on appropriateness depending on the user’s educational level as well as content accuracy. Results show that <fixed-case>LLM</fixed-case>s efficiently switch style when provided with information about the user’s age while managing content still requires the adoption of safety measures.</abstract>
      <url hash="be2d23d0">2026.iwsds-1.31</url>
      <bibkey>di-maro-etal-2026-evaluating</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>S</fixed-case>peak<fixed-case>RL</fixed-case>: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning</title>
      <author><first>Emre</first><last>Acikgoz</last></author>
      <author><first>Jinoh</first><last>Oh</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <author><first>Joo</first><last>Jeon</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <author><first>Gokhan</first><last>Tur</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Chengyuan</first><last>Ma</last></author>
      <author><first>Xing</first><last>Fan</last></author>
      <pages>312–325</pages>
      <abstract>Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (<fixed-case>LM</fixed-case>s), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce <fixed-case>S</fixed-case>peak<fixed-case>RL</fixed-case>, a reinforcement learning (<fixed-case>RL</fixed-case>) method that enhances agents’ conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate <fixed-case>S</fixed-case>peak<fixed-case>ER</fixed-case>, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.</abstract>
      <url hash="d97c1697">2026.iwsds-1.32</url>
      <bibkey>acikgoz-etal-2026-speakrl</bibkey>
    </paper>
    <paper id="33">
      <title>Adaptive Multimodal Sentiment Analysis with Stream-Based Active Learning for Spoken Dialogue Systems</title>
      <author><first>Atsuto</first><last>Ajichi</last></author>
      <author><first>Takato</first><last>Hayashi</last></author>
      <author><first>Kazunori</first><last>Komatani</last></author>
      <author><first>Shogo</first><last>Okada</last></author>
      <pages>326–337</pages>
      <abstract>In empathic dialogue systems, it is crucial to continuously monitor and adapt to the user’s emotional state. To capture user-specific mappings between multimodal behaviors and emotional states, directly asking users about their emotions during dialogue is the most straightforward and effective approach. However, frequent questioning can cause inconvenience to users and diminish the user experience, so the number of queries should be minimized. In this study, we formulate personalized multimodal sentiment analysis (<fixed-case>MSA</fixed-case>) as a stream-based active learning problem, where user behaviors are observed sequentially, and we assume that the system has an ability to decide at each step whether to request an emotion label from the user. Simulation experiments using a human–agent dialogue corpus demonstrate that the proposed method efficiently improves performance even under few-shot conditions. These results indicate that our approach is effective for developing dialogue systems that achieve cost-efficient personalized <fixed-case>MSA</fixed-case>.</abstract>
      <url hash="999233d6">2026.iwsds-1.33</url>
      <bibkey>ajichi-etal-2026-adaptive</bibkey>
    </paper>
    <paper id="34">
      <title>Predicting Turn-Taking in Child–Adult Conversations Using Voice Activity Projection</title>
      <author><first>Youcef</first><last>Brahimi</last></author>
      <author><first>César</first><last>Blanc</last></author>
      <author><first>Abdellah</first><last>Fourtassi</last></author>
      <pages>338–347</pages>
      <abstract>Turn-taking is a hallmark of human conversation, yet its developmental trajectory remains poorly understood. Adults typically respond within a few hundred milliseconds, suggesting reliance on predictive cues rather than simply waiting for silence. In contrast, children’s longer gaps raise the question of whether they depend on simpler, reactive strategies. This study provides the first large-scale test of competing hypotheses about children’s turn-taking, using corpora of child–adult and adult–adult dialogues. In Study 1, we compared a simple silence-based threshold model with the Voice Activity Projection (<fixed-case>VAP</fixed-case>) model, which predicts upcoming speech activity from acoustic features. Results showed that silence alone could not account for children’s behavior, whereas predictive acoustic models performed well, indicating that even early turn-taking relies on anticipatory mechanisms. In Study 2, we asked what cues support these predictions by comparing models based on acoustic features alone with models combining acoustic and lexical information. For adult conversations, lexical cues improved prediction, but for child–adult dialogues, acoustic information was sufficient to solve the task. Together, these findings suggest that children’s turn-taking is predictive but primarily grounded in acoustic patterns, revealing both continuity with adult mechanisms and developmental differences in how linguistic cues are integrated.</abstract>
      <url hash="5fed72f2">2026.iwsds-1.34</url>
      <bibkey>brahimi-etal-2026-predicting</bibkey>
    </paper>
    <paper id="35">
      <title>Supporting human operators during customer service interactions with agentic-<fixed-case>RAG</fixed-case></title>
      <author><first>Juan</first><last>Barrionuevo-Valenzuela</last></author>
      <author><first>Daniel</first><last>Calderón-González</last></author>
      <author><first>Zoraida</first><last>Callejas</last></author>
      <author><first>David</first><last>Griol</last></author>
      <pages>348–356</pages>
      <abstract>This paper focuses on improving customer service in call centers, where finding accurate answers in the shortest possible time is crucial. The proposed solution is the development of a conversational <fixed-case>AI</fixed-case> system that acts as a "copilot" for human operators. The main goal of this copilot is to assist the operator in real time by providing conversation summaries, relevant domain information, and suggested responses that help guide the interaction toward a successful resolution. To achieve this, different approaches to Retrieval Augmented Generation (<fixed-case>RAG</fixed-case>) have been explored. The proposed agentic-<fixed-case>RAG</fixed-case> architecture integrates multiple autonomous agents for routing, retrieval validation, and response generation, achieving consistent improvements in real-time performance, grounding, and overall user experience across diverse service scenarios. Empirical results with the Action-Based Conversations Dataset (<fixed-case>ABCD</fixed-case>) corpus show that the use of agents proved to be effective in handling unstructured conversational data. The proposed approach showed an improvement in the quality, relevance, and accuracy of the generated responses with respect to a naïve <fixed-case>RAG</fixed-case> baseline. It is important to emphasize that this system is not intended to replace the operator, but rather to act as a support tool to enhance efficiency and customer satisfaction.</abstract>
      <url hash="b752a54f">2026.iwsds-1.35</url>
      <bibkey>barrionuevo-valenzuela-etal-2026-supporting</bibkey>
    </paper>
    <paper id="36">
      <title>Analysis of Child-Caregiver Interactions for Developing a Caregiver Spoken Dialogue System</title>
      <author><first>Sanae</first><last>Yamashita</last></author>
      <author><first>Shota</first><last>Mochizuki</last></author>
      <author><first>Yuko</first><last>Kuma</last></author>
      <author><first>Ray</first><last>Sakai</last></author>
      <author><first>Ayaka</first><last>Sasaki</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>357–368</pages>
      <abstract>We aim to develop a caregiver spoken dialogue system for remote childcare services. As a first step toward this goal, this study investigates how interactions occur between children and caregivers. We collected <fixed-case>J</fixed-case>apanese child–caregiver dialogue data through a remote childcare service in which participants engaged in activities such as introductions, quizzes, and free conversations. The collected data were analyzed and compared with existing child–caregiver dialogue data from both acoustic and linguistic perspectives. The results showed that, acoustically, child–caregiver dialogues contained fewer overlapping utterances than adult dialogues. Linguistically, the distribution and transitions of utterance intentions differed across dialogue parts, reflecting the diverse structures of each activity. These findings provide useful insights for building future caregiver spoken dialogue systems, suggesting that a turn-based interaction structure may be sufficient and that dialogue control should be adapted to each part of the dialogue.</abstract>
      <url hash="6ed81506">2026.iwsds-1.36</url>
      <bibkey>yamashita-etal-2026-analysis</bibkey>
    </paper>
    <paper id="37">
      <title>Can code-switching improve the user experience with a dialogue system app for recording endangered languages?</title>
      <author><first>Jacqueline</first><last>Brixey</last></author>
      <author><first>David</first><last>Traum</last></author>
      <pages>369–378</pages>
      <abstract>This paper investigates whether a multilingual spoken dialogue system can be used to help collect and preserve endangered language data. In this work, we extend <fixed-case>DAPEL</fixed-case> (Dialogue <fixed-case>AP</fixed-case>p for Endangered Languages), which is designed to help preserve any language. Our focus, for testing purposes, is on the <fixed-case>A</fixed-case>merican Indigenous language <fixed-case>C</fixed-case>hoctaw. The system uses <fixed-case>E</fixed-case>nglish as a common language, and we test whether incorporating code-switching—the act of alternating between languages—enhances the user experience and/or increases the amount of recorded language data. Our results indicate that users have a positive response to interacting in both languages with the system, that the system plays a meaningful role in language documentation, and, notably, that participants who speak <fixed-case>C</fixed-case>hoctaw as their first language are more receptive to a code-switching system than to a monolingual <fixed-case>E</fixed-case>nglish-based system.</abstract>
      <url hash="54aa3db8">2026.iwsds-1.37</url>
      <bibkey>brixey-traum-2026-code</bibkey>
    </paper>
    <paper id="38">
      <title>Estimating Relationships between Participants in Multi-Party Chat Corpus</title>
      <author><first>Akane</first><last>Fukushige</last></author>
      <author><first>Koji</first><last>Inoue</last></author>
      <author><first>Keiko</first><last>Ochi</last></author>
      <author><first>Tatsuya</first><last>Kawahara</last></author>
      <author><first>Sanae</first><last>Yamashita</last></author>
      <author><first>Ryuichiro</first><last>Higashinaka</last></author>
      <pages>379–390</pages>
      <abstract>While most existing dialogue studies focus on dyadic (one-on-one) interactions, research on multi-party dialogues has gained increasing importance. One key challenge in multi-party dialogues is identifying and interpreting the relationships between participants. This study focuses on multi-party chat corpus and aims to estimate participant pairs with specific relationships, such as family and acquaintances. We evaluated the performance of large language models (<fixed-case>LLM</fixed-case>s) in estimating these relationships, comparing them with a logistic regression model that uses interpretable textual features, including the number of turns and the frequency of honorific expressions. The results show that even advanced <fixed-case>LLM</fixed-case>s struggle with social relationship estimation, performing worse than a simple heuristic-based approach. This finding highlights the need for further improvement in enabling <fixed-case>LLM</fixed-case>s to naturally capture social relationships in multi-party dialogues.</abstract>
      <url hash="d868bd9b">2026.iwsds-1.38</url>
      <bibkey>fukushige-etal-2026-estimating</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>WER</fixed-case> is Unaware: Assessing How <fixed-case>ASR</fixed-case> Errors Distort Clinical Understanding in Patient Facing Dialogue</title>
      <author><first>Zachary</first><last>Ellis</last></author>
      <author><first>Jared</first><last>Joselowitz</last></author>
      <author><first>Yash</first><last>Deo</last></author>
      <author><first>Yajie</first><last>He</last></author>
      <author><first>Anna</first><last>Kalygina</last></author>
      <author><first>Aisling</first><last>Higham</last></author>
      <author><first>Mana</first><last>Rahimzadeh</last></author>
      <author><first>Yan</first><last>Jia</last></author>
      <author><first>Ibrahim</first><last>Habli</last></author>
      <author><first>Ernest</first><last>Lim</last></author>
      <pages>391–417</pages>
      <abstract>As Automatic Speech Recognition (<fixed-case>ASR</fixed-case>) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (<fixed-case>WER</fixed-case>). This paper challenges that standard, investigating whether <fixed-case>WER</fixed-case> or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their <fixed-case>ASR</fixed-case>-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that <fixed-case>WER</fixed-case> and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an <fixed-case>LLM</fixed-case>-as-a-Judge, programmatically optimized using <fixed-case>GEPA</fixed-case> to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong <fixed-case>C</fixed-case>ohen’s kappa of 0.816. This work provides a validated, automated framework for moving <fixed-case>ASR</fixed-case> evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.</abstract>
      <url hash="06984904">2026.iwsds-1.39</url>
      <bibkey>ellis-etal-2026-wer</bibkey>
    </paper>
    <paper id="40">
      <title><fixed-case>R</fixed-case>eflect<fixed-case>OR</fixed-case>: an <fixed-case>LLM</fixed-case>-based Agent for Post-Operative Surgical Debriefing</title>
      <author><first>Lorenzo</first><last>Fumi</last></author>
      <author><first>Marco</first><last>Bombieri</last></author>
      <author><first>Sara</first><last>Allievi</last></author>
      <author><first>Stefano</first><last>Bonvini</last></author>
      <author><first>Theodora</first><last>Chaspari</last></author>
      <author><first>Marco</first><last>Zenati</last></author>
      <author><first>Paolo</first><last>Giorgini</last></author>
      <pages>418–427</pages>
      <abstract>Ineffective teamwork and communication can generate medical errors in the high-pressure environment of surgery, making post-operative debriefings essential for enhancing team performance and patient safety. However, these sessions are frequently rushed or incomplete due to clinicians’ limited time. This paper introduces <fixed-case>R</fixed-case>eflect<fixed-case>OR</fixed-case>, an Agentic-<fixed-case>AI</fixed-case> architecture designed to support surgical debriefings by processing audio recordings from the operating room. The system employs specialized sub-agents that perform tasks such as generating summaries, constructing timelines of intraoperative events, identifying potential errors and counting the materials used. A qualitative evaluation indicates that the system effectively contextualizes transcripts, demonstrating its potential as a valuable tool for surgical debriefing. The paper also outlines key considerations for applying such an architecture in real-world clinical environments.</abstract>
      <url hash="78df73c0">2026.iwsds-1.40</url>
      <bibkey>fumi-etal-2026-reflector</bibkey>
    </paper>
    <paper id="41">
      <title>Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue</title>
      <author><first>Run</first><last>Chen</last></author>
      <author><first>Wen</first><last>Liang</last></author>
      <author><first>Ziwei</first><last>Gong</last></author>
      <author><first>Lin</first><last>Ai</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <pages>428–440</pages>
      <abstract>Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark <fixed-case>SPEECHMENTALMANIP</fixed-case> that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.</abstract>
      <url hash="ef3c4120">2026.iwsds-1.41</url>
      <bibkey>chen-etal-2026-detecting</bibkey>
    </paper>
    <paper id="42">
      <title><fixed-case>C</fixed-case>o<fixed-case>V</fixed-case>a<fixed-case>P</fixed-case>h: A Vision-Language Multi-Agent Dialogue System for Tool-Augmented Pharmacogenetic Reasoning and Personalized Guidance</title>
      <author><first>Shang-Chun</first><last>Lu</last></author>
      <author><first>Hsin</first><last>Yang</last></author>
      <author><first>Hui-Hsin</first><last>Xue</last></author>
      <author><first>Ping</first><last>Tsai</last></author>
      <author><first>Yu</first><last>Weng</last></author>
      <author><first>Shiou-Chi</first><last>Li</last></author>
      <author><first>Jen-Wei</first><last>Huang</last></author>
      <author><first>Hui</first><last>Chang</last></author>
      <pages>441–451</pages>
      <abstract>The post-pandemic healthcare labor crisis has intensified the demand for accessible, high-precision pharmaceutical care. To meet this challenge, we introduce <fixed-case>C</fixed-case>o<fixed-case>V</fixed-case>a<fixed-case>P</fixed-case>h, a multi-agent pharmacogenetic framework that integrates information retrieval with Large Language Model (<fixed-case>LLM</fixed-case>) and Vision-Language Model (<fixed-case>VLM</fixed-case>) technologies. At its core, a fine-tuned query rewriting module transforms clinical inquiries into structured search indices, ensuring precise multimodal retrieval from <fixed-case>CPIC</fixed-case> and <fixed-case>P</fixed-case>harm<fixed-case>GKB</fixed-case> while mitigating hallucination risks. By synthesizing structured <fixed-case>API</fixed-case> data with unstructured evidence from guidelines, our framework delivers highly reliable, context-aware responses, surpassing benchmarks by 10% on expert-curated datasets. This approach provides a scalable solution to alleviate clinical workloads and democratize access to specialized medical knowledge.</abstract>
      <url hash="1232bf9b">2026.iwsds-1.42</url>
      <bibkey>lu-etal-2026-covaph</bibkey>
    </paper>
  </volume>
</collection>
