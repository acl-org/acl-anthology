<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.iwslt">
  <volume id="1" ingest-date="2023-07-08" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)</booktitle>
      <editor><first>Elizabeth</first><last>Salesky</last></editor>
      <editor><first>Marcello</first><last>Federico</last></editor>
      <editor><first>Marine</first><last>Carpuat</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada (in-person and online)</address>
      <month>July</month>
      <year>2023</year>
      <url hash="8e78b9e2">2023.iwslt-1</url>
      <venue>iwslt</venue>
    </meta>
    <frontmatter>
      <url hash="ec1df76e">2023.iwslt-1.0</url>
      <bibkey>iwslt-2023-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>FINDINGS</fixed-case> <fixed-case>OF</fixed-case> <fixed-case>THE</fixed-case> <fixed-case>IWSLT</fixed-case> 2023 <fixed-case>EVALUATION</fixed-case> <fixed-case>CAMPAIGN</fixed-case></title>
      <author><first>Milind</first><last>Agarwal</last><affiliation>GMU</affiliation></author>
      <author><first>Sweta</first><last>Agrawal</last><affiliation>UMD</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>GMU</affiliation></author>
      <author><first>Luisa</first><last>Bentivogli</last><affiliation>FBK</affiliation></author>
      <author><first>Ondřej</first><last>Bojar</last><affiliation>Charles U.</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>U. Malta</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>UMD</affiliation></author>
      <author><first>Roldano</first><last>Cattoni</last><affiliation>FBK</affiliation></author>
      <author><first>Mauro</first><last>Cettolo</last><affiliation>FBK</affiliation></author>
      <author><first>Mingda</first><last>Chen</last><affiliation>Meta</affiliation></author>
      <author><first>William</first><last>Chen</last><affiliation>CMU</affiliation></author>
      <author><first>Khalid</first><last>Choukri</last><affiliation>ELDA</affiliation></author>
      <author><first>Alexandra</first><last>Chronopoulou</last><affiliation>LMU</affiliation></author>
      <author><first>Anna</first><last>Currey</last><affiliation>AWS</affiliation></author>
      <author><first>Thierry</first><last>Declerck</last><affiliation>DFKI</affiliation></author>
      <author><first>Qianqian</first><last>Dong</last><affiliation>Bytedance</affiliation></author>
      <author><first>Kevin</first><last>Duh</last><affiliation>JHU</affiliation></author>
      <author><first>Yannick</first><last>Estève</last><affiliation>Avignon U.</affiliation></author>
      <author><first>Marcello</first><last>Federico</last><affiliation>AWS</affiliation></author>
      <author><first>Souhir</first><last>Gahbiche</last><affiliation>Airbus</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>U. Edinburgh</affiliation></author>
      <author><first>Benjamin</first><last>Hsu</last><affiliation>AWS</affiliation></author>
      <author><first>Phu</first><last>Mon Htut</last><affiliation>AWS</affiliation></author>
      <author><first>Hirofumi</first><last>Inaguma</last><affiliation>Meta</affiliation></author>
      <author><first>Dávid</first><last>Javorský</last><affiliation>Charles U.</affiliation></author>
      <author><first>John</first><last>Judge</last><affiliation>DCU</affiliation></author>
      <author><first>Yasumasa</first><last>Kano</last><affiliation>NAIST</affiliation></author>
      <author><first>Tom</first><last>Ko</last><affiliation>Bytedance</affiliation></author>
      <author><first>Rishu</first><last>Kumar</last><affiliation>Charles U.</affiliation></author>
      <author><first>Pengwei</first><last>Li</last><affiliation>Meta</affiliation></author>
      <author><first>Xutai</first><last>Ma</last><affiliation>Meta</affiliation></author>
      <author><first>Prashant</first><last>Mathur</last><affiliation>AWS</affiliation></author>
      <author><first>Evgeny</first><last>Matusov</last><affiliation>AppTek</affiliation></author>
      <author><first>Paul</first><last>McNamee</last><affiliation>JHU</affiliation></author>
      <author><first>John</first><last>P. McCrae</last><affiliation>U. Galway</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>JHU</affiliation></author>
      <author><first>Maria</first><last>Nadejde</last><affiliation>AWS</affiliation></author>
      <author><first>Satoshi</first><last>Nakamura</last><affiliation>NAIST</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>FBK</affiliation></author>
      <author><first>Ha</first><last>Nguyen</last><affiliation>Avignon U.</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>KIT</affiliation></author>
      <author><first>Xing</first><last>Niu</last><affiliation>AWS</affiliation></author>
      <author><first>Atul</first><last>Kr. Ojha</last><affiliation>U. Galway</affiliation></author>
      <author><first>John</first><last>E. Ortega</last><affiliation>Northeastern U.</affiliation></author>
      <author><first>Proyag</first><last>Pal</last><affiliation>U. Edinburgh</affiliation></author>
      <author><first>Juan</first><last>Pino</last><affiliation>Meta</affiliation></author>
      <author><first>Lonneke</first><last>van der Plas</last><affiliation>IDIAP</affiliation></author>
      <author><first>Peter</first><last>Polák</last><affiliation>Charles U.</affiliation></author>
      <author><first>Elijah</first><last>Rippeth</last><affiliation>UMD</affiliation></author>
      <author><first>Elizabeth</first><last>Salesky</last><affiliation>JHU</affiliation></author>
      <author><first>Jiatong</first><last>Shi</last><affiliation>CMU</affiliation></author>
      <author><first>Matthias</first><last>Sperber</last><affiliation>Apple</affiliation></author>
      <author><first>Sebastian</first><last>Stüker</last><affiliation>Zoom</affiliation></author>
      <author><first>Katsuhito</first><last>Sudoh</last><affiliation>NAIST</affiliation></author>
      <author><first>Yun</first><last>Tang</last><affiliation>Meta</affiliation></author>
      <author><first>Brian</first><last>Thompson</last><affiliation>AWS</affiliation></author>
      <author><first>Kevin</first><last>Tran</last><affiliation>Meta</affiliation></author>
      <author><first>Marco</first><last>Turchi</last><affiliation>Zoom</affiliation></author>
      <author><first>Alex</first><last>Waibel</last><affiliation>CMU</affiliation></author>
      <author><first>Mingxuan</first><last>Wang</last><affiliation>Bytedance</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>CMU</affiliation></author>
      <author><first>Rodolfo</first><last>Zevallos</last><affiliation>U. Pompeu Fabra</affiliation></author>
      <pages>1-61</pages>
      <abstract>This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.</abstract>
      <url hash="a13b69f7">2023.iwslt-1.1</url>
      <bibkey>agrawal-etal-2023-findings</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.1</doi>
      <revision id="1" href="2023.iwslt-1.1v1" hash="88b6899b"/>
      <revision id="2" href="2023.iwslt-1.1v2" hash="a13b69f7" date="2023-09-09">Minor updates.</revision>
    </paper>
    <paper id="2">
      <title>Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology</title>
      <author><first>Elizabeth</first><last>Salesky</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Kareem</first><last>Darwish</last><affiliation>aiXplain Inc.</affiliation></author>
      <author><first>Mohamed</first><last>Al-Badrashiny</last><affiliation>aiXplain inc.</affiliation></author>
      <author><first>Mona</first><last>Diab</last><affiliation>Meta Responsible AI</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <pages>62-78</pages>
      <abstract>We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.</abstract>
      <url hash="78b6c7c4">2023.iwslt-1.2</url>
      <attachment type="dataset" hash="8c714d0c">2023.iwslt-1.2.dataset.zip</attachment>
      <bibkey>salesky-etal-2023-evaluating</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.2</doi>
    </paper>
    <paper id="3">
      <title>The <fixed-case>M</fixed-case>ine<fixed-case>T</fixed-case>rans Systems for <fixed-case>IWSLT</fixed-case> 2023 Offline Speech Translation and Speech-to-Speech Translation Tasks</title>
      <author><first>Yichao</first><last>Du</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Guo</first><last>Zhengsheng</last><affiliation>tencent</affiliation></author>
      <author><first>Jinchuan</first><last>Tian</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhirui</first><last>Zhang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xing</first><last>Wang</last><affiliation>Tencent</affiliation></author>
      <author><first>Jianwei</first><last>Yu</last><affiliation>Tencent AI lab</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Tong</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Enhong</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>79-88</pages>
      <abstract>This paper presents the extscMineTrans English-to-Chinese speech translation systems developed for two challenge tracks of IWSLT 2023, i.e., Offline Speech Translation (S2T) and Speech-to-Speech Translation (S2ST). For the S2T track, extscMineTrans employs a practical cascaded system to explore the limits of translation performance in both constrained and unconstrained settings, where the whole system consists of automatic speech recognition (ASR), punctuation recognition (PC), and machine translation (MT) modules. We also investigate the effectiveness of multiple ASR architectures and explore two MT strategies: supervised in-domain fine-tuning and prompt-guided translation using a large language model. For the S2ST track, we explore a speech-to-unit (S2U) framework to build an end-to-end S2ST system. This system encodes the target speech as discrete units via our trained HuBERT. Then it leverages the standard sequence-to-sequence model to directly learn the mapping between source speech and discrete units without any auxiliary recognition tasks (i.e., ASR and MT tasks). Various efforts are made to improve the extscMineTrans’s performance, such as acoustic model pre-training on large-scale data, data filtering, data augmentation, speech segmentation, knowledge distillation, consistency training, model ensembles, etc.</abstract>
      <url hash="a19a990f">2023.iwslt-1.3</url>
      <bibkey>du-etal-2023-minetrans</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.3</doi>
    </paper>
    <paper id="4">
      <title>Improving End-to-End Speech Translation by Imitation-Based Knowledge Distillation with Synthetic Transcripts</title>
      <author><first>Rebekka</first><last>Hubert</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Artem</first><last>Sokolov</last><affiliation>Google</affiliation></author>
      <author><first>Stefan</first><last>Riezler</last><affiliation>Heidelberg University</affiliation></author>
      <pages>89-101</pages>
      <abstract>End-to-end automatic speech translation (AST) relies on data that combines audio inputs with text translation outputs. Previous work used existing large parallel corpora of transcriptions and translations in a knowledge distillation (KD) setup to distill a neural machine translation (NMT) into an AST student model. While KD allows using larger pretrained models, the reliance of previous KD approaches on manual audio transcripts in the data pipeline restricts the applicability of this framework to AST. We present an imitation learning approach where a teacher NMT system corrects the errors of an AST student without relying on manual transcripts. We show that the NMT teacher can recover from errors in automatic transcriptions and is able to correct erroneous translations of the AST student, leading to improvements of about 4 BLEU points over the standard AST end-to-end baseline on the English-German CoVoST-2 and MuST-C datasets, respectively. Code and data are publicly available: <url>https://github.com/HubReb/imitkd_ast/releases/tag/v1.1</url></abstract>
      <url hash="34801608">2023.iwslt-1.4</url>
      <bibkey>hubert-etal-2023-improving</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.4</doi>
    </paper>
    <paper id="5">
      <title>The <fixed-case>USTC</fixed-case>’s Dialect Speech Translation System for <fixed-case>IWSLT</fixed-case> 2023</title>
      <author><first>Pan</first><last>Deng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Shihao</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Weitai</first><last>Zhang</last><affiliation>USTC</affiliation></author>
      <author><first>Jie</first><last>Zhang</last><affiliation>University of Science &amp;Technology of China</affiliation></author>
      <author><first>Lirong</first><last>Dai</last><affiliation>University of Science &amp;Technology of China</affiliation></author>
      <pages>102-112</pages>
      <abstract>This paper presents the USTC system for the IWSLT 2023 Dialectal and Low-resource shared task, which involves translation from Tunisian Arabic to English. We aim to investigate the mutual transfer between Tunisian Arabic and Modern Standard Arabic (MSA) to enhance the performance of speech translation (ST) by following standard pre-training and fine-tuning pipelines. We synthesize a substantial amount of pseudo Tunisian-English paired data using a multi-step pre-training approach. Integrating a Tunisian-MSA translation module into the end-to-end ST model enables the transfer from Tunisian to MSA and facilitates linguistic normalization of the dialect. To increase the robustness of the ST system, we optimize the model’s ability to adapt to ASR errors and propose a model ensemble method. Results indicate that applying the dialect transfer method can increase the BLEU score of dialectal ST. It is shown that the optimal system ensembles both cascaded and end-to-end ST models, achieving BLEU improvements of 2.4 and 2.8 in test1 and test2 sets, respectively, compared to the best published system.</abstract>
      <url hash="0909713e">2023.iwslt-1.5</url>
      <bibkey>deng-etal-2023-ustcs</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>KIT</fixed-case>’s Multilingual Speech Translation System for <fixed-case>IWSLT</fixed-case> 2023</title>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Thai</first><last>Binh Nguyen</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Sai</first><last>Koneru</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Enes</first><last>Yavuz Ugan</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Ngoc-Quan</first><last>Pham</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Tuan</first><last>Nam Nguyen</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Tu</first><last>Anh Dinh</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Carlos</first><last>Mullov</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Alexander</first><last>Waibel</last><affiliation>Carnegie Mellon</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <pages>113-122</pages>
      <abstract>Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (<tex-math>k</tex-math>NN-MT) for effective adaptation (<tex-math>+0.8</tex-math> BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.</abstract>
      <url hash="54f9aafa">2023.iwslt-1.6</url>
      <bibkey>liu-etal-2023-kits</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.6</doi>
    </paper>
    <paper id="7">
      <title>The <fixed-case>BIGAI</fixed-case> Offline Speech Translation Systems for <fixed-case>IWSLT</fixed-case> 2023 Evaluation</title>
      <author><first>Zhihang</first><last>Xie</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <pages>123-129</pages>
      <abstract>This paper describes the BIGAI’s submission to IWSLT 2023 Offline Speech Translation task on three language tracks from English to Chinese, German and Japanese. The end-to-end systems are built upon a Wav2Vec2 model for speech recognition and mBART50 models for machine translation. An adapter module is applied to bridge the speech module and the translation module. The CTC loss between speech features and source token sequence is incorporated during training. Experiments show that the systems can generate reasonable translations on three languages. The proposed models achieve BLEU scores of 22.3 for en→de, 10.7 for en→ja and 33.0 for en→zh on tst2023 TED datasets. However, the performance is decreased by a significant margin on complex scenarios like persentations and interview.</abstract>
      <url hash="1561e975">2023.iwslt-1.7</url>
      <bibkey>xie-2023-bigai</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.7</doi>
    </paper>
    <paper id="8">
      <title>Enhancing Video Translation Context with Object Labels</title>
      <author><first>Jeremy</first><last>Gwinnup</last><affiliation>Air Force Research Laboratory</affiliation></author>
      <author><first>Tim</first><last>Anderson</last><affiliation>Air Force Research Laboratory</affiliation></author>
      <author><first>Brian</first><last>Ore</last><affiliation>AFRL</affiliation></author>
      <author><first>Eric</first><last>Hansen</last><affiliation>Air Force Research Laboratory</affiliation></author>
      <author><first>Kevin</first><last>Duh</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>130-137</pages>
      <abstract>We present a simple yet efficient method to enhance the quality of machine translation models trained on multimodal corpora by augmenting the training text with labels of detected objects in the corresponding video segments. We then test the effects of label augmentation in both baseline and two automatic speech recognition (ASR) conditions. In contrast with multimodal techniques that merge visual and textual features, our modular method is easy to implement and the results are more interpretable. Comparisons are made with Transformer translation architectures trained with baseline and augmented labels, showing improvements of up to +1.0 BLEU on the How2 dataset.</abstract>
      <url hash="f520aea5">2023.iwslt-1.8</url>
      <bibkey>gwinnup-etal-2023-enhancing</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.8</doi>
    </paper>
    <paper id="9">
      <title>Length-Aware <fixed-case>NMT</fixed-case> and Adaptive Duration for Automatic Dubbing</title>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Jinlong</first><last>Yang</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuhao</first><last>Xie</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Bin</first><last>Wei</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Jiawei</first><last>Zheng</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <pages>138-143</pages>
      <abstract>This paper presents the submission of Huawei Translation Services Center for the IWSLT 2023 dubbing task in the unconstrained setting. The proposed solution consists of a Transformer-based machine translation model and a phoneme duration predictor. The Transformer is deep and multiple target-to-source length-ratio class labels are used to control target lengths. The variation predictor in FastSpeech2 is utilized to predict phoneme durations. To optimize the isochrony in dubbing, re-ranking and scaling are performed. The source audio duration is used as a reference to re-rank the translations of different length-ratio labels, and the one with minimum time deviation is preferred. Additionally, the phoneme duration outputs are scaled within a defined threshold to narrow the duration gap with the source audio.</abstract>
      <url hash="465376d6">2023.iwslt-1.9</url>
      <bibkey>rao-etal-2023-length</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>NAVER</fixed-case> <fixed-case>LABS</fixed-case> <fixed-case>E</fixed-case>urope’s Multilingual Speech Translation Systems for the <fixed-case>IWSLT</fixed-case> 2023 Low-Resource Track</title>
      <author><first>Edward</first><last>Gow-Smith</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Alexandre</first><last>Berard</last><affiliation>Naver Labs Europe</affiliation></author>
      <author><first>Marcely</first><last>Zanon Boito</last><affiliation>NAVER LABS Europe</affiliation></author>
      <author><first>Ioan</first><last>Calapodescu</last><affiliation>Naver Labs Europe</affiliation></author>
      <pages>144-158</pages>
      <abstract>This paper presents NAVER LABS Europe’s systems for Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our work attempts to maximize translation quality in low-resource settings using multilingual parameter-efficient solutions that leverage strong pre-trained models. Our primary submission for Tamasheq outperforms the previous state of the art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU on this year’s test set, outperforming the second best participant by 7.7 points. For Quechua, we also rank first and achieve 17.7 BLEU, despite having only two hours of translation data. Finally, we show that our proposed multilingual architecture is also competitive for high-resource languages, outperforming the best unconstrained submission to the IWSLT 2021 Multilingual track, despite using much less training data and compute.</abstract>
      <url hash="9124a6d8">2023.iwslt-1.10</url>
      <bibkey>gow-smith-etal-2023-naver</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.10</doi>
    </paper>
    <paper id="11">
      <title>Direct Models for Simultaneous Translation and Automatic Subtitling: <fixed-case>FBK</fixed-case>@<fixed-case>IWSLT</fixed-case>2023</title>
      <author><first>Sara</first><last>Papi</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Marco</first><last>Gaido</last><affiliation>Fondazione Bruno Kessler, University of Trento</affiliation></author>
      <author><first>Matteo</first><last>Negri</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>159-168</pages>
      <abstract>This paper describes the FBK’s participation in the Simultaneous Translation and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission focused on the use of direct architectures to perform both tasks: for the simultaneous one, we leveraged the knowledge already acquired by offline-trained models and directly applied a policy to obtain the real-time inference; for the subtitling one, we adapted the direct ST model to produce well-formed subtitles and exploited the same architecture to produce timestamps needed for the subtitle synchronization with audiovisual content. Our English-German SimulST system shows a reduced computational-aware latency compared to the one achieved by the top-ranked systems in the 2021 and 2022 rounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling system outperforms the only-existing solution based on a direct system by 3.7 and 1.7 SubER in English-German and English-Spanish respectively.</abstract>
      <url hash="c4023a91">2023.iwslt-1.11</url>
      <bibkey>papi-etal-2023-direct</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.11</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>MT</fixed-case> Metrics Correlate with Human Ratings of Simultaneous Speech Translation</title>
      <author><first>Dominik</first><last>Macháček</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <author><first>Ondřej</first><last>Bojar</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>NICT</affiliation></author>
      <pages>169-179</pages>
      <abstract>There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human ratings of SST, which has been recently collected as Continuous Ratings (CR), are unclear. In this paper, we leverage the evaluations of candidate systems submitted to the English-German SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and the aforementioned metrics. Our study reveals that the offline metrics are well correlated with CR and can be reliably used for evaluating machine translation in simultaneous mode, with some limitations on the test set size. We conclude that given the current quality levels of SST, these metrics can be used as proxies for CR, alleviating the need for large scale human evaluation. Additionally, we observe that correlations of the metrics with translation as a reference is significantly higher than with simultaneous interpreting, and thus we recommend the former for reliable evaluation.</abstract>
      <url hash="d66a6517">2023.iwslt-1.12</url>
      <bibkey>machacek-etal-2023-mt</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.12</doi>
    </paper>
    <paper id="13">
      <title>Improving Neural Machine Translation Formality Control with Domain Adaptation and Reranking-based Transductive Learning</title>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jinlong</first><last>Yang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuhao</first><last>Xie</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Bin</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jiawei</first><last>Zheng</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Ming</first><last>Zhu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <author><first>Yanfei</first><last>Jiang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <pages>180-186</pages>
      <abstract>This paper presents Huawei Translation Service Center (HW-TSC)’s submission on the IWSLT 2023 formality control task, which provides two training scenarios: supervised and zero-shot, each containing two language pairs, and sets constrained and unconstrained conditions. We train the formality control models for these four language pairs under these two conditions respectively, and submit the corresponding translation results. Our efforts are divided into two fronts: enhancing general translation quality and improving formality control capability. According to the different requirements of the formality control task, we use a multi-stage pre-training method to train a bilingual or multilingual neural machine translation (NMT) model as the basic model, which can improve the general translation quality of the base model to a relatively high level. Then, under the premise of affecting the general translation quality of the basic model as little as possible, we adopt domain adaptation and reranking-based transductive learning methods to improve the formality control capability of the model.</abstract>
      <url hash="2ba7e950">2023.iwslt-1.13</url>
      <bibkey>wu-etal-2023-improving</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case> at <fixed-case>IWSLT</fixed-case>2023: Break the Quality Ceiling of Offline Track via Pre-Training and Domain Adaptation</title>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Xie</first><last>YuHao</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Guo</first><last>JiaXin</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Wang</first><last>Minghan</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Li</first><last>ShaoJun</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Lei</first><last>LiZhi</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <pages>187-193</pages>
      <abstract>This paper presents HW-TSC’s submissions to the IWSLT 2023 Offline Speech Translation task, including speech translation of talks from English to German, Chinese, and Japanese, respectively. We participate in all three conditions (constrained training, constrained with large language models training, and unconstrained training) with models of cascaded architectures. We use data enhancement, pre-training models and other means to improve the ASR quality, and R-Drop, deep model, domain data selection, etc. to improve the translation quality. Compared with last year’s best results, we achieve 2.1 BLEU improvement on the MuST-C English-German test set.</abstract>
      <url hash="8c672044">2023.iwslt-1.14</url>
      <bibkey>li-etal-2023-hw</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.14</doi>
    </paper>
    <paper id="15">
      <title>Submission of <fixed-case>USTC</fixed-case>’s System for the <fixed-case>IWSLT</fixed-case> 2023 - Offline Speech Translation Track</title>
      <author><first>Xinyuan</first><last>Zhou</last><affiliation>iflytek</affiliation></author>
      <author><first>Jianwei</first><last>Cui</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhongyi</first><last>Ye</last><affiliation>iflytek</affiliation></author>
      <author><first>Yichi</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Luzhen</first><last>Xu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hanyi</first><last>Zhang</last><affiliation>iflytek</affiliation></author>
      <author><first>Weitai</first><last>Zhang</last><affiliation>USTC</affiliation></author>
      <author><first>Lirong</first><last>Dai</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>194-201</pages>
      <abstract>This paper describes the submissions of the research group USTC-NELSLIP to the 2023 IWSLT Offline Speech Translation competition, which involves translating spoken English into written Chinese. We utilize both cascaded models and end-to-end models for this task. To improve the performance of the cascaded models, we introduce Whisper to reduce errors in the intermediate source language text, achieving a significant improvement in ASR recognition performance. For end-to-end models, we propose Stacked Acoustic-and-Textual En- coding extension (SATE-ex), which feeds the output of the acoustic decoder into the textual decoder for information fusion and to prevent error propagation. Additionally, we improve the performance of the end-to-end system in translating speech by combining the SATE-ex model with the encoder-decoder model through ensembling.</abstract>
      <url hash="d2be0096">2023.iwslt-1.15</url>
      <bibkey>zhou-etal-2023-submission</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>I</fixed-case>2<fixed-case>R</fixed-case>’s End-to-End Speech Translation System for <fixed-case>IWSLT</fixed-case> 2023 Offline Shared Task</title>
      <author><first>Muhammad</first><last>Huzaifah</last><affiliation>Agency for Science, Technology and Research</affiliation></author>
      <author><first>Kye</first><last>Min Tan</last><affiliation>Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Richeng</first><last>Duan</last><affiliation>ASTAR</affiliation></author>
      <pages>202-210</pages>
      <abstract>This paper describes I2R’s submission to the offline speech translation track for IWSLT 2023. We focus on an end-to-end approach for translation from English audio to German text, one of the three available language directions in this year’s edition. The I2R system leverages on pretrained models that have been exposed to large-scale audio and text data for our base model. We introduce several stages of additional pretraining followed by fine-tuning to adapt the system for the downstream speech translation task. The strategy is supplemented by other techniques such as data augmentation, domain tagging, knowledge distillation, and model ensemble, among others. We evaluate the system on several publicly available test sets for comparison.</abstract>
      <url hash="30cf7c8b">2023.iwslt-1.16</url>
      <bibkey>huzaifah-etal-2023-i2rs</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.16</doi>
    </paper>
    <paper id="17">
      <title>The <fixed-case>N</fixed-case>iu<fixed-case>T</fixed-case>rans End-to-End Speech Translation System for <fixed-case>IWSLT</fixed-case>23 <fixed-case>E</fixed-case>nglish-to-<fixed-case>C</fixed-case>hinese Offline Task</title>
      <author><first>Yuchen</first><last>Han</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Xiaoqian</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Hao</first><last>Chen</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yuhao</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Chen</first><last>Xu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jingbo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>211-218</pages>
      <abstract>This paper describes the NiuTrans end-to-end speech translation system submitted for the IWSLT 2023 English-to-Chinese offline task. Our speech translation models are composed of pre-trained ASR and MT models under the SATE framework. Several pre-trained models with diverse architectures and input representations (e.g., log Mel-filterbank and waveform) were utilized. We proposed an IDA method to iteratively improve the performance of the MT models and generate the pseudo ST data through MT systems. We then trained ST models with different structures and data settings to enhance ensemble performance. Experimental results demonstrate that our NiuTrans system achieved a BLEU score of 29.22 on the MuST-C En-Zh tst-COMMON set, outperforming the previous year’s submission by 0.12 BLEU despite using less MT training data.</abstract>
      <url hash="59560a4f">2023.iwslt-1.17</url>
      <bibkey>han-etal-2023-niutrans</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.17</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>ON</fixed-case>-<fixed-case>TRAC</fixed-case> Consortium Systems for the <fixed-case>IWSLT</fixed-case> 2023 Dialectal and Low-resource Speech Translation Tasks</title>
      <author><first>Antoine</first><last>Laurent</last><affiliation>LIUM - Laboratoire Informatique Universit du Mans</affiliation></author>
      <author><first>Souhir</first><last>Gahbiche</last><affiliation>Airbus</affiliation></author>
      <author><first>Ha</first><last>Nguyen</last><affiliation>LIA - Universit Avignon</affiliation></author>
      <author><first>Haroun</first><last>Elleuch</last><affiliation>Elyadata</affiliation></author>
      <author><first>Fethi</first><last>Bougares</last><affiliation>Elyadata</affiliation></author>
      <author><first>Antoine</first><last>Thiol</last><affiliation>Airbus</affiliation></author>
      <author><first>Hugo</first><last>Riguidel</last><affiliation>LIUM - Le Mans University</affiliation></author>
      <author><first>Salima</first><last>Mdhaffar</last><affiliation>LIA - University of Avignon</affiliation></author>
      <author><first>Gaëlle</first><last>Laperrière</last><affiliation>Avignon University LIA</affiliation></author>
      <author><first>Lucas</first><last>Maison</last><affiliation>LIA - Avignon University</affiliation></author>
      <author><first>Sameer</first><last>Khurana</last><affiliation>MIT</affiliation></author>
      <author><first>Yannick</first><last>Estève</last><affiliation>LIA - Avignon University</affiliation></author>
      <pages>219-226</pages>
      <abstract>This paper describes the ON-TRAC consortium speech translation systems developed for IWSLT 2023 evaluation campaign. Overall, we participated in three speech translation tracks featured in the low-resource and dialect speech translation shared tasks, namely; i) spoken Tamasheq to written French, ii) spoken Pashto to written French, and iii) spoken Tunisian to written English. All our primary submissions are based on the end-to-end speech-to-text neural architecture using a pretrained SAMU-XLSR model as a speech encoder and a mbart model as a decoder. The SAMU-XLSR model is built from the XLS-R 128 in order to generate language agnostic sentence-level embeddings. This building is driven by the LaBSE model trained on multilingual text dataset. This architecture allows us to improve the input speech representations and achieve significant improvements compared to conventional end-to-end speech translation systems.</abstract>
      <url hash="ff5abece">2023.iwslt-1.18</url>
      <bibkey>laurent-etal-2023-trac</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>BUT</fixed-case> Systems for <fixed-case>IWSLT</fixed-case> 2023 <fixed-case>M</fixed-case>arathi - <fixed-case>H</fixed-case>indi Low Resource Speech Translation Task</title>
      <author><first>Santosh</first><last>Kesiraju</last><affiliation>Brno University of Technology</affiliation></author>
      <author><first>Karel</first><last>Beneš</last><affiliation>Brno University of Technology</affiliation></author>
      <author><first>Maksim</first><last>Tikhonov</last><affiliation>Brno University of Technology</affiliation></author>
      <author><first>Jan</first><last>Černocký</last><affiliation>Brno University of Technology</affiliation></author>
      <pages>227-234</pages>
      <abstract>This paper describes the systems submitted for Marathi to Hindi low-resource speech translation task. Our primary submission is based on an end-to-end direct speech translation system, whereas the contrastive one is a cascaded system. The backbone of both the systems is a Hindi-Marathi bilingual ASR system trained on 2790 hours of imperfect transcribed speech. The end-to-end speech translation system was directly initialized from the ASR, and then fine-tuned for direct speech translation with an auxiliary CTC loss for translation. The MT model for the cascaded system is initialized from a cross-lingual language model, which was then fine-tuned using 1.6 M parallel sentences. All our systems were trained from scratch on publicly available datasets. In the end, we use a language model to re-score the n-best hypotheses. Our primary submission achieved 30.5 and 39.6 BLEU whereas the contrastive system obtained 21.7 and 28.6 BLEU on official dev and test sets respectively. The paper also presents the analysis on several experiments that were conducted and outlines the strategies for improving speech translation in low-resource scenarios.</abstract>
      <url hash="e82e4d94">2023.iwslt-1.19</url>
      <bibkey>kesiraju-etal-2023-systems</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>CMU</fixed-case>’s <fixed-case>IWSLT</fixed-case> 2023 Simultaneous Speech Translation System</title>
      <author><first>Brian</first><last>Yan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Jiatong</first><last>Shi</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Soumi</first><last>Maiti</last><affiliation>ML researcher</affiliation></author>
      <author><first>William</first><last>Chen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Xinjian</first><last>Li</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yifan</first><last>Peng</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Siddhant</first><last>Arora</last><affiliation>Student at Carnegie Mellon Univeristy</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>235-240</pages>
      <abstract>This paper describes CMU’s submission to the IWSLT 2023 simultaneous speech translation shared task for translating English speech to both German text and speech in a streaming fashion. We first build offline speech-to-text (ST) models using the joint CTC/attention framework. These models also use WavLM front-end features and mBART decoder initialization. We adapt our offline ST models for simultaneous speech-to-text translation (SST) by 1) incrementally encoding chunks of input speech, re-computing encoder states for each new chunk and 2) incrementally decoding output text, pruning beam search hypotheses to 1-best after processing each chunk. We then build text-to-speech (TTS) models using the VITS framework and achieve simultaneous speech-to-speech translation (SS2ST) by cascading our SST and TTS models.</abstract>
      <url hash="40383252">2023.iwslt-1.20</url>
      <bibkey>yan-etal-2023-cmus</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.20</doi>
    </paper>
    <paper id="21">
      <title>Improving Low Resource Speech Translation with Data Augmentation and Ensemble Strategies</title>
      <author><first>Akshaya Vishnu Kudlu</first><last>Shanbhogue</last><affiliation>Amazon</affiliation></author>
      <author><first>Ran</first><last>Xue</last><affiliation>Amazon</affiliation></author>
      <author><first>Soumya</first><last>Saha</last><affiliation>Amazon</affiliation></author>
      <author><first>Daniel</first><last>Zhang</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Ashwinkumar</first><last>Ganesan</last><affiliation>Amazon Alexa AI</affiliation></author>
      <pages>241-250</pages>
      <abstract>This paper describes the speech translation system submitted as part of the IWSLT 2023 shared task on low resource speech translation. The low resource task aids in building models for language pairs where the training corpus is limited. In this paper, we focus on two language pairs, namely, Tamasheq-French (Tmh→Fra) and Marathi-Hindi (Mr→Hi) and implement a speech translation system that is unconstrained. We evaluate three strategies in our system: (a) Data augmentation where we perform different operations on audio as well as text samples, (b) an ensemble model that integrates a set of models trained using a combination of augmentation strategies, and (c) post-processing techniques where we explore the use of large language models (LLMs) to improve the quality of sentences that are generated. Experiments show how data augmentation can relatively improve the BLEU score by 5.2% over the baseline system for Tmh→Fra while an ensemble model further improves performance by 17% for Tmh→Fra and 23% for Mr→Hi task.</abstract>
      <url hash="5163ab5e">2023.iwslt-1.21</url>
      <bibkey>vishnu-kudlu-shanbhogue-etal-2023-improving</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.21</doi>
    </paper>
    <paper id="22">
      <title>Speech Translation with Style: <fixed-case>A</fixed-case>pp<fixed-case>T</fixed-case>ek’s Submissions to the <fixed-case>IWSLT</fixed-case> Subtitling and Formality Tracks in 2023</title>
      <author><first>Parnia</first><last>Bahar</last><affiliation>AppTek</affiliation></author>
      <author><first>Patrick</first><last>Wilken</last><affiliation>AppTek</affiliation></author>
      <author><first>Javier</first><last>Iranzo-Sánchez</last><affiliation>Universitat Politcnica de Valncia</affiliation></author>
      <author><first>Mattia</first><last>Di Gangi</last><affiliation>AppTek GmbH</affiliation></author>
      <author><first>Evgeny</first><last>Matusov</last><affiliation>AppTek</affiliation></author>
      <author><first>Zoltán</first><last>Tüske</last><affiliation>IBM Research</affiliation></author>
      <pages>251-260</pages>
      <abstract>AppTek participated in the subtitling and formality tracks of the IWSLT 2023 evaluation. This paper describes the details of our subtitling pipeline - speech segmentation, speech recognition, punctuation prediction and inverse text normalization, text machine translation and direct speech-to-text translation, intelligent line segmentation - and how we make use of the provided subtitling-specific data in training and fine-tuning. The evaluation results show that our final submissions are competitive, in particular outperforming the submissions by other participants by 5% absolute as measured by the SubER subtitle quality metric. For the formality track, we participate with our En-Ru and En-Pt production models, which support formality control via prefix tokens. Except for informal Portuguese, we achieve near perfect formality level accuracy while at the same time offering high general translation quality.</abstract>
      <url hash="7006e994">2023.iwslt-1.22</url>
      <bibkey>bahar-etal-2023-speech</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>QUESPA</fixed-case> Submission for the <fixed-case>IWSLT</fixed-case> 2023 Dialect and Low-resource Speech Translation Tasks</title>
      <author><first>John</first><last>E. Ortega</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Rodolfo</first><last>Zevallos</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <author><first>William</first><last>Chen</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>261-268</pages>
      <abstract>This article describes the QUESPA team speech translation (ST) submissions for the Quechua to Spanish (QUE–SPA) track featured in the Evaluation Campaign of IWSLT 2023: low-resource and dialect speech translation. Two main submission types were supported in the campaign: constrained and unconstrained. We submitted six total systems of which our best (primary) constrained system consisted of an ST model based on the Fairseq S2T framework where the audio representations were created using log mel-scale filter banks as features and the translations were performed using a transformer. The best (primary) unconstrained system used a pipeline approach which combined automatic speech recognition (ASR) with machine translation (MT). The ASR transcriptions for the best unconstrained system were computed using a pre-trained XLS-R-based model along with a fine-tuned language model. Transcriptions were translated using a MT system based on a fine-tuned, pre-trained language model (PLM). The four other submissions are presented in this article (2 constrained and 2 unconstrained) for comparison because they consist of various architectures. Our results show that direct ST (ASR and MT combined together) can be more effective than a PLM in a low-resource (constrained) setting for Quechua to Spanish. On the other hand, we show that fine-tuning of any type on both the ASR and MT system is worthwhile, resulting in nearly 16 BLEU for the unconstrained task.</abstract>
      <url hash="40ce4130">2023.iwslt-1.23</url>
      <bibkey>e-ortega-etal-2023-quespa</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.23</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>GMU</fixed-case> Systems for the <fixed-case>IWSLT</fixed-case> 2023 Dialect and Low-resource Speech Translation Tasks</title>
      <author><first>Jonathan</first><last>Mbuya</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>George Mason University</affiliation></author>
      <pages>269-276</pages>
      <abstract>This paper describes the GMU Systems for the IWSLT 2023 Dialect and Low-resource Speech Translation Tasks. We submitted systems for five low-resource tasks and the dialectal task. In this work, we explored self-supervised pre-trained speech models and finetuned them on speech translation downstream tasks. We use the Wav2vec 2.0, XLSR-53, and Hubert as self-supervised models. Unlike Hubert, Wav2vec 2.0 and XLSR-53 achieve the best results when we remove the top three layers. Our results show that Wav2vec 2.0 and Hubert perform similarly with their relative best configuration. In addition, we found that Wav2vec 2.0 pre-trained on audio data of the same language as the source language of a speech translation model achieves better results. For the low-resource setting, the best results are achieved using either the Wav2vec 2.0 or Hubert models, while XLSR-53 achieves the best results for the dialectal transfer task. We find that XLSR-53 does not perform well for low-resource tasks. Using Wav2vec 2.0, we report close to 2 BLEU point improvements on the test set for the Tamasheq-French compared to the baseline system at the IWSLT 2022.</abstract>
      <url hash="2a0e165c">2023.iwslt-1.24</url>
      <bibkey>mbuya-anastasopoulos-2023-gmu</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.24</doi>
    </paper>
    <paper id="25">
      <title>The <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Speech-to-Speech Translation System for <fixed-case>IWSLT</fixed-case> 2023</title>
      <author><first>Minghan</first><last>Wang</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Yinglu</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Huawei</affiliation></author>
      <author><first>Shimin</first><last>Tao</last><affiliation>huawei</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <pages>277-282</pages>
      <abstract>This paper describes our work on the IWSLT2023 Speech-to-Speech task. Our proposed cascaded system consists of an ensemble of Conformer and S2T-Transformer-based ASR models, a Transformer-based MT model, and a Diffusion-based TTS model. Our primary focus in this competition was to investigate the modeling ability of the Diffusion model for TTS tasks in high-resource scenarios and the role of TTS in the overall S2S task. To this end, we proposed DTS, an end-to-end diffusion-based TTS model that takes raw text as input and generates waveform by iteratively denoising on pure Gaussian noise. Compared to previous TTS models, the speech generated by DTS is more natural and performs better in code-switching scenarios. As the training process is end-to-end, it is relatively straightforward. Our experiments demonstrate that DTS outperforms other TTS models on the GigaS2S benchmark, and also brings positive gains for the entire S2S system.</abstract>
      <url hash="46b8df8e">2023.iwslt-1.25</url>
      <bibkey>wang-etal-2023-hw</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.25</doi>
    </paper>
    <paper id="26">
      <title><fixed-case>JHU</fixed-case> <fixed-case>IWSLT</fixed-case> 2023 Dialect Speech Translation System Description</title>
      <author><first>Amir</first><last>Hussein</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Cihan</first><last>Xiao</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Neha</first><last>Verma</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Thomas</first><last>Thebaud</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Matthew</first><last>Wiesner</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Sanjeev</first><last>Khudanpur</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>283-290</pages>
      <abstract>This paper presents JHU’s submissions to the IWSLT 2023 dialectal and low-resource track of Tunisian Arabic to English speech translation. The Tunisian dialect lacks formal orthography and abundant training data, making it challenging to develop effective speech translation (ST) systems. To address these challenges, we explore the integration of large pre-trained machine translation (MT) models, such as mBART and NLLB-200 in both end-to-end (E2E) and cascaded speech translation (ST) systems. We also improve the performance of automatic speech recognition (ASR) through the use of pseudo-labeling data augmentation and channel matching on telephone data. Finally, we combine our E2E and cascaded ST systems with Minimum Bayes-Risk decoding. Our combined system achieves a BLEU score of 21.6 and 19.1 on test2 and test3, respectively.</abstract>
      <url hash="9a9b1d46">2023.iwslt-1.26</url>
      <bibkey>hussein-etal-2023-jhu</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.26</doi>
    </paper>
    <paper id="27">
      <title>Learning Nearest Neighbour Informed Latent Word Embeddings to Improve Zero-Shot Machine Translation</title>
      <author><first>Nishant</first><last>Kambhatla</last><affiliation>Simon Fraser University</affiliation></author>
      <author><first>Logan</first><last>Born</last><affiliation>Simon Fraser University</affiliation></author>
      <author><first>Anoop</first><last>Sarkar</last><affiliation>Simon Fraser University</affiliation></author>
      <pages>291-301</pages>
      <abstract>Multilingual neural translation models exploit cross-lingual transfer to perform zero-shot translation between unseen language pairs. Past efforts to improve cross-lingual transfer have focused on aligning contextual sentence-level representations. This paper introduces three novel contributions to allow exploiting nearest neighbours at the token level during training, including: (i) an efficient, gradient-friendly way to share representations between neighboring tokens; (ii) an attentional semantic layer which extracts latent features from shared embeddings; and (iii) an agreement loss to harmonize predictions across different sentence representations. Experiments on two multilingual datasets demonstrate consistent gains in zero shot translation over strong baselines.</abstract>
      <url hash="6f9b0c40">2023.iwslt-1.27</url>
      <bibkey>kambhatla-etal-2023-learning</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.27</doi>
    </paper>
    <paper id="28">
      <title><fixed-case>JHU</fixed-case> <fixed-case>IWSLT</fixed-case> 2023 Multilingual Speech Translation System Description</title>
      <author><first>Henry Li</first><last>Xinyuan</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Neha</first><last>Verma</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Bismarck</first><last>Bamfo Odoom</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Ujvala</first><last>Pradeep</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Matthew</first><last>Wiesner</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Sanjeev</first><last>Khudanpur</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>302-310</pages>
      <abstract>We describe the Johns Hopkins ACL 60-60 Speech Translation systems submitted to the IWSLT 2023 Multilingual track, where we were tasked to translate ACL presentations from English into 10 languages. We developed cascaded speech translation systems for both the constrained and unconstrained subtracks. Our systems make use of pre-trained models as well as domain-specific corpora for this highly technical evaluation-only task. We find that the specific technical domain which ACL presentations fall into presents a unique challenge for both ASR and MT, and we present an error analysis and an ACL-specific corpus we produced to enable further work in this area.</abstract>
      <url hash="f849c76e">2023.iwslt-1.28</url>
      <bibkey>xinyuan-etal-2023-jhu</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.28</doi>
    </paper>
    <paper id="29">
      <title>The <fixed-case>NPU</fixed-case>-<fixed-case>MSXF</fixed-case> Speech-to-Speech Translation System for <fixed-case>IWSLT</fixed-case> 2023 Speech-to-Speech Translation Task</title>
      <author><first>Kun</first><last>Song</last><affiliation>Northwestern Polytechnical University</affiliation></author>
      <author><first>Yi</first><last>Lei</last><affiliation>Northwestern Polytechnical University</affiliation></author>
      <author><first>Peikun</first><last>Chen</last><affiliation>Northwestern Polytechnical University</affiliation></author>
      <author><first>Yiqing</first><last>Cao</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Kun</first><last>Wei</last><affiliation>Northwestern Polytechnical University</affiliation></author>
      <author><first>Yongmao</first><last>Zhang</last><affiliation>Northwestern Polytechnical University</affiliation></author>
      <author><first>Lei</first><last>Xie</last><affiliation>Northwestern Polytechnical University</affiliation></author>
      <author><first>Ning</first><last>Jiang</last><affiliation>MaShang Consumer Finance Co.Ltd</affiliation></author>
      <author><first>Guoqing</first><last>Zhao</last><affiliation>MaShang Consumer Finance Co.Ltd</affiliation></author>
      <pages>311-320</pages>
      <abstract>This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech translation (S2ST) task which aims to translate from English speech of multi-source to Chinese speech. The system is built in a cascaded manner consisting of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS). We make tremendous efforts to handle the challenging multi-source input. Specifically, to improve the robustness to multi-source speech input, we adopt various data augmentation strategies and a ROVER-based score fusion on multiple ASR model outputs. To better handle the noisy ASR transcripts, we introduce a three-stage fine-tuning strategy to improve translation accuracy. Finally, we build a TTS model with high naturalness and sound quality, which leverages a two-stage framework, using network bottleneck features as a robust intermediate representation for speaker timbre and linguistic content disentanglement. Based on the two-stage framework, pre-trained speaker embedding is leveraged as a condition to transfer the speaker timbre in the source English speech to the translated Chinese speech. Experimental results show that our system has high translation accuracy, speech naturalness, sound quality, and speaker similarity. Moreover, it shows good robustness to multi-source data.</abstract>
      <url hash="66d4202b">2023.iwslt-1.29</url>
      <bibkey>song-etal-2023-npu</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.29</doi>
    </paper>
    <paper id="30">
      <title>Low-Resource Formality Controlled <fixed-case>NMT</fixed-case> Using Pre-trained <fixed-case>LM</fixed-case></title>
      <author><first>Priyesh</first><last>Vakharia</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Shree</first><last>Vignesh S</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Pranjali</first><last>Basmatkar</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>321-329</pages>
      <abstract>This paper describes the UCSC’s submission to the shared task on formality control for spoken language translation at IWSLT 2023. For this task, we explored the use of ‘additive style intervention’ using a pre-trained multilingual translation model, namely mBART. Compared to prior approaches where a single style-vector was added to all tokens in the encoder output, we explored an alternative approach in which we learn a unique style-vector for each input token. We believe this approach, which we call ‘style embedding intervention,’ is better suited for formality control as it can potentially learn which specific input tokens to modify during decoding. While the proposed approach obtained similar performance to ‘additive style intervention’ for the supervised English-to-Vietnamese task, it performed significantly better for English-to-Korean, in which it achieved an average matched accuracy of 90.6 compared to 85.2 for the baseline. When we constrained the model further to only perform style intervention on the &lt;bos&gt; (beginning of sentence) token, the average matched accuracy improved further to 92.0, indicating that the model could learn to control the formality of the translation output based solely on the embedding of the &lt;bos&gt; token.</abstract>
      <url hash="9a3fc1ee">2023.iwslt-1.30</url>
      <bibkey>vakharia-etal-2023-low</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.30</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>NAIST</fixed-case> Simultaneous Speech-to-speech Translation System for <fixed-case>IWSLT</fixed-case> 2023</title>
      <author><first>Ryo</first><last>Fukuda</last><affiliation>NAIST</affiliation></author>
      <author><first>Yuta</first><last>Nishikawa</last><affiliation>NAIST</affiliation></author>
      <author><first>Yasumasa</first><last>Kano</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Yuka</first><last>Ko</last><affiliation>NAIST</affiliation></author>
      <author><first>Tomoya</first><last>Yanagita</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Kosuke</first><last>Doi</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Mana</first><last>Makinae</last><affiliation>NAIST</affiliation></author>
      <author><first>Sakriani</first><last>Sakti</last><affiliation>JAIST/NAIST</affiliation></author>
      <author><first>Katsuhito</first><last>Sudoh</last><affiliation>NAIST</affiliation></author>
      <author><first>Satoshi</first><last>Nakamura</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>330-340</pages>
      <abstract>This paper describes NAIST’s submission to the IWSLT 2023 Simultaneous Speech Translation task: English-to-German, Japanese, Chinese speech-to-text translation and English-to-Japanese speech-to-speech translation. Our speech-to-text system uses an end-to-end multilingual speech translation model based on large-scale pre-trained speech and text models. We add Inter-connections into the model to incorporate the outputs from intermediate layers of the pre-trained speech model and augment prefix-to-prefix text data using Bilingual Prefix Alignment to enhance the simultaneity of the offline speech translation model. Our speech-to-speech system employs an incremental text-to-speech module that consists of a Japanese pronunciation estimation model, an acoustic model, and a neural vocoder.</abstract>
      <url hash="11fea76c">2023.iwslt-1.31</url>
      <bibkey>fukuda-etal-2023-naist</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.31</doi>
    </paper>
    <paper id="32">
      <title>Language Model Based Target Token Importance Rescaling for Simultaneous Neural Machine Translation</title>
      <author><first>Aditi</first><last>Jain</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Nishant</first><last>Kambhatla</last><affiliation>Simon Fraser University</affiliation></author>
      <author><first>Anoop</first><last>Sarkar</last><affiliation>Simon Fraser University</affiliation></author>
      <pages>341-356</pages>
      <abstract>The decoder in simultaneous neural machine translation receives limited information from the source while having to balance the opposing requirements of latency versus translation quality. In this paper, we use an auxiliary target-side language model to augment the training of the decoder model. Under this notion of target adaptive training, generating rare or difficult tokens is rewarded which improves the translation quality while reducing latency. The predictions made by a language model in the decoder are combined with the traditional cross entropy loss which frees up the focus on the source side context. Our experimental results over multiple language pairs show that compared to previous state of the art methods in simultaneous translation, we can use an augmented target side context to improve BLEU scores significantly. We show improvements over the state of the art in the low latency range with lower average lagging values (faster output).</abstract>
      <url hash="959b1181">2023.iwslt-1.32</url>
      <bibkey>jain-etal-2023-language</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.32</doi>
    </paper>
    <paper id="33">
      <title>The <fixed-case>K</fixed-case>yoto Speech-to-Speech Translation System for <fixed-case>IWSLT</fixed-case> 2023</title>
      <author><first>Zhengdong</first><last>Yang</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Shuichiro</first><last>Shimizu</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Wangjin</first><last>Zhou</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Sheng</first><last>Li</last><affiliation>NICT), Advanced Speech Technology Laboratory</affiliation></author>
      <author><first>Chenhui</first><last>Chu</last><affiliation>Kyoto University</affiliation></author>
      <pages>357-362</pages>
      <abstract>This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.</abstract>
      <url hash="b96c3e0d">2023.iwslt-1.33</url>
      <bibkey>yang-etal-2023-kyoto</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.33</doi>
    </paper>
    <paper id="34">
      <title>Tagged End-to-End Simultaneous Speech Translation Training Using Simultaneous Interpretation Data</title>
      <author><first>Yuka</first><last>Ko</last><affiliation>NAIST</affiliation></author>
      <author><first>Ryo</first><last>Fukuda</last><affiliation>NAIST</affiliation></author>
      <author><first>Yuta</first><last>Nishikawa</last><affiliation>NAIST</affiliation></author>
      <author><first>Yasumasa</first><last>Kano</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Katsuhito</first><last>Sudoh</last><affiliation>NAIST</affiliation></author>
      <author><first>Satoshi</first><last>Nakamura</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>363-375</pages>
      <abstract>Simultaneous speech translation (SimulST) translates partial speech inputs incrementally. Although the monotonic correspondence between input and output is preferable for smaller latency, it is not the case for distant language pairs such as English and Japanese. A prospective approach to this problem is to mimic simultaneous interpretation (SI) using SI data to train a SimulST model. However, the size of such SI data is limited, so the SI data should be used together with ordinary bilingual data whose translations are given in offline. In this paper, we propose an effective way to train a SimulST model using mixed data of SI and offline. The proposed method trains a single model using the mixed data with style tags that tell the model to generate SI- or offline-style outputs. Experiment results show improvements of BLEURT in different latency ranges, and our analyses revealed the proposed model generates SI-style outputs more than the baseline.</abstract>
      <url hash="587ed719">2023.iwslt-1.34</url>
      <bibkey>ko-etal-2023-tagged</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.34</doi>
    </paper>
    <paper id="35">
      <title>The <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Simultaneous Speech-to-Text Translation System for <fixed-case>IWSLT</fixed-case> 2023 Evaluation</title>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Minghan</first><last>Wang</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yuhao</first><last>Xie</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <pages>376-382</pages>
      <abstract>In this paper, we present our submission to the IWSLT 2023 Simultaneous Speech-to-Text Translation competition. Our participation involves three language directions: English-German, English-Chinese, and English-Japanese. Our proposed solution is a cascaded incremental decoding system that comprises an ASR model and an MT model. The ASR model is based on the U2++ architecture and can handle both streaming and offline speech scenarios with ease. Meanwhile, the MT model adopts the Deep-Transformer architecture. To improve performance, we explore methods to generate a confident partial target text output that guides the next MT incremental decoding process. In our experiments, we demonstrate that our simultaneous strategies achieve low latency while maintaining a loss of no more than 2 BLEU points when compared to offline systems.</abstract>
      <url hash="337d8a4e">2023.iwslt-1.35</url>
      <bibkey>guo-etal-2023-hw</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.35</doi>
    </paper>
    <paper id="36">
      <title>The <fixed-case>HW</fixed-case>-<fixed-case>TSC</fixed-case>’s Simultaneous Speech-to-Speech Translation System for <fixed-case>IWSLT</fixed-case> 2023 Evaluation</title>
      <author><first>Hengchao</first><last>Shang</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zhiqiang</first><last>Rao</last><affiliation>Huawei Translation Service Center, Beijing, China</affiliation></author>
      <author><first>Zongyao</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zhanglin</first><last>Wu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Jiaxin</first><last>Guo</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Minghan</first><last>Wang</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Daimeng</first><last>Wei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Shaojun</first><last>Li</last><affiliation>Huawei Translation Services Center</affiliation></author>
      <author><first>Zhengzhe</first><last>Yu</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei</affiliation></author>
      <author><first>Lizhi</first><last>Lei</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Huawei Co. Ltd</affiliation></author>
      <pages>383-388</pages>
      <abstract>In this paper, we present our submission to the IWSLT 2023 Simultaneous Speech-to-Speech Translation competition. Our participation involves three language directions: English-German, English-Chinese, and English-Japanese. Our solution is a cascaded incremental decoding system, consisting of an ASR model, an MT model, and a TTS model. By adopting the strategies used in the Speech-to-Text track, we have managed to generate a more confident target text for each audio segment input, which can guide the next MT incremental decoding process. Additionally, we have integrated the TTS model to seamlessly reproduce audio files from the translation hypothesis. To enhance the effectiveness of our experiment, we have utilized a range of methods to reduce error conditions in the TTS input text and improve the smoothness of the TTS output audio.</abstract>
      <url hash="9c895bc8">2023.iwslt-1.36</url>
      <bibkey>shang-etal-2023-hw</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.36</doi>
    </paper>
    <paper id="37">
      <title>Towards Efficient Simultaneous Speech Translation: <fixed-case>CUNI</fixed-case>-<fixed-case>KIT</fixed-case> System for Simultaneous Track at <fixed-case>IWSLT</fixed-case> 2023</title>
      <author><first>Peter</first><last>Polák</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <author><first>Danni</first><last>Liu</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Ngoc-Quan</first><last>Pham</last><affiliation>Karlsruhe Institute of Technology</affiliation></author>
      <author><first>Jan</first><last>Niehues</last><affiliation>Karlsruhe Institut of Technology</affiliation></author>
      <author><first>Alexander</first><last>Waibel</last><affiliation>Carnegie Mellon</affiliation></author>
      <author><first>Ondřej</first><last>Bojar</last><affiliation>Charles University, MFF UFAL</affiliation></author>
      <pages>389-396</pages>
      <abstract>In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).</abstract>
      <url hash="a8342aa1">2023.iwslt-1.37</url>
      <bibkey>polak-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.37</doi>
    </paper>
    <paper id="38">
      <title>Speech Translation with Foundation Models and Optimal Transport: <fixed-case>UPC</fixed-case> at <fixed-case>IWSLT</fixed-case>23</title>
      <author><first>Ioannis</first><last>Tsiamas</last><affiliation>UPC</affiliation></author>
      <author><first>Gerard</first><last>I. Gállego</last><affiliation>Universitat Politcnica de Catalunya</affiliation></author>
      <author><first>Jose</first><last>Fonollosa</last><affiliation>Universitat Politecnica de Catalunya</affiliation></author>
      <author><first>Marta</first><last>R. Costa-jussá</last><affiliation>Meta AI</affiliation></author>
      <pages>397-410</pages>
      <abstract>This paper describes the submission of the UPC Machine Translation group to the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We incorporate a Siamese pretraining step of the speech and text encoders with CTC and Optimal Transport, to adapt the speech representations to the space of the text model, thus maximizing transfer learning from MT. After this pretraining, we fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge Distillation. Apart from the available ST corpora, we create synthetic data with SegAugment to better adapt our models to the custom segmentations of the IWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C tst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released IWSLT.ACLdev2023.</abstract>
      <url hash="5ec26e2e">2023.iwslt-1.38</url>
      <bibkey>tsiamas-etal-2023-speech</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.38</doi>
    </paper>
    <paper id="39">
      <title>The Xiaomi <fixed-case>AI</fixed-case> Lab’s Speech Translation Systems for <fixed-case>IWSLT</fixed-case> 2023 Offline Task, Simultaneous Task and Speech-to-Speech Task</title>
      <author><first>Wuwei</first><last>Huang</last><affiliation>Xiaomi AI Lab</affiliation></author>
      <author><first>Mengge</first><last>Liu</last><affiliation>Beijing Institute Of Technology</affiliation></author>
      <author><first>Xiang</first><last>Li</last><affiliation>Xiaomi AI Lab</affiliation></author>
      <author><first>Yanzhi</first><last>Tian</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Fengyu</first><last>Yang</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Wen</first><last>Zhang</last><affiliation>Xiaomi AI Lab</affiliation></author>
      <author><first>Jian</first><last>Luan</last><affiliation>Xiaomi AI Lab</affiliation></author>
      <author><first>Bin</first><last>Wang</last><affiliation>Xiaomi AI Lab</affiliation></author>
      <author><first>Yuhang</first><last>Guo</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen university</affiliation></author>
      <pages>411-419</pages>
      <abstract>This system description paper introduces the systems submitted by Xiaomi AI Lab to the three tracks of the IWSLT 2023 Evaluation Campaign, namely the offline speech translation (Offline-ST) track, the offline speech-to-speech translation (Offline-S2ST) track, and the simultaneous speech translation (Simul-ST) track. All our submissions for these three tracks only involve the English-Chinese language direction. Our English-Chinese speech translation systems are constructed using large-scale pre-trained models as the foundation. Specifically, we fine-tune these models’ corresponding components for various downstream speech translation tasks. Moreover, we implement several popular techniques, such as data filtering, data augmentation, speech segmentation, and model ensemble, to improve the system’s overall performance. Extensive experiments show that our systems achieve a significant improvement over the strong baseline systems in terms of the automatic evaluation metric.</abstract>
      <url hash="b42668b2">2023.iwslt-1.39</url>
      <bibkey>huang-etal-2023-xiaomi</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.39</doi>
    </paper>
    <paper id="40">
      <title>Improving Formality-Sensitive Machine Translation Using Data-Centric Approaches and Prompt Engineering</title>
      <author><first>Seungjun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>420-432</pages>
      <abstract>In this paper, we present the KU x Upstage team’s submission for the Special Task on Formality Control on Spoken Language Translation, which involves translating English into four languages with diverse grammatical formality markers. Our methodology comprises two primary components: 1) a language-specific data-driven approach, and 2) the generation of synthetic data through the employment of large-scale language models and empirically-grounded prompt engineering. By adapting methodologies and models to accommodate the unique linguistic properties of each language, we observe a notable enhancement in performance relative to the baseline, substantiating the heightened efficacy of data-driven approaches. Moreover, our devised prompt engineering strategy yields superior synthetic translation instances.</abstract>
      <url hash="16b2e8db">2023.iwslt-1.40</url>
      <bibkey>lee-etal-2023-improving-formality</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.40</doi>
    </paper>
    <paper id="41">
      <title><fixed-case>UM</fixed-case>-<fixed-case>DFKI</fixed-case> <fixed-case>M</fixed-case>altese Speech Translation</title>
      <author><first>Aiden</first><last>Williams</last><affiliation>University of Malta</affiliation></author>
      <author><first>Kurt</first><last>Abela</last><affiliation>University of Malta</affiliation></author>
      <author><first>Rishu</first><last>Kumar</last><affiliation>Charles University</affiliation></author>
      <author><first>Martin</first><last>Bär</last><affiliation>University of Malta</affiliation></author>
      <author><first>Hannah</first><last>Billinghurst</last><affiliation>University of Malta</affiliation></author>
      <author><first>Kurt</first><last>Micallef</last><affiliation>University of Malta</affiliation></author>
      <author><first>Ahnaf</first><last>Mozib Samin</last><affiliation>University of Malta</affiliation></author>
      <author><first>Andrea</first><last>DeMarco</last><affiliation>University of Malta</affiliation></author>
      <author><first>Lonneke</first><last>van der Plas</last><affiliation>IDIAP</affiliation></author>
      <author><first>Claudia</first><last>Borg</last><affiliation>University of Malta</affiliation></author>
      <pages>433-441</pages>
      <abstract>For the 2023 IWSLT Maltese Speech Translation Task, UM-DFKI jointly presents a cascade solution which achieves 0.6 BLEU. While this is the first time that a Maltese speech translation task has been released by IWSLT, this paper explores previous solutions for other speech translation tasks, focusing primarily on low-resource scenarios. Moreover, we present our method of fine-tuning XLS-R models for Maltese ASR using a collection of multi-lingual speech corpora as well as the fine-tuning of the mBART model for Maltese to English machine translation.</abstract>
      <url hash="821fcbb5">2023.iwslt-1.41</url>
      <bibkey>williams-etal-2023-um</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.41</doi>
    </paper>
    <paper id="42">
      <title><fixed-case>NVIDIA</fixed-case> <fixed-case>N</fixed-case>e<fixed-case>M</fixed-case>o Offline Speech Translation Systems for <fixed-case>IWSLT</fixed-case> 2023</title>
      <author><first>Oleksii</first><last>Hrinchuk</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Vladimir</first><last>Bataev</last><affiliation>STC-innovations Ltd</affiliation></author>
      <author><first>Evelina</first><last>Bakhturina</last><affiliation>Nvidia</affiliation></author>
      <author><first>Boris</first><last>Ginsburg</last><affiliation>NVIDIA</affiliation></author>
      <pages>442-448</pages>
      <abstract>This paper provides an overview of NVIDIA NeMo’s speech translation systems for the IWSLT 2023 Offline Speech Translation Task. This year, we focused on end-to-end system which capitalizes on pre-trained models and synthetic data to mitigate the problem of direct speech translation data scarcity. When trained on IWSLT 2022 constrained data, our best En-&gt;De end-to-end model achieves the average score of 31 BLEU on 7 test sets from IWSLT 2010-2020 which improves over our last year cascade (28.4) and end-to-end (25.7) submissions. When trained on IWSLT 2023 constrained data, the average score drops to 29.5 BLEU.</abstract>
      <url hash="0f15c9d0">2023.iwslt-1.42</url>
      <bibkey>hrinchuk-etal-2023-nvidia</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.42</doi>
    </paper>
    <paper id="43">
      <title><fixed-case>SRI</fixed-case>-<fixed-case>B</fixed-case>’s Systems for <fixed-case>IWSLT</fixed-case> 2023 Dialectal and Low-resource Track: <fixed-case>M</fixed-case>arathi-<fixed-case>H</fixed-case>indi Speech Translation</title>
      <author><first>Balaji</first><last>Radhakrishnan</last><affiliation>Samsung R&amp;D Institute Bangalore</affiliation></author>
      <author><first>Saurabh</first><last>Agrawal</last><affiliation>Samsung R&amp;D Institute Bangalore</affiliation></author>
      <author><first>Raj</first><last>Prakash Gohil</last><affiliation>Samsung R&amp;D Institute Bangalore</affiliation></author>
      <author><first>Kiran</first><last>Praveen</last><affiliation>Samsung R&amp;D Institute Bangalore</affiliation></author>
      <author><first>Advait</first><last>Vinay Dhopeshwarkar</last><affiliation>Samsung R&amp;D Institute Bangalore</affiliation></author>
      <author><first>Abhishek</first><last>Pandey</last><affiliation>Samsung R&amp;D Institute, Bangalore</affiliation></author>
      <pages>449-454</pages>
      <abstract>This paper describes the speech translation systems SRI-B developed for the IWSLT 2023 Evaluation Campaign Dialectal and Low-resource track: Marathi-Hindi Speech Translation. We propose systems for both the constrained (systems are trained only on the datasets provided by the organizers) and the unconstrained conditions (systems can be trained with any resource). For both the conditions, we build end-to-end speech translation networks comprising of a conformer encoder and a transformer decoder. Under both the conditions, we leverage Marathi Automatic Speech Recognition (ASR) data to pre-train the encoder and subsequently train the entire model on the speech translation data. Our results demonstrate that pre-training the encoder with ASR data is a key step in significantly improving the speech translation performance. We also show that conformer encoders are inherently superior to its transformer counterparts for speech translation tasks. Our primary submissions achieved a BLEU% score of 31.2 on the constrained condition and 32.4 on the unconstrained condition. We secured the top position in the constrained condition and second position in the unconstrained condition.</abstract>
      <url hash="00db8971">2023.iwslt-1.43</url>
      <bibkey>radhakrishnan-etal-2023-sri</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.43</doi>
    </paper>
    <paper id="44">
      <title><fixed-case>BIT</fixed-case>’s System for Multilingual Track</title>
      <author><first>Zhipeng</first><last>Wang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yuhang</first><last>Guo</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Shuoying</first><last>Chen</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>455-460</pages>
      <abstract>This paper describes the system we submitted to the IWSLT 2023 multilingual speech translation track, with input being English speech and output being text in 10 target languages. Our system consists of CNN and Transformer, convolutional neural networks downsample speech features and extract local information, while transformer extract global features and output the final results. In our system, we use speech recognition tasks to pre-train encoder parameters, and then use speech translation corpus to train the multilingual speech translation model. We have also adopted other methods to optimize the model, such as data augmentation, model ensemble, etc. Our system can obtain satisfactory results on test sets of 10 languages in the MUST-C corpus.</abstract>
      <url hash="e68eb841">2023.iwslt-1.44</url>
      <bibkey>wang-etal-2023-bits</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.44</doi>
    </paper>
    <paper id="45">
      <title>Matesub: The Translated Subtitling Tool at the <fixed-case>IWSLT</fixed-case>2023 Subtitling Task</title>
      <author><first>Simone</first><last>Perone</last><affiliation>Translated</affiliation></author>
      <pages>461-464</pages>
      <abstract>This paper briefly describes Matesub, the subtitling tool Translated used to participate in the Subtitling shared task at IWSLT 2023. Matesub is a professional web-based tool that combines state-of-the-art AI with a WYSIWYG editor. The automatic generation of subtitles in Matesub is based on a cascade architecture, composed of ASR, text segmenter and MT neural models, which allows covering any pair from about 70 languages and their variants.</abstract>
      <url hash="ae4ac48f">2023.iwslt-1.45</url>
      <bibkey>perone-2023-matesub</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.45</doi>
    </paper>
    <paper id="46">
      <title>Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling</title>
      <author><first>Itai</first><last>Gat</last><affiliation>Meta</affiliation></author>
      <author><first>Felix</first><last>Kreuk</last><affiliation>Meta</affiliation></author>
      <author><first>Tu</first><last>Anh Nguyen</last><affiliation>Laboratoire de Sciences Cognitives et Psycholinguistique, ENS-PSL/EHESS/CNRS/INRIA</affiliation></author>
      <author><first>Ann</first><last>Lee</last><affiliation>Meta AI</affiliation></author>
      <author><first>Jade</first><last>Copet</last><affiliation>Meta</affiliation></author>
      <author><first>Gabriel</first><last>Synnaeve</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Emmanuel</first><last>Dupoux</last><affiliation>Ecole des Hautes Etudes en Sciences Sociales</affiliation></author>
      <author><first>Yossi</first><last>Adi</last><affiliation>The Hebrew University of Jerusalem / Facebook AI Research</affiliation></author>
      <pages>465-477</pages>
      <abstract>Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines.</abstract>
      <url hash="8a1339dd">2023.iwslt-1.46</url>
      <bibkey>gat-etal-2023-augmentation</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.46</doi>
    </paper>
    <paper id="47">
      <title><fixed-case>D</fixed-case>e<fixed-case>PA</fixed-case>: Improving Non-autoregressive Translation with Dependency-Aware Decoder</title>
      <author><first>Jiaao</first><last>Zhan</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Qian</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei</affiliation></author>
      <author><first>Wen</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yu</first><last>Bai</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Gao</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>478-490</pages>
      <abstract>Non-autoregressive machine translation (NAT) models have lower translation quality than autoregressive translation (AT) models because NAT decoders do not depend on previous target tokens in the decoder input. We propose a novel and general Dependency-Aware Decoder (DePA) to enhance target dependency modeling in the decoder of fully NAT models from two perspectives: decoder self-attention and decoder input. First, we propose an autoregressive forward-backward pre-training phase before NAT training, which enables the NAT decoder to gradually learn bidirectional target dependencies for the final NAT training. Second, we transform the decoder input from the source language representation space to the target language representation space through a novel attentive transformation process, which enables the decoder to better capture target dependencies. DePA can be applied to any fully NAT models. Extensive experiments show that DePA consistently improves highly competitive and state-of-the-art fully NAT models on widely used WMT and IWSLT benchmarks by up to 1.88 BLEU gain, while maintaining the inference latency comparable to other fully NAT models.</abstract>
      <url hash="ae82a891">2023.iwslt-1.47</url>
      <bibkey>zhan-etal-2023-depa</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.47</doi>
    </paper>
    <paper id="48">
      <title>On the Copying Problem of Unsupervised <fixed-case>NMT</fixed-case>: A Training Schedule with a Language Discriminator Loss</title>
      <author><first>Yihong</first><last>Liu</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Alexandra</first><last>Chronopoulou</last><affiliation>LMU Munich</affiliation></author>
      <author><first>Hinrich</first><last>Schütze</last><affiliation>Center for Information and Language Processing, University of Munich</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Ludwig-Maximilians-Universitt Mnchen</affiliation></author>
      <pages>491-502</pages>
      <abstract>Although unsupervised neural machine translation (UNMT) has achieved success in many language pairs, the copying problem, i.e., directly copying some parts of the input sentence as the translation, is common among distant language pairs, especially when low-resource languages are involved. We find this issue is closely related to an unexpected copying behavior during online back-translation (BT). In this work, we propose a simple but effective training schedule that incorporates a language discriminator loss. The loss imposes constraints on the intermediate translation so that the translation is in the desired language. By conducting extensive experiments on different language pairs, including similar and distant, high and low-resource languages, we find that our method alleviates the copying problem, thus improving the translation performance on low-resource languages.</abstract>
      <url hash="d1497fc5">2023.iwslt-1.48</url>
      <bibkey>liu-etal-2023-copying</bibkey>
      <doi>10.18653/v1/2023.iwslt-1.48</doi>
    </paper>
  </volume>
</collection>
