<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.sustainlp">
  <volume id="1" ingest-date="2023-07-12" type="proceedings">
    <meta>
      <booktitle>Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)</booktitle>
      <editor><first>Nafise</first><last>Sadat Moosavi</last></editor>
      <editor><first>Iryna</first><last>Gurevych</last></editor>
      <editor><first>Yufang</first><last>Hou</last></editor>
      <editor><first>Gyuwan</first><last>Kim</last></editor>
      <editor><first>Young Jin</first><last>Kim</last></editor>
      <editor><first>Tal</first><last>Schuster</last></editor>
      <editor><first>Ameeta</first><last>Agrawal</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Toronto, Canada (Hybrid)</address>
      <month>July</month>
      <year>2023</year>
      <url hash="479148d0">2023.sustainlp-1</url>
      <venue>sustainlp</venue>
    </meta>
    <frontmatter>
      <url hash="32f1578c">2023.sustainlp-1.0</url>
      <bibkey>sustainlp-2023-simple</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>K</fixed-case>wik<fixed-case>B</fixed-case>ucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals</title>
      <author><first>Sandeep</first><last>Silwal</last><affiliation>MIT</affiliation></author>
      <author><first>Sara</first><last>Ahmadian</last><affiliation>Google Research</affiliation></author>
      <author><first>Andrew</first><last>Nystrom</last><affiliation>Google AI</affiliation></author>
      <author><first>Andrew</first><last>Mccallum</last><affiliation>UMass Amherst</affiliation></author>
      <author><first>Deepak</first><last>Ramachandran</last><affiliation>Google Research</affiliation></author>
      <author><first>Mehran</first><last>Kazemi</last><affiliation>Google Research</affiliation></author>
      <pages>1-31</pages>
      <url hash="42f2422c">2023.sustainlp-1.1</url>
      <bibkey>silwal-etal-2023-kwikbucks</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.1</doi>
      <video href="2023.sustainlp-1.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Semantic-Oriented Unlabeled Priming for Large-Scale Language Models</title>
      <author><first>Yanchen</first><last>Liu</last><affiliation>Harvard University</affiliation></author>
      <author><first>Timo</first><last>Schick</last><affiliation>Meta AI</affiliation></author>
      <author><first>Hinrich</first><last>Schtze</last><affiliation>Center for Information and Language Processing, University of Munich</affiliation></author>
      <pages>32-38</pages>
      <url hash="db63d1f5">2023.sustainlp-1.2</url>
      <bibkey>liu-etal-2023-semantic</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>o<fixed-case>BERT</fixed-case>a: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes</title>
      <author><first>Daniel</first><last>Campos</last><affiliation>University of Illinois Urbana Champaign</affiliation></author>
      <author><first>Alexandre</first><last>Marques</last><affiliation>Neural Magic</affiliation></author>
      <author><first>Mark</first><last>Kurtz</last><affiliation>Neural Magic</affiliation></author>
      <author><first>Cheng</first><last>Xiang Zhai</last><affiliation>University of Illinois Urbana Champaign</affiliation></author>
      <pages>39-58</pages>
      <url hash="73f7d2b7">2023.sustainlp-1.3</url>
      <bibkey>campos-etal-2023-oberta</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.3</doi>
      <video href="2023.sustainlp-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Quick Dense Retrievers Consume <fixed-case>KALE</fixed-case>: Post Training <fixed-case>K</fixed-case>ullback<fixed-case>L</fixed-case>eibler Alignment of Embeddings for Asymmetrical dual encoders</title>
      <author><first>Daniel</first><last>Campos</last><affiliation>University of Illinois Urbana Champaign</affiliation></author>
      <author><first>Alessandro</first><last>Magnani</last><affiliation>Walmart Labs</affiliation></author>
      <author><first>Chengxiang</first><last>Zhai</last><affiliation>University of Illinois Urbana Champaign</affiliation></author>
      <pages>59-77</pages>
      <url hash="5e36886a">2023.sustainlp-1.4</url>
      <bibkey>campos-etal-2023-quick</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.4</doi>
      <video href="2023.sustainlp-1.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Lessons on Parameter Sharing across Layers in Transformers</title>
      <author><first>Sho</first><last>Takase</last><affiliation>LINE Corporation</affiliation></author>
      <author><first>Shun</first><last>Kiyono</last><affiliation>LINE Corporation</affiliation></author>
      <pages>78-90</pages>
      <url hash="e10f1d68">2023.sustainlp-1.5</url>
      <bibkey>takase-kiyono-2023-lessons</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.5</doi>
      <video href="2023.sustainlp-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency</title>
      <author><first>Daniel</first><last>Campos</last><affiliation>University of Illinois Urbana Champaign</affiliation></author>
      <author><first>Chengxiang</first><last>Zhai</last><affiliation>University of Illinois Urbana Champaign</affiliation></author>
      <pages>91-109</pages>
      <url hash="9dceca34">2023.sustainlp-1.6</url>
      <bibkey>campos-zhai-2023-asymmetry</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Small is the New Big: Pre-finetuned compact models are better for Asynchronous Active Learning</title>
      <author><first>Dantong</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Kaushik</first><last>Pavani</last><affiliation>Amazon</affiliation></author>
      <author><first>Sunny</first><last>Dasgupta</last><affiliation>Amazon</affiliation></author>
      <pages>110-120</pages>
      <url hash="ff648f01">2023.sustainlp-1.7</url>
      <bibkey>liu-etal-2023-small</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.7</doi>
      <video href="2023.sustainlp-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title><fixed-case>ADEPT</fixed-case>: Adapter-based Efficient Prompt Tuning Approach for Language Models</title>
      <author><first>Aditya</first><last>Shah</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Surendrabikram</first><last>Thapa</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Aneesh</first><last>Jain</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>121-128</pages>
      <url hash="132e9f1c">2023.sustainlp-1.8</url>
      <bibkey>shah-etal-2023-adept</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.8</doi>
      <video href="2023.sustainlp-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title><fixed-case>NLU</fixed-case> on Data Diets: Dynamic Data Subset Selection for <fixed-case>NLP</fixed-case> Classification Tasks</title>
      <author><first>Jean-michel</first><last>Attendu</last><affiliation>Nuance Communications</affiliation></author>
      <author><first>Jean-philippe</first><last>Corbeil</last><affiliation>Nuance Communications</affiliation></author>
      <pages>129-146</pages>
      <url hash="9ade188e">2023.sustainlp-1.9</url>
      <bibkey>attendu-corbeil-2023-nlu</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.9</doi>
      <video href="2023.sustainlp-1.9.mp4"/>
    </paper>
    <paper id="10">
      <title>On the Interactions of Structural Constraints and Data Resources for Structured Prediction</title>
      <author><first>Zhisong</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Emma</first><last>Strubell</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Eduard</first><last>Hovy</last><affiliation>University of Melbourne</affiliation></author>
      <pages>147-157</pages>
      <url hash="35a336ec">2023.sustainlp-1.10</url>
      <bibkey>zhang-etal-2023-interactions</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.10</doi>
      <video href="2023.sustainlp-1.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Can we Pretrain a <fixed-case>S</fixed-case>ot<fixed-case>A</fixed-case> Legal Language Model on a Budget From Scratch?</title>
      <author><first>Joel</first><last>Niklaus</last><affiliation>University of Bern</affiliation></author>
      <author><first>Daniele</first><last>Giofre</last><affiliation>Thomson Reuters</affiliation></author>
      <pages>158-182</pages>
      <url hash="99bc755e">2023.sustainlp-1.11</url>
      <bibkey>niklaus-giofre-2023-pretrain</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.11</doi>
      <video href="2023.sustainlp-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Is a Video worth n n Images? A Highly Efficient Approach to Transformer-based Video Question Answering</title>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Tianbo</first><last>Ji</last><affiliation>Nantong University</affiliation></author>
      <author><first>Yvette</first><last>Graham</last><affiliation>ADAPT, Trinity College Dublin</affiliation></author>
      <author><first>Jennifer</first><last>Foster</last><affiliation>Dublin City University</affiliation></author>
      <pages>183-189</pages>
      <url hash="223795ba">2023.sustainlp-1.12</url>
      <bibkey>lyu-etal-2023-video</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.12</doi>
      <video href="2023.sustainlp-1.12.mp4"/>
    </paper>
    <paper id="13">
      <title>How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?</title>
      <author><first>Xin</first><last>Xu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yuqi</first><last>Zhu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xiaohan</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>190-200</pages>
      <url hash="0ed4b9d5">2023.sustainlp-1.13</url>
      <bibkey>xu-etal-2023-unleash</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.13</doi>
      <video href="2023.sustainlp-1.13.mp4"/>
    </paper>
    <paper id="14">
      <title>Prompting language models improves performance in imbalanced setting</title>
      <author><first>Jay</first><last>Mohta</last><affiliation>Amazon</affiliation></author>
      <pages>201-211</pages>
      <url hash="22b5ed95">2023.sustainlp-1.14</url>
      <bibkey>mohta-2023-prompting</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>KGQA</fixed-case> Without Retraining</title>
      <author><first>Nick</first><last>Mckenna</last><affiliation>University of Edinburgh, School of Informatics</affiliation></author>
      <author><first>Priyanka</first><last>Sen</last><affiliation>Amazon</affiliation></author>
      <pages>212-218</pages>
      <url hash="71faa19b">2023.sustainlp-1.15</url>
      <bibkey>mckenna-sen-2023-kgqa</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.15</doi>
      <video href="2023.sustainlp-1.15.mp4"/>
    </paper>
    <paper id="16">
      <title><fixed-case>MANER</fixed-case>: Mask Augmented Named Entity Recognition for Extreme Low-Resource Languages</title>
      <author><first>Shashank</first><last>Sonkar</last><affiliation>Rice University</affiliation></author>
      <author><first>Zichao</first><last>Wang</last><affiliation>Rice University</affiliation></author>
      <author><first>Richard</first><last>Baraniuk</last><affiliation>Rice University</affiliation></author>
      <pages>219-226</pages>
      <url hash="c6531a16">2023.sustainlp-1.16</url>
      <bibkey>sonkar-etal-2023-maner</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.16</doi>
      <video href="2023.sustainlp-1.16.mp4"/>
    </paper>
    <paper id="17">
      <title>Efficient and Interpretable Compressive Text Summarisation with Unsupervised Dual-Agent Reinforcement Learning</title>
      <author><first>Peggy</first><last>Tang</last><affiliation>The University of Sydney</affiliation></author>
      <author><first>Junbin</first><last>Gao</last><affiliation>The University of Sydney</affiliation></author>
      <author><first>Lei</first><last>Zhang</last><affiliation>International Digital Economy Academy (IDEA)</affiliation></author>
      <author><first>Zhiyong</first><last>Wang</last><affiliation>The University of Sydney</affiliation></author>
      <pages>227-238</pages>
      <url hash="7550c35c">2023.sustainlp-1.17</url>
      <bibkey>tang-etal-2023-efficient</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.17</doi>
      <video href="2023.sustainlp-1.17.mp4"/>
    </paper>
    <paper id="18">
      <title>Exploring the Effect of Frequency Resolution in <fixed-case>FN</fixed-case>et</title>
      <author><first>Gregory</first><last>Szumel</last><affiliation>Duke University</affiliation></author>
      <author><first>Ghazal</first><last>Khalighinejad</last><affiliation>Duke University</affiliation></author>
      <author><first>Rickard</first><last>Stureborg</last><affiliation>Duke University</affiliation></author>
      <author><first>Sam</first><last>Wiseman</last><affiliation>Duke University</affiliation></author>
      <pages>239-244</pages>
      <url hash="cdd26a24">2023.sustainlp-1.18</url>
      <bibkey>szumel-etal-2023-exploring</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.18</doi>
      <video href="2023.sustainlp-1.18.mp4"/>
    </paper>
    <paper id="19">
      <title>Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory</title>
      <author><first>Aliki</first><last>Anagnostopoulou</last><affiliation>Carl von Ossietzky University of Oldenburg / German Research Center for Artificial Intelligence</affiliation></author>
      <author><first>Mareike</first><last>Hartmann</last><affiliation>Saarland University / German Research Center for Artificial Intelligence</affiliation></author>
      <author><first>Daniel</first><last>Sonntag</last><affiliation>Carl von Ossietzky University of Oldenburg / German Research Center for Artificial Intelligence</affiliation></author>
      <pages>245-256</pages>
      <url hash="43b07614">2023.sustainlp-1.19</url>
      <bibkey>anagnostopoulou-etal-2023-towards</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.19</doi>
      <video href="2023.sustainlp-1.19.mp4"/>
    </paper>
    <paper id="20">
      <title>Corpus Complexity Matters in Pretraining Language Models</title>
      <author><first>Ameeta</first><last>Agrawal</last><affiliation>Portland State University</affiliation></author>
      <author><first>Suresh</first><last>Singh</last><affiliation>Portland State University</affiliation></author>
      <pages>257-263</pages>
      <url hash="96d8d311">2023.sustainlp-1.20</url>
      <bibkey>agrawal-singh-2023-corpus</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>P</fixed-case>ersona<fixed-case>PKT</fixed-case>: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer</title>
      <author><first>Xu</first><last>Han</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Bin</first><last>Guo</last><affiliation>Amazon.com</affiliation></author>
      <author><first>Yoon</first><last>Jung</last><affiliation>Amazon</affiliation></author>
      <author><first>Benjamin</first><last>Yao</last><affiliation>Amazon</affiliation></author>
      <author><first>Yu</first><last>Zhang</last><affiliation>Amazon.com</affiliation></author>
      <author><first>Xiaohu</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Chenlei</first><last>Guo</last><affiliation>Amazon</affiliation></author>
      <pages>264-273</pages>
      <url hash="ad700145">2023.sustainlp-1.21</url>
      <bibkey>han-etal-2023-personapkt</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.21</doi>
      <video href="2023.sustainlp-1.21.mp4"/>
    </paper>
    <paper id="22">
      <title>Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints</title>
      <author><first>Ganesh</first><last>Jawahar</last><affiliation>The University of British Columbia</affiliation></author>
      <author><first>Subhabrata</first><last>Mukherjee</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Debadeepta</first><last>Dey</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-mageed</last><affiliation>The University of British Columbia</affiliation></author>
      <author><first>Laks</first><last>Lakshmanan, V.s.</last><affiliation>UBC</affiliation></author>
      <author><first>Caio</first><last>Mendes</last><affiliation>Microsoft</affiliation></author>
      <author><first>Gustavo</first><last>De Rosa</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Shital</first><last>Shah</last><affiliation>Microsoft Research</affiliation></author>
      <pages>274-289</pages>
      <url hash="54617ff3">2023.sustainlp-1.22</url>
      <bibkey>jawahar-etal-2023-small</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.22</doi>
      <video href="2023.sustainlp-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency</title>
      <author><first>Yuxuan</first><last>Wang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Lyu</first><last>Hong</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>290-298</pages>
      <url hash="0d625866">2023.sustainlp-1.23</url>
      <bibkey>wang-hong-2023-query</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.23</doi>
      <video href="2023.sustainlp-1.23.mp4"/>
    </paper>
    <paper id="24">
      <title>Minimalist Entity Disambiguation for Mid-Resource Languages</title>
      <author><first>Benno</first><last>Kruit</last><affiliation>VU Amsterdam</affiliation></author>
      <pages>299-306</pages>
      <url hash="e013c84f">2023.sustainlp-1.24</url>
      <bibkey>kruit-2023-minimalist</bibkey>
      <doi>10.18653/v1/2023.sustainlp-1.24</doi>
      <video href="2023.sustainlp-1.24.mp4"/>
    </paper>
  </volume>
</collection>
