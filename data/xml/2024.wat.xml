<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.wat">
  <volume id="1" ingest-date="2024-11-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Eleventh Workshop on Asian Translation (WAT 2024)</booktitle>
      <editor><first>Toshiaki</first><last>Nakazawa</last></editor>
      <editor><first>Isao</first><last>Goto</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Miami, Florida, USA</address>
      <month>November</month>
      <year>2024</year>
      <url hash="49a77437">2024.wat-1</url>
      <venue>wat</venue>
      <doi>10.18653/v1/2024.wat-1</doi>
    </meta>
    <frontmatter>
      <url hash="4cf2c49d">2024.wat-1.0</url>
      <bibkey>wat-2024-1</bibkey>
      <doi>10.18653/v1/2024.wat-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Creative and Context-Aware Translation of <fixed-case>E</fixed-case>ast <fixed-case>A</fixed-case>sian Idioms with <fixed-case>GPT</fixed-case>-4</title>
      <author><first>Kenan</first><last>Tang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Peiyang</first><last>Song</last><affiliation>California Institute of Technology</affiliation></author>
      <author><first>Yao</first><last>Qin</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Xifeng</first><last>Yan</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>1-21</pages>
      <abstract>As a type of figurative language, an East Asian idiom condenses rich cultural background into only a few characters. Translating such idioms is challenging for human translators, who often resort to choosing a context-aware translation from an existing list of candidates. However, compiling a dictionary of candidate translations demands much time and creativity even for expert translators. To alleviate such burden, we evaluate if GPT-4 can help generate high-quality translations. Based on automatic evaluations of faithfulness and creativity, we first identify Pareto-optimal prompting strategies that can outperform translation engines from Google and DeepL. Then, at a low cost, our context-aware translations can achieve far more high-quality translations per idiom than the human baseline. We open-source all code and data to facilitate further research.</abstract>
      <url hash="8f53f1df">2024.wat-1.1</url>
      <attachment type="SupplementaryMaterial" hash="99d2b4fd">2024.wat-1.1.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="9c5fcac3">2024.wat-1.1.SupplementaryMaterial.zip</attachment>
      <bibkey>tang-etal-2024-creative-context</bibkey>
      <doi>10.18653/v1/2024.wat-1.1</doi>
    </paper>
    <paper id="2">
      <title>An Empirical Study of Multilingual Vocabulary for Neural Machine Translation Models</title>
      <author><first>Kenji</first><last>Imamura</last><affiliation>National Institute of Information and Communications Technology</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>NICT</affiliation></author>
      <pages>22-35</pages>
      <abstract>In this paper, we discuss multilingual vocabulary for neural machine translation models. Multilingual vocabularies should generate highly accurate machine translations regardless of the languages, and have preferences so that tokenized strings contain rare out-of-vocabulary (OOV) tokens and token sequences are short. In this paper, we discuss the characteristics of various multilingual vocabularies via tokenization and translation experiments. We also present our recommended vocabulary and tokenizer.</abstract>
      <url hash="72a0a1ed">2024.wat-1.2</url>
      <attachment type="SupplementaryMaterial" hash="6e209c03">2024.wat-1.2.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="bec11c1e">2024.wat-1.2.SupplementaryMaterial.txt</attachment>
      <bibkey>imamura-utiyama-2024-empirical</bibkey>
      <doi>10.18653/v1/2024.wat-1.2</doi>
    </paper>
    <paper id="3">
      <title>Machine Translation Of <fixed-case>M</fixed-case>arathi Dialects: A Case Study Of Kadodi</title>
      <author><first>Raj</first><last>Dabre</last><affiliation>NICT</affiliation></author>
      <author><first>Mary</first><last>Dabre</last><affiliation>Independent</affiliation></author>
      <author><first>Teresa</first><last>Pereira</last><affiliation>St. Gonsalo Garcia College, Vasai</affiliation></author>
      <pages>36-44</pages>
      <abstract>While Marathi is considered as a low- to middle-resource language, its 42 dialects have mostly been ignored, mainly because these dialects are mostly spoken and rarely written, making them extremely low-resource. In this paper we explore the machine translation (MT) of Kadodi, also known as Samvedi, which is a dialect of Marathi. We first discuss the Kadodi dialect, highlighting the differences from the standard dialect, followed by presenting a manually curated dataset called Suman consisting of a trilingual Kadodi-Marathi-English dictionary of 949 entries and 942 simple sentence triples and idioms created by native Kadodi speakers. We then evaluate 3 existing large language models (LLMs) supporting Marathi, namely Gemma-2-9b, Sarvam-2b-0.5 and LLaMa-3.1-8b, in few-shot prompting style to determine their efficacy for translation involving Kadodi. We observe that these models exhibit rather lackluster performance in handling Kadodi even for simple sentences, indicating a dire situation.</abstract>
      <url hash="aaa157de">2024.wat-1.3</url>
      <attachment type="SupplementaryMaterial" hash="05613c0b">2024.wat-1.3.SupplementaryMaterial.txt</attachment>
      <bibkey>dabre-etal-2024-machine</bibkey>
      <doi>10.18653/v1/2024.wat-1.3</doi>
    </paper>
    <paper id="4">
      <title>Are Large Language Models State-of-the-art Quality Estimators for Machine Translation of User-generated Content?</title>
      <author><first>Shenbin</first><last>Qian</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Constantin</first><last>Orasan</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Diptesh</first><last>Kanojia</last><affiliation>University of Surrey</affiliation></author>
      <author><first>FÃ©lix</first><last>Do Carmo</last><affiliation>University of Surrey</affiliation></author>
      <pages>45-55</pages>
      <abstract>This paper investigates whether large language models (LLMs) are state-of-the-art quality estimators for machine translation of user-generated content (UGC) that contains emotional expressions, without the use of reference translations. To achieve this, we employ an existing emotion-related dataset with human-annotated errors and calculate quality evaluation scores based on the Multi-dimensional Quality Metrics. We compare the accuracy of several LLMs with that of our fine-tuned baseline models, under in-context learning and parameter-efficient fine-tuning (PEFT) scenarios. We find that PEFT of LLMs leads to better performance in score prediction with human interpretable explanations than fine-tuned models. However, a manual analysis of LLM outputs reveals that they still have problems such as refusal to reply to a prompt and unstable output while evaluating machine translation of UGC.</abstract>
      <url hash="a462c08f">2024.wat-1.4</url>
      <attachment type="SupplementaryMaterial" hash="7c8a38a8">2024.wat-1.4.SupplementaryMaterial.txt</attachment>
      <bibkey>qian-etal-2024-large-language</bibkey>
      <doi>10.18653/v1/2024.wat-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>AI</fixed-case>-Tutor: Interactive Learning of Ancient Knowledge from Low-Resource Languages</title>
      <author><first>Siddhartha</first><last>Dalal</last><affiliation>Columbia University</affiliation></author>
      <author><first>Rahul</first><last>Aditya</last><affiliation>Columbia University</affiliation></author>
      <author><first>Vethavikashini</first><last>Chithrra Raghuram</last><affiliation>Columbia University</affiliation></author>
      <author><first>Prahlad</first><last>Koratamaddi</last><affiliation>Columbia University</affiliation></author>
      <pages>56-66</pages>
      <abstract>Many low-resource languages, such as Prakrit, present significant linguistic complexities and have limited modern-day resources. These languages often have multiple derivatives; for example, Prakrit, a language in use by masses around 2500 years ago for 500 years, includes Pali and Gandhari, which encompass a vast body of Buddhist literature, as well as Ardhamagadhi, rich in Jain literature. Despite these challenges, these languages are invaluable for their historical, religious, and cultural insights needed by non-language experts and others.To explore and understand the deep knowledge within these ancient texts for non-language experts, we propose a novel approach: translating multiple dialects of the parent language into a contemporary language and then enabling them to interact with the system in their native language, including English, Hindi, French and German, through a question-and-answer interface built on Large Language Models. We demonstrate the effectiveness of this novel AI-Tutor system by focusing on Ardhamagadhi and Pali.</abstract>
      <url hash="8582cee2">2024.wat-1.5</url>
      <attachment type="SupplementaryMaterial" hash="a4df5788">2024.wat-1.5.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="3efa0b58">2024.wat-1.5.SupplementaryMaterial.zip</attachment>
      <bibkey>dalal-etal-2024-ai</bibkey>
      <doi>10.18653/v1/2024.wat-1.5</doi>
    </paper>
  </volume>
</collection>
