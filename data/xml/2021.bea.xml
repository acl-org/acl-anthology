<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.bea">
  <volume id="1" ingest-date="2021-04-19" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications</booktitle>
      <editor><first>Jill</first><last>Burstein</last></editor>
      <editor><first>Andrea</first><last>Horbach</last></editor>
      <editor><first>Ekaterina</first><last>Kochmar</last></editor>
      <editor><first>Ronja</first><last>Laarmann-Quante</last></editor>
      <editor><first>Claudia</first><last>Leacock</last></editor>
      <editor><first>Nitin</first><last>Madnani</last></editor>
      <editor><first>Ildikó</first><last>Pilán</last></editor>
      <editor><first>Helen</first><last>Yannakoudakis</last></editor>
      <editor><first>Torsten</first><last>Zesch</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>April</month>
      <year>2021</year>
      <venue>bea</venue>
    </meta>
    <frontmatter>
      <url hash="8a6b2c7d">2021.bea-1.0</url>
      <bibkey>bea-2021-innovative</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Negation Scope Resolution for <fixed-case>C</fixed-case>hinese as a Second Language</title>
      <author><first>Mengyu</first><last>Zhang</last></author>
      <author><first>Weiqi</first><last>Wang</last></author>
      <author><first>Shuqiao</first><last>Sun</last></author>
      <author><first>Weiwei</first><last>Sun</last></author>
      <pages>1–10</pages>
      <abstract>This paper studies Negation Scope Resolution (NSR) for Chinese as a Second Language (CSL), which shows many unique characteristics that distinguish itself from “standard” Chinese. We annotate a new moderate-sized corpus that covers two background L1 languages, viz. English and Japanese. We build a neural NSR system, which achieves a new state-of-the-art accuracy on English benchmark data. We leverage this system to gauge how successful NSR for CSL can be. Different native language backgrounds of language learners result in unequal cross-lingual transfer, which has a significant impact on processing second language data. In particular, manual annotation, empirical evaluation and error analysis indicate two non-obvious facts: 1) L2-Chinese, L1-Japanese data are more difficult to analyze and thus annotate than L2-Chinese, L1-English data; 2) computational models trained on L2-Chinese, L1-Japanese data perform better than models trained on L2-Chinese, L1-English data.</abstract>
      <url hash="f3f9c66d">2021.bea-1.1</url>
      <bibkey>zhang-etal-2021-negation</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>T</fixed-case>ext <fixed-case>S</fixed-case>implification by <fixed-case>T</fixed-case>agging</title>
      <author><first>Kostiantyn</first><last>Omelianchuk</last></author>
      <author><first>Vipul</first><last>Raheja</last></author>
      <author><first>Oleksandr</first><last>Skurzhanskyi</last></author>
      <pages>11–25</pages>
      <abstract>Edit-based approaches have recently shown promising results on multiple monolingual sequence transduction tasks. In contrast to conventional sequence-to-sequence (Seq2Seq) models, which learn to generate text from scratch as they are trained on parallel corpora, these methods have proven to be much more effective since they are able to learn to make fast and accurate transformations while leveraging powerful pre-trained language models. Inspired by these ideas, we present TST, a simple and efficient Text Simplification system based on sequence Tagging, leveraging pre-trained Transformer-based encoders. Our system makes simplistic data augmentations and tweaks in training and inference on a pre-existing system, which makes it less reliant on large amounts of parallel training data, provides more control over the outputs and enables faster inference speeds. Our best model achieves near state-of-the-art performance on benchmark test datasets for the task. Since it is fully non-autoregressive, it achieves faster inference speeds by over 11 times than the current state-of-the-art text simplification system.</abstract>
      <url hash="065ea94e">2021.bea-1.2</url>
      <bibkey>omelianchuk-etal-2021-text</bibkey>
      <pwccode url="https://github.com/grammarly/gector" additional="false">grammarly/gector</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/asset">ASSET</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/turkcorpus">TurkCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilarge">WikiLarge</pwcdataset>
    </paper>
    <paper id="3">
      <title>Employing distributional semantics to organize task-focused vocabulary learning</title>
      <author><first>Haemanth</first><last>Santhi Ponnusamy</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <pages>26–36</pages>
      <abstract>How can a learner systematically prepare for reading a book they are interested in? In this paper, we explore how computational linguistic methods such as distributional semantics, morphological clustering, and exercise generation can be combined with graph-based learner models to answer this question both conceptually and in practice. Based on highly structured learner models and concepts from network analysis, the learner is guided to efficiently explore the targeted lexical space. They practice using multi-gap learning activities generated from the book. In sum, the approach combines computational linguistic methods with concepts from network analysis and tutoring systems to support learners in pursuing their individual reading task goals.</abstract>
      <url hash="0d9f4c4d">2021.bea-1.3</url>
      <bibkey>santhi-ponnusamy-meurers-2021-employing</bibkey>
    </paper>
    <paper id="4">
      <title>Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models</title>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Shankar</first><last>Kumar</last></author>
      <pages>37–47</pages>
      <abstract>Synthetic data generation is widely known to boost the accuracy of neural grammatical error correction (GEC) systems, but existing methods often lack diversity or are too simplistic to generate the broad range of grammatical errors made by human writers. In this work, we use error type tags from automatic annotation tools such as ERRANT to guide synthetic data generation. We compare several models that can produce an ungrammatical sentence given a clean sentence and an error type tag. We use these models to build a new, large synthetic pre-training data set with error tag frequency distributions matching a given development set. Our synthetic data set yields large and consistent gains, improving the state-of-the-art on the BEA-19 and CoNLL-14 test sets. We also show that our approach is particularly effective in adapting a GEC system, trained on mixed native and non-native English, to a native English test set, even surpassing real training data consisting of high-quality sentence pairs.</abstract>
      <url hash="b0dcb98e">2021.bea-1.4</url>
      <attachment type="OptionalSupplementaryMaterial" hash="64dee257">2021.bea-1.4.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>stahlberg-kumar-2021-synthetic</bibkey>
      <pwccode url="https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction" additional="false">google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
    </paper>
    <paper id="5">
      <title>Broad Linguistic Complexity Analysis for <fixed-case>G</fixed-case>reek Readability Classification</title>
      <author><first>Savvas</first><last>Chatzipanagiotidis</last></author>
      <author><first>Maria</first><last>Giagkou</last></author>
      <author><first>Detmar</first><last>Meurers</last></author>
      <pages>48–58</pages>
      <abstract>This paper explores the linguistic complexity of Greek textbooks as a readability classification task. We analyze textbook corpora for different school subjects and textbooks for Greek as a Second Language, covering a very wide spectrum of school age groups and proficiency levels. A broad range of quantifiable linguistic complexity features (lexical, morphological and syntactic) are extracted and calculated. Conducting experiments with different feature subsets, we show that the different linguistic dimensions contribute orthogonal information, each contributing towards the highest result achieved using all linguistic feature subsets. A readability classifier trained on this basis reaches a classification accuracy of 88.16% for the Greek as a Second Language corpus. To investigate the generalizability of the classification models, we also perform cross-corpus evaluations. We show that the model trained on the most varied text collection (for Greek as a school subject) generalizes best. In addition to advancing the state of the art for Greek readability analysis, the paper also contributes insights on the role of different feature sets and training setups for generalizable readability classification.</abstract>
      <url hash="8904c1b6">2021.bea-1.5</url>
      <bibkey>chatzipanagiotidis-etal-2021-broad</bibkey>
    </paper>
    <paper id="6">
      <title>Character Set Construction for <fixed-case>C</fixed-case>hinese Language Learning</title>
      <author><first>Chak Yan</first><last>Yeung</last></author>
      <author><first>John</first><last>Lee</last></author>
      <pages>59–63</pages>
      <abstract>To promote efficient learning of Chinese characters, pedagogical materials may present not only a single character, but a set of characters that are related in meaning and in written form. This paper investigates automatic construction of these character sets. The proposed model represents a character as averaged word vectors of common words containing the character. It then identifies sets of characters with high semantic similarity through clustering. Human evaluation shows that this representation outperforms direct use of character embeddings, and that the resulting character sets capture distinct semantic ranges.</abstract>
      <url hash="521b195f">2021.bea-1.6</url>
      <bibkey>yeung-lee-2021-character</bibkey>
    </paper>
    <paper id="7">
      <title>Identifying negative language transfer in learner errors using <fixed-case>POS</fixed-case> information</title>
      <author><first>Leticia</first><last>Farias Wanderley</last></author>
      <author><first>Carrie</first><last>Demmans Epp</last></author>
      <pages>64–74</pages>
      <abstract>A common mistake made by language learners is the misguided usage of first language rules when communicating in another language. In this paper, n-gram and recurrent neural network language models are used to represent language structures and detect when Chinese native speakers incorrectly transfer rules from their first language (i.e., Chinese) into their English writing. These models make it possible to inform corrective error feedback with error causes, such as negative language transfer. We report the results of our negative language detection experiments with n-gram and recurrent neural network models that were trained using part-of-speech tags. The best performing model achieves an F1-score of 0.51 when tasked with recognizing negative language transfer in English learner data.</abstract>
      <url hash="aa33ce5b">2021.bea-1.7</url>
      <bibkey>farias-wanderley-demmans-epp-2021-identifying</bibkey>
      <pwccode url="https://github.com/EdTeKLA/LanguageTransfer" additional="false">EdTeKLA/LanguageTransfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="8">
      <title>Document-level grammatical error correction</title>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Christopher</first><last>Bryant</last></author>
      <pages>75–84</pages>
      <abstract>Document-level context can provide valuable information in grammatical error correction (GEC), which is crucial for correcting certain errors and resolving inconsistencies. In this paper, we investigate context-aware approaches and propose document-level GEC systems. Additionally, we employ a three-step training strategy to benefit from both sentence-level and document-level data. Our system outperforms previous document-level and all other NMT-based single-model systems, achieving state of the art on a common test set.</abstract>
      <url hash="d6f9aa9b">2021.bea-1.8</url>
      <bibkey>yuan-bryant-2021-document</bibkey>
      <pwccode url="https://github.com/chrisjbryant/doc-gec" additional="false">chrisjbryant/doc-gec</pwccode>
    </paper>
    <paper id="9">
      <title>Essay Quality Signals as Weak Supervision for Source-based Essay Scoring</title>
      <author><first>Haoran</first><last>Zhang</last></author>
      <author><first>Diane</first><last>Litman</last></author>
      <pages>85–96</pages>
      <abstract>Human essay grading is a laborious task that can consume much time and effort. Automated Essay Scoring (AES) has thus been proposed as a fast and effective solution to the problem of grading student writing at scale. However, because AES typically uses supervised machine learning, a human-graded essay corpus is still required to train the AES model. Unfortunately, such a graded corpus often does not exist, so creating a corpus for machine learning can also be a laborious task. This paper presents an investigation of replacing the use of human-labeled essay grades when training an AES system with two automatically available but weaker signals of essay quality: word count and topic distribution similarity. Experiments using two source-based essay scoring (evidence score) corpora show that while weak supervision does not yield a competitive result when training a neural source-based AES model, it can be used to successfully extract Topical Components (TCs) from a source text, which are required by a supervised feature-based AES model. In particular, results show that feature-based AES performance is comparable with either automatically or manually constructed TCs.</abstract>
      <url hash="bf57a2fe">2021.bea-1.9</url>
      <bibkey>zhang-litman-2021-essay</bibkey>
    </paper>
    <paper id="10">
      <title>Parsing Argumentative Structure in <fixed-case>E</fixed-case>nglish-as-Foreign-Language Essays</title>
      <author><first>Jan Wira Gotama</first><last>Putra</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <author><first>Takenobu</first><last>Tokunaga</last></author>
      <pages>97–109</pages>
      <abstract>This paper presents a study on parsing the argumentative structure in English-as-foreign-language (EFL) essays, which are inherently noisy. The parsing process consists of two steps, linking related sentences and then labelling their relations. We experiment with several deep learning architectures to address each task independently. In the sentence linking task, a biaffine model performed the best. In the relation labelling task, a fine-tuned BERT model performed the best. Two sentence encoders are employed, and we observed that non-fine-tuning models generally performed better when using Sentence-BERT as opposed to BERT encoder. We trained our models using two types of parallel texts: original noisy EFL essays and those improved by annotators, then evaluate them on the original essays. The experiment shows that an end-to-end in-domain system achieved an accuracy of .341. On the other hand, the cross-domain system achieved 94% performance of the in-domain system. This signals that well-written texts can also be useful to train argument mining system for noisy texts.</abstract>
      <url hash="6ccc28c6">2021.bea-1.10</url>
      <bibkey>putra-etal-2021-parsing</bibkey>
      <pwccode url="https://github.com/wiragotama/bea2021" additional="false">wiragotama/bea2021</pwccode>
    </paper>
    <paper id="11">
      <title>Training and Domain Adaptation for Supervised Text Segmentation</title>
      <author><first>Goran</first><last>Glavaš</last></author>
      <author><first>Ananya</first><last>Ganesh</last></author>
      <author><first>Swapna</first><last>Somasundaran</last></author>
      <pages>110–116</pages>
      <abstract>Unlike traditional unsupervised text segmentation methods, recent supervised segmentation models rely on Wikipedia as the source of large-scale segmentation supervision. These models have, however, predominantly been evaluated on the in-domain (Wikipedia-based) test sets, preventing conclusions about their general segmentation efficacy. In this work, we focus on the domain transfer performance of supervised neural text segmentation in the educational domain. To this end, we first introduce K12Seg, a new dataset for evaluation of supervised segmentation, created from educational reading material for grade-1 to college-level students. We then benchmark a hierarchical text segmentation model (HITS), based on RoBERTa, in both in-domain and domain-transfer segmentation experiments. While HITS produces state-of-the-art in-domain performance (on three Wikipedia-based test sets), we show that, subject to the standard full-blown fine-tuning, it is susceptible to domain overfitting. We identify adapter-based fine-tuning as a remedy that substantially improves transfer performance.</abstract>
      <url hash="c3e4e112">2021.bea-1.11</url>
      <bibkey>glavas-etal-2021-training</bibkey>
    </paper>
    <paper id="12">
      <title>Data Strategies for Low-Resource Grammatical Error Correction</title>
      <author><first>Simon</first><last>Flachs</last></author>
      <author><first>Felix</first><last>Stahlberg</last></author>
      <author><first>Shankar</first><last>Kumar</last></author>
      <pages>117–122</pages>
      <abstract>Grammatical Error Correction (GEC) is a task that has been extensively investigated for the English language. However, for low-resource languages the best practices for training GEC systems have not yet been systematically determined. We investigate how best to take advantage of existing data sources for improving GEC systems for languages with limited quantities of high quality training data. We show that methods for generating artificial training data for GEC can benefit from including morphological errors. We also demonstrate that noisy error correction data gathered from Wikipedia revision histories and the language learning website Lang8, are valuable data sources. Finally, we show that GEC systems pre-trained on noisy data sources can be fine-tuned effectively using small amounts of high quality, human-annotated data.</abstract>
      <url hash="4b2d5709">2021.bea-1.12</url>
      <bibkey>flachs-etal-2021-data</bibkey>
    </paper>
    <paper id="13">
      <title>Towards a Data Analytics Pipeline for the Visualisation of Complexity Metrics in <fixed-case>L</fixed-case>2 writings</title>
      <author><first>Thomas</first><last>Gaillat</last></author>
      <author><first>Anas</first><last>Knefati</last></author>
      <author><first>Antoine</first><last>Lafontaine</last></author>
      <pages>123–129</pages>
      <abstract>We present the design of a tool for the visualisation of linguistic complexity in second language (L2) learner writings. We show how metrics can be exploited to visualise complexity in L2 writings in relation to CEFR levels.</abstract>
      <url hash="27718c41">2021.bea-1.13</url>
      <bibkey>gaillat-etal-2021-towards</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>E</fixed-case>stonian as a Second Language Teacher’s Tools</title>
      <author><first>Tiiu</first><last>Üksik</last></author>
      <author><first>Jelena</first><last>Kallas</last></author>
      <author><first>Kristina</first><last>Koppel</last></author>
      <author><first>Katrin</first><last>Tsepelina</last></author>
      <author><first>Raili</first><last>Pool</last></author>
      <pages>130–134</pages>
      <abstract>The paper presents the results of the project “Teacher’s Tools” (et Õpetaja tööriistad) published as a subpage of the new language portal Sõnaveeb developed by the Institute of the Estonian Language. The toolbox includes four modules: vocabulary, grammar, communicative language activities and text evaluation. The tools are aimed to help teachers and specialists of Estonian as a second language plan courses, create new educational materials, exercises and tests based on CEFR level descriptions.</abstract>
      <url hash="822d445a">2021.bea-1.14</url>
      <bibkey>uksik-etal-2021-estonian</bibkey>
    </paper>
    <paper id="15">
      <title>Assessing Grammatical Correctness in Language Learning</title>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <author><first>Roman</first><last>Yangarber</last></author>
      <pages>135–146</pages>
      <abstract>We present experiments on assessing the grammatical correctness of learners’ answers in a language-learning System (references to the System, and the links to the released data and code are withheld for anonymity). In particular, we explore the problem of detecting alternative-correct answers: when more than one inflected form of a lemma fits syntactically and semantically in a given context. We approach the problem with the methods for grammatical error detection (GED), since we hypothesize that models for detecting grammatical mistakes can assess the correctness of potential alternative answers in a learning setting. Due to the paucity of training data, we explore the ability of pre-trained BERT to detect grammatical errors and then fine-tune it using synthetic training data. In this work, we focus on errors in inflection. Our experiments show a. that pre-trained BERT performs worse at detecting grammatical irregularities for Russian than for English; b. that fine-tuned BERT yields promising results on assessing the correctness of grammatical exercises; and c. establish a new benchmark for Russian. To further investigate its performance, we compare fine-tuned BERT with one of the state-of-the-art models for GED (Bell et al., 2019) on our dataset and RULEC-GEC (Rozovskaya and Roth, 2019). We release the manually annotated learner dataset, used for testing, for general use.</abstract>
      <url hash="a92b7b43">2021.bea-1.15</url>
      <bibkey>katinskaia-yangarber-2021-assessing</bibkey>
    </paper>
    <paper id="16">
      <title>On the application of Transformers for estimating the difficulty of Multiple-Choice Questions from text</title>
      <author><first>Luca</first><last>Benedetto</last></author>
      <author><first>Giovanni</first><last>Aradelli</last></author>
      <author><first>Paolo</first><last>Cremonesi</last></author>
      <author><first>Andrea</first><last>Cappelli</last></author>
      <author><first>Andrea</first><last>Giussani</last></author>
      <author><first>Roberto</first><last>Turrin</last></author>
      <pages>147–157</pages>
      <abstract>Classical approaches to question calibration are either subjective or require newly created questions to be deployed before being calibrated. Recent works explored the possibility of estimating question difficulty from text, but did not experiment with the most recent NLP models, in particular Transformers. In this paper, we compare the performance of previous literature with Transformer models experimenting on a public and a private dataset. Our experimental results show that Transformers are capable of outperforming previously proposed models. Moreover, if an additional corpus of related documents is available, Transformers can leverage that information to further improve calibration accuracy. We characterize the dependence of the model performance on some properties of the questions, showing that it performs best on questions ending with a question mark and Multiple-Choice Questions (MCQs) with one correct choice.</abstract>
      <url hash="8c2c03a0">2021.bea-1.16</url>
      <bibkey>benedetto-etal-2021-application</bibkey>
    </paper>
    <paper id="17">
      <title>Automatically Generating Cause-and-Effect Questions from Passages</title>
      <author><first>Katherine</first><last>Stasaski</last></author>
      <author><first>Manav</first><last>Rathod</last></author>
      <author><first>Tony</first><last>Tu</last></author>
      <author><first>Yunfang</first><last>Xiao</last></author>
      <author><first>Marti A.</first><last>Hearst</last></author>
      <pages>158–170</pages>
      <abstract>Automated question generation has the potential to greatly aid in education applications, such as online study aids to check understanding of readings. The state-of-the-art in neural question generation has advanced greatly, due in part to the availability of large datasets of question-answer pairs. However, the questions generated are often surface-level and not challenging for a human to answer. To develop more challenging questions, we propose the novel task of cause-and-effect question generation. We build a pipeline that extracts causal relations from passages of input text, and feeds these as input to a state-of-the-art neural question generator. The extractor is based on prior work that classifies causal relations by linguistic category (Cao et al., 2016; Altenberg, 1984). This work results in a new, publicly available collection of cause-and-effect questions. We evaluate via both automatic and manual metrics and find performance improves for both question generation and question answering when we utilize a small auxiliary data source of cause-and-effect questions for fine-tuning. Our approach can be easily applied to generate cause-and-effect questions from other text collections and educational material, allowing for adaptable large-scale generation of cause-and-effect questions.</abstract>
      <url hash="4521e16a">2021.bea-1.17</url>
      <attachment type="OptionalSupplementaryMaterial" hash="32bcc5d1">2021.bea-1.17.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>stasaski-etal-2021-automatically</bibkey>
      <pwccode url="https://github.com/kstats/causalqg" additional="false">kstats/causalqg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tqa">TQA</pwcdataset>
    </paper>
    <paper id="18">
      <title>Interventions Recommendation: Professionals’ Observations Analysis in Special Needs Education</title>
      <author><first>Javier</first><last>Muñoz</last></author>
      <author><first>Felipe</first><last>Bravo-Marquez</last></author>
      <pages>171–179</pages>
      <abstract>We present a new task in educational NLP, recommend the best interventions to help special needs education professionals to work with students with different disabilities. We use the professionals’ observations of the students together with the students diagnosis and other chosen interventions to predict the best interventions for Chilean special needs students.</abstract>
      <url hash="072ea13c">2021.bea-1.18</url>
      <bibkey>munoz-bravo-marquez-2021-interventions</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>C</fixed-case>-Test Collector: A Proficiency Testing Application to Collect Training Data for <fixed-case>C</fixed-case>-Tests</title>
      <author><first>Christian</first><last>Haring</last></author>
      <author><first>Rene</first><last>Lehmann</last></author>
      <author><first>Andrea</first><last>Horbach</last></author>
      <author><first>Torsten</first><last>Zesch</last></author>
      <pages>180–184</pages>
      <abstract>We present the C-Test Collector, a web-based tool that allows language learners to test their proficiency level using c-tests. Our tool collects anonymized data on test performance, which allows teachers to gain insights into common error patterns. At the same time, it allows NLP researchers to collect training data for being able to generate c-test variants at the desired difficulty level.</abstract>
      <url hash="2f718d2d">2021.bea-1.19</url>
      <bibkey>haring-etal-2021-c</bibkey>
    </paper>
    <paper id="20">
      <title>Virtual Pre-Service Teacher Assessment and Feedback via Conversational Agents</title>
      <author><first>Debajyoti</first><last>Datta</last></author>
      <author><first>Maria</first><last>Phillips</last></author>
      <author><first>James P.</first><last>Bywater</last></author>
      <author><first>Jennifer</first><last>Chiu</last></author>
      <author><first>Ginger S.</first><last>Watson</last></author>
      <author><first>Laura</first><last>Barnes</last></author>
      <author><first>Donald</first><last>Brown</last></author>
      <pages>185–198</pages>
      <abstract>Conversational agents and assistants have been used for decades to facilitate learning. There are many examples of conversational agents used for educational and training purposes in K-12, higher education, healthcare, the military, and private industry settings. The most common forms of conversational agents in education are teaching agents that directly teach and support learning, peer agents that serve as knowledgeable learning companions to guide learners in the learning process, and teachable agents that function as a novice or less-knowledgeable student trained and taught by a learner who learns by teaching. The Instructional Quality Assessment (IQA) provides a robust framework to evaluate reading comprehension and mathematics instruction. We developed a system for pre-service teachers, individuals in a teacher preparation program, to evaluate teaching instruction quality based on a modified interpretation of IQA metrics. Our demonstration and approach take advantage of recent advances in Natural Language Processing (NLP) and deep learning for each dialogue system component. We built an open-source conversational agent system to engage pre-service teachers in a specific mathematical scenario focused on scale factor with the aim to provide feedback on pre-service teachers’ questioning strategies. We believe our system is not only practical for teacher education programs but can also enable other researchers to build new educational scenarios with minimal effort.</abstract>
      <url hash="a9375e1d">2021.bea-1.20</url>
      <bibkey>datta-etal-2021-virtual</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="21">
      <title>Automated Classification of Written Proficiency Levels on the <fixed-case>CEFR</fixed-case>-Scale through Complexity Contours and <fixed-case>RNN</fixed-case>s</title>
      <author><first>Elma</first><last>Kerz</last></author>
      <author><first>Daniel</first><last>Wiechmann</last></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Emma</first><last>Tseng</last></author>
      <author><first>Marcus</first><last>Ströbel</last></author>
      <pages>199–209</pages>
      <abstract>Automatically predicting the level of second language (L2) learner proficiency is an emerging topic of interest and research based on machine learning approaches to language learning and development. The key to the present paper is the combined use of what we refer to as ‘complexity contours’, a series of measurements of indices of L2 proficiency obtained by a computational tool that implements a sliding window technique, and recurrent neural network (RNN) classifiers that adequately capture the sequential information in those contours. We used the EF-Cambridge Open Language Database (Geertzen et al. 2013) with its labelled Common European Framework of Reference (CEFR) levels (Council of Europe 2018) to predict six classes of L2 proficiency levels (A1, A2, B1, B2, C1, C2) in the assessment of writing skills. Our experiments demonstrate that an RNN classifier trained on complexity contours achieves higher classification accuracy than one trained on text-average complexity scores. In a secondary experiment, we determined the relative importance of features from four distinct categories through a sensitivity-based pruning technique. Our approach makes an important contribution to the field of automated identification of language proficiency levels, more specifically, to the increasing efforts towards the empirical validation of CEFR levels.</abstract>
      <url hash="df88c3d0">2021.bea-1.21</url>
      <attachment type="OptionalSupplementaryMaterial" hash="bd598e6f">2021.bea-1.21.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>kerz-etal-2021-automated</bibkey>
    </paper>
    <paper id="22">
      <title>“Sharks are not the threat humans are”: Argument Component Segmentation in School Student Essays</title>
      <author><first>Tariq</first><last>Alhindi</last></author>
      <author><first>Debanjan</first><last>Ghosh</last></author>
      <pages>210–222</pages>
      <abstract>Argument mining is often addressed by a pipeline method where segmentation of text into argumentative units is conducted first and proceeded by an argument component identification task. In this research, we apply a token-level classification to identify claim and premise tokens from a new corpus of argumentative essays written by middle school students. To this end, we compare a variety of state-of-the-art models such as discrete features and deep learning architectures (e.g., BiLSTM networks and BERT-based architectures) to identify the argument components. We demonstrate that a BERT-based multi-task learning architecture (i.e., token and sentence level classification) adaptively pretrained on a relevant unlabeled dataset obtains the best results.</abstract>
      <url hash="825b3481">2021.bea-1.22</url>
      <bibkey>alhindi-ghosh-2021-sharks</bibkey>
    </paper>
    <paper id="23">
      <title>Using Linguistic Features to Predict the Response Process Complexity Associated with Answering Clinical <fixed-case>MCQ</fixed-case>s</title>
      <author><first>Victoria</first><last>Yaneva</last></author>
      <author><first>Daniel</first><last>Jurich</last></author>
      <author><first>Le An</first><last>Ha</last></author>
      <author><first>Peter</first><last>Baldwin</last></author>
      <pages>223–232</pages>
      <abstract>This study examines the relationship between the linguistic characteristics of a test item and the complexity of the response process required to answer it correctly. Using data from a large-scale medical licensing exam, clustering methods identified items that were similar with respect to their relative difficulty and relative response-time intensiveness to create low response process complexity and high response process complexity item classes. Interpretable models were used to investigate the linguistic features that best differentiated between these classes from a descriptive and predictive framework. Results suggest that nuanced features such as the number of ambiguous medical terms help explain response process complexity beyond superficial item characteristics such as word count. Yet, although linguistic features carry signal relevant to response process complexity, the classification of individual items remains challenging.</abstract>
      <url hash="5672ec2a">2021.bea-1.23</url>
      <bibkey>yaneva-etal-2021-using</bibkey>
    </paper>
  </volume>
</collection>
