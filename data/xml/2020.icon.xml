<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.icon">
  <volume id="main" ingest-date="2021-06-27">
    <meta>
      <booktitle>Proceedings of the 17th International Conference on Natural Language Processing (ICON)</booktitle>
      <editor><first>Pushpak</first><last>Bhattacharyya</last></editor>
      <editor><first>Dipti Misra</first><last>Sharma</last></editor>
      <editor><first>Rajeev</first><last>Sangal</last></editor>
      <publisher>NLP Association of India (NLPAI)</publisher>
      <address>Indian Institute of Technology Patna, Patna, India</address>
      <month>December</month>
      <year>2020</year>
      <url hash="35c7bd87">2020.icon-main</url>
      <venue>icon</venue>
    </meta>
    <frontmatter>
      <url hash="3d4ad274">2020.icon-main.0</url>
      <bibkey>icon-2020-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>The <fixed-case>WEAVE</fixed-case> Corpus: Annotating Synthetic Chemical Procedures in Patents with Chemical Named Entities</title>
      <author><first>Ravindra</first><last>Nittala</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>1–9</pages>
      <abstract>The Modern pharmaceutical industry depends on the iterative design of novel synthetic routes for drugs while not infringing on existing intellectual property rights. Such a design process calls for analyzing many existing synthetic chemical reactions and planning the synthesis of novel chemicals. These procedures have been historically available in unstructured raw text form in publications and patents. To facilitate automated synthetic chemical reactions analysis and design the novel synthetic reactions using Natural Language Processing (NLP) methods, we introduce a Named Entity Recognition (NER) dataset of the Examples section in 180 full-text patent documents with 5188 synthetic procedures annotated by domain experts. All the chemical entities which are part of the synthetic discourse were annotated with suitable class labels. We present the second-largest chemical NER corpus with 100,129 annotations and the highest IAA value of 98.73% (F-measure) on a 45 document subset. We discuss this new resource in detail and highlight some specific challenges in annotating synthetic chemical procedures with chemical named entities. We make the corpus available to the community to promote further research and development of downstream NLP systems applications. We also provide baseline results for the NER model to the community to improve on.</abstract>
      <url hash="b38b0528">2020.icon-main.1</url>
      <bibkey>nittala-shrivastava-2020-weave</bibkey>
      <pwccode url="https://github.com/nv-ravindra/the-weave-corpus" additional="false">nv-ravindra/the-weave-corpus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="2">
      <title>Increasing accuracy of a semantic word labelling tool based on a small lexicon</title>
      <author><first>Hugo</first><last>Sanjurjo-González</last></author>
      <pages>10–14</pages>
      <abstract>Semantic annotation has become an important piece of information within corpus linguistics. This information is usually included for every lexical unit of the corpus providing a more exhaustive analysis of language. There are some resources such as lexicons or ontologies that allow this type of annotation. However, expanding these resources is a time-consuming task. This paper describes a simple NLP baseline for increasing accuracy of the existing semantic resources of the UCREL Semantic Analysis System (USAS). In our experiments, Spanish token accuracy is improved by up to 30% using this method.</abstract>
      <url hash="76043588">2020.icon-main.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="4950f0d4">2020.icon-main.2.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>sanjurjo-gonzalez-2020-increasing</bibkey>
    </paper>
    <paper id="3">
      <title>Treatment of optional forms in Mathematical modelling of <fixed-case>P</fixed-case>āṇini</title>
      <author><first>Anupriya</first><last>Aggarwal</last></author>
      <author><first>Malhar</first><last>Kulkarni</last></author>
      <pages>15–21</pages>
      <abstract>Pāṇini in his Aṣṭādhyāyī has written the grammar of Sanskrit in an extremely concise manner in the form of about 4000 sūtras. We have attempted to mathematically remodel the data produced by these sūtras. The mathematical modelling is a way to show that the Pāṇinian approach is a minimal method of capturing the grammatical data for Sanskrit which is a natural language. The sūtras written by Pāṇini can be written as functions, that is for a single input the function produces a single output of the form y=f(x), where x and y is the input and output respectively. However, we observe that for some input dhātus, we get multiple outputs. For such cases, we have written multivalued functions that is the functions which give two or more outputs for a single input. In other words, multivalued function is a way to represent optional output forms which are expressed in Pāṇinian grammar with the help of 3 terms i.e. vā, vibhaṣā, and anyatarasyam. Comparison between the techniques employed by Pāṇini and our notation of functions helps us understand how Pāṇinian techniques ensure brevity and terseness, hence illustrating that Pāṇinian grammar is minimal.</abstract>
      <url hash="0c362af0">2020.icon-main.3</url>
      <bibkey>aggarwal-kulkarni-2020-treatment</bibkey>
    </paper>
    <paper id="4">
      <title>Automatic <fixed-case>H</fixed-case>adith Segmentation using <fixed-case>PPM</fixed-case> Compression</title>
      <author><first>Taghreed</first><last>Tarmom</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <author><first>Mohammad</first><last>Alsalka</last></author>
      <pages>22–29</pages>
      <abstract>In this paper we explore the use of Prediction by partial matching (PPM) compression based to segment Hadith into its two main components (Isnad and Matan). The experiments utilized the PPMD variant of the PPM, showing that PPMD is effective in Hadith segmentation. It was also tested on Hadith corpora of different structures. In the first experiment we used the non- authentic Hadith (NAH) corpus for train- ing models and testing, and in the second experiment we used the NAH corpus for training models and the Leeds University and King Saud University (LK) Hadith cor- pus for testing PPMD segmenter. PPMD of order 7 achieved an accuracy of 92.76% and 90.10% in the first and second experiments, respectively.</abstract>
      <url hash="7b2af24e">2020.icon-main.4</url>
      <bibkey>tarmom-etal-2020-automatic</bibkey>
    </paper>
    <paper id="5">
      <title>Using multiple <fixed-case>ASR</fixed-case> hypotheses to boost i18n <fixed-case>NLU</fixed-case> performance</title>
      <author><first>Charith</first><last>Peris</last></author>
      <author><first>Gokmen</first><last>Oz</last></author>
      <author><first>Khadige</first><last>Abboud</last></author>
      <author><first>Venkata sai Varada</first><last>Varada</last></author>
      <author><first>Prashan</first><last>Wanigasekara</last></author>
      <author><first>Haidar</first><last>Khan</last></author>
      <pages>30–39</pages>
      <abstract>Current voice assistants typically use the best hypothesis yielded by their Automatic Speech Recognition (ASR) module as input to their Natural Language Understanding (NLU) module, thereby losing helpful information that might be stored in lower-ranked ASR hypotheses. We explore the change in performance of NLU associated tasks when utilizing five-best ASR hypotheses when compared to status quo for two language datasets, German and Portuguese. To harvest information from the ASR five-best, we leverage extractive summarization and joint extractive-abstractive summarization models for Domain Classification (DC) experiments while using a sequence-to-sequence model with a pointer generator network for Intent Classification (IC) and Named Entity Recognition (NER) multi-task experiments. For the DC full test set, we observe significant improvements of up to 7.2% and 15.5% in micro-averaged F1 scores, for German and Portuguese, respectively. In cases where the best ASR hypothesis was not an exact match to the transcribed utterance (mismatched test set), we see improvements of up to 6.7% and 8.8% micro-averaged F1 scores, for German and Portuguese, respectively. For IC and NER multi-task experiments, when evaluating on the mismatched test set, we see improvements across all domains in German and in 17 out of 19 domains in Portuguese (improvements based on change in SeMER scores). Our results suggest that the use of multiple ASR hypotheses, as opposed to one, can lead to significant performance improvements in the DC task for these non-English datasets. In addition, it could lead to significant improvement in the performance of IC and NER tasks in cases where the ASR model makes mistakes.</abstract>
      <url hash="6de5fc0d">2020.icon-main.5</url>
      <bibkey>peris-etal-2020-using</bibkey>
    </paper>
    <paper id="6">
      <title>A Grammatical Sketch of Asur: A <fixed-case>N</fixed-case>orth <fixed-case>M</fixed-case>unda language</title>
      <author><first>Zoya</first><last>Khalid</last></author>
      <pages>40–49</pages>
      <abstract>Asur belongs to North Munda sub-branch of Austro-Asiatic languages which now has less than 10,000 speakers. This is a very first attempt at describing and documenting Asur language, therefore the approach of this paper is descriptive rather than that of answering research questions. The paper attempts to describe the grammatical features such as gender, number, case, pronouns, tense-aspect-mood, negation, question formation, etc. of Asur language. It briefly touches upon the morphosyntactic and typological features of Asur, with the intent to present a concise overview of the language, which has so far remained almost untouched by documentary linguistics.</abstract>
      <url hash="fde28d80">2020.icon-main.6</url>
      <bibkey>khalid-2020-grammatical</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>E</fixed-case>nglish to <fixed-case>M</fixed-case>anipuri and Mizo Post-Editing Effort and its Impact on Low Resource Machine Translation</title>
      <author><first>Loitongbam</first><last>Sanayai Meetei</last></author>
      <author><first>Thoudam Doren</first><last>Singh</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <author><first>Mihaela</first><last>Vela</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>50–59</pages>
      <abstract>We present the first study on the post-editing (PE) effort required to build a parallel dataset for English-Manipuri and English-Mizo, in the context of a project on creating data for machine translation (MT). English source text from a local daily newspaper are machine translated into Manipuri and Mizo using PBSMT systems built in-house. A Computer Assisted Translation (CAT) tool is used to record the time, keystroke and other indicators to measure PE effort in terms of temporal and technical effort. A positive correlation between the technical effort and the number of function words is seen for English-Manipuri and English-Mizo but a negative correlation between the technical effort and the number of noun words for English-Mizo. However, average time spent per token in PE English-Mizo text is negatively correlated with the temporal effort. The main reason for these results are due to (i) English and Mizo using the same script, while Manipuri uses a different script and (ii) the agglutinative nature of Manipuri. Further, we check the impact of training a MT system in an incremental approach, by including the post-edited dataset as additional training data. The result shows an increase in HBLEU of up to 4.6 for English-Manipuri.</abstract>
      <url hash="5209bb04">2020.icon-main.7</url>
      <bibkey>sanayai-meetei-etal-2020-english</bibkey>
    </paper>
    <paper id="8">
      <title>Learning to Interact: An Adaptive Interaction Framework for Knowledge Graph Embeddings</title>
      <author><first>.</first><last>Chandrahas</last></author>
      <author><first>Nilesh</first><last>Agrawal</last></author>
      <author><first>Partha</first><last>Talukdar</last></author>
      <pages>60–69</pages>
      <abstract>Knowledge Graph (KG) Embedding methods have been widely studied in the past few years and many methods have been proposed. These methods represent entities and relations in the KG as vectors in a vector space, trained to distinguish correct edges from the incorrect ones. For this distinction, simple functions of vectors’ dimensions, called interactions, are used. These interactions are used to calculate the candidate tail entity vector which is matched against all entities in the KG. However, for most of the existing methods, these interactions are fixed and manually specified. In this work, we propose an automated framework for discovering the interactions while training the KG Embeddings. The proposed method learns relevant interactions along with other parameters during training, allowing it to adapt to different datasets. Many of the existing methods can be seen as special cases of the proposed framework. We demonstrate the effectiveness of the proposed method on link prediction task by extensive experiments on multiple benchmark datasets.</abstract>
      <url hash="2b7c809e">2020.icon-main.8</url>
      <bibkey>chandrahas-etal-2020-learning</bibkey>
    </paper>
    <paper id="9">
      <title>Inducing Interpretability in Knowledge Graph Embeddings</title>
      <author><first>.</first><last>Chandrahas</last></author>
      <author><first>Tathagata</first><last>Sengupta</last></author>
      <author><first>Cibi</first><last>Pragadeesh</last></author>
      <author><first>Partha</first><last>Talukdar</last></author>
      <pages>70–75</pages>
      <abstract>We study the problem of inducing interpretability in Knowledge Graph (KG) embeddings. Learning KG embeddings has been an active area of research in the past few years, resulting in many different models. However, most of these methods do not address the interpretability (semantics) of individual dimensions of the learned embeddings. In this work, we study this problem and propose a method for inducing interpretability in KG embeddings using entity co-occurrence statistics. The proposed method significantly improves the interpretability, while maintaining comparable performance in other KG tasks.</abstract>
      <url hash="31cf7c3b">2020.icon-main.9</url>
      <bibkey>chandrahas-etal-2020-inducing</bibkey>
    </paper>
    <paper id="10">
      <title>Solving Arithmetic Word Problems Using Transformer and Pre-processing of Problem Texts</title>
      <author><first>Kaden</first><last>Griffith</last></author>
      <author><first>Jugal</first><last>Kalita</last></author>
      <pages>76–84</pages>
      <abstract>This paper outlines the use of Transformer networks trained to translate math word problems to equivalent arithmetic expressions in infix, prefix, and postfix notations. We compare results produced by a large number of neural configurations and find that most configurations outperform previously reported approaches on three of four datasets with significant increases in accuracy of over 20 percentage points. The best neural approaches boost accuracy by 30% on average when compared to the previous state-of-the-art.</abstract>
      <url hash="b9bb0f89">2020.icon-main.10</url>
      <bibkey>griffith-kalita-2020-solving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/mawps">MAWPS</pwcdataset>
    </paper>
    <paper id="11">
      <title>Clickbait in <fixed-case>H</fixed-case>indi News Media : A Preliminary Study</title>
      <author><first>Vivek</first><last>Kaushal</last></author>
      <author><first>Kavita</first><last>Vemuri</last></author>
      <pages>85–89</pages>
      <abstract>A corpus of Hindi news headlines shared on Twitter was created by collecting tweets of 5 mainstream Hindi news sources for a period of 4 months. 7 independent annotators were recruited to mark the 20 most retweeted news posts by each of the 5 news sources on its clickbait nature. The clickbait score hence generated was assessed for its correlation with interactions on the platform (retweets, favorites, reader replies), tweet word count, and normalized POS (part-of-speech) tag counts in tweets. A positive correlation was observed between readers’ interactions with tweets and tweets’ clickbait score. Significant correlations were also observed for POS tag counts and clickbait score. The prevalence of clickbait in mainstream Hindi news media was found to be similar to its prevalence in English news media. We hope that our observations would provide a platform for discussions on clickbait in mainstream Hindi news media.</abstract>
      <url hash="6d4ffa0c">2020.icon-main.11</url>
      <bibkey>kaushal-vemuri-2020-clickbait</bibkey>
    </paper>
    <paper id="12">
      <title>Self Attended Stack-Pointer Networks for Learning Long Term Dependencies</title>
      <author><first>Salih</first><last>Tuc</last></author>
      <author><first>Burcu</first><last>Can</last></author>
      <pages>90–100</pages>
      <abstract>We propose a novel deep neural architecture for dependency parsing, which is built upon a Transformer Encoder (Vaswani et al. 2017) and a Stack Pointer Network (Ma et al. 2018). We first encode each sentence using a Transformer Network and then the dependency graph is generated by a Stack Pointer Network by selecting the head of each word in the sentence through a head selection process. We evaluate our model on Turkish and English treebanks. The results show that our trasformer-based model learns long term dependencies efficiently compared to sequential models such as recurrent neural networks. Our self attended stack pointer network improves UAS score around 6% upon the LSTM based stack pointer (Ma et al. 2018) for Turkish sentences with a length of more than 20 words.</abstract>
      <url hash="bdf171ad">2020.icon-main.12</url>
      <bibkey>tuc-can-2020-self</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="13">
      <title>Creation of Corpus and Analysis in Code-Mixed <fixed-case>K</fixed-case>annada-<fixed-case>E</fixed-case>nglish Social Media Data for <fixed-case>POS</fixed-case> Tagging</title>
      <author><first>Abhinav Reddy</first><last>Appidi</last></author>
      <author><first>Vamshi Krishna</first><last>Srirangam</last></author>
      <author><first>Darsi</first><last>Suhas</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>101–107</pages>
      <abstract>Part-of-Speech (POS) is one of the essential tasks for many Natural Language Processing (NLP) applications. There has been a significant amount of work done in POS tagging for resource-rich languages. POS tagging is an essential phase of text analysis in understanding the semantics and context of language. These tags are useful for higher-level tasks such as building parse trees, which can be used for Named Entity Recognition, Coreference resolution, Sentiment Analysis, and Question Answering. There has been work done on code-mixed social media corpus but not on POS tagging of Kannada-English code-mixed data. Here, we present Kannada-English code- mixed social media corpus annotated with corresponding POS tags. We also experimented with machine learning classification models CRF, Bi-LSTM, and Bi-LSTM-CRF models on our corpus.</abstract>
      <url hash="5b6be213">2020.icon-main.13</url>
      <bibkey>appidi-etal-2020-creation-corpus</bibkey>
    </paper>
    <paper id="14">
      <title>Identifying Complaints from Product Reviews: A Case Study on <fixed-case>H</fixed-case>indi</title>
      <author><first>Raghvendra Pratap</first><last>Singh</last></author>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Mohammed</first><last>Hasanuzzaman</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>108–116</pages>
      <abstract>Automatic recognition of customer complaints on products or services that they purchase can be crucial for the organisations, multinationals and online retailers since they can exploit this information to fulfil their customers’ expectations including managing and resolving the complaints. Recently, researchers have applied supervised learning strategies to automatically identify users’ complaints expressed in English on Twitter. The downside of these approaches is that they require labeled training data for learning, which is expensive to create. This poses a barrier for them being applied to low-resource languages and domains for which task-specific data is not available. Machine translation (MT) can be used as an alternative to the tools that require such task-specific data. In this work, we use state-of-the-art neural MT (NMT) models for translating Hindi reviews into English and investigate performance of the downstream classification task (complaints identification) on their English translations.</abstract>
      <url hash="ab3e1537">2020.icon-main.14</url>
      <bibkey>singh-etal-2020-identifying</bibkey>
      <pwccode url="https://github.com/mrraghav/complaints-mining-from-hindi-product-reviews" additional="false">mrraghav/complaints-mining-from-hindi-product-reviews</pwccode>
    </paper>
    <paper id="15">
      <title>Generative Adversarial Networks for Annotated Data Augmentation in Data Sparse <fixed-case>NLU</fixed-case></title>
      <author><first>Olga</first><last>Golovneva</last></author>
      <author><first>Charith</first><last>Peris</last></author>
      <pages>117–126</pages>
      <abstract>Data sparsity is one of the key challenges associated with model development in Natural Language Understanding (NLU) for conversational agents. The challenge is made more complex by the demand for high quality annotated utterances commonly required for supervised learning, usually resulting in weeks of manual labor and high cost. In this paper, we present our results on boosting NLU model performance through training data augmentation using a sequential generative adversarial network (GAN). We explore data generation in the context of two tasks, the bootstrapping of a new language and the handling of low resource features. For both tasks we explore three sequential GAN architectures, one with a token-level reward function, another with our own implementation of a token-level Monte Carlo rollout reward, and a third with sentence-level reward. We evaluate the performance of these feedback models across several sampling methodologies and compare our results to upsampling the original data to the same scale. We further improve the GAN model performance through the transfer learning of the pre-trained embeddings. Our experiments reveal synthetic data generated using the sequential generative adversarial network provides significant performance boosts across multiple metrics and can be a major benefit to the NLU tasks.</abstract>
      <url hash="c6db1bbe">2020.icon-main.15</url>
      <attachment type="OptionalSupplementaryMaterial" hash="e1f19ad5">2020.icon-main.15.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>golovneva-peris-2020-generative</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>B</fixed-case>ert<fixed-case>AA</fixed-case> : <fixed-case>BERT</fixed-case> fine-tuning for Authorship Attribution</title>
      <author><first>Maël</first><last>Fabien</last></author>
      <author><first>Esau</first><last>Villatoro-Tello</last></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <pages>127–137</pages>
      <abstract>Identifying the author of a given text can be useful in historical literature, plagiarism detection, or police investigations. Authorship Attribution (AA) has been well studied and mostly relies on a large feature engineering work. More recently, deep learning-based approaches have been explored for Authorship Attribution (AA). In this paper, we introduce BertAA, a fine-tuning of a pre-trained BERT language model with an additional dense layer and a softmax activation to perform authorship classification. This approach reaches competitive performances on Enron Email, Blog Authorship, and IMDb (and IMDb62) datasets, up to 5.3% (relative) above current state-of-the-art approaches. We performed an exhaustive analysis allowing to identify the strengths and weaknesses of the proposed method. In addition, we evaluate the impact of including additional features (e.g. stylometric and hybrid features) in an ensemble approach, improving the macro-averaged F1-Score by 2.7% (relative) on average.</abstract>
      <url hash="96da91db">2020.icon-main.16</url>
      <bibkey>fabien-etal-2020-bertaa</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>TREE</fixed-case> <fixed-case>ADJOINING</fixed-case> <fixed-case>GRAMMAR</fixed-case> <fixed-case>BASED</fixed-case> “<fixed-case>LANGUAGE</fixed-case> <fixed-case>INDEPENDENT</fixed-case> <fixed-case>GENERATOR</fixed-case>”</title>
      <author><first>Pavan</first><last>Kurariya</last></author>
      <author><first>Prashant</first><last>Chaudhary</last></author>
      <author><first>Jahnavi</first><last>Bodhankar</last></author>
      <author><first>Lenali</first><last>Singh</last></author>
      <author><first>Ajai</first><last>Kumar</last></author>
      <author><first>Hemant</first><last>Darbari</last></author>
      <pages>138–143</pages>
      <abstract>This paper proposes language independent natural language generator for Tree Adjoining Grammar (TAG)[8] based Machine Translation System. In this model, the TAG based parsing and generation approach considered for the syntactic and semantic analysis of a source language. This model provides an efficient and a systematic way of encapsulating language resources with engineering solution to develop the machine translation System. A TAG based Generator is developed with existing resources using TAG formalism to generate the target language from TAG based parser derivation. The process allows syntactic feature-marking, the Subject-Predicate Agreement marking and multiple synthesized generated outputs in complex and morphological rich language. The challenge in applying such approach is to handle the linguistically diversified features. It is achieved using rule-based translation grammar model to align the source language to corresponding target languages. The computational experiments demonstrate that substantial performance in terms of time and memory could also be obtained by using this approach. Nevertheless, this paper also describes the process of lexicalization and explain the state charts, TAG based adjunction and substitution function and the complexity and challenges beneath parsing-generation process.</abstract>
      <url hash="fb41e724">2020.icon-main.17</url>
      <bibkey>kurariya-etal-2020-tree</bibkey>
    </paper>
    <paper id="18">
      <title>Exploration of Cross-lingual Summarization for <fixed-case>K</fixed-case>annada-<fixed-case>E</fixed-case>nglish<fixed-case>L</fixed-case>anguage Pair</title>
      <author><first>Vinayaka</first><last>R Kamath</last></author>
      <author><first>Rachana</first><last>Aithal K R</last></author>
      <author><first>Vennela</first><last>K</last></author>
      <author><first>Mamatha</first><last>Hr</last></author>
      <pages>144–148</pages>
      <abstract>Cross-lingual summarization(CLS) is the process of generating a summary in one particular language for a source document in a different language. Low resource languages like Kannada greatly benefit from such systems because they help in delivering a concise representation of the same information in a different popular language. We propose a novel dataset generation pipeline and a first of its kind dataset that will aid in CLS for Kannada-English language pair. This work is also an attempt to inspect the existing systems and extend them to the Kannada-English language pair using our dataset.</abstract>
      <url hash="aec3a9de">2020.icon-main.18</url>
      <bibkey>r-kamath-etal-2020-exploration</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
    </paper>
    <paper id="19">
      <title>Hater-<fixed-case>O</fixed-case>-Genius Aggression Classification using Capsule Networks</title>
      <author><first>Parth</first><last>Patwa</last></author>
      <author><first>Srinivas</first><last>Pykl</last></author>
      <author><first>Amitava</first><last>Das</last></author>
      <author><first>Prerana</first><last>Mukherjee</last></author>
      <author><first>Viswanath</first><last>Pulabaigari</last></author>
      <pages>149–154</pages>
      <abstract>Contending hate speech in social media is one of the most challenging social problems of our time. There are various types of anti-social behavior in social media. Foremost of them is aggressive behavior, which is causing many social issues such as affecting the social lives and mental health of social media users. In this paper, we propose an end-to-end ensemble-based architecture to automatically identify and classify aggressive tweets. Tweets are classified into three categories - Covertly Aggressive, Overtly Aggressive, and Non-Aggressive. The proposed architecture is an ensemble of smaller subnetworks that are able to characterize the feature embeddings effectively. We demonstrate qualitatively that each of the smaller subnetworks is able to learn unique features. Our best model is an ensemble of Capsule Networks and results in a 65.2% F1 score on the Facebook test set, which results in a performance gain of 0.95% over the TRAC-2018 winners. The code and the model weights are publicly available at https://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks.</abstract>
      <url hash="74c49c3e">2020.icon-main.19</url>
      <bibkey>patwa-etal-2020-hater</bibkey>
      <pwccode url="https://github.com/parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks" additional="false">parthpatwa/Hater-O-Genius-Aggression-Classification-using-Capsule-Networks</pwccode>
    </paper>
    <paper id="20">
      <title>A New Approach to Claim Check-Worthiness Prediction and Claim Verification</title>
      <author><first>Shukrity</first><last>Si</last></author>
      <author><first>Anisha</first><last>Datta</last></author>
      <author><first>Sudip</first><last>Naskar</last></author>
      <pages>155–160</pages>
      <abstract>The more we are advancing towards a modern world, the more it opens the path to falsification in every aspect of life. Even in case of knowing the surrounding, common people can not judge the actual scenario as the promises, comments and opinions of the influential people at power keep changing every day. Therefore computationally determining the truthfulness of such claims and comments has a very important societal impact. This paper describes a unique method to extract check-worthy claims from the 2016 US presidential debates and verify the truthfulness of the check-worthy claims. We classify the claims for check-worthiness with our modified Tf-Idf model which is used in background training on fact-checking news articles (NBC News and Washington Post). We check the truthfulness of the claims by using POS, sentiment score and cosine similarity features.</abstract>
      <url hash="3c54f0ed">2020.icon-main.20</url>
      <bibkey>si-etal-2020-new</bibkey>
    </paper>
    <paper id="21">
      <title>Improving Passage Re-Ranking with Word N-Gram Aware Coattention Encoder</title>
      <author><first>Chaitanya</first><last>Alaparthi</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <pages>161–169</pages>
      <abstract>In text matching applications, coattentions have proved to be highly effective attention mechanisms. Coattention enables the learning to attend based on computing word level affinity scores between two texts. In this paper, we propose two improvements to coattention mechanism in the context of passage ranking (re-ranking). First, we extend the coattention mechanism by applying it across all word n-grams of query and passage. We show that these word n-gram coattentions can capture local context in query and passage to better judge the relevance between them. Second, we further improve the model performance by proposing a query based attention pooling on passage encodings. We evaluate these two methods on MSMARCO passage re-ranking task. The experiment results shows that these two methods resulted in a relative increase of 8.04% in Mean Reciprocal Rank @10 (MRR@10) compared to the naive coattention mechanism. At the time of writing this paper, our methods are the best non transformer model on MS MARCO passage re-ranking task and are competitive to BERT base while only having less than 10% of the parameters.</abstract>
      <url hash="a0cec884">2020.icon-main.21</url>
      <bibkey>alaparthi-shrivastava-2020-improving</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="22">
      <title>Language Model Metrics and <fixed-case>P</fixed-case>rocrustes Analysis for Improved Vector Transformation of <fixed-case>NLP</fixed-case> Embeddings</title>
      <author><first>Thomas</first><last>Conley</last></author>
      <author><first>Jugal</first><last>Kalita</last></author>
      <pages>170–174</pages>
      <abstract>Artificial Neural networks are mathematical models at their core. This truism presents some fundamental difficulty when networks are tasked with Natural Language Processing. A key problem lies in measuring the similarity or distance among vectors in NLP embedding space, since the mathematical concept of distance does not always agree with the linguistic concept. We suggest that the best way to measure linguistic distance among vectors is by employing the Language Model (LM) that created them. We introduce Language Model Distance (LMD) for measuring accuracy of vector transformations based on the Distributional Hypothesis ( LMD Accuracy ). We show the efficacy of this metric by applying it to a simple neural network learning the Procrustes algorithm for bilingual word mapping.</abstract>
      <url hash="0e68c2c1">2020.icon-main.22</url>
      <bibkey>conley-kalita-2020-language</bibkey>
    </paper>
    <paper id="23">
      <title>Cognitively Aided Zero-Shot Automatic Essay Grading</title>
      <author><first>Sandeep</first><last>Mathias</last></author>
      <author><first>Rudra</first><last>Murthy</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>175–180</pages>
      <abstract>Automatic essay grading (AEG) is a process in which machines assign a grade to an essay written in response to a topic, called the prompt. Zero-shot AEG is when we train a system to grade essays written to a new prompt which was not present in our training data. In this paper, we describe a solution to the problem of zero-shot automatic essay grading, using cognitive information, in the form of gaze behaviour. Our experiments show that using gaze behaviour helps in improving the performance of AEG systems, especially when we provide a new essay written in response to a new prompt for scoring, by an average of almost 5 percentage points of QWK.</abstract>
      <url hash="ed8ba7da">2020.icon-main.23</url>
      <bibkey>mathias-etal-2020-cognitively</bibkey>
    </paper>
    <paper id="24">
      <title>Automated <fixed-case>A</fixed-case>rabic Essay Evaluation</title>
      <author><first>Abeer</first><last>Alqahtani</last></author>
      <author><first>Amal</first><last>Alsaif</last></author>
      <pages>181–190</pages>
      <abstract>Although the manual evaluation of essays is a time-consuming process, writing essays has a significant role in assessing learning outcomes. Therefore, automated essay evaluation represents a solution, especially for schools, universities, and testing companies. Moreover, the existence of such systems overcomes some factors that influence manual evaluation such as the evaluator’s mental state, the disparity between evaluators, and others. In this paper, we propose an Arabic essay evaluation system based on a support vector regression (SVR) model along with a wide range of features including morphological, syntactic, semantic, and discourse features. The system evaluates essays according to five criteria: spelling, essay structure, coherence level, style, and punctuation marks, without the need for domain-representative essays (a model essay). A specific model is developed for each criterion; thus, the overall evaluation of the essay is a combination of the previous criteria results. We develop our dataset based on essays written by university students and journalists whose native language is Arabic. The dataset is then evaluated by experts. The experimental results show that 96% of our dataset is correctly evaluated in the overall score and the correlation between the system and the experts’ evaluation is 0.87. Additionally, the system shows variant results in evaluating criteria separately.</abstract>
      <url hash="0a365bfd">2020.icon-main.24</url>
      <bibkey>alqahtani-alsaif-2020-automated</bibkey>
    </paper>
    <paper id="25">
      <title>Semantic Extractor-Paraphraser based Abstractive Summarization</title>
      <author><first>Anubhav</first><last>Jangra</last></author>
      <author><first>Raghav</first><last>Jain</last></author>
      <author><first>Vaibhav</first><last>Mavi</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>191–199</pages>
      <abstract>The anthology of spoken languages today is inundated with textual information, necessitating the development of automatic summarization models. In this manuscript, we propose an extractor-paraphraser based abstractive summarization system that exploits semantic overlap as opposed to its predecessors that focus more on syntactic information overlap. Our model outperforms the state-of-the-art baselines in terms of ROUGE, METEOR and word mover similarity (WMS), establishing the superiority of the proposed system via extensive ablation experiments. We have also challenged the summarization capabilities of the state of the art Pointer Generator Network (PGN), and through thorough experimentation, shown that PGN is more of a paraphraser, contrary to the prevailing notion of a summarizer; illustrating it’s incapability to accumulate information across multiple sentences.</abstract>
      <url hash="3accfe8f">2020.icon-main.25</url>
      <bibkey>jangra-etal-2020-semantic</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>T</fixed-case>hamizhi<fixed-case>UD</fixed-case>p: A Dependency Parser for <fixed-case>T</fixed-case>amil</title>
      <author><first>Kengatharaiyer</first><last>Sarveswaran</last></author>
      <author><first>Gihan</first><last>Dias</last></author>
      <pages>200–207</pages>
      <abstract>This paper describes how we developed a neural-based dependency parser, namely ThamizhiUDp, which provides a complete pipeline for the dependency parsing of the Tamil language text using Universal Dependency formalism. We have considered the phases of the dependency parsing pipeline and identified tools and resources in each of these phases to improve the accuracy and to tackle data scarcity. ThamizhiUDp uses Stanza for tokenisation and lemmatisation, ThamizhiPOSt and ThamizhiMorph for generating Part of Speech (POS) and Morphological annotations, and uuparser with multilingual training for dependency parsing. ThamizhiPOSt is our POS tagger, which is based on the Stanza, trained with Amrita POS-tagged corpus. It is the current state-of-the-art in Tamil POS tagging with an F1 score of 93.27. Our morphological analyzer, ThamizhiMorph is a rule-based system with a very good coverage of Tamil. Our dependency parser ThamizhiUDp was trained using multilingual data. It shows a Labelled Assigned Score (LAS) of 62.39, 4 points higher than the current best achieved for Tamil dependency parsing. Therefore, we show that breaking up the dependency parsing pipeline to accommodate existing tools and resources is a viable approach for low-resource languages.</abstract>
      <url hash="22feeeb6">2020.icon-main.26</url>
      <bibkey>sarveswaran-dias-2020-thamizhiudp</bibkey>
      <pwccode url="https://github.com/sarves/thamizhi-pos" additional="true">sarves/thamizhi-pos</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="27">
      <title>Constructing a <fixed-case>K</fixed-case>orean Named Entity Recognition Dataset for the Financial Domain using Active Learning</title>
      <author><first>Dong-Ho</first><last>Jeong</last></author>
      <author><first>Min-Kang</first><last>Heo</last></author>
      <author><first>Hyung-Chul</first><last>Kim</last></author>
      <author><first>Sang-Won</first><last>Park</last></author>
      <pages>208–212</pages>
      <abstract>The performance of deep learning models depends on the quality and quantity of data. Data construction, however, is time- consuming and costly. In addition, when expert domain data are constructed, the availability of experts is limited. In such cases, active learning can efficiently increase the performance of the learning models with minimal data construction. Although various datasets have been constructed using active learning techniques, vigorous studies on the construction of Korean data on expert domains are yet to be conducted. In this study, a corpus for named entity recognition was constructed for the financial domain using the active learning technique. The contributions of the study are as follows. (1) It was verified that the active learning technique could effectively construct the named entity recognition corpus for the financial domain, and (2) a named entity recognizer for the financial domain was developed. Data of 8,043 sentences were constructed using the proposed method, and the performance of the named entity recognizer reached 80.84%. Moreover, the proposed method reduced data construction costs by 12–25%</abstract>
      <url hash="8ef59718">2020.icon-main.27</url>
      <bibkey>jeong-etal-2020-constructing</bibkey>
    </paper>
    <paper id="28">
      <title>Self-Supervised Claim Identification for Automated Fact Checking</title>
      <author><first>Archita</first><last>Pathak</last></author>
      <author><first>Mohammad Abuzar</first><last>Shaikh</last></author>
      <author><first>Rohini</first><last>Srihari</last></author>
      <pages>213–227</pages>
      <abstract>We propose a novel, attention-based self-supervised approach to identify “claim-worthy” sentences in a fake news article, an important first step in automated fact-checking. We leverage <i>aboutness</i> of headline and content using attention mechanism for this task. The identified claims can be used for downstream task of claim verification for which we are releasing a benchmark dataset of manually selected compelling articles with veracity labels and associated evidence. This work goes beyond stylistic analysis to identifying content that influences reader belief. Experiments with three datasets show the strength of our model.</abstract>
      <url hash="cd5ce1ae">2020.icon-main.28</url>
      <bibkey>pathak-etal-2020-self</bibkey>
      <pwccode url="https://github.com/architapathak/Self-Supervised-ClaimIdentification" additional="false">architapathak/Self-Supervised-ClaimIdentification</pwccode>
    </paper>
    <paper id="29">
      <title><fixed-case>SUKHAN</fixed-case>: Corpus of <fixed-case>H</fixed-case>indi Shayaris annotated with Sentiment Polarity Information</title>
      <author><first>Salil</first><last>Aggarwal</last></author>
      <author><first>Abhigyan</first><last>Ghosh</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>228–233</pages>
      <abstract>Shayari is a form of poetry mainly popular in the Indian subcontinent, in which the poet expresses his emotions and feelings in a very poetic manner. It is one of the best ways to express our thoughts and opinions. Therefore, it is of prime importance to have an annotated corpus of Hindi shayaris for the task of sentiment analysis. In this paper, we introduce SUKHAN, a dataset consisting of Hindi shayaris along with sentiment polarity labels. To the best of our knowledge, this is the first corpus of Hindi shayaris annotated with sentiment polarity information. This corpus contains a total of 733 Hindi shayaris of various genres. Also, this dataset is of utmost value as all the annotation is done manually by five annotators and this makes it a very rich dataset for training purposes. This annotated corpus is also used to build baseline sentiment classification models using machine learning techniques.</abstract>
      <url hash="8ce90bdc">2020.icon-main.29</url>
      <bibkey>aggarwal-etal-2020-sukhan</bibkey>
    </paper>
    <paper id="30">
      <title>Improving Neural Machine Translation for <fixed-case>S</fixed-case>anskrit-<fixed-case>E</fixed-case>nglish</title>
      <author><first>Ravneet</first><last>Punia</last></author>
      <author><first>Aditya</first><last>Sharma</last></author>
      <author><first>Sarthak</first><last>Pruthi</last></author>
      <author><first>Minni</first><last>Jain</last></author>
      <pages>234–238</pages>
      <abstract>Sanskrit is one of the oldest languages of the Asian Subcontinent that fell out of common usage around 600 B.C. In this paper, we attempt to translate Sanskrit to English using Neural Machine Translation approaches based on Reinforcement Learning and Transfer learning that were never tried and tested on Sanskrit. Along with the paper, we also release monolingual Sanskrit and parallel aligned Sanskrit-English corpora for the research community. Our methodologies outperform the previous approaches applied to Sanskrit by various re- searchers and will further help the linguistic community to accelerate the costly and time consuming manual translation process.</abstract>
      <url hash="c0b96393">2020.icon-main.30</url>
      <bibkey>punia-etal-2020-improving</bibkey>
    </paper>
    <paper id="31">
      <title>Parsing <fixed-case>I</fixed-case>ndian <fixed-case>E</fixed-case>nglish News Headlines</title>
      <author><first>Samapika</first><last>Roy</last></author>
      <author><first>Sukhada</first><last>Sukhada</last></author>
      <author><first>Anil</first><last>Kumar Singh</last></author>
      <pages>239–242</pages>
      <abstract>Parsing news Headlines is one of the difficult tasks of Natural Language Processing. It is mostly because news Headlines (NHs) are not complete grammatical sentences. News editors use all sorts of tricks to grab readers’ attention, for instance, unusual capitalization as in the headline’ Ear SHOT ashok rajagopalan’; some are world knowledge demanding like ‘Church reformation celebrated’ where the ‘Church reformation’ refers to a historical event and not a piece of news about an ordinary church. The lack of transparency in NHs can be linguistic, cultural, social, or contextual. The lack of space provided for a news headline has led to creative liberty. Though many works like news value extraction, summary generation, emotion classification of NHs have been going on, parsing them had been a tough challenge. Linguists have also been interested in NHs for creativity in the language used by bending traditional grammar rules. Researchers have conducted studies on news reportage, discourse analysis of NHs, and many more. While the creativity seen in NHs is fascinating for language researchers, it poses a computational challenge for Natural Language Processing researchers. This paper presents an outline of the ongoing doctoral research on the parsing of Indian English NHs. The ultimate aim of this research is to provide a module that will generate correctly parsed NHs. The intention is to enhance the broad applicability of newspaper corpus for future Natural Language Processing applications.</abstract>
      <url hash="2e89e26b">2020.icon-main.31</url>
      <bibkey>roy-etal-2020-parsing</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>WORD</fixed-case> <fixed-case>SENSE</fixed-case> <fixed-case>DISAMBIUATION</fixed-case> <fixed-case>FOR</fixed-case> <fixed-case>KASHMIRI</fixed-case> <fixed-case>LANGUAGE</fixed-case> <fixed-case>USING</fixed-case> <fixed-case>SUPERVISED</fixed-case> <fixed-case>MACHINE</fixed-case> <fixed-case>LEARNING</fixed-case></title>
      <author><first>Tawseef Ahmad</first><last>Mir</last></author>
      <author><first>Aadil Ahmad</first><last>Lawaye</last></author>
      <pages>243–245</pages>
      <abstract>Every language used in this word has ambiguous words. The process of analyzing the word tokens and assigning the correct meanings to the ambiguous words according the context in which they are used is called word sense disambiguation(WSD). WSD is a very hot research topic in Natural Language Processing. The main purpose of my research work is to tackle the WSD problem for Kashmiri language using Supervised Machine Learning Approaches</abstract>
      <url hash="c6f77af5">2020.icon-main.32</url>
      <bibkey>mir-lawaye-2020-word</bibkey>
    </paper>
    <paper id="33">
      <title>Sentimental Poetry Generation</title>
      <author><first>Kasper Aalberg</first><last>Røstvold</last></author>
      <author><first>Björn</first><last>Gambäck</last></author>
      <pages>246–256</pages>
      <abstract>The paper investigates how well poetry can be generated to contain a specific sentiment, and whether readers of the poetry experience the intended sentiment. The poetry generator consists of a bi-directional Long Short-Term Memory (LSTM) model, combined with rhyme pair generation, rule-based word prediction methods, and tree search for extending generation possibilities. The LSTM network was trained on a set of English poetry written and published by users on a public website. Human judges evaluated poems generated by the system, both with a positive and negative sentiment. The results indicate that while there are some weaknesses in the system compared to other state-of-the-art solutions, it is fully capable of generating poetry with an inherent sentiment that is perceived by readers.</abstract>
      <url hash="19e2c85b">2020.icon-main.33</url>
      <bibkey>rostvold-gamback-2020-sentimental</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="34">
      <title><fixed-case>WEKA</fixed-case> in Forensic Authorship Analysis: A corpus-based approach of Saudi Authors</title>
      <author><first>Mashael</first><last>AlAmr</last></author>
      <author><first>Eric</first><last>Atwell</last></author>
      <pages>257–260</pages>
      <abstract>This is a pilot study that aims to explore the potential of using WEKA in forensic authorship analysis. It is a corpus-based research using data from Twitter collected from thirteen authors from Riyadh, Saudi Arabia. It examines the performance of unbalanced and balanced data sets using different classifiers and parameters of word grams. The attributes are dialect-specific linguistic features categorized as word grams. The findings further support previous studies in computational authorship identification.</abstract>
      <url hash="8811664b">2020.icon-main.34</url>
      <bibkey>alamr-atwell-2020-weka</bibkey>
    </paper>
    <paper id="35">
      <title>Native-Language Identification with Attention</title>
      <author><first>Stian</first><last>Steinbakken</last></author>
      <author><first>Björn</first><last>Gambäck</last></author>
      <pages>261–271</pages>
      <abstract>The paper explores how an attention-based approach can increase performance on the task of native-language identification (NLI), i.e., to identify an author’s first language given information expressed in a second language. Previously, Support Vector Machines have consistently outperformed deep learning-based methods on the TOEFL11 data set, the de facto standard for evaluating NLI systems. The attention-based system BERT (Bidirectional Encoder Representations from Transformers) was first tested in isolation on the TOEFL11 data set, then used in a meta-classifier stack in combination with traditional techniques to produce an accuracy of 0.853. However, more labelled NLI data is now available, so BERT was also trained on the much larger Reddit-L2 data set, containing 50 times as many examples as previously used for English NLI, giving an accuracy of 0.902 on the Reddit-L2 in-domain test scenario, improving the state-of-the-art by 21.2 percentage points.</abstract>
      <url hash="eba43b9a">2020.icon-main.35</url>
      <bibkey>steinbakken-gamback-2020-native</bibkey>
    </paper>
    <paper id="36">
      <title>Does a Hybrid Neural Network based Feature Selection Model Improve Text Classification?</title>
      <author><first>Suman</first><last>Dowlagar</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>272–280</pages>
      <abstract>Text classification is a fundamental problem in the field of natural language processing. Text classification mainly focuses on giving more importance to all the relevant features that help classify the textual data. Apart from these, the text can have redundant or highly correlated features. These features increase the complexity of the classification algorithm. Thus, many dimensionality reduction methods were proposed with the traditional machine learning classifiers. The use of dimensionality reduction methods with machine learning classifiers has achieved good results. In this paper, we propose a hybrid feature selection method for obtaining relevant features by combining various filter-based feature selection methods and fastText classifier. We then present three ways of implementing a feature selection and neural network pipeline. We observed a reduction in training time when feature selection methods are used along with neural networks. We also observed a slight increase in accuracy on some datasets.</abstract>
      <url hash="0185e33f">2020.icon-main.36</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2a0ec10f">2020.icon-main.36.OptionalSupplementaryMaterial.pdf</attachment>
      <bibkey>dowlagar-mamidi-2020-hybrid</bibkey>
    </paper>
    <paper id="37">
      <title>Efforts Towards Developing a <fixed-case>T</fixed-case>amang <fixed-case>N</fixed-case>epali Machine Translation System</title>
      <author><first>Binaya Kumar</first><last>Chaudhary</last></author>
      <author><first>Bal Krishna</first><last>Bal</last></author>
      <author><first>Rasil</first><last>Baidar</last></author>
      <pages>281–286</pages>
      <abstract>The Tamang language is spoken mainly in Nepal, Sikkim, West Bengal, some parts of Assam, and the North East region of India. As per the 2011 census conducted by the Nepal Government, there are about 1.35 million Tamang speakers in Nepal itself. In this regard, a Machine Translation System for Tamang-Nepali language pair is significant both from research and practical outcomes in terms of enabling communication between the Tamang and the Nepali communities. In this work, we train the Transformer Neural Machine Translation (NMT) architecture with attention using a small hand-labeled or aligned Tamang-Nepali corpus (15K sentence pairs). Our preliminary results show BLEU scores of 27.74 for the Nepali→Tamang direction and 23.74 in the Tamang→Nepali direction. We are currently working on increasing the datasets as well as improving the model to obtain better BLEU scores. We also plan to extend the work to add the English language to the model, thus making it a trilingual Machine Translation System for Tamang-Nepali-English languages.</abstract>
      <url hash="1b7bdec4">2020.icon-main.37</url>
      <bibkey>chaudhary-etal-2020-efforts</bibkey>
    </paper>
    <paper id="38">
      <title>Event Argument Extraction using Causal Knowledge Structures</title>
      <author><first>Debanjana</first><last>Kar</last></author>
      <author><first>Sudeshna</first><last>Sarkar</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>287–296</pages>
      <abstract>Event Argument extraction refers to the task of extracting structured information from unstructured text for a particular event of interest. The existing works exhibit poor capabilities to extract causal event arguments like Reason and After Effects. Futhermore, most of the existing works model this task at a sentence level, restricting the context to a local scope. While it may be effective for short spans of text, for longer bodies of text such as news articles, it has often been observed that the arguments for an event do not necessarily occur in the same sentence as that containing an event trigger. To tackle the issue of argument scattering across sentences, the use of global context becomes imperative in this task. In our work, we propose an external knowledge aided approach to infuse document level event information to aid the extraction of complex event arguments. We develop a causal network for our event-annotated dataset by extracting relevant event causal structures from ConceptNet and phrases from Wikipedia. We use the extracted event causal features in a bi-directional transformer encoder to effectively capture long-range inter-sentence dependencies. We report the effectiveness of our proposed approach through both qualitative and quantitative analysis. In this task, we establish our findings on an event annotated dataset in 5 Indian languages. This dataset adds further complexity to the task by labeling arguments of entity type (like Time, Place) as well as more complex argument types (like Reason, After-Effect). Our approach achieves state-of-the-art performance across all the five languages. Since our work does not rely on any language specific features, it can be easily extended to other languages as well.</abstract>
      <url hash="5b0d9f23">2020.icon-main.38</url>
      <bibkey>kar-etal-2020-event</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="39">
      <title>Claim extraction from text using transfer learning.</title>
      <author><first>Acharya Ashish</first><last>Prabhakar</last></author>
      <author><first>Salar</first><last>Mohtaj</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>297–302</pages>
      <abstract>Building an end to end fake news detection system consists of detecting claims in text and later verifying them for their authenticity. Although most of the recent works have focused on political claims, fake news can also be propagated in the form of religious intolerance, conspiracy theories etc. Since there is a lack of training data specific to all these scenarios, we compiled a homogeneous and balanced dataset by combining some of the currently available data. Moreover, it is shown in the paper that how recent advancements in transfer learning can be leveraged to detect claims, in general. The obtained result shows that the recently developed transformers can transfer the tendency of research from claim detection to the problem of check worthiness of claims in domains of interest.</abstract>
      <url hash="414be08d">2020.icon-main.39</url>
      <bibkey>prabhakar-etal-2020-claim</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="40">
      <title><fixed-case>A</fixed-case>ssamese Word Sense Disambiguation using Genetic Algorithm</title>
      <author><first>Arjun</first><last>Gogoi</last></author>
      <author><first>Nomi</first><last>Baruah</last></author>
      <author><first>Shikhar Kr.</first><last>Sarma</last></author>
      <pages>303–307</pages>
      <abstract>Word sense disambiguation (WSD) is a problem to determine a word according to a context in which it occurs. There are plenty amount of works done in WSD for some languages such as English, but research work on Assamese WSD remains limited. It is a more exigent task because Assamese has an intrinsic complexity in its writing structure and ambiguity, such as syntactic, semantic, and anaphoric ambiguity levels.A novel unsupervised genetic word sense disambiguation algorithm is proposed in this paper. The algorithm first uses WordNet to extract all possible senses for a given ambiguous word, then a genetic algorithm is used taking Wu-Palmer’s similarity measure as the fitness function and calculating the similarity measure for all extracted senses. The winner sense which will have the highest score declared as he winner sense.</abstract>
      <url hash="3abf5c90">2020.icon-main.40</url>
      <bibkey>gogoi-etal-2020-assamese</bibkey>
    </paper>
    <paper id="41">
      <title>Free Word Order in <fixed-case>S</fixed-case>anskrit and Well-nestedness</title>
      <author><first>Sanal</first><last>Vikram</last></author>
      <author><first>Amba</first><last>Kulkarni</last></author>
      <pages>308–316</pages>
      <abstract>The common wisdom about Sanskrit is that it is free word order language. This word order poses challenges such as handling non-projectivity in parsing. The earlier works on the word order of Sanskrit have shown that there are syntactic structures in Sanskrit which cannot be covered under even the non-planarity. In this paper, we study these structures further to investigate if they can fall under well-nestedness or not. A small manually tagged corpus of the verses of Śrīmad-Bhagavad-Gītā was considered for this study. It was noticed that there are as many well-nested trees as there are ill-nested ones. From the linguistic point of view, we could get a list of relations that are involved in the planarity violations. All these relations had one thing in common - that they have unilateral expectancy. It was this loose binding, as against the mutual expectancy with certain other relations, that allowed them to cross the phrasal boundaries.</abstract>
      <url hash="278a10fe">2020.icon-main.41</url>
      <bibkey>vikram-kulkarni-2020-free</bibkey>
    </paper>
    <paper id="42">
      <title>A Multi-modal Personality Prediction System</title>
      <author><first>Chanchal</first><last>Suman</last></author>
      <author><first>Aditya</first><last>Gupta</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>317–322</pages>
      <abstract>Automatic prediction of personality traits has many real-life applications, e.g., in forensics, recommender systems, personalized services etc.. In this work, we have proposed a solution framework for solving the problem of predicting the personality traits of a user from videos. Ambient, facial and the audio features are extracted from the video of the user. These features are used for the final output prediction. The visual and audio modalities are combined in two different ways: averaging of predictions obtained from the individual modalities, and concatenation of features in multi-modal setting. The dataset released in Chalearn-16 is used for evaluating the performance of the system. Experimental results illustrate that it is possible to obtain better performance with a hand full of images, rather than using all the images present in the video</abstract>
      <url hash="18348f6a">2020.icon-main.42</url>
      <bibkey>suman-etal-2020-multi</bibkey>
    </paper>
    <paper id="43">
      <title><fixed-case>D</fixed-case>-Coref: A Fast and Lightweight Coreference Resolution Model using <fixed-case>D</fixed-case>istil<fixed-case>BERT</fixed-case></title>
      <author><first>Chanchal</first><last>Suman</last></author>
      <author><first>Jeetu</first><last>Kumar</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>323–328</pages>
      <abstract>Smart devices are often deployed in some edge-devices, which require quality solutions in limited amount of memory usage. In most of the user-interaction based smart devices, coreference resolution is often required. Keeping this in view, we have developed a fast and lightweight coreference resolution model which meets the minimum memory requirement and converges faster. In order to generate the embeddings for solving the task of coreference resolution, DistilBERT, a light weight BERT module is utilized. DistilBERT consumes less memory (only 60% of memory in comparison to BERT-based heavy model) and it is suitable for deployment in edge devices. DistilBERT embedding helps in 60% faster convergence with an accuracy compromise of 2.59%, and 6.49% with respect to its base model and current state-of-the-art, respectively.</abstract>
      <url hash="d07474db">2020.icon-main.43</url>
      <bibkey>suman-etal-2020-coref</bibkey>
    </paper>
    <paper id="44">
      <title>Semantic Slot Prediction on low corpus data using finite user defined list</title>
      <author><first>Bharatram</first><last>Natarajan</last></author>
      <author><first>Dharani</first><last>Simma</last></author>
      <author><first>Chirag</first><last>Singh</last></author>
      <author><first>Anish</first><last>Nediyanchath</last></author>
      <author><first>Sreoshi</first><last>Sengupta</last></author>
      <pages>329–333</pages>
      <abstract>Semantic slot prediction is one of the important task for natural language understanding (NLU). They depend on the quality and quantity of the human crafted training data, which affects model generalization. With the advent of voice assistants exposing AI platforms to third party developers, training data quality and quantity matters for any machine learning algorithm to learn and generalize properly.AI platforms provides provision to add custom external plist defined by the developers for the training data. Hence we are exploring dataset, called LowCorpusSlotData, containing low corpus training data with larger number of slots and significant test data. We also use external plist for the above dataset to aid in slot identification. We experimented using state of the art architectures like Bi-directional Encoder Representations from Transformers (BERT) with variants and Bi-directional Encoder with Custom Decoder. To address the low corpus problem, we propose a pipeline approach where we extract candidate slot information using the external plist extractor module and feed as input along with utterance.</abstract>
      <url hash="0122256c">2020.icon-main.44</url>
      <bibkey>natarajan-etal-2020-semantic</bibkey>
    </paper>
    <paper id="45">
      <title>Leveraging Latent Representations of Speech for <fixed-case>I</fixed-case>ndian Language Identification</title>
      <author><first>Samarjit</first><last>Karmakar</last></author>
      <author><first>P Radha</first><last>Krishna</last></author>
      <pages>334–340</pages>
      <abstract>Identification of the language spoken from speech utterances is an interesting task because of the diversity associated with different languages and human voices. Indian languages have diverse origins and identifying them from speech utterances would help several language recognition, translation and relationship mining tasks. The current approaches for tackling the problem of languages identification in the Indian context heavily use feature engineering and classical speech processing techniques. This is a bottleneck for language identification systems, as we require to exploit necessary features in speech, required for machine identification, which are learnt by a probabilistic framework, rather than handcrafted feature engineering. In this paper, we tackle the problem of language identification using latent representations learnt from speech using Variational Autoencoders (VAEs) and leverage the representations learnt to train sequence models. Our framework attains an accuracy of 89% in the identification of 8 well known Indian languages (namely Tamil, Telugu, Punjabi, Marathi, Gujarati, Hindi, Kannada and Bengali) from the CMU Indic Speech Database. The presented approach can be applied to several scenarios for speech processing by employing representation learning and leveraging them for sequence models.</abstract>
      <url hash="ff65f57e">2020.icon-main.45</url>
      <bibkey>karmakar-krishna-2020-leveraging</bibkey>
    </paper>
    <paper id="46">
      <title>Acoustic Analysis of Native (<fixed-case>L</fixed-case>1) <fixed-case>B</fixed-case>engali Speakers’ Phonological Realization of <fixed-case>E</fixed-case>nglish Lexical Stress Contrast</title>
      <author><first>Shambhu Nath</first><last>Saha</last></author>
      <author><first>Shyamal Kr.</first><last>Das Mandal</last></author>
      <pages>341–348</pages>
      <abstract>Acoustically, English lexical stress is multidimensional and involving manipulation of duration, intensity, fundamental frequency (F0) and vowel quality. The current study investigates the acquisition of English lexical stress by L1 Bengali speakers at the phonological level in terms of the properties of acoustic cues. For this purpose, this study compares 20 L1 Bengali speakers’ use of acoustic correlates for the production of English lexical stress in context sentence and neutral frame sentence. The result of this study showed that L1 Bengali speakers were not able to achieve neutral frame sentence like control over duration, intensity, F0 and to a limited extent vowel quality in context sentence. As a result, unlike neutral frame sentence, L1 Bengali speakers were not sensitive to English lexical stress contrast in context sentence. This analysis reveals that, the difference between the neutral frame and context sentences in terms of L1 Bengali speakers’ realization of phonology of English lexical stress contrast was probably due to the influence of Bengali phonology of lexical stress placement (restricted to the initial syllable of a word) on L1 Bengali speakers’ English speech.</abstract>
      <url hash="3c348002">2020.icon-main.46</url>
      <bibkey>saha-das-mandal-2020-acoustic</bibkey>
    </paper>
    <paper id="47">
      <title>Towards Performance Improvement in <fixed-case>I</fixed-case>ndian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Recognition</title>
      <author><first>Kinjal</first><last>Mistree</last></author>
      <author><first>Devendra</first><last>Thakor</last></author>
      <author><first>Brijesh</first><last>Bhatt</last></author>
      <pages>349–354</pages>
      <abstract>Sign language is a complete natural language used by deaf and dumb people. It has its own grammar and it differs with spoken language to a great extent. Since people without hearing and speech impairment lack the knowledge of the sign language, the deaf and dumb people find it difficult to communicate with them. The conception of system that would be able to translate the sign language into text would facilitate understanding of sign language without human interpreter. This paper describes a systematic approach that takes Indian Sign Language (ISL) video as input and converts it into text using frame sequence generator and image augmentation techniques. By incorporating these two concepts, we have increased dataset size and reduced overfitting. It is demonstrated that using simple image manipulation techniques and batch of shifted frames of videos, performance of sign language recognition can be significantly improved. Approach described in this paper achieves 99.57% accuracy on the dynamic gesture dataset of ISL.</abstract>
      <url hash="1e586eac">2020.icon-main.47</url>
      <bibkey>mistree-etal-2020-towards</bibkey>
    </paper>
    <paper id="48">
      <title>Question and Answer pair generation for <fixed-case>T</fixed-case>elugu short stories</title>
      <author><first>Meghana</first><last>Bommadi</last></author>
      <author><first>Shreya</first><last>Terupally</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>355–361</pages>
      <abstract>Question Answer pair generation is a task that has been worked upon by multiple researchers in many languages. It has been a topic of interest due to its extensive uses in different fields like self assessment, academics, business website FAQs etc. Many experiments were conducted on Question Answering pair generation in English, concentrating on basic Wh-questions with a rule-based approach. We have built the first hybrid machine learning and rule-based solution in Telugu which is efficient for short stories or short passages in children’s books. Our work covers the fundamental question forms with the question types: adjective, yes/no, adverb, verb, when, where, whose, quotative, and quantitative(how many/ how much). We constructed rules for question generation using POS tags and UD tags along with linguistic information of the surrounding context of the word.</abstract>
      <url hash="16839d0d">2020.icon-main.48</url>
      <bibkey>bommadi-etal-2020-question</bibkey>
    </paper>
    <paper id="49">
      <title>Detection of Similar Languages and Dialects Using Deep Supervised Autoencoder</title>
      <author><first>Shantipriya</first><last>Parida</last></author>
      <author><first>Esau</first><last>Villatoro-Tello</last></author>
      <author><first>Sajit</first><last>Kumar</last></author>
      <author><first>Maël</first><last>Fabien</last></author>
      <author><first>Petr</first><last>Motlicek</last></author>
      <pages>362–367</pages>
      <abstract>Language detection is considered a difficult task especially for similar languages, varieties, and dialects. With the growing number of online content in different languages, the need for reliable and robust language detection tools also increased. In this work, we use supervised autoencoders with a bayesian optimizer for language detection and highlights its efficiency in detecting similar languages with dialect variance in comparison to other state-of-the-art techniques. We evaluated our approach on multiple datasets (Ling10, Discriminating between Similar Language (DSL), and Indo-Aryan Language Identification (ILI)). Obtained results demonstrate that SAE are higly effective in detecting languages, up to a 100% accuracy in the Ling10. Similarly, we obtain a competitive performance in identifying similar languages, and dialects, 92% and 85% for DSL ans ILI datasets respectively.</abstract>
      <url hash="731c73c6">2020.icon-main.49</url>
      <bibkey>parida-etal-2020-detection</bibkey>
    </paper>
    <paper id="50">
      <title>Weak Supervision using Linguistic Knowledge for Information Extraction</title>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <author><first>Ankita</first><last>Jain</last></author>
      <author><first>Jyoti</first><last>Bhat</last></author>
      <author><first>Simi</first><last>Johnson</last></author>
      <pages>368–372</pages>
      <abstract>In this paper, we propose to use linguistic knowledge to automatically augment a small manually annotated corpus to obtain a large annotated corpus for training Information Extraction models. We propose a powerful patterns specification language for specifying linguistic rules for entity extraction. We define an Enriched Text Format (ETF) to represent rich linguistic information about a text in the form of XML-like tags. The patterns in our patterns specification language are then matched on the ETF text rather than raw text to extract various entity mentions. We demonstrate how an entity extraction system can be quickly built for a domain-specific entity type for which there are no readily available annotated datasets.</abstract>
      <url hash="4fcb584d">2020.icon-main.50</url>
      <bibkey>pawar-etal-2020-weak</bibkey>
    </paper>
    <paper id="51">
      <title>Leveraging Alignment and Phonology for low-resource Indic to <fixed-case>E</fixed-case>nglish Neural Machine Transliteration</title>
      <author><first>Parth</first><last>Patel</last></author>
      <author><first>Manthan</first><last>Mehta</last></author>
      <author><first>Pushpak</first><last>Bhattacharya</last></author>
      <author><first>Arjun</first><last>Atreya</last></author>
      <pages>373–378</pages>
      <abstract>In this paper we present a novel transliteration technique based on Orthographic Syllable(OS) segmentation for low-resource Indian languages (ILs). Given that alignment has produced promising results in Statistical Machine Transliteration systems and phonology plays an important role in transliteration, we introduce a new model which uses alignment representation similar to that of IBM model 3 to pre-process the tokenized input sequence and then use pre-trained source and target OS-embeddings for training. We apply our model for transliteration from ILs to English and report our accuracy based on Top-1 Exact Match. We also compare our accuracy with a previously proposed Phrase-Based model and report improvements.</abstract>
      <url hash="64e2c482">2020.icon-main.51</url>
      <attachment type="OptionalSupplementaryMaterial" hash="2fa6cde6">2020.icon-main.51.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>patel-etal-2020-leveraging</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>STHAL</fixed-case>: Location-mention Identification in Tweets of <fixed-case>I</fixed-case>ndian-context</title>
      <author><first>Kartik</first><last>Verma</last></author>
      <author><first>Shobhit</first><last>Sinha</last></author>
      <author><first>Md. Shad</first><last>Akhtar</last></author>
      <author><first>Vikram</first><last>Goyal</last></author>
      <pages>379–383</pages>
      <abstract>We investigate the problem of extracting Indian-locations from a given crowd-sourced textual dataset. The problem of extracting fine-grained Indian-locations has many challenges. One challenge in the task is to collect relevant dataset from the crowd-sourced platforms that contain locations. The second challenge lies in extracting the location entities from the collected data. We provide an in-depth review of the information collection process and our annotation guidelines such that a reliable dataset annotation is guaranteed. We evaluate many recent algorithms and models, including Conditional Random fields (CRF), Bi-LSTM-CNN and BERT (Bidirectional Encoder Representations from Transformers), on our developed dataset named . The study shows the best F1-score of 72.49% for BERT, followed by Bi-LSTM-CNN and CRF. As a result of our work, we prepare a publicly-available annotated dataset of Indian geolocations that can be used by the research community. Code and dataset are available at https://github.com/vkartik2k/STHAL.</abstract>
      <url hash="e109b666">2020.icon-main.52</url>
      <bibkey>verma-etal-2020-sthal</bibkey>
      <pwccode url="https://github.com/vkartik2k/sthal" additional="false">vkartik2k/sthal</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2002">CoNLL 2002</pwcdataset>
    </paper>
    <paper id="53">
      <title>On-Device detection of sentence completion for voice assistants with low-memory footprint</title>
      <author><first>Rahul</first><last>Kumar</last></author>
      <author><first>Vijeta</first><last>Gour</last></author>
      <author><first>Chandan</first><last>Pandey</last></author>
      <author><first>Godawari Sudhakar</first><last>Rao</last></author>
      <author><first>Priyadarshini</first><last>Pai</last></author>
      <author><first>Anmol</first><last>Bhasin</last></author>
      <author><first>Ranjan</first><last>Samal</last></author>
      <pages>384–392</pages>
      <abstract>Sentence completion detection (SCD) is an important task for various downstream Natural Language Processing (NLP) based applications. For NLP based applications, which use the Automatic Speech Recognition (ASR) from third parties as a service, SCD is essential to prevent unnecessary processing. Conventional approaches for SCD operate within the confines of sentence boundary detection using language models or sentence end detection using speech and text features. These have limitations in terms of relevant available data for training, performance within the memory and latency constraints, and the generalizability across voice assistant domains. In this paper, we propose a novel sentence completion detection method with low memory footprint for On-Device applications. We explore various sequence-level and sentence-level experiments using state-of-the-art Bi-LSTM and BERT based models for English language.</abstract>
      <url hash="7fc5f9bb">2020.icon-main.53</url>
      <bibkey>kumar-etal-2020-device</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="54">
      <title>Polarization and its Life on Social Media: A Case Study on Sabarimala and Demonetisation</title>
      <author><first>Ashutosh</first><last>Ranjan</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <author><first>Radhika</first><last>Krishnan</last></author>
      <pages>393–399</pages>
      <abstract>This paper is an attempt to study polarisation on social media data. We focus on two hugely controversial and talked about events in the Indian diaspora, namely 1) the Sabarimala Temple (located in Kerala, India) incident which became a nationwide controversy when two women under the age of 50 secretly entered the temple breaking a long standing temple rule that disallowed women of menstruating age (10-50) to enter the temple and 2) the Indian government’s move to demonetise all existing 500 and 1000 denomination banknotes, comprising of 86% of the currency in circulation, in November 2016. We gather tweets around these two events in various time periods, preprocess and annotate them with their sentiment polarity and emotional category, and analyse trends to help us understand changing polarity over time around controversial events. The tweets collected are in English, Hindi and code-mixed Hindi-English. Apart from the analysis on the annotated data, we also present the twitter data comprising a total of around 1.5 million tweets.</abstract>
      <url hash="6c3cd379">2020.icon-main.54</url>
      <attachment type="OptionalSupplementaryMaterial" hash="da6b1415">2020.icon-main.54.OptionalSupplementaryMaterial.txt</attachment>
      <bibkey>ranjan-etal-2020-polarization</bibkey>
    </paper>
    <paper id="55">
      <title>A Rule Based Lightweight <fixed-case>B</fixed-case>engali Stemmer</title>
      <author><first>Souvick</first><last>Das</last></author>
      <author><first>Rajat</first><last>Pandit</last></author>
      <author><first>Sudip Kumar</first><last>Naskar</last></author>
      <pages>400–408</pages>
      <abstract>In the field of Natural Language Processing (NLP) the process of stemming plays a significant role. Stemmer transforms an inflected word to its root form. Stemmer significantly increases the efficiency of Information Retrieval (IR) systems. It is a very basic yet fundamental text pre-processing task widely used in many NLP tasks. Several important works on stemming have been carried out by researchers in English and other major languages. In this paper, we study and review existing works on stemming in Bengali and other Indian languages. Finally, we propose a rule based approach that explores Bengali morphology and leverages WordNet to achieve better accuracy. Our algorithm produced stemming accuracy of 98.86% for Nouns and 99.75% for Verbs.</abstract>
      <url hash="51012610">2020.icon-main.55</url>
      <bibkey>das-etal-2020-rule</bibkey>
    </paper>
    <paper id="56">
      <title>End-to-End Automatic Speech Recognition for <fixed-case>G</fixed-case>ujarati</title>
      <author><first>Deepang</first><last>Raval</last></author>
      <author><first>Vyom</first><last>Pathak</last></author>
      <author><first>Muktan</first><last>Patel</last></author>
      <author><first>Brijesh</first><last>Bhatt</last></author>
      <pages>409–419</pages>
      <abstract>We present a novel approach for improving the performance of an End-to-End speech recognition system for the Gujarati language. We follow a deep learning based approach which includes Convolutional Neural Network (CNN), Bi-directional Long Short Term Memory (BiLSTM) layers, Dense layers, and Connectionist Temporal Classification (CTC) as a loss function. In order to improve the performance of the system with the limited size of the dataset, we present a combined language model (WLM and CLM) based prefix decoding technique and Bidirectional Encoder Representations from Transformers (BERT) based post-processing technique. To gain key insights from our Automatic Speech Recognition (ASR) system, we proposed different analysis methods. These insights help to understand our ASR system based on a particular language (Gujarati) as well as can govern ASR systems’ to improve the performance for low resource languages. We have trained the model on the Microsoft Speech Corpus, and we observe a 5.11% decrease in Word Error Rate (WER) with respect to base-model WER.</abstract>
      <url hash="24222724">2020.icon-main.56</url>
      <bibkey>raval-etal-2020-end</bibkey>
      <pwccode url="https://github.com/01-vyom/End_2_End_Automatic_Speech_Recognition_For_Gujarati" additional="false">01-vyom/End_2_End_Automatic_Speech_Recognition_For_Gujarati</pwccode>
    </paper>
    <paper id="57">
      <title>Deep Neural Model for <fixed-case>M</fixed-case>anipuri Multiword Named Entity Recognition with Unsupervised Cluster Feature</title>
      <author><first>Jimmy</first><last>Laishram</last></author>
      <author><first>Kishorjit</first><last>Nongmeikapam</last></author>
      <author><first>Sudip</first><last>Naskar</last></author>
      <pages>420–429</pages>
      <abstract>The recognition task of Multi-Word Named Entities (MNEs) in itself is a challenging task when the language is inflectional and agglutinative. Having breakthrough NLP researches with deep neural network and language modelling techniques, the applicability of such techniques/algorithms for Indian language like Manipuri remains unanswered. In this paper an attempt to recognize Manipuri MNE is performed using a Long Short Term Memory (LSTM) recurrent neural network model in conjunction with Part Of Speech (POS) embeddings. To further improve the classification accuracy, word cluster information using K-means clustering approach is added as a feature embedding. The cluster information is generated using a Skip-gram based words vector that contains the semantic and syntactic information of each word. The model so proposed does not use extensive language morphological features to elevate its accuracy. Finally the model’s performance is compared with the other machine learning based Manipuri MNE models.</abstract>
      <url hash="b8dea91c">2020.icon-main.57</url>
      <bibkey>laishram-etal-2020-deep</bibkey>
    </paper>
    <paper id="58">
      <title><fixed-case>S</fixed-case>c<fixed-case>AA</fixed-case>: A Dataset for Automated Short Answer Grading of Children’s free-text Answers in <fixed-case>H</fixed-case>indi and <fixed-case>M</fixed-case>arathi</title>
      <author><first>Dolly</first><last>Agarwal</last></author>
      <author><first>Somya</first><last>Gupta</last></author>
      <author><first>Nishant</first><last>Baghel</last></author>
      <pages>430–436</pages>
      <abstract>Automatic short answer grading (ASAG) techniques are designed to automatically assess short answers written in natural language. Apart from MCQs, evaluating free text answer is essential to assess the knowledge and understanding of children in the subject. But assessing descriptive answers in low resource languages in a linguistically diverse country like India poses significant hurdles. To solve this assessment problem and advance NLP research in regional Indian languages, we present the Science Answer Assessment (ScAA) dataset of children’s answers in the age group of 8-14. ScAA dataset is a 2-way (correct/incorrect) labeled dataset and contains 10,988 and 1,955 pairs of natural answers along with model answers for Hindi and Marathi respectively for 32 questions. We benchmark various state-of-the-art ASAG methods, and show the data presents a strong challenge for future research.</abstract>
      <url hash="770d9f1c">2020.icon-main.58</url>
      <bibkey>agarwal-etal-2020-scaa</bibkey>
    </paper>
    <paper id="59">
      <title>Exploring Pair-Wise <fixed-case>NMT</fixed-case> for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Kartheek</first><last>Akella</last></author>
      <author><first>Sai Himal</first><last>Allu</last></author>
      <author><first>Sridhar</first><last>Suresh Ragupathi</last></author>
      <author><first>Aman</first><last>Singhal</last></author>
      <author><first>Zeeshan</first><last>Khan</last></author>
      <author><first>C.v.</first><last>Jawahar</last></author>
      <author><first>Vinay</first><last>P. Namboodiri</last></author>
      <pages>437–443</pages>
      <abstract>In this paper, we address the task of improving pair-wise machine translation for specific low resource Indian languages. Multilingual NMT models have demonstrated a reasonable amount of effectiveness on resource-poor languages. In this work, we show that the performance of these models can be significantly improved upon by using back-translation through a filtered back-translation process and subsequent fine-tuning on the limited pair-wise language corpora. The analysis in this paper suggests that this method can significantly improve multilingual models’ performance over its baseline, yielding state-of-the-art results for various Indian languages.</abstract>
      <url hash="31a7114d">2020.icon-main.59</url>
      <bibkey>akella-etal-2020-exploring</bibkey>
    </paper>
    <paper id="60">
      <title>Only text? only image? or both? Predicting sentiment of internet memes</title>
      <author><first>Pranati</first><last>Behera</last></author>
      <author><first>Mamta</first><last>.</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>444–452</pages>
      <abstract>Nowadays, the spread of Internet memes on online social media platforms such as Instagram, Facebook, Reddit, and Twitter is very fast. Analyzing the sentiment of memes can provide various useful insights. Meme sentiment classification is a new area of research that is not explored yet. Recently SemEval provides a dataset for meme sentiment classification. As this dataset is highly imbalanced, we extend this dataset by annotating new instances and use a sampling strategy to build a meme sentiment classifier. We propose a multi-modal framework for meme sentiment classification by utilizing textual and visual features of the meme. We found that for meme sentiment classification, only textual or only visual features are not sufficient. Our proposed framework utilizes textual as well as visual features together. We propose to use the attention mechanism to improve meme classification performance. Our proposed framework achieves macro F1 and accuracy of 34.23 and 50.02, respectively. It increases the accuracy by 6.77 and 7.86 compared to only textual and visual features, respectively.</abstract>
      <url hash="346315f2">2020.icon-main.60</url>
      <bibkey>behera-etal-2020-text</bibkey>
    </paper>
    <paper id="61">
      <title>Towards <fixed-case>B</fixed-case>engali Word Embedding: Corpus Creation, Intrinsic and Extrinsic Evaluations</title>
      <author><first>Md. Rajib</first><last>Hossain</last></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last></author>
      <pages>453–459</pages>
      <abstract>Distributional word vector representation or word embedding has become an essential ingredient in many natural language processing (NLP) tasks such as machine translation, document classification, information retrieval and question answering. Investigation of embedding model helps to reduce the feature space and improves textual semantic as well as syntactic relations. This paper presents three embedding techniques (such as Word2Vec, GloVe, and FastText) with different hyperparameters implemented on a Bengali corpus consists of 180 million words. The performance of the embedding techniques is evaluated with extrinsic and intrinsic ways. Extrinsic performance evaluated by text classification, which achieved a maximum of 96.48% accuracy. Intrinsic performance evaluated by word similarity (e.g., semantic, syntactic and relatedness) and analogy tasks. The maximum Pearson (rˆ) correlation accuracy of 60.66% (Ssrˆ) achieved for semantic similarities and 71.64% (Syrˆ) for syntactic similarities whereas the relatedness obtained 79.80% (Rsrˆ). The semantic word analogy tasks achieved 44.00% of accuracy while syntactic word analogy tasks obtained 36.00%.</abstract>
      <url hash="9afdbed0">2020.icon-main.61</url>
      <bibkey>hossain-hoque-2020-towards</bibkey>
    </paper>
    <paper id="62">
      <title>Annotated Corpus of Tweets in <fixed-case>E</fixed-case>nglish from Various Domains for Emotion Detection</title>
      <author><first>Soumitra</first><last>Ghosh</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <author><first>Sriparna</first><last>Saha</last></author>
      <author><first>Vipin</first><last>Tyagi</last></author>
      <author><first>Alka</first><last>Kumar</last></author>
      <author><first>Shikha</first><last>Srivastava</last></author>
      <author><first>Nitish</first><last>Kumar</last></author>
      <pages>460–469</pages>
      <abstract>Emotion recognition is a very well-attended problem in Natural Language Processing (NLP). Most of the existing works on emotion recognition focus on the general domain and in some cases to specific domains like fairy tales, blogs, weather, Twitter etc. But emotion analysis systems in the domains of security, social issues, technology, politics, sports, etc. are very rare. In this paper, we create a benchmark setup for emotion recognition in these specialised domains. First, we construct a corpus of 18,921 tweets in English annotated with Paul Ekman’s six basic emotions (Anger, Disgust, Fear, Happiness, Sadness, Surprise) and a non-emotive class Others. Thereafter, we propose a deep neural framework to perform emotion recognition in an end-to-end setting. We build various models based on Convolutional Neural Network (CNN), Bi-directional Long Short Term Memory (Bi-LSTM), Bi-directional Gated Recurrent Unit (Bi-GRU). We propose a Hierarchical Attention-based deep neural network for Emotion Detection (HAtED). We also develop multiple systems by considering different sets of emotion classes for each system and report the detailed comparative analysis of the results. Experiments show the hierarchical attention-based model achieves best results among the considered baselines with accuracy of 69%.</abstract>
      <url hash="da8894c4">2020.icon-main.62</url>
      <bibkey>ghosh-etal-2020-annotated</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>P</fixed-case>hrase<fixed-case>O</fixed-case>ut: A Code Mixed Data Augmentation Method for <fixed-case>M</fixed-case>ultilingual<fixed-case>N</fixed-case>eural Machine Tranlsation</title>
      <author><first>Binu</first><last>Jasim</last></author>
      <author><first>Vinay</first><last>Namboodiri</last></author>
      <author><first>C V</first><last>Jawahar</last></author>
      <pages>470–474</pages>
      <abstract>Data Augmentation methods for Neural Machine Translation (NMT) such as back- translation (BT) and self-training (ST) are quite popular. In a multilingual NMT system, simply copying monolingual source sentences to the target (Copying) is an effective data augmentation method. Back-translation aug- ments parallel data by translating monolingual sentences in the target side to source language. In this work we propose to use a partial back- translation method in a multilingual setting. Instead of translating the entire monolingual target sentence back into the source language, we replace selected high confidence phrases only and keep the rest of the words in the target language itself. (We call this method PhraseOut). Our experiments on low resource multilingual translation models show that PhraseOut gives reasonable improvements over the existing data augmentation methods.</abstract>
      <url hash="f66c4d3a">2020.icon-main.63</url>
      <bibkey>jasim-etal-2020-phraseout</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>CLPLM</fixed-case>: Character Level Pretrained Language Model for <fixed-case>E</fixed-case>xtracting<fixed-case>S</fixed-case>upport Phrases for Sentiment Labels</title>
      <author><first>Raj</first><last>Pranesh</last></author>
      <author><first>Sumit</first><last>Kumar</last></author>
      <author><first>Ambesh</first><last>Shekhar</last></author>
      <pages>475–480</pages>
      <abstract>In this paper, we have designed a character-level pre-trained language model for extracting support phrases from tweets based on the sentiment label. We also propose a character-level ensemble model designed by properly blending Pre-trained Contextual Embeddings (PCE) models- RoBERTa, BERT, and ALBERT along with Neural network models- RNN, CNN and WaveNet at different stages of the model. For a given tweet and associated sentiment label, our model predicts the span of phrases in a tweet that prompts the particular sentiment in the tweet. In our experiments, we have explored various model architectures and configuration for both single as well as ensemble models. We performed a systematic comparative analysis of all the model’s performance based on the Jaccard score obtained. The best performing ensemble model obtained the highest Jaccard scores of 73.5, giving it a relative improvement of 2.4% over the best performing single RoBERTa based character-level model, at 71.5(Jaccard score).</abstract>
      <url hash="95bf6a88">2020.icon-main.64</url>
      <bibkey>pranesh-etal-2020-clplm</bibkey>
    </paper>
    <paper id="65">
      <title>Developing a <fixed-case>F</fixed-case>aroese <fixed-case>P</fixed-case>o<fixed-case>S</fixed-case>-tagging solution using <fixed-case>I</fixed-case>celandic methods</title>
      <author><first>Hinrik</first><last>Hafsteinsson</last></author>
      <author><first>Anton Karl</first><last>Ingason</last></author>
      <pages>481–490</pages>
      <abstract>We describe the development of a dedicated, high-accuracy part-of-speech (PoS) tagging solution for Faroese, a North Germanic language with about 50,000 speakers. To achieve this, a state-of-the-art neural PoS tagger for Icelandic, ABLTagger, was trained on a 100,000 word PoS-tagged corpus for Faroese, standardised with methods previously applied to Icelandic corpora. This tagger was supplemented with a novel Experimental Database of Faroese Inflection (EDFM), which contains morphological information on 67,488 Faroese words with about one million inflectional forms. This approach produced a PoS-tagging model for Faroese which achieves a 91.40% overall accuracy when evaluated with 10-fold cross validation, which is currently the highest reported accuracy for a dedicated Faroese PoS-tagger. The tagging model, morphological database, proposed revised PoS tagset for Faroese as well as a revised and standardised PoS tagged corpus are all presented as products of this project and are made available for use in further research in Faroese language technology</abstract>
      <url hash="aede1fc4">2020.icon-main.65</url>
      <bibkey>hafsteinsson-ingason-2020-developing</bibkey>
    </paper>
    <paper id="66">
      <title>Leveraging Multi-domain, Heterogeneous Data using Deep Multitask Learning for Hate Speech Detection</title>
      <author><first>Prashant</first><last>Kapil</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>491–500</pages>
      <abstract>With the exponential rise in user-generated web content on social media, the proliferation of abusive languages towards an individual or a group across the different sections of the internet is also rapidly increasing. It is very challenging for human moderators to identify the offensive contents and filter those out. Deep neural networks have shown promise with reasonable accuracy for hate speech detection and allied applications. However, the classifiers are heavily dependent on the size and quality of the training data. Such a high-quality large data set is not easy to obtain. Moreover, the existing data sets that have emerged in recent times are not created following the same annotation guidelines and are often concerned with different types and sub-types related to hate. To solve this data sparsity problem, and to obtain more global representative features, we propose a Convolution Neural Network (CNN) based multi-task learning models (MTLs) to leverage information from multiple sources. Empirical analysis performed on three benchmark datasets shows the efficacy of the proposed approach with the significant improvement in accuracy and F-score to obtain state-of-the-art performance with respect to the existing systems.</abstract>
      <url hash="9df153fe">2020.icon-main.66</url>
      <bibkey>kapil-ekbal-2020-leveraging</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
    </paper>
  </volume>
  <volume id="techdofication" ingest-date="2021-06-27">
    <meta>
      <booktitle>Proceedings of the 17th International Conference on Natural Language Processing (ICON): TechDOfication 2020 Shared Task</booktitle>
      <editor><first>Dipti Misra</first><last>Sharma</last></editor>
      <editor><first>Asif</first><last>Ekbal</last></editor>
      <editor><first>Karunesh</first><last>Arora</last></editor>
      <editor><first>Sudip Kumar</first><last>Naskar</last></editor>
      <editor><first>Dipankar</first><last>Ganguly</last></editor>
      <editor><first>Sobha</first><last>L</last></editor>
      <editor><first>Radhika</first><last>Mamidi</last></editor>
      <editor><first>Sunita</first><last>Arora</last></editor>
      <editor><first>Pruthwik</first><last>Mishra</last></editor>
      <editor><first>Vandan</first><last>Mujadia</last></editor>
      <publisher>NLP Association of India (NLPAI)</publisher>
      <address>Patna, India</address>
      <month>December</month>
      <year>2020</year>
      <url hash="dde39593">2020.icon-techdofication</url>
      <venue>icon</venue>
    </meta>
    <frontmatter>
      <url hash="7c752ea6">2020.icon-techdofication.0</url>
      <bibkey>icon-2020-international-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>MUCS</fixed-case>@<fixed-case>T</fixed-case>ech<fixed-case>DO</fixed-case>fication using <fixed-case>F</fixed-case>ine<fixed-case>T</fixed-case>uned Vectors and n-grams</title>
      <author><first>Fazlourrahman</first><last>Balouchzahi</last></author>
      <author><first>M D</first><last>Anusha</last></author>
      <author><first>H L</first><last>Shashirekha</last></author>
      <pages>1–5</pages>
      <abstract>The increase in domain specific text processing applications are demanding tools and techniques for domain specific Text Classification (TC) which may be helpful in many downstream applications like Machine Translation, Summarization, Question Answering etc. Further, many TC algorithms are applied on globally recognized languages like English giving less importance for local languages particularly Indian languages. To boost the research for technical domains and text processing activities in Indian languages, a shared task named ”TechDOfication2020” is organized by ICON’20. The objective of this shared task is to automatically identify the technical domain of a given text which provides information about coarse grained technical domains and fine grained subdomains in eight languages. To tackle this challenge we, team MUCS have proposed three models, namely, DL-FineTuned model applied for all subtasks, and VC-FineTuned and VC-ngrams models applied only for some subtasks. n-grams and word embedding with a step of fine-tuning are used as features and machine learning and deep learning algorithms are used as classifiers in the proposed models. The proposed models outperformed in most of subtasks and also obtained first rank in subTask1b (Bangla) and subTask1e (Malayalam) with f1 score of 0.8353 and 0.3851 respectively using DL-FineTuned model for both the subtasks.</abstract>
      <url hash="5262a0ea">2020.icon-techdofication.1</url>
      <bibkey>balouchzahi-etal-2020-mucs</bibkey>
    </paper>
    <paper id="2">
      <title>A Graph Convolution Network-based System for Technical Domain Identification</title>
      <author><first>Alapan</first><last>Kuila</last></author>
      <author><first>Ayan</first><last>Das</last></author>
      <author><first>Sudeshna</first><last>Sarkar</last></author>
      <pages>6–10</pages>
      <abstract>This paper presents the IITKGP contribution at the Technical DOmain Identification (TechDOfication) shared task at ICON 2020. In the preprocessing stage, we applied part-of-speech (PoS) taggers and dependency parsers to tag the data. We trained a graph convolution neural network (GCNN) based system that uses the tokens along with their PoS and dependency relations as features to identify the domain of a given document. We participated in the subtasks for coarse-grained domain classification in the English (Subtask 1a), Bengali (Subtask 1b) and Hindi language (Subtask 1d), and, the subtask for fine-grained domain classification task within Computer Science domain in English language (Subtask 2a).</abstract>
      <url hash="d55e638e">2020.icon-techdofication.2</url>
      <bibkey>kuila-etal-2020-graph</bibkey>
    </paper>
    <paper id="3">
      <title>Multichannel <fixed-case>LSTM</fixed-case>-<fixed-case>CNN</fixed-case> for <fixed-case>T</fixed-case>elugu Text Classification</title>
      <author><first>Sunil</first><last>Gundapu</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>11–15</pages>
      <abstract>With the instantaneous growth of text information, retrieving domain-oriented information from the text data has a broad range of applications in Information Retrieval and Natural language Processing. Thematic keywords give a compressed representation of the text. Usually, Domain Identification plays a significant role in Machine Translation, Text Summarization, Question Answering, Information Extraction, and Sentiment Analysis. In this paper, we proposed the Multichannel LSTM-CNN methodology for Technical Domain Identification for Telugu. This architecture was used and evaluated in the context of the ICON shared task “TechDOfication 2020” (task h), and our system got 69.9% of the F1 score on the test dataset and 90.01% on the validation set.</abstract>
      <url hash="36c96bb0">2020.icon-techdofication.3</url>
      <bibkey>gundapu-mamidi-2020-multichannel</bibkey>
    </paper>
    <paper id="4">
      <title>Multilingual Pre-Trained Transformers and Convolutional <fixed-case>NN</fixed-case> Classification Models for Technical Domain Identification</title>
      <author><first>Suman</first><last>Dowlagar</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>16–20</pages>
      <abstract>In this paper, we present a transfer learning system to perform technical domain identification on multilingual text data. We have submitted two runs, one uses the transformer model BERT, and the other uses XLM-ROBERTa with the CNN model for text classification. These models allowed us to identify the domain of the given sentences for the ICON 2020 shared Task, TechDOfication: Technical Domain Identification. Our system ranked the best for the subtasks 1d, 1g for the given TechDOfication dataset.</abstract>
      <url hash="4f0f85f6">2020.icon-techdofication.4</url>
      <bibkey>dowlagar-mamidi-2020-multilingual</bibkey>
    </paper>
    <paper id="5">
      <title>Technical Domain Identification using word2vec and <fixed-case>B</fixed-case>i<fixed-case>LSTM</fixed-case></title>
      <author><first>Koyel</first><last>Ghosh</last></author>
      <author><first>Dr. Apurbalal</first><last>Senapati</last></author>
      <author><first>Dr. Ranjan</first><last>Maity</last></author>
      <pages>21–26</pages>
      <abstract>Coarse-grained and Fine-grained classification tasks are mostly based on sentiment or basic emotion analysis. Now, switching from emotion and sentiment analysis to another domain, in this paper, we are going to work on technical domain identification. The task is to identify the technical domain of a given English text. In the case of Coarse-grained domain classification, such a piece of text provides information about specific Coarse-grained technical domains like Computer Science, Physics, Math, etc, and in Fine-grained domain classification, Fine-grained subdomains for Computer science domain, it can be like Artificial Intelligence, Algorithm, Computer Architecture, Computer Networks, Database Management system, etc. To do the task, Word2Vec skip-gram model is used for word embedding, later, applied the Bidirectional Long Short Term memory (BiLSTM) model to classify Coarse-grained domains and Fine-grained sub-domains. To evaluate the performance of the approached model accuracy, precision, recall, and F1-score have been applied.</abstract>
      <url hash="f04fb532">2020.icon-techdofication.5</url>
      <bibkey>ghosh-etal-2020-technical</bibkey>
    </paper>
    <paper id="6">
      <title>Automatic Technical Domain Identification</title>
      <author><first>Hema</first><last>Ala</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>27–30</pages>
      <abstract>In this paper we present two Machine Learning algorithms namely Stochastic Gradient Descent and Multi Layer Perceptron to Identify the technical domain of given text as such text provides information about the specific domain. We performed our experiments on Coarse-grained technical domains like Computer Science, Physics, Law, etc for English, Bengali, Gujarati, Hindi, Malayalam, Marathi, Tamil, and Telugu languages, and on fine-grained sub domains for Computer Science like Operating System, Computer Network, Database etc for only English language. Using TFIDF as a feature extraction method we show how both the machine learning models perform on the mentioned languages.</abstract>
      <url hash="9a31adee">2020.icon-techdofication.6</url>
      <bibkey>ala-sharma-2020-automatic</bibkey>
    </paper>
    <paper id="7">
      <title>Fine-grained domain classification using Transformers</title>
      <author><first>Akshat</first><last>Gahoi</last></author>
      <author><first>Akshat</first><last>Chhajer</last></author>
      <author><first>Dipti</first><last>Mishra Sharma</last></author>
      <pages>31–34</pages>
      <abstract>The introduction of transformers in 2017 and successively BERT in 2018 brought about a revolution in the field of natural language processing. Such models are pretrained on vast amounts of data, and are easily extensible to be used for a wide variety of tasks through transfer learning. Continual work on transformer based architectures has led to a variety of new models with state of the art results. RoBERTa(CITATION) is one such model, which brings about a series of changes to the BERT architecture and is capable of producing better quality embeddings at an expense of functionality. In this paper, we attempt to solve the well known text classification task of fine-grained domain classification using BERT and RoBERTa and perform a comparative analysis of the same. We also attempt to evaluate the impact of data preprocessing specially in the context of fine-grained domain classification. The results obtained outperformed all the other models at the ICON TechDOfication 2020 (subtask-2a) Fine-grained domain classification task and ranked first. This proves the effectiveness of our approach.</abstract>
      <url hash="8e538aa1">2020.icon-techdofication.7</url>
      <bibkey>gahoi-etal-2020-fine</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>T</fixed-case>ech<fixed-case>T</fixed-case>ex<fixed-case>C</fixed-case>: Classification of Technical Texts using Convolution and Bidirectional Long Short Term Memory Network</title>
      <author><first>Omar</first><last>Sharif</last></author>
      <author><first>Eftekhar</first><last>Hossain</last></author>
      <author><first>Mohammed Moshiul</first><last>Hoque</last></author>
      <pages>35–39</pages>
      <abstract>This paper illustrates the details description of technical text classification system and its results that developed as a part of participation in the shared task TechDofication 2020. The shared task consists of two sub-tasks: (i) first task identify the coarse-grained technical domain of given text in a specified language and (ii) the second task classify a text of computer science domain into fine-grained sub-domains. A classification system (called ‘TechTexC’) is developed to perform the classification task using three techniques: convolution neural network (CNN), bidirectional long short term memory (BiLSTM) network, and combined CNN with BiLSTM. Results show that CNN with BiLSTM model outperforms the other techniques concerning task-1 of sub-tasks (a, b, c and g) and task-2a. This combined model obtained f1 scores of 82.63 (sub-task a), 81.95 (sub-task b), 82.39 (sub-task c), 84.37 (sub-task g), and 67.44 (task-2a) on the development dataset. Moreover, in the case of test set, the combined CNN with BiLSTM approach achieved that higher accuracy for the subtasks 1a (70.76%), 1b (79.97%), 1c (65.45%), 1g (49.23%) and 2a (70.14%).</abstract>
      <url hash="249ce306">2020.icon-techdofication.8</url>
      <bibkey>sharif-etal-2020-techtexc</bibkey>
    </paper>
    <paper id="9">
      <title>An Attention Ensemble Approach for Efficient Text Classification of <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Atharva</first><last>Kulkarni</last></author>
      <author><first>Amey</first><last>Hengle</last></author>
      <author><first>Rutuja</first><last>Udyawar</last></author>
      <pages>40–46</pages>
      <abstract>The recent surge of complex attention-based deep learning architectures has led to extraordinary results in various downstream NLP tasks in the English language. However, such research for resource-constrained and morphologically rich Indian vernacular languages has been relatively limited. This paper proffers a solution for the TechDOfication 2020 subtask-1f: which focuses on the coarse-grained technical domain identification of short text documents in Marathi, a Devanagari script-based Indian language. Availing the large dataset at hand, a hybrid CNN-BiLSTM attention ensemble model is proposed that competently combines the intermediate sentence representations generated by the convolutional neural network and the bidirectional long short-term memory, leading to efficient text classification. Experimental results show that the proposed model outperforms various baseline machine learning and deep learning models in the given task, giving the best validation accuracy of 89.57% and f1-score of 0.8875. Furthermore, the solution resulted in the best system submission for this subtask, giving a test accuracy of 64.26% and f1-score of 0.6157, transcending the performances of other teams as well as the baseline system given by the organizers of the shared task.</abstract>
      <url hash="fbd3918a">2020.icon-techdofication.9</url>
      <bibkey>kulkarni-etal-2020-attention</bibkey>
    </paper>
  </volume>
  <volume id="termtraction" ingest-date="2021-06-27">
    <meta>
      <booktitle>Proceedings of the 17th International Conference on Natural Language Processing (ICON): TermTraction 2020 Shared Task</booktitle>
      <editor><first>Dipti Misra</first><last>Sharma</last></editor>
      <editor><first>Asif</first><last>Ekbal</last></editor>
      <editor><first>Karunesh</first><last>Arora</last></editor>
      <editor><first>Sudip Kumar</first><last>Naskar</last></editor>
      <editor><first>Dipankar</first><last>Ganguly</last></editor>
      <editor><first>Sobha</first><last>L</last></editor>
      <editor><first>Radhika</first><last>Mamidi</last></editor>
      <editor><first>Sunita</first><last>Arora</last></editor>
      <editor><first>Pruthwik</first><last>Mishra</last></editor>
      <editor><first>Vandan</first><last>Mujadia</last></editor>
      <publisher>NLP Association of India (NLPAI)</publisher>
      <address>Patna, India</address>
      <month>December</month>
      <year>2020</year>
      <url hash="cf80b319">2020.icon-termtraction</url>
      <venue>icon</venue>
    </meta>
    <frontmatter>
      <url hash="38cc8282">2020.icon-termtraction.0</url>
      <bibkey>icon-2020-international-natural-language</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Graph Based Automatic Domain Term Extraction</title>
      <author><first>Hema</first><last>Ala</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>1–4</pages>
      <abstract>We present a Graph Based Approach to automatically extract domain specific terms from technical domains like Biochemistry, Communication, Computer Science and Law. Our approach is similar to TextRank with an extra post-processing step to reduce the noise. We performed our experiments on the mentioned domains provided by ICON TermTraction - 2020 shared task. Presented precision, recall and f1-score for all experiments. Further, it is observed that our method gives promising results without much noise in domain terms.</abstract>
      <url hash="8b15281a">2020.icon-termtraction.1</url>
      <bibkey>ala-sharma-2020-graph</bibkey>
    </paper>
    <paper id="2">
      <title>Unsupervised Technical Domain Terms Extraction using Term Extractor</title>
      <author><first>Suman</first><last>Dowlagar</last></author>
      <author><first>Radhika</first><last>Mamidi</last></author>
      <pages>5–8</pages>
      <abstract>Terminology extraction, also known as term extraction, is a subtask of information extraction. The goal of terminology extraction is to extract relevant words or phrases from a given corpus automatically. This paper focuses on the unsupervised automated domain term extraction method that considers chunking, preprocessing, and ranking domain-specific terms using relevance and cohesion functions for ICON 2020 shared task 2: TermTraction.</abstract>
      <url hash="30df0c5c">2020.icon-termtraction.2</url>
      <bibkey>dowlagar-mamidi-2020-unsupervised</bibkey>
    </paper>
    <paper id="3">
      <title>N-Grams <fixed-case>T</fixed-case>ext<fixed-case>R</fixed-case>ank A Novel Domain Keyword Extraction Technique</title>
      <author><first>Saransh</first><last>Rajput</last></author>
      <author><first>Akshat</first><last>Gahoi</last></author>
      <author><first>Manvith</first><last>Reddy</last></author>
      <author><first>Dipti</first><last>Mishra Sharma</last></author>
      <pages>9–12</pages>
      <abstract>The rapid growth of the internet has given us a wealth of information and data spread across the web. However, as the data begins to grow we simultaneously face the grave problem of an <i>Information Explosion</i>. An abundance of data can lead to large scale data management problems as well as the loss of the true meaning of the data. In this paper, we present an advanced domain specific keyword extraction algorithm in order to tackle this problem of paramount importance. Our algorithm is based on a modified version of TextRank algorithm - an algorithm based on PageRank to successfully determine the keywords from a domain specific document. Furthermore, this paper proposes a modification to the traditional TextRank algorithm that takes into account bigrams and trigrams and returns results with an extremely high precision. We observe how the precision and f1-score of this model outperforms other models in many domains and the recall can be easily increased by increasing the number of results without affecting the precision. We also discuss about the future work of extending the same algorithm to Indian languages.</abstract>
      <url hash="c689263e">2020.icon-termtraction.3</url>
      <bibkey>rajput-etal-2020-n</bibkey>
    </paper>
  </volume>
  <volume id="adapmt" ingest-date="2021-06-27">
    <meta>
      <booktitle>Proceedings of the 17th International Conference on Natural Language Processing (ICON): Adap-MT 2020 Shared Task</booktitle>
      <editor><first>Dipti Misra</first><last>Sharma</last></editor>
      <editor><first>Asif</first><last>Ekbal</last></editor>
      <editor><first>Karunesh</first><last>Arora</last></editor>
      <editor><first>Sudip Kumar</first><last>Naskar</last></editor>
      <editor><first>Dipankar</first><last>Ganguly</last></editor>
      <editor><first>Sobha</first><last>L</last></editor>
      <editor><first>Radhika</first><last>Mamidi</last></editor>
      <editor><first>Sunita</first><last>Arora</last></editor>
      <editor><first>Pruthwik</first><last>Mishra</last></editor>
      <editor><first>Vandan</first><last>Mujadia</last></editor>
      <publisher>NLP Association of India (NLPAI)</publisher>
      <address>Patna, India</address>
      <month>December</month>
      <year>2020</year>
      <url hash="37e55ecd">2020.icon-adapmt</url>
      <venue>icon</venue>
    </meta>
    <frontmatter>
      <url hash="07914496">2020.icon-adapmt.0</url>
      <bibkey>icon-2020-international-natural-language-processing</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>JUNLP</fixed-case>@<fixed-case>ICON</fixed-case>2020: Low Resourced Machine Translation for Indic Languages</title>
      <author><first>Sainik</first><last>Mahata</last></author>
      <author><first>Dipankar</first><last>Das</last></author>
      <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
      <pages>1–5</pages>
      <abstract>In the current work, we present the description of the systems submitted to a machine translation shared task organized by ICON 2020: 17th International Conference on Natural Language Processing. The systems were developed to show the capability of general domain machine translation when translating into Indic languages, English-Hindi, in our case. The paper shows the training process and quantifies the performance of two state-of-the-art translation systems, viz., Statistical Machine Translation and Neural Machine Translation. While Statistical Machine Translation systems work better in a low-resource setting, Neural Machine Translation systems are able to generate sentences that are fluent in nature. Since both these systems have contrasting advantages, a hybrid system, incorporating both, was also developed to leverage all the strong points. The submitted systems garnered BLEU scores of 8.701943312, 0.6361336198, and 11.78873307 respectively and the scores of the hybrid system helped us to the fourth spot in the competition leaderboard.</abstract>
      <url hash="d6850c93">2020.icon-adapmt.1</url>
      <bibkey>mahata-etal-2020-junlp</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>A</fixed-case>dap<fixed-case>NMT</fixed-case> : Neural Machine Translation with Technical Domain Adaptation for Indic Languages</title>
      <author><first>Hema</first><last>Ala</last></author>
      <author><first>Dipti</first><last>Sharma</last></author>
      <pages>6–10</pages>
      <abstract>Adapting new domain is highly challenging task for Neural Machine Translation (NMT). In this paper we show the capability of general domain machine translation when translating into Indic languages (English - Hindi , English - Telugu and Hindi - Telugu), and low resource domain adaptation of MT systems using existing general parallel data and small in domain parallel data for AI and Chemistry Domains. We carried out our experiments using Byte Pair Encoding(BPE) as it solves rare word problems. It has been observed that with addition of little amount of in-domain data to the general data improves the BLEU score significantly.</abstract>
      <url hash="3926881f">2020.icon-adapmt.2</url>
      <bibkey>ala-sharma-2020-adapnmt</bibkey>
    </paper>
    <paper id="3">
      <title>Domain Adaptation of <fixed-case>NMT</fixed-case> models for <fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi Machine Translation Task : <fixed-case>A</fixed-case>dap<fixed-case>MT</fixed-case> Shared Task <fixed-case>ICON</fixed-case> 2020</title>
      <author><first>Ramchandra</first><last>Joshi</last></author>
      <author><first>Rusbabh</first><last>Karnavat</last></author>
      <author><first>Kaustubh</first><last>Jirapure</last></author>
      <author><first>Raviraj</first><last>Joshi</last></author>
      <pages>11–16</pages>
      <abstract>Recent advancements in Neural Machine Translation (NMT) models have proved to produce a state of the art results on machine translation for low resource Indian languages. This paper describes the neural machine translation systems for the English-Hindi language presented in AdapMT Shared Task ICON 2020. The shared task aims to build a translation system for Indian languages in specific domains like Artificial Intelligence (AI) and Chemistry using a small in-domain parallel corpus. We evaluated the effectiveness of two popular NMT models i.e, LSTM, and Transformer architectures for the English-Hindi machine translation task based on BLEU scores. We train these models primarily using the out of domain data and employ simple domain adaptation techniques based on the characteristics of the in-domain dataset. The fine-tuning and mixed-domain data approaches are used for domain adaptation. The system achieved the second-highest score on chemistry and general domain En-Hi translation task and the third-highest score on the AI domain En-Hi translation task.</abstract>
      <url hash="d119a2b2">2020.icon-adapmt.3</url>
      <bibkey>joshi-etal-2020-domain</bibkey>
    </paper>
    <paper id="4">
      <title>Terminology-Aware Sentence Mining for <fixed-case>NMT</fixed-case> Domain Adaptation: <fixed-case>ADAPT</fixed-case>’s Submission to the Adap-<fixed-case>MT</fixed-case> 2020 <fixed-case>E</fixed-case>nglish-to-<fixed-case>H</fixed-case>indi <fixed-case>AI</fixed-case> Translation Shared Task</title>
      <author><first>Rejwanul</first><last>Haque</last></author>
      <author><first>Yasmin</first><last>Moslem</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <pages>17–23</pages>
      <abstract>This paper describes the ADAPT Centre’s submission to the Adap-MT 2020 AI Translation Shared Task for English-to-Hindi. The neural machine translation (NMT) systems that we built to translate AI domain texts are state-of-the-art Transformer models. In order to improve the translation quality of our NMT systems, we made use of both in-domain and out-of-domain data for training and employed different fine-tuning techniques for adapting our NMT systems to this task, e.g. mixed fine-tuning and on-the-fly self-training. For this, we mined parallel sentence pairs and monolingual sentences from large out-of-domain data, and the mining process was facilitated through automatic extraction of terminology from the in-domain data. This paper outlines the experiments we carried out for this task and reports the performance of our NMT systems on the evaluation test set.</abstract>
      <url hash="7578da73">2020.icon-adapmt.4</url>
      <bibkey>haque-etal-2020-terminology</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/indicnlp-corpus">IndicNLP Corpus</pwcdataset>
    </paper>
    <paper id="5">
      <title><fixed-case>MUCS</fixed-case>@Adap-<fixed-case>MT</fixed-case> 2020: Low Resource Domain Adaptation for Indic Machine Translation</title>
      <author><first>Asha</first><last>Hegde</last></author>
      <author><first>H.l.</first><last>Shashirekha</last></author>
      <pages>24–28</pages>
      <abstract>Machine Translation (MT) is the task of automatically converting the text in source language to text in target language by preserving the meaning. MT usually require large corpus for training the translation models. Due to scarcity of resources very less attention is given to translating into low resource languages and in particular into Indic languages. In this direction, a shared task called “Adap-MT 2020: Low Resource Domain Adaptation for Indic Machine Translation” is organized to illustrate the capability of general domain MT when translating into Indic languages and low resource domain adaptation of MT systems. In this paper, we, team MUCS, describe a simple word extraction based domain adaptation approach applied to English-Hindi MT only. MT in the proposed model is carried out using Open-NMT - a popular Neural Machine Translation tool. A general domain corpus is built effectively combining the available English-Hindi corpora and removing the duplicate sentences. Further, domain specific corpus is updated by extracting the sentences from generic corpus that contains the words given in the domain specific corpus. The proposed model exhibited satisfactory results for small domain specific AI and CHE corpora provided by the organizers in terms of BLEU score with 1.25 and 2.72 respectively. Further, this methodology is quite generic and can easily be extended to other low resource language pairs as well.</abstract>
      <url hash="d63bf14e">2020.icon-adapmt.5</url>
      <bibkey>hegde-shashirekha-2020-mucs</bibkey>
    </paper>
  </volume>
  <volume id="demos" ingest-date="2021-06-27">
    <meta>
      <booktitle>Proceedings of the 17th International Conference on Natural Language Processing (ICON): System Demonstrations</booktitle>
      <editor><first>Vishal</first><last>Goyal</last></editor>
      <editor><first>Asif</first><last>Ekbal</last></editor>
      <publisher>NLP Association of India (NLPAI)</publisher>
      <address>Patna, India</address>
      <month>DECEMBER</month>
      <year>2020</year>
      <url hash="8d4df6fe">2020.icon-demos</url>
      <venue>icon</venue>
    </meta>
    <frontmatter>
      <url hash="d49599b4">2020.icon-demos.0</url>
      <bibkey>icon-2020-international-natural-language-processing-icon</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Demonstration of a Literature Based Discovery System based on Ontologies, Semantic Filters and Word Embeddings for the Raynaud Disease-Fish Oil Rediscovery</title>
      <author><first>Toby</first><last>Reed</last></author>
      <author><first>Vassilis</first><last>Cutsuridis</last></author>
      <pages>1–3</pages>
      <abstract>A novel literature-based discovery system based on UMLS Ontologies, Semantic Filters, Statistics, and Word Embed-dings was developed and validated against the well-established Raynaud’s disease – Fish Oil discovery by min-ing different size and specificity corpora of Pubmed titles and abstracts. Results show an ‘inverse effect’ between open ver-sus closed discovery search modes. In open discovery, a more general and bigger corpus (Vascular disease or Peri-vascular disease) produces better results than a more specific and smaller in size corpus (Raynaud disease), whereas in closed discovery, the exact opposite is true.</abstract>
      <url hash="99888929">2020.icon-demos.1</url>
      <bibkey>reed-cutsuridis-2020-demonstration</bibkey>
    </paper>
    <paper id="2">
      <title>Development of Hybrid Algorithm for Automatic Extraction of Multiword Expressions from Monolingual and Parallel Corpus of <fixed-case>E</fixed-case>nglish and <fixed-case>P</fixed-case>unjabi</title>
      <author><first>Kapil Dev</first><last>Goyal</last></author>
      <author><first>Vishal</first><last>Goyal</last></author>
      <pages>4–6</pages>
      <abstract>Identification and extraction of Multiword Expressions (MWEs) is very hard and challenging task in various Natural Language processing applications like Information Retrieval (IR), Information Extraction (IE), Question-Answering systems, Speech Recognition and Synthesis, Text Summarization and Machine Translation (MT). Multiword Expressions are two or more consecutive words but treated as a single word and actual meaning this expression cannot be extracted from meaning of individual word. If any systems recognized this expression as separate words, then results of system will be incorrect. Therefore it is mandatory to identify these expressions to improve the result of the system. In this report, our main focus is to develop an automated tool to extract Multiword Expressions from monolingual and parallel corpus of English and Punjabi. In this tool, Rule based approach, Linguistic approach, statistical approach, and many more approaches were used to identify and extract MWEs from monolingual and parallel corpus of English and Punjabi and achieved more than 90% f-score value in some types of MWEs.</abstract>
      <url hash="d9f6e280">2020.icon-demos.2</url>
      <bibkey>goyal-goyal-2020-development</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>P</fixed-case>unjabi to <fixed-case>E</fixed-case>nglish Bidirectional <fixed-case>NMT</fixed-case> System</title>
      <author><first>Kamal</first><last>Deep</last></author>
      <author><first>Ajit</first><last>Kumar</last></author>
      <author><first>Vishal</first><last>Goyal</last></author>
      <pages>7–9</pages>
      <abstract>Machine Translation is ongoing research for last few decades. Today, Corpus-based Machine Translation systems are very popular. Statistical Machine Translation and Neural Machine Translation are based on the parallel corpus. In this research, the Punjabi to English Bidirectional Neural Machine Translation system is developed. To improve the accuracy of the Neural Machine Translation system, Word Embedding and Byte Pair Encoding is used. The claimed BLEU score is 38.30 for Punjabi to English Neural Machine Translation system and 36.96 for English to Punjabi Neural Machine Translation system.</abstract>
      <url hash="4e8853b6">2020.icon-demos.3</url>
      <bibkey>deep-etal-2020-punjabi</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>EXTRACTING</fixed-case> <fixed-case>PARALLEL</fixed-case> <fixed-case>PHRASES</fixed-case> <fixed-case>FROM</fixed-case> <fixed-case>COMPARABLE</fixed-case> <fixed-case>ENGLISH</fixed-case> <fixed-case>AND</fixed-case> <fixed-case>PUNJABI</fixed-case> <fixed-case>CORPORA</fixed-case> <fixed-case>USING</fixed-case> <fixed-case>AN</fixed-case> <fixed-case>INTEGRATED</fixed-case> <fixed-case>APPROACH</fixed-case></title>
      <author><first>Manpreet Singh</first><last>Lehal</last></author>
      <author><first>Vishal</first><last>Goyal</last></author>
      <pages>10–12</pages>
      <abstract>Machine translation from English to Indian languages is always a difficult task due to the unavailability of a good quality corpus and morphological richness in the Indian languages. For a system to produce better translations, the size of the corpus should be huge. We have employed three similarity and distance measures for the research and developed a software to extract parallel data from comparable corpora automatically with high precision using minimal resources. The software works upon four algorithms. The three algorithms have been used for finding Cosine Similarity, Euclidean Distance Similarity and Jaccard Similarity. The fourth algorithm is to integrate the outputs of the three algorithms in order to improve the efficiency of the system.</abstract>
      <url hash="87f8eace">2020.icon-demos.4</url>
      <bibkey>lehal-goyal-2020-extracting</bibkey>
    </paper>
    <paper id="5">
      <title>A <fixed-case>SANSKRIT</fixed-case> <fixed-case>TO</fixed-case> <fixed-case>HINDI</fixed-case> <fixed-case>LANGUAGE</fixed-case> <fixed-case>MACHINE</fixed-case> <fixed-case>TRANSLATOR</fixed-case> <fixed-case>USING</fixed-case> <fixed-case>RULE</fixed-case> <fixed-case>BASED</fixed-case> <fixed-case>APPROACH</fixed-case></title>
      <author><first>Prateek</first><last>Agrawal</last></author>
      <author><first>Vishu</first><last>Madaan</last></author>
      <pages>13–15</pages>
      <abstract>Hindi and Sanskrit both the languages are having the same script i.e. Devnagari Script which results in few basic similarities in their grammar rules. As we know that Hindi ranks fourth in terms of speaker’s size in the world and over 60 million people in India are Hindi internet users. In India itself, there are approximately 120 languages and 240 mother tongues but hardly a few languages are recognized worldwide while the others are losing their existence in society day by day. Likewise, Sanskrit is one of those important languages that are being ignored in society. As per census report of India in 2001, less than 15000 citizens have returned Sanskrit as their Mother tongue or preferred medium of communication. A key reason behind poor acceptance of Sanskrit is due to language barrier among Indian masses and lack of knowledge about this language among people. Therefore, our attempt is just to connect a big crowd of Hindi users with Sanskrit language and make them familiar at least with the basics of Sanskrit. We developed a translation tool that parses Sanskrit words (prose) one by one and translate it into equivalent Hindi language in step by step manner: (i) We created a strong Hindi-Sanskrit corpus that can deal with Sanskrit words effectively and efficiently. (ii) We proposed an algorithm to stem Sanskrit word that chops off the starts/ends of words to find the root words in the form of nouns and verbs. (iii) After stemming, we developed an algorithm to search the equivalent Hindi meaning of stemmed words from the corpus-based on semantic analysis. (iv)We developed an algorithm to implement semantic analysis to translate words that help the tool to identify required parameter details like gender, number, case etc. (v) Next, we developed an algorithm for discourse integration to dis-join each translated sentence based on subject/noun dependency. (vi) Next, we implemented pragmatic analysis algorithm that ensures the meaningful validation of these translated Hindi sentences syntactically and semantically. (vii) We further extended our work to summarize the translated text story and suggest a suitable heading/title. For this, we referred ripple down rule-based parts of speech (RDR-POS) Tagger for word tagging in the POS tagger corpora. (viii) We proposed a title generation algorithm which suggests some suitable title of the translated text. (ix) Finally, we assembled all phases to one translation tool that takes a story of maximum one hundred words as input and translates it into equivalent Hindi language.</abstract>
      <url hash="74ed9471">2020.icon-demos.5</url>
      <attachment type="OptionalSupplementaryMaterial" hash="646d61da">2020.icon-demos.5.OptionalSupplementaryMaterial.zip</attachment>
      <bibkey>agrawal-madaan-2020-sanskrit</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>U</fixed-case>rdu To <fixed-case>P</fixed-case>unjabi Machine Translation System</title>
      <author><first>Umrinder Pal</first><last>Singh</last></author>
      <author><first>Vishal</first><last>Goyal</last></author>
      <author><first>Gurpreet</first><last>Lehal</last></author>
      <pages>16–18</pages>
      <abstract>Machine Translation is a popular area of NLP research field. There are various approaches to develop a machine translation system like Rule-Based, Statistical, Neural and Hybrid. A rule-Based system is based on grammatical rules and uses bilingual lexicons. Statistical and Neural use the large parallel corpus for training the respective models. Where the Hybrid MT system is a mixture of different approaches. In these days the corpus-based machine translation system is quite popular in NLP research area. But these models demands huge parallel corpus. In this research, we have used a hybrid approach to develop Urdu to Punjabi machine translation system. In the developed system, statistical and various sub-system based on the linguistic rule has been used. The system yield 80% accuracy on a different set of the sentence related to domains like Political, Entertainment, Tourism, Sports and Health. The complete system has been developed in a C#.NET programming language.</abstract>
      <url hash="cdbc71aa">2020.icon-demos.6</url>
      <bibkey>singh-etal-2020-urdu</bibkey>
    </paper>
    <paper id="7">
      <title>The <fixed-case>H</fixed-case>indi to <fixed-case>D</fixed-case>ogri Machine Translation System</title>
      <author><first>Preeti</first><last>Dubey</last></author>
      <pages>19–20</pages>
      <abstract>The Hindi to Dogri Machine translation system is a rule-based MT developed and copyrighted by GoI in 2014. It is the first system developed to convert Hindi text into Dogri (the regional language of Jammu). The system is developed using ASP.Net and the databases are in MS-Access. This Machine Translation system accepts Hindi text as input and provides Dogri text as output in Unicode.</abstract>
      <url hash="3985ad6b">2020.icon-demos.7</url>
      <bibkey>dubey-2020-hindi</bibkey>
    </paper>
    <paper id="8">
      <title>Opinion Mining System for Processing <fixed-case>H</fixed-case>indi Text for Home Remedies Domain</title>
      <author><first>Arpana</first><last>Prasad</last></author>
      <author><first>Neeraj</first><last>Sharma</last></author>
      <author><first>Shubhangi</first><last>Sharma</last></author>
      <pages>21–23</pages>
      <abstract>Opinion Mining (OM) is a field of study in Computer Science that deals with development of software applications related to text classifications and summarizations. Researchers working in this field contribute lexical resources, computing methodologies, text classification approaches, and summarization modules to perform OM tasks across various domains and different languages. Lexical and computational components developed for an Opinion Mining System that processes Hindi text taken from weblogs are presented in the paper for the demonstration. Text chosen for processing are the ones demonstrating cause and effect relationship between related entities ‘Food’ and ‘Health Issues’. The work is novel and lexical resources developed are useful in current research and may be of importance for future research in the field. The resources are developed for an algorithm ‘A’ such that for a sentence ‘Y’ which is a domain specific sentence from weblogs in Hindi, A(Y) returns a set F, HI, p, s such that F is a subset of set, FOOD=set of word or phrases in Hindi used for an edible item and HI is a subset of set, HEALTH_ISSUE= set of word or phrases in Hindi used for a part of body composition ‘BODY_COMPONENT’ UNION set of word or phrases in Hindi used for a health problem a human being face ‘HEALTH_PROBLEM’. Element ‘p’ takes numeric value ‘1’ or ‘-1’ where value ‘1’ means that from the text ‘Y’, algorithm ‘A’ computationally derived that the food entities mentioned in set ‘F’ have a positive effect in health issues mentioned in set ‘HI’ and the numeric value ‘-1’ means that the food entities in set ‘F’ have a negative effect in health issues in set ‘HI’. The element‘s’ may take value ‘1’ or ‘2’ indicating that the strength of polarity ‘p’ is medium or strong.</abstract>
      <url hash="1187407a">2020.icon-demos.8</url>
      <bibkey>prasad-etal-2020-opinion</bibkey>
    </paper>
    <paper id="9">
      <title>Sentiment Analysis of <fixed-case>E</fixed-case>nglish-<fixed-case>P</fixed-case>unjabi Code-Mixed Social Media Content</title>
      <author><first>Mukhtiar</first><last>Singh</last></author>
      <author><first>Vishal</first><last>Goyal</last></author>
      <pages>24–25</pages>
      <abstract>Sentiment analysis is a field of study for analyzing people’s emotions, such as Nice, Happy, ਦੁਖੀ (sad), changa (Good), etc. towards the entities and attributes expressed in written text. It noticed that, on microblogging websites (Facebook, YouTube, Twitter ), most people used more than one language to express their emotions. The change of one language to another language within the same written text is called code-mixing. In this research, we gathered the English-Punjabi code-mixed corpus from micro-blogging websites. We have performed language identification of code-mix text, which includes Phonetic Typing, Abbreviation, Wordplay, Intentionally misspelled words and Slang words. Then we performed tokenization of English and Punjabi language words consisting of different spellings. Then we performed sentiment analysis based on the above text based on the lexicon approach. The dictionary created for English Punjabi code mixed consists of opinionated words. The opinionated words are then categorized into three categories i.e. positive words list, negative words list, and neutral words list. The rest of the words are being stored in an unsorted word list. By using the N-gram approach, a statistical technique is applied at sentence level sentiment polarity of the English-Punjabi code-mixed dataset. Our results show an accuracy of 83% with an F-1 measure of 77%.</abstract>
      <url hash="d40185e2">2020.icon-demos.9</url>
      <bibkey>singh-goyal-2020-sentiment</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>NLP</fixed-case> Tools for <fixed-case>K</fixed-case>hasi, a low resource language</title>
      <author><first>Medari</first><last>Tham</last></author>
      <pages>26–27</pages>
      <abstract>Khasi is an Austro Asiatic language spoken by one of the tribes in Meghalaya, and parts of Assam and Bangladesh. The fact that some NLP tools for Khasi are now available online for testing purposes is the culmination of the arduous investment in time and effort. Initially when work for Khasi was initiated, resources for Khasi, such as tagset and annotated corpus or any NLP tools, were nonexistent. As part of the author’s ongoing work for her doctoral program, currently, the resources for Khasi that are in place are the BIS (Bureau of Indian Standards) tagset for Khasi, a 90k annotated corpus, and NLP tools such as POS (parts of speech) taggers and shallow parsers. These mentioned tools are highlighted in this demonstration paper.</abstract>
      <url hash="8fd1849b">2020.icon-demos.10</url>
      <bibkey>tham-2020-nlp</bibkey>
    </paper>
    <paper id="11">
      <title>A Chatbot in <fixed-case>M</fixed-case>alayalam using Hybrid Approach</title>
      <author><first>Praveen</first><last>Prasannan</last></author>
      <author><first>Stephy</first><last>Joseph</last></author>
      <author><first>Rajeev R</first><last>R</last></author>
      <pages>28–29</pages>
      <abstract>Chatbot is defined as one of the most advanced and promising expressions of interaction between humans and machines. They are sometimes called as digital assistants that can analyze human capabilities. There are so many chatbots already developed in English with supporting libraries and packages. But to customize these engines in other languages is a tedious process. Also there are many barriers to train these engines with other morphologically rich languages. Artificial Intelligence (AI) based or Machine Learning based Chatbots can answer complex ambiguous questions. The AI chatbots are capable of creating replies from scratch using Natural Language Processing techniques. Both categories have their advantages and disadvantages. Rule based chatbots can give more reliable and grammatically correct answers but fail to respond to questions outside their knowledge base. On the other hand, machine learning based chatbots need a vast amount of learning data and necessitated continuous improvement to the data-base to improve the cognitive capabilities.A hybrid chatbot employs the concepts of both AI and rule based bots, it can handle situations with both the approaches. One of the biggest threat faced by the society during the Corona pandemic was Mis-Information, Dis-information and Mal- information. Government wanted to establish a single source of truth, where the public can rely for authentic information. To support the cause and to fulfill the need to support the general public due to the rapid spread of COVID-19 Pandemic during the months of February and March 2020, ICFOSS has developed an interactive bot which is based on ‘hybrid technology’ and interacts with the people in regional language (Malayalam).</abstract>
      <url hash="9c943aa7">2020.icon-demos.11</url>
      <bibkey>prasannan-etal-2020-chatbot</bibkey>
    </paper>
    <paper id="12">
      <title>Language Identification and Normalization of Code Mixed <fixed-case>E</fixed-case>nglish and <fixed-case>P</fixed-case>unjabi Text</title>
      <author><first>Neetika</first><last>Bansal</last></author>
      <author><first>Dr. Vishal</first><last>Goyal</last></author>
      <author><first>Dr. Simpel</first><last>Rani</last></author>
      <pages>30–31</pages>
      <abstract>Code mixing is prevalent when users use two or more languages while communicating. It becomes more complex when users prefer romanized text to Unicode typing. The automatic processing of social media data has become one of popular areas of interest. Especially since COVID period the involvement of youngsters has attained heights. Walking with the pace our intended software deals with Language Identification and Normalization of English and Punjabi code mixed text. The software designed follows a pipeline which includes data collection, pre-processing, language identification, handling Out of Vocabulary words, normalization and transliteration of English- Punjabi text. After applying five-fold cross validation on the corpus, the accuracy of 96.8% is achieved on a trained dataset of around 80025 tokens. After the prediction of the tags: the slangs, contractions in the user input are normalized to their standard form. In addition, the words with Punjabi as predicted tags are transliterated to Punjabi.</abstract>
      <url hash="aac87fc7">2020.icon-demos.12</url>
      <bibkey>bansal-etal-2020-language</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>P</fixed-case>unjabi to <fixed-case>U</fixed-case>rdu Machine Translation System</title>
      <author><first>Nitin</first><last>Bansal</last></author>
      <author><first>Ajit</first><last>Kumar</last></author>
      <pages>32–34</pages>
      <abstract>Abstract Development of Machine Translation System (MTS) for any language pair is a challenging task for several reasons. Lack of lexical resources for any language is one of the major issue arise while developing MTS using that language. For example, during the development of Punjabi to Urdu MTS, many issues were recognized while preparing lexical resources for both the language. Since there is no machine readable dictionary is available for Punjabi to Urdu which can be directly used for translation; however various dictionaries are available to explain the meaning of word. Along with this, handling of OOV (out of vocabulary words), handling of multiple sense Punjabi word in Urdu, identification of proper nouns, identification of collocations in the source sentence i.e. Punjabi sentence in our case, are the issues which we are facing during development of this system. Since MTSs are in great demand from the last one decade and are being widely used in applications such as in case of smart phones. Therefore, development of such a system becomes more demanding and more users friendly. There usage is mainly in large scale translations, automated translations; act as an instrument to bridge a digital divide.</abstract>
      <url hash="51081841">2020.icon-demos.13</url>
      <bibkey>bansal-kumar-2020-punjabi</bibkey>
    </paper>
    <paper id="14">
      <title>Design and Implementation of Anaphora Resolution in <fixed-case>P</fixed-case>unjabi Language</title>
      <author><first>Kawaljit</first><last>Kaur</last></author>
      <author><first>Dr Vishal</first><last>Goyal</last></author>
      <author><first>Dr Kamlesh</first><last>Dutta</last></author>
      <pages>35–36</pages>
      <abstract>Natural Language Processing (NLP) is the most attention-grabbing field of artificial intelligence. It focuses on the interaction between humans and computers. Through NLP we can make thec omputers recognize, decode and deduce the meaning ofhuman dialect splendidly. But there are numerous difficulties that are experienced in NLP and, Anaphora is one such issue. Anaphora emerges often in composed writings and oral talk. Anaphora Resolution is the process of finding antecedent of corresponding referent and is required in different applications of NLP.Appreciable works have been accounted for anaphora in English and different languages, but no work has been done in Punjabi Language. Through this paper we are enumerating the introduction of Anaphora Resolution in Punjabi language. The accuracy achieved for the system is 47%.</abstract>
      <url hash="34753411">2020.icon-demos.14</url>
      <bibkey>kaur-etal-2020-design</bibkey>
    </paper>
    <paper id="15">
      <title>Airport Announcement System for Deaf</title>
      <author><first>Rakesh</first><last>Kumar</last></author>
      <author><first>Vishal</first><last>Goyal</last></author>
      <author><first>Lalit</first><last>Goyal</last></author>
      <pages>37–39</pages>
      <abstract>People belonging to hearing-impaired community feels very uncomfortable while travelling or visiting at airport without the help of human interpreter. Hearing-impaired people are not able to hear any announcements made at airport like which flight heading to which destination. They remain ignorant about the choosing of gate number or counter number without the help of interpreter. Even they cannot find whether flight is on time, delayed or cancelled. The Airport Announcement System for Deaf is a rule-based MT developed. It is the first system developed in the domain of public places to translate all the announcements used at Airport into Indian Sign Language (ISL) synthetic animations. The system is developed using Python and Flask Framework. This Machine Translation system accepts announcements in the form of English text as input and produces Indian Sign Language (ISL) synthetic animations as output.</abstract>
      <url hash="d72d6e29">2020.icon-demos.15</url>
      <bibkey>kumar-etal-2020-airport</bibkey>
    </paper>
    <paper id="16">
      <title>Railway Stations Announcement System for Deaf</title>
      <author><first>Rakesh</first><last>Kumar</last></author>
      <author><first>Vishal</first><last>Goyal</last></author>
      <author><first>Lalit</first><last>Goyal</last></author>
      <pages>40–42</pages>
      <abstract>People belonging to hearing-impaired community feels very uncomfortable while travelling or visiting at Railway Stations without the help of human interpreter. Hearing-impaired people are not able to hear any announcements made at Railway Stations like which train heading to which destination. They remain ignorant about the choosing of platform number or counter number without the help of interpreter. Even they cannot find whether train is on time, delayed or cancelled. The Railway Stations Announcement System for Deaf is a rule-based MT developed. It is the first system developed in the domain of public places to translate all the announcements used at Railway Stations into Indian Sign Language (ISL) synthetic animations. The system is developed using Python and Flask Framework. This Machine Translation system accepts announcements in the form of English text as input and produces Indian Sign Language (ISL) synthetic animations as output.</abstract>
      <url hash="54bcb948">2020.icon-demos.16</url>
      <bibkey>kumar-etal-2020-railway</bibkey>
    </paper>
    <paper id="17">
      <title>Automatic Translation of Complex <fixed-case>E</fixed-case>nglish Sentences to <fixed-case>I</fixed-case>ndian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Synthetic Video Animations</title>
      <author><first>Deepali</first><last>Goyal</last></author>
      <author><first>Vishal</first><last>Goyal</last></author>
      <author><first>Lalit</first><last>Goyal</last></author>
      <pages>43–45</pages>
      <abstract>Sign Language is the natural way of expressing thoughts and feelings for the deaf community. Sign language is a diagrammatic and non-verbal language used by the impaired community to communicate their feeling to their lookalike one. Today we live in the era of technological development, owing to which instant communication is quite easy but even then, a lot of work needs to be done in the field of Sign language automation to improve the quality of life among the deaf community. The traditional approaches used for representing the signs are in the form of videos or text that are expensive, time-consuming, and are not easy to use. In this research work, an attempt is made for the conversion of Complex and Compound English sentences to Indian Sign Language (ISL) using synthetic video animations. The translation architecture includes a parsing module that parses the input complex or compound English sentences to their simplified versions by using complex to simple and compound to simple English grammar rules respectively. The simplified sentence is then forwarded to the conversion segment that rearranges the words of the English language into its corresponding ISL using the devised grammar rules. The next segment constitutes the removal of unwanted words or stop words. This segment gets an input sentence generated by ISL grammar rules. Unwanted or unnecessary words are eliminated by this segment. This removal is important because ISL needs only a meaningful sentence rather than unnecessary usage of linking verbs, helping verbs, and so on. After parsing through the eliminator segment, the sentence is sent to the concordance segment. This segment checks each word in the sentence and translates them into their respective lemma. Lemma is the basic requiring node of each word because sign language makes use of basic words irrespective of other languages that make use of gerund, suffixes, three forms of verbs, different kinds of nouns, adjectives, pronouns in their sentence theory. All the words of the sentence are checked in the lexicon which contains the English word with its HamNoSys notation and the words that are not in the lexicon are replaced by their synonym. The words of the sentence are replaced by their counter HamNoSys code. In case the word is not present in the lexicon, the HamNoSys code will be taken for each alphabet of the word in sequence. The HamNoSys code is converted into the SiGML tags (a form of XML tags) and these SiGML tags are then sent to the animation module which converts the SiGML code into the synthetic animation using avatar (computer-generated animation character).</abstract>
      <url hash="e55e6e01">2020.icon-demos.17</url>
      <bibkey>goyal-etal-2020-automatic</bibkey>
    </paper>
    <paper id="18">
      <title>Plagiarism Detection Tool for <fixed-case>I</fixed-case>ndian Languages with Special focus on <fixed-case>H</fixed-case>indi and <fixed-case>P</fixed-case>unjabi</title>
      <author><first>Vishal</first><last>Goyal</last></author>
      <author><first>Rajeev</first><last>Puri</last></author>
      <author><first>Jitesh</first><last>Pubreja</last></author>
      <author><first>Jaswinder</first><last>Singh</last></author>
      <pages>46–47</pages>
      <abstract>Plagiarism is closely linked with Intellectual Property Rights and Copyrights laws, both of which have been formed to protect the ownership of the concept. Most of the available tools for detecting plagiarism when tested with sample Punjabi text, failed to recognise the Punjabi text and the ones, which supported Punjabi text, did a simple string comparison for detecting the suspected copy-paste plagiarism, ignoring the other forms of plagiarism such as word switching, synonym replacement and sentence switching etc.</abstract>
      <url hash="ba977d2c">2020.icon-demos.18</url>
      <bibkey>goyal-etal-2020-plagiarism</bibkey>
    </paper>
  </volume>
  <volume id="workshop" ingest-date="2021-06-27">
    <meta>
      <booktitle>Proceedings of the Workshop on Joint NLP Modelling for Conversational AI @ ICON 2020</booktitle>
      <editor><first>Praveen Kumar G</first><last>S</last></editor>
      <editor><first>Siddhartha</first><last>Mukherjee</last></editor>
      <editor><first>Ranjan</first><last>Samal</last></editor>
      <publisher>NLP Association of India (NLPAI)</publisher>
      <address>Patna, India</address>
      <month>December</month>
      <year>2020</year>
      <url hash="09fbefa9">2020.icon-workshop</url>
      <venue>icon</venue>
    </meta>
    <frontmatter>
      <url hash="9c6f186a">2020.icon-workshop.0</url>
      <bibkey>icon-2020-joint</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Neighbor Contextual Information Learners for Joint Intent and Slot Prediction</title>
      <author><first>Bharatram</first><last>Natarajan</last></author>
      <author><first>Gaurav</first><last>Mathur</last></author>
      <author><first>Sameer</first><last>Jain</last></author>
      <pages>1–9</pages>
      <abstract>Intent Identification and Slot Identification aretwo important task for Natural Language Understanding(NLU). Exploration in this areahave gained significance using networks likeRNN, LSTM and GRU. However, modelscontaining the above modules are sequentialin nature, which consumes lot of resourceslike memory to train the model in cloud itself.With the advent of many voice assistantsdelivering offline solutions for manyapplications, there is a need for finding replacementfor such sequential networks. Explorationin self-attention, CNN modules hasgained pace in the recent times. Here we exploreCNN based models like Trellis and modifiedthe architecture to make it bi-directionalwith fusion techniques. In addition, we proposeCNN with Self Attention network calledNeighbor Contextual Information Projector usingMulti Head Attention (NCIPMA) architecture.These architectures beat state of the art inopen source datasets like ATIS, SNIPS.</abstract>
      <url hash="e889ac02">2020.icon-workshop.1</url>
      <bibkey>natarajan-etal-2020-neighbor</bibkey>
    </paper>
    <paper id="2">
      <title>Unified Multi Intent Order and Slot Prediction using Selective Learning Propagation</title>
      <author><first>Bharatram</first><last>Natarajan</last></author>
      <author><first>Priyank</first><last>Chhipa</last></author>
      <author><first>Kritika</first><last>Yadav</last></author>
      <author><first>Divya Verma</first><last>Gogoi</last></author>
      <pages>10–18</pages>
      <abstract>Natural Language Understanding (NLU) involves two important task namely Intent Determination(ID) and Slot Filling (SF). With recent advancements in Intent Determination and Slot Filling tasks, explorations on handling of multiple intent information in a single utterance is increasing to make the NLU more conversation-based rather than command execution-based. Many have proven this task with huge multi-intent training data. In addition, lots of research have addressed multi intent problem only. The problem of multi intent also poses the challenge of addressing the order of execution of intents found. Hence, we are proposing a unified architecture to address multi-intent detection, associated slotsdetection and order of execution of found intents using low proportion multi-intent corpusin the training data. This architecture consists of Multi Word Importance relation propagator using Multi-Head GRU and Importance learner propagator module using self-attention. This architecture has beaten state-of-the-art by 2.58% on the MultiIntentData dataset.</abstract>
      <url hash="b88f2254">2020.icon-workshop.2</url>
      <bibkey>natarajan-etal-2020-unified</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>E</fixed-case>mp<fixed-case>L</fixed-case>ite: A Lightweight Sequence Labeling Model for Emphasis Selection of Short Texts</title>
      <author><first>Vibhav</first><last>Agarwal</last></author>
      <author><first>Sourav</first><last>Ghosh</last></author>
      <author><first>Kranti</first><last>Ch</last></author>
      <author><first>Bharath</first><last>Challa</last></author>
      <author><first>Sonal</first><last>Kumari</last></author>
      <author><first/><last>Harshavardhana</last></author>
      <author><first>Barath Raj</first><last>Kandur Raja</last></author>
      <pages>19–26</pages>
      <abstract>Word emphasis in textual content aims at conveying the desired intention by changing the size, color, typeface, style (bold, italic, etc.), and other typographical features. The emphasized words are extremely helpful in drawing the readers’ attention to specific information that the authors wish to emphasize. However, performing such emphasis using a soft keyboard for social media interactions is time-consuming and has an associated learning curve. In this paper, we propose a novel approach to automate the emphasis word detection on short written texts. To the best of our knowledge, this work presents the first lightweight deep learning approach for smartphone deployment of emphasis selection. Experimental results show that our approach achieves comparable accuracy at a much lower model size than existing models. Our best lightweight model has a memory footprint of 2.82 MB with a matching score of 0.716 on SemEval-2020 public benchmark dataset.</abstract>
      <url hash="de471aeb">2020.icon-workshop.3</url>
      <bibkey>agarwal-etal-2020-emplite</bibkey>
    </paper>
    <paper id="4">
      <title>Named Entity Popularity Determination using Ensemble Learning</title>
      <author><first>Vikram</first><last>Karthikeyan</last></author>
      <author><first>B Shrikara</first><last>Varna</last></author>
      <author><first>Amogha</first><last>Hegde</last></author>
      <author><first>Govind</first><last>Satwani</last></author>
      <author><first>Shambhavi</first><last>B R</last></author>
      <author><first>Jayarekha</first><last>P</last></author>
      <author><first>Ranjan</first><last>Samal</last></author>
      <pages>27–32</pages>
      <abstract>Determining the popularity of a Named Entity after completion of Named Entity Recognition (NER) task finds many applications. This work studies Named Entities of Music and Movie domains and solves the problem considering relevant 11 features. Decision Trees and Random Forests approaches were applied on the dataset and the latter algorithm resulted in acceptable accuracy.</abstract>
      <url hash="c8d255f9">2020.icon-workshop.4</url>
      <bibkey>karthikeyan-etal-2020-named</bibkey>
    </paper>
    <paper id="5">
      <title>Optimized Web-Crawling of Conversational Data from Social Media and Context-Based Filtering</title>
      <author><first>Annapurna P</first><last>Patil</last></author>
      <author><first>Rajarajeswari</first><last>Subramanian</last></author>
      <author><first>Gaurav</first><last>Karkal</last></author>
      <author><first>Keerthana</first><last>Purushotham</last></author>
      <author><first>Jugal</first><last>Wadhwa</last></author>
      <author><first>K Dhanush</first><last>Reddy</last></author>
      <author><first>Meer</first><last>Sawood</last></author>
      <pages>33–39</pages>
      <abstract>Building Chabot’s requires a large amount of conversational data. In this paper, a web crawler is designed to fetch multi-turn dialogues from websites such as Twitter, YouTube and Reddit in the form of a JavaScript Object Notation (JSON) file. Tools like Twitter Application Programming Interface (API), LXML Library, and JSON library are used to crawl Twitter, YouTube and Reddit to collect conversational chat data. The data obtained in a raw form cannot be used directly as it will have only text metadata such as author or name, time to provide more information on the chat data being scraped. The data collected has to be formatted for proper use case and the JSON library of python allows us to format the data easily. The scraped dialogues are further filtered based on the context of a search keyword without introducing bias and with flexible strictness of classification.</abstract>
      <url hash="2233ef28">2020.icon-workshop.5</url>
      <bibkey>patil-etal-2020-optimized</bibkey>
    </paper>
    <paper id="6">
      <title>A character representation enhanced on-device Intent Classification</title>
      <author><first>Sudeep Deepak</first><last>Shivnikar</last></author>
      <author><first>Himanshu</first><last>Arora</last></author>
      <author><first>Harichandana</first><last>B S S</last></author>
      <pages>40–46</pages>
      <abstract>Intent classification is an important task in natural language understanding systems. Existing approaches have achieved perfect scores on the benchmark datasets. However they are not suitable for deployment on low-resource devices like mobiles, tablets, etc. due to their massive model size. Therefore, in this paper, we present a novel light-weight architecture for intent classification that can run efficiently on a device. We use character features to enrich the word representation. Our experiments prove that our proposed model outperforms existing approaches and achieves state-of-the-art results on benchmark datasets. We also report that our model has tiny memory footprint of ~5 MB and low inference time of ~2 milliseconds, which proves its efficiency in a resource-constrained environment.</abstract>
      <url hash="826854d9">2020.icon-workshop.6</url>
      <bibkey>shivnikar-etal-2020-character</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
  </volume>
</collection>
