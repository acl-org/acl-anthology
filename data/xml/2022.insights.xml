<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.insights">
  <volume id="1" ingest-date="2022-05-15" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Insights from Negative Results in NLP</booktitle>
      <editor><first>Shabnam</first><last>Tafreshi</last></editor>
      <editor><first>João</first><last>Sedoc</last></editor>
      <editor><first>Anna</first><last>Rogers</last></editor>
      <editor><first>Aleksandr</first><last>Drozd</last></editor>
      <editor><first>Anna</first><last>Rumshisky</last></editor>
      <editor><first>Arjun</first><last>Akula</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Dublin, Ireland</address>
      <month>May</month>
      <year>2022</year>
      <url hash="986cd1e3">2022.insights-1</url>
      <venue>insights</venue>
    </meta>
    <frontmatter>
      <url hash="be06b363">2022.insights-1.0</url>
      <bibkey>insights-2022-insights</bibkey>
    </frontmatter>
    <paper id="1">
      <title>On Isotropy Calibration of Transformer Models</title>
      <author><first>Yue</first><last>Ding</last></author>
      <author><first>Karolis</first><last>Martinkus</last></author>
      <author><first>Damian</first><last>Pascual</last></author>
      <author><first>Simon</first><last>Clematide</last></author>
      <author><first>Roger</first><last>Wattenhofer</last></author>
      <pages>1-9</pages>
      <abstract>Different studies of the embedding space of transformer models suggest that the distribution of contextual representations is highly anisotropic - the embeddings are distributed in a narrow cone. Meanwhile, static word representations (e.g., Word2Vec or GloVe) have been shown to benefit from isotropic spaces. Therefore, previous work has developed methods to calibrate the embedding space of transformers in order to ensure isotropy. However, a recent study (Cai et al. 2021) shows that the embedding space of transformers is locally isotropic, which suggests that these models are already capable of exploiting the expressive capacity of their embedding space. In this work, we conduct an empirical evaluation of state-of-the-art methods for isotropy calibration on transformers and find that they do not provide consistent improvements across models and tasks. These results support the thesis that, given the local isotropy, transformers do not benefit from additional isotropy calibration.</abstract>
      <url hash="f0fc70dc">2022.insights-1.1</url>
      <bibkey>ding-etal-2022-isotropy</bibkey>
      <doi>10.18653/v1/2022.insights-1.1</doi>
      <video href="2022.insights-1.1.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="2">
      <title>Do Dependency Relations Help in the Task of Stance Detection?</title>
      <author><first>Alessandra Teresa</first><last>Cignarella</last></author>
      <author><first>Cristina</first><last>Bosco</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>10-17</pages>
      <abstract>In this paper we present a set of multilingual experiments tackling the task of Stance Detection in five different languages: English, Spanish, Catalan, French and Italian. Furthermore, we study the phenomenon of stance with respect to six different targets – one per language, and two different for Italian – employing a variety of machine learning algorithms that primarily exploit morphological and syntactic knowledge as features, represented throughout the format of Universal Dependencies. Results seem to suggest that the methodology employed is not beneficial per se, but might be useful to exploit the same features with a different methodology.</abstract>
      <url hash="b50c8b6c">2022.insights-1.2</url>
      <bibkey>cignarella-etal-2022-dependency</bibkey>
      <doi>10.18653/v1/2022.insights-1.2</doi>
      <video href="2022.insights-1.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Evaluating the Practical Utility of Confidence-score based Techniques for Unsupervised Open-world Classification</title>
      <author><first>Sopan</first><last>Khosla</last></author>
      <author><first>Rashmi</first><last>Gangadharaiah</last></author>
      <pages>18-23</pages>
      <abstract>Open-world classification in dialog systems require models to detect open intents, while ensuring the quality of in-domain (ID) intent classification. In this work, we revisit methods that leverage distance-based statistics for unsupervised out-of-domain (OOD) detection. We show that despite their superior performance on threshold-independent metrics like AUROC on test-set, threshold values chosen based on the performance on a validation-set do not generalize well to the test-set, thus resulting in substantially lower performance on ID or OOD detection accuracy and F1-scores. Our analysis shows that this lack of generalizability can be successfully mitigated by setting aside a hold-out set from validation data for threshold selection (sometimes achieving relative gains as high as 100%). Extensive experiments on seven benchmark datasets show that this fix puts the performance of these methods at par with, or sometimes even better than, the current state-of-the-art OOD detection techniques.</abstract>
      <url hash="1c3fcf07">2022.insights-1.3</url>
      <bibkey>khosla-gangadharaiah-2022-evaluating</bibkey>
      <doi>10.18653/v1/2022.insights-1.3</doi>
      <video href="2022.insights-1.3.mp4"/>
    </paper>
    <paper id="4">
      <title>Extending the Scope of Out-of-Domain: Examining <fixed-case>QA</fixed-case> models in multiple subdomains</title>
      <author><first>Chenyang</first><last>Lyu</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <pages>24-37</pages>
      <abstract>Past work that investigates out-of-domain performance of QA systems has mainly focused on general domains (e.g. news domain, wikipedia domain), underestimating the importance of subdomains defined by the internal characteristics of QA datasets. In this paper, we extend the scope of “out-of-domain” by splitting QA examples into different subdomains according to their internal characteristics including question type, text length, answer position. We then examine the performance of QA systems trained on the data from different subdomains. Experimental results show that the performance of QA systems can be significantly reduced when the train data and test data come from different subdomains. These results question the generalizability of current QA systems in multiple subdomains, suggesting the need to combat the bias introduced by the internal characteristics of QA datasets.</abstract>
      <url hash="60ac7946">2022.insights-1.4</url>
      <bibkey>lyu-etal-2022-extending</bibkey>
      <doi>10.18653/v1/2022.insights-1.4</doi>
      <video href="2022.insights-1.4.mp4"/>
      <pwccode url="https://github.com/lyuchenyang/analysing-question-answering-data" additional="false">lyuchenyang/analysing-question-answering-data</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="5">
      <title>What Do You Get When You Cross Beam Search with Nucleus Sampling?</title>
      <author><first>Uri</first><last>Shaham</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>38-45</pages>
      <abstract>We combine beam search with the probabilistic pruning technique of nucleus sampling to create two deterministic nucleus search algorithms for natural language generation. The first algorithm, p-exact search, locally prunes the next-token distribution and performs an exact search over the remaining space. The second algorithm, dynamic beam search, shrinks and expands the beam size according to the entropy of the candidate’s probability distribution. Despite the probabilistic intuition behind nucleus search, experiments on machine translation and summarization benchmarks show that both algorithms reach the same performance levels as standard beam search.</abstract>
      <url hash="30268966">2022.insights-1.5</url>
      <bibkey>shaham-levy-2022-get</bibkey>
      <doi>10.18653/v1/2022.insights-1.5</doi>
      <video href="2022.insights-1.5.mp4"/>
    </paper>
    <paper id="6">
      <title>How Much Do Modifications to Transformer Language Models Affect Their Ability to Learn Linguistic Knowledge?</title>
      <author><first>Simeng</first><last>Sun</last></author>
      <author><first>Brian</first><last>Dillon</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>46-53</pages>
      <abstract>Recent progress in large pretrained language models (LMs) has led to a growth of analyses examining what kinds of linguistic knowledge are encoded by these models. Due to computational constraints, existing analyses are mostly conducted on publicly-released LM checkpoints, which makes it difficult to study how various factors during <i>training</i> affect the models’ acquisition of linguistic knowledge. In this paper, we train a suite of small-scale Transformer LMs that differ from each other with respect to architectural decisions (e.g., self-attention configuration) or training objectives (e.g., multi-tasking, focal loss). We evaluate these LMs on BLiMP, a targeted evaluation benchmark of multiple English linguistic phenomena. Our experiments show that while none of these modifications yields significant improvements on aggregate, changes to the loss function result in promising improvements on several subcategories (e.g., detecting adjunct islands, correctly scoping negative polarity items). We hope our work offers useful insights for future research into designing Transformer LMs that more effectively learn linguistic knowledge.</abstract>
      <url hash="ed9bc02c">2022.insights-1.6</url>
      <bibkey>sun-etal-2022-much</bibkey>
      <doi>10.18653/v1/2022.insights-1.6</doi>
      <video href="2022.insights-1.6.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/blimp">BLiMP</pwcdataset>
    </paper>
    <paper id="7">
      <title>Cross-lingual Inflection as a Data Augmentation Method for Parsing</title>
      <author><first>Alberto</first><last>Muñoz-Ortiz</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <author><first>David</first><last>Vilares</last></author>
      <pages>54-61</pages>
      <abstract>We propose a morphology-based method for low-resource (LR) dependency parsing. We train a morphological inflector for target LR languages, and apply it to related rich-resource (RR) treebanks to create cross-lingual (x-inflected) treebanks that resemble the target LR language. We use such inflected treebanks to train parsers in zero- (training on x-inflected treebanks) and few-shot (training on x-inflected and target language treebanks) setups. The results show that the method sometimes improves the baselines, but not consistently.</abstract>
      <url hash="6e323907">2022.insights-1.7</url>
      <bibkey>munoz-ortiz-etal-2022-cross</bibkey>
      <doi>10.18653/v1/2022.insights-1.7</doi>
      <video href="2022.insights-1.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Is <fixed-case>BERT</fixed-case> Robust to Label Noise? A Study on Learning with Noisy Labels in Text Classification</title>
      <author><first>Dawei</first><last>Zhu</last></author>
      <author><first>Michael A.</first><last>Hedderich</last></author>
      <author><first>Fangzhou</first><last>Zhai</last></author>
      <author><first>David</first><last>Adelani</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>62-67</pages>
      <abstract>Incorrect labels in training data occur when human annotators make mistakes or when the data is generated via weak or distant supervision. It has been shown that complex noise-handling techniques - by modeling, cleaning or filtering the noisy instances - are required to prevent models from fitting this label noise. However, we show in this work that, for text classification tasks with modern NLP models like BERT, over a variety of noise types, existing noise-handling methods do not always improve its performance, and may even deteriorate it, suggesting the need for further investigation. We also back our observations with a comprehensive analysis.</abstract>
      <url hash="3135461b">2022.insights-1.8</url>
      <bibkey>zhu-etal-2022-bert</bibkey>
      <doi>10.18653/v1/2022.insights-1.8</doi>
      <video href="2022.insights-1.8.mp4"/>
      <pwccode url="https://github.com/uds-lsv/bert-lnl" additional="false">uds-lsv/bert-lnl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="9">
      <title>Ancestor-to-Creole Transfer is Not a Walk in the Park</title>
      <author><first>Heather</first><last>Lent</last></author>
      <author><first>Emanuele</first><last>Bugliarello</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>68-74</pages>
      <abstract>We aim to learn language models for Creole languages for which large volumes of data are not readily available, and therefore explore the potential transfer from ancestor languages (the ‘Ancestry Transfer Hypothesis’). We find that standard transfer methods do not facilitate ancestry transfer. Surprisingly, different from other non-Creole languages, a very distinct two-phase pattern emerges for Creoles: As our training losses plateau, and language models begin to overfit on their source languages, perplexity on the Creoles drop. We explore if this compression phase can lead to practically useful language models (the ‘Ancestry Bottleneck Hypothesis’), but also falsify this. Moreover, we show that Creoles even exhibit this two-phase pattern even when training on random, unrelated languages. Thus Creoles seem to be typological outliers and we speculate whether there is a link between the two observations.</abstract>
      <url hash="3f99dae5">2022.insights-1.9</url>
      <bibkey>lent-etal-2022-ancestor</bibkey>
      <doi>10.18653/v1/2022.insights-1.9</doi>
    </paper>
    <paper id="10">
      <title>What <fixed-case>GPT</fixed-case> Knows About Who is Who</title>
      <author><first>Xiaohan</first><last>Yang</last></author>
      <author><first>Eduardo</first><last>Peynetti</last></author>
      <author><first>Vasco</first><last>Meerman</last></author>
      <author><first>Chris</first><last>Tanner</last></author>
      <pages>75-81</pages>
      <abstract>Coreference resolution – which is a crucial task for understanding discourse and language at large – has yet to witness widespread benefits from large language models (LLMs). Moreover, coreference resolution systems largely rely on supervised labels, which are highly expensive and difficult to annotate, thus making it ripe for prompt engineering. In this paper, we introduce a QA-based prompt-engineering method and discern <i>generative</i>, pre-trained LLMs’ abilities and limitations toward the task of coreference resolution. Our experiments show that GPT-2 and GPT-Neo can return valid answers, but that their capabilities to identify coreferent mentions are limited and prompt-sensitive, leading to inconsistent results.</abstract>
      <url hash="0476de36">2022.insights-1.10</url>
      <attachment type="OptionalSupplementarySoftware" hash="d9115711">2022.insights-1.10.OptionalSupplementarySoftware.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="5354e2d3">2022.insights-1.10.OptionalSupplementaryData.zip</attachment>
      <bibkey>yang-etal-2022-gpt</bibkey>
      <doi>10.18653/v1/2022.insights-1.10</doi>
      <video href="2022.insights-1.10.mp4"/>
      <pwccode url="https://github.com/awesomecoref/prompt-coref" additional="false">awesomecoref/prompt-coref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="11">
      <title>Evaluating Biomedical Word Embeddings for Vocabulary Alignment at Scale in the <fixed-case>UMLS</fixed-case> <fixed-case>M</fixed-case>etathesaurus Using <fixed-case>S</fixed-case>iamese Networks</title>
      <author><first>Goonmeet</first><last>Bajaj</last></author>
      <author><first>Vinh</first><last>Nguyen</last></author>
      <author><first>Thilini</first><last>Wijesiriwardene</last></author>
      <author><first>Hong Yung</first><last>Yip</last></author>
      <author><first>Vishesh</first><last>Javangula</last></author>
      <author><first>Amit</first><last>Sheth</last></author>
      <author><first>Srinivasan</first><last>Parthasarathy</last></author>
      <author><first>Olivier</first><last>Bodenreider</last></author>
      <pages>82-87</pages>
      <abstract>Recent work uses a Siamese Network, initialized with BioWordVec embeddings (distributed word embeddings), for predicting synonymy among biomedical terms to automate a part of the UMLS (Unified Medical Language System) Metathesaurus construction process. We evaluate the use of contextualized word embeddings extracted from nine different biomedical BERT-based models for synonym prediction in the UMLS by replacing BioWordVec embeddings with embeddings extracted from each biomedical BERT model using different feature extraction methods. Finally, we conduct a thorough grid search, which prior work lacks, to find the best set of hyperparameters. Surprisingly, we find that Siamese Networks initialized with BioWordVec embeddings still out perform the Siamese Networks initialized with embedding extracted from biomedical BERT model.</abstract>
      <url hash="149ea7d1">2022.insights-1.11</url>
      <bibkey>bajaj-etal-2022-evaluating</bibkey>
      <doi>10.18653/v1/2022.insights-1.11</doi>
      <video href="2022.insights-1.11.mp4"/>
    </paper>
    <paper id="12">
      <title>On the Impact of Data Augmentation on Downstream Performance in Natural Language Processing</title>
      <author><first>Itsuki</first><last>Okimura</last></author>
      <author><first>Machel</first><last>Reid</last></author>
      <author><first>Makoto</first><last>Kawano</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <pages>88-93</pages>
      <abstract>With in the broader scope of machine learning, data augmentation is a common strategy to improve generalization and robustness of machine learning models. While data augmentation has been widely used within computer vision, its use in the NLP has been been comparably rather limited. The reason for this is that within NLP, the impact of proposed data augmentation methods on performance has not been evaluated in a unified manner, and effective data augmentation methods are unclear. In this paper, we look to tackle this by evaluating the impact of 12 data augmentation methods on multiple datasets when finetuning pre-trained language models. We find minimal improvements when data sizes are constrained to a few thousand, with performance degradation when data size is increased. We also use various methods to quantify the strength of data augmentations, and find that these values, though weakly correlated with downstream performance, correlate negatively or positively depending on the task. Furthermore, we find a glaring lack of consistently performant data augmentations. This all alludes to the difficulty of data augmentations for NLP tasks and we are inclined to believe that static data augmentations are not broadly applicable given these properties.</abstract>
      <url hash="32ceab8e">2022.insights-1.12</url>
      <bibkey>okimura-etal-2022-impact</bibkey>
      <doi>10.18653/v1/2022.insights-1.12</doi>
      <video href="2022.insights-1.12.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst-2">SST-2</pwcdataset>
    </paper>
    <paper id="13">
      <title>Can Question Rewriting Help Conversational Question Answering?</title>
      <author><first>Etsuko</first><last>Ishii</last></author>
      <author><first>Yan</first><last>Xu</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Bryan</first><last>Wilie</last></author>
      <pages>94-99</pages>
      <abstract>Question rewriting (QR) is a subtask of conversational question answering (CQA) aiming to ease the challenges of understanding dependencies among dialogue history by reformulating questions in a self-contained form. Despite seeming plausible, little evidence is available to justify QR as a mitigation method for CQA. To verify the effectiveness of QR in CQA, we investigate a reinforcement learning approach that integrates QR and CQA tasks and does not require corresponding QR datasets for targeted CQA.We find, however, that the RL method is on par with the end-to-end baseline. We provide an analysis of the failure and describe the difficulty of exploiting QR for CQA.</abstract>
      <url hash="d81f6d7b">2022.insights-1.13</url>
      <bibkey>ishii-etal-2022-question</bibkey>
      <doi>10.18653/v1/2022.insights-1.13</doi>
      <video href="2022.insights-1.13.mp4"/>
      <pwccode url="https://github.com/hltchkust/cqr4cqa" additional="false">hltchkust/cqr4cqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/canard">CANARD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qrecc">QReCC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
    </paper>
    <paper id="14">
      <title>Clustering Examples in Multi-Dataset Benchmarks with Item Response Theory</title>
      <author><first>Pedro</first><last>Rodriguez</last></author>
      <author><first>Phu Mon</first><last>Htut</last></author>
      <author><first>John</first><last>Lalor</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <pages>100-112</pages>
      <abstract>In natural language processing, multi-dataset benchmarks for common tasks (e.g., SuperGLUE for natural language inference and MRQA for question answering) have risen in importance. Invariably, tasks and individual examples vary in difficulty. Recent analysis methods infer properties of examples such as difficulty. In particular, Item Response Theory (IRT) jointly infers example and model properties from the output of benchmark tasks (i.e., scores for each model-example pair). Therefore, it seems sensible that methods like IRT should be able to detect differences between datasets in a task. This work shows that current IRT models are not as good at identifying differences as we would expect, explain why this is difficult, and outline future directions that incorporate more (textual) signal from examples.</abstract>
      <url hash="a3ecade1">2022.insights-1.14</url>
      <bibkey>rodriguez-etal-2022-clustering</bibkey>
      <doi>10.18653/v1/2022.insights-1.14</doi>
      <video href="2022.insights-1.14.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dynasent">DynaSent</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="15">
      <title>On the Limits of Evaluating Embodied Agent Model Generalization Using Validation Sets</title>
      <author><first>Hyounghun</first><last>Kim</last></author>
      <author><first>Aishwarya</first><last>Padmakumar</last></author>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>113-118</pages>
      <abstract>Natural language guided embodied task completion is a challenging problem since it requires understanding natural language instructions, aligning them with egocentric visual observations, and choosing appropriate actions to execute in the environment to produce desired changes. We experiment with augmenting a transformer model for this task with modules that effectively utilize a wider field of view and learn to choose whether the next step requires a navigation or manipulation action. We observed that the proposed modules resulted in improved, and in fact state-of-the-art performance on an unseen validation set of a popular benchmark dataset, ALFRED. However, our best model selected using the unseen validation set underperforms on the unseen test split of ALFRED, indicating that performance on the unseen validation set may not in itself be a sufficient indicator of whether model improvements generalize to unseen test sets. We highlight this result as we believe it may be a wider phenomenon in machine learning tasks but primarily noticeable only in benchmarks that limit evaluations on test splits, and highlights the need to modify benchmark design to better account for variance in model performance.</abstract>
      <url hash="d37bb9a1">2022.insights-1.15</url>
      <bibkey>kim-etal-2022-limits</bibkey>
      <doi>10.18653/v1/2022.insights-1.15</doi>
      <video href="2022.insights-1.15.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ai2-thor">AI2-THOR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/alfred">ALFRED</pwcdataset>
    </paper>
    <paper id="16">
      <title>Do Data-based Curricula Work?</title>
      <author><first>Maxim</first><last>Surkov</last></author>
      <author><first>Vladislav</first><last>Mosin</last></author>
      <author><first>Ivan</first><last>Yamshchikov</last></author>
      <pages>119-128</pages>
      <abstract>Current state-of-the-art NLP systems use large neural networks that require extensive computational resources for training. Inspired by human knowledge acquisition, researchers have proposed curriculum learning - sequencing tasks (task-based curricula) or ordering and sampling the datasets (data-based curricula) that facilitate training. This work investigates the benefits of data-based curriculum learning for large language models such as BERT and T5. We experiment with various curricula based on complexity measures and different sampling strategies. Extensive experiments on several NLP tasks show that curricula based on various complexity measures rarely have any benefits, while random sampling performs either as well or better than curricula.</abstract>
      <url hash="e310549a">2022.insights-1.16</url>
      <bibkey>surkov-etal-2022-data</bibkey>
      <doi>10.18653/v1/2022.insights-1.16</doi>
      <video href="2022.insights-1.16.mp4"/>
    </paper>
    <paper id="17">
      <title>The Document Vectors Using Cosine Similarity Revisited</title>
      <author><first>Zhang</first><last>Bingyu</last></author>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <pages>129-133</pages>
      <abstract>The current state-of-the-art test accuracy (97.42%) on the IMDB movie reviews dataset was reported by Thongtan and Phienthrakul (2019) and achieved by the logistic regression classifier trained on the Document Vectors using Cosine Similarity (DV-ngrams-cosine) proposed in their paper and the Bag-of-N-grams (BON) vectors scaled by Naïve Bayesian weights. While large pre-trained Transformer-based models have shown SOTA results across many datasets and tasks, the aforementioned model has not been surpassed by them, despite being much simpler and pre-trained on the IMDB dataset only. In this paper, we describe an error in the evaluation procedure of this model, which was found when we were trying to analyze its excellent performance on the IMDB dataset. We further show that the previously reported test accuracy of 97.42% is invalid and should be corrected to 93.68%. We also analyze the model performance with different amounts of training data (subsets of the IMDB dataset) and compare it to the Transformer-based RoBERTa model. The results show that while RoBERTa has a clear advantage for larger training sets, the DV-ngrams-cosine performs better than RoBERTa when the labeled training set is very small (10 or 20 documents). Finally, we introduce a sub-sampling scheme based on Naïve Bayesian weights for the training process of the DV-ngrams-cosine, which leads to faster training and better quality.</abstract>
      <url hash="98b28aca">2022.insights-1.17</url>
      <bibkey>bingyu-arefyev-2022-document</bibkey>
      <doi>10.18653/v1/2022.insights-1.17</doi>
      <video href="2022.insights-1.17.mp4"/>
      <pwccode url="https://github.com/bgzh/dv_cosine_revisited" additional="false">bgzh/dv_cosine_revisited</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="18">
      <title>Challenges in including extra-linguistic context in pre-trained language models</title>
      <author><first>Ionut</first><last>Sorodoc</last></author>
      <author><first>Laura</first><last>Aina</last></author>
      <author><first>Gemma</first><last>Boleda</last></author>
      <pages>134-138</pages>
      <abstract>To successfully account for language, computational models need to take into account both the linguistic context (the content of the utterances) and the extra-linguistic context (for instance, the participants in a dialogue). We focus on a referential task that asks models to link entity mentions in a TV show to the corresponding characters, and design an architecture that attempts to account for both kinds of context. In particular, our architecture combines a previously proposed specialized module (an “entity library”) for character representation with transfer learning from a pre-trained language model. We find that, although the model does improve linguistic contextualization, it fails to successfully integrate extra-linguistic information about the participants in the dialogue. Our work shows that it is very challenging to incorporate extra-linguistic information into pre-trained language models.</abstract>
      <url hash="9362e98a">2022.insights-1.18</url>
      <bibkey>sorodoc-etal-2022-challenges</bibkey>
      <doi>10.18653/v1/2022.insights-1.18</doi>
      <video href="2022.insights-1.18.mp4"/>
    </paper>
    <paper id="19">
      <title>Label Errors in <fixed-case>BANKING</fixed-case>77</title>
      <author><first>Cecilia</first><last>Ying</last></author>
      <author><first>Stephen</first><last>Thomas</last></author>
      <pages>139-143</pages>
      <abstract>We investigate potential label errors present in the popular BANKING77 dataset and the associated negative impacts on intent classification methods. Motivated by our own negative results when constructing an intent classifier, we applied two automated approaches to identify potential label errors in the dataset. We found that over 1,400 (14%) of the 10,003 training utterances may have been incorrectly labelled. In a simple experiment, we found that by removing the utterances with potential errors, our intent classifier saw an increase of 4.5% and 8% for the F1-Score and Adjusted Rand Index, respectively, in supervised and unsupervised classification. This paper serves as a warning of the potential of noisy labels in popular NLP datasets. Further study is needed to fully identify the breadth and depth of label errors in BANKING77 and other datasets.</abstract>
      <url hash="5af87c09">2022.insights-1.19</url>
      <bibkey>ying-thomas-2022-label</bibkey>
      <doi>10.18653/v1/2022.insights-1.19</doi>
      <video href="2022.insights-1.19.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
    </paper>
    <paper id="20">
      <title>Pathologies of Pre-trained Language Models in Few-shot Fine-tuning</title>
      <author><first>Hanjie</first><last>Chen</last></author>
      <author><first>Guoqing</first><last>Zheng</last></author>
      <author><first>Ahmed</first><last>Awadallah</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <pages>144-153</pages>
      <abstract>Although adapting pre-trained language models with few examples has shown promising performance on text classification, there is a lack of understanding of where the performance gain comes from. In this work, we propose to answer this question by interpreting the adaptation behavior using post-hoc explanations from model predictions. By modeling feature statistics of explanations, we discover that (1) without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although few-shot fine-tuning can mitigate the prediction bias and demonstrate promising prediction performance, our analysis shows models gain performance improvement by capturing non-task-related features (e.g. stop words) or shallow data patterns (e.g. lexical overlaps). These observations alert that pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot fine-tuning.</abstract>
      <url hash="a927d615">2022.insights-1.20</url>
      <attachment type="OptionalSupplementarySoftware" hash="29645c05">2022.insights-1.20.OptionalSupplementarySoftware.zip</attachment>
      <bibkey>chen-etal-2022-pathologies</bibkey>
      <doi>10.18653/v1/2022.insights-1.20</doi>
      <video href="2022.insights-1.20.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="21">
      <title>An Empirical study to understand the Compositional Prowess of Neural Dialog Models</title>
      <author><first>Vinayshekhar</first><last>Kumar</last></author>
      <author><first>Vaibhav</first><last>Kumar</last></author>
      <author><first>Mukul</first><last>Bhutani</last></author>
      <author><first>Alexander</first><last>Rudnicky</last></author>
      <pages>154-158</pages>
      <abstract>In this work, we examine the problems associated with neural dialog models under the common theme of compositionality. Specifically, we investigate three manifestations of compositionality: (1) Productivity, (2) Substitutivity, and (3) Systematicity. These manifestations shed light on the generalization, syntactic robustness, and semantic capabilities of neural dialog models. We design probing experiments by perturbing the training data to study the above phenomenon. We make informative observations based on automated metrics and hope that this work increases research interest in understanding the capacity of these models.</abstract>
      <url hash="47556e99">2022.insights-1.21</url>
      <bibkey>kumar-etal-2022-empirical</bibkey>
      <doi>10.18653/v1/2022.insights-1.21</doi>
      <video href="2022.insights-1.21.mp4"/>
      <pwccode url="https://github.com/vinayshekharcmu/ComposionalityOfDialogModels" additional="false">vinayshekharcmu/ComposionalityOfDialogModels</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mutualfriends">MutualFriends</pwcdataset>
    </paper>
    <paper id="22">
      <title>Combining Extraction and Generation for Constructing Belief-Consequence Causal Links</title>
      <author><first>Maria</first><last>Alexeeva</last></author>
      <author><first>Allegra A.</first><last>Beal Cohen</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>159-164</pages>
      <abstract>In this paper, we introduce and justify a new task—causal link extraction based on beliefs—and do a qualitative analysis of the ability of a large language model—InstructGPT-3—to generate implicit consequences of beliefs. With the language model-generated consequences being promising, but not consistent, we propose directions of future work, including data collection, explicit consequence extraction using rule-based and language modeling-based approaches, and using explicitly stated consequences of beliefs to fine-tune or prompt the language model to produce outputs suitable for the task.</abstract>
      <url hash="f268c8c3">2022.insights-1.22</url>
      <bibkey>alexeeva-etal-2022-combining</bibkey>
      <doi>10.18653/v1/2022.insights-1.22</doi>
      <video href="2022.insights-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Replicability under Near-Perfect Conditions – A Case-Study from Automatic Summarization</title>
      <author><first>Margot</first><last>Mieskes</last></author>
      <pages>165-171</pages>
      <abstract>Replication of research results has become more and more important in Natural Language Processing. Nevertheless, we still rely on results reported in the literature for comparison. Additionally, elements of an experimental setup are not always completely reported. This includes, but is not limited to reporting specific parameters used or omitting an implementational detail. In our experiment based on two frequently used data sets from the domain of automatic summarization and the seemingly full disclosure of research artefacts, we examine how well results reported are replicable and what elements influence the success or failure of replication. Our results indicate that publishing research artifacts is far from sufficient, that that publishing all relevant parameters in all possible detail is cruicial.</abstract>
      <url hash="a382435b">2022.insights-1.23</url>
      <bibkey>mieskes-2022-replicability</bibkey>
      <doi>10.18653/v1/2022.insights-1.23</doi>
      <video href="2022.insights-1.23.mp4"/>
    </paper>
    <paper id="24">
      <title><fixed-case>BPE</fixed-case> beyond Word Boundary: How <fixed-case>NOT</fixed-case> to use Multi Word Expressions in Neural Machine Translation</title>
      <author><first>Dipesh</first><last>Kumar</last></author>
      <author><first>Avijit</first><last>Thawani</last></author>
      <pages>172-179</pages>
      <abstract>BPE tokenization merges characters into longer tokens by finding frequently occurring <b>contiguous</b> patterns <b>within</b> the word boundary. An intuitive relaxation would be to extend a BPE vocabulary with multi-word expressions (MWEs): bigrams (<tex-math>in\_a</tex-math>), trigrams (<tex-math>out\_of\_the</tex-math>), and skip-grams (<tex-math>he . his</tex-math>). In the context of Neural Machine Translation (NMT), we replace the least frequent subword/whole-word tokens with the most frequent MWEs. We find that these modifications to BPE end up hurting the model, resulting in a net drop of BLEU and chrF scores across two language pairs. We observe that naively extending BPE beyond word boundaries results in incoherent tokens which are themselves better represented as individual words. Moreover, we find that Pointwise Mutual Information (PMI) instead of frequency finds better MWEs (e.g., <tex-math>New\_York</tex-math>, <tex-math>Statue\_of\_Liberty</tex-math>, <tex-math>neither . nor</tex-math>) which consistently improves translation performance.We release all code at <url>https://github.com/pegasus-lynx/mwe-bpe</url>.</abstract>
      <url hash="65615233">2022.insights-1.24</url>
      <attachment type="OptionalSupplementarySoftware" hash="ccbdbdc5">2022.insights-1.24.OptionalSupplementarySoftware.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="d8ccba59">2022.insights-1.24.OptionalSupplementaryData.zip</attachment>
      <bibkey>kumar-thawani-2022-bpe</bibkey>
      <doi>10.18653/v1/2022.insights-1.24</doi>
      <video href="2022.insights-1.24.mp4"/>
      <pwccode url="https://github.com/pegasus-lynx/mwe-bpe" additional="false">pegasus-lynx/mwe-bpe</pwccode>
    </paper>
    <paper id="25">
      <title>Pre-trained language models evaluating themselves - A comparative study</title>
      <author><first>Philipp</first><last>Koch</last></author>
      <author><first>Matthias</first><last>Aßenmacher</last></author>
      <author><first>Christian</first><last>Heumann</last></author>
      <pages>180-187</pages>
      <abstract>Evaluating generated text received new attention with the introduction of model-based metrics in recent years. These new metrics have a higher correlation with human judgments and seemingly overcome many issues of previous n-gram based metrics from the symbolic age. In this work, we examine the recently introduced metrics BERTScore, BLEURT, NUBIA, MoverScore, and Mark-Evaluate (Petersen). We investigate their sensitivity to different types of semantic deterioration (part of speech drop and negation), word order perturbations, word drop, and the common problem of repetition. No metric showed appropriate behaviour for negation, and further none of them was overall sensitive to the other issues mentioned above.</abstract>
      <url hash="6fe00304">2022.insights-1.25</url>
      <bibkey>koch-etal-2022-pre</bibkey>
      <doi>10.18653/v1/2022.insights-1.25</doi>
      <video href="2022.insights-1.25.mp4"/>
      <pwccode url="https://github.com/lazerlambda/metricscomparison" additional="false">lazerlambda/metricscomparison</pwccode>
    </paper>
  </volume>
</collection>
