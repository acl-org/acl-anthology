<?xml version='1.0' encoding='UTF-8'?>
<collection id="2017.jeptalnrecital">
  <volume id="long" ingest-date="2020-07-21">
    <meta>
      <booktitle>Actes des 24ème Conférence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs</booktitle>
      <editor><first>Iris</first><last>Eshkol-Taravella</last></editor>
      <editor><first>Jean-Yves</first><last>Antoine</last></editor>
      <publisher>ATALA</publisher>
      <address>Orléans, France</address>
      <month>6</month>
      <year>2017</year>
    </meta>
    <frontmatter>
      <url hash="ede7a90d">2017.jeptalnrecital-long.0</url>
    </frontmatter>
    <paper id="1">
      <title>Ajout automatique de disfluences pour la synthèse de la parole spontanée : formalisation et preuve de concept (Automatic disfluency insertion towards spontaneous <fixed-case>TTS</fixed-case> : formalization and proof of concept)</title>
      <language>fra</language>
      <author><first>Raheel</first><last>Qader</last></author>
      <author><first>Gwénolé</first><last>Lecorvé</last></author>
      <author><first>Damien</first><last>Lolive</last></author>
      <author><first>Pascale</first><last>Sébillot</last></author>
      <pages>1–15</pages>
      <abstract>Cet article présente un travail exploratoire sur l’ajout automatique de disfluences, c’est-à-dire de pauses, de répétitions et de révisions, dans les énoncés en entrée d’un système de synthèse de la parole. L’objectif est de conférer aux signaux ainsi synthétisés un caractère plus spontané et expressif. Pour cela, nous présentons une formalisation novatrice du processus de production de disfluences à travers un mécanisme de composition de ces disfluences. Cette formalisation se distingue notamment des approches visant la détection ou le nettoyage de disfluences dans des transcriptions, ou de celles en synthèse de la parole qui ne s’intéressent qu’au seul ajout de pauses. Nous présentons une première implémentation de notre processus fondée sur des champs aléatoires conditionnels et des modèles de langage, puis conduisons des évaluations objectives et perceptives. Celles-ci nous permettent de conclure à la fonctionnalité de notre proposition et d’en discuter les pistes principales d’amélioration.</abstract>
      <url hash="b2b15d82">2017.jeptalnrecital-long.1</url>
    </paper>
    <paper id="2">
      <title>Normalisation automatique du vocabulaire source pour traduire depuis une langue à morphologie riche (Learning Morphological Normalization for Translation from Morphologically Rich Languages)</title>
      <language>fra</language>
      <author><first>Franck</first><last>Burlot</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>16–31</pages>
      <abstract>Lorsqu’ils sont traduits depuis une langue à morphologie riche vers l’anglais, les mots-formes sources contiennent des marques d’informations grammaticales pouvant être jugées redondantes par rapport à l’anglais, causant une variabilité formelle qui nuit à l’estimation des modèles probabilistes. Un moyen bien documenté pour atténuer ce problème consiste à supprimer l’information non pertinente de la source en la normalisant. Ce pré-traitement est généralement effectué de manière déterministe, à l’aide de règles produites manuellement. Une telle normalisation est, par essence, sous-optimale et doit être adaptée pour chaque paire de langues. Nous présentons, dans cet article, une méthode simple pour rechercher automatiquement une normalisation optimale de la morphologie source par rapport à la langue cible et montrons que celle-ci peut améliorer la traduction automatique.</abstract>
      <url hash="f754b3c2">2017.jeptalnrecital-long.2</url>
    </paper>
    <paper id="3">
      <title>Représentations continues dérivées des caractères pour un modèle de langue neuronal à vocabulaire ouvert (Opening the vocabulary of neural language models with character-level word representations)</title>
      <language>fra</language>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <pages>32–46</pages>
      <abstract>Cet article propose une architecture neuronale pour un modèle de langue à vocabulaire ouvert. Les représentations continues des mots sont calculées à la volée à partir des caractères les composant, gràce à une couche convolutionnelle suivie d’une couche de regroupement (pooling). Cela permet au modèle de représenter n’importe quel mot, qu’il fasse partie du contexte ou soit évalué pour la prédiction. La fonction objectif est dérivée de l’estimation contrastive bruitée (Noise Contrastive Estimation, ou NCE), calculable dans notre cas sans vocabulaire. Nous évaluons la capacité de notre modèle à construire des représentations continues de mots inconnus sur la tâche de traduction automatique IWSLT-2016, de l’Anglais vers le Tchèque, en ré-évaluant les N meilleures hypothèses (N-best reranking). Les résultats expérimentaux permettent des gains jusqu’à 0,7 point BLEU. Ils montrent aussi la difficulté d’utiliser des représentations dérivées des caractères pour la prédiction.</abstract>
      <url hash="c234a7de">2017.jeptalnrecital-long.3</url>
    </paper>
    <paper id="4">
      <title>Utilisation d’indices phraséologiques pour évaluer des textes en langue étrangère : comparaison des bigrammes et des trigrammes (Collocation measures and automated scoring of foreign language texts : Comparing bigrams and trigrams)</title>
      <language>fra</language>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>47–62</pages>
      <abstract>Cette recherche a pour principal objectif d’évaluer l’utilité de prendre en compte des mesures totalement automatiques de la compétence phraséologique pour estimer la qualité de textes d’apprenants de l’anglais langue étrangère. Les analyses, menées sur plus de 1000 copies d’examen du First Certificate in English, librement mises à disposition par Yannakoudakis et coll., confirment que l’approche qui consiste à assigner aux bigrammes et aux trigrammes de mots présents dans un texte des scores d’association collocationnelle calculés sur la base d’un grand corpus de référence natif est particulièrement efficace. Si les indices extraits des trigrammes sont moins efficaces que ceux extraits des bigrammes, ils apportent une contribution utile à ces derniers. Les analyses soulignent aussi les bénéfices apportés par un emploi simultané de plusieurs mesures d’association collocationnelle.</abstract>
      <url hash="8d8851b8">2017.jeptalnrecital-long.4</url>
    </paper>
    <paper id="5">
      <title>Traitement des Mots Hors Vocabulaire pour la Traduction Automatique de Document <fixed-case>OCR</fixed-case>isés en Arabe (This article presents a new system that automatically translates images of arabic documents)</title>
      <language>fra</language>
      <author><first>Kamel</first><last>Bouzidi</last></author>
      <author><first>Zied</first><last>Elloumi</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Mohamed-Faouzi</first><last>Benzeghiba</last></author>
      <pages>63–76</pages>
      <abstract>Cet article présente un système original de traduction de documents numérisés en arabe. Deux modules sont cascadés : un système de reconnaissance optique de caractères (OCR) en arabe et un système de traduction automatique (TA) arabe-français. Le couplage OCR-TA a été peu abordé dans la littérature et l’originalité de cette étude consiste à proposer un couplage étroit entre OCR et TA ainsi qu’un traitement spécifique des mots hors vocabulaire (MHV) engendrés par les erreurs d’OCRisation. Le couplage OCR-TA par treillis et notre traitement des MHV par remplacement selon une mesure composite qui prend en compte forme de surface et contexte du mot, permettent une amélioration significative des performances de traduction. Les expérimentations sont réalisés sur un corpus de journaux numérisés en arabe et permettent d’obtenir des améliorations en score BLEU de 3,73 et 5,5 sur les corpus de développement et de test respectivement.</abstract>
      <url hash="023ce836">2017.jeptalnrecital-long.5</url>
    </paper>
    <paper id="6">
      <title>Représentation et analyse automatique des discontinuités syntaxiques dans les corpus arborés en constituants du français (Representation and parsing of syntactic discontinuities in <fixed-case>F</fixed-case>rench constituent treebanks)</title>
      <language>fra</language>
      <author><first>Maximin</first><last>Coavoux</last></author>
      <author><first>Benoît</first><last>Crabbé</last></author>
      <pages>77–92</pages>
      <abstract>Nous présentons de nouvelles instanciations de trois corpus arborés en constituants du français, où certains phénomènes syntaxiques à l’origine de dépendances à longue distance sont représentés directement à l’aide de constituants discontinus. Les arbres obtenus relèvent de formalismes grammaticaux légèrement sensibles au contexte (LCFRS). Nous montrons ensuite qu’il est possible d’analyser automatiquement de telles structures de manière efficace à condition de s’appuyer sur une méthode d’inférence approximative. Pour cela, nous présentons un analyseur syntaxique par transitions, qui réalise également l’analyse morphologique et l’étiquetage fonctionnel des mots de la phrase. Enfin, nos expériences montrent que la rareté des phénomènes concernés dans les données françaises pose des difficultés pour l’apprentissage et l’évaluation des structures discontinues.</abstract>
      <url hash="2a2be6d3">2017.jeptalnrecital-long.6</url>
    </paper>
    <paper id="7">
      <title>Construire des représentations denses à partir de thésaurus distributionnels (Distributional Thesaurus Embedding and its Applications)</title>
      <language>fra</language>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>93–108</pages>
      <abstract>Dans cet article, nous nous intéressons à un nouveau problème, appelé plongement de thésaurus, consistant à transformer un thésaurus distributionnel en une représentation dense de mots. Nous proposons de traiter ce problème par une méthode fondée sur l’association d’un plongement de graphe et de l’injection de relations dans des représentations denses. Nous avons appliqué et évalué cette méthode pour un large ensemble de noms en anglais et montré que les représentations denses produites obtiennent de meilleures performances, selon une évaluation intrinsèque, que les représentations denses construites selon les méthodes de l’état de l’art sur le même corpus. Nous illustrons aussi l’intérêt de la méthode développée pour améliorer les représentations denses existantes à la fois de façon endogène et exogène.</abstract>
      <url hash="f373b903">2017.jeptalnrecital-long.7</url>
    </paper>
    <paper id="8">
      <title>Projection Aléatoire Non-Négative pour le Calcul de Word Embedding / Non-Negative Randomized Word Embedding</title>
      <author><first>Behrang</first><last>Qasemizadeh</last></author>
      <author><first>Laura</first><last>Kallmeyer</last></author>
      <author><first>Aurelie</first><last>Herbelot</last></author>
      <pages>109–122</pages>
      <abstract>Non-Negative Randomized Word Embedding We propose a word embedding method which is based on a novel random projection technique. We show that weighting methods such as positive pointwise mutual information (PPMI) can be applied to our models after their construction and at a reduced dimensionality. Hence, the proposed technique can efficiently transfer words onto semantically discriminative spaces while demonstrating high computational performance, besides benefits such as ease of update and a simple mechanism for interoperability. We report the performance of our method on several tasks and show that it yields competitive results compared to neural embedding methods in monolingual corpus-based setups.</abstract>
      <url hash="41643705">2017.jeptalnrecital-long.8</url>
    </paper>
    <paper id="9">
      <title>Création et validation de signatures sémantiques : application à la mesure de similarité sémantique et à la substitution lexicale (Creating and validating semantic signatures : application for measuring semantic similarity and lexical substitution)</title>
      <language>fra</language>
      <author><first>Mokhtar-Boumedyen</first><last>Billami</last></author>
      <author><first>Núria</first><last>Gala</last></author>
      <pages>123–138</pages>
      <abstract>L’intégration de la notion de similarité sémantique entre les unités lexicales est essentielle dans différentes applications de Traitement Automatique des Langues (TAL). De ce fait, elle a reçu un intérêt considérable qui a eu comme conséquence le développement d’une vaste gamme d’approches pour en déterminer une mesure. Ainsi, plusieurs types de mesures de similarité existent, elles utilisent différentes représentations obtenues à partir d’informations soit dans des ressources lexicales, soit dans de gros corpus de données ou bien dans les deux. Dans cet article, nous nous intéressons à la création de signatures sémantiques décrivant des représentations vectorielles de mots à partir du réseau lexical JeuxDeMots (JDM). L’évaluation de ces signatures est réalisée sur deux tâches différentes : mesures de similarité sémantique et substitution lexicale. Les résultats obtenus sont très satisfaisants et surpassent, dans certains cas, les performances des systèmes de l’état de l’art.</abstract>
      <url hash="e8afda94">2017.jeptalnrecital-long.9</url>
    </paper>
    <paper id="10">
      <title>Vers une solution légère de production de données pour le <fixed-case>TAL</fixed-case> : création d’un tagger de l’alsacien par crowdsourcing bénévole (Toward a lightweight solution to the language resources bottleneck issue: creating a <fixed-case>POS</fixed-case> tagger for <fixed-case>A</fixed-case>lsatian using voluntary crowdsourcing)</title>
      <language>fra</language>
      <author><first>Alice</first><last>Millour</last></author>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <author><first>Lucie</first><last>Steiblé</last></author>
      <pages>139–154</pages>
      <abstract>Nous présentons ici les résultats d’une expérience menée sur l’annotation en parties du discours d’un corpus d’une langue régionale encore peu dotée, l’alsacien, via une plateforme de myriadisation (crowdsourcing) bénévole développée spécifiquement à cette fin : Bisame1 . La plateforme, mise en ligne en mai 2016, nous a permis de recueillir 15 846 annotations grâce à 42 participants. L’évaluation des annotations, réalisée sur un corpus de référence, montre que la F-mesure des annotations volontaires est de 0, 93. Le tagger entraîné sur le corpus annoté atteint lui 82 % d’exactitude. Il s’agit du premier tagger spécifique à l’alsacien. Cette méthode de développement de ressources langagières est donc efficace et prometteuse pour certaines langues peu dotées, dont un nombre suffisant de locuteurs est connecté et actif sur le Web. Le code de la plateforme, le corpus annoté et le tagger sont librement disponibles.</abstract>
      <url hash="61d43588">2017.jeptalnrecital-long.10</url>
    </paper>
    <paper id="11">
      <title>Sciences participatives et <fixed-case>TAL</fixed-case>: jusqu’où ? comment ? pourquoi ? (Citizen science and <fixed-case>NLP</fixed-case> : how far ? how ? why ? This paper investigates the modalities of achievement of citizen science in <fixed-case>NLP</fixed-case>, by considering existing participative projects but also historical studies on the relationships between science and opinion)</title>
      <language>fra</language>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <author><first>Anaïs</first><last>Lefeuvre-Halftermeyer</last></author>
      <pages>155–168</pages>
      <abstract>Cet article s’interroge sur les modalités de participation citoyenne aux recherches en TALN, à la lumière des projets actuels en sciences citoyennes mais aussi d’études menées sur le sujet en histoire des sciences. Il vise à montrer comment une science participative est déjà en marche en TALN, à interroger ses modalités et également à en circonscrire les limites.</abstract>
      <url hash="06316f13">2017.jeptalnrecital-long.11</url>
    </paper>
    <paper id="12">
      <title>Construction automatique d’une base de données étymologiques à partir du wiktionary (Automatic construction of an etymological database using <fixed-case>W</fixed-case>iktionary)</title>
      <language>fra</language>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>169–181</pages>
      <abstract>Les ressources lexicales électroniques ne contiennent quasiment jamais d’informations étymologiques. De telles informations, convenablement formalisées, permettraient pourtant de développer des outils automatiques au service de la linguistique historique et comparative, ainsi que d’améliorer significativement le traitement automatique de langues anciennes. Nous décrivons ici le processus que nous avons mis en œuvre pour extraire des données étymologiques à partir des notices étymologiques du wiktionary, rédigées en anglais. Nous avons ainsi produit une base multilingue de près d’un million de lexèmes et une base de plus d’un demi-million de relations étymologiques entre lexèmes.</abstract>
      <url hash="19c35d74">2017.jeptalnrecital-long.12</url>
    </paper>
    <paper id="13">
      <title>Apprendre des représentations jointes de mots et d’entités pour la désambiguïsation d’entités (Combining Word and Entity Embeddings for Entity Linking)</title>
      <language>fra</language>
      <author><first>José</first><last>Moreno</last></author>
      <author><first>Romaric</first><last>Besançon</last></author>
      <author><first>Romain</first><last>Beaumont</last></author>
      <author><first>Eva</first><last>D’Hondt</last></author>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Sophie</first><last>Rosset</last></author>
      <author><first>Xavier</first><last>Tannier</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <pages>182–195</pages>
      <abstract>La désambiguïsation d’entités (ou liaison d’entités), qui consiste à relier des mentions d’entités d’un texte à des entités d’une base de connaissance, est un problème qui se pose, entre autre, pour le peuplement automatique de bases de connaissances à partir de textes. Une difficulté de cette tâche est la résolution d’ambiguïtés car les systèmes ont à choisir parmi un nombre important de candidats. Cet article propose une nouvelle approche fondée sur l’apprentissage joint de représentations distribuées des mots et des entités dans le même espace, ce qui permet d’établir un modèle robuste pour la comparaison entre le contexte local de la mention d’entité et les entités candidates.</abstract>
      <url hash="e25d97e4">2017.jeptalnrecital-long.13</url>
    </paper>
    <paper id="14">
      <title>Analyse et évolution de la compréhension de termes techniques (Analysis and Evolution of Understanding of Technical Terms)</title>
      <language>fra</language>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Thierry</first><last>Hamon</last></author>
      <pages>196–211</pages>
      <abstract>Nous faisons l’hypothèse que les mots techniques inconnus dotés d’une structure interne (mots affixés ou composés) peuvent fournir des indices linguistiques à un locuteur, ce qui peut l’aider à analyser et à comprendre ces mots. Afin de tester notre hypothèse, nous proposons de travailler sur un ensemble de mots techniques provenant du domaine médical. Un grand ensemble de mots techniques est annoté par cinq annotateurs. Nous effectuons deux types d’analyses : l’analyse de l’évolution des mots compréhensibles et incompréhensibles (de manière générale et en fonction de certains suffixes) et l’analyse des clusters avec ces mots créés par apprentissage non-supervisé, sur la base des descripteurs linguistiques et extra-linguistiques. Nos résultats indiquent que, selon la sensibilité linguistique des annotateurs, les mots techniques peuvent devenir décodables et compréhensibles. Quant aux clusters, le contenu de certains reflète la difficulté des mots qui les composent et montre également la progression des annotateurs dans leur compréhension. La ressource construite est disponible pour la recherche : http://natalia.grabar.free.fr/rated-lexicon.html.</abstract>
      <url hash="5eb830b7">2017.jeptalnrecital-long.14</url>
    </paper>
  </volume>
  <volume id="court" ingest-date="2020-07-21">
    <meta>
      <booktitle>Actes des 24ème Conférence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts</booktitle>
      <editor><first>Iris</first><last>Eshkol-Taravella</last></editor>
      <editor><first>Jean-Yves</first><last>Antoine</last></editor>
      <publisher>ATALA</publisher>
      <address>Orléans, France</address>
      <month>6</month>
      <year>2017</year>
    </meta>
    <frontmatter>
      <url hash="64588f7b">2017.jeptalnrecital-court.0</url>
    </frontmatter>
    <paper id="1">
      <title>Annotation d’expressions polylexicales verbales en français (Annotation of verbal multiword expressions in <fixed-case>F</fixed-case>rench)</title>
      <language>fra</language>
      <author><first>Marie</first><last>Candito</last></author>
      <author><first>Mathieu</first><last>Constant</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <author><first>Agata</first><last>Savary</last></author>
      <author><first>Yannick</first><last>Parmentier</last></author>
      <author><first>Caroline</first><last>Pasquer</last></author>
      <author><first>Jean-Yves</first><last>Antoine</last></author>
      <pages>1–9</pages>
      <abstract>Nous décrivons la partie française des données produites dans le cadre de la campagne multilingue PARSEME sur l’identification d’expressions polylexicales verbales (Savary et al., 2017). Les expressions couvertes pour le français sont les expressions verbales idiomatiques, les verbes intrinsèquement pronominaux et une généralisation des constructions à verbe support. Ces phénomènes ont été annotés sur le corpus French-UD (Nivre et al., 2016) et le corpus Sequoia (Candito &amp; Seddah, 2012), soit un corpus de 22 645 phrases, pour un total de 4 962 expressions annotées. On obtient un ratio d’une expression annotée tous les 100 tokens environ, avec un fort taux d’expressions discontinues (40%).</abstract>
      <url hash="025db948">2017.jeptalnrecital-court.1</url>
    </paper>
    <paper id="2">
      <title>Évaluation de mesures d’association pour les bigrammes et les trigrammes au moyen du test exact de Fisher (Using Fisher’s Exact Test to Evaluate Association Measures for Bigrams and Trigrams)</title>
      <language>fra</language>
      <author><first>Yves</first><last>Bestgen</last></author>
      <pages>10–18</pages>
      <abstract>Pour déterminer si certaines mesures d’association lexicale fréquemment employées en TAL attribuent des scores élevés à des n-grammes que le hasard aurait pu produire aussi souvent qu’observé, nous avons utilisé une extension du test exact de Fisher à des séquences de plus de deux mots. Les analyses ont porté sur un corpus de quatre millions de mots d’anglais conversationnel extrait du BNC. Les résultats, basés sur la courbe précision-rappel et sur la précision moyenne, montrent que le LL-simple est extrêmement efficace. IM3 est plus efficace que les autres mesures basées sur les tests d’hypothèse et atteint même un niveau de performance presque égal à LL-simple pour les trigrammes.</abstract>
      <url hash="78220ca5">2017.jeptalnrecital-court.2</url>
    </paper>
    <paper id="3">
      <title>Réseaux neuronaux profonds pour l’étiquetage de séquences (Deep Neural Networks for Sequence Labeling)</title>
      <language>fra</language>
      <author><first>Yoann</first><last>Dupont</last></author>
      <author><first>Marco</first><last>Dinarelli</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <pages>19–27</pages>
      <abstract>Depuis quelques années les réseaux neuronaux se montrent très efficaces dans toutes les tâches de Traitement Automatique des Langues (TAL). Récemment, une variante de réseau neuronal particulièrement adapté à l’étiquetage de séquences textuelles a été proposée, utilisant des représentations distributionnelles des étiquettes. Dans cet article, nous reprenons cette variante et nous l’améliorons avec une version profonde. Dans cette version, différentes couches cachées permettent de prendre en compte séparément les différents types d’informations données en entrée au réseau. Nous évaluons notre modèle sur les mêmes tâches que la première version de réseau de laquelle nous nous sommes inspirés. Les résultats montrent que notre variante de réseau neuronal est plus efficace que les autres, mais aussi qu’elle est plus efficace que tous les autres modèles évalués sur ces tâches, obtenant l’état-de-l’art.</abstract>
      <url hash="83a1ab96">2017.jeptalnrecital-court.3</url>
    </paper>
    <paper id="4">
      <title>Schémas <fixed-case>W</fixed-case>inograd en français: une étude statistique et comportementale (<fixed-case>W</fixed-case>inograd schemas in <fixed-case>F</fixed-case>rench : a statistical and behavioral study)</title>
      <language>fra</language>
      <author><first>Pascal</first><last>Amsili</last></author>
      <author><first>Olga</first><last>Seminck</last></author>
      <pages>28–35</pages>
      <abstract>Nous présentons dans cet article une collection de schémas Winograd en français, adaptée de la liste proposée par Levesque et al. (2012) pour l’anglais. Les schémas Winograd sont des problèmes de résolution d’anaphore conçus pour être IA-complets. Nous montrons que notre collection vérifie deux propriétés cruciales : elle est robuste vis-à-vis de méthodes statistiques simples (“Google-proof”), tout en étant largement dépourvue d’ambiguïté pour les sujets humains que nous avons testés.</abstract>
      <url hash="d4fca65f">2017.jeptalnrecital-court.4</url>
    </paper>
    <paper id="5">
      <title>Critères numériques dans les essais cliniques : annotation, détection et normalisation (Numerical criteria in clinical trials : annotation, detection and normalization)</title>
      <language>fra</language>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Vincent</first><last>Claveau</last></author>
      <pages>36–43</pages>
      <abstract>Les essais cliniques sont un élément fondamental pour l’évaluation de nouvelles thérapies ou techniques de diagnostic, de leur sécurité et efficacité. Ils exigent d’avoir un échantillon convenable de la population. Le défi consiste alors à recruter le nombre suffisant de participants avec des caractéristiques similaires pour garantir que les résultats des essais sont bien contrôlés et dus aux facteurs étudiés. C’est une tâche difficile, effectuée essentiellement manuellement. Comme les valeurs numériques sont une information très fréquente et importante, nous proposons un système automatique qui vise leur extraction et normalisation.</abstract>
      <url hash="fa26057e">2017.jeptalnrecital-court.5</url>
    </paper>
    <paper id="6">
      <title>Analyse automatique <fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et : une étude sur un corpus français de textes encyclopédiques (<fixed-case>F</fixed-case>rame<fixed-case>N</fixed-case>et automatic analysis : a study on a <fixed-case>F</fixed-case>rench corpus of encyclopedic texts)</title>
      <language>fra</language>
      <author><first>Gabriel</first><last>Marzinotto</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <pages>44–51</pages>
      <abstract>Cet article présente un système d’analyse automatique en cadres sémantiques évalué sur un corpus de textes encyclopédiques d’histoire annotés selon le formalisme FrameNet. L’approche choisie repose sur un modèle intégré d’étiquetage de séquence qui optimise conjointement l’identification des cadres, la segmentation et l’identification des rôles sémantiques associés. Nous cherchons dans cette étude à analyser la complexité de la tâche selon plusieurs dimensions. Une analyse détaillée des performances du système est ainsi proposée, à la fois selon l’angle des paramètres du modèle et de la nature des données.</abstract>
      <url hash="8975cd4d">2017.jeptalnrecital-court.6</url>
    </paper>
    <paper id="7">
      <title>Détection de coréférences de bout en bout en français (End-to-end coreference resolution for <fixed-case>F</fixed-case>rench)</title>
      <language>fra</language>
      <author><first>Elisabeth</first><last>Godbert</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <pages>52–59</pages>
      <abstract>Notre objectif est l’élaboration d’un système de détection automatique de relations de coréférence le plus général possible, pour le traitement des anaphores pronominales et les coréférences directes. Nous décrivons dans cet article les différentes étapes de traitement des textes dans le système que nous avons développé : (i) l’annotation en traits lexicaux et syntaxiques par le système Macaon ; (ii) le repérage des mentions par un modèle obtenu par apprentissage sur le corpus ANCOR ; (iii) l’annotation sémantique des mentions à partir de deux ressources : le DEM et le LVF ; (iv) l’annotation en coréférences par un système à base de règles. Le système est évalué sur le corpus ANCOR.</abstract>
      <url hash="d2ea8a54">2017.jeptalnrecital-court.7</url>
    </paper>
    <paper id="8">
      <title>Une approche hybride pour la construction de lexiques bilingues d’expressions multi-mots à partir de corpus parallèles (A hybrid approach to build bilingual lexicons of multiword expressions from parallel corpora)</title>
      <language>fra</language>
      <author><first>Nasredine</first><last>Semmar</last></author>
      <author><first>Morgane</first><last>Marchand</last></author>
      <pages>60–68</pages>
      <abstract>Les expressions multi-mots jouent un rôle important dans différentes applications du Traitement Automatique de la Langue telles que la traduction automatique et la recherche d’information interlingue. Cet article, d’une part, décrit une approche hybride pour l’acquisition d’un lexique bilingue d’expressions multi-mots à partir d’un corpus parallèle anglais-français, et d’autre part, présente l’impact de l’utilisation d’un lexique bilingue spécialisé d’expressions multi-mots produit par cette approche sur les résultats du système de traduction statistique libre Moses. Nous avons exploré deux métriques basées sur la co-occurrence pour évaluer les liens d’alignement entre les expressions multi-mots des langues source et cible. Les résultats obtenus montrent que la métrique utilisant un dictionnaire bilingue amorce de mots simples améliore aussi bien la qualité de l’alignement d’expressions multi-mots que celle de la traduction.</abstract>
      <url hash="1fbb4678">2017.jeptalnrecital-court.8</url>
    </paper>
    <paper id="9">
      <title>Une interprétation probabiliste des informations de factivité (Factuality information as sets of probabilities)</title>
      <language>fra</language>
      <author><first>Timothée</first><last>Bernard</last></author>
      <pages>69–76</pages>
      <abstract>Nous présentons une nouvelle formalisation de la factivité, la dimension représentant le degré de croyance qu’une source – l’auteur ou tout autre agent mentionné dans un texte – accorde à une éventualité donnée. Nous insistons sur l’aspect dynamique de cette notion ainsi que sur ses interactions avec la structure discursive. Nous montrons comment une interprétation en termes d’ensembles de probabilités permet de s’affranchir des principaux problèmes que posait la formalisation utilisée dans les travaux précédents au calcul d’une factivité cohérente à l’échelle du texte dans sa totalité.</abstract>
      <url hash="c3dea755">2017.jeptalnrecital-court.9</url>
    </paper>
    <paper id="10">
      <title>Une approche universelle pour l’abstraction automatique d’alternances morphophonologiques (A universal algorithm for the automatical abstraction of morphophonological alternations)</title>
      <language>fra</language>
      <author><first>Sacha</first><last>Beniamine</last></author>
      <pages>77–85</pages>
      <abstract>Cet article présente un algorithme implémenté pour l’inférence de patrons d’alternances morphophonologiques entre mots-formes. Il est universel au sens où il permet d’obtenir des classifications comparables d’une langue à l’autre sans préjuger des types d’alternances. Les patrons constituent une première étape pour les travaux quantitatifs dans l’approche Mot et Paradigme de la morphologie.</abstract>
      <url hash="7a21c058">2017.jeptalnrecital-court.10</url>
    </paper>
    <paper id="11">
      <title>Détection automatique de métaphores dans des textes de Géographie : une étude prospective (Automatic detection of metaphors in Geographical research papers : a prospective study)</title>
      <language>fra</language>
      <author><first>Max</first><last>Beligné</last></author>
      <author><first>Aleksandra</first><last>Campar</last></author>
      <author><first>Jean-Hugues</first><last>Chauchat</last></author>
      <author><first>Melanie</first><last>Lefeuvre</last></author>
      <author><first>Isabelle</first><last>Lefort</last></author>
      <author><first>Sabine</first><last>Loudcher</last></author>
      <author><first>Julien</first><last>Velcin</last></author>
      <pages>86–93</pages>
      <abstract>Cet article s’intègre dans un projet collaboratif qui vise à réaliser une analyse longitudinale de la production universitaire en Géographie. En particulier, nous présentons les premiers résultats de l’application d’une méthode de détection automatique de métaphores basée sur les modèles de thématiques latentes. Une analyse détaillée permet de mieux comprendre l’impact de certains choix et de réfléchir aux pistes de recherche que nous serons amenés à explorer pour améliorer ces résultats.</abstract>
      <url hash="acc2ec03">2017.jeptalnrecital-court.11</url>
    </paper>
    <paper id="12">
      <title>Describing derivational polysemy with <fixed-case>XMG</fixed-case></title>
      <author><first>Marios</first><last>Andreou</last></author>
      <author><first>Simon</first><last>Petitjean</last></author>
      <pages>94–101</pages>
      <abstract>In this paper, we model and test the monosemy and polysemy approaches to derivational multiplicity of meaning, using Frame Semantics and XMG. In order to illustrate our claims and proposals, we use data from deverbal nominalizations with the suffix -al on verbs of change of possession (e.g. rental, disbursal). In our XMG implementation, we show that the underspecified meaning of affixes cannot always be reduced to a single unitary meaning and that the polysemy approach to multiplicity of meaning is more judicious compared to the monosemy approach. We also introduce constraints on the potential referents of derivatives. These constraints have the form of type constraints and specify which arguments in the frame of the verbal base are compatible with the referential argument of the derivative. The introduction of type constraints rules out certain readings because frame unification only succeeds if types are compatible.</abstract>
      <url hash="b996720c">2017.jeptalnrecital-court.12</url>
    </paper>
    <paper id="13">
      <title>Changement stylistique de phrases par apprentissage faiblement supervisé (Textual Style Transfer using Weakly Supervised Learning)</title>
      <language>fra</language>
      <author><first>Damien</first><last>Sileo</last></author>
      <author><first>Camille</first><last>Pradel</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Tim</first><last>Van de Cruys</last></author>
      <pages>102–109</pages>
      <abstract>Plusieurs tâches en traitement du langage naturel impliquent de modifier des phrases en conservant au mieux leur sens, comme la reformulation, la compression, la simplification, chacune avec leurs propres données et modèles. Nous introduisons ici une méthode générale s’adressant à tous ces problèmes, utilisant des données plus simples à obtenir : un ensemble de phrases munies d’indicateurs sur leur style, comme des phrases et le type de sentiment qu’elles expriment. Cette méthode repose sur un modèle d’apprentissage de représentations non supervisé (un auto-encodeur variationnel), puis sur le changement des représentations apprises pour correspondre à un style donné. Le résultat est évalué qualitativement, puis quantitativement sur le jeu de données de compression de phrases Microsoft, avec des résultats encourageants.</abstract>
      <url hash="f2afd341">2017.jeptalnrecital-court.13</url>
    </paper>
    <paper id="14">
      <title>Amélioration de la similarité sémantique vectorielle par méthodes non-supervisées (Improved the Semantic Similarity with Weighting Vectors)</title>
      <language>fra</language>
      <author><first>El-Moatez-Billah</first><last>Nagoudi</last></author>
      <author><first>Jérémy</first><last>Ferrero</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>110–117</pages>
      <abstract/>
      <url hash="da9be31c">2017.jeptalnrecital-court.14</url>
    </paper>
    <paper id="15">
      <title>Typologies pour l’annotation de textes non standard en français (Typologies for the annotation of non-standard <fixed-case>F</fixed-case>rench texts)</title>
      <language>fra</language>
      <author><first>Louise</first><last>Tarrade</last></author>
      <author><first>Cédric</first><last>Lopez</last></author>
      <author><first>Rachel</first><last>Panckhurst</last></author>
      <author><first>Geroges</first><last>Antoniadis</last></author>
      <pages>118–125</pages>
      <abstract>La tâche de normalisation automatique des messages issus de la communication électronique médiée requiert une étape préalable consistant à identifier les phénomènes linguistiques. Dans cet article, nous proposons deux typologies pour l’annotation de textes non standard en français, relevant respectivement des niveaux morpho-lexical et morpho-syntaxique. Ces typologies ont été développées en conciliant les typologies existantes et en les faisant évoluer en parallèle d’une annotation manuelle de tweets et de SMS.</abstract>
      <url hash="5102b9bc">2017.jeptalnrecital-court.15</url>
    </paper>
    <paper id="16">
      <title>Simbow : une mesure de similarité sémantique entre textes (Simbow : a semantic similarity metric between texts)</title>
      <language>fra</language>
      <author><first>Delphine</first><last>Charlet</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <pages>126–133</pages>
      <abstract>Cet article décrit une mesure de similarité sémantique non-supervisée qui repose sur l’introduction d’une matrice de relations entre mots, dans un paradigme de mesure cosinus entre sacs de mots. La métrique obtenue, apparentée à soft-cosinus, tient compte des relations entre mots qui peuvent être d’ordre lexical ou sémantique selon la matrice considérée. La mise en œuvre de cette métrique sur la tâche qui consiste à mesurer des similarités sémantiques entre questions posées sur un forum, a remporté la campagne d’évaluation SemEval2017. Si l’approche soumise à la campagne est une combinaison supervisée de différentes mesures non-supervisées, nous présentons dans cet article en détail les métriques non-supervisées, qui présentent l’avantage de produire de bons résultats sans nécessiter de ressources spécifiques autres que des données non annotées du domaine considéré.</abstract>
      <url hash="60d3c3a5">2017.jeptalnrecital-court.16</url>
    </paper>
    <paper id="17">
      <title>Adaptation au domaine pour l’analyse morpho-syntaxique (Domain Adaptation for <fixed-case>P</fixed-case>o<fixed-case>S</fixed-case> tagging)</title>
      <language>fra</language>
      <author><first>Éléonor</first><last>Bartenlian</last></author>
      <author><first>Margot</first><last>Lacour</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>134–141</pages>
      <abstract>Ce travail cherche à comprendre pourquoi les performances d’un analyseur morpho-syntaxiques chutent fortement lorsque celui-ci est utilisé sur des données hors domaine. Nous montrons à l’aide d’une expérience jouet que ce comportement peut être dû à un phénomène de masquage des caractéristiques lexicalisées par les caractéristiques non lexicalisées. Nous proposons plusieurs modèles essayant de réduire cet effet.</abstract>
      <url hash="36d7e860">2017.jeptalnrecital-court.17</url>
    </paper>
    <paper id="18">
      <title>Représentation vectorielle de sens pour la désambiguïsation lexicale à base de connaissances (Sense Embeddings in Knowledge-Based Word Sense Disambiguation)</title>
      <language>fra</language>
      <author><first>Loïc</first><last>Vial</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>142–149</pages>
      <abstract>Dans cet article, nous proposons une nouvelle méthode pour représenter sous forme vectorielle les sens d’un dictionnaire. Nous utilisons les termes employés dans leur définition en les projetant dans un espace vectoriel, puis en additionnant les vecteurs résultants, avec des pondérations dépendantes de leur partie du discours et de leur fréquence. Le vecteur de sens résultant est alors utilisé pour trouver des sens reliés, permettant de créer un réseau lexical de manière automatique. Le réseau obtenu est ensuite évalué par rapport au réseau lexical de WordNet, construit manuellement. Pour cela nous comparons l’impact des différents réseaux sur un système de désambiguïsation lexicale basé sur la mesure de Lesk. L’avantage de notre méthode est qu’elle peut être appliquée à n’importe quelle langue ne possédant pas un réseau lexical comme celui de WordNet. Les résultats montrent que notre réseau automatiquement généré permet d’améliorer le score du système de base, atteignant quasiment la qualité du réseau de WordNet.</abstract>
      <url hash="64774371">2017.jeptalnrecital-court.18</url>
    </paper>
    <paper id="19">
      <title>Parcourir, reconnaître et réfléchir. Combinaison de méthodes légères pour l’extraction de relations sémantiques (Browse, recognize and think)</title>
      <language>fra</language>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Nathalie</first><last>Le Brun</last></author>
      <pages>150–157</pages>
      <abstract>La capture de relations sémantiques entre termes à partir de textes est un moyen privilégié de constituer/alimenter une base de connaissances, ressource indispensable pour l’analyse de textes. Nous proposons et évaluons la combinaison de trois méthodes de production de relations lexicosémantiques.</abstract>
      <url hash="1781ba3c">2017.jeptalnrecital-court.19</url>
    </paper>
    <paper id="20">
      <title>Si les souris étaient des reptiles, alors les reptiles pourraient être des mammifères ou Comment détecter les anomalies dans le réseau <fixed-case>JDM</fixed-case> ? (If mice were reptiles, then the reptiles could be mammals, or How to detect errors in a lexical network?)</title>
      <language>fra</language>
      <author><first>Alain</first><last>Joubert</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <author><first>Nathalie</first><last>Le Brun</last></author>
      <pages>158–164</pages>
      <abstract>La correction des erreurs dans une collection de données est un problème délicat. Elle peut être réalisée manuellement par un expert, ou en utilisant des méthodes de crowdsourcing, ou encore automatiquement au moyen d’algorithmes. Nous présentons ici des méthodes automatiques permettant de détecter les erreurs potentielles « secondaires » induites par les mécanismes automatiques d’inférences de relations, lorsqu’ils s’appuient sur des relations erronées « initiales » détectées manuellement. Des résultats encourageants, mesurés sur le réseau JeuxDeMots, nous invitent à envisager également des stratégies qui permettraient de détecter automatiquement les relations erronées « initiales », ce qui pourrait conduire à une détection automatique de la majorité des erreurs présentes dans le réseau.</abstract>
      <url hash="3cef853a">2017.jeptalnrecital-court.20</url>
    </paper>
    <paper id="21">
      <title>Vers l’annotation par le jeu de corpus (plus) complexes : le cas de la langue de spécialité (Towards (more) complex corpora annotation using a game with a purpose : the case of scientific language)</title>
      <language>fra</language>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <author><first>Nicolas</first><last>Lefebvre</last></author>
      <author><first>Laura</first><last>Ramírez</last></author>
      <author><first>Mathilde</first><last>Regnault</last></author>
      <author><first>Mary</first><last>Collins</last></author>
      <author><first>Oksana</first><last>Gavrilova</last></author>
      <author><first>Tanti</first><last>Kristanti</last></author>
      <pages>165–173</pages>
      <abstract>Nous avons précédemment montré qu’il est possible de faire produire des annotations syntaxiques de qualité par des participants à un jeu ayant un but. Nous présentons ici les résultats d’une expérience visant à évaluer leur production sur un corpus plus complexe, en langue de spécialité, en l’occurrence un corpus de textes scientifiques sur l’ADN. Nous déterminons précisément la complexité de ce corpus, puis nous évaluons les annotations en syntaxe de dépendances produites par les joueurs par rapport à une référence mise au point par des experts du domaine.</abstract>
      <url hash="1713ac34">2017.jeptalnrecital-court.21</url>
    </paper>
    <paper id="22">
      <title>Détection des mots non-standards dans les tweets avec des réseaux de neurones (Detecting non-standard words in tweets with neural networks)</title>
      <language>fra</language>
      <author><first>Tian</first><last>Tian</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <author><first>Marco</first><last>Dinarelli</last></author>
      <author><first>Pedro</first><last>Cardoso</last></author>
      <pages>174–182</pages>
      <abstract>Dans cet article, nous proposons un modèle pour détecter dans les textes générés par des utilisateurs (en particulier les tweets), les mots non-standards à corriger. Nous utilisons pour cela des réseaux de neurones convolutifs au niveau des caractères, associés à des “plongements” (embeddings) des mots présents dans le contexte du mot courant. Nous avons utilisé pour l’évaluation trois corpus de référence. Nous avons testé différents modèles qui varient suivant leurs plongements pré-entrainés, leurs configurations et leurs optimisations. Nous avons finalement obtenu une F1-mesure de 0.972 en validation croisée pour la classe des mots non-standards. Cette détection des mots à corriger est l’étape préliminaire pour la normalisation des textes non standards comme les tweets.</abstract>
      <url hash="d31cc79b">2017.jeptalnrecital-court.22</url>
    </paper>
    <paper id="23">
      <title><fixed-case>MAR</fixed-case>-<fixed-case>REL</fixed-case> : une base de marqueurs de relations conceptuelles pour la détection de Contextes Riches en Connaissances (<fixed-case>MAR</fixed-case>-<fixed-case>REL</fixed-case> : a conceptual relation markers database for Knowledge-Rich Contexts extraction)</title>
      <language>fra</language>
      <author><first>Luce</first><last>Lefeuvre</last></author>
      <author><first>Anne</first><last>Condamines</last></author>
      <pages>183–191</pages>
      <abstract>Les marqueurs de relation conceptuelle sont un moyen efficace de détecter automatiquement en corpus des Contextes Riches en Connaissances. Dans le domaine de la terminologie ou de l’ingénierie des connaissances, les Contextes Riches en Connaissances peuvent être utiles pour l’élaboration de ressources termino-ontologiques. Bien que la littérature concernant ce sujet soit riche, il n’existe pas de recensement systématique ni d’évaluation à grande échelle des marqueurs de relation conceptuelle. Pour ces raisons notamment, nous avons constitué une base de marqueurs pour les relations d’hyperonymie, de méronymie, et de cause, en français. Pour chacun de ces marqueurs, son taux de précision est proposé pour des corpus qui varient en fonction du domaine et du genre textuel.</abstract>
      <url hash="794da40d">2017.jeptalnrecital-court.23</url>
    </paper>
    <paper id="24">
      <title>Apprentissage en ligne interactif d’un générateur en langage naturel neuronal pour le dialogue homme-machine (On-line Interactive Learning of Natural Language Neural Generation for Human-machine)</title>
      <language>fra</language>
      <author><first>Matthieu</first><last>Riou</last></author>
      <author><first>Bassam</first><last>Jabaian</last></author>
      <author><first>Stéphane</first><last>Huet</last></author>
      <author><first>Fabrice</first><last>Lefèvre</last></author>
      <pages>192–199</pages>
      <abstract>Récemment, de nouveaux modèles à base de réseaux de neurones récurrents ont été proposés pour traiter la génération en langage naturel dans des systèmes de dialogue (Wen et al., 2016a). Ces modèles demandent une grande quantité de données d’apprentissage ; or la collecte et l’annotation de ces données peuvent être laborieuses. Pour répondre à cette problématique, nous nous intéressons ici à la mise en place d’un protocole d’apprentissage en ligne basé sur un apprentissage par renforcement, permettant d’améliorer l’utilisation d’un modèle initial appris sur un corpus plus restreint généré par patrons. Dans cette étude exploratoire, nous proposons une approche basée sur un algorithme de bandit contre un adversaire, afin d’en étudier l’intérêt et les limites.</abstract>
      <url hash="2bd857d8">2017.jeptalnrecital-court.24</url>
    </paper>
    <paper id="25">
      <title>Apports des analyses syntaxiques pour la détection automatique de mentions dans un corpus de français oral (Experiences in using deep and shallow parsing to detect entity mentions in oral <fixed-case>F</fixed-case>rench)</title>
      <language>fra</language>
      <author><first>Loïc</first><last>Grobol</last></author>
      <author><first>Isabelle</first><last>Tellier</last></author>
      <author><first>Éric</first><last>de La Clergerie</last></author>
      <author><first>Marco</first><last>Dinarelli</last></author>
      <author><first>Frédéric</first><last>Landragin</last></author>
      <pages>200–208</pages>
      <abstract>Cet article présente trois expériences de détection de mentions dans un corpus de français oral : ANCOR. Ces expériences utilisent des outils préexistants d’analyse syntaxique du français et des méthodes issues de travaux sur la coréférence, les anaphores et la détection d’entités nommées. Bien que ces outils ne soient pas optimisés pour le traitement de l’oral, la qualité de la détection des mentions que nous obtenons est comparable à l’état de l’art des systèmes conçus pour l’écrit dans d’autres langues. Nous concluons en proposant des perspectives pour l’amélioration des résultats que nous obtenons et la construction d’un système end-to-end pour lequel nos expériences peuvent servir de base de travail.</abstract>
      <url hash="20f4e849">2017.jeptalnrecital-court.25</url>
    </paper>
    <paper id="26">
      <title>Un nouveau besoin dans l’industrie : une aide au « rédacteur traduisant » (A new need in companies: writing assistance for “translating writers”)</title>
      <language>fra</language>
      <author><first>Claire</first><last>Lemaire</last></author>
      <pages>209–217</pages>
      <abstract>Nous mettons en relief, grâce à une expérimentation avec questionnaires et corpus parallèle, une situation nouvelle en entreprise de rédaction multilingue, pour laquelle il n’existe pas de technologie TAL dédiée. Nous suggérons de tirer profit de cette situation inédite de rédacteur traduisant, pour utiliser l’expertise du rédacteur pendant le processus de traduction et nous préconisons de développer une TA permettant une édition en cours de processus.</abstract>
      <url hash="92d2855a">2017.jeptalnrecital-court.26</url>
    </paper>
    <paper id="27">
      <title>Adaptation incrémentale de modèles de traduction neuronaux (Incremental adaptation of neural machine translation models)</title>
      <language>fra</language>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>218–225</pages>
      <abstract>L’adaptation au domaine est un verrou scientifique en traduction automatique. Il englobe généralement l’adaptation de la terminologie et du style, en particulier pour la post-édition humaine dans le cadre d’une traduction assistée par ordinateur. Avec la traduction automatique neuronale, nous étudions une nouvelle approche d’adaptation au domaine que nous appelons “spécialisation” et qui présente des résultats prometteurs tant dans la vitesse d’apprentissage que dans les scores de traduction. Dans cet article, nous proposons d’explorer cette approche.</abstract>
      <url hash="5c491754">2017.jeptalnrecital-court.27</url>
    </paper>
    <paper id="28">
      <title>Détection de concepts et granularité de l’annotation (Concept detection and annotation granularity )</title>
      <language>fra</language>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <pages>226–233</pages>
      <abstract>Nous nous intéressons ici à une tâche de détection de concepts dans des textes sans exigence particulière de passage par une phase de détection d’entités avec leurs frontières. Il s’agit donc d’une tâche de catégorisation de textes multiétiquette, avec des jeux de données annotés au niveau des textes entiers. Nous faisons l’hypothèse qu’une annotation à un niveau de granularité plus fin, typiquement au niveau de l’énoncé, devrait améliorer la performance d’un détecteur automatique entraîné sur ces données. Nous examinons cette hypothèse dans le cas de textes courts particuliers : des certificats de décès où l’on cherche à reconnaître des diagnostics, avec des jeux de données initialement annotés au niveau du certificat entier. Nous constatons qu’une annotation au niveau de la « ligne » améliore effectivement les résultats, mais aussi que le simple fait d’appliquer au niveau de la ligne un classifieur entraîné au niveau du texte est déjà une source d’amélioration.</abstract>
      <url hash="2e6bb367">2017.jeptalnrecital-court.28</url>
    </paper>
    <paper id="29">
      <title>Tri Automatique de la Littérature pour les Revues Systématiques (Automatically Ranking the Literature in Support of Systematic Reviews)</title>
      <language>fra</language>
      <author><first>Christopher</first><last>Norman</last></author>
      <author><first>Mariska</first><last>Leeflang</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <pages>234–241</pages>
      <abstract>Les revues systématiques de la littérature dans le domaine biomédical reposent essentiellement sur le travail bibliographique manuel d’experts. Nous évaluons les performances de la classification supervisée pour la découverte automatique d’articles à l’aide de plusieurs définitions des critères d’inclusion. Nous appliquons un modèle de regression logistique sur deux corpus issus de revues systématiques conduites dans le domaine du traitement automatique de la langue et de l’efficacité des médicaments. La classification offre une aire sous la courbe moyenne (AUC) de 0.769 si le classifieur est contruit à partir des jugements experts portés sur les titres et résumés des articles, et de 0.835 si on utilise les jugements portés sur le texte intégral. Ces résultats indiquent l’importance des jugements portés dès le début du processus de sélection pour développer un classifieur efficace pour accélérer l’élaboration des revues systématiques à l’aide d’un algorithme de classification standard.</abstract>
      <url hash="e660e625">2017.jeptalnrecital-court.29</url>
    </paper>
    <paper id="30">
      <title>Une approche linguistique pour la détection des dialectes arabes (A linguistic approach for the detection of <fixed-case>A</fixed-case>rabic dialects)</title>
      <language>fra</language>
      <author><first>Houda</first><last>Saâdane</last></author>
      <author><first>Damien</first><last>Nouvel</last></author>
      <author><first>Hosni</first><last>Seffih</last></author>
      <author><first>Christian</first><last>Fluhr</last></author>
      <pages>242–250</pages>
      <abstract>Dans cet article, nous présentons un processus d’identification automatique de l’origine dialectale pour la langue arabe de textes écrits en caractères arabes ou en écriture latine (arabizi). Nous décrivons le processus d’annotation des ressources construites et du système de translittération adopté. Deux approches d’identification de la langue sont comparées : la première est linguistique et exploite des dictionnaires, la seconde est statistique et repose sur des méthodes traditionnelles d’apprentissage automatique (n-grammes). L’évaluation de ces approches montre que la méthode linguistique donne des résultats satisfaisants, sans être dépendante des corpus d’apprentissage.</abstract>
      <url hash="9dfe4d07">2017.jeptalnrecital-court.30</url>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2020-07-21">
    <meta>
      <booktitle>Actes des 24ème Conférence sur le Traitement Automatique des Langues Naturelles. 19es REncontres jeunes Chercheurs en Informatique pour le TAL (RECITAL 2017)</booktitle>
      <editor><first>Iris</first><last>Eshkol-Taravella</last></editor>
      <editor><first>Jean-Yves</first><last>Antoine</last></editor>
      <publisher>ATALA</publisher>
      <address>Orléans, France</address>
      <month>6</month>
      <year>2017</year>
    </meta>
    <frontmatter>
      <url hash="f4611ca2">2017.jeptalnrecital-recital.0</url>
    </frontmatter>
    <paper id="1">
      <title>Machine Translation of Speech-Like Texts: Strategies for the Inclusion of Context</title>
      <author><first>Rachel</first><last>Bawden</last></author>
      <pages>1–14</pages>
      <abstract>Whilst the focus of Machine Translation (MT) has for a long time been the translation of planned, written texts, more and more research is being dedicated to translating speech-like texts (informal or spontaneous discourse or dialogue). To achieve high quality and natural translation of speechlike texts, the integration of context is needed, whether it is extra-linguistic (speaker identity, the interaction between speaker and interlocutor) or linguistic (coreference and stylistic phenomena linked to the spontaneous and informal nature of the texts). However, the integration of contextual information in MT systems remains limited in most current systems. In this paper, we present and critique three experiments for the integration of context into a MT system, each focusing on a different type of context and exploiting a different method: adaptation to speaker gender, cross-lingual pronoun prediction and the generation of tag questions from French into English.</abstract>
      <url hash="de8d432a">2017.jeptalnrecital-recital.1</url>
    </paper>
    <paper id="2">
      <title>Construction de lexiques pour l’extraction des mentions de maladies dans les forums de santé (Building lexica for extraction of mentions of diseases from healthcare fora)</title>
      <language>fra</language>
      <author><first>Elise</first><last>Bigeard</last></author>
      <pages>15–27</pages>
      <abstract>Les forums de discussion et les réseaux sociaux sont des sources potentielles de différents types d’information, qui ne sont en général pas accessibles par ailleurs. Par exemple, dans les forums de santé, il est possible de trouver les informations sur les habitudes et le mode de vie des personnes. Ces informations sont rarement partagées avec les médecins. Il est donc possible de se fonder sur ces informations pour évaluer les pratiques réelles des patients. Il s’agit cependant d’une source d’information difficile à traiter, essentiellement à cause des spécificités linguistiques qu’elle présente. Si une première étape pour l’exploration des forums consiste à indexer les termes médicaux présents dans les messages avec des concepts issus de terminologies médicales, cela s’avère extrêmement compliqué car les formulations des patients sont très différentes des terminologies officielles. Nous proposons une méthode permettant de créer et enrichir des lexiques de termes et expressions désignant une maladie ou un trouble, avec un intérêt particulier pour les troubles de l’humeur. Nous utilisons des ressources existantes ainsi que des méthodes non supervisées. Les ressources construites dans le cadre du travail nous permettent d’améliorer la détection de messages pertinents.</abstract>
      <url hash="767a7a99">2017.jeptalnrecital-recital.2</url>
    </paper>
    <paper id="3">
      <title>Création automatique d’une grammaire syntaxico-sémantique (Syntactic-semantic grammar automatic creation)</title>
      <language>fra</language>
      <author><first>Emilie</first><last>Colin</last></author>
      <pages>28–41</pages>
      <abstract>Nous proposons une nouvelle méthode pour la création automatique de grammaires lexicalisées syntaxico-sémantiques. A l’heure actuelle, la création de grammaire résulte soit d’un travail manuel soit d’un traitement automatisé de corpus arboré. Notre proposition est d’extraire à partir de données VerbNet une grammaire noyau (formes canoniques des verbes et des groupes nominaux) de l’anglais intégrant une sémantique VerbNet. Notre objectif est de profiter des larges ressources existantes pour produire un système de génération de texte symbolique de qualité en domaine restreint.</abstract>
      <url hash="40d81b0b">2017.jeptalnrecital-recital.3</url>
    </paper>
    <paper id="4">
      <title>Exploration de traits pour la reconnaissance d’entités nommées du Français par apprentissage automatique (Feature exploration for <fixed-case>F</fixed-case>rench Named Entity Recognition with Machine Learning)</title>
      <language>fra</language>
      <author><first>Yoann</first><last>Dupont</last></author>
      <pages>42–55</pages>
      <abstract>Dans cet article, nous explorons divers traits proposés dans la littérature afin de fournir un détecteur d’entités nommées pour le Français appris automatiquement sur le French Treebank. Nous étudions l’intégration de connaissances en domaine, l’apport de la classification des verbes, la gestion des mots inconnus et l’intégration de traits non locaux. Nous comparons ensuite notre système aux récents réseaux de neurones.</abstract>
      <url hash="3d6a379c">2017.jeptalnrecital-recital.4</url>
    </paper>
    <paper id="5">
      <title>Aligner production et normalisation : une première approche pour l’étude d’écrits scolaires (To align production and normalization : first approach to study school learner’s writings)</title>
      <language>fra</language>
      <author><first>Claire</first><last>Wolfarth</last></author>
      <pages>56–69</pages>
      <abstract>L’émergence des corpus scolaires et la volonté d’outiller ces corpus spécifiques font apparaitre de nouvelles problématiques de recherche pour le traitement automatique des langues (TAL). Nous exposons ici une recherche qui vise le traitement de productions d’apprenants en début d’apprentissage de l’écriture, en vue d’une annotation et d’une exploitation ultérieure. À cette fin, nous proposons d’envisager cette étape comme une tâche d’alignement entre la production de l’apprenant et une normalisation produite manuellement. Ce procédé permet d’augmenter significativement les scores d’identification des formes et lemmes produits et améliore les perspectives d’annotation.</abstract>
      <url hash="515b2a16">2017.jeptalnrecital-recital.5</url>
    </paper>
    <paper id="6">
      <title>Générer une grammaire d’arbres adjoints pour l’arabe à partir d’une méta-grammaire (Generate a tree adjoining grammar for arabic from a meta-grammar)</title>
      <language>fra</language>
      <author><first>Cherifa</first><last>Ben Khelil</last></author>
      <pages>70–80</pages>
      <abstract>La rareté des ressources numériques pour la langue arabe, telles que les grammaires et corpus, rend son traitement plus difficile que les autres langues naturelles. A ce jour il n’existe pas une grammaire formelle à large couverture de l’arabe. Dans ce papier, nous présentons une nouvelle approche qui facilite la description de l’arabe avec le formalisme des grammaires d’arbres adjoints en utilisant une méta-grammaire. Nous exposons les premiers résultats de notre grammaire ainsi que les problèmes rencontrés pour son évaluation.</abstract>
      <url hash="6ca585c8">2017.jeptalnrecital-recital.6</url>
    </paper>
    <paper id="7">
      <title>Déterminants et quantificateurs généralisés dynamiques (Determiners and dynamic generalised quantifiers)</title>
      <language>fra</language>
      <author><first>Clément</first><last>Beysson</last></author>
      <pages>81–93</pages>
      <abstract>Dans cet article, nous proposons une classification des déterminants en étudiant leur capacité à introduire de nouveaux référents du discours et l’accessibilité de ces référents. Cette classification se fonde sur des aspects de logique dynamique (Groenendijk &amp; Stokhof, 1991) dans la tradition montagovienne. Nous montrons ensuite que ces classes raffinent d’autres classifications plus linguistiques en étudiant chaque espèce de déterminants une à une. L’analyse de ces propriété est un première étape dans la définition des quantificateurs généralisés dynamiques nécessaires pour dénoter la sémantique des déterminants.</abstract>
      <url hash="63875461">2017.jeptalnrecital-recital.7</url>
    </paper>
    <paper id="8">
      <title>Détection de l’incertitude et de la négation : un état de l’art (Identifying uncertainty and negation’s cues and scope : State of the art One of the goals of our endeavours is to turn a corpus of medical documents into more easily readable structured data)</title>
      <language>fra</language>
      <author><first>Clément</first><last>Dalloux</last></author>
      <pages>94–107</pages>
      <abstract>L’un des objectifs de nos travaux, à terme, est de transformer un corpus de documents médicaux en données structurées pour en faciliter l’exploitation. Ainsi, il est nécessaire non seulement de détecter les concepts médicaux évoqués, mais aussi d’intégrer un processus capable d’identifier le contexte dans lequel est évoqué chaque concept médical. Dans cet article, nous revenons principalement sur les systèmes par apprentissage supervisé qui ont été proposé pour la détection de l’incertitude et de la négation. Ces dix dernières années, les travaux pour détecter l’incertitude et la négation dans les textes en anglais ont donné des résultats satisfaisants. Cependant, il existe encore une marge de progression non-négligeable.</abstract>
      <url hash="c317b634">2017.jeptalnrecital-recital.8</url>
    </paper>
    <paper id="9">
      <title>Normalisation de termes complexes par sémantique distributionnelle guidée par une ontologie (Normalization of complex terms with distributional semantics guided by an ontology)</title>
      <language>fra</language>
      <author><first>Arnaud</first><last>Ferré</last></author>
      <pages>108–120</pages>
      <abstract>Nous proposons dans cet article une méthode semi-supervisée originale pour la création de représentations vectorielles pour des termes (complexes ou non) dans un espace sémantique pertinent pour une tâche de normalisation de termes désignant des entités dans un corpus. Notre méthode s’appuie en partie sur une approche de sémantique distributionnelle, celle-ci générant des vecteurs initiaux pour chacun des termes extraits. Ces vecteurs sont alors plongés dans un autre espace vectoriel construit à partir de la structure d’une ontologie. Pour la construction de ce second espace vectoriel ontologique, plusieurs méthodes sont testées et comparées. Le plongement s’effectue par entraînement d’un modèle linéaire. Un calcul de distance (en utilisant la similarité cosinus) est enfin effectué pour déterminer la proximité entre vecteurs de termes et vecteurs de concepts de l’ontologie servant à la normalisation. La performance de cette méthode a atteint un rang honorable, ouvrant d’encourageantes perspectives.</abstract>
      <url hash="6771e0eb">2017.jeptalnrecital-recital.9</url>
    </paper>
    <paper id="10">
      <title>Annotation automatique des lieux dans l’oral spontané transcrit (Automatic annotation of places in the transcribed oral)</title>
      <language>fra</language>
      <author><first>Hélène</first><last>Flamein</last></author>
      <pages>121–134</pages>
      <abstract>Cet article a pour but de présenter une démarche généraliste pour l’annotation automatique des lieux dans l’oral transcrit. Cette annotation est effectuée sur le corpus ESLO (Enquête SocioLinguistique à Orléans) et suppose une réflexion sur les caractéristiques propres à la désignation d’un lieu à l’oral. Avant d’expliciter la méthode employée pour traiter automatiquement notre corpus, nous présenterons le travail préparatoire de la constitution d’une convention d’annotation et d’un corpus de référence indispensable pour l’évaluation du système.</abstract>
      <url hash="5f10556a">2017.jeptalnrecital-recital.10</url>
    </paper>
    <paper id="11">
      <title>Vers détection automatique des affirmations inappropriées dans les articles scientifiques (Towards automatic detection of inadequate claims in scientific articles)</title>
      <language>fra</language>
      <author><first>Anna</first><last>Koroleva</last></author>
      <pages>135–148</pages>
      <abstract>Dans cet article nous considérons l’apport du Traitement Automatique des Langues (TAL) au problème de la détection automatique de « l’embellissement » (en anglais « spin ») des résultats de recherche dans les publications scientifiques du domaine biomédical. Nous cherchons à identifier les affirmations inappropriées dans les articles, c’est-à-dire les affirmations où l’effet positif du traitement étudié est plus grand que celui effectivement prouvé par la recherche. Après une description du problème de point de vue du TAL, nous présentons les pistes de recherche qui nous semblent les plus prometteuses pour automatiser la détection de l’embellissement. Ensuite nous analysons l’état de l’art sur les tâches comparables et présentons les premiers résultats obtenus dans notre projet avec des méthodes de base (grammaires locales) pour la tâche de l’extraction des entités spécifiques à notre objectif.</abstract>
      <url hash="86a1302e">2017.jeptalnrecital-recital.11</url>
    </paper>
    <paper id="12">
      <title>Finding Missing Categories in Incomplete Utterances</title>
      <author><first>Mehdi</first><last>Mirzapour</last></author>
      <pages>149–160</pages>
      <abstract>Finding Missing Categories in Incomplete Utterances This paper introduces an efficient algorithm (O(n4 )) for finding a missing category in an incomplete utterance by using unification technique as when learning categorial grammars, and dynamic programming as in Cocke–Younger–Kasami algorithm. Using syntax/semantic interface of categorial grammar, this work can be used for deriving possible semantic readings of an incomplete utterance. The paper illustrates the problem with running examples.</abstract>
      <url hash="28c927d6">2017.jeptalnrecital-recital.12</url>
    </paper>
    <paper id="13">
      <title>Expressions polylexicales verbales : étude de la variabilité en corpus (Verbal <fixed-case>MWE</fixed-case>s : a corpus-based study of variability)</title>
      <language>fra</language>
      <author><first>Caroline</first><last>Pasquer</last></author>
      <pages>161–174</pages>
      <abstract>La reconnaissance et le traitement approprié des expressions polylexicales (EP) constituent un enjeu pour différentes applications en traitement automatique des langues. Ces expressions sont susceptibles d’apparaître sous d’autres formes que leur forme canonique, d’où l’intérêt d’étudier leur profil de variabilité. Dans cet article, nous proposons de donner un aperçu de motifs de variation syntaxiques et/ou morphologiques d’après un corpus de 4441 expressions polylexicales verbales (EPV) annotées manuellement. L’objectif poursuivi est de générer automatiquement les différentes variantes pour améliorer la performance des techniques de traitement automatique des EPV.</abstract>
      <url hash="d465bf41">2017.jeptalnrecital-recital.13</url>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2020-07-21">
    <meta>
      <booktitle>Actes des 24ème Conférence sur le Traitement Automatique des Langues Naturelles. Volume 3 - Démonstrations</booktitle>
      <editor><first>Iris</first><last>Eshkol-Taravella</last></editor>
      <editor><first>Jean-Yves</first><last>Antoine</last></editor>
      <publisher>ATALA</publisher>
      <address>Orléans, France</address>
      <month>6</month>
      <year>2017</year>
    </meta>
    <frontmatter>
      <url hash="ab5edebd">2017.jeptalnrecital-demo.0</url>
    </frontmatter>
    <paper id="1">
      <title>Les <fixed-case>TIC</fixed-case> au service de l’enseignement : Cas de la formation et auto-formation de la langue amazighe (Information and Communication Technologies for Education: the case of amazigh language teaching and self-teaching)</title>
      <language>fra</language>
      <author><first>Mounia</first><last>Boumediane</last></author>
      <pages>1–7</pages>
      <abstract>Le Centre des Études Informatiques, des Systèmes d’Information et de Communication (CEISIC), issu de l’Institut Royal de la Culture Amazighe (IRCAM), fédère au sein du portail TALAM, un ensemble de ressources linguistiques informatisées et d’outils de traitement de la langue dédiées à l’amazighe. Dans ce qui suit, nous présenterons les différentes ressources, applications et outils linguistiques développés en langue amazighe pour accompagner toute personne, de différente tranche d’âge, assoiffée d’apprendre la langue Amazighe.</abstract>
      <url hash="6b1eea07">2017.jeptalnrecital-demo.1</url>
    </paper>
    <paper id="2">
      <title>Wordsurf : un outil pour naviguer dans un espace de « Word Embeddings » (Wordsurf : a tool to surf in a “word embeddings” space)</title>
      <language>fra</language>
      <author><first>Philippe</first><last>Suignard</last></author>
      <pages>8–10</pages>
      <abstract>Dans cet article, nous présentons un outil appelé « Wordsurf » pour faciliter la phase d’exploration et de navigation dans un espace de « Word Embeddings » préalablement entrainé sur des corpus de textes avec Word2Vec.</abstract>
      <url hash="344f245a">2017.jeptalnrecital-demo.2</url>
    </paper>
    <paper id="3">
      <title>Un outil pour la manipulation de ressources arborées (A tool for handling tree-based linguistic resources)</title>
      <language>fra</language>
      <author><first>Yannick</first><last>Parmentier</last></author>
      <pages>11–14</pages>
      <abstract>Dans cet article, nous présentons brièvement pytreeview, un outil pour la manipulation de ressources arborées (corpus annotés, grammaires électroniques). Initialement conçu pour assiter les utilisateurs linguistes dans leur tâche de développement de grammaires arborescentes, pytreeview a été étendu pour permettre de manipuler des ressources arborées variées (grammaires mais aussi corpus aux formats FTB, PTB, CoNLL, Tiger), afin d’en extraire des informations utiles (par exemple la distribution des cadres de sous-catégorisation). pytreeview est actuellement utilisé dans le cadre d’un projet visant l’extraction semi-automatique de grammaires abstraites (méta-grammaires) à partir de corpus arborés.</abstract>
      <url hash="b71bbbc2">2017.jeptalnrecital-demo.3</url>
    </paper>
    <paper id="4">
      <title>Un étiqueteur en ligne du Français (An online tagger for <fixed-case>F</fixed-case>rench)</title>
      <language>fra</language>
      <author><first>Yoann</first><last>Dupont</last></author>
      <author><first>Clément</first><last>Plancq</last></author>
      <pages>15–16</pages>
      <abstract>Nous proposons ici une interface en ligne pour étiqueter des textes en français selon trois niveaux d’analyses : la morphosyntaxe, le chunking et la reconnaissance des entités nommées. L’interface se veut simple et les étiquetages réutilisables, ces derniers pouvant être exportés en différents formats.</abstract>
      <url hash="ee1baf81">2017.jeptalnrecital-demo.4</url>
    </paper>
    <paper id="5">
      <title>Apprentissage d’agents conversationnels pour la gestion de relations clients (Training chatbots for customer relation management)</title>
      <language>fra</language>
      <author><first>Benoit</first><last>Favre</last></author>
      <author><first>Frederic</first><last>Bechet</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <author><first>Delphine</first><last>Charlet</last></author>
      <pages>17–18</pages>
      <abstract>Ce travail démontre la faisabilité d’entraîner des chatbots sur des traces de conversations dans le domaine de la relation client. Des systèmes à base de modèles de langage, de recherche d’information et de traduction sont comparés pour la tâche.</abstract>
      <url hash="b801a3de">2017.jeptalnrecital-demo.5</url>
    </paper>
    <paper id="6">
      <title>Conception d’une solution de détection d’événements basée sur <fixed-case>T</fixed-case>witter (Design of a solution for event detection from Tweeter)</title>
      <language>fra</language>
      <author><first>Christophe</first><last>Servan</last></author>
      <author><first>Catherine</first><last>Kobus</last></author>
      <author><first>Yongchao</first><last>Deng</last></author>
      <author><first>Cyril</first><last>Touffet</last></author>
      <author><first>Jungi</first><last>Kim</last></author>
      <author><first>Inès</first><last>Kapp</last></author>
      <author><first>Djamel</first><last>Mostefa</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>Aurélien</first><last>Coquard</last></author>
      <author><first>Jean</first><last>Senellart</last></author>
      <pages>19–20</pages>
      <abstract>Cet article présente un système d’alertes fondé sur la masse de données issues de Tweeter. L’objectif de l’outil est de surveiller l’actualité, autour de différents domaines témoin incluant les événements sportifs ou les catastrophes naturelles. Cette surveillance est transmise à l’utilisateur sous forme d’une interface web contenant la liste d’événements localisés sur une carte.</abstract>
      <url hash="ff369147">2017.jeptalnrecital-demo.6</url>
    </paper>
    <paper id="7">
      <title>Une plateforme de recommandation automatique d’emojis (An emoji recommandation platform)</title>
      <language>fra</language>
      <author><first>Gaël</first><last>Guibon</last></author>
      <author><first>Magalie</first><last>Ochs</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <pages>21–23</pages>
      <abstract>Nous présentons une interface de recommandation d’emojis porteurs de sentiments qui utilise un modèle de prédiction appris sur des messages informels privés. Chacun étant associé à deux scores de polarité prédits. Cette interface permet permet également d’enregistrer les choix de l’utilisateur pour confirmer ou infirmer la recommandation.</abstract>
      <url hash="ddd7dffe">2017.jeptalnrecital-demo.7</url>
    </paper>
    <paper id="8">
      <title>Un outil modulaire libre pour le résumé automatique (A Modular Open Source Tool for Automatic Summarization)</title>
      <language>fra</language>
      <author><first>Valentin</first><last>Nyzam</last></author>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <pages>24–26</pages>
      <abstract>automatique Valentin Nyzam Aurélien Bossard LIASD, Université Paris 8 - IUT de Montreuil, 140 rue de la Nouvelle France, 93100 Montreuil, France valentin.nyzam@iut.univ-paris8.fr, aurelien.bossard@iut.univ-paris8.fr R ÉSUMÉ Nous proposons une démonstration d’un outil modulaire et évolutif de résumé automatique qui implémente trois méthodes d’extraction de phrases de l’état de l’art ainsi que sept méthodes d’évaluation des phrases. L’outil est développé en Java et est d’ores-et-déjà disponible sur la plateforme Github.</abstract>
      <url hash="37a2be4d">2017.jeptalnrecital-demo.8</url>
    </paper>
    <paper id="9">
      <title>Uniformisation de corpus anglais annotés en sens (Unification of sense annotated <fixed-case>E</fixed-case>nglish corpora for word sense disambiguation)</title>
      <language>fra</language>
      <author><first>Loïc</first><last>Vial</last></author>
      <author><first>Benjamin</first><last>Lecouteux</last></author>
      <author><first>Didier</first><last>Schwab</last></author>
      <pages>27–29</pages>
      <abstract>Pour la désambiguïsation lexicale en anglais, on compte aujourd’hui une quinzaine de corpus annotés en sens dans des formats souvent différents et provenant de différentes versions du Princeton WordNet. Nous présentons un format pour uniformiser ces corpus, et nous fournissons à la communauté l’ensemble des corpus annotés en anglais portés à notre connaissance avec des sens uniformisés du Princeton WordNet 3.0, lorsque les droits le permettent et le code source pour construire l’ensemble des corpus à partir des données originales.</abstract>
      <url hash="b84d6fbd">2017.jeptalnrecital-demo.9</url>
    </paper>
    <paper id="10">
      <title>Résumer automatiquement en ligne : démonstration d’un service web de résumé multidocument (Summarizing Automatically Online : We propose a demonstration of an automatic multidocument summarization web service)</title>
      <language>fra</language>
      <author><first>Valentin</first><last>Nyzam</last></author>
      <author><first>Nathan</first><last>Gatto</last></author>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <pages>30–32</pages>
      <abstract>r automatiquement en ligne : démonstration d’un service web de résumé multidocument Valentin Nyzam Nathan Gatto Aurélien Bossard LIASD, Université Paris 8 - IUT de Montreuil, 140 rue de la Nouvelle France, 93100 Montreuil, France valentin.nyzam@iut.univ-paris8.fr, nathan.gatto@free.fr, aurelien.bossard@iut.univ-paris8.fr R ÉSUMÉ Nous proposons une démonstration d’un webservice de résumé automatique multidocument. Ce webservice s’appuie sur un outil ouvert qui implémente plusieurs algorithmes reconnus de résumé automatique, et permet de résumer des documents en utilisant des configurations différentes.</abstract>
      <url hash="6f1d18ee">2017.jeptalnrecital-demo.10</url>
    </paper>
    <paper id="11">
      <title>Traitement automatique de la langue biomédicale au <fixed-case>LIMSI</fixed-case> (Biomedical language processing at <fixed-case>LIMSI</fixed-case>)</title>
      <language>fra</language>
      <author><first>Christopher</first><last>Norman</last></author>
      <author><first>Cyril</first><last>Grouin</last></author>
      <author><first>Thomas</first><last>Lavergne</last></author>
      <author><first>Aurélie</first><last>Névéol</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>33–34</pages>
      <abstract>Nous proposons des démonstrations de trois outils développés par le LIMSI en traitement automatique des langues appliqué au domaine biomédical : la détection de concepts médicaux dans des textes courts, la catégorisation d’articles scientifiques pour l’assistance à l’écriture de revues systématiques, et l’anonymisation de textes cliniques.</abstract>
      <url hash="dde1ceef">2017.jeptalnrecital-demo.11</url>
    </paper>
    <paper id="12">
      <title>Proxem Studio : la plate-forme d’analyse sémantique qui transforme l’utilisateur métier en text scientist (<fixed-case>P</fixed-case>roxem<fixed-case>S</fixed-case>tudio: the semantic analysis platform that turns the business user into a text scientist)</title>
      <language>fra</language>
      <author><first>Francois-Regis</first><last>Chaumartin</last></author>
      <pages>35–36</pages>
      <abstract>Proxem édite depuis 2011 une plate-forme d’analyse sémantique multilingue utilisé en entreprise pour de multiples usages : relation clients, ressources humaines, veille stratégique... La version la plus récente du logiciel, lancée en mars 2017, lève le principal goulet d’étranglement des outils classiques de text mining : un utilisateur métier devient enfin autonome pour définir lui-même les ressources linguistiques nécessaires à l’analyse sémantique d’un corpus donné. Une fois le corpus chargé, la plate-forme en extrait une terminologie et organise les termes en regroupements hiérarchisés de proto-concepts ; l’utilisateur n’a plus qu’à valider ces concepts au niveau de granularité qui lui semble pertinent pour constituer un extracteur d’entités nommées de granularité fine, adapté au corpus à traiter, avec un rappel élevé grâce à l’identification automatique des quasisynonymes. La plate-forme détecte aussi dans ces termes les homonymes potentiels et propose à l’utilisateur des contextes de désambiguïsation, fournissant ainsi une bonne précision.</abstract>
      <url hash="75c71923">2017.jeptalnrecital-demo.12</url>
    </paper>
    <paper id="13">
      <title>Translittération automatique pour une paire de langues peu dotée ()</title>
      <language>fra</language>
      <author><first>Ngoc</first><last>Tan Le</last></author>
      <author><first>Fatiha</first><last>Sadat</last></author>
      <author><first>Lucie</first><last>Ménard</last></author>
      <pages>37–40</pages>
      <abstract>La translittération convertit phonétiquement les mots dans une langue source (i.e. français) en mots équivalents dans une langue cible (i.e. vietnamien). Cette conversion nécessite un nombre considérable de règles définies par les experts linguistes pour déterminer comment les phonèmes sont alignés ainsi que prendre en compte le système de phonologie de la langue cible. La problématique pour les paires de langues peu dotées lie à la pénurie des ressources linguistiques. Dans ce travail de recherche, nous présentons une démonstration de conversion de graphème en phonème pour pallier au problème de translittération pour une paire de langues peu dotée, avec une application sur français-vietnamien. Notre système nécessite un petit corpus d’apprentissage phonétique bilingue. Nous avons obtenu des résultats prometteurs, avec un gain de +4,40% de score BLEU, par rapport au système de base utilisant l’approche de traduction automatique statistique.</abstract>
      <url hash="57457222">2017.jeptalnrecital-demo.13</url>
    </paper>
    <paper id="14">
      <title>Motor, un outil de segmentation accessible en ligne (Motor, a segmentation tool accessible online)</title>
      <language>fra</language>
      <author><first>Guillaume</first><last>de Malézieux</last></author>
      <author><first>Jennifer</first><last>Lewis-Wong</last></author>
      <author><first>Vincent</first><last>Berment</last></author>
      <pages>41–42</pages>
      <abstract>Dans cette démonstration, nous montrons le fonctionnement des segmenteurs disponibles en ligne pour diverses langues (birman, khmer, lao, thaï et tibétain) et réalisés avec l’outil Motor.</abstract>
      <url hash="f04a4158">2017.jeptalnrecital-demo.14</url>
    </paper>
  </volume>
</collection>
