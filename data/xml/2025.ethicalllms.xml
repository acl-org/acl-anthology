<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.ethicalllms">
  <volume id="1" ingest-date="2026-01-07" type="proceedings">
    <meta>
      <booktitle>Proceedings of the First Workshop on Ethical Concerns in Training, Evaluating and Deploying Large Language Models</booktitle>
      <editor><first>Damith</first><last>Premasiri</last></editor>
      <editor><first>Tharindu</first><last>Ranasinghe</last></editor>
      <editor><first>Hansi</first><last>Hettiarachchi</last></editor>
      <publisher>INCOMA Ltd., Shoumen, Bulgaria</publisher>
      <address>Varna, Bulgaria</address>
      <month>September</month>
      <year>2025</year>
      <url hash="5887a207">2025.ethicalllms-1</url>
      <venue>ethicalllms</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="0e9be915">2025.ethicalllms-1.0</url>
      <bibkey>ethicalllms-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>T</fixed-case>ext<fixed-case>B</fixed-case>andit: Evaluating Probabilistic Reasoning in <fixed-case>LLM</fixed-case>s Through Language-Only Decision Tasks</title>
      <author><first>Arjun</first><last>Damerla</last></author>
      <author><first>Jimin</first><last>Lim</last></author>
      <author><first>Yanxi</first><last>Jiang</last></author>
      <author><first>Nam Nguyen Hoai</first><last>Le</last></author>
      <author><first>Nikil</first><last>Selladurai</last></author>
      <pages>1–8</pages>
      <abstract>Large language models (LLMs) have shown to be increasingly capable of performing reasoning tasks, but their ability to make sequential decisions under uncertainty only using natural language remains under-explored. We introduce a novel benchmark in which LLMs interact with multi-armed bandit environments using purely textual feedback, “you earned a token”, without access to numerical cues or explicit probabilities, resulting in the model to infer latent reward structures purely off linguistic cues and to adapt accordingly. We evaluated the performance of four open-source LLMs and compare their performance to standard decision-making algorithms such as Thompson Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice. While most of the LLMs underperformed compared to the baselines, Qwen3-4B, achieved the best-arm selection rate of 89.2% , which significantly outperformed both the larger LLMs and traditional methods. Our findings suggest that probabilistic reasoning is able to emerge from language alone, and we present this benchmark as a step towards evaluating decision-making capabilities in naturalistic, non-numeric contexts.</abstract>
      <url hash="d7c53805">2025.ethicalllms-1.1</url>
      <bibkey>damerla-etal-2025-textbandit</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>C</fixed-case>o<fixed-case>V</fixed-case>e<fixed-case>GAT</fixed-case>: A Hybrid <fixed-case>LLM</fixed-case> &amp; <fixed-case>G</fixed-case>raph‐<fixed-case>A</fixed-case>ttention Pipeline for Accurate <fixed-case>C</fixed-case>itation‐<fixed-case>A</fixed-case>ligned Claim Verification</title>
      <author><first>Max</first><last>Bader</last></author>
      <author><first>Akshatha</first><last>Arunkumar</last></author>
      <author><first>Ohan</first><last>Ahmad</last></author>
      <author><first>Maruf</first><last>Hassen</last></author>
      <author><first>Charles</first><last>Duong</last></author>
      <author><first>Kevin</first><last>Zhu</last></author>
      <pages>9–16</pages>
      <abstract>Modern LLMs often generate fluent text yet fabricate, misquote, or misattribute evidence. To quantify this flaw, we built a balanced Citation‐Alignment Dataset of 500 genuine, expert‐verified claim–quote pairs and 500 minimally perturbed false variants from news, legal, scientific, and literary sources. We then propose CoVeGAT, which converts claims and citations into SVO triplets (with trigram fallback), scores each pair via an LLM‐driven chain of verification, and embeds them in a weighted semantic graph. A Graph Attention Network over BERT embeddings issues strict pass/fail judgments on alignment. Zero‐shot evaluation of seven top LLMs (e.g., GPT‐4o, Gemini 1.5, Mistral 7B) reveals a trade‐off: decisive models reach 82.5 % accuracy but err confidently, while cautious ones fall below 50 %. A MiniLM + RBF kernel baseline, by contrast, achieves 96.4 % accuracy, underscoring the power of simple, interpretable methods.</abstract>
      <url hash="e61a825b">2025.ethicalllms-1.2</url>
      <bibkey>bader-etal-2025-covegat</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>TVS</fixed-case> Sidekick: Challenges and Practical Insights from Deploying Large Language Models in the Enterprise</title>
      <author><first>Paula</first><last>Reyero Lobo</last></author>
      <author><first>Kevin</first><last>Johnson</last></author>
      <author><first>Bill</first><last>Buchanan</last></author>
      <author><first>Matthew</first><last>Shardlow</last></author>
      <author><first>Ashley</first><last>Williams</last></author>
      <author><first>Sam</first><last>Attwood</last></author>
      <pages>17–26</pages>
      <abstract>Many enterprises are increasingly adopting Artificial Intelligence (AI) to make internal processes more competitive and efficient. In response to public concern and new regulations for the ethical and responsible use of AI, implementing AI governance frameworks could help to integrate AI within organisations and mitigate associated risks. However, the rapid technological advances and lack of shared ethical AI infrastructures creates barriers to their practical adoption in businesses. This paper presents a real-world AI application at TVS Supply Chain Solutions, reporting on the experience developing an AI assistant underpinned by large language models and the ethical, regulatory, and sociotechnical challenges in deployment for enterprise use.</abstract>
      <url hash="5d7e6181">2025.ethicalllms-1.3</url>
      <bibkey>reyero-lobo-etal-2025-tvs</bibkey>
    </paper>
  </volume>
</collection>
