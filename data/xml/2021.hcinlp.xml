<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.hcinlp">
  <volume id="1" ingest-date="2021-04-19">
    <meta>
      <booktitle>Proceedings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing</booktitle>
      <editor><first>Su Lin</first><last>Blodgett</last></editor>
      <editor><first>Michael</first><last>Madaio</last></editor>
      <editor><first>Brendan</first><last>O'Connor</last></editor>
      <editor><first>Hanna</first><last>Wallach</last></editor>
      <editor><first>Qian</first><last>Yang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>April</month>
      <year>2021</year>
      <venue>hcinlp</venue>
    </meta>
    <frontmatter>
      <url hash="9060d684">2021.hcinlp-1.0</url>
      <bibkey>hcinlp-2021-bridging</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards Hybrid Human-Machine Workflow for Natural Language Generation</title>
      <author><first>Neslihan</first><last>Iskender</last></author>
      <author><first>Tim</first><last>Polzehl</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>1–7</pages>
      <abstract>In recent years, crowdsourcing has gained much attention from researchers to generate data for the Natural Language Generation (NLG) tools or to evaluate them. However, the quality of crowdsourced data has been questioned repeatedly because of the complexity of NLG tasks and crowd workers’ unknown skills. Moreover, crowdsourcing can also be costly and often not feasible for large-scale data generation or evaluation. To overcome these challenges and leverage the complementary strengths of humans and machine tools, we propose a hybrid human-machine workflow designed explicitly for NLG tasks with real-time quality control mechanisms under budget constraints. This hybrid methodology is a powerful tool for achieving high-quality data while preserving efficiency. By combining human and machine intelligence, the proposed workflow decides dynamically on the next step based on the data from previous steps and given constraints. Our goal is to provide not only the theoretical foundations of the hybrid workflow but also to provide its implementation as open-source in future work.</abstract>
      <url hash="a0f2cf5b">2021.hcinlp-1.1</url>
      <bibkey>iskender-etal-2021-towards</bibkey>
    </paper>
    <paper id="2">
      <title>Spellchecking for Children in Web Search: a Natural Language Interface Case-study</title>
      <author><first>Casey</first><last>Kennington</last></author>
      <author><first>Jerry Alan</first><last>Fails</last></author>
      <author><first>Katherine</first><last>Landau Wright</last></author>
      <author><first>Maria Soledad</first><last>Pera</last></author>
      <pages>8–13</pages>
      <abstract>Given the more widespread nature of natural language interfaces, it is increasingly important to understand who are accessing those interfaces, and how those interfaces are being used. In this paper, we explore spellchecking in the context of web search with children as the target audience. In particular, via a literature review we show that, while widely used, popular search tools are ill-designed for children. We then use spellcheckers as a case study to highlight the need for an interdisciplinary approach that brings together natural language processing, education, human-computer interaction to address a known information retrieval problem: query misspelling. We conclude that it is imperative that those for whom the interfaces are designed have a voice in the design process.</abstract>
      <url hash="3f7cd24f">2021.hcinlp-1.2</url>
      <bibkey>kennington-etal-2021-spellchecking</bibkey>
    </paper>
    <paper id="3">
      <title>Capturing Covertly Toxic Speech via Crowdsourcing</title>
      <author><first>Alyssa</first><last>Lees</last></author>
      <author><first>Daniel</first><last>Borkan</last></author>
      <author><first>Ian</first><last>Kivlichan</last></author>
      <author><first>Jorge</first><last>Nario</last></author>
      <author><first>Tesh</first><last>Goyal</last></author>
      <pages>14–20</pages>
      <abstract>We study the task of labeling covert or veiled toxicity in online conversations. Prior research has highlighted the difficulty in creating language models that recognize nuanced toxicity such as microaggressions. Our investigations further underscore the difficulty in parsing such labels reliably from raters via crowdsourcing. We introduce an initial dataset, COVERTTOXICITY, which aims to identify and categorize such comments from a refined rater template. Finally, we fine-tune a comment-domain BERT model to classify covertly offensive comments and compare against existing baselines.</abstract>
      <url hash="34db5491">2021.hcinlp-1.3</url>
      <bibkey>lees-etal-2021-capturing</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/sbic">SBIC</pwcdataset>
    </paper>
    <paper id="4">
      <title>Towards Human-Centered Summarization: A Case Study on Financial News</title>
      <author><first>Tatiana</first><last>Passali</last></author>
      <author><first>Alexios</first><last>Gidiotis</last></author>
      <author><first>Efstathios</first><last>Chatzikyriakidis</last></author>
      <author><first>Grigorios</first><last>Tsoumakas</last></author>
      <pages>21–27</pages>
      <abstract>Recent Deep Learning (DL) summarization models greatly outperform traditional summarization methodologies, generating high-quality summaries. Despite their success, there are still important open issues, such as the limited engagement and trust of users in the whole process. In order to overcome these issues, we reconsider the task of summarization from a human-centered perspective. We propose to integrate a user interface with an underlying DL model, instead of tackling summarization as an isolated task from the end user. We present a novel system, where the user can actively participate in the whole summarization process. We also enable the user to gather insights into the causative factors that drive the model’s behavior, exploiting the self-attention mechanism. We focus on the financial domain, in order to demonstrate the efficiency of generic DL models for domain-specific applications. Our work takes a first step towards a model-interface co-design approach, where DL models evolve along user needs, paving the way towards human-computer text summarization interfaces.</abstract>
      <url hash="c0c3ef39">2021.hcinlp-1.4</url>
      <bibkey>passali-etal-2021-towards</bibkey>
    </paper>
    <paper id="5">
      <title>Methods for the Design and Evaluation of <fixed-case>HCI</fixed-case>+<fixed-case>NLP</fixed-case> Systems</title>
      <author><first>Hendrik</first><last>Heuer</last></author>
      <author><first>Daniel</first><last>Buschek</last></author>
      <pages>28–33</pages>
      <abstract>HCI and NLP traditionally focus on different evaluation methods. While HCI involves a small number of people directly and deeply, NLP traditionally relies on standardized benchmark evaluations that involve a larger number of people indirectly. We present five methodological proposals at the intersection of HCI and NLP and situate them in the context of ML-based NLP models. Our goal is to foster interdisciplinary collaboration and progress in both fields by emphasizing what the fields can learn from each other.</abstract>
      <url hash="d4a6277b">2021.hcinlp-1.5</url>
      <bibkey>heuer-buschek-2021-methods</bibkey>
    </paper>
    <paper id="6">
      <title>Context-sensitive evaluation of automatic speech recognition: considering user experience &amp; language variation</title>
      <author><first>Nina</first><last>Markl</last></author>
      <author><first>Catherine</first><last>Lai</last></author>
      <pages>34–40</pages>
      <abstract>Commercial Automatic Speech Recognition (ASR) systems tend to show systemic predictive bias for marginalised speaker/user groups. We highlight the need for an interdisciplinary and context-sensitive approach to documenting this bias incorporating perspectives and methods from sociolinguistics, speech &amp; language technology and human-computer interaction in the context of a case study. We argue evaluation of ASR systems should be disaggregated by speaker group, include qualitative error analysis, and consider user experience in a broader sociolinguistic and social context.</abstract>
      <url hash="b70a54c2">2021.hcinlp-1.6</url>
      <bibkey>markl-lai-2021-context</bibkey>
    </paper>
    <paper id="7">
      <title>Embodied Multimodal Agents to Bridge the Understanding Gap</title>
      <author><first>Nikhil</first><last>Krishnaswamy</last></author>
      <author><first>Nada</first><last>Alalyani</last></author>
      <pages>41–46</pages>
      <abstract>In this paper we argue that embodied multimodal agents, i.e., avatars, can play an important role in moving natural language processing toward “deep understanding.” Fully-featured interactive agents, model encounters between two “people,” but a language-only agent has little environmental and situational awareness. Multimodal agents bring new opportunities for interpreting visuals, locational information, gestures, etc., which are more axes along which to communicate. We propose that multimodal agents, by facilitating an embodied form of human-computer interaction, provide additional structure that can be used to train models that move NLP systems closer to genuine “understanding” of grounded language, and we discuss ongoing studies using existing systems.</abstract>
      <url hash="716675dd">2021.hcinlp-1.7</url>
      <bibkey>krishnaswamy-alalyani-2021-embodied</bibkey>
    </paper>
    <paper id="8">
      <title>Putting Humans in the Natural Language Processing Loop: A Survey</title>
      <author><first>Zijie J.</first><last>Wang</last></author>
      <author><first>Dongjin</first><last>Choi</last></author>
      <author><first>Shenyu</first><last>Xu</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>47–52</pages>
      <abstract>How can we design Natural Language Processing (NLP) systems that learn from human feedback? There is a growing research body of Human-in-the-loop (HITL) NLP frameworks that continuously integrate human feedback to improve the model itself. HITL NLP research is nascent but multifarious—solving various NLP problems, collecting diverse feedback from different people, and applying different methods to learn from human feedback. We present a survey of HITL NLP work from both Machine Learning (ML) and Human-computer Interaction (HCI) communities that highlights its short yet inspiring history, and thoroughly summarize recent frameworks focusing on their tasks, goals, human interactions, and feedback learning methods. Finally, we discuss future studies for integrating human feedback in the NLP development loop.</abstract>
      <url hash="1e3ea97a">2021.hcinlp-1.8</url>
      <bibkey>wang-etal-2021-putting</bibkey>
    </paper>
    <paper id="9">
      <title>Can You Distinguish Truthful from Fake Reviews? User Analysis and Assistance Tool for Fake Review Detection</title>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Junmo</first><last>Kang</last></author>
      <author><first>Suwon</first><last>Shin</last></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <pages>53–59</pages>
      <abstract>Customer reviews are useful in providing an indirect, secondhand experience of a product. People often use reviews written by other customers as a guideline prior to purchasing a product. Such behavior signifies the authenticity of reviews in e-commerce platforms. However, fake reviews are increasingly becoming a hassle for both consumers and product owners. To address this issue, we propose You Only Need Gold (YONG), an essential information mining tool for detecting fake reviews and augmenting user discretion. Our experimental results show the poor human performance on fake review detection, substantially improved user capability given our tool, and the ultimate need for user reliance on the tool.</abstract>
      <url hash="51f1dcdd">2021.hcinlp-1.9</url>
      <bibkey>kim-etal-2021-distinguish</bibkey>
    </paper>
    <paper id="10">
      <title>Challenges in Designing Games with a Purpose for Abusive Language Annotation</title>
      <author><first>Federico</first><last>Bonetti</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <pages>60–65</pages>
      <abstract>In this paper we discuss several challenges related to the development of a 3D game, whose goal is to raise awareness on cyberbullying while collecting linguistic annotation on offensive language. The game is meant to be used by teenagers, thus raising a number of issues that need to be tackled during development. For example, the game aesthetics should be appealing for players belonging to this age group, but at the same time all possible solutions should be implemented to meet privacy requirements. Also, the task of linguistic annotation should be possibly hidden, adopting so-called orthogonal game mechanics, without affecting the quality of collected data. While some of these challenges are being tackled in the game development, some others are discussed in this paper but still lack an ultimate solution.</abstract>
      <url hash="897c18f7">2021.hcinlp-1.10</url>
      <bibkey>bonetti-tonelli-2021-challenges</bibkey>
    </paper>
    <paper id="11">
      <title>Challenges in Designing Natural Language Interfaces for Complex Visual Models</title>
      <author><first>Henrik</first><last>Voigt</last></author>
      <author><first>Monique</first><last>Meuschke</last></author>
      <author><first>Kai</first><last>Lawonn</last></author>
      <author><first>Sina</first><last>Zarrieß</last></author>
      <pages>66–73</pages>
      <abstract>Intuitive interaction with visual models becomes an increasingly important task in the field of Visualization (VIS) and verbal interaction represents a significant aspect of it. Vice versa, modeling verbal interaction in visual environments is a major trend in ongoing research in NLP. To date, research on Language &amp; Vision, however, mostly happens at the intersection of NLP and Computer Vision (CV), and much less at the intersection of NLP and Visualization, which is an important area in Human-Computer Interaction (HCI). This paper presents a brief survey of recent work on interactive tasks and set-ups in NLP and Visualization. We discuss the respective methods, show interesting gaps, and conclude by suggesting neural, visually grounded dialogue modeling as a promising potential for NLIs for visual models.</abstract>
      <url hash="fb2e8946">2021.hcinlp-1.11</url>
      <bibkey>voigt-etal-2021-challenges</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>A</fixed-case>fri<fixed-case>KI</fixed-case>: Machine-in-the-Loop <fixed-case>A</fixed-case>frikaans Poetry Generation</title>
      <author><first>Imke</first><last>van Heerden</last></author>
      <author><first>Anil</first><last>Bas</last></author>
      <pages>74–80</pages>
      <abstract>This paper proposes a generative language model called AfriKI. Our approach is based on an LSTM architecture trained on a small corpus of contemporary fiction. With the aim of promoting human creativity, we use the model as an authoring tool to explore machine-in-the-loop Afrikaans poetry generation. To our knowledge, this is the first study to attempt creative text generation in Afrikaans.</abstract>
      <url hash="247274a2">2021.hcinlp-1.12</url>
      <bibkey>van-heerden-bas-2021-afriki</bibkey>
    </paper>
    <paper id="13">
      <title>Situation-Specific Multimodal Feature Adaptation</title>
      <author><first>Özge</first><last>Alacam</last></author>
      <pages>81–87</pages>
      <abstract>In the next decade, we will see a considerable need for NLP models for situated settings where diversity of situations and also different modalities including eye-movements should be taken into account in order to grasp the intention of the user. However, language comprehension in situated settings can not be handled in isolation, where different multimodal cues are inherently present and essential parts of the situations. In this research proposal, we aim to quantify the influence of each modality in interaction with various referential complexities. We propose to encode the referential complexity of the situated settings in the embeddings during pre-training to implicitly guide the model to the most plausible situation-specific deviations. We summarize the challenges of intention extraction and propose a methodological approach to investigate a situation-specific feature adaptation to improve crossmodal mapping and meaning recovery from noisy communication settings.</abstract>
      <url hash="41735010">2021.hcinlp-1.13</url>
      <bibkey>alacam-2021-situation</bibkey>
    </paper>
    <paper id="14">
      <title>Machine Translation Believability</title>
      <author><first>Marianna</first><last>Martindale</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>88–95</pages>
      <abstract>Successful Machine Translation (MT) deployment requires understanding not only the intrinsic qualities of MT output, such as fluency and adequacy, but also user perceptions. Users who do not understand the source language respond to MT output based on their perception of the likelihood that the meaning of the MT output matches the meaning of the source text. We refer to this as believability. Output that is not believable may be off-putting to users, but believable MT output with incorrect meaning may mislead them. In this work, we study the relationship of believability to fluency and adequacy by applying traditional MT direct assessment protocols to annotate all three features on the output of neural MT systems. Quantitative analysis of these annotations shows that believability is closely related to but distinct from fluency, and initial qualitative analysis suggests that semantic features may account for the difference.</abstract>
      <url hash="94888bde">2021.hcinlp-1.14</url>
      <bibkey>martindale-etal-2021-machine</bibkey>
      <pwccode url="https://github.com/mjmartindale/mt_believability" additional="false">mjmartindale/mt_believability</pwccode>
    </paper>
    <paper id="15">
      <title>Provocation: Contestability in Large-Scale Interactive <fixed-case>NLP</fixed-case> Systems</title>
      <author><first>Tanushree</first><last>Mitra</last></author>
      <pages>96–100</pages>
      <abstract/>
      <url hash="7e5bbb70">2021.hcinlp-1.15</url>
      <bibkey>mitra-2021-provocation</bibkey>
    </paper>
    <paper id="16">
      <title>An <fixed-case>IDR</fixed-case> Framework of Opportunities and Barriers between <fixed-case>HCI</fixed-case> and <fixed-case>NLP</fixed-case></title>
      <author><first>Nanna</first><last>Inie</last></author>
      <author><first>Leon</first><last>Derczynski</last></author>
      <pages>101–108</pages>
      <abstract>This paper presents a framework of opportunities and barriers/risks between the two research fields Natural Language Processing (NLP) and Human-Computer Interaction (HCI). The framework is constructed by following an interdisciplinary research-model (IDR), combining field-specific knowledge with existing work in the two fields. The resulting framework is intended as a departure point for discussion and inspiration for research collaborations.</abstract>
      <url hash="d0ddbf15">2021.hcinlp-1.16</url>
      <bibkey>inie-derczynski-2021-idr</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>RE</fixed-case>-<fixed-case>AIM</fixed-case>ing Predictive Text</title>
      <author><first>Matthew</first><last>Higgs</last></author>
      <author><first>Claire</first><last>McCallum</last></author>
      <author><first>Selina</first><last>Sutton</last></author>
      <author><first>Mark</first><last>Warner</last></author>
      <pages>109–115</pages>
      <abstract>Our increasing reliance on mobile applications means much of our communication is mediated with the support of predictive text systems. How do these systems impact interpersonal communication and broader society? In what ways are predictive text systems harmful, to whom, and why? In this paper, we focus on predictive text systems on mobile devices and attempt to answer these questions. We introduce the concept of a ‘text entry intervention’ as a way to evaluate predictive text systems through an interventional lens, and consider the Reach, Effectiveness, Adoption, Implementation, and Maintenance (RE-AIM) of predictive text systems. We finish with a discussion of opportunities for NLP.</abstract>
      <url hash="ed2bcacc">2021.hcinlp-1.17</url>
      <bibkey>higgs-etal-2021-aiming</bibkey>
    </paper>
    <paper id="18">
      <title>How do people interact with biased text prediction models while writing?</title>
      <author><first>Advait</first><last>Bhat</last></author>
      <author><first>Saaket</first><last>Agashe</last></author>
      <author><first>Anirudha</first><last>Joshi</last></author>
      <pages>116–121</pages>
      <abstract>Recent studies have shown that a bias in thetext suggestions system can percolate in theuser’s writing. In this pilot study, we ask thequestion: How do people interact with text pre-diction models, in an inline next phrase sugges-tion interface and how does introducing senti-ment bias in the text prediction model affecttheir writing? We present a pilot study as afirst step to answer this question.</abstract>
      <url hash="a8177809">2021.hcinlp-1.18</url>
      <bibkey>bhat-etal-2021-people</bibkey>
    </paper>
  </volume>
</collection>
