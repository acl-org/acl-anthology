<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.bionlp">
  <volume id="1" ingest-date="2025-07-23" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 24th Workshop on Biomedical Language Processing</booktitle>
      <editor><first>Dina</first><last>Demner-Fushman</last></editor>
      <editor><first>Sophia</first><last>Ananiadou</last></editor>
      <editor><first>Makoto</first><last>Miwa</last></editor>
      <editor id="junichi-tsujii"><first>Junichi</first><last>Tsujii</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Viena, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="fd0a8a95">2025.bionlp-1</url>
      <venue>bionlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-275-6</isbn>
      <doi>10.18653/v1/2025.bionlp-1</doi>
    </meta>
    <frontmatter>
      <url hash="9d628c45">2025.bionlp-1.0</url>
      <bibkey>bionlp-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title>Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain</title>
      <author><first>Shintaro</first><last>Ozaki</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Yuta</first><last>Kato</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Siyuan</first><last>Feng</last><affiliation>The University of Tokyo, Department of Language and Information Sciences</affiliation></author>
      <author><first>Masayo</first><last>Tomita</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Kazuki</first><last>Hayashi</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Wataru</first><last>Hashimoto</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Ryoma</first><last>Obara</last><affiliation>NEC</affiliation></author>
      <author><first>Masafumi</first><last>Oyamada</last><affiliation>NEC</affiliation></author>
      <author><first>Katsuhiko</first><last>Hayashi</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>1-17</pages>
      <abstract>Retrieval Augmented Generation (RAG) complements the knowledge of Large Language Models (LLMs) by leveraging external information to enhance response accuracy for queries. This approach is widely applied in several fields by taking its advantage of injecting the most up-to-date information, and researchers are focusing on understanding and improving this aspect to unlock the full potential of RAG in such high-stakes applications.However, despite the potential of RAG to address these needs, the mechanisms behind the confidence levels of its outputs remain underexplored.Our study focuses on the impact of RAG, specifically examining whether RAG increases the confidence of LLM outputs in the medical domain.We conduct this analysis across various configurations and models.We evaluate confidence by treating the model’s predicted probability as its output and calculating several evaluation metrics which include calibration error method, entropy, best probability, and accuracy.Experimental results across multiple datasets confirmed that certain models possess the capability to judge for themselves whether an inserted document relates to the correct answer. These results suggest that evaluating models based on their output probabilities determine whether they function as generators in the RAG framework.Our approach allows to evaluate whether the models handle retrieved documents.</abstract>
      <url hash="c80101c5">2025.bionlp-1.1</url>
      <attachment type="SupplementaryMaterial" hash="31d7426a">2025.bionlp-1.1.SupplementaryMaterial.txt</attachment>
      <bibkey>ozaki-etal-2025-understanding</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Effect of Multilingual and Domain-adapted Continual Pre-training on Few-shot Promptability</title>
      <author><first>Ken</first><last>Yano</last><affiliation>The National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Makoto</first><last>Miwa</last><affiliation>Toyota Technological Institute</affiliation></author>
      <pages>18-26</pages>
      <abstract>Continual Pre-training (CPT) can help pre-trained large language models (LLMs) effectively adapt to new or under-trained domains or low-resource languages without re-training from scratch.Nevertheless, during CPT, the model’s few-shot transfer ability is known to be affected for emergent tasks.We verified this by comparing the performance between the few-shot and fine-tuning settings on the same tasks.We used Llama3-ELAINE-medLLM, which was continually pre-trained on Llama3-8B, targeted for the biomedical domain, and adapted for multilingual languages (English, Japanese, and Chinese).We compared the performance of Llama3-ELAINE-medLLM and Llama3-8B in three emergent tasks: two related domain tasks, entity recognition (NER) and machine translation (MT), and one out-of-domain task, summarization (SUM). Our experimental results show that degradation in few-shot transfer ability does not necessarily indicate the model’s underlying potential on the same task after fine-tuning.</abstract>
      <url hash="f920ce88">2025.bionlp-1.2</url>
      <attachment type="SupplementaryMaterial" hash="2cd6a632">2025.bionlp-1.2.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="b194921b">2025.bionlp-1.2.SupplementaryMaterial.txt</attachment>
      <bibkey>yano-miwa-2025-effect</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>M</fixed-case>ed<fixed-case>S</fixed-case>umm<fixed-case>RAG</fixed-case>: Domain-Specific Retrieval for Medical Summarization</title>
      <author><first>Guanting</first><last>Luo</last><affiliation>The University of Osaka</affiliation></author>
      <author id="yuki-arase"><first>Yuki</first><last>Arase</last><affiliation>Institute of Science Tokyo</affiliation></author>
      <pages>27-33</pages>
      <abstract>Medical text summarization faces significant challenges due to the complexity and domain-specific nature of the language. Although large language models have achieved significant success in general domains, their effectiveness in the medical domain remains limited. This limitation stems from their insufficient understanding of domain-specific terminology and difficulty in interpreting complex medical relationships, which often results in suboptimal summarization quality. To address these challenges, we propose MedSummRAG, a novel retrieval-augmented generation (RAG) framework that integrates external knowledge to enhance summarization. Our approach employs a fine-tuned dense retriever, trained with contrastive learning, to retrieve relevant documents for medical summarization. The retrieved documents are then integrated with the input text to generate high-quality summaries. Experimental results show that MedSummRAG achieves significant improvements in ROUGE scores on both zero/few-shot and fine-tuned language models, outperforming baseline methods. These findings underscore the importance of RAG and domain adaptation of the retriever for medical text summarization. The source code of this paper can be obtained from: https://github.com/guantingluo98/MedSummRAG</abstract>
      <url hash="5a2e7d66">2025.bionlp-1.3</url>
      <attachment type="SupplementaryMaterial" hash="431bb38c">2025.bionlp-1.3.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="eb244228">2025.bionlp-1.3.SupplementaryMaterial.txt</attachment>
      <bibkey>luo-arase-2025-medsummrag</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Enhancing Stress Detection on Social Media Through Multi-Modal Fusion of Text and Synthesized Visuals</title>
      <author><first>Efstathia</first><last>Soufleri</last><affiliation>Athena RC</affiliation></author>
      <author><first>Sophia</first><last>Ananiadou</last><affiliation>University of Manchester</affiliation></author>
      <pages>34-43</pages>
      <abstract>Social media platforms generate an enormous volume of multi-modal data, yet stress detection research has predominantly relied on text-based analysis. In this work, we propose a novel framework that integrates textual content with synthesized visual cues to enhance stress detection. Using the generative model DALL·E, we synthesize images from social media posts, which are then fused with text through the multi-modal capabilities of a pre-trained CLIP model. Our approach is evaluated on the Dreaddit dataset, where a classifier trained on frozen CLIP features achieves 94.90% accuracy, and full fine-tuning further improves performance to 98.41%. These results underscore the integration of synthesized visuals with textual data not only enhances stress detection but also offers a robust method over traditional text-only methods, paving the way for innovative approaches in mental health monitoring and social media analytics.</abstract>
      <url hash="925bca56">2025.bionlp-1.4</url>
      <attachment type="SupplementaryMaterial" hash="1814f640">2025.bionlp-1.4.SupplementaryMaterial.txt</attachment>
      <bibkey>soufleri-ananiadou-2025-enhancing</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Fine-tuning <fixed-case>LLM</fixed-case>s to Extract Epilepsy Seizure Frequency Data from Health Records</title>
      <author><first>Ben</first><last>Holgate</last><affiliation>King’s College London</affiliation></author>
      <author><first>Joe</first><last>Davies</last><affiliation>King’s College London</affiliation></author>
      <author><first>Shichao</first><last>Fang</last><affiliation>King’s College London</affiliation></author>
      <author><first>Joel</first><last>Winston</last><affiliation>King’s College London</affiliation></author>
      <author><first>James</first><last>Teo</last><affiliation>King’s College London</affiliation></author>
      <author><first>Mark</first><last>Richardson</last><affiliation>King’s College London</affiliation></author>
      <pages>44-55</pages>
      <abstract>We developed a new methodology of extracting the frequency of a patient’s epilepsy seizures from unstructured, free-text outpatient clinic letters by: first, devising a singular unit of measurement for seizure frequency; and second, fine-tuning a generative Large Language Model (LLM) on our bespoke annotated dataset. We measured frequency by the number of seizures per month: one seizure or more requires an integer; and less than one a decimal. This approach enables us to track whether a patient”s seizures are improving or not over time. We found fine-tuning improves the F1 score of our best-performing LLM, Ministral-8B-Instruct-2410, by around three times compared to an untrained model. We also found Ministral demonstrated an impressive ability for mathematical reasoning.</abstract>
      <url hash="0b846c08">2025.bionlp-1.5</url>
      <attachment type="SupplementaryMaterial" hash="6df14bd5">2025.bionlp-1.5.SupplementaryMaterial.txt</attachment>
      <bibkey>holgate-etal-2025-fine</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>A</fixed-case>da<fixed-case>B</fixed-case>io<fixed-case>BERT</fixed-case>: Adaptive Token Sequence Learning for Biomedical Named Entity Recognition</title>
      <author><first>Sumit</first><last>Kumar</last><affiliation>Indian Institute of Science Education and Research Bhopal</affiliation></author>
      <author><first>Tanmay</first><last>Basu</last><affiliation>Indian Institute of Science Education and Research Bhopal</affiliation></author>
      <pages>56-62</pages>
      <abstract>Accurate identification and labeling of biomedical entities, such as diseases, genes, chemical and species, within scientific texts are crucial for understanding complex relationships. We propose Adaptive BERT or AdaBioBERT, a robust named entity recognition (NER) model that builds upon BioBERT (Biomedical Bidirectional Encoded Representation from Transformers) based on an adaptive loss function to learn different types of biomedical token sequence. This adaptive loss function combines the standard Cross Entropy (CE) loss and Conditional Random Field (CRF) loss to optimize both token level accuracy and sequence-level coherence. AdaBioBERT captures rich semantic nuances by leveraging pre-trained contextual embeddings from BioBERT. On the other hand, the CRF loss of AdaBioBERT ensures proper identification of complex multi-token biomedical entities in a sequence and the CE loss can capture the simple unigram entities in a sequence. The empirical analysis on multiple standard biomedical coprora demonstrates that AdaBioBERT performs better than the state of the arts for most of the datasets in terms of macro and micro averaged F1 score.’</abstract>
      <url hash="97d2bd19">2025.bionlp-1.6</url>
      <attachment type="SupplementaryMaterial" hash="e352a4fc">2025.bionlp-1.6.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="f61cc08b">2025.bionlp-1.6.SupplementaryMaterial.zip</attachment>
      <bibkey>kumar-basu-2025-adabiobert</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Transformer-Based Medical Statement Classification in Doctor-Patient Dialogues</title>
      <author><first>Farnod</first><last>Bahrololloomi</last><affiliation>RheinMain University of Applied Sciences</affiliation></author>
      <author><first>Johannes</first><last>Luderschmidt</last><affiliation>RheinMain University of Applied Sciences</affiliation></author>
      <author><first>Biying</first><last>Fu</last><affiliation>RheinMain University of Applied Sciences</affiliation></author>
      <pages>63-73</pages>
      <abstract>The classification of medical statements in German doctor-patient interactions presents significant challenges for automated medical information extraction, particularly due to complex domain-specific terminology and the limited availability of specialized training data. To address this, we introduce a manually annotated dataset specifically designed for distinguishing medical from non-medical statements. This dataset incorporates the nuances of German medical terminology and provides a valuable foundation for further research in this domain. We systematically evaluate Transformer-based models and multimodal embedding techniques, comparing them against traditional embedding-based machine learning (ML) approaches and domain-specific models such as medBERT.de. Our empirical results show that Transformer-based architectures, such as the Sentence-BERT model combined with a support vector machine (SVM), achieve the highest accuracy of 79.58% and a weighted F1-Score of 78.81%, demonstrating an average performance improvement of up to 10% over domain-specific counterparts. Additionally, we highlight the potential of lightweight ML-models for resource-efficient deployment on mobile devices, enabling real-time medical information processing in practical settings. These findings emphasize the importance of embedding selection for optimizing classification performance in the medical domain and establish a robust foundation for the development of advanced, domain-adapted German language models.</abstract>
      <url hash="29ce56d1">2025.bionlp-1.7</url>
      <attachment type="SupplementaryMaterial" hash="52862d44">2025.bionlp-1.7.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="e2ad63b2">2025.bionlp-1.7.SupplementaryMaterial.txt</attachment>
      <bibkey>bahrololloomi-etal-2025-transformer</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>P</fixed-case>re<fixed-case>C</fixed-case>lin<fixed-case>IE</fixed-case>: An Annotated Corpus for Information Extraction in Preclinical Studies</title>
      <author><first>Simona</first><last>Doneva</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Hanna</first><last>Hubarava</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Pia</first><last>Härvelid</last><affiliation>University of Zurich, Switzerland</affiliation></author>
      <author><first>Wolfgang</first><last>Zürrer</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Julia</first><last>Bugajska</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Bernard</first><last>Hild</last><affiliation>University of Zurich</affiliation></author>
      <author><first>David</first><last>Brüschweiler</last><affiliation>University of Zurich, Switzerland</affiliation></author>
      <author><first>Gerold</first><last>Schneider</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Tilia</first><last>Ellendorff</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Benjamin</first><last>Ineichen</last><affiliation>University of Zurich, Center for Reproducible Science</affiliation></author>
      <pages>74-87</pages>
      <abstract>Animal research, sometimes referred to as preclinical research, plays a vital role in bridging the gap between basic science and clinical applications. However, the rapid increase in publications and the complexity of reported findings make it increasingly difficult for researchers to extract and assess relevant information. While automation through natural language processing (NLP) holds great potential for addressing this challenge, progress is hindered by the absence of high-quality, comprehensive annotated resources specific to preclinical studies. To fill this gap, we introduce PreClinIE, a fully open manually annotated dataset. The corpus consists of abstracts and methods sections from 725 publications, annotated for study rigor indicators (e.g., random allocation) and other study characteristics (e.g., species). We describe the data collection and annotation process, outlining the challenges of working with preclinical literature. By providing this resource, we aim to accelerate the development of NLP tools that enhance literature mining in preclinical research.</abstract>
      <url hash="a7c6f135">2025.bionlp-1.8</url>
      <attachment type="SupplementaryMaterial" hash="f3d60f7c">2025.bionlp-1.8.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="51e8fe26">2025.bionlp-1.8.SupplementaryMaterial.txt</attachment>
      <bibkey>doneva-etal-2025-preclinie</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.8</doi>
    </paper>
    <paper id="9">
      <title>Benchmarking zero-shot biomedical relation triplet extraction across language model architectures</title>
      <author><first>Frederik</first><last>Gade</last><affiliation>Technical University of Denmark</affiliation></author>
      <author><first>Ole</first><last>Lund</last><affiliation>Technical University of Denmark</affiliation></author>
      <author><first>Marie Lisandra</first><last>Mendoza</last><affiliation>Novo Nordisk Research Centre Oxford</affiliation></author>
      <pages>88-100</pages>
      <abstract>Many language models (LMs) in the literature claim excellent zero-shot and/or few-shot capabilities for named entity recognition (NER) and relation extraction (RE) tasks and assert their ability to generalize beyond their training datasets. However, these claims have yet to be tested across different model architectures. This paper presents a performance evaluation of zero-shot relation triplet extraction (NER followed by RE of the entities) for both small and large LMs, utilizing 13,867 texts from 61 biomedical corpora and encompassing 151 unique entity types. This comprehensive evaluation offers valuable insights into the practical applicability and performance of LMs within the intricate domain of biomedical relation triplet extraction, highlighting their effectiveness in managing a diverse range of relations and entity types. Gemini 1.5 Pro, the largest LM included in the study, was the top-performing zero-shot model, achieving an average partial match micro F1 of 0.492 for NER, followed closely by SciLitLLM 1.5 14B with a score of 0.475. Fine-tuned models generally outperformed others on the corpora they were trained on, even in a few-shot setting, but struggled to generalize across all datasets with similar entity types. No models achieved an F1 score above 0.5 for the RTE task on any dataset, and their scores fluctuated based on the specific class of entity and the dataset involved. This observation highlights that there is still large room for improvement on the zero-shot utility of LMs in biomedical RTE applications.</abstract>
      <url hash="69d072cd">2025.bionlp-1.9</url>
      <attachment type="SupplementaryMaterial" hash="10ffad97">2025.bionlp-1.9.SupplementaryMaterial.txt</attachment>
      <bibkey>gade-etal-2025-benchmarking</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>R</fixed-case>ad<fixed-case>QA</fixed-case>-<fixed-case>DPO</fixed-case>: A Radiology Question Answering System with Encoder-Decoder Models Enhanced by Direct Preference Optimization</title>
      <author><first>Md Sultan Al</first><last>Nahian</last><affiliation>University of Kentucky</affiliation></author>
      <author><first>Ramakanth</first><last>Kavuluru</last><affiliation>University of Kentucky</affiliation></author>
      <pages>101-113</pages>
      <abstract>Extractive question answering over clinical text is a crucial need to help deal with the deluge of clinical text generated in hospitals. While encoder models (e.g., BERT) have been popular for this reading comprehension–style question answering task, recently encoder-decoder models (e.g., T5) are on the rise. There is also the emergence of preference optimization techniques to align decoder-only LLMs with human preferences. In this paper, we combine encoder-decoder models with the direct preference optimization (DPO) method for the RadQA radiology question answering task. Our approach achieves a 12–15 F1 point improvement over previous state-of-the-art models. To the best of our knowledge, this effort is the first to show that DPO method also works for reading comprehension via novel heuristics to generate preference data without human inputs.</abstract>
      <url hash="9113b702">2025.bionlp-1.10</url>
      <attachment type="SupplementaryMaterial" hash="e39e5bc3">2025.bionlp-1.10.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="b5390a1c">2025.bionlp-1.10.SupplementaryMaterial.txt</attachment>
      <bibkey>nahian-kavuluru-2025-radqa</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.10</doi>
      <revision id="1" href="2025.bionlp-1.10v1" hash="6f9d891b"/>
      <revision id="2" href="2025.bionlp-1.10v2" hash="9113b702" date="2025-09-06">Codebase link update.</revision>
    </paper>
    <paper id="11">
      <title>Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in <fixed-case>P</fixed-case>ub<fixed-case>M</fixed-case>ed Abstracts</title>
      <author><first>Elizabeth</first><last>Schaefer</last><affiliation>Yale University</affiliation></author>
      <author><first>Kirk</first><last>Roberts</last><affiliation>University of Texas Health Science Center at Houston</affiliation></author>
      <pages>114-123</pages>
      <abstract>This paper presents a pipeline for mitigating gender bias in large language models (LLMs) used in medical literature by neutralizing gendered occupational pronouns. A set of 379,000 PubMed abstracts from 1965-1980 was processed to identify and modify pronouns tied to professions. We developed a BERT-based model, Modern Occupational Bias Elimination with Refined Training, or MOBERT, trained on these neutralized abstracts, and compared it with 1965BERT, trained on the original dataset. MOBERT achieved a 70% inclusive replacement rate, while 1965BERT reached only 4%. A further analysis of MOBERT revealed that pronoun replacement accuracy correlated with the frequency of occupational terms in the training data. We propose expanding the dataset and refining the pipeline to improve performance and ensure more equitable language modeling in medical applications.</abstract>
      <url hash="b4b90cb6">2025.bionlp-1.11</url>
      <attachment type="SupplementaryMaterial" hash="9c34387c">2025.bionlp-1.11.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="d35ba64c">2025.bionlp-1.11.SupplementaryMaterial.txt</attachment>
      <bibkey>schaefer-roberts-2025-gender</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Error Detection in Medical Note through Multi Agent Debate</title>
      <author><first>Abdine</first><last>Maiga</last><affiliation>University College London</affiliation></author>
      <author><first>Anoop</first><last>Shah</last><affiliation>University College London</affiliation></author>
      <author><first>Emine</first><last>Yilmaz</last><affiliation>UCL &amp; Amazon</affiliation></author>
      <pages>124-135</pages>
      <abstract>Large Language Models (LLMs) have approached human-level performance in text generation and summarization, yet their application in clinical settings remains constrained by potential inaccuracies that could lead to serious consequences. This work addresses the critical safety weaknesses in medical documentation systems by focusing on detecting subtle errors that require specialized medical expertise. We introduce a novel multi-agent debating framework that achieves 78.8% accuracy on medical error detection, significantly outperforming both single-agent approaches and previous multi-agent systems. Our framework leverages specialized LLM agents with asymmetric access to complementary medical knowledge sources (Mayo Clinic and WebMD), engaging them in structured debate to identify inaccuracies in clinical notes. A judge agent evaluates these arguments based solely on their medical reasoning quality, with agent-specific performance metrics incorporated as feedback for developing situation-specific trust models.</abstract>
      <url hash="4560c2cd">2025.bionlp-1.12</url>
      <attachment type="SupplementaryMaterial" hash="163118d1">2025.bionlp-1.12.SupplementaryMaterial.txt</attachment>
      <bibkey>maiga-etal-2025-error</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Accelerating Cross-Encoders in Biomedical Entity Linking</title>
      <author><first>Javier</first><last>Sanz-Cruzado</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Jake</first><last>Lever</last><affiliation>University of Glasgow</affiliation></author>
      <pages>136-147</pages>
      <abstract>Biomedical entity linking models disambiguate mentions in text by matching them with unique biomedical concepts. This problem is commonly addressed using a two-stage pipeline comprising an inexpensive candidate generator, which filters a subset of suitable entities for a mention, and a costly but precise reranker that provides the final matching between the mention and the concept. With the goal of applying two-stage entity linking at scale, we explore the construction of effective cross-encoder reranker models, capable of scoring multiple mention-entity pairs simultaneously. Through experiments on four entity linking datasets, we show that our cross-encoder models provide between 2.7 to 36.97 times faster training speeds and 3.42 to 26.47 times faster inference speeds than a base cross-encoder model capable of scoring only one entity, while achieving similar accuracy (differences between -3.42% to 2.76% Acc@1).</abstract>
      <url hash="c0330039">2025.bionlp-1.13</url>
      <attachment type="SupplementaryMaterial" hash="70d643bd">2025.bionlp-1.13.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="a7264280">2025.bionlp-1.13.SupplementaryMaterial.zip</attachment>
      <bibkey>sanz-cruzado-lever-2025-accelerating</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Advancing Biomedical Claim Verification by Using Large Language Models with Better Structured Prompting Strategies</title>
      <author><first>Siting</first><last>Liang</last><affiliation>German Research Center for Artificial Intelligence</affiliation></author>
      <author><first>Daniel</first><last>Sonntag</last><affiliation>German Research Center for Artificial Intelligence</affiliation></author>
      <pages>148-166</pages>
      <abstract>In this work, we propose a structured four-step prompting strategy that explicitly guides large language models (LLMs) through (1) claim comprehension, (2) evidence analysis, (3) intermediate conclusion, and (4) entailment decision-making to improve the accuracy of biomedical claim verification. This strategy leverages compositional and human-like reasoning to enhance logical consistency and factual grounding to reduce reliance on memorizing few-Shot exemplars and help LLMs generalize reasoning patterns across different biomedical claim verification tasks. Through extensive evaluation on biomedical NLI benchmarks, we analyze the individual contributions of each reasoning step. Our findings demonstrate that comprehension, evidence analysis, and intermediate conclusion each play distinct yet complementary roles. Systematic prompting and carefully designed step-wise instructions not only unlock the latent cognitive abilities of LLMs but also enhance interpretability by making it easier to trace errors and understand the model’s reasoning process. Our research aims to improve the reliability of AI-driven biomedical claim verification.</abstract>
      <url hash="88c4bab1">2025.bionlp-1.14</url>
      <attachment type="SupplementaryMaterial" hash="79866266">2025.bionlp-1.14.SupplementaryMaterial.txt</attachment>
      <bibkey>liang-sonntag-2025-advancing</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>A Retrieval-Based Approach to Medical Procedure Matching in <fixed-case>R</fixed-case>omanian</title>
      <author><first>Andrei</first><last>Niculae</last><affiliation>National University of Science and Technology Politehnica Bucharest</affiliation></author>
      <author><first>Adrian</first><last>Cosma</last><affiliation>National University of Science and Technology Politehnica Bucharest</affiliation></author>
      <author><first>Emilian</first><last>Radoi</last><affiliation>National University of Science and Technology Politehnica Bucharest</affiliation></author>
      <pages>167-175</pages>
      <abstract>Accurately mapping medical procedure names from healthcare providers to standardized terminology used by insurance companies is a crucial yet complex task. Inconsistencies in naming conventions lead to missclasified procedures, causing administrative inefficiencies and insurance claim problems in private healthcare settings. Many companies still use human resources for manual mapping, while there is a clear opportunity for automation. This paper proposes a retrieval-based architecture leveraging sentence embeddings for medical name matching in the Romanian healthcare system. This challenge is significantly more difficult in underrepresented languages such as Romanian, where existing pretrained language models lack domain-specific adaptation to medical text. We evaluate multiple embedding models, including Romanian, multilingual, and medical-domain-specific representations, to identify the most effective solution for this task. Our findings contribute to the broader field of medical NLP for low-resource languages such as Romanian.</abstract>
      <url hash="d71fc6d0">2025.bionlp-1.15</url>
      <attachment type="SupplementaryMaterial" hash="7a29c7e5">2025.bionlp-1.15.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="f613ebe4">2025.bionlp-1.15.SupplementaryMaterial.txt</attachment>
      <bibkey>niculae-etal-2025-retrieval</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Improving Barrett’s Oesophagus Surveillance Scheduling with Large Language Models: A Structured Extraction Approach</title>
      <author><first>Xinyue</first><last>Zhang</last><affiliation>King’s College London</affiliation></author>
      <author><first>Agathe</first><last>Zecevic</last><affiliation>Guy’s and St Thomas’ NHS Foundation Trust</affiliation></author>
      <author><first>Sebastian</first><last>Zeki</last><affiliation>Guy’s and St Thomas’ NHS Foundation Trust</affiliation></author>
      <author><first>Angus</first><last>Roberts</last><affiliation>King’s College London</affiliation></author>
      <pages>176-189</pages>
      <abstract>Gastroenterology (GI) cancer surveillance scheduling relies on extracting structured data from unstructured clinical texts, such as endoscopy and pathology reports. Traditional Natural Language Processing (NLP) models have been employed for this task, but recent advancements in Large Language Models (LLMs) present a new opportunity for automation without requiring extensive labeled datasets. In this study, we propose an LLM-based entity extraction and rule-based decision support framework for Barrett’s Oesophagus (BO) surveillance timing prediction. Our approach processes endoscopy and pathology reports to extract clinically relevant information and structures it into a standardised format, which is then used to determine appropriate surveillance intervals. We evaluate multiple state-of-the-art LLMs on real-world clinical datasets from two hospitals, assessing their performance in accuracy and running time cost. The results demonstrate that LLMs, particularly Phi-4 and (DeepSeek distilled) Qwen-2.5, can effectively automate the extraction of BO surveillance-related information with high accuracy, while Phi-4 is also efficient during inference. We also compared the trade-offs between LLMs and fine-tuned non-LLMs. Our findings indicate that LLM extraction based methods can support clinical decision-making by providing justifications from report extractions, reducing manual workload, and improving guideline adherence in BO surveillance scheduling.</abstract>
      <url hash="410d6ec3">2025.bionlp-1.16</url>
      <attachment type="SupplementaryMaterial" hash="7d4f2c53">2025.bionlp-1.16.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="b6b73e8d">2025.bionlp-1.16.SupplementaryMaterial.txt</attachment>
      <bibkey>zhang-etal-2025-improving-barretts</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Prompting Large Language Models for <fixed-case>I</fixed-case>talian Clinical Reports: A Benchmark Study</title>
      <author><first>Livia</first><last>Lilli</last><affiliation>Fondazione Policlinico Universitario Agostino Gemelli IRCCS</affiliation></author>
      <author><first>Carlotta</first><last>Masciocchi</last><affiliation>Fondazione Policlinico Universitario Agostino Gemelli IRCCS</affiliation></author>
      <author><first>Antonio</first><last>Marchetti</last><affiliation>Fondazione Policlinico Universitario Agostino Gemelli IRCCS</affiliation></author>
      <author><first>Giovanni</first><last>Arcuri</last><affiliation>Fondazione Policlinico Universitario Agostino Gemelli IRCCS</affiliation></author>
      <author><first>Stefano</first><last>Patarnello</last><affiliation>Fondazione Policlinico Universitario Agostino Gemelli IRCCS</affiliation></author>
      <pages>190-200</pages>
      <abstract>Large Language Models (LLMs) have significantly impacted medical Natural Language Processing (NLP), enabling automated information extraction from unstructured clinical texts. However, selecting the most suitable approach requires careful evaluation of different model architectures, such as generative LLMs and BERT-based models, along with appropriate adaptation strategies, including prompting techniques, or fine-tuning. Several studies explored different LLM implementations, highlighting their effectiveness in medical domain, including complex diagnostics patterns as for example in rheumatology. However, their application to Italian remains limited, serving as a key example of the broader gap in non-English language research. In this study, we present a task-specific benchmark analysis comparing generative LLMs and BERT-based models, on real-world Italian clinical reports. We evaluated zero-shot prompting, in-context learning (ICL), and fine-tuning across eight diagnostic categories in the rheumatology area. Results show that ICL improves performance over zero-shot-prompting, particularly for Mixtral and Gemma models. Overall, BERT fine-tuning present the highest performance, while ICL outperforms BERT in specific diagnoses, such as renal and systemic, suggesting that prompting can be a potential alternative when labeled data is scarce.</abstract>
      <url hash="6600ef62">2025.bionlp-1.17</url>
      <attachment type="SupplementaryMaterial" hash="70af474b">2025.bionlp-1.17.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="984f43b0">2025.bionlp-1.17.SupplementaryMaterial.zip</attachment>
      <bibkey>lilli-etal-2025-prompting</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.17</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>Q</fixed-case>o<fixed-case>LAS</fixed-case>: A <fixed-case>R</fixed-case>eddit Corpus of Health-Related Quality of Life Aspects of Mental Disorders</title>
      <author><first>Lynn</first><last>Greschner</last><affiliation>University of Bamberg</affiliation></author>
      <author id="amelie-wuhrl"><first>Amelie</first><last>Wührl</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>University of Bamberg</affiliation></author>
      <pages>201-216</pages>
      <abstract>Quality of Life (QoL) refers to a person’s subjective perception of various aspects of their life. For medical practitioners, it is one of the most important concepts for treatment decisions. Therefore, it is essential to understand in which aspects a medical condition affects a patient’s subjective perception of their life. With this paper, we focus on the under-resourced domain of mental health-related QoL, and contribute the first corpus to study and model this concept: We (1) annotate 240 Reddit posts with a set of 11 QoL aspects (such as ‘independence’, ‘mood’, or ‘relationships’) and their sentiment polarity. Based on this novel corpus, we (2) evaluate a pipeline to detect QoL mentions and classify them into aspects using open-domain aspect-based sentiment analysis. We find that users frequently discuss health-related QoL in their posts, focusing primarily on the aspects ‘relationships’ and ‘selfimage’. Our method reliably predicts such mentions and their sentiment, however, detecting fine-grained individual aspects remains challenging. An analysis of a large corpus of automatically labeled data reveals that social media content contains novel aspects pertinent to patients that are not covered by existing QoL taxonomies.</abstract>
      <url hash="c07c31f2">2025.bionlp-1.18</url>
      <attachment type="SupplementaryMaterial" hash="7af6df5c">2025.bionlp-1.18.SupplementaryMaterial.txt</attachment>
      <bibkey>greschner-etal-2025-qolas</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>LLM</fixed-case>s as Medical Safety Judges: Evaluating Alignment with Human Annotation in Patient-Facing <fixed-case>QA</fixed-case></title>
      <author><first>Yella</first><last>Diekmann</last><affiliation>Department of Computer Science, Emory University</affiliation></author>
      <author><first>Chase</first><last>Fensore</last><affiliation>Department of Computer Science, Emory University</affiliation></author>
      <author><first>Rodrigo</first><last>Carrillo-Larco</last><affiliation>Rollins School of Public Health, Emory University</affiliation></author>
      <author><first>Eduard</first><last>Castejon Rosales</last><affiliation>Department of Family and Preventive Medicine, Emory University School of Medicine</affiliation></author>
      <author><first>Sakshi</first><last>Shiromani</last><affiliation>Department of Ophthalmology, Emory University School of Medicine</affiliation></author>
      <author><first>Rima</first><last>Pai</last><affiliation>Rollins School of Public Health, Emory University</affiliation></author>
      <author><first>Megha</first><last>Shah</last><affiliation>Department of Family and Preventive Medicine, Emory University School of Medicine</affiliation></author>
      <author><first>Joyce</first><last>Ho</last><affiliation>Department of Computer Science, Emory University</affiliation></author>
      <pages>217-224</pages>
      <abstract>The increasing deployment of LLMs in patient-facing medical QA raises concerns about the reliability and safety of their responses. Traditional evaluation methods rely on expert human annotation, which is costly, time-consuming, and difficult to scale. This study explores the feasibility of using LLMs as automated judges for medical QA evaluation. We benchmark LLMs against human annotators across eight qualitative safety metrics and introduce adversarial question augmentation to assess LLMs’ robustness in evaluating medical responses. Our findings reveal that while LLMs achieve high accuracy in objective metrics such as scientific consensus and grammaticality, they struggle with more subjective categories like empathy and extent of harm. This work contributes to the ongoing discussion on automating safety assessments in medical AI and informs the development of more reliable evaluation methodologies.</abstract>
      <url hash="10561041">2025.bionlp-1.19</url>
      <attachment type="SupplementaryMaterial" hash="6efd9589">2025.bionlp-1.19.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="e3b9febc">2025.bionlp-1.19.SupplementaryMaterial.zip</attachment>
      <bibkey>diekmann-etal-2025-llms</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>Effective Multi-Task Learning for Biomedical Named Entity Recognition</title>
      <author><first>João</first><last>Ruano</last><affiliation>Priberam</affiliation></author>
      <author><first>Gonçalo</first><last>Correia</last><affiliation>Priberam</affiliation></author>
      <author><first>Leonor</first><last>Barreiros</last><affiliation>Priberam</affiliation></author>
      <author id="alfonso-mendes"><first>Afonso</first><last>Mendes</last><affiliation>Priberam Informática, SA.</affiliation></author>
      <pages>225-239</pages>
      <abstract>Biomedical Named Entity Recognition presents significant challenges due to the complexity of biomedical terminology and inconsistencies in annotation across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER), a novel approach designed to handle nested named entities while integrating multiple datasets through an effective multi-task learning strategy. SRU-NER mitigates annotation gaps by dynamically adjusting loss computation to avoid penalizing predictions of entity types absent in a given dataset. Through extensive experiments, including a cross-corpus evaluation and human assessment of the model’s predictions, SRU-NER achieves competitive performance in biomedical and general-domain NER tasks, while improving cross-domain generalization.</abstract>
      <url hash="e7c7ddc6">2025.bionlp-1.20</url>
      <attachment type="SupplementaryMaterial" hash="2bd90fbb">2025.bionlp-1.20.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="16042e20">2025.bionlp-1.20.SupplementaryMaterial.zip</attachment>
      <bibkey>ruano-etal-2025-effective</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.20</doi>
    </paper>
    <paper id="21">
      <title>Can Large Language Models Classify and Generate Antimicrobial Resistance Genes?</title>
      <author><first>Hyunwoo</first><last>Yoo</last><affiliation>Drexel University</affiliation></author>
      <author><first>Haebin</first><last>Shin</last><affiliation>KAIST AI</affiliation></author>
      <author><first>Gail</first><last>Rosen</last><affiliation>Drexel University</affiliation></author>
      <pages>240-248</pages>
      <abstract>This study explores the application of generative Large Language Models (LLMs) in DNA sequence analysis, highlighting their advantages over encoder-based models like DNABERT2 and Nucleotide Transformer. While encoder models excel in classification, they struggle to integrate external textual information. In contrast, generative LLMs can incorporate domain knowledge, such as BLASTn annotations, to improve classification accuracy even without fine-tuning. We evaluate this capability on antimicrobial resistance (AMR) gene classification, comparing generative LLMs with encoder-based baselines. Results show that LLMs significantly enhance classification when supplemented with textual information. Additionally, we demonstrate their potential in DNA sequence generation, further expanding their applicability. Our findings suggest that LLMs offer a novel paradigm for integrating biological sequences with external knowledge, bridging gaps in traditional classification methods.</abstract>
      <url hash="a86b727a">2025.bionlp-1.21</url>
      <attachment type="SupplementaryMaterial" hash="5a3a2164">2025.bionlp-1.21.SupplementaryMaterial.txt</attachment>
      <bibkey>yoo-etal-2025-large</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>C</fixed-case>ase<fixed-case>R</fixed-case>eport<fixed-case>C</fixed-case>ollective: A Large-Scale <fixed-case>LLM</fixed-case>-Extracted Dataset for Structured Medical Case Reports</title>
      <author><first>Xiao Yu Cindy</first><last>Zhang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Melissa</first><last>Fong</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Wyeth</first><last>Wasserman</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Jian</first><last>Zhu</last><affiliation>University of British Columbia</affiliation></author>
      <pages>249-262</pages>
      <abstract>Case reports provide critical insights into rare and atypical diseases, but extracting structured knowledge remains challenging due to unstructured text and domain-specific terminology. We introduce CaseReportCollective, an LLM-extracted dataset of 85,961 open-access case reports spanning 37 years across 14 medical domains, validated through programmatic and human evaluation. Our dataset reveals key publication and demographic trends, including a significant increase in open-access case reports over the past decade, shifts in focus from oncology to COVID-19, and sex disparities in reporting across different medical conditions. Over time, the gap between male and female case reports has narrowed, suggesting greater equity in case reporting. Using CaseReportCollective, we further explore embedding-based retrieval for similar medical topics through accumulated similarity scores across extracted structured information. We also conducted detailed error analyses on the retrieval ranking, finding that high-reported topics dominate retrieval. Such retrieval is driven by lexical overlap rather than underlying clinical relevance, often failing to distinguish between semantically similar yet mechanistically distinct conditions. Future work should focus on clinical-aware embeddings adjusted for long-tailed case distributions to improve retrieval accuracy.</abstract>
      <url hash="16784a35">2025.bionlp-1.22</url>
      <attachment type="SupplementaryMaterial" hash="17bef7c5">2025.bionlp-1.22.SupplementaryMaterial.txt</attachment>
      <bibkey>zhang-etal-2025-casereportcollective</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.22</doi>
    </paper>
    <paper id="23">
      <title>Enhancing Antimicrobial Drug Resistance Classification by Integrating Sequence-Based and Text-Based Representations</title>
      <author><first>Hyunwoo</first><last>Yoo</last><affiliation>Drexel University</affiliation></author>
      <author><first>Bahrad</first><last>Sokhansanj</last><affiliation>Drexel University</affiliation></author>
      <author><first>James</first><last>Brown</last><affiliation>Drexel University</affiliation></author>
      <pages>263-273</pages>
      <abstract>Antibiotic resistance identification is essential for public health, medical treatment, and drug development. Traditional sequence-based models struggle with accurate resistance prediction due to the lack of biological context. To address this, we propose an NLP-based model that integrates genetic sequences with structured textual annotations, including gene family classifications and resistance mechanisms. Our approach leverages pretrained language models for both genetic sequences and biomedical text, aligning biological metadata with sequence-based embeddings. We construct a novel dataset based on the Antibiotic Resistance Ontology (ARO), consolidating gene sequences with resistance-related textual information. Experiments show that incorporating domain knowledge significantly improves classification accuracy over sequence-only models, reducing reliance on exhaustive laboratory testing. By integrating genetic sequence processing with biomedical text understanding, our approach provides a scalable and interpretable solution for antibiotic resistance prediction.</abstract>
      <url hash="44e11907">2025.bionlp-1.23</url>
      <attachment type="SupplementaryMaterial" hash="945659d5">2025.bionlp-1.23.SupplementaryMaterial.txt</attachment>
      <bibkey>yoo-etal-2025-enhancing</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Questioning Our Questions: How Well Do Medical <fixed-case>QA</fixed-case> Benchmarks Evaluate Clinical Capabilities of Language Models?</title>
      <author><first>Siun</first><last>Kim</last><affiliation>Seoul Natoinal University Hospital</affiliation></author>
      <author><first>Hyung-Jin</first><last>Yoon</last><affiliation>Biomedical Engineering, Seoul National University College of Medicine</affiliation></author>
      <pages>274-296</pages>
      <abstract>Recent advances in large language models (LLMs) have led to impressive performance on medical question-answering (QA) benchmarks. However, the extent to which these benchmarks reflect real-world clinical capabilities remains uncertain. To address this gap, we systematically analyzed the correlation between LLM performance on major medical QA benchmarks (e.g., MedQA, MedMCQA, PubMedQA, and MMLU medicine subjects) and clinical performance in real-world settings. Our dataset included 702 clinical evaluations of 85 LLMs from 168 studies. Benchmark scores demonsrated a moderate correlation with clinical performance (Spearman’s rho = 0.59), albeit substantially lower than inter-benchmark correlations. Among them, MedQA was the most predictive but failed to capture essential competencies such as patient communication, longitudinal care, and clinical information extraction. Using Bayesian hierarchical modeling, we estimated representative clinical performance and identified GPT-4 and GPT-4o as consistently top-performing models, often matching or exceeding human physicians. Despite longstanding concerns about the clinical validity of medical QA benchmarks, this study offers the first quantitative analysis of their alignment with real-world clinical performance.</abstract>
      <url hash="5fde0c3b">2025.bionlp-1.24</url>
      <attachment type="SupplementaryMaterial" hash="9255bdb0">2025.bionlp-1.24.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="0f2d15ae">2025.bionlp-1.24.SupplementaryMaterial.zip</attachment>
      <bibkey>kim-yoon-2025-questioning</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.24</doi>
    </paper>
    <paper id="25">
      <title>Beyond Citations: Integrating Finding-Based Relations for Improved Biomedical Article Representations</title>
      <author><first>Yuan</first><last>Liang</last><affiliation>Queen Mary University of London</affiliation></author>
      <author id="massimo-poesio"><first>Massimo</first><last>Poesio</last><affiliation>Queen Mary University of London and University of Utrecht</affiliation></author>
      <author><first>Roonak</first><last>Rezvani</last><affiliation>Recursion</affiliation></author>
      <pages>297-306</pages>
      <abstract>High-quality scientific article embeddings are essential for tasks like document retrieval, citation recommendation, and classification. Traditional citation-based approaches assume citations reflect semantic similarity—an assumption that introduces bias and noise. Recent models like SciNCL and SPECTER2 have attempted to refine citation-based representations but still struggle with noisy citation edges and fail to fully leverage textual information. To address these limitations, we propose a hybrid approach that combines Finding-Citation Graphs (FCG) with contrastive learning. Our method improves triplet selection by filtering out less important citations and incorporating finding similarity relations, leading to better semantic relationship capture. Evaluated on the SciRepEval benchmark, our approach consistently outperforms citation-only baselines, showing the value of text-based semantic structures. While we do not surpass state-of-the-art models in most tasks, our results reveal the limitations of purely citation-based embeddings and suggest paths for improvement through enhanced semantic integration and domain-specific adaptations.</abstract>
      <url hash="a80661bf">2025.bionlp-1.25</url>
      <attachment type="SupplementaryMaterial" hash="c9a3fe6c">2025.bionlp-1.25.SupplementaryMaterial.txt</attachment>
      <bibkey>liang-etal-2025-beyond</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.25</doi>
    </paper>
    <paper id="26">
      <title>Converting Annotated Clinical Cases into Structured Case Report Forms</title>
      <author><first>Pietro</first><last>Ferrazzi</last><affiliation>University of Padova</affiliation></author>
      <author id="alberto-lavelli"><first>Alberto</first><last>Lavelli</last><affiliation>FBK</affiliation></author>
      <author id="bernardo-magnini"><first>Bernardo</first><last>Magnini</last><affiliation>FBK</affiliation></author>
      <pages>307-318</pages>
      <abstract>Case Report Forms (CRFs) are largely used in medical research as they ensure accuracy, reliability, and validity of results in clinical studies. However, publicly available, well-annotated CRF datasets are scarce, limiting the development of CRF slot filling systems able to fill in a CRF from clinical notes. To mitigate the scarcity of CRF datasets, we propose to take advantage of available datasets annotated for information extraction tasks and to convert them into structured CRFs. We present a semi-automatic conversion methodology, which has been applied to the E3C dataset in two languages (English and Italian), resulting in a new, high-quality dataset for CRF slot filling. Through several experiments on the created dataset, we report that slot filling achieves 59.7% for Italian and 67.3% for English on a closed Large Language Models (zero-shot) and worse performances on three families of open-source models, showing that filling CRFs is challenging even for recent state-of-the-art LLMs.</abstract>
      <url hash="bb3e2e37">2025.bionlp-1.26</url>
      <attachment type="SupplementaryMaterial" hash="f2e2ead5">2025.bionlp-1.26.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="5d1ea0d1">2025.bionlp-1.26.SupplementaryMaterial.txt</attachment>
      <bibkey>ferrazzi-etal-2025-converting</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.26</doi>
    </paper>
    <paper id="27">
      <title><fixed-case>M</fixed-case>u<fixed-case>C</fixed-case>o<fixed-case>S</fixed-case>: Efficient Drug–Target Discovery via Multi-Context-Aware Sampling in Knowledge Graphs</title>
      <author><first>Haji</first><last>Gul</last></author>
      <author><first>Abdul Ghani</first><last>Naim</last></author>
      <author><first>Ajaz Ahmad</first><last>Bhat</last></author>
      <pages>319-327</pages>
      <abstract>Accurate prediction of drug–target interactions is critical for accelerating drug discovery. In this work, we frame drug–target prediction as a link prediction task on heterogeneous biomedical knowledge graphs (KG) that integrate drugs, proteins, diseases, pathways, and other relevant entities. Conventional KG embedding methods such as TransE and ComplEx-SE are hindered by their reliance on computationally intensive negative sampling and their limited generalization to unseen drug–target pairs. To address these challenges, we propose Multi-Context-Aware Sampling (MuCoS), a novel framework that prioritizes high-density neighbours to capture salient structural patterns and integrates these with contextual embeddings derived from BERT. By unifying structural and textual modalities and selectively sampling highly informative patterns, MuCoS circumvents the need for negative sampling, significantly reducing computational overhead while enhancing predictive accuracy for novel drug–target associations and drug targets. Extensive experiments on the KEGG50k and PharmKG-8k datasets demonstrate that MuCoS outperforms baselines, achieving up to a 13% improvement in MRR for general relation prediction on KEGG50k, a 22% improvement on PharmKG-8k, and a 6% gain in dedicated drug–target relation prediction on KEGG50k.</abstract>
      <url hash="08983335">2025.bionlp-1.27</url>
      <attachment type="SupplementaryMaterial" hash="4532e9c3">2025.bionlp-1.27.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="cd70a3f2">2025.bionlp-1.27.SupplementaryMaterial.txt</attachment>
      <bibkey>gul-etal-2025-mucos</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.27</doi>
    </paper>
    <paper id="28">
      <title>Overcoming Data Scarcity in Named Entity Recognition: Synthetic Data Generation with Large Language Models</title>
      <author><first>An</first><last>Dao</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Hiroki</first><last>Teranishi</last><affiliation>RIKEN Center for Advanced Intelligence Project</affiliation></author>
      <author id="yuji-matsumoto"><first>Yuji</first><last>Matsumoto</last><affiliation>Riken Center for Advanced Intelligence Project</affiliation></author>
      <author><first>Florian</first><last>Boudin</last><affiliation>Nantes University</affiliation></author>
      <author id="akiko-aizawa"><first>Akiko</first><last>Aizawa</last><affiliation>National Institute of Informatics</affiliation></author>
      <pages>328-340</pages>
      <abstract>Named Entity Recognition (NER) is crucial for extracting domain-specific entities from text, particularly in biomedical and chemical fields. Developing high-quality NER models in specialized domains is challenging due to the limited availability of annotated data, with manual annotation being a key method of data construction. However, manual annotation is time-consuming and requires domain expertise, making it difficult in specialized domains. Traditional data augmentation (DA) techniques also rely on annotated data to some extent, further limiting their effectiveness. In this paper, we propose a novel approach to synthetic data generation for NER using large language models (LLMs) to generate sentences based solely on a set of example entities. This method simplifies the augmentation process and is effective even with a limited set of entities.We evaluate our approach using BERT-based models on the BC4CHEMD, BC5CDR, and TDMSci datasets, demonstrating that synthetic data significantly improves model performance and robustness, particularly in low-resource settings. This work provides a scalable solution for enhancing NER in specialized domains, overcoming the limitations of manual annotation and traditional augmentation methods.</abstract>
      <url hash="e346e1f9">2025.bionlp-1.28</url>
      <attachment type="SupplementaryMaterial" hash="29366aa1">2025.bionlp-1.28.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="13f43304">2025.bionlp-1.28.SupplementaryMaterial.zip</attachment>
      <bibkey>dao-etal-2025-overcoming</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>P</fixed-case>et<fixed-case>EVAL</fixed-case>: A veterinary free text electronic health records benchmark</title>
      <author><first>Sean</first><last>Farrell</last><affiliation>Durham University</affiliation></author>
      <author><first>Alan</first><last>Radford</last><affiliation>University of Liverpool</affiliation></author>
      <author><first>Noura</first><last>Al Moubayed</last><affiliation>Durham University</affiliation></author>
      <author><first>Peter-John</first><last>Noble</last><affiliation>University of Liverpool</affiliation></author>
      <pages>341-353</pages>
      <abstract>We introduce PetEVAL, the first benchmark dataset derived from real-world, free-text veterinary electronic health records (EHRs). PetEVAL comprises 17,600 professionally annotated EHRs from first-opinion veterinary practices across the UK, partitioned into training (11,000), evaluation (1,600), and test (5,000) sets with distinct clinic distributions to assess model generalisability. Each record is annotated with International Classification of Disease 11 (ICD-11) syndromic chapter labels (20,408 labels), disease Named Entity Recognition (NER) tags (429 labels), and anonymisation NER tags (8,244 labels). PetEVAL enables evaluating Natural Language Processing (NLP) tools across applications, including syndrome surveillance and disease outbreak detection. We implement a multistage anonymisation protocol, replacing identifiable information with clinically relevant pseudonyms while establishing the first definition of identifiers in veterinary free text. PetEVAL introduces three core tasks: syndromic classification, disease entity recognition, and anonymisation. We provide baseline results using BERT-base, PetBERT, and LLaMA 3.1 8B generative models. Our experiments demonstrate the unique challenges of veterinary text, showcasing the importance of domain-specific approaches. By fostering advancements in veterinary informatics and epidemiology, we envision PetEVAL catalysing innovations in veterinary care, animal health, and comparative biomedical research through access to real-world, annotated veterinary clinical data.</abstract>
      <url hash="9ce5b160">2025.bionlp-1.29</url>
      <attachment type="SupplementaryMaterial" hash="ef7124cb">2025.bionlp-1.29.SupplementaryMaterial.txt</attachment>
      <attachment type="SupplementaryMaterial" hash="7b35a31d">2025.bionlp-1.29.SupplementaryMaterial.zip</attachment>
      <bibkey>farrell-etal-2025-peteval</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.29</doi>
    </paper>
    <paper id="30">
      <title>Virtual <fixed-case>CRISPR</fixed-case>: Can <fixed-case>LLM</fixed-case>s Predict <fixed-case>CRISPR</fixed-case> Screen Results?</title>
      <author><first>Steven</first><last>Song</last></author>
      <author><first>Abdalla</first><last>Abdrabou</last></author>
      <author><first>Asmita</first><last>Dabholkar</last></author>
      <author><first>Kastan</first><last>Day</last></author>
      <author><first>Pavan</first><last>Dharmoju</last></author>
      <author><first>Jason</first><last>Perera</last></author>
      <author><first>Volodymyr</first><last>Kindratenko</last></author>
      <author><first>Aly A.</first><last>Khan</last></author>
      <pages>354-364</pages>
      <abstract>CRISPR-Cas systems enable systematic investigation of gene function, but experimental CRISPR screens are resource-intensive. Here, we investigate the potential of Large Language Models (LLMs) to predict the outcomes of CRISPR screens in silico, thereby prioritizing experiments and accelerating biological discovery. We introduce a benchmark dataset derived from BioGRID-ORCS and manually curated sources, and evaluate the performance of several LLMs across various prompting strategies, including chain-of-thought and few-shot learning. Furthermore, we develop a novel, efficient prediction framework using LLM-derived embeddings, achieving significantly improved performance and scalability compared to direct prompting. Our results demonstrate the feasibility of using LLMs to guide CRISPR screen experiments.</abstract>
      <url hash="20eada0e">2025.bionlp-1.30</url>
      <attachment type="SupplementaryMaterial" hash="b80917d4">2025.bionlp-1.30.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="83c2e286">2025.bionlp-1.30.SupplementaryMaterial.txt</attachment>
      <bibkey>song-etal-2025-virtual</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.30</doi>
    </paper>
    <paper id="31">
      <title>Overview of the <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm 2025 Shared Task on Lay Summarization of Biomedical Research Articles and Radiology Reports</title>
      <author><first>Chenghao</first><last>Xiao</last><affiliation>Durham University</affiliation></author>
      <author><first>Kun</first><last>Zhao</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>Xiao</first><last>Wang</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Siwei</first><last>Wu</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Sixing</first><last>Yan</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Tomas</first><last>Goldsack</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Sophia</first><last>Ananiadou</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Noura</first><last>Al Moubayed</last><affiliation>Durham University</affiliation></author>
      <author><first>Liang</first><last>Zhan</last><affiliation>University of Pittsburgh</affiliation></author>
      <author><first>William K.</first><last>Cheung</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>Department of Computer Science, University of Manchester</affiliation></author>
      <pages>365-377</pages>
      <abstract>This paper presents the setup and results of the third edition of the BioLaySumm shared task on Lay Summarization of Biomedical Research Articles and Radiology Reports, hosted at the BioNLP Workshop at ACL 2025. In this task edition, we aim to build on the first two editions’ successes by further increasing research interest in this important task and encouraging participants to explore novel approaches that will help advance the state-of-the-art. Specifically, we introduce the new task of Radiology Report Generation with Layman’s terms, which is parallel to the task of lay summarization of biomedical articles in the first two editions. Overall, our results show that a broad range of innovative approaches were adopted by task participants, including inspiring explorations of latest RL techniques adopted in the training of general-domain large reasoning models.</abstract>
      <url hash="059921cf">2025.bionlp-1.31</url>
      <attachment type="SupplementaryMaterial" hash="436528cc">2025.bionlp-1.31.SupplementaryMaterial.txt</attachment>
      <bibkey>xiao-etal-2025-overview</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.31</doi>
    </paper>
    <paper id="32">
      <title>Overview of the <fixed-case>C</fixed-case>lin<fixed-case>IQL</fixed-case>ink 2025 Shared Task on Medical Question-Answering</title>
      <author><first>Brandon</first><last>Colelough</last><affiliation>NIH</affiliation></author>
      <author><first>Davis</first><last>Bartels</last><affiliation>National Institutes of Health</affiliation></author>
      <author><first>Dina</first><last>Demner-Fushman</last><affiliation>National Library of Medicine</affiliation></author>
      <pages>378-387</pages>
      <abstract>In this paper, we present an overview of CLINIQLINK a shared task, collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test large language models (LLMs) on medically-oriented question answering aimed at the level of a General Practitioner. The challenge supplies 4 978 expert-verified, medical source-grounded question–answer pairs that cover seven formats - true/false, multiple choice, unordered list, short answer, short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled in Docker or Apptainer images, are executed on the CodaBench platform or the University of Maryland’s Zaratan cluster. An automated harness (Task 1) scores closed-ended items by exact match and open-ended items with a three-tier embedding metric. A subsequent physician panel (Task 2) audits the top model responses.</abstract>
      <url hash="698d5efe">2025.bionlp-1.32</url>
      <attachment type="SupplementaryMaterial" hash="af75f410">2025.bionlp-1.32.SupplementaryMaterial.txt</attachment>
      <bibkey>colelough-etal-2025-overview</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>SMAFIRA</fixed-case> Shared Task at the <fixed-case>B</fixed-case>io<fixed-case>NLP</fixed-case>’2025 Workshop: Assessing the Similarity of the Research Goal</title>
      <author><first>Mariana</first><last>Neves</last><affiliation>German Federal Institute for Risk Assessment</affiliation></author>
      <author><first>Iva</first><last>Sovadinova</last><affiliation>Masaryk University</affiliation></author>
      <author><first>Susanne</first><last>Fieberg</last><affiliation>German Federal Institute for Risk Assessment</affiliation></author>
      <author><first>Celine</first><last>Heinl</last><affiliation>German Federal Institute for Risk Assessment</affiliation></author>
      <author><first>Diana</first><last>Rubel</last><affiliation>German Federal Institute for Risk Assessment</affiliation></author>
      <author><first>Gilbert</first><last>Schönfelder</last><affiliation>German Federal Institute for Risk Assessment</affiliation></author>
      <author><first>Bettina</first><last>Bert</last><affiliation>German Federal Institute for Risk Assessment</affiliation></author>
      <pages>388-395</pages>
      <abstract>We organized the SMAFIRA Shared in the scope of the BioNLP’2025 Workshop. Given two articles, our goal was to collect annotations about the similarity of their research goal. The test sets consisted of a list of reference articles and their corresponding top 20 similar articles from PubMed. The task consisted in annotating the similar articles regarding the similarity of their research goal with respect to the one from the corresponding reference article. The assessment of the similarity was based on three labels: "“similar”", "“uncertain”", or "“not similar”". We released two batches of test sets: (a) a first batch of 25 reference articles for five diseases; and (b) a second batch of 80 reference articles for 16 diseases. We collected manual annotations from two teams (RCX and Bf3R) and automatic predictions from two large language models (GPT-4omini and Llama3.3). The preliminary evaluation showed a rather low agreement between the annotators, however, some pairs could potentially be part of a future dataset.</abstract>
      <url hash="888501ca">2025.bionlp-1.33</url>
      <attachment type="SupplementaryMaterial" hash="fcbeca57">2025.bionlp-1.33.SupplementaryMaterial.txt</attachment>
      <bibkey>neves-etal-2025-smafira</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.33</doi>
    </paper>
    <paper id="34">
      <title>Overview of the <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025 Shared Task on Grounded Question Answering from Electronic Health Records</title>
      <author><first>Sarvesh</first><last>Soni</last><affiliation>National Library of Medicine</affiliation></author>
      <author><first>Soumya</first><last>Gayen</last><affiliation>NLM NIH</affiliation></author>
      <author><first>Dina</first><last>Demner-Fushman</last><affiliation>National Library of Medicine</affiliation></author>
      <pages>396-405</pages>
      <abstract>This paper presents an overview of the ArchEHR-QA 2025 shared task, which was organized with the 24th BioNLP Workshop at ACL 2025. The goal of this shared task is to develop automated responses to patients’ questions by generating answers that are grounded in key clinical evidence from patients’ electronic health records (EHRs). A total of 29 teams participated in the task, collectively submitting 75 systems, with 24 teams providing their system descriptions. The submitted systems encompassed diverse architectures (including approaches that select the most relevant evidence prior to answer generation), leveraging both proprietary and open-weight large language models, as well as employing various tuning strategies such as fine-tuning and few-shot learning. In this paper, we describe the task setup, the dataset used, the evaluation criteria, and the baseline systems. Furthermore, we summarize the methodologies adopted by participating teams and present a comprehensive evaluation and analysis of the submitted systems.</abstract>
      <url hash="89e64928">2025.bionlp-1.34</url>
      <attachment type="SupplementaryMaterial" hash="fddf0635">2025.bionlp-1.34.SupplementaryMaterial.txt</attachment>
      <bibkey>soni-etal-2025-overview</bibkey>
      <doi>10.18653/v1/2025.bionlp-1.34</doi>
    </paper>
  </volume>
  <volume id="share" ingest-date="2025-07-20" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 24th Workshop on Biomedical Language Processing (Shared Tasks)</booktitle>
      <editor><first>Sarvesh</first><last>Soni</last></editor>
      <editor><first>Dina</first><last>Demner-Fushman</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="96cd1904">2025.bionlp-share</url>
      <venue>bionlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-276-3</isbn>
      <doi>10.18653/v1/2025.bionlp-share</doi>
    </meta>
    <frontmatter>
      <url hash="7ce72d0e">2025.bionlp-share.0</url>
      <bibkey>bionlp-ws-2025-share</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.0</doi>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>A</fixed-case>rg<fixed-case>H</fixed-case>i<fixed-case>TZ</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality</title>
      <author><first>Adrian</first><last>Cuadron Cortes</last><affiliation>EHU/UPV</affiliation></author>
      <author><first>Aimar</first><last>Sagasti</last><affiliation>UPV-EHU</affiliation></author>
      <author><first>Maitane</first><last>Urruela</last><affiliation>UPV/EHU</affiliation></author>
      <author><first>Iker</first><last>De La Iglesia</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Ane</first><last>García Domingo-aldama</last><affiliation>UPV-EHU</affiliation></author>
      <author><first>Aitziber</first><last>Atutxa Salazar</last><affiliation>EHU/UPV</affiliation></author>
      <author><first>Josu</first><last>Goikoetxea</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Ander</first><last>Barrena</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>1-10</pages>
      <abstract>This work presents three different approaches to address the ArchEHR-QA 2025 Shared Task on automated patient question answering. We introduce an end-to-end prompt-based baseline and two two-step methods to divide the task, without utilizing any external knowledge. Both two step approaches first extract essential sentences from the clinical text—by prompt or similarity ranking—, and then generate the final answer from these notes. Results indicate that the re-ranker based two-step system performs best, highlighting the importance of selecting the right approach for each subtask. Our best run achieved an overall score of 0.44, ranking 8th out of 30 on the leaderboard, securing the top position in overall factuality.</abstract>
      <url hash="6b8f74c7">2025.bionlp-share.1</url>
      <bibkey>cuadron-cortes-etal-2025-arghitz</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.1</doi>
    </paper>
    <paper id="2">
      <title><fixed-case>UNIBUC</fixed-case>-<fixed-case>SD</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Prompting Our Way to Clinical <fixed-case>QA</fixed-case> with Multi-Model Ensembling</title>
      <author><first>Dragos</first><last>Ghinea</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Ștefania</first><last>Rîncu</last><affiliation>University of Bucharest</affiliation></author>
      <pages>11-21</pages>
      <abstract>In response to the ArchEHR-QA 2025 shared task, we present an efficient approach to patient question answering using small, pre-trained models that are widely available to the research community. Our method employs multi-prompt ensembling with models such as Gemma and Mistral, generating binary relevance judgments for clinical evidence extracted from electronic health records (EHRs). We use two distinct prompts (A and B) to assess the relevance of paragraphs to a patient’s question and aggregate the model outputs via a majority vote ensemble. The relevant passages are then summarized using a third prompt (C) with Gemma. By leveraging off-the-shelf models and consumer-grade hardware (1x RTX 5090), we demonstrate that it is possible to improve performance without relying on resource-intensive fine-tuning or training. Additionally, we explore the impact of Chain-of-Thought (CoT) prompting and compare the performance of specialized versus general-purpose models, showing that significant improvements can be achieved through effective use of existing models.</abstract>
      <url hash="c1edb173">2025.bionlp-share.2</url>
      <bibkey>ghinea-rincu-2025-unibuc</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.2</doi>
    </paper>
    <paper id="3">
      <title>Loyola at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Exploring Unsupervised Attribution of Generated Text: Attention and Clustering-Based Methods</title>
      <author><first>Rohan</first><last>Sethi</last><affiliation>Loyola University Chicago and Stritch School of Medicine</affiliation></author>
      <author><first>Timothy A.</first><last>Miller</last><affiliation>Boston Children’s Hospital, Harvard Medical School</affiliation></author>
      <author><first>Majid</first><last>Afshar</last><affiliation>University of Wisconsin-Madison</affiliation></author>
      <author><first>Dmitriy</first><last>Dligach</last><affiliation>Loyola University Chicago</affiliation></author>
      <pages>22-26</pages>
      <abstract>The increasing volume of patient messages via electronic health record (EHR) portals has contributed significantly to clinician workload. Automating responses to these messages can help alleviate this burden, but it is essential to ensure that the generated responses are grounded in accurate clinical evidence. As part of the ArchEHR-QA 2025 BioNLP ACL shared task, we explore unsupervised methods for generating patient question responses that are both contextually accurate and evidence-backed. We investigate three novel approaches: zero-shot prompting, clustering-based evidence selection, and attention-based evidence attribution, along with a hybrid model that combines clustering and attention. Our methods do not require model fine-tuning and leverage the inherent structure of the input data to identify the most relevant supporting evidence from clinical notes. Our best-performing approach, which integrates clustering and attention, demonstrates a substantial improvement in factuality over baseline zero-shot methods, highlighting the potential of unsupervised strategies for enhancing the clinical utility of large language models in EHR contexts.</abstract>
      <url hash="f6312d01">2025.bionlp-share.3</url>
      <bibkey>sethi-etal-2025-loyola</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.3</doi>
    </paper>
    <paper id="4">
      <title><fixed-case>CUNI</fixed-case>-a at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Do we need Giant <fixed-case>LLM</fixed-case>s for Clinical <fixed-case>QA</fixed-case>?</title>
      <author><first>Vojtech</first><last>Lanz</last><affiliation>Charles University</affiliation></author>
      <author><first>Pavel</first><last>Pecina</last><affiliation>Charles University</affiliation></author>
      <pages>27-40</pages>
      <abstract>In this paper, we present our submission to the ArchEHR-QA 2025 shared task, which focuses on answering patient questions based on excerpts from electronic health record (EHR) discharge summaries. Our approach identifies essential sentences relevant to a patient’s question using a combination of few-shot inference with the Med42-8B model, cosine similarity over clinical term embeddings, and the MedCPT cross-encoder relevance model. Then, concise answers are generated on the basis of these selected sentences. Despite not relying on large language models (LLMs) with tens of billions of parameters, our method achieves competitive results, demonstrating the potential of resource-efficient solutions for clinical NLP applications.</abstract>
      <url hash="6810e73e">2025.bionlp-share.4</url>
      <bibkey>lanz-pecina-2025-cuni</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>W</fixed-case>is<fixed-case>P</fixed-case>er<fixed-case>M</fixed-case>ed at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Modular, Relevance-First Approach for Grounded Question Answering on Eletronic Health Records</title>
      <author><first>Jan-Henning</first><last>Büns</last><affiliation>University of Applied Sciences and Arts Dortmund</affiliation></author>
      <author><first>Hendrik</first><last>Damm</last><affiliation>Department of Computer Science, University of Applied Sciences and Arts Dortmund, Institute for Medical Informatics, Biometry and Epidemiology, University Hospital Essen</affiliation></author>
      <author><first>Tabea</first><last>Pakull</last><affiliation>Institute for Transfusion Medicine, University Hospital Essen</affiliation></author>
      <author><first>Felix</first><last>Nensa</last><affiliation>Institute of Diagnostic and Interventional Radiology and Neuroradiology, University Hospital Essen, Institute for Artificial Intelligence in Medicine (IKIM), University Hospital Essen</affiliation></author>
      <author><first>Elisabeth</first><last>Livingstone</last><affiliation>Institute for Transfusion Medicine, University Hospital Essen</affiliation></author>
      <pages>41-49</pages>
      <abstract>Automatically answering patient questions based on electronic health records (EHRs) requires systems that both identify relevant evidence and generate accurate, grounded responses. We present a three-part pipeline developed by WisPerMed for the ArchEHR-QA 2025 shared task. First, a fine-tuned BioClinicalBERT model classifies note sentences by their relevance using synonym-based and paraphrased data augmentation. Second, a constrained generation step uses DistilBART-MedSummary to produce faithful answers strictly limited to top-ranked evidence. Third, we align each answer sentence to its supporting evidence via BiomedBERT embeddings and ROUGE-based similarity scoring to ensure citation transparency. Our system achieved a 35.0% overall score on the hidden test set, outperforming the organizer’s baseline by 4.3 percentage points. Gains in BERTScore (+44%) and SARI (+119%) highlight substantial improvements in semantic accuracy and relevance. This modular approach demonstrates that enforcing evidence-awareness and citation grounding enhances both answer quality and trustworthiness in clinical QA systems.</abstract>
      <url hash="269aa872">2025.bionlp-share.5</url>
      <bibkey>buns-etal-2025-wispermed</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.5</doi>
    </paper>
    <paper id="6">
      <title>hei<fixed-case>DS</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation</title>
      <author><first>Ashish</first><last>Chouhan</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Michael</first><last>Gertz</last><affiliation>Heidelberg University</affiliation></author>
      <pages>50-61</pages>
      <abstract>This paper presents the approach of our team called heiDS for the ArchEHR-QA 2025 shared task. A pipeline using a retrieval augmented generation (RAG) framework is designed to generate answers that are attributed to clinical evidence from the electronic health records (EHRs) of patients in response to patient-specific questions. We explored various components of a RAG framework, focusing on ranked list truncation (RLT) retrieval strategies and attribution approaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a query-dependent-k retrieval strategy, including the existing surprise and autocut methods and two new methods proposed in this work, autocut* and elbow. The experimental results show the benefits of our strategy in producing factual and relevant answers when compared to a fixed-k.</abstract>
      <url hash="d13c1197">2025.bionlp-share.6</url>
      <bibkey>chouhan-gertz-2025-heids</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.6</doi>
    </paper>
    <paper id="7">
      <title><fixed-case>U</fixed-case>ni<fixed-case>B</fixed-case>uc-<fixed-case>SB</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Resource-Constrained Pipeline for Relevance Classification and Grounded Answer Synthesis</title>
      <author><first>Sebastian</first><last>Balmus</last><affiliation>National Institute of Research and Development in Informatics - ICI Bucharest</affiliation></author>
      <author><first>Dura</first><last>Bogdan</last><affiliation>National Institute of Research and Development in Informatics - ICI Bucharest</affiliation></author>
      <author><first>Ana Sabina</first><last>Uban</last><affiliation>University of Bucharest</affiliation></author>
      <pages>62-68</pages>
      <abstract>We describe the UniBuc-SB submission to the ArchEHR-QA shared task, which involved generating grounded answers to patient questions based on electronic health records. Our system exceeded the performance of the provided baseline, achieving higher performance in generating contextually relevant responses. Notably, we developed our approach under constrained computational resources, utilizing only a single NVIDIA RTX 4090 GPU. We refrained from incorporating any external datasets, relying solely on the limited training data supplied by the organizers. To address the challenges posed by the low-resource setting, we leveraged off-the-shelf pre-trained language models and fine-tuned them minimally, aiming to maximize performance while minimizing overfitting.</abstract>
      <url hash="ae73bdd9">2025.bionlp-share.7</url>
      <bibkey>balmus-etal-2025-unibuc</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>KR</fixed-case> Labs at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Verbatim Approach for Evidence-Based Question Answering</title>
      <author><first>Adam</first><last>Kovacs</last><affiliation>KR Labs</affiliation></author>
      <author><first>Paul</first><last>Schmitt</last><affiliation>TU Vienna</affiliation></author>
      <author><first>Gabor</first><last>Recski</last><affiliation>TU Wien</affiliation></author>
      <pages>69-74</pages>
      <abstract>We present a lightweight, domain‐agnostic verbatim pipeline for evidence‐grounded question answering. Our pipeline operates in two steps: first, a sentence-level extractor flags relevant note sentences using either zero-shot LLM prompts or supervised ModernBERT classifiers. Next, an LLM drafts a question-specific template, which is filled verbatim with sentences from the extraction step. This prevents hallucinations and ensures traceability. In the ArchEHR‐QA 2025 shared task, our system scored 42.01%, ranking top‐10 in core metrics and outperforming the organiser’s 70B‐parameter Llama‐3.3 baseline. We publicly release our code and inference scripts under an MIT license.</abstract>
      <url hash="8826551b">2025.bionlp-share.8</url>
      <bibkey>kovacs-etal-2025-kr</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>LAIL</fixed-case>ab at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Test-time scaling for evidence selection in grounded question answering from electronic health records</title>
      <author><first>Tuan Dung</first><last>Le</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Thanh</first><last>Duong</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <author><first>Shohreh</first><last>Haddadan</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <author><first>Behzad</first><last>Jazayeri</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <author><first>Brandon</first><last>Manley</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <author><first>Thanh</first><last>Thieu</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <pages>75-80</pages>
      <abstract>This paper presents our approach to the ArchEHR shared task on generating answers to real-world patient questions grounded in evidence from electronic health records (EHRs). We investigate the zero-shot capabilities of general-purpose, domain-agnostic large language models (LLMs) in two key aspects: identifying essential supporting evidence and producing concise, coherent answers. To this aim, we propose a two-stage pipeline: (1) evidence identification via test-time scaling (TTS) and (2) generating the final answer conditioned on selected evidences from the previous stage.Our approach leverages high-temperature sampling to generate multiple outputs during the evidence selection phase. This TTS-based approach effectively explore more potential evidences which results in significant improvement of the factuality score of the answers.</abstract>
      <url hash="6996873e">2025.bionlp-share.9</url>
      <bibkey>le-etal-2025-lailab</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.9</doi>
    </paper>
    <paper id="10">
      <title><fixed-case>UTSA</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Improving <fixed-case>EHR</fixed-case> Question Answering via Self-Consistency Prompting</title>
      <author><first>Sara</first><last>Shields-Menard</last><affiliation>UTSA</affiliation></author>
      <author><first>Zach</first><last>Reimers</last><affiliation>UTSA</affiliation></author>
      <author><first>Joshua</first><last>Gardner</last><affiliation>UTSA</affiliation></author>
      <author><first>David</first><last>Perry</last><affiliation>UTSA</affiliation></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>81-90</pages>
      <abstract>We describe our system for the ArchEHR-QA Shared Task on answering clinical questions using electronic health records (EHRs). Our approach uses large language models in two steps: first, to find sentences in the EHR relevant to a clinician’s question, and second, to generate a short, citation-supported response based on those sentences. We use few-shot prompting, self-consistency, and thresholding to improve the sentence classification step to decide which sentences are essential. We compare several models and find that a smaller 8B model performs better than a larger 70B model for identifying relevant information. Our results show that accurate sentence selection is critical for generating high-quality responses and that self-consistency with thresholding helps make these decisions more reliable.</abstract>
      <url hash="02f79433">2025.bionlp-share.10</url>
      <bibkey>shields-menard-etal-2025-utsa</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>UTS</fixed-case>amuel at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Clinical Question Answering System for Responding to Patient Portal Messages Using Generative <fixed-case>AI</fixed-case></title>
      <author><first>Samuel</first><last>Reason</last><affiliation>UTHealth</affiliation></author>
      <author><first>Liwei</first><last>Wang</last><affiliation>UTHealth</affiliation></author>
      <author><first>Hongfang</first><last>Liu</last><affiliation>UTHealth</affiliation></author>
      <author><first>Ming</first><last>Huang</last><affiliation>UTHealth</affiliation></author>
      <pages>91-95</pages>
      <abstract>Responding to patient portal messages places a substantial burden on clinicians. To mitigate this, automatically generating answers to patient questions by considering their medical records is a critical solution. In this study, we proposed a clinical question answering system for the BioNLP 2025 Shared Task on Grounded Electronic Health Record Question Answering. The system processed each patient message case by selecting relevant sentences as evidences from the associated clinical notes and generating a concise, medically accurate answer to the patient’s question. A generative AI model from OpenAI (GPT-4o) was leveraged to assist with sentence selection and answer generation. Each response is grounded in source text, limited to 75 words, and includes sentence-level citations. The system was evaluated on 100 test cases using alignment, citation, and summarization metrics. Our results indicate the significant potential of the clinical question answering system based on generative AI models to streamline communication between patients and healthcare providers by automatically generating responses to patient messages.</abstract>
      <url hash="92e43748">2025.bionlp-share.11</url>
      <bibkey>reason-etal-2025-utsamuel</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.11</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>LAMAR</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Clinically Aligned <fixed-case>LLM</fixed-case>-Generated Few-Shot Learning for <fixed-case>EHR</fixed-case>-Grounded Patient Question Answering</title>
      <author><first>Seksan</first><last>Yoadsanit</last><affiliation>Faculty of Medicine Ramathibodi Hospital, Mahidol University</affiliation></author>
      <author><first>Nopporn</first><last>Lekuthai</last><affiliation>Faculty of Medicine Ramathibodi Hospital</affiliation></author>
      <author><first>Watcharitpol</first><last>Sermsrisuwan</last><affiliation>Faculty of Medicine, Ramathibodi Hospital, Mahidol University</affiliation></author>
      <author><first>Titipat</first><last>Achakulvisut</last><affiliation>Department of Biomedical Engineering, Mahidol University</affiliation></author>
      <pages>96-103</pages>
      <abstract>This paper presents an approach to answering patient-specific medical questions using electronic health record (EHR) grounding with ArchEHR-QA 2025 datasets. We address medical question answering as an alignment problem, focusing on generating responses factually consistent with patient-specific clinical notes through in-context learning techniques. We show that LLM-generated responses, used as few-shot examples with GPT-4.1 and Gemini-2.5-Pro, significantly outperform baseline approaches (overall score = 49.1), achieving strict precision, recall, and F1-micro scores of 60.6, 53.6, and 56.9, respectively, on the ArchEHR-QA 2025 test leaderboard. It achieves textual similarity between answers and essential evidence using BLEU, ROUGE, SARI, BERTScore, AlignScore, and MEDCON scores of 6.0, 32.1, 65.8, 36.4, 64.3, and 43.6, respectively. Our findings highlight the effectiveness of combining EHR grounding with few-shot examples for personalized medical question answering, establishing a promising approach for developing accurate and personalized medical question answering systems. We release our code at https://github.com/biodatlab/archehr-qa-lamar.</abstract>
      <url hash="cb9aa417">2025.bionlp-share.12</url>
      <bibkey>yoadsanit-etal-2025-lamar</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.12</doi>
      <revision id="1" href="2025.bionlp-share.12v1" hash="8d78bd8b"/>
      <revision id="2" href="2025.bionlp-share.12v2" hash="cb9aa417" date="2025-09-06">Minor fixes.</revision>
    </paper>
    <paper id="13">
      <title>Neural at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering</title>
      <author><first>Sai Prasanna Teja Reddy</first><last>Bogireddy</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Abrar</first><last>Majeedi</last><affiliation>UW Madison</affiliation></author>
      <author><first>Viswanath</first><last>Gajjala</last><affiliation>UW Madison</affiliation></author>
      <author><first>Zhuoyan</first><last>Xu</last><affiliation>UW Madison</affiliation></author>
      <author><first>Siddhant</first><last>Rai</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Vaishnav</first><last>Potlapalli</last><affiliation>NYU</affiliation></author>
      <pages>104-109</pages>
      <abstract>Automated question answering (QA) over electronic health records (EHRs) can bridge critical information gaps for clinicians and patients, yet it demands both precise evidence retrieval and faithful answer generation under limited supervision. In this work, we present Neural, the runner-up in the BioNLP 2025 ArchEHR-QA shared task on evidence grounded clinical QA. Our proposed method decouples the task into (1) sentence-level evidence identification and (2) answer synthesis with explicit citations. For each stage, we automatically explore the prompt space with DSPy’s MIPROv2 optimizer, jointly tuning instructions and few-shot demonstrations on the development set. A self-consistency voting scheme further improves evidence recall without sacrificing precision. On the hidden test set, our method attains an overall score of 51.5, placing second stage while outperforming standard zero-shot and few-shot prompting by over 20 and 10 points, respectively. These results indicate that data-driven prompt optimization is a cost-effective alternative to model fine-tuning for high-stakes clinical QA, advancing the reliability of AI assistants in healthcare.</abstract>
      <url hash="275a572f">2025.bionlp-share.13</url>
      <bibkey>bogireddy-etal-2025-neural</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>UIC</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Tri-Step Pipeline for Reliable Grounded Medical Question Answering</title>
      <author><first>Mohammad</first><last>Arvan</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Anuj</first><last>Gautam</last><affiliation>University of Illinois</affiliation></author>
      <author><first>Mohan</first><last>Zalake</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Karl M.</first><last>Kochendorfer</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>110-117</pages>
      <abstract>Automated response generation from electronic health records (EHRs) holds potential to reduce clinician workload, but it introduces important challenges related to factual accuracy and reliable grounding in clinical evidence. We present a structured three-step pipeline that uses large language models (LLMs) for evidence classification, guided response generation, and iterative quality control. To enable rigorous evaluation, our framework combines traditional reference-based metrics with a claim-level “LLM-as-a-Judge” methodology. On the ArchEHR-QA benchmark, our system achieves 82.0 percent claim-level evidence faithfulness and 51.6 percent citation-level factuality, demonstrating strong performance in generating clinically grounded responses. These findings highlight the utility of structured LLM pipelines in healthcare applications, while also underscoring the importance of transparent evaluation and continued refinement. All code, prompt templates, and evaluation tools are publicly available.</abstract>
      <url hash="b56c2ae2">2025.bionlp-share.14</url>
      <bibkey>arvan-etal-2025-uic</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>DMIS</fixed-case> Lab at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Evidence-Grounded Answer Generation for <fixed-case>EHR</fixed-case>-based <fixed-case>QA</fixed-case> via a Multi-Agent Framework</title>
      <author><first>Hyeon</first><last>Hwang</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeongsoon</first><last>Hwang</last><affiliation>Korea University</affiliation></author>
      <author><first>Jongmyung</first><last>Jung</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaehoon</first><last>Yun</last><affiliation>Hanyang University, College of Medicine</affiliation></author>
      <author><first>Minju</first><last>Song</last><affiliation>Korea University</affiliation></author>
      <author><first>Yein</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Dain</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Taewhoo</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Jiwoong</first><last>Sohn</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanwoong</first><last>Yoon</last><affiliation>Korea University</affiliation></author>
      <author><first>Sihyeon</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Jiwoo</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Heechul</first><last>Yang</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>118-125</pages>
      <abstract>The increasing utilization of patient portals has amplified clinicians’ workloads, primarily due to the necessity of addressing detailed patient inquiries related to their health concerns. The ArchEHR-QA 2025 shared task aims to alleviate this burden by automatically generating accurate, evidence-grounded responses to patients’ questions based on their Electronic Health Records (EHRs). This paper presents a six-stage multi-agent framework specifically developed to identify essential clinical sentences for answering patient questions, leveraging large language models (LLMs). Our approach begins with OpenAI’s o3 model generating focused medical context to guide downstream reasoning. In the subsequent stages, GPT-4.1-based agents assess the relevance of individual sentences, recruit domain experts, and consolidate their judgments to identify essential information for constructing coherent, evidence-grounded responses. Our framework achieved an Overall Factuality score of 62.0 and an Overall Relevance Score of 52.9 on the development set, and corresponding scores of 58.6 and 48.8, respectively, on the test set.</abstract>
      <url hash="6b7c7a70">2025.bionlp-share.15</url>
      <bibkey>hwang-etal-2025-dmis</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.15</doi>
      <revision id="1" href="2025.bionlp-share.15v1" hash="7bd35afc"/>
      <revision id="2" href="2025.bionlp-share.15v2" hash="6b7c7a70" date="2025-11-06">Update acknowledgments.</revision>
    </paper>
    <paper id="16">
      <title><fixed-case>C</fixed-case>og<fixed-case>S</fixed-case>tack-<fixed-case>KCL</fixed-case>-<fixed-case>UCL</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Investigating Hybrid <fixed-case>LLM</fixed-case> Approaches for Grounded Clinical Question Answering</title>
      <author><first>Shubham</first><last>Agarwal</last><affiliation>King’s College London</affiliation></author>
      <author><first>Thomas</first><last>Searle</last><affiliation>King’s College London</affiliation></author>
      <author><first>Kawsar</first><last>Noor</last><affiliation>University College London</affiliation></author>
      <author><first>Richard</first><last>Dobson</last><affiliation>King’s College London</affiliation></author>
      <pages>126-135</pages>
      <abstract>We present our system for the ArchEHR shared task, which focuses on answering clinical and patient-facing questions grounded in real-world EHR data. Our core contribution is a 2-Stage prompting pipeline that separates evidence selection from answer generation while employing in-context learning strategies. Our experimentation leveraged the open-weight Gemma-v3 family of models, with our best submission using the Gemma-12B model securing 5th place overall on the unseen test set. Through systematic experimentation, we demonstrate the effectiveness of task decomposition in improving both factual accuracy and answer relevance in grounded clinical question answering.</abstract>
      <url hash="d6530e30">2025.bionlp-share.16</url>
      <bibkey>agarwal-etal-2025-cogstack</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>S</fixed-case>zeged<fixed-case>AI</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Combining <fixed-case>LLM</fixed-case>s with traditional methods for grounded question answering</title>
      <author><first>Soma</first><last>Nagy</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Bálint</first><last>Nyerges</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Zsombor</first><last>Kispéter</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Gábor</first><last>Tóth</last><affiliation>University of Szeged</affiliation></author>
      <author><first>András</first><last>Szlúka</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Gábor</first><last>Kőrösi</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Zsolt</first><last>Szántó</last><affiliation>University of Szeged</affiliation></author>
      <author id="richard-farkas"><first>Richárd</first><last>Farkas</last><affiliation>University of Szeged</affiliation></author>
      <pages>136-149</pages>
      <abstract>In this paper, we present the SzegedAI team’s submissions to the ArchEHR-QA 2025 shared task. Our approaches include multiple prompting techniques for large language models (LLMs), sentence similarity methods, and traditional feature engineering. We are aiming to explore both modern and traditional solutions to the task. To combine the strengths of these diverse methods, we employed different ensembling strategies.</abstract>
      <url hash="a8298331">2025.bionlp-share.17</url>
      <bibkey>nagy-etal-2025-szegedai</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.17</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>LIMICS</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Prompting <fixed-case>LLM</fixed-case>s Beats Fine-Tuned Embeddings</title>
      <author><first>Adam</first><last>Remaki</last><affiliation>Sorbonne Univserity</affiliation></author>
      <author><first>Armand</first><last>Violle</last><affiliation>Sorbonne Université, Inserm, Université Sorbonne Paris-Nord, Laboratoire d’Informatique {{Médicale et d’Ingénierie des Connaissances en e-Santé, Paris, France</affiliation></author>
      <author><first>Vikram</first><last>Natraj</last><affiliation>Sorbonne Université, Inserm, Université Sorbonne Paris-Nord, Laboratoire d’Informatique Médicale et d’Ingénierie des Connaissances en e-Santé, Paris, France</affiliation></author>
      <author><first>Étienne</first><last>Guével</last><affiliation>Sorbonne Cluster for Artificial Intelligence, Paris, France</affiliation></author>
      <author><first>Akram</first><last>Redjdal</last><affiliation>Sorbonne Université, Inserm, Université Sorbonne Paris-Nord, Laboratoire d’Informatique Médicale et d’Ingénierie des Connaissances en e-Santé, Paris. FranceUniv Gustave Eiffel, Aix-Marseille Univ, LBA, F-13016 Marseille, France</affiliation></author>
      <pages>150-159</pages>
      <abstract>In this paper, we investigated two approaches to clinical question-answering based on patient-formulated questions, supported by their narratives and brief medical records. The first approach leverages zero- and few-shot prompt engineering techniques with GPT-based Large Language Models (LLMs), incorporating strategies such as prompt chaining and chain-of-thought reasoning to guide the models in generating answers. The second approach adopts a two-steps structure: first, a text-classification stage uses embedding-based models (e.g., BERT variants) to identify sentences within the medical record that are most relevant to the given question; then, we prompt an LLM to paraphrase them into an answer so that it is generated exclusively from these selected sentences. Our empirical results demonstrate that the first approach outperforms the classification-guided pipeline, achieving the highest score on the development set and the test set using prompt chaining. Code: github.com/armandviolle/BioNLP-2025</abstract>
      <url hash="c4f86a5a">2025.bionlp-share.18</url>
      <bibkey>remaki-etal-2025-limics</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.18</doi>
    </paper>
    <paper id="19">
      <title>razreshili at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Contrastive Fine-Tuning for Retrieval-Augmented Biomedical <fixed-case>QA</fixed-case></title>
      <author><first>Arina</first><last>Zemchyk</last><affiliation>-</affiliation></author>
      <pages>160-164</pages>
      <abstract>We present a retrieval-augmented system for the ArchEHR-QA 2025 shared task, which focuses on generating concise, medically accurate answers to clinical questions based on a patient’s electronic health record (EHR). A key challenge is following a strict cita- tion format that references relevant sentence IDs. To improve retrieval, we fine-tuned an all-MiniLM-L6-v2 embedding model using contrastive learning on over 2,300 question–sentence triplets, with DoRA for efficient adaptation. Sentences were selected using cosine similarity thresholds and passed into a quantized Mistral-7B-Instruct model along with a structured prompt. Our system achieved similar relevance to the baseline but lower overall performance (19.3 vs. 30.7), due to issues with citation formatting and generation quality. We discuss limitations such as threshold tuning, prompt-following ability, and model size, and suggest future directions for improving structured biomedical QA.</abstract>
      <url hash="20da774a">2025.bionlp-share.19</url>
      <bibkey>zemchyk-2025-razreshili</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>DKITNLP</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Retrieval Augmented <fixed-case>LLM</fixed-case> Pipeline for Evidence-Based Patient Question Answering</title>
      <author><first>Provia</first><last>Kadusabe</last><affiliation>Dundalk Institute of Technology</affiliation></author>
      <author><first>Abhishek</first><last>Kaushik</last><affiliation>Dundalk Institute of Technology</affiliation></author>
      <author><first>Fiona</first><last>Lawless</last><affiliation>Dundalk Institute of Technology</affiliation></author>
      <pages>165-170</pages>
      <abstract>This paper describes our submission for the BioNLP ACL 2025 Shared task on grounded Question Answering (QA) from Electronic Health Records (EHRs). The task aims to automatically generate answers to patients’ health related questions that are grounded in the evidence from their clinical notes. We propose a two stage retrieval pipeline to identify relevant sentences to guide response generation by a Large Language Model (LLM). Specifically, our approach uses a BioBERT based bi-encoder for initial retrieval, followed by a re-ranking step using a fine-tuned cross-encoder to enhance retrieval precision. The final set of selected sentences serve as an input to Mistral 7B model which generates answers through few-shot prompting. Our approach achieves an overall score of 31.6 on the test set, outperforming a substantially larger baseline model LLaMA 3.3 70B (30.7), which demonstrates the effectiveness of retrieval-augmented generation for grounded QA.</abstract>
      <url hash="734aacdd">2025.bionlp-share.20</url>
      <bibkey>kadusabe-etal-2025-dkitnlp</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.20</doi>
    </paper>
    <paper id="21">
      <title><fixed-case>AEHRC</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm 2025: Leveraging T5 for Lay Summarisation of Radiology Reports</title>
      <author><first>Wenjun</first><last>Zhang</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Shekhar</first><last>Chandra</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Bevan</first><last>Koopman</last><affiliation>CSIRO</affiliation></author>
      <author><first>Jason</first><last>Dowling</last><affiliation>Australian e-Health Research Centre, CSIRO Health and Biosecurity</affiliation></author>
      <author><first>Aaron</first><last>Nicolson</last><affiliation>Australian e-Health Research Centre, CSIRO Health and Biosecurity</affiliation></author>
      <pages>171-178</pages>
      <abstract>Biomedical texts, such as research articles and clinical reports, are often written in highly technical language, making them difficult for patients and the general public to understand. The BioLaySumm 2025 Shared Task addresses this challenge by promoting the development of models that generate lay summarisation of biomedical content. This paper focuses on Subtask 2.1: Radiology Report Generation with Layman’s Terms. In this work, we evaluate two large language model (LLM) architectures, T5-large (700M parameter encoder–decoder model) and LLaMA-3.2-3B (3B parameter decoder-only model). Both models are trained under fully-supervised conditions using the task’s multi-source dataset. Our results show that T5-large consistently outperforms LLaMA-3.2-3B across nine out of ten metrics, including relevance, readability, and clinical accuracy, despite having only a quarter of the parameters. Our T5-based model achieved the top rank in both the open-source and close-source tracks of the subtask 2.1.</abstract>
      <url hash="796c5e06">2025.bionlp-share.21</url>
      <bibkey>zhang-etal-2025-aehrc</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>M</fixed-case>etnin<fixed-case>O</fixed-case>z<fixed-case>U</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Text Summarization with Reverse Data Augmentation and Injecting Salient Sentences</title>
      <author><first>Egecan</first><last>Evgin</last><affiliation>MSc AI Student, Ozyegin University</affiliation></author>
      <author><first>Ilknur</first><last>Karadeniz</last><affiliation>Ozyegin University</affiliation></author>
      <author><first>Olcay Taner</first><last>Yıldız</last><affiliation>Department of Computer Engineering, Ozyegin University</affiliation></author>
      <pages>179-184</pages>
      <abstract>In this paper, we present our approach to the BioLaySumm 2025 Shared Task on lay summarization of biomedical research articles, which was conducted as part of the BioNLP Workshop 2025. The aim of the task is to create lay summaries from scientific articles to improve accessibility for a non-expert audience. To this end, we applied preprocessing techniques to clean and standardize the input texts, and fine-tuned Qwen2.5 and Qwen3-based language models for the summarization task. For abstract-based fine-tuning, we investigated whether we can insert salient sentences from the main article into the summary to enrich the input. We also curated a dataset of child-friendly articles with corresponding gold-standard summaries and used large language models to rewrite them into more complex scientific variants to augment our training data with more examples.</abstract>
      <url hash="25ef6a14">2025.bionlp-share.22</url>
      <bibkey>evgin-etal-2025-metninozu</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.22</doi>
    </paper>
    <paper id="23">
      <title>Shared Task at Biolaysumm2025 : Extract then summarize approach Augmented with <fixed-case>UMLS</fixed-case> based Definition Retrieval for Lay Summary generation.</title>
      <author><first>Aaradhya</first><last>Gupta</last><affiliation>International Institute of Information Technology, Hyderabad</affiliation></author>
      <author><first>Parameswari</first><last>Krishnamurthy</last><affiliation>Assistant Professor, IIIT Hyderabad</affiliation></author>
      <pages>185-189</pages>
      <abstract>The paper presents a modular, two‐track lay‐summary generation system for biomedical research articles, evaluated on the PLOS and eLife subsets of the BioLaySumm2025 shared task. In Task 1, it extracts salient sentences via an LLM–based chunking and summarization pipeline, then applies iterative rewriting to produce an accessible summary. In Task 2, it augments that summary with UMLS‐sourced definitions identified by a BioBERT NER model, yielding improved readability and factual consistency, at the cost of slight reductions in n‐gram overlap metrics like ROUGE and BLEU.</abstract>
      <url hash="45da512a">2025.bionlp-share.23</url>
      <bibkey>gupta-krishnamurthy-2025-shared</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.23</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>R</fixed-case>ain<fixed-case>C</fixed-case>ity<fixed-case>NLP</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Extract then Summarize at Home</title>
      <author><first>Jen</first><last>Wilson</last><affiliation>University of Washington</affiliation></author>
      <author><first>Michael</first><last>Pollack</last><affiliation>University of Washington</affiliation></author>
      <author><first>Rachel</first><last>Edwards</last><affiliation>University of Washington</affiliation></author>
      <author><first>Avery</first><last>Bellamy</last><affiliation>University of Washington</affiliation></author>
      <author><first>Helen</first><last>Salgi</last><affiliation>University of Washington</affiliation></author>
      <pages>190-195</pages>
      <abstract>As part of the BioLaySumm shared task at ACL 2025, we developed a summarization tool designed to translate complex biomedical texts into layperson-friendly summaries. Our goal was to enhance accessibility and comprehension for patients and others without specialized medical knowledge. The system employed an extractive-then-abstractive summarization pipeline. For the abstractive component, we experimented with two models: Pegasus-XSum and a Falcons.ai model pre-trained on medical data. Final outputs were evaluated using the official BioLaySumm 2025 metrics. To promote practical accessibility, we completed all experimentation on consumer-grade hardware, demonstrating the feasibility of our approach in low-resource settings.</abstract>
      <url hash="8c2236bf">2025.bionlp-share.24</url>
      <bibkey>wilson-etal-2025-raincitynlp</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>TLPIQ</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Hide and Seq, a <fixed-case>FLAN</fixed-case>-T5 Model for Biomedical Summarization</title>
      <author><first>Melody</first><last>Bechler</last><affiliation>University of Washington</affiliation></author>
      <author><first>Carly</first><last>Crowther</last><affiliation>UW</affiliation></author>
      <author><first>Emily</first><last>Luedke</last><affiliation>University of Washington</affiliation></author>
      <author><first>Natasha</first><last>Schimka</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ibrahim</first><last>Sharaf</last><affiliation>University of Washington</affiliation></author>
      <pages>196-201</pages>
      <abstract>BioLaySumm 2025 is a shared task that aims to automatically generate lay summaries of scientific papers for a wider audience of readers without domain-specific knowledge, making scientific discoveries in the domain of biology and medicine more accessible to the general public. Our submission to the task is a FLAN-T5 base model fine-tuned on the abstract and conclusion of articles and expert-written lay summaries from the shared task’s provided datasets. We find that our system performs competitively in terms of relevance, exceeds the baseline on factuality, but falls short on readability.</abstract>
      <url hash="48cd678f">2025.bionlp-share.25</url>
      <bibkey>bechler-etal-2025-tlpiq</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.25</doi>
    </paper>
    <paper id="26">
      <title><fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm<fixed-case>X</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Retrieval-Augmented Fine-Tuning for Biomedical Lay Summarization Using Abstracts and Retrieved Full-Text Context</title>
      <author><first>Fan</first><last>Lin</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Dezhi</first><last>Yu</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>202-214</pages>
      <abstract>Generating lay summaries of biomedical research remains a time-intensive task, despite their importance in bridging the gap between scientific findings and non-expert audiences. This study introduces a retrieval-augmented fine-tuning framework for biomedical lay summarization, integrating abstract-driven semantic retrieval with LoRA-tuned LLaMA 3.1 models. Abstracts are used as queries to retrieve relevant text segments from full-text articles, which are then incorporated into prompts for supervised fine-tuning. Evaluations on the PLOS and eLife datasets show that this hybrid approach significantly improves relevance and factuality metrics compared to both base models and those tuned individually, while maintaining competitive readability. Prompt design experiments highlight a trade-off between readability and factual accuracy. Our fine-tuned model demonstrates strong performance in relevance and factuality among open-source systems and rivals closed-source models such as GPT, providing an efficient and effective solution for domain-specific lay summarization.</abstract>
      <url hash="00ddb96d">2025.bionlp-share.26</url>
      <bibkey>lin-yu-2025-laysummx</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.26</doi>
    </paper>
    <paper id="27">
      <title>5c<fixed-case>NLP</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Prompts, Retrieval, and Multimodal Fusion</title>
      <author><first>Juan Antonio</first><last>Lossio-Ventura</last><affiliation>National Institutes of Health</affiliation></author>
      <author><first>Callum</first><last>Chan</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Arshitha</first><last>Basavaraj</last><affiliation>International Institute of Information Technology, Bangalore, India</affiliation></author>
      <author><first>Hugo</first><last>Alatrista-Salas</last><affiliation>De Vinci Research Center, Paris, France</affiliation></author>
      <author><first>Francisco</first><last>Pereira</last><affiliation>National Institute of Mental Health</affiliation></author>
      <author id="diana-inkpen"><first>Diana</first><last>Inkpen</last><affiliation>University of Ottawa</affiliation></author>
      <pages>215-231</pages>
      <abstract>In this work, we present our approach to addressing all subtasks of the BioLaySumm 2025 shared task by leveraging prompting and retrieval strategies, as well as multimodal input fusion. Our method integrates: (1) zero-shot and few-shot prompting with large language models (LLMs); (2) semantic similarity-based dynamic few-shot prompting; (3) retrieval-augmented generation (RAG) incorporating biomedical knowledge from the Unified Medical Language System (UMLS); and (4) a multimodal fusion pipeline that combines images and captions using image-text-to-text generation for enriched lay summarization. Our framework enables lightweight adaptation of pretrained LLMs for generating lay summaries from scientific articles and radiology reports. Using modern LLMs, including Llama-3.3-70B-Instruct and GPT-4.1, our 5cNLP team achieved third place in Subtask 1.2 and second place in Subtask 2.1, among all submissions.</abstract>
      <url hash="a6c9f792">2025.bionlp-share.27</url>
      <bibkey>lossio-ventura-etal-2025-5cnlp</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.27</doi>
    </paper>
    <paper id="28">
      <title><fixed-case>MIRAGES</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: The Impact of Search Terms and Data Curation for Biomedical Lay Summarization</title>
      <author><first>Benjamin</first><last>Pong</last><affiliation>University of Washington</affiliation></author>
      <author><first>J u - H u i</first><last>Chen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jonathan</first><last>Jiang</last><affiliation>University of Washington</affiliation></author>
      <author><first>Abimael</first><last>Jimenez</last><affiliation>University of Washington</affiliation></author>
      <author><first>Melody</first><last>Vahadi</last><affiliation>University of Washington</affiliation></author>
      <pages>232-239</pages>
      <abstract>Biomedical articles are often inaccessible to non-experts due to their technical complexity. To improve readability and factuality of lay summaries, we built on an extract-then-summarize framework by experimenting with novel extractive summarization strategies and employing Low Rank Adaptation (LoRA) fine-tuning of Meta-Llama-3-8B-Instruct on data selected by these strategies. We also explored counterfactual data augmentation and post-processing definition insertion to further enhance factual grounding and accessibility. Our best performing system treats the article’s title and keywords (i.e. search terms) as a single semantic centroid and ranks sentences by their semantic similarity to this centroid. This constrained selection of data serves as input for fine-tuning, achieving marked improvements in readability and factuality of downstream abstractive summaries while maintaining relevance. Our approach highlights the importance of quality data curation for biomedicallay summarization, resulting in 4th best overall performance and 2nd best Readability performance for the BioLaySumm 2025 Shared Task at BioNLP 2025.</abstract>
      <url hash="4c4a3d2d">2025.bionlp-share.28</url>
      <bibkey>pong-etal-2025-mirages</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>SUWMIT</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Instruction-based Summarization with Contrastive Decoding</title>
      <author><first>Priyam</first><last>Basu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jose</first><last>Cols</last><affiliation>University of Washington</affiliation></author>
      <author><first>Daniel</first><last>Jarvis</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yongsin</first><last>Park</last><affiliation>University of Washington</affiliation></author>
      <author><first>Daniel</first><last>Rodabaugh</last><affiliation>University of Washington</affiliation></author>
      <pages>240-248</pages>
      <abstract>In the following paper, we present our team’s approach to subtask 1.1 of the BioLaySumm 2025 shared task, which entails the automated generation of lay summaries from biomedical articles. To this end, we experiment with a variety of methods for text preprocessing, extractive summarization, model fine-tuning, and abstractive summarization. Our final results are generated on a fine-tuned Llama 3.1 Instruct (8B) model, notably achieving top scores on two out of four relevance metrics, as well as the highest overall ranking among this year’s participating teams on the plain lay summarization subtask.</abstract>
      <url hash="91a41f39">2025.bionlp-share.29</url>
      <bibkey>basu-etal-2025-suwmit</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.29</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>BDA</fixed-case>-<fixed-case>UC</fixed-case>3<fixed-case>M</fixed-case> @ <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Efficient Lay Summarization with Small-Scale <fixed-case>S</fixed-case>o<fixed-case>TA</fixed-case> <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ilyass</first><last>Ramzi</last><affiliation>Universidad Carlos III de Madrid</affiliation></author>
      <author><first>Isabel</first><last>Bedmar</last><affiliation>Computer Science and Engineering Department, Universidad Carlos III de Madrid</affiliation></author>
      <pages>249-255</pages>
      <abstract>This paper presents an efficient system for the BioLaySumm 2025 Shared Task on biomedical lay summarization. The approach leverages compact, state-of-the-art language models (4–7 billion parameters), including Gemma3 4B, Qwen3 4B, and GPT-4.1-mini, optimized for relevance, readability, and factuality. Through dynamic 4-bit quantization, parameter-efficient fine-tuning, advanced extractive preprocessing, and direct preference optimization, the system achieves performance competitive with much larger baselines. Comprehensive experiments on the eLife and PLOS datasets demonstrate that small language models can deliver high-quality, accessible biomedical summaries using modest computational resources. The findings suggest that resource-efficient models can help democratize access to scientific information, supporting broader scientific communication goals.</abstract>
      <url hash="f12fb63c">2025.bionlp-share.30</url>
      <bibkey>ramzi-bedmar-2025-bda</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.30</doi>
    </paper>
    <paper id="31">
      <title><fixed-case>KHU</fixed-case>_<fixed-case>LDI</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Fine-tuning and Refinement for Lay Radiology Report Generation</title>
      <author><first>Nur Alya Dania</first><last>Binti Moriazi</last><affiliation>Kyung Hee University</affiliation></author>
      <author><first>Mujeen</first><last>Sung</last><affiliation>Kyung Hee University</affiliation></author>
      <pages>256-268</pages>
      <abstract>Though access to one’s own radiology reports has improved over the years, the use of complex medical terms makes understanding these reports difficult. To tackle this issue, we explored two approaches: supervised fine-tuning open-source large language models using QLoRA, and refinement, which improves a given generated output using feedback generated by a feedback model. Despite the fine-tuned model outperforming refinement on the test data, refinement showed good results on the validation set, thus showing good potential in the generation of lay radiology reports. Our submission achieved 2nd place in the open track of Subtask 2.1 of the BioLaySumm 2025 shared task.</abstract>
      <url hash="c9610ff1">2025.bionlp-share.31</url>
      <bibkey>binti-moriazi-sung-2025-khu</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.31</doi>
    </paper>
    <paper id="32">
      <title><fixed-case>CUTN</fixed-case>_<fixed-case>B</fixed-case>io at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Multi-Task Prompt Tuning with External Knowledge and Readability adaptation for Layman Summarization</title>
      <author><first>Bhuvaneswari</first><last>Sivagnanam</last><affiliation>Central University of Tamil Nadu</affiliation></author>
      <author><first>Rivo Krishnu</first><last>C H</last><affiliation>Central University of Tamil Nadu</affiliation></author>
      <author><first>Princi</first><last>Chauhan</last><affiliation>Central University of Tamil Nadu</affiliation></author>
      <author><first>Saranya</first><last>Rajiakodi</last><affiliation>Central University of Tamil Nadu</affiliation></author>
      <pages>269-274</pages>
      <abstract>In this study, we presented a prompt based layman summarization framework for the biomedical articles and radiology reports developed as part of the BioLaySumm 2025 shared task at the BioNLP Workshop, ACL 2025. For Subtask 1.1 (Plain Lay Summarization), we utilized the abstract as input and employed Meta-LLaMA-3-8B-Instruct with a Tree-of-Thought prompting strategy and obtained 13th rank. In Subtask 1.2 (Lay Summarization with External Knowledge), we adopted an extractive plus prompt approach by combining LEAD-K sentence extraction with Meta-LLaMA-3-8B-Instruct. Medical concepts were identified using MedCAT, and their definitions were taken from Wikipedia to enrich the generated summaries. Our system secured the 2nd position in this subtask. For Subtask 2.1 (Radiology Report Translation), we implemented a Retrieval-Augmented Generation (RAG) approach using the Zephyr model to convert professional radiology reports into layman terms, achieved 3rd place in the shared task.</abstract>
      <url hash="c84ac665">2025.bionlp-share.32</url>
      <bibkey>sivagnanam-etal-2025-cutn</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.32</doi>
    </paper>
    <paper id="33">
      <title>Team <fixed-case>XSZ</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Section-Wise Summarization, Retrieval-Augmented <fixed-case>LLM</fixed-case>, and Reinforcement Learning Fine-Tuning for Lay Summaries</title>
      <author><first>Pengcheng</first><last>Xu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Sicheng</first><last>Shen</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Jieli</first><last>Zhou</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hongyi</first><last>Xin</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>275-280</pages>
      <abstract>We propose a unified, multi-stage lay summarization pipeline for BioLaySumm 2025 (Subtask 1.1) that (1) selects and summarizes key article sections via BioBART, (2) retrieves K-shot demonstrations using BGE embeddings for in-context Llama 3 8B prompting, (3) applies LoRA adapters to Llama 3 8B for supervised fine-tuning, (4) merges section summaries with a second BioBART pass, and (5) refines outputs through reinforcement learning (PPO &amp; GRPO) using a composite reward of factuality (AlignScore, SummaC), relevance (ROUGE-L, BERTScore), and readability (LENS, FKGL, DCRS, CLI). On PLOS and eLife validation sets, our complete systemreduces DCRS from 9.23 to 8.56 and reduces CLI from 12.98 to 12.65, ranking 3rd in readability. and outperforms llama3 finetune baseline in AlignScore 0.722 to 0.862, ranking 5th in factuality, demonstrating balanced gains across readability, relevance, and factuality.</abstract>
      <url hash="c1b81dc3">2025.bionlp-share.33</url>
      <bibkey>xu-etal-2025-team</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.33</doi>
      <revision id="1" href="2025.bionlp-share.33v1" hash="331a8305"/>
      <revision id="2" href="2025.bionlp-share.33v2" hash="c1b81dc3" date="2025-09-02">This revision mainly updated some citations.</revision>
    </paper>
    <paper id="34">
      <title><fixed-case>V</fixed-case>e<fixed-case>R</fixed-case>ea<fixed-case>F</fixed-case>ine: Iterative Verification Reasoning Refinement <fixed-case>RAG</fixed-case> for Hallucination-Resistant on Open-Ended Clinical <fixed-case>QA</fixed-case></title>
      <author><first>Pakawat</first><last>Phasook</last><affiliation>King Mongkut’s University of Technology Thonburi</affiliation></author>
      <author><first>Rapepong</first><last>Pitijaroonpong</last><affiliation>PreceptorAI</affiliation></author>
      <author><first>Jiramet</first><last>Kinchagawat</last><affiliation>CARIVA (Thailand) Company Limited</affiliation></author>
      <author><first>Amrest</first><last>Chinkamol</last><affiliation>School of Information Science and Technology, VISTEC</affiliation></author>
      <author><first>Tossaporn</first><last>Saengja</last><affiliation>Vidyasirimedhi Institute of Science and Technology (VISTEC)</affiliation></author>
      <author><first>Kiartnarin</first><last>Udomlapsakul</last><affiliation>Cariva (Thailand)</affiliation></author>
      <author><first>Jitkapat</first><last>Sawatphol</last><affiliation>CARIVA (Thailand) Co, Ltd.</affiliation></author>
      <author><first>Piyalitt</first><last>Ittichaiwong</last><affiliation>Faculty of Medicine Siriraj Hospital, Mahidol University</affiliation></author>
      <pages>281-288</pages>
      <abstract>We present VeReaFine, a novel “Verifier-RAG” pipeline designed to eliminate hallucinations in open-ended clinical question answering. VeReaFine interleaves three tightly coupled stages—retrieval, verification, and generation—across up to three iterations. First, a two-stage dense retriever (BM-Retriever-410M → BM-Reranker-2B) fetches and ranks top-k biomedical passages; an 8B-parameter MedReason verifier then filters these for direct relevance and identifies missing evidence. When the verifier deems the context insufficient, it formulates a focused “feedback query” to retrieve additional passages (bounded to prevent infinite loops). Once a minimal ground-truth context is assembled, a 7B-parameter generator (Qwen2.5-7B-Instruct) drafts an answer purely from that vetted context, and the verifier performs a final check—prompting the generator to refine any remaining unsupported claims. By iteratively fetching only missing facts and ensuring every assertion is evidence-backed, VeReaFine achieves monotonic factuality improvements with minimal overhead. On the BioNLP 2025 ClinIQLink “LLM Lie-Detector” shared task, our 7B generator augmented with VeReaFine matches or surpasses a 32B medical model on open-ended reasoning metrics, reducing multi-hop inverse step-identification errors by 26%. These findings demonstrate that moderate-size LLMs, when guided by targeted verification loops, can deliver expert-level reliability in clinical QA.</abstract>
      <url hash="041ae80b">2025.bionlp-share.34</url>
      <bibkey>phasook-etal-2025-vereafine</bibkey>
      <doi>10.18653/v1/2025.bionlp-share.34</doi>
    </paper>
  </volume>
</collection>
