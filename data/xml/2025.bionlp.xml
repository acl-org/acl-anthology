<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.bionlp">
  <volume id="share" ingest-date="2025-07-20" type="proceedings">
    <meta>
      <booktitle>BioNLP 2025 Shared Tasks</booktitle>
      <editor><first>Sarvesh</first><last>Soni</last></editor>
      <editor><first>Dina</first><last>Demner-Fushman</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>August</month>
      <year>2025</year>
      <url hash="96cd1904">2025.bionlp-share</url>
      <venue>bionlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-276-3</isbn>
    </meta>
    <frontmatter>
      <url hash="7ce72d0e">2025.bionlp-share.0</url>
      <bibkey>bionlp-ws-2025-share</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>A</fixed-case>rg<fixed-case>H</fixed-case>i<fixed-case>TZ</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality</title>
      <author><first>Adrian</first><last>Cuadron Cortes</last><affiliation>EHU/UPV</affiliation></author>
      <author><first>Aimar</first><last>Sagasti</last><affiliation>UPV-EHU</affiliation></author>
      <author><first>Maitane</first><last>Urruela</last><affiliation>UPV/EHU</affiliation></author>
      <author><first>Iker</first><last>De La Iglesia</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Ane</first><last>García Domingo-aldama</last><affiliation>UPV-EHU</affiliation></author>
      <author><first>Aitziber</first><last>Atutxa Salazar</last><affiliation>EHU/UPV</affiliation></author>
      <author><first>Josu</first><last>Goikoetxea</last><affiliation>University of the Basque Country</affiliation></author>
      <author><first>Ander</first><last>Barrena</last><affiliation>University of the Basque Country</affiliation></author>
      <pages>1-10</pages>
      <abstract>This work presents three different approaches to address the ArchEHR-QA 2025 Shared Task on automated patient question answering. We introduce an end-to-end prompt-based baseline and two two-step methods to divide the task, without utilizing any external knowledge. Both two step approaches first extract essential sentences from the clinical text—by prompt or similarity ranking—, and then generate the final answer from these notes. Results indicate that the re-ranker based two-step system performs best, highlighting the importance of selecting the right approach for each subtask. Our best run achieved an overall score of 0.44, ranking 8th out of 30 on the leaderboard, securing the top position in overall factuality.</abstract>
      <url hash="6b8f74c7">2025.bionlp-share.1</url>
      <bibkey>cuadron-cortes-etal-2025-arghitz</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>UNIBUC</fixed-case>-<fixed-case>SD</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Prompting Our Way to Clinical <fixed-case>QA</fixed-case> with Multi-Model Ensembling</title>
      <author><first>Dragos</first><last>Ghinea</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Ștefania</first><last>Rîncu</last><affiliation>University of Bucharest</affiliation></author>
      <pages>11-21</pages>
      <abstract>In response to the ArchEHR-QA 2025 shared task, we present an efficient approach to patient question answering using small, pre-trained models that are widely available to the research community. Our method employs multi-prompt ensembling with models such as Gemma and Mistral, generating binary relevance judgments for clinical evidence extracted from electronic health records (EHRs). We use two distinct prompts (A and B) to assess the relevance of paragraphs to a patient’s question and aggregate the model outputs via a majority vote ensemble. The relevant passages are then summarized using a third prompt (C) with Gemma. By leveraging off-the-shelf models and consumer-grade hardware (1x RTX 5090), we demonstrate that it is possible to improve performance without relying on resource-intensive fine-tuning or training. Additionally, we explore the impact of Chain-of-Thought (CoT) prompting and compare the performance of specialized versus general-purpose models, showing that significant improvements can be achieved through effective use of existing models.</abstract>
      <url hash="c1edb173">2025.bionlp-share.2</url>
      <bibkey>ghinea-rincu-2025-unibuc</bibkey>
    </paper>
    <paper id="3">
      <title>Loyola at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Exploring Unsupervised Attribution of Generated Text: Attention and Clustering-Based Methods</title>
      <author><first>Rohan</first><last>Sethi</last><affiliation>Loyola University Chicago and Stritch School of Medicine</affiliation></author>
      <author><first>Timothy</first><last>Miller</last><affiliation>Boston Children’s Hospital, Harvard Medical School</affiliation></author>
      <author><first>Majid</first><last>Afshar</last><affiliation>University of Wisconsin-Madison</affiliation></author>
      <author><first>Dmitriy</first><last>Dligach</last><affiliation>Loyola University Chicago</affiliation></author>
      <pages>22-26</pages>
      <abstract>The increasing volume of patient messages via electronic health record (EHR) portals has contributed significantly to clinician workload. Automating responses to these messages can help alleviate this burden, but it is essential to ensure that the generated responses are grounded in accurate clinical evidence. As part of the ArchEHR-QA 2025 BioNLP ACL shared task, we explore unsupervised methods for generating patient question responses that are both contextually accurate and evidence-backed. We investigate three novel approaches: zero-shot prompting, clustering-based evidence selection, and attention-based evidence attribution, along with a hybrid model that combines clustering and attention. Our methods do not require model fine-tuning and leverage the inherent structure of the input data to identify the most relevant supporting evidence from clinical notes. Our best-performing approach, which integrates clustering and attention, demonstrates a substantial improvement in factuality over baseline zero-shot methods, highlighting the potential of unsupervised strategies for enhancing the clinical utility of large language models in EHR contexts.</abstract>
      <url hash="f6312d01">2025.bionlp-share.3</url>
      <bibkey>sethi-etal-2025-loyola</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>CUNI</fixed-case>-a at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Do we need Giant <fixed-case>LLM</fixed-case>s for Clinical <fixed-case>QA</fixed-case>?</title>
      <author><first>Vojtech</first><last>Lanz</last><affiliation>Charles University</affiliation></author>
      <author><first>Pavel</first><last>Pecina</last><affiliation>Charles University</affiliation></author>
      <pages>27-40</pages>
      <abstract>In this paper, we present our submission to the ArchEHR-QA 2025 shared task, which focuses on answering patient questions based on excerpts from electronic health record (EHR) discharge summaries. Our approach identifies essential sentences relevant to a patient’s question using a combination of few-shot inference with the Med42-8B model, cosine similarity over clinical term embeddings, and the MedCPT cross-encoder relevance model. Then, concise answers are generated on the basis of these selected sentences. Despite not relying on large language models (LLMs) with tens of billions of parameters, our method achieves competitive results, demonstrating the potential of resource-efficient solutions for clinical NLP applications.</abstract>
      <url hash="6810e73e">2025.bionlp-share.4</url>
      <bibkey>lanz-pecina-2025-cuni</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>W</fixed-case>is<fixed-case>P</fixed-case>er<fixed-case>M</fixed-case>ed at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Modular, Relevance-First Approach for Grounded Question Answering on Eletronic Health Records</title>
      <author><first>Jan-Henning</first><last>Büns</last><affiliation>University of Applied Sciences and Arts Dortmund</affiliation></author>
      <author><first>Hendrik</first><last>Damm</last><affiliation>Department of Computer Science, University of Applied Sciences and Arts Dortmund, Institute for Medical Informatics, Biometry and Epidemiology, University Hospital Essen</affiliation></author>
      <author><first>Tabea</first><last>Pakull</last><affiliation>Institute for Transfusion Medicine, University Hospital Essen</affiliation></author>
      <author><first>Felix</first><last>Nensa</last><affiliation>Institute of Diagnostic and Interventional Radiology and Neuroradiology, University Hospital Essen, Institute for Artificial Intelligence in Medicine (IKIM), University Hospital Essen</affiliation></author>
      <author><first>Elisabeth</first><last>Livingstone</last><affiliation>Institute for Transfusion Medicine, University Hospital Essen</affiliation></author>
      <pages>41-49</pages>
      <abstract>Automatically answering patient questions based on electronic health records (EHRs) requires systems that both identify relevant evidence and generate accurate, grounded responses. We present a three-part pipeline developed by WisPerMed for the ArchEHR-QA 2025 shared task. First, a fine-tuned BioClinicalBERT model classifies note sentences by their relevance using synonym-based and paraphrased data augmentation. Second, a constrained generation step uses DistilBART-MedSummary to produce faithful answers strictly limited to top-ranked evidence. Third, we align each answer sentence to its supporting evidence via BiomedBERT embeddings and ROUGE-based similarity scoring to ensure citation transparency. Our system achieved a 35.0% overall score on the hidden test set, outperforming the organizer’s baseline by 4.3 percentage points. Gains in BERTScore (+44%) and SARI (+119%) highlight substantial improvements in semantic accuracy and relevance. This modular approach demonstrates that enforcing evidence-awareness and citation grounding enhances both answer quality and trustworthiness in clinical QA systems.</abstract>
      <url hash="269aa872">2025.bionlp-share.5</url>
      <bibkey>buns-etal-2025-wispermed</bibkey>
    </paper>
    <paper id="6">
      <title>hei<fixed-case>DS</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation</title>
      <author><first>Ashish</first><last>Chouhan</last><affiliation>Heidelberg University</affiliation></author>
      <author><first>Michael</first><last>Gertz</last><affiliation>Heidelberg University</affiliation></author>
      <pages>50-61</pages>
      <abstract>This paper presents the approach of our team called heiDS for the ArchEHR-QA 2025 shared task. A pipeline using a retrieval augmented generation (RAG) framework is designed to generate answers that are attributed to clinical evidence from the electronic health records (EHRs) of patients in response to patient-specific questions. We explored various components of a RAG framework, focusing on ranked list truncation (RLT) retrieval strategies and attribution approaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a query-dependent-k retrieval strategy, including the existing surprise and autocut methods and two new methods proposed in this work, autocut* and elbow. The experimental results show the benefits of our strategy in producing factual and relevant answers when compared to a fixed-k.</abstract>
      <url hash="d13c1197">2025.bionlp-share.6</url>
      <bibkey>chouhan-gertz-2025-heids</bibkey>
    </paper>
    <paper id="7">
      <title><fixed-case>U</fixed-case>ni<fixed-case>B</fixed-case>uc-<fixed-case>SB</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Resource-Constrained Pipeline for Relevance Classification and Grounded Answer Synthesis</title>
      <author><first>Sebastian</first><last>Balmus</last><affiliation>National Institute of Research and Development in Informatics - ICI Bucharest</affiliation></author>
      <author><first>Dura</first><last>Bogdan</last><affiliation>National Institute of Research and Development in Informatics - ICI Bucharest</affiliation></author>
      <author><first>Ana Sabina</first><last>Uban</last><affiliation>University of Bucharest</affiliation></author>
      <pages>62-68</pages>
      <abstract>We describe the UniBuc-SB submission to the ArchEHR-QA shared task, which involved generating grounded answers to patient questions based on electronic health records. Our system exceeded the performance of the provided baseline, achieving higher performance in generating contextually relevant responses. Notably, we developed our approach under constrained computational resources, utilizing only a single NVIDIA RTX 4090 GPU. We refrained from incorporating any external datasets, relying solely on the limited training data supplied by the organizers. To address the challenges posed by the low-resource setting, we leveraged off-the-shelf pre-trained language models and fine-tuned them minimally, aiming to maximize performance while minimizing overfitting.</abstract>
      <url hash="ae73bdd9">2025.bionlp-share.7</url>
      <bibkey>balmus-etal-2025-unibuc</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>KR</fixed-case> Labs at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Verbatim Approach for Evidence-Based Question Answering</title>
      <author><first>Adam</first><last>Kovacs</last><affiliation>KR Labs</affiliation></author>
      <author><first>Paul</first><last>Schmitt</last><affiliation>TU Vienna</affiliation></author>
      <author><first>Gabor</first><last>Recski</last><affiliation>TU Wien</affiliation></author>
      <pages>69-74</pages>
      <abstract>We present a lightweight, domain‐agnostic verbatim pipeline for evidence‐grounded question answering. Our pipeline operates in two steps: first, a sentence-level extractor flags relevant note sentences using either zero-shot LLM prompts or supervised ModernBERT classifiers. Next, an LLM drafts a question-specific template, which is filled verbatim with sentences from the extraction step. This prevents hallucinations and ensures traceability. In the ArchEHR‐QA 2025 shared task, our system scored 42.01%, ranking top‐10 in core metrics and outperforming the organiser’s 70B‐parameter Llama‐3.3 baseline. We publicly release our code and inference scripts under an MIT license.</abstract>
      <url hash="8826551b">2025.bionlp-share.8</url>
      <bibkey>kovacs-etal-2025-kr</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>LAIL</fixed-case>ab at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Test-time scaling for evidence selection in grounded question answering from electronic health records</title>
      <author><first>Tuan Dung</first><last>Le</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Thanh</first><last>Duong</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <author><first>Shohreh</first><last>Haddadan</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <author><first>Behzad</first><last>Jazayeri</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <author><first>Brandon</first><last>Manley</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <author><first>Thanh</first><last>Thieu</last><affiliation>Moffitt Cancer Center</affiliation></author>
      <pages>75-80</pages>
      <abstract>This paper presents our approach to the ArchEHR shared task on generating answers to real-world patient questions grounded in evidence from electronic health records (EHRs). We investigate the zero-shot capabilities of general-purpose, domain-agnostic large language models (LLMs) in two key aspects: identifying essential supporting evidence and producing concise, coherent answers. To this aim, we propose a two-stage pipeline: (1) evidence identification via test-time scaling (TTS) and (2) generating the final answer conditioned on selected evidences from the previous stage.Our approach leverages high-temperature sampling to generate multiple outputs during the evidence selection phase. This TTS-based approach effectively explore more potential evidences which results in significant improvement of the factuality score of the answers.</abstract>
      <url hash="6996873e">2025.bionlp-share.9</url>
      <bibkey>le-etal-2025-lailab</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>UTSA</fixed-case>-<fixed-case>NLP</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Improving <fixed-case>EHR</fixed-case> Question Answering via Self-Consistency Prompting</title>
      <author><first>Sara</first><last>Shields-Menard</last><affiliation>UTSA</affiliation></author>
      <author><first>Zach</first><last>Reimers</last><affiliation>UTSA</affiliation></author>
      <author><first>Joshua</first><last>Gardner</last><affiliation>UTSA</affiliation></author>
      <author><first>David</first><last>Perry</last><affiliation>UTSA</affiliation></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>81-90</pages>
      <abstract>We describe our system for the ArchEHR-QA Shared Task on answering clinical questions using electronic health records (EHRs). Our approach uses large language models in two steps: first, to find sentences in the EHR relevant to a clinician’s question, and second, to generate a short, citation-supported response based on those sentences. We use few-shot prompting, self-consistency, and thresholding to improve the sentence classification step to decide which sentences are essential. We compare several models and find that a smaller 8B model performs better than a larger 70B model for identifying relevant information. Our results show that accurate sentence selection is critical for generating high-quality responses and that self-consistency with thresholding helps make these decisions more reliable.</abstract>
      <url hash="02f79433">2025.bionlp-share.10</url>
      <bibkey>shields-menard-etal-2025-utsa</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>UTS</fixed-case>amuel at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Clinical Question Answering System for Responding to Patient Portal Messages Using Generative <fixed-case>AI</fixed-case></title>
      <author><first>Samuel</first><last>Reason</last><affiliation>UTHealth</affiliation></author>
      <author><first>Liwei</first><last>Wang</last><affiliation>UTHealth</affiliation></author>
      <author><first>Hongfang</first><last>Liu</last><affiliation>UTHealth</affiliation></author>
      <author><first>Ming</first><last>Huang</last><affiliation>UTHealth</affiliation></author>
      <pages>91-95</pages>
      <abstract>Responding to patient portal messages places a substantial burden on clinicians. To mitigate this, automatically generating answers to patient questions by considering their medical records is a critical solution. In this study, we proposed a clinical question answering system for the BioNLP 2025 Shared Task on Grounded Electronic Health Record Question Answering. The system processed each patient message case by selecting relevant sentences as evidences from the associated clinical notes and generating a concise, medically accurate answer to the patient’s question. A generative AI model from OpenAI (GPT-4o) was leveraged to assist with sentence selection and answer generation. Each response is grounded in source text, limited to 75 words, and includes sentence-level citations. The system was evaluated on 100 test cases using alignment, citation, and summarization metrics. Our results indicate the significant potential of the clinical question answering system based on generative AI models to streamline communication between patients and healthcare providers by automatically generating responses to patient messages.</abstract>
      <url hash="92e43748">2025.bionlp-share.11</url>
      <bibkey>reason-etal-2025-utsamuel</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>LAMAR</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Clinically Aligned <fixed-case>LLM</fixed-case>-Generated Few-Shot Learning for <fixed-case>EHR</fixed-case>-Grounded Patient Question Answering</title>
      <author><first>Seksan</first><last>Yoadsanit</last><affiliation>Faculty of Medicine Ramathibodi Hospital, Mahidol University</affiliation></author>
      <author><first>Nopporn</first><last>Lekuthai</last><affiliation>Faculty of Medicine Ramathibodi Hospital</affiliation></author>
      <author><first>Watcharitpol</first><last>Sermsrisuwan</last><affiliation>Faculty of Medicine, Ramathibodi Hospital, Mahidol University</affiliation></author>
      <author><first>Titipat</first><last>Achakulvisut</last><affiliation>Department of Biomedical Engineering, Mahidol University</affiliation></author>
      <pages>96-103</pages>
      <abstract>This paper presents an approach to answering patient-specific medical questions using electronic health record (EHR) grounding with ArchEHR-QA 2025 datasets. We address medical question answering as an alignment problem, focusing on generating responses factually consistent with patient-specific clinical notes through in-context learning techniques. We show that LLM-generated responses, used as few-shot examples with GPT-4.1 and Gemini-2.5-Pro, significantly outperform baseline approaches (overall score = 49.1), achieving strict precision, recall, and F1-micro scores of 60.6, 53.6, and 56.9, respectively, on the ArchEHR-QA 2025 test leaderboard. It achieves textual similarity between answers and essential evidence using BLEU, ROUGE, SARI, BERTScore, AlignScore, and MEDCON scores of 6.0, 32.1, 65.8, 36.4, 64.3, and 43.6, respectively. Our findings highlight the effectiveness of combining EHR grounding with few-shot examples for personalized medical question answering, establishing a promising approach for developing accurate and personalized medical question answering systems. We release our code at https://github.com/biodatlab/archehr-qa-lamar.</abstract>
      <url hash="8d78bd8b">2025.bionlp-share.12</url>
      <bibkey>yoadsanit-etal-2025-lamar</bibkey>
    </paper>
    <paper id="13">
      <title>Neural at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering</title>
      <author><first>Sai Prasanna Teja Reddy</first><last>Bogireddy</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Abrar</first><last>Majeedi</last><affiliation>UW Madison</affiliation></author>
      <author><first>Viswanath</first><last>Gajjala</last><affiliation>UW Madison</affiliation></author>
      <author><first>Zhuoyan</first><last>Xu</last><affiliation>UW Madison</affiliation></author>
      <author><first>Siddhant</first><last>Rai</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Vaishnav</first><last>Potlapalli</last><affiliation>NYU</affiliation></author>
      <pages>104-109</pages>
      <abstract>Automated question answering (QA) over electronic health records (EHRs) can bridge critical information gaps for clinicians and patients, yet it demands both precise evidence retrieval and faithful answer generation under limited supervision. In this work, we present Neural, the runner-up in the BioNLP 2025 ArchEHR-QA shared task on evidence grounded clinical QA. Our proposed method decouples the task into (1) sentence-level evidence identification and (2) answer synthesis with explicit citations. For each stage, we automatically explore the prompt space with DSPy’s MIPROv2 optimizer, jointly tuning instructions and few-shot demonstrations on the development set. A self-consistency voting scheme further improves evidence recall without sacrificing precision. On the hidden test set, our method attains an overall score of 51.5, placing second stage while outperforming standard zero-shot and few-shot prompting by over 20 and 10 points, respectively. These results indicate that data-driven prompt optimization is a cost-effective alternative to model fine-tuning for high-stakes clinical QA, advancing the reliability of AI assistants in healthcare.</abstract>
      <url hash="275a572f">2025.bionlp-share.13</url>
      <bibkey>bogireddy-etal-2025-neural</bibkey>
    </paper>
    <paper id="14">
      <title><fixed-case>UIC</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Tri-Step Pipeline for Reliable Grounded Medical Question Answering</title>
      <author><first>Mohammad</first><last>Arvan</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Anuj</first><last>Gautam</last><affiliation>University of Illinois</affiliation></author>
      <author><first>Mohan</first><last>Zalake</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Karl M.</first><last>Kochendorfer</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <pages>110-117</pages>
      <abstract>Automated response generation from electronic health records (EHRs) holds potential to reduce clinician workload, but it introduces important challenges related to factual accuracy and reliable grounding in clinical evidence. We present a structured three-step pipeline that uses large language models (LLMs) for evidence classification, guided response generation, and iterative quality control. To enable rigorous evaluation, our framework combines traditional reference-based metrics with a claim-level “LLM-as-a-Judge” methodology. On the ArchEHR-QA benchmark, our system achieves 82.0 percent claim-level evidence faithfulness and 51.6 percent citation-level factuality, demonstrating strong performance in generating clinically grounded responses. These findings highlight the utility of structured LLM pipelines in healthcare applications, while also underscoring the importance of transparent evaluation and continued refinement. All code, prompt templates, and evaluation tools are publicly available.</abstract>
      <url hash="b56c2ae2">2025.bionlp-share.14</url>
      <bibkey>arvan-etal-2025-uic</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>DMIS</fixed-case> Lab at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Evidence-Grounded Answer Generation for <fixed-case>EHR</fixed-case>-based <fixed-case>QA</fixed-case> via a Multi-Agent Framework</title>
      <author><first>Hyeon</first><last>Hwang</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeongsoon</first><last>Hwang</last><affiliation>Korea University</affiliation></author>
      <author><first>Jongmyung</first><last>Jung</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaehoon</first><last>Yun</last><affiliation>Hanyang University, College of Medicine</affiliation></author>
      <author><first>Minju</first><last>Song</last><affiliation>Korea University</affiliation></author>
      <author><first>Yein</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Dain</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Taewhoo</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Jiwoong</first><last>Sohn</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanwoong</first><last>Yoon</last><affiliation>Korea University</affiliation></author>
      <author><first>Sihyeon</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Jiwoo</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Heechul</first><last>Yang</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>118-125</pages>
      <abstract>The increasing utilization of patient portals has amplified clinicians’ workloads, primarily due to the necessity of addressing detailed patient inquiries related to their health concerns. The ArchEHR-QA 2025 shared task aims to alleviate this burden by automatically generating accurate, evidence-grounded responses to patients’ questions based on their Electronic Health Records (EHRs). This paper presents a six-stage multi-agent framework specifically developed to identify essential clinical sentences for answering patient questions, leveraging large language models (LLMs). Our approach begins with OpenAI’s o3 model generating focused medical context to guide downstream reasoning. In the subsequent stages, GPT-4.1-based agents assess the relevance of individual sentences, recruit domain experts, and consolidate their judgments to identify essential information for constructing coherent, evidence-grounded responses. Our framework achieved an Overall Factuality score of 62.0 and an Overall Relevance Score of 52.9 on the development set, and corresponding scores of 58.6 and 48.8, respectively, on the test set.</abstract>
      <url hash="7bd35afc">2025.bionlp-share.15</url>
      <bibkey>hwang-etal-2025-dmis</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>C</fixed-case>og<fixed-case>S</fixed-case>tack-<fixed-case>KCL</fixed-case>-<fixed-case>UCL</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Investigating Hybrid <fixed-case>LLM</fixed-case> Approaches for Grounded Clinical Question Answering</title>
      <author><first>Shubham</first><last>Agarwal</last><affiliation>King’s College London</affiliation></author>
      <author><first>Thomas</first><last>Searle</last><affiliation>King’s College London</affiliation></author>
      <author><first>Kawsar</first><last>Noor</last><affiliation>University College London</affiliation></author>
      <author><first>Richard</first><last>Dobson</last><affiliation>King’s College London</affiliation></author>
      <pages>126-135</pages>
      <abstract>We present our system for the ArchEHR shared task, which focuses on answering clinical and patient-facing questions grounded in real-world EHR data. Our core contribution is a 2-Stage prompting pipeline that separates evidence selection from answer generation while employing in-context learning strategies. Our experimentation leveraged the open-weight Gemma-v3 family of models, with our best submission using the Gemma-12B model securing 5th place overall on the unseen test set. Through systematic experimentation, we demonstrate the effectiveness of task decomposition in improving both factual accuracy and answer relevance in grounded clinical question answering.</abstract>
      <url hash="d6530e30">2025.bionlp-share.16</url>
      <bibkey>agarwal-etal-2025-cogstack</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>S</fixed-case>zeged<fixed-case>AI</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Combining <fixed-case>LLM</fixed-case>s with traditional methods for grounded question answering</title>
      <author><first>Soma</first><last>Nagy</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Bálint</first><last>Nyerges</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Zsombor</first><last>Kispéter</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Gábor</first><last>Tóth</last><affiliation>University of Szeged</affiliation></author>
      <author><first>András</first><last>Szlúka</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Gábor</first><last>Kőrösi</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Zsolt</first><last>Szántó</last><affiliation>University of Szeged</affiliation></author>
      <author><first>Richárd</first><last>Farkas</last><affiliation>University of Szeged</affiliation></author>
      <pages>136-149</pages>
      <abstract>In this paper, we present the SzegedAI team’s submissions to the ArchEHR-QA 2025 shared task. Our approaches include multiple prompting techniques for large language models (LLMs), sentence similarity methods, and traditional feature engineering. We are aiming to explore both modern and traditional solutions to the task. To combine the strengths of these diverse methods, we employed different ensembling strategies.</abstract>
      <url hash="a8298331">2025.bionlp-share.17</url>
      <bibkey>nagy-etal-2025-szegedai</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>LIMICS</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Prompting <fixed-case>LLM</fixed-case>s Beats Fine-Tuned Embeddings</title>
      <author><first>Adam</first><last>Remaki</last><affiliation>Sorbonne Univserity</affiliation></author>
      <author><first>Armand</first><last>Violle</last><affiliation>Sorbonne Université, Inserm, Université Sorbonne Paris-Nord, Laboratoire d’Informatique {{Médicale et d’Ingénierie des Connaissances en e-Santé, Paris, France</affiliation></author>
      <author><first>Vikram</first><last>Natraj</last><affiliation>Sorbonne Université, Inserm, Université Sorbonne Paris-Nord, Laboratoire d’Informatique Médicale et d’Ingénierie des Connaissances en e-Santé, Paris, France</affiliation></author>
      <author><first>Étienne</first><last>Guével</last><affiliation>Sorbonne Cluster for Artificial Intelligence, Paris, France</affiliation></author>
      <author><first>Akram</first><last>Redjdal</last><affiliation>Sorbonne Université, Inserm, Université Sorbonne Paris-Nord, Laboratoire d’Informatique Médicale et d’Ingénierie des Connaissances en e-Santé, Paris. FranceUniv Gustave Eiffel, Aix-Marseille Univ, LBA, F-13016 Marseille, France</affiliation></author>
      <pages>150-159</pages>
      <abstract>In this paper, we investigated two approaches to clinical question-answering based on patient-formulated questions, supported by their narratives and brief medical records. The first approach leverages zero- and few-shot prompt engineering techniques with GPT-based Large Language Models (LLMs), incorporating strategies such as prompt chaining and chain-of-thought reasoning to guide the models in generating answers. The second approach adopts a two-steps structure: first, a text-classification stage uses embedding-based models (e.g., BERT variants) to identify sentences within the medical record that are most relevant to the given question; then, we prompt an LLM to paraphrase them into an answer so that it is generated exclusively from these selected sentences. Our empirical results demonstrate that the first approach outperforms the classification-guided pipeline, achieving the highest score on the development set and the test set using prompt chaining. Code: github.com/armandviolle/BioNLP-2025</abstract>
      <url hash="c4f86a5a">2025.bionlp-share.18</url>
      <bibkey>remaki-etal-2025-limics</bibkey>
    </paper>
    <paper id="19">
      <title>razreshili at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: Contrastive Fine-Tuning for Retrieval-Augmented Biomedical <fixed-case>QA</fixed-case></title>
      <author><first>Arina</first><last>Zemchyk</last><affiliation>-</affiliation></author>
      <pages>160-164</pages>
      <abstract>We present a retrieval-augmented system for the ArchEHR-QA 2025 shared task, which focuses on generating concise, medically accurate answers to clinical questions based on a patient’s electronic health record (EHR). A key challenge is following a strict cita- tion format that references relevant sentence IDs. To improve retrieval, we fine-tuned an all-MiniLM-L6-v2 embedding model using contrastive learning on over 2,300 question–sentence triplets, with DoRA for efficient adaptation. Sentences were selected using cosine similarity thresholds and passed into a quantized Mistral-7B-Instruct model along with a structured prompt. Our system achieved similar relevance to the baseline but lower overall performance (19.3 vs. 30.7), due to issues with citation formatting and generation quality. We discuss limitations such as threshold tuning, prompt-following ability, and model size, and suggest future directions for improving structured biomedical QA.</abstract>
      <url hash="20da774a">2025.bionlp-share.19</url>
      <bibkey>zemchyk-2025-razreshili</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>DKITNLP</fixed-case> at <fixed-case>A</fixed-case>rch<fixed-case>EHR</fixed-case>-<fixed-case>QA</fixed-case> 2025: A Retrieval Augmented <fixed-case>LLM</fixed-case> Pipeline for Evidence-Based Patient Question Answering</title>
      <author><first>Provia</first><last>Kadusabe</last><affiliation>Dundalk Institute of Technology</affiliation></author>
      <author><first>Abhishek</first><last>Kaushik</last><affiliation>Dundalk Institute of Technology</affiliation></author>
      <author><first>Fiona</first><last>Lawless</last><affiliation>Dundalk Institute of Technology</affiliation></author>
      <pages>165-170</pages>
      <abstract>This paper describes our submission for the BioNLP ACL 2025 Shared task on grounded Question Answering (QA) from Electronic Health Records (EHRs). The task aims to automatically generate answers to patients’ health related questions that are grounded in the evidence from their clinical notes. We propose a two stage retrieval pipeline to identify relevant sentences to guide response generation by a Large Language Model (LLM). Specifically, our approach uses a BioBERT based bi-encoder for initial retrieval, followed by a re-ranking step using a fine-tuned cross-encoder to enhance retrieval precision. The final set of selected sentences serve as an input to Mistral 7B model which generates answers through few-shot prompting. Our approach achieves an overall score of 31.6 on the test set, outperforming a substantially larger baseline model LLaMA 3.3 70B (30.7), which demonstrates the effectiveness of retrieval-augmented generation for grounded QA.</abstract>
      <url hash="734aacdd">2025.bionlp-share.20</url>
      <bibkey>kadusabe-etal-2025-dkitnlp</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>AEHRC</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm 2025: Leveraging T5 for Lay Summarisation of Radiology Reports</title>
      <author><first>Wenjun</first><last>Zhang</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Shekhar</first><last>Chandra</last><affiliation>University of Queensland</affiliation></author>
      <author><first>Bevan</first><last>Koopman</last><affiliation>CSIRO</affiliation></author>
      <author><first>Jason</first><last>Dowling</last><affiliation>Australian e-Health Research Centre, CSIRO Health and Biosecurity</affiliation></author>
      <author><first>Aaron</first><last>Nicolson</last><affiliation>Australian e-Health Research Centre, CSIRO Health and Biosecurity</affiliation></author>
      <pages>171-178</pages>
      <abstract>Biomedical texts, such as research articles and clinical reports, are often written in highly technical language, making them difficult for patients and the general public to understand. The BioLaySumm 2025 Shared Task addresses this challenge by promoting the development of models that generate lay summarisation of biomedical content. This paper focuses on Subtask 2.1: Radiology Report Generation with Layman’s Terms. In this work, we evaluate two large language model (LLM) architectures, T5-large (700M parameter encoder–decoder model) and LLaMA-3.2-3B (3B parameter decoder-only model). Both models are trained under fully-supervised conditions using the task’s multi-source dataset. Our results show that T5-large consistently outperforms LLaMA-3.2-3B across nine out of ten metrics, including relevance, readability, and clinical accuracy, despite having only a quarter of the parameters. Our T5-based model achieved the top rank in both the open-source and close-source tracks of the subtask 2.1.</abstract>
      <url hash="796c5e06">2025.bionlp-share.21</url>
      <bibkey>zhang-etal-2025-aehrc</bibkey>
    </paper>
    <paper id="22">
      <title><fixed-case>M</fixed-case>etnin<fixed-case>O</fixed-case>z<fixed-case>U</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Text Summarization with Reverse Data Augmentation and Injecting Salient Sentences</title>
      <author><first>Egecan</first><last>Evgin</last><affiliation>MSc AI Student, Ozyegin University</affiliation></author>
      <author><first>Ilknur</first><last>Karadeniz</last><affiliation>Ozyegin University</affiliation></author>
      <author><first>Olcay Taner</first><last>Yıldız</last><affiliation>Department of Computer Engineering, Ozyegin University</affiliation></author>
      <pages>179-184</pages>
      <abstract>In this paper, we present our approach to the BioLaySumm 2025 Shared Task on lay summarization of biomedical research articles, which was conducted as part of the BioNLP Workshop 2025. The aim of the task is to create lay summaries from scientific articles to improve accessibility for a non-expert audience. To this end, we applied preprocessing techniques to clean and standardize the input texts, and fine-tuned Qwen2.5 and Qwen3-based language models for the summarization task. For abstract-based fine-tuning, we investigated whether we can insert salient sentences from the main article into the summary to enrich the input. We also curated a dataset of child-friendly articles with corresponding gold-standard summaries and used large language models to rewrite them into more complex scientific variants to augment our training data with more examples.</abstract>
      <url hash="25ef6a14">2025.bionlp-share.22</url>
      <bibkey>evgin-etal-2025-metninozu</bibkey>
    </paper>
    <paper id="23">
      <title>Shared Task at Biolaysumm2025 : Extract then summarize approach Augmented with <fixed-case>UMLS</fixed-case> based Definition Retrieval for Lay Summary generation.</title>
      <author><first>Aaradhya</first><last>Gupta</last><affiliation>International Institute of Information Technology, Hyderabad</affiliation></author>
      <author><first>Parameswari</first><last>Krishnamurthy</last><affiliation>Assistant Professor, IIIT Hyderabad</affiliation></author>
      <pages>185-189</pages>
      <abstract>The paper presents a modular, two‐track lay‐summary generation system for biomedical research articles, evaluated on the PLOS and eLife subsets of the BioLaySumm2025 shared task. In Task 1, it extracts salient sentences via an LLM–based chunking and summarization pipeline, then applies iterative rewriting to produce an accessible summary. In Task 2, it augments that summary with UMLS‐sourced definitions identified by a BioBERT NER model, yielding improved readability and factual consistency, at the cost of slight reductions in n‐gram overlap metrics like ROUGE and BLEU.</abstract>
      <url hash="45da512a">2025.bionlp-share.23</url>
      <bibkey>gupta-krishnamurthy-2025-shared</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>R</fixed-case>ain<fixed-case>C</fixed-case>ity<fixed-case>NLP</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Extract then Summarize at Home</title>
      <author><first>Jen</first><last>Wilson</last><affiliation>University of Washington</affiliation></author>
      <author><first>Michael</first><last>Pollack</last><affiliation>University of Washington</affiliation></author>
      <author><first>Rachel</first><last>Edwards</last><affiliation>University of Washington</affiliation></author>
      <author><first>Avery</first><last>Bellamy</last><affiliation>University of Washington</affiliation></author>
      <author><first>Helen</first><last>Salgi</last><affiliation>University of Washington</affiliation></author>
      <pages>190-195</pages>
      <abstract>As part of the BioLaySumm shared task at ACL 2025, we developed a summarization tool designed to translate complex biomedical texts into layperson-friendly summaries. Our goal was to enhance accessibility and comprehension for patients and others without specialized medical knowledge. The system employed an extractive-then-abstractive summarization pipeline. For the abstractive component, we experimented with two models: Pegasus-XSum and a Falcons.ai model pre-trained on medical data. Final outputs were evaluated using the official BioLaySumm 2025 metrics. To promote practical accessibility, we completed all experimentation on consumer-grade hardware, demonstrating the feasibility of our approach in low-resource settings.</abstract>
      <url hash="8c2236bf">2025.bionlp-share.24</url>
      <bibkey>wilson-etal-2025-raincitynlp</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>TLPIQ</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Hide and Seq, a <fixed-case>FLAN</fixed-case>-T5 Model for Biomedical Summarization</title>
      <author><first>Melody</first><last>Bechler</last><affiliation>University of Washington</affiliation></author>
      <author><first>Carly</first><last>Crowther</last><affiliation>UW</affiliation></author>
      <author><first>Emily</first><last>Luedke</last><affiliation>University of Washington</affiliation></author>
      <author><first>Natasha</first><last>Schimka</last><affiliation>University of Washington</affiliation></author>
      <author><first>Ibrahim</first><last>Sharaf</last><affiliation>University of Washington</affiliation></author>
      <pages>196-201</pages>
      <abstract>BioLaySumm 2025 is a shared task that aims to automatically generate lay summaries of scientific papers for a wider audience of readers without domain-specific knowledge, making scientific discoveries in the domain of biology and medicine more accessible to the general public. Our submission to the task is a FLAN-T5 base model fine-tuned on the abstract and conclusion of articles and expert-written lay summaries from the shared task’s provided datasets. We find that our system performs competitively in terms of relevance, exceeds the baseline on factuality, but falls short on readability.</abstract>
      <url hash="48cd678f">2025.bionlp-share.25</url>
      <bibkey>bechler-etal-2025-tlpiq</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm<fixed-case>X</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Retrieval-Augmented Fine-Tuning for Biomedical Lay Summarization Using Abstracts and Retrieved Full-Text Context</title>
      <author><first>Fan</first><last>Lin</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Dezhi</first><last>Yu</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>202-214</pages>
      <abstract>Generating lay summaries of biomedical research remains a time-intensive task, despite their importance in bridging the gap between scientific findings and non-expert audiences. This study introduces a retrieval-augmented fine-tuning framework for biomedical lay summarization, integrating abstract-driven semantic retrieval with LoRA-tuned LLaMA 3.1 models. Abstracts are used as queries to retrieve relevant text segments from full-text articles, which are then incorporated into prompts for supervised fine-tuning. Evaluations on the PLOS and eLife datasets show that this hybrid approach significantly improves relevance and factuality metrics compared to both base models and those tuned individually, while maintaining competitive readability. Prompt design experiments highlight a trade-off between readability and factual accuracy. Our fine-tuned model demonstrates strong performance in relevance and factuality among open-source systems and rivals closed-source models such as GPT, providing an efficient and effective solution for domain-specific lay summarization.</abstract>
      <url hash="00ddb96d">2025.bionlp-share.26</url>
      <bibkey>lin-yu-2025-laysummx</bibkey>
    </paper>
    <paper id="27">
      <title>5c<fixed-case>NLP</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Prompts, Retrieval, and Multimodal Fusion</title>
      <author><first>Juan Antonio</first><last>Lossio-Ventura</last><affiliation>National Institutes of Health</affiliation></author>
      <author><first>Callum</first><last>Chan</last><affiliation>University of Ottawa</affiliation></author>
      <author><first>Arshitha</first><last>Basavaraj</last><affiliation>International Institute of Information Technology, Bangalore, India</affiliation></author>
      <author><first>Hugo</first><last>Alatrista-Salas</last><affiliation>De Vinci Research Center, Paris, France</affiliation></author>
      <author><first>Francisco</first><last>Pereira</last><affiliation>National Institute of Mental Health</affiliation></author>
      <author><first>Diana</first><last>Inkpen</last><affiliation>University of Ottawa</affiliation></author>
      <pages>215-231</pages>
      <abstract>In this work, we present our approach to addressing all subtasks of the BioLaySumm 2025 shared task by leveraging prompting and retrieval strategies, as well as multimodal input fusion. Our method integrates: (1) zero-shot and few-shot prompting with large language models (LLMs); (2) semantic similarity-based dynamic few-shot prompting; (3) retrieval-augmented generation (RAG) incorporating biomedical knowledge from the Unified Medical Language System (UMLS); and (4) a multimodal fusion pipeline that combines images and captions using image-text-to-text generation for enriched lay summarization. Our framework enables lightweight adaptation of pretrained LLMs for generating lay summaries from scientific articles and radiology reports. Using modern LLMs, including Llama-3.3-70B-Instruct and GPT-4.1, our 5cNLP team achieved third place in Subtask 1.2 and second place in Subtask 2.1, among all submissions.</abstract>
      <url hash="a6c9f792">2025.bionlp-share.27</url>
      <bibkey>lossio-ventura-etal-2025-5cnlp</bibkey>
    </paper>
    <paper id="28">
      <title><fixed-case>MIRAGES</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: The Impact of Search Terms and Data Curation for Biomedical Lay Summarization</title>
      <author><first>Benjamin</first><last>Pong</last><affiliation>University of Washington</affiliation></author>
      <author><first>J u - H u i</first><last>Chen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jonathan</first><last>Jiang</last><affiliation>University of Washington</affiliation></author>
      <author><first>Abimael</first><last>Jimenez</last><affiliation>University of Washington</affiliation></author>
      <author><first>Melody</first><last>Vahadi</last><affiliation>University of Washington</affiliation></author>
      <pages>232-239</pages>
      <abstract>Biomedical articles are often inaccessible to non-experts due to their technical complexity. To improve readability and factuality of lay summaries, we built on an extract-then-summarize framework by experimenting with novel extractive summarization strategies and employing Low Rank Adaptation (LoRA) fine-tuning of Meta-Llama-3-8B-Instruct on data selected by these strategies. We also explored counterfactual data augmentation and post-processing definition insertion to further enhance factual grounding and accessibility. Our best performing system treats the article’s title and keywords (i.e. search terms) as a single semantic centroid and ranks sentences by their semantic similarity to this centroid. This constrained selection of data serves as input for fine-tuning, achieving marked improvements in readability and factuality of downstream abstractive summaries while maintaining relevance. Our approach highlights the importance of quality data curation for biomedicallay summarization, resulting in 4th best overall performance and 2nd best Readability performance for the BioLaySumm 2025 Shared Task at BioNLP 2025.</abstract>
      <url hash="4c4a3d2d">2025.bionlp-share.28</url>
      <bibkey>pong-etal-2025-mirages</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>SUWMIT</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Instruction-based Summarization with Contrastive Decoding</title>
      <author><first>Priyam</first><last>Basu</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jose</first><last>Cols</last><affiliation>University of Washington</affiliation></author>
      <author><first>Daniel</first><last>Jarvis</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yongsin</first><last>Park</last><affiliation>University of Washington</affiliation></author>
      <author><first>Daniel</first><last>Rodabaugh</last><affiliation>University of Washington</affiliation></author>
      <pages>240-248</pages>
      <abstract>In the following paper, we present our team’s approach to subtask 1.1 of the BioLaySumm 2025 shared task, which entails the automated generation of lay summaries from biomedical articles. To this end, we experiment with a variety of methods for text preprocessing, extractive summarization, model fine-tuning, and abstractive summarization. Our final results are generated on a fine-tuned Llama 3.1 Instruct (8B) model, notably achieving top scores on two out of four relevance metrics, as well as the highest overall ranking among this year’s participating teams on the plain lay summarization subtask.</abstract>
      <url hash="91a41f39">2025.bionlp-share.29</url>
      <bibkey>basu-etal-2025-suwmit</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>BDA</fixed-case>-<fixed-case>UC</fixed-case>3<fixed-case>M</fixed-case> @ <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Efficient Lay Summarization with Small-Scale <fixed-case>S</fixed-case>o<fixed-case>TA</fixed-case> <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ilyass</first><last>Ramzi</last><affiliation>Universidad Carlos III de Madrid</affiliation></author>
      <author><first>Isabel</first><last>Bedmar</last><affiliation>Computer Science and Engineering Department, Universidad Carlos III de Madrid</affiliation></author>
      <pages>249-255</pages>
      <abstract>This paper presents an efficient system for the BioLaySumm 2025 Shared Task on biomedical lay summarization. The approach leverages compact, state-of-the-art language models (4–7 billion parameters), including Gemma3 4B, Qwen3 4B, and GPT-4.1-mini, optimized for relevance, readability, and factuality. Through dynamic 4-bit quantization, parameter-efficient fine-tuning, advanced extractive preprocessing, and direct preference optimization, the system achieves performance competitive with much larger baselines. Comprehensive experiments on the eLife and PLOS datasets demonstrate that small language models can deliver high-quality, accessible biomedical summaries using modest computational resources. The findings suggest that resource-efficient models can help democratize access to scientific information, supporting broader scientific communication goals.</abstract>
      <url hash="f12fb63c">2025.bionlp-share.30</url>
      <bibkey>ramzi-bedmar-2025-bda</bibkey>
    </paper>
    <paper id="31">
      <title><fixed-case>KHU</fixed-case>_<fixed-case>LDI</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Fine-tuning and Refinement for Lay Radiology Report Generation</title>
      <author><first>Nur Alya Dania</first><last>Binti Moriazi</last><affiliation>Kyung Hee University</affiliation></author>
      <author><first>Mujeen</first><last>Sung</last><affiliation>Kyung Hee University</affiliation></author>
      <pages>256-268</pages>
      <abstract>Though access to one’s own radiology reports has improved over the years, the use of complex medical terms makes understanding these reports difficult. To tackle this issue, we explored two approaches: supervised fine-tuning open-source large language models using QLoRA, and refinement, which improves a given generated output using feedback generated by a feedback model. Despite the fine-tuned model outperforming refinement on the test data, refinement showed good results on the validation set, thus showing good potential in the generation of lay radiology reports. Our submission achieved 2nd place in the open track of Subtask 2.1 of the BioLaySumm 2025 shared task.</abstract>
      <url hash="c9610ff1">2025.bionlp-share.31</url>
      <bibkey>binti-moriazi-sung-2025-khu</bibkey>
    </paper>
    <paper id="32">
      <title><fixed-case>CUTN</fixed-case>_<fixed-case>B</fixed-case>io at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm: Multi-Task Prompt Tuning with External Knowledge and Readability adaptation for Layman Summarization</title>
      <author><first>Bhuvaneswari</first><last>Sivagnanam</last><affiliation>Central University of Tamil Nadu</affiliation></author>
      <author><first>Rivo Krishnu</first><last>C H</last><affiliation>Central University of Tamil Nadu</affiliation></author>
      <author><first>Princi</first><last>Chauhan</last><affiliation>Central University of Tamil Nadu</affiliation></author>
      <author><first>Saranya</first><last>Rajiakodi</last><affiliation>Central University of Tamil Nadu</affiliation></author>
      <pages>269-274</pages>
      <abstract>In this study, we presented a prompt based layman summarization framework for the biomedical articles and radiology reports developed as part of the BioLaySumm 2025 shared task at the BioNLP Workshop, ACL 2025. For Subtask 1.1 (Plain Lay Summarization), we utilized the abstract as input and employed Meta-LLaMA-3-8B-Instruct with a Tree-of-Thought prompting strategy and obtained 13th rank. In Subtask 1.2 (Lay Summarization with External Knowledge), we adopted an extractive plus prompt approach by combining LEAD-K sentence extraction with Meta-LLaMA-3-8B-Instruct. Medical concepts were identified using MedCAT, and their definitions were taken from Wikipedia to enrich the generated summaries. Our system secured the 2nd position in this subtask. For Subtask 2.1 (Radiology Report Translation), we implemented a Retrieval-Augmented Generation (RAG) approach using the Zephyr model to convert professional radiology reports into layman terms, achieved 3rd place in the shared task.</abstract>
      <url hash="c84ac665">2025.bionlp-share.32</url>
      <bibkey>sivagnanam-etal-2025-cutn</bibkey>
    </paper>
    <paper id="33">
      <title>Team <fixed-case>XSZ</fixed-case> at <fixed-case>B</fixed-case>io<fixed-case>L</fixed-case>ay<fixed-case>S</fixed-case>umm2025: Section-Wise Summarization, Retrieval-Augmented <fixed-case>LLM</fixed-case>, and Reinforcement Learning Fine-Tuning for Lay Summaries</title>
      <author><first>Pengcheng</first><last>Xu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Sicheng</first><last>Shen</last><affiliation>University of Michigan</affiliation></author>
      <author><first>Jieli</first><last>Zhou</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hongyi</first><last>Xin</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>275-280</pages>
      <abstract>We propose a unified, multi-stage lay summarization pipeline for BioLaySumm 2025 (Subtask 1.1) that (1) selects and summarizes key article sections via BioBART, (2) retrieves K-shot demonstrations using BGE embeddings for in-context Llama 3 8B prompting, (3) applies LoRA adapters to Llama 3 8B for supervised fine-tuning, (4) merges section summaries with a second BioBART pass, and (5) refines outputs through reinforcement learning (PPO &amp; GRPO) using a composite reward of factuality (AlignScore, SummaC), relevance (ROUGE-L, BERTScore), and readability (LENS, FKGL, DCRS, CLI). On PLOS and eLife validation sets, our complete systemreduces DCRS from 9.23 to 8.56 and reduces CLI from 12.98 to 12.65, ranking 3rd in readability. and outperforms llama3 finetune baseline in AlignScore 0.722 to 0.862, ranking 5th in factuality, demonstrating balanced gains across readability, relevance, and factuality.</abstract>
      <url hash="331a8305">2025.bionlp-share.33</url>
      <bibkey>xu-etal-2025-team</bibkey>
    </paper>
    <paper id="34">
      <title><fixed-case>V</fixed-case>e<fixed-case>R</fixed-case>ea<fixed-case>F</fixed-case>ine: Iterative Verification Reasoning Refinement <fixed-case>RAG</fixed-case> for Hallucination-Resistant on Open-Ended Clinical <fixed-case>QA</fixed-case></title>
      <author><first>Pakawat</first><last>Phasook</last><affiliation>King Mongkut’s University of Technology Thonburi</affiliation></author>
      <author><first>Rapepong</first><last>Pitijaroonpong</last><affiliation>PreceptorAI</affiliation></author>
      <author><first>Jiramet</first><last>Kinchagawat</last><affiliation>CARIVA (Thailand) Company Limited</affiliation></author>
      <author><first>Amrest</first><last>Chinkamol</last><affiliation>School of Information Science and Technology, VISTEC</affiliation></author>
      <author><first>Tossaporn</first><last>Saengja</last><affiliation>Vidyasirimedhi Institute of Science and Technology (VISTEC)</affiliation></author>
      <author><first>Kiartnarin</first><last>Udomlapsakul</last><affiliation>Cariva (Thailand)</affiliation></author>
      <author><first>Jitkapat</first><last>Sawatphol</last><affiliation>CARIVA (Thailand) Co, Ltd.</affiliation></author>
      <author><first>Piyalitt</first><last>Ittichaiwong</last><affiliation>Faculty of Medicine Siriraj Hospital, Mahidol University</affiliation></author>
      <pages>281-288</pages>
      <abstract>We present VeReaFine, a novel “Verifier-RAG” pipeline designed to eliminate hallucinations in open-ended clinical question answering. VeReaFine interleaves three tightly coupled stages—retrieval, verification, and generation—across up to three iterations. First, a two-stage dense retriever (BM-Retriever-410M → BM-Reranker-2B) fetches and ranks top-k biomedical passages; an 8B-parameter MedReason verifier then filters these for direct relevance and identifies missing evidence. When the verifier deems the context insufficient, it formulates a focused “feedback query” to retrieve additional passages (bounded to prevent infinite loops). Once a minimal ground-truth context is assembled, a 7B-parameter generator (Qwen2.5-7B-Instruct) drafts an answer purely from that vetted context, and the verifier performs a final check—prompting the generator to refine any remaining unsupported claims. By iteratively fetching only missing facts and ensuring every assertion is evidence-backed, VeReaFine achieves monotonic factuality improvements with minimal overhead. On the BioNLP 2025 ClinIQLink “LLM Lie-Detector” shared task, our 7B generator augmented with VeReaFine matches or surpasses a 32B medical model on open-ended reasoning metrics, reducing multi-hop inverse step-identification errors by 26%. These findings demonstrate that moderate-size LLMs, when guided by targeted verification loops, can deliver expert-level reliability in clinical QA.</abstract>
      <url hash="041ae80b">2025.bionlp-share.34</url>
      <bibkey>phasook-etal-2025-vereafine</bibkey>
    </paper>
  </volume>
</collection>
