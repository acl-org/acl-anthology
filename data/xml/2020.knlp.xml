<?xml version='1.0' encoding='UTF-8'?>
<collection id="2020.knlp">
  <volume id="1" ingest-date="2020-12-02" type="proceedings">
    <meta>
      <booktitle>Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP</booktitle>
      <editor><first>Oren Sar</first><last>Shalom</last></editor>
      <editor><first>Alexander</first><last>Panchenko</last></editor>
      <editor><first>Cicero</first><last>dos Santos</last></editor>
      <editor><first>Varvara</first><last>Logacheva</last></editor>
      <editor><first>Alessandro</first><last>Moschitti</last></editor>
      <editor><first>Ido</first><last>Dagan</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Suzhou, China</address>
      <month>December</month>
      <year>2020</year>
      <venue>knlp</venue>
    </meta>
    <frontmatter>
      <url hash="61ff76fc">2020.knlp-1.0</url>
      <bibkey>knlp-2020-knowledgeable</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>COVID</fixed-case>-19 Knowledge Graph: Accelerating Information Retrieval and Discovery for Scientific Literature</title>
      <author><first>Colby</first><last>Wise</last></author>
      <author><first>Miguel</first><last>Romero Calvo</last></author>
      <author><first>Pariminder</first><last>Bhatia</last></author>
      <author><first>Vassilis</first><last>Ioannidis</last></author>
      <author><first>George</first><last>Karypus</last></author>
      <author><first>George</first><last>Price</last></author>
      <author><first>Xiang</first><last>Song</last></author>
      <author><first>Ryan</first><last>Brand</last></author>
      <author><first>Ninad</first><last>Kulkani</last></author>
      <pages>1–10</pages>
      <abstract>The coronavirus disease (COVID-19) has claimed the lives of over one million people and infected more than thirty-five million people worldwide. Several search engines have surfaced to provide researchers with additional tools to find and retrieve information from the rapidly growing corpora on COVID19. These engines lack extraction and visualization tools necessary to retrieve and interpret complex relations inherent to scientific literature. Moreover, because these engines mainly rely upon semantic information, their ability to capture complex global relationships across documents is limited, which reduces the quality of similarity-based article recommendations for users. In this work, we present the COVID-19 Knowledge Graph (CKG), a heterogeneous graph for extracting and visualizing complex relationships between COVID-19 scientific articles. The CKG combines semantic information with document topological information for the application of similar document retrieval. The CKG is constructed using the latent schema of the data, and then enriched with biomedical entity information extracted from the unstructured text of articles using scalable AWS technologies to form relations in the graph. Finally, we propose a document similarity engine that leverages low-dimensional graph embeddings from the CKG with semantic embeddings for similar article retrieval. Analysis demonstrates the quality of relationships in the CKG and shows that it can be used to uncover meaningful information in COVID-19 scientific articles. The CKG helps power www.cord19.aws and is publicly available.</abstract>
      <url hash="fdf17423">2020.knlp-1.1</url>
      <bibkey>wise-etal-2020-covid</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
    </paper>
    <paper id="2">
      <title>Dialogue over Context and Structured Knowledge using a Neural Network Model with External Memories</title>
      <author><first>Yuri</first><last>Murayama</last></author>
      <author><first>Lis</first><last>Kanashiro Pereira</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <pages>11–20</pages>
      <abstract>The Differentiable Neural Computer (DNC), a neural network model with an addressable external memory, can solve algorithmic and question answering tasks. There are various improved versions of DNC, such as rsDNC and DNC-DMS. However, how to integrate structured knowledge into these DNC models remains a challenging research question. We incorporate an architecture for knowledge into such DNC models, i.e. DNC, rsDNC and DNC-DMS, to improve the ability to generate correct responses using both contextual information and structured knowledge. Our improved rsDNC model improves the mean accuracy by approximately 20% to the original rsDNC on tasks requiring knowledge in the dialog bAbI tasks. In addition, our improved rsDNC and DNC-DMS models also yield better performance than their original models in the Movie Dialog dataset.</abstract>
      <url hash="5e6c0146">2020.knlp-1.2</url>
      <bibkey>murayama-etal-2020-dialogue</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/csqa">CSQA</pwcdataset>
    </paper>
    <paper id="3">
      <title>Social Media Medical Concept Normalization using <fixed-case>R</fixed-case>o<fixed-case>BERT</fixed-case>a in Ontology Enriched Text Similarity Framework</title>
      <author><first>Katikapalli Subramanyam</first><last>Kalyan</last></author>
      <author><first>Sivanesan</first><last>Sangeetha</last></author>
      <pages>21–26</pages>
      <abstract>Pattisapu et al. (2020) formulate medical concept normalization (MCN) as text similarity problem and propose a model based on RoBERTa and graph embedding based target concept vectors. However, graph embedding techniques ignore valuable information available in the clinical ontology like concept description and synonyms. In this work, we enhance the model of Pattisapu et al. (2020) with two novel changes. First, we use retrofitted target concept vectors instead of graph embedding based vectors. It is the first work to leverage both concept description and synonyms to represent concepts in the form of retrofitted target concept vectors in text similarity framework based social media MCN. Second, we generate both concept and concept mention vectors with same size which eliminates the need of dense layers to project concept mention vectors into the target concept embedding space. Our model outperforms existing methods with improvements up to 3.75% on two standard datasets. Further when trained only on mapping lexicon synonyms, our model outperforms existing methods with significant improvements up to 14.61%. We attribute these significant improvements to the two novel changes introduced.</abstract>
      <url hash="ff2a5c1b">2020.knlp-1.3</url>
      <bibkey>kalyan-sangeetha-2020-social</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>BERTC</fixed-case>hem-<fixed-case>DDI</fixed-case> : Improved Drug-Drug Interaction Prediction from text using Chemical Structure Information</title>
      <author><first>Ishani</first><last>Mondal</last></author>
      <pages>27–32</pages>
      <abstract>Traditional biomedical version of embeddings obtained from pre-trained language models have recently shown state-of-the-art results for relation extraction (RE) tasks in the medical domain. In this paper, we explore how to incorporate domain knowledge, available in the form of molecular structure of drugs, for predicting Drug-Drug Interaction from textual corpus. We propose a method, BERTChem-DDI, to efficiently combine drug embeddings obtained from the rich chemical structure of drugs (encoded in SMILES) along with off-the-shelf domain-specific BioBERT embedding-based RE architecture. Experiments conducted on the DDIExtraction 2013 corpus clearly indicate that this strategy improves other strong baselines architectures by 3.4% macro F1-score.</abstract>
      <url hash="873fd0c4">2020.knlp-1.4</url>
      <bibkey>mondal-2020-bertchem</bibkey>
    </paper>
  </volume>
</collection>
