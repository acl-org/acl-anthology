<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.findings">
  <volume id="eacl" ingest-date="2024-03-03" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: EACL 2024</booktitle>
      <editor><first>Yvette</first><last>Graham</last></editor>
      <editor><first>Matthew</first><last>Purver</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>St. Julian’s, Malta</address>
      <month>March</month>
      <year>2024</year>
      <url hash="f35e73f5">2024.findings-eacl</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="24d75a3c">2024.findings-eacl.0</url>
      <bibkey>findings-2024-findings</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Chem-<fixed-case>FINESE</fixed-case>: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction</title>
      <author><first>Qingyun</first><last>Wang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Hongxiang</first><last>Li</last><affiliation>UIUC</affiliation></author>
      <author><first>Xuan</first><last>Liu</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Huimin</first><last>Zhao</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>1-16</pages>
      <abstract>Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction process. Finally, we release ChemNER+, a new fine-grained chemical entity extraction dataset that is annotated by domain experts with the ChemNER schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets show that our newly proposed framework has contributed up to 8.26% and 6.84% absolute F1-score gains respectively.</abstract>
      <url hash="d115d641">2024.findings-eacl.1</url>
      <attachment type="software" hash="4203c223">2024.findings-eacl.1.software.zip</attachment>
      <attachment type="note" hash="c244f46e">2024.findings-eacl.1.note.zip</attachment>
      <bibkey>wang-etal-2024-chem</bibkey>
      <video href="2024.findings-eacl.1.mp4"/>
      <revision id="1" href="2024.findings-eacl.1v1" hash="b832457e"/>
      <revision id="2" href="2024.findings-eacl.1v2" hash="d115d641" date="2024-05-30">Fix two typos in equation 2 and 4.</revision>
    </paper>
    <paper id="2">
      <title><fixed-case>GPT</fixed-case>s Are Multilingual Annotators for Sequence Generation Tasks</title>
      <author><first>Juhwan</first><last>Choi</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Eunju</first><last>Lee</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>Kyohoon</first><last>Jin</last><affiliation>Chung-Ang University</affiliation></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>ChungAng University</affiliation></author>
      <pages>17-40</pages>
      <abstract>Data annotation is an essential step for constructing new datasets. However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive. In addition, the complexity of this process increases when dealing with low-resource languages owing to the difference in the language pool of crowdworkers. To address these issues, this study proposes an autonomous annotation method by utilizing large language models, which have been recently demonstrated to exhibit remarkable performance. Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for low-resource language annotation. Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study. We have opened our source code for further study and reproducibility.</abstract>
      <url hash="d0582eb1">2024.findings-eacl.2</url>
      <attachment type="software" hash="fff5e2cf">2024.findings-eacl.2.software.zip</attachment>
      <attachment type="note" hash="cce955c1">2024.findings-eacl.2.note.zip</attachment>
      <bibkey>choi-etal-2024-gpts</bibkey>
      <video href="2024.findings-eacl.2.mp4"/>
    </paper>
    <paper id="3">
      <title>Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive <fixed-case>EHR</fixed-case> Modelling with Hierarchical Regularisation</title>
      <author><first>Heejoon</first><last>Koo</last></author>
      <pages>41-55</pages>
      <abstract>Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical codes representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.</abstract>
      <url hash="27b828b0">2024.findings-eacl.3</url>
      <bibkey>koo-2024-next</bibkey>
      <video href="2024.findings-eacl.3.mp4"/>
    </paper>
    <paper id="4">
      <title><fixed-case>F</fixed-case>lexi<fixed-case>QA</fixed-case>: Leveraging <fixed-case>LLM</fixed-case>’s Evaluation Capabilities for Flexible Knowledge Selection in Open-domain Question Answering</title>
      <author><first>Yuhan</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Shuqi</first><last>Li</last></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>56-66</pages>
      <abstract>Nowadays, large language models (LLMs) have demonstrated their ability to be a powerful knowledge generator of generate-then-read paradigm for open-domain question answering (ODQA). However this new paradigm mainly suffers from the “hallucination” and struggles to handle time-sensitive issue because of its expensive knowledge update costs. On the other hand, retrieve-then-read, as a traditional paradigm, is more limited by the relevance of acquired knowledge to the given question. In order to combine the strengths of both paradigms, and overcome their respective shortcomings, we design a new pipeline called “FlexiQA”, in which we utilize the diverse evaluation capabilities of LLMs to select knowledge effectively and flexibly. First, given a question, we prompt a LLM as a discriminator to identify whether it is time-sensitive. For time-sensitive questions, we follow the retrieve-then-read paradigm to obtain the answer. For the non time-sensitive questions, we further prompt the LLM as an evaluator to select a better document from two perspectives: factuality and relevance. Based on the selected document, we leverage a reader to get the final answer. We conduct extensive experiments on three widely-used ODQA benchmarks, the experimental results fully confirm the effectiveness of our approach.</abstract>
      <url hash="cbe0ec51">2024.findings-eacl.4</url>
      <bibkey>chen-etal-2024-flexiqa</bibkey>
      <video href="2024.findings-eacl.4.mp4"/>
    </paper>
    <paper id="5">
      <title>Hyper-<fixed-case>BTS</fixed-case> Dataset: Scalability and Enhanced Analysis of Back <fixed-case>T</fixed-case>ran<fixed-case>S</fixed-case>cription (<fixed-case>BTS</fixed-case>) for <fixed-case>ASR</fixed-case> Post-Processing</title>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Seolhwa</first><last>Lee</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Junyoung</first><last>Son</last></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Sugyeong</first><last>Eo</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanhee</first><last>Lee</last><affiliation>NAVER</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>67-78</pages>
      <abstract>The recent advancements in the realm of Automatic Speech Recognition (ASR) post-processing have been primarily driven by sequence-to-sequence paradigms. Despite their effectiveness, these methods often demand substantial amounts of data, necessitating the expensive recruitment of phonetic transcription experts to rectify the erroneous outputs of ASR systems, thereby creating the desired training data. Back TranScription (BTS) alleviates this issue by generating ASR inputs from clean text via a Text-to-Speech (TTS) system. While initial studies on BTS exhibited promise, they were constrained by a limited dataset of just 200,000 sentence pairs, leaving the scalability of this method in question. In this study, we delve into the potential scalability of BTS. We introduce the “Hyper-BTS” dataset, a corpus approximately five times larger than that utilized in prior research. Additionally, we present innovative criteria for categorizing error types within ASR post-processing. This not only facilitates a more comprehensive qualitative analysis, which was absent in preceding studies, but also enhances the understanding of ASR error patterns. Our empirical results, both quantitative and qualitative, suggest that the enlarged scale of the Hyper-BTS dataset sufficiently addresses a vast majority of the ASR error categories. We make the Hyper-BTS dataset publicly available.</abstract>
      <url hash="822de166">2024.findings-eacl.5</url>
      <bibkey>park-etal-2024-hyper</bibkey>
      <video href="2024.findings-eacl.5.mp4"/>
    </paper>
    <paper id="6">
      <title><fixed-case>P</fixed-case>arrot<fixed-case>TTS</fixed-case>: Text-to-speech synthesis exploiting disentangled self-supervised representations</title>
      <author><first>Neil</first><last>Shah</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Saiteja</first><last>Kosgi</last></author>
      <author><first>Vishal</first><last>Tambrahalli</last><affiliation>International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Neha</first><last>S</last><affiliation>International Institute of Information Technology Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Anil</first><last>Nelakanti</last></author>
      <author><first>Vineet</first><last>Gandhi</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>79-91</pages>
      <abstract>We present ParrotTTS, a modularized text-to-speech synthesis model leveraging disentangled self-supervised speech representations. It can train a multi-speaker variant effectively using transcripts from a single speaker. ParrotTTS adapts to a new language in low resource setup and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on bilingual or parallel examples, ParrotTTS can transfer voices across languages while preserving the speaker-specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker’s voice and accent. We present extensive results in monolingual and multi-lingual scenarios. ParrotTTS outperforms state-of-the-art multi-lingual text-to-speech (TTS) models using only a fraction of paired data as latter. Speech samples from ParrotTTS and code can be found at https://parrot-tts.github.io/tts/</abstract>
      <url hash="1104692c">2024.findings-eacl.6</url>
      <bibkey>shah-etal-2024-parrottts</bibkey>
      <video href="2024.findings-eacl.6.mp4"/>
    </paper>
    <paper id="7">
      <title><fixed-case>N</fixed-case>av<fixed-case>H</fixed-case>int: Vision and Language Navigation Agent with a Hint Generator</title>
      <author><first>Yue</first><last>Zhang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Quan</first><last>Guo</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>92-103</pages>
      <abstract>The existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment.In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions.The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent’s attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment.We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the agent’s interpretability.</abstract>
      <url hash="63815ab4">2024.findings-eacl.7</url>
      <bibkey>zhang-etal-2024-navhint</bibkey>
      <video href="2024.findings-eacl.7.mp4"/>
    </paper>
    <paper id="8">
      <title>Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?</title>
      <author><first>Piush</first><last>Aggarwal</last><affiliation>Fernuniversität Gesamthochschule Hagen and Fernuniversität Gesamthochschule Hagen</affiliation></author>
      <author><first>Jawar</first><last>Mehrabanian</last></author>
      <author><first>Weigang</first><last>Huang</last><affiliation>Universität Duisburg-Essen</affiliation></author>
      <author><first>Özge</first><last>Alacam</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Torsten</first><last>Zesch</last><affiliation>Fernuniversität in Hagen</affiliation></author>
      <pages>104-117</pages>
      <abstract>This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide evidence supporting the hypothesis that only the textual component of hateful memes enables the multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme’s image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confounders as compared to image confounders with average ∆F1 of 0.18.</abstract>
      <url hash="38939fde">2024.findings-eacl.8</url>
      <bibkey>aggarwal-etal-2024-text</bibkey>
    </paper>
    <paper id="9">
      <title>Where are we Still Split on Tokenization?</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>118-137</pages>
      <abstract>Many Natural Language Processing (NLP) tasks are labeled on the token level, forthese tasks, the first step is to identify the tokens (tokenization). Becausethis step is often considered to be a solved problem, gold tokenization iscommonly assumed. In this paper, we propose an efficient method fortokenization with subword-based language models, and reflect on the status ofperformance on the tokenization task by evaluating on 122 languages in 20different scripts. We show that our proposed model performs on par with thestate-of-the-art, and that tokenization performance is mainly dependent on theamount and consistency of annotated data. We conclude that besidesinconsistencies in the data and exceptional cases the task can be consideredsolved for Latin languages for in-dataset settings (&gt;99.5 F1). However,performance is 0.75 F1 point lower on average for datasets in other scripts andperformance deteriorates in cross-dataset setups.</abstract>
      <url hash="e9d06d61">2024.findings-eacl.9</url>
      <attachment type="software" hash="4898d0e8">2024.findings-eacl.9.software.tgz</attachment>
      <bibkey>goot-2024-still</bibkey>
      <video href="2024.findings-eacl.9.mp4"/>
    </paper>
    <paper id="10">
      <title>A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages</title>
      <author><first>Nikita</first><last>Martynov</last><affiliation>New Economic School</affiliation></author>
      <author><first>Mark</first><last>Baushenko</last></author>
      <author><first>Anastasia</first><last>Kozlova</last></author>
      <author><first>Katerina</first><last>Kolomeytseva</last></author>
      <author><first>Aleksandr</first><last>Abramov</last></author>
      <author><first>Alena</first><last>Fenogenova</last></author>
      <pages>138-155</pages>
      <abstract>Large language models excel in text generation and generalization, however they face challenges in text editing tasks, especially in correcting spelling errors and mistyping.In this paper, we present a methodology for generative spelling correction (SC), tested on English and Russian languages and potentially can be extended to any language with minor changes. Our research mainly focuses on exploring natural spelling errors and mistyping in texts and studying how those errors can be emulated in correct sentences to enrich generative models’ pre-train procedure effectively. We investigate the effects of emulations in various text domains and examine two spelling corruption techniques: 1) first one mimics human behavior when making a mistake through leveraging statistics of errors from a particular dataset, and 2) second adds the most common spelling errors, keyboard miss clicks, and some heuristics within the texts.We conducted experiments employing various corruption strategies, models’ architectures, and sizes in the pre-training and fine-tuning stages and evaluated the models using single-domain and multi-domain test sets. As a practical outcome of our work, we introduce SAGE (Spell checking via Augmentation and Generative distribution Emulation).</abstract>
      <url hash="86d5f92e">2024.findings-eacl.10</url>
      <attachment type="software" hash="41b27f50">2024.findings-eacl.10.software.zip</attachment>
      <attachment type="note" hash="72e71561">2024.findings-eacl.10.note.zip</attachment>
      <bibkey>martynov-etal-2024-methodology</bibkey>
    </paper>
    <paper id="11">
      <title>How Does In-Context Learning Help Prompt Tuning?</title>
      <author><first>Simeng</first><last>Sun</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Dan</first><last>Iter</last></author>
      <author><first>Chenguang</first><last>Zhu</last><affiliation>Zoom</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>156-165</pages>
      <abstract>Fine-tuning large language models is becoming ever more impractical due to their rapidly-growing scale. This motivates the use of parameter-efficient adaptation methods such as prompt tuning (PT), which adds a small number of tunable embeddings to an otherwise frozen model, and in-context learning (ICL), in which demonstrations of the task are provided to the model in natural language without any additional training. Recently, (CITATION) propose “instruction prompt tuning” (IPT), which combines PT with ICL by concatenating a natural language demonstration with learned prompt embeddings. While all of these methods have proven effective on different tasks, how they interact with each other remains unexplored. In this paper, we empirically study when and how in-context examples improve prompt tuning by measuring the effectiveness of ICL, PT, and IPT on five text generation tasks with multiple base language models. We observe that (1) IPT does <i>not</i> always outperform PT, and in fact requires the in-context demonstration to be semantically similar to the test input to yield improvements; (2) PT is unstable and exhibits high variance, but combining PT and ICL (into IPT) consistently reduces variance across all five tasks; and(3) prompts learned for a specific source task via PT exhibit positive transfer when paired with in-context examples of a different target task. Our results offer actionable insights on choosing a suitable parameter-efficient adaptation method for a given task.</abstract>
      <url hash="a17235c9">2024.findings-eacl.11</url>
      <bibkey>sun-etal-2024-context</bibkey>
      <video href="2024.findings-eacl.11.mp4"/>
    </paper>
    <paper id="12">
      <title>Large Language Models for Psycholinguistic Plausibility Pretesting</title>
      <author><first>Samuel</first><last>Amouyal</last></author>
      <author><first>Aya</first><last>Meltzer-Asscher</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Jonathan</first><last>Berant</last><affiliation>Google and Tel Aviv University</affiliation></author>
      <pages>166-181</pages>
      <abstract>In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.</abstract>
      <url hash="08a290d1">2024.findings-eacl.12</url>
      <attachment type="software" hash="d5b10b6a">2024.findings-eacl.12.software.zip</attachment>
      <attachment type="note" hash="6832e347">2024.findings-eacl.12.note.zip</attachment>
      <bibkey>amouyal-etal-2024-large</bibkey>
      <video href="2024.findings-eacl.12.mp4"/>
    </paper>
    <paper id="13">
      <title>Modeling Aspect Sentiment Coherency via Local Sentiment Aggregation</title>
      <author><first>Heng</first><last>Yang</last></author>
      <author><first>Ke</first><last>Li</last><affiliation>University of Exeter</affiliation></author>
      <pages>182-195</pages>
      <abstract>Aspect sentiment coherency is an intriguing yet underexplored topic in the field of aspect-based sentiment classification. This concept reflects the common pattern where adjacent aspects often share similar sentiments. Despite its prevalence, current studies have not fully recognized the potential of modeling aspect sentiment coherency, including its implications in adversarial defense. To model aspect sentiment coherency, we propose a novel local sentiment aggregation (LSA) paradigm based on constructing a differential-weighted sentiment aggregation window. We have rigorously evaluated our model through experiments, and the results affirm the proficiency of LSA in terms of aspect coherency prediction and aspect sentiment classification. For instance, it outperforms existing models and achieves state-of-the-art sentiment classification performance across five public datasets. Furthermore, we demonstrate the promising ability of LSA in ABSC adversarial defense, thanks to its sentiment coherency modeling. To encourage further exploration and application of this concept, we have made our code publicly accessible. This will provide researchers with a valuable tool to delve into sentiment coherency modeling in future research.</abstract>
      <url hash="e0ee34c5">2024.findings-eacl.13</url>
      <attachment type="software" hash="7abddaeb">2024.findings-eacl.13.software.zip</attachment>
      <attachment type="note" hash="7abddaeb">2024.findings-eacl.13.note.zip</attachment>
      <bibkey>yang-li-2024-modeling</bibkey>
      <video href="2024.findings-eacl.13.mp4"/>
    </paper>
    <paper id="14">
      <title>An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics</title>
      <author><first>Saba</first><last>Ahmadi</last></author>
      <author><first>Aishwarya</first><last>Agrawal</last><affiliation>Université de Montréal, Mila – Quebec AI Institute and Google DeepMind</affiliation></author>
      <pages>196-208</pages>
      <abstract>Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negation, and CLIPScore and PAC-S are insensitive to the structure of the caption to a great extent. We hope our findings will guide further improvements in reference-free evaluation of image captioning.</abstract>
      <url hash="98d0754c">2024.findings-eacl.14</url>
      <bibkey>ahmadi-agrawal-2024-examination</bibkey>
      <video href="2024.findings-eacl.14.mp4"/>
    </paper>
    <paper id="15">
      <title>Barriers to Effective Evaluation of Simultaneous Interpretation</title>
      <author><first>Shira</first><last>Wein</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Te</first><last>I</last><affiliation>Google</affiliation></author>
      <author><first>Colin</first><last>Cherry</last><affiliation>Google</affiliation></author>
      <author><first>Juraj</first><last>Juraska</last><affiliation>Google</affiliation></author>
      <author><first>Dirk</first><last>Padfield</last><affiliation>Google</affiliation></author>
      <author><first>Wolfgang</first><last>Macherey</last><affiliation>Google</affiliation></author>
      <pages>209-219</pages>
      <abstract>Simultaneous interpretation is an especially challenging form of translation because it requires converting speech from one language to another in real-time. Though prior work has relied on out-of-the-box machine translation metrics to evaluate interpretation data, we hypothesize that strategies common in high-quality human interpretations, such as summarization, may not be handled well by standard machine translation metrics. In this work, we examine both qualitatively and quantitatively four potential barriers to evaluation of interpretation: disfluency, summarization, paraphrasing, and segmentation. Our experiments reveal that, while some machine translation metrics correlate fairly well with human judgments of interpretation quality, much work is still needed to account for strategies of interpretation during evaluation. As a first step to address this, we develop a fine-tuned model for interpretation evaluation, and achieve better correlation with human judgments than the state-of-the-art machine translation metrics.</abstract>
      <url hash="2ec8040a">2024.findings-eacl.15</url>
      <bibkey>wein-etal-2024-barriers</bibkey>
      <video href="2024.findings-eacl.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Inconsistent dialogue responses and how to recover from them</title>
      <author><first>Mian</first><last>Zhang</last></author>
      <author><first>Lifeng</first><last>Jin</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Haitao</first><last>Mi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>220-230</pages>
      <abstract>One critical issue for chat systems is to stay consistent about preferences, opinions, beliefs and facts of itself, which has been shown a difficult problem. In this work, we study methods to assess and bolster utterance consistency of chat systems. A dataset is first developed for studying the inconsistencies, where inconsistent dialogue responses, explanations of the inconsistencies, and recovery utterances are authored by annotators. This covers the life span of inconsistencies, namely introduction, understanding, and resolution. Building on this, we introduce a set of tasks centered on dialogue consistency, specifically focused on its detection and resolution. Our experimental findings indicate that our dataset significantly helps the progress in identifying and resolving conversational inconsistencies, and current popular large language models like ChatGPT which are good at resolving inconsistencies however still struggle with detection.</abstract>
      <url hash="def387c7">2024.findings-eacl.16</url>
      <bibkey>zhang-etal-2024-inconsistent</bibkey>
    </paper>
    <paper id="17">
      <title><fixed-case>MUG</fixed-case>: Interactive Multimodal Grounding on User Interfaces</title>
      <author><first>Tao</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Gang</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Jingjie</first><last>Zheng</last></author>
      <author><first>Purple</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Yang</first><last>Li</last><affiliation>Google</affiliation></author>
      <pages>231-251</pages>
      <abstract>We present MUG, a novel interactive task for multimodal grounding where a user and an agent work collaboratively on an interface screen. Prior works modeled multimodal UI grounding in one round: the user gives a command and the agent responds to the command. Yet, in a realistic scenario, a user command can be ambiguous when the target action is inherently difficult to articulate in natural language. MUG allows multiple rounds of interactions such that upon seeing the agent responses, the user can give further commands for the agent to refine or even correct its actions. Such interaction is critical for improving grounding performances in real-world use cases. To investigate the problem, we create a new dataset that consists of 77,820 sequences of human user-agent interaction on mobile interfaces in which 20% involves multiple rounds of interactions. To establish benchmark, we experiment with a range of modeling variants and evaluation strategies, including both offline and online evaluation—the online strategy consists of both human evaluation and automatic with simulators. Our experiments show that iterative interaction significantly improves the absolute task completion by 18% over the entire test set and 31% over the challenging split. Our results lay the foundation for further investigation of the problem.</abstract>
      <url hash="ba7f75c0">2024.findings-eacl.17</url>
      <bibkey>li-etal-2024-mug</bibkey>
    </paper>
    <paper id="18">
      <title><fixed-case>PRIL</fixed-case>o<fixed-case>RA</fixed-case>: Pruned and Rank-Increasing Low-Rank Adaptation</title>
      <author><first>Nadav</first><last>Benedek</last></author>
      <author><first>Lior</first><last>Wolf</last><affiliation>Tel Aviv University, Tel Aviv University and Tel Aviv University</affiliation></author>
      <pages>252-263</pages>
      <abstract>With the proliferation of large pre-trained language models (PLMs), fine-tuning all model parameters becomes increasingly inefficient, particularly when dealing with numerous downstream tasks that entail substantial training and storage costs. Several approaches aimed at achieving parameter-efficient fine-tuning (PEFT) have been proposed. Among them, Low-Rank Adaptation (LoRA) stands out as an archetypal method, incorporating trainable rank decomposition matrices into each target module. Nevertheless, LoRA does not consider the varying importance of each layer. To address these challenges, we introduce PRILoRA, which linearly allocates a different rank for each layer, in an increasing manner, and performs pruning throughout the training process, considering both the temporary magnitude of weights and the accumulated statistics of the input to any given layer. We validate the effectiveness of PRILoRA through extensive experiments on eight GLUE benchmarks, setting a new state of the art.</abstract>
      <url hash="b7def7f3">2024.findings-eacl.18</url>
      <attachment type="software" hash="300fc956">2024.findings-eacl.18.software.zip</attachment>
      <bibkey>benedek-wolf-2024-prilora</bibkey>
      <video href="2024.findings-eacl.18.mp4"/>
    </paper>
    <paper id="19">
      <title>Revamping Multilingual Agreement Bidirectionally via Switched Back-translation for Multilingual Neural Machine Translation</title>
      <author><first>Hongyuan</first><last>Lu</last></author>
      <author><first>Haoyang</first><last>Huang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Dongdong</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>264-275</pages>
      <abstract>Despite the fact that multilingual agreement (MA) has shown its importance for multilingual neural machine translation (MNMT), current methodologies in the field have two shortages: (i) require parallel data between multiple language pairs, which is not always realistic and (ii) optimize the agreement in an ambiguous direction, which hampers the translation performance. We present <b>B</b>idirectional <b>M</b>ultilingual <b>A</b>greement via <b>S</b>witched <b>B</b>ack-<b>t</b>ranslation (<b>BMA-SBT</b>), a novel and universal multilingual agreement framework for fine-tuning pre-trained MNMT models, which (i) exempts the need for aforementioned parallel data by using a novel method called switched BT that creates synthetic text written in another source language using the translation target and (ii) optimizes the agreement bidirectionally with the Kullback-Leibler Divergence loss. Experiments indicate that BMA-SBT clearly improves the strong baselines on the task of MNMT with three benchmarks: TED Talks, News, and Europarl. In-depth analyzes indicate that BMA-SBT brings additive improvements to the conventional BT method.</abstract>
      <url hash="05159d1d">2024.findings-eacl.19</url>
      <bibkey>lu-etal-2024-revamping</bibkey>
    </paper>
    <paper id="20">
      <title>m<fixed-case>PLM</fixed-case>-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models</title>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>Chengzhi</first><last>Hu</last></author>
      <author><first>Zheyu</first><last>Zhang</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>276-310</pages>
      <abstract>Recent multilingual pretrained language models (mPLMs) have been shown to encode strong language-specific signals, which are not explicitly provided during pretraining. It remains an open question whether it is feasible to employ mPLMs to measure language similarity, and subsequently use the similarity results to select source languages for boosting cross-lingual transfer. To investigate this, we propose mPLM-Sim, a language similarity measure that induces the similarities across languages from mPLMs using multi-parallel corpora. Our study shows that mPLM-Sim exhibits moderately high correlations with linguistic similarity measures, such as lexicostatistics, genealogical language family, and geographical sprachbund. We also conduct a case study on languages with low correlation and observe that mPLM-Sim yields more accurate similarity results. Additionally, we find that similarity results vary across different mPLMs and different layers within an mPLM. We further investigate whether mPLM-Sim is effective for zero-shot cross-lingual transfer by conducting experiments on both low-level syntactic tasks and high-level semantic tasks. The experimental results demonstrate that mPLM-Sim is capable of selecting better source languages than linguistic measures, resulting in a 1%-2% improvement in zero-shot cross-lingual transfer performance.</abstract>
      <url hash="accddcb1">2024.findings-eacl.20</url>
      <bibkey>lin-etal-2024-mplm</bibkey>
      <video href="2024.findings-eacl.20.mp4"/>
    </paper>
    <paper id="21">
      <title><fixed-case>OYXOY</fixed-case>: A <fixed-case>M</fixed-case>odern <fixed-case>NLP</fixed-case> Test Suite for <fixed-case>M</fixed-case>odern <fixed-case>G</fixed-case>reek</title>
      <author><first>Konstantinos</first><last>Kogkalidis</last></author>
      <author><first>Stergios</first><last>Chatzikyriakidis</last><affiliation>University of Gothenburg</affiliation></author>
      <author><first>Eirini</first><last>Giannikouri</last><affiliation>University of Crete</affiliation></author>
      <author><first>Vasiliki</first><last>Katsouli</last><affiliation>University of Crete</affiliation></author>
      <author><first>Christina</first><last>Klironomou</last></author>
      <author><first>Christina</first><last>Koula</last></author>
      <author><first>Dimitris</first><last>Papadakis</last></author>
      <author><first>Thelka</first><last>Pasparaki</last></author>
      <author><first>Erofili</first><last>Psaltaki</last><affiliation>University of Crete</affiliation></author>
      <author><first>Efthymia</first><last>Sakellariou</last><affiliation>University of Crete</affiliation></author>
      <author><first>Charikleia</first><last>Soupiona</last></author>
      <pages>311-322</pages>
      <abstract>This paper serves as a foundational step towards the development of a linguistically motivated and technically relevant evaluation suite for Greek NLP. We initiate this endeavor by introducing four expert-verified evaluation tasks, specifically targeted at natural language inference, word sense disambiguation (through example comparison or sense selection) and metaphor detection. More than language-adapted replicas of existing tasks, we contribute two innovations which will resonate with the broader resource and evaluation community. Firstly, our inference dataset is the first of its kind, marking not just one, but rather all possible inference labels, accounting for possible shifts due to e.g. ambiguity or polysemy. Secondly, we demonstrate a cost-efficient method to obtain datasets for under-resourced languages. Using ChatGPT as a language-neutral parser, we transform the Dictionary of Standard Modern Greek into a structured format, from which we derive the other three tasks through simple projections. Alongside each task, we conduct experiments using currently available state of the art machinery. Our experimental baselines affirm the challenging nature of our tasks and highlight the need for expedited progress in order for the Greek NLP ecosystem to keep pace with contemporary mainstream research.</abstract>
      <url hash="e3c361c4">2024.findings-eacl.21</url>
      <attachment type="software" hash="5bafd75d">2024.findings-eacl.21.software.zip</attachment>
      <attachment type="note" hash="eb900342">2024.findings-eacl.21.note.zip</attachment>
      <bibkey>kogkalidis-etal-2024-oyxoy</bibkey>
    </paper>
    <paper id="22">
      <title>A Comprehensive Evaluation of Inductive Reasoning Capabilities and Problem Solving in Large Language Models</title>
      <author><first>Chen</first><last>Bowen</last></author>
      <author><first>Rune</first><last>Sætre</last><affiliation>Norwegian University of Science and Technology</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>323-339</pages>
      <abstract>Inductive reasoning is fundamental to both human and artificial intelligence. The inductive reasoning abilities of current Large Language Models (LLMs) are evaluated in this research.We argue that only considering induction of rules is too narrow and unrealistic, since inductive reasoning is usually mixed with other abilities, like rules application, results/rules validation, and updated information integration.We probed the LLMs with a set of designed symbolic tasks and found that even state-of-the-art (SotA) LLMs fail significantly, showing the inability of LLMs to perform these intuitively simple tasks.Furthermore, we found that perfect accuracy in a small-size problem does not guarantee the same accuracy in a larger-size version of the same problem, provoking the question of how we can assess the LLMs’ actual problem-solving capabilities.We also argue that Chain-of-Thought prompts help the LLMs by decomposing the problem-solving process, but the LLMs still learn limitedly.Furthermore, we reveal that few-shot examples assist LLM generalization in out-of-domain (OOD) cases, albeit limited. The LLM starts to fail when the problem deviates from the provided few-shot examples.</abstract>
      <url hash="85cb68ea">2024.findings-eacl.22</url>
      <attachment type="software" hash="2f3e1abc">2024.findings-eacl.22.software.zip</attachment>
      <bibkey>bowen-etal-2024-comprehensive</bibkey>
      <video href="2024.findings-eacl.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Towards efficient self-supervised representation learning in speech processing</title>
      <author><first>Luis</first><last>Lugo</last></author>
      <author><first>Valentin</first><last>Vielzeuf</last><affiliation>Orange-labs</affiliation></author>
      <pages>340-346</pages>
      <abstract>Self-supervised learning has achieved impressive results in speech processing, but current models are computationally expensive, generating environmental concerns because of their high energy consumption. Therefore, we propose an efficient self-supervised approach to address high computational costs, using a single GPU during 24 to 48 hours of pretraining. The proposed approach combines linear, convolutional, and self-attention layers with several optimizations, including dynamic batching, flash attention, mixed-precision training, gradient accumulation, and acoustic feature extraction with input preprocessing. Computational cost estimations for our proposed model represent up to two orders of magnitude improvements in computational efficiency against existing speech models.</abstract>
      <url hash="7d046408">2024.findings-eacl.23</url>
      <bibkey>lugo-vielzeuf-2024-towards</bibkey>
    </paper>
    <paper id="24">
      <title>Improving Cross-Domain Low-Resource Text Generation through <fixed-case>LLM</fixed-case> Post-Editing: A Programmer-Interpreter Approach</title>
      <author><first>Zhuang</first><last>Li</last><affiliation>Monash University</affiliation></author>
      <author><first>Levon</first><last>Haroutunian</last><affiliation>Openstream, Inc.</affiliation></author>
      <author><first>Raj</first><last>Tumuluri</last><affiliation>Openstream Inc</affiliation></author>
      <author><first>Philip</first><last>Cohen</last></author>
      <author><first>Reza</first><last>Haf</last><affiliation>Monash University</affiliation></author>
      <pages>347-354</pages>
      <abstract>Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs’ ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs while editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5’s performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.</abstract>
      <url hash="f39ab06e">2024.findings-eacl.24</url>
      <bibkey>li-etal-2024-improving-cross</bibkey>
    </paper>
    <paper id="25">
      <title>Noise Contrastive Estimation-based Matching Framework for Low-Resource Security Attack Pattern Recognition</title>
      <author><first>Tu</first><last>Nguyen</last><affiliation>Huawei R&amp;D Munich</affiliation></author>
      <author><first>Nedim</first><last>Šrndić</last><affiliation>Huawei Technologies Duesseldorf GmbH</affiliation></author>
      <author><first>Alexander</first><last>Neth</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>355-373</pages>
      <abstract>Techniques, Tactics and Procedures (TTP) mapping is an important and difficult task in the application of cyber threat intelligence (CTI) extraction for threat reports. TTPs are typically expressed in semantic forms within security knowledge bases like MITRE ATT&amp;CK, serving as textual high-level descriptions for sophisticated attack patterns. Conversely, attacks in CTI threat reports are detailed in a combination of natural and technical language forms, presenting a significant challenge even for security experts to establish correlations or mappings with the corresponding TTPs.Conventional learning approaches often target the TTP mapping problem in the classical multiclass/label classification setting. This setting hinders the learning capabilities of the model, due to the large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. In this work, we approach the problem in a different learning paradigm, such that the assignment of a text to a TTP label is essentially decided by the direct semantic similarity between the two, thus, reducing the complexity of competing solely over the large labeling space. In order that, we propose a neural matching architecture that incorporates a sampling based learn-to-compare mechanism, facilitating the learning process of the matching model despite constrained resources.</abstract>
      <url hash="5bf9f38e">2024.findings-eacl.25</url>
      <attachment type="note" hash="c0b2ed72">2024.findings-eacl.25.note.tgz</attachment>
      <bibkey>nguyen-etal-2024-noise</bibkey>
      <video href="2024.findings-eacl.25.mp4"/>
    </paper>
    <paper id="26">
      <title>Large Language Models for Scientific Information Extraction: An Empirical Study for Virology</title>
      <author><first>Mahsa</first><last>Shamsabadi</last><affiliation>TIB Hannover</affiliation></author>
      <author><first>Jennifer</first><last>D’Souza</last><affiliation>TIB Hannover</affiliation></author>
      <author><first>Sören</first><last>Auer</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <pages>374-392</pages>
      <abstract>In this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like Wikipedia infoboxes or structured Amazon product descriptions. These representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. Our novel automated approach leverages the robust text generation capabilities of LLMs to produce structured scholarly contribution summaries, offering both a practical solution and insights into LLMs’ emergent abilities.For LLMs, the prime focus is on improving their general intelligence as conversational agents. We argue that these models can also be applied effectively in information extraction (IE), specifically in complex IE tasks within terse domains like Science. This paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer parameters than the state-of-the-art GPT-davinci is competitive for the task.</abstract>
      <url hash="41888965">2024.findings-eacl.26</url>
      <attachment type="software" hash="bce15d0c">2024.findings-eacl.26.software.zip</attachment>
      <attachment type="note" hash="27bd030b">2024.findings-eacl.26.note.zip</attachment>
      <bibkey>shamsabadi-etal-2024-large</bibkey>
      <video href="2024.findings-eacl.26.mp4"/>
    </paper>
    <paper id="27">
      <title>Re3val: Reinforced and Reranked Generative Retrieval</title>
      <author><first>EuiYul</first><last>Song</last><affiliation>Samsung Electronics</affiliation></author>
      <author><first>Sangryul</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Haeju</first><last>Lee</last><affiliation>LG Corporation</affiliation></author>
      <author><first>Joonkee</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>393-409</pages>
      <abstract>Generative retrieval models encode pointers to information in a corpus as an index within the model’s parameters. These models serve as part of a larger pipeline, where retrieved information conditions generation for knowledge-intensive NLP tasks. However, we identify two limitations: the generative retrieval does not account for contextual information. Secondly, the retrieval can’t be tuned for the downstream readers as decoding the page title is a non-differentiable operation. This paper introduces Re3val, trained with generative reranking and reinforcement learning using limited data. Re3val leverages context acquired via Dense Passage Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated by constrained decoding. Additionally, we generate questions from our pre-training dataset to mitigate epistemic uncertainty and bridge the domain gap between the pre-training and fine-tuning datasets. Subsequently, we extract and rerank contexts from the KILT database using the rerank page titles. Upon grounding the top five reranked contexts, Re3val demonstrates the Top 1 KILT scores compared to all other generative retrieval models across five KILT datasets.</abstract>
      <url hash="9e63f5ec">2024.findings-eacl.27</url>
      <bibkey>song-etal-2024-re3val</bibkey>
      <video href="2024.findings-eacl.27.mp4"/>
    </paper>
    <paper id="28">
      <title>Entity Linking in the Job Market Domain</title>
      <author><first>Mike</first><last>Zhang</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <pages>410-419</pages>
      <abstract>In Natural Language Processing, entity linking (EL) has centered around Wikipedia, but yet remains underexplored for the job market domain. Disambiguating skill mentions can help us get insight into the current labor market demands. In this work, we are the first to explore EL in this domain, specifically targeting the linkage of occupational skills to the ESCO taxonomy (le Vrang et al., 2014). Previous efforts linked coarse-grained (full) sentences to a corresponding ESCO skill. In this work, we link more fine-grained span-level mentions of skills. We tune two high-performing neural EL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et al., 2021), on a synthetically generated mention–skill pair dataset and evaluate them on a human-annotated skill-linking benchmark. Our findings reveal that both models are capable of linking implicit mentions of skills to their correct taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict evaluation, but GENRE performs better in loose evaluation (accuracy@k).</abstract>
      <url hash="21d3a5dd">2024.findings-eacl.28</url>
      <bibkey>zhang-etal-2024-entity</bibkey>
      <video href="2024.findings-eacl.28.mp4"/>
    </paper>
    <paper id="29">
      <title>(Chat)<fixed-case>GPT</fixed-case> v <fixed-case>BERT</fixed-case> Dawn of Justice for Semantic Change Detection</title>
      <author><first>Francesco</first><last>Periti</last><affiliation>University of Milan</affiliation></author>
      <author><first>Haim</first><last>Dubossarsky</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Nina</first><last>Tahmasebi</last><affiliation>Göteborg University</affiliation></author>
      <pages>420-436</pages>
      <abstract>In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes.</abstract>
      <url hash="248fa74e">2024.findings-eacl.29</url>
      <attachment type="software" hash="58309920">2024.findings-eacl.29.software.zip</attachment>
      <attachment type="note" hash="6ee98e73">2024.findings-eacl.29.note.zip</attachment>
      <bibkey>periti-etal-2024-chat</bibkey>
    </paper>
    <paper id="30">
      <title>Towards Unified Uni- and Multi-modal News Headline Generation</title>
      <author><first>Mateusz</first><last>Krubiński</last><affiliation>Charles University</affiliation></author>
      <author><first>Pavel</first><last>Pecina</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>437-450</pages>
      <abstract>Thanks to the recent progress in vision-language modeling and the evolving nature of news consumption, the tasks of automatic summarization and headline generation based on multimodal news articles have been gaining popularity. One of the limitations of the current approaches is caused by the commonly used sophisticated modular architectures built upon hierarchical cross-modal encoders and modality-specific decoders, which restrict the model’s applicability to specific data modalities – once trained on, e.g., text+video pairs there is no straightforward way to apply the model to text+image or text-only data. In this work, we propose a unified task formulation that utilizes a simple encoder-decoder model to generate headlines from uni- and multi-modal news articles. This model is trained jointly on data of several modalities and extends the textual decoder to handle the multimodal output.</abstract>
      <url hash="3aed0449">2024.findings-eacl.30</url>
      <bibkey>krubinski-pecina-2024-towards</bibkey>
      <video href="2024.findings-eacl.30.mp4"/>
    </paper>
    <paper id="31">
      <title>On the Relationship between Sentence Analogy Identification and Sentence Structure Encoding in Large Language Models</title>
      <author><first>Thilini</first><last>Wijesiriwardene</last></author>
      <author><first>Ruwan</first><last>Wickramarachchi</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Aishwarya Naresh</first><last>Reganti</last><affiliation>Amazon</affiliation></author>
      <author><first>Vinija</first><last>Jain</last><affiliation>Stanford University</affiliation></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon</affiliation></author>
      <author><first>Amit</first><last>Sheth</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Amitava</first><last>Das</last><affiliation>University of South Carolina</affiliation></author>
      <pages>451-457</pages>
      <abstract>The ability of Large Language Models (LLMs) to encode syntactic and semantic structures of language is well examined in NLP. Additionally, analogy identification, in the form of word analogies are extensively studied in the last decade of language modeling literature. In this work we specifically look at how LLMs’ abilities to capture sentence analogies (sentences that convey analogous meaning to each other) vary with LLMs’ abilities to encode syntactic and semantic structures of sentences. Through our analysis, we find that LLMs’ ability to identify sentence analogies is positively correlated with their ability to encode syntactic and semantic structures of sentences. Specifically, we find that the LLMs which capture syntactic structures better, also have higher abilities in identifying sentence analogies.</abstract>
      <url hash="6fe2b6ac">2024.findings-eacl.31</url>
      <bibkey>wijesiriwardene-etal-2024-relationship</bibkey>
      <video href="2024.findings-eacl.31.mp4"/>
    </paper>
    <paper id="32">
      <title>Contextualization Distillation from Large Language Model for Knowledge Graph Completion</title>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>Zhen</first><last>Tan</last></author>
      <author><first>Tianlong</first><last>Chen</last></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Arizona State University</affiliation></author>
      <pages>458-477</pages>
      <abstract>While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the <i>Contextualization Distillation</i> strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks—reconstruction and contextualization—allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into how to generate high-quality corpora for KGC, as well as the selection of suitable distillation tasks.</abstract>
      <url hash="4b81611d">2024.findings-eacl.32</url>
      <bibkey>li-etal-2024-contextualization</bibkey>
      <revision id="1" href="2024.findings-eacl.32v1" hash="7f273319"/>
      <revision id="2" href="2024.findings-eacl.32v2" hash="4b81611d" date="2024-03-30">This revision corrects the citation display problem in the Appendix.</revision>
      <video href="2024.findings-eacl.32.mp4"/>
    </paper>
    <paper id="33">
      <title>Differentially Private Natural Language Models: Recent Advances and Future Directions</title>
      <author><first>Lijie</first><last>Hu</last><affiliation>KAUST</affiliation></author>
      <author><first>Ivan</first><last>Habernal</last><affiliation>Universität Paderborn</affiliation></author>
      <author><first>Lei</first><last>Shen</last><affiliation>JD AI Research, Beijing, China</affiliation></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>478-499</pages>
      <abstract>Recent developments in deep learning have led to great success in various natural language processing (NLP) tasks. However, these applications may involve data that contain sensitive information. Therefore, how to achieve good performance while also protecting the privacy of sensitive data is a crucial challenge in NLP. To preserve privacy, Differential Privacy (DP), which can prevent reconstruction attacks and protect against potential side knowledge, is becoming a de facto technique for private data analysis. In recent years, NLP in DP models (DP-NLP) has been studied from different perspectives, which deserves a comprehensive review. In this paper, we provide the first systematic review of recent advances in DP deep learning models in NLP. In particular, we first discuss some differences and additional challenges of DP-NLP compared with the standard DP deep learning. Then, we investigate some existing work on DP-NLP andpresent its recent developments from three aspects: gradient perturbation based methods, embedding vector perturbation based methods, and ensemble model based methods. We also discuss some challenges and future directions.</abstract>
      <url hash="25234848">2024.findings-eacl.33</url>
      <bibkey>hu-etal-2024-differentially</bibkey>
      <video href="2024.findings-eacl.33.mp4"/>
    </paper>
    <paper id="34">
      <title>Learning to Compare Financial Reports for Financial Forecasting</title>
      <author><first>Ross</first><last>Koval</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Nicholas</first><last>Andrews</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Xifeng</first><last>Yan</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>500-512</pages>
      <abstract>Public companies in the US are required to publish annual reports that detail their recent financial performance, present the current state of ongoing business operations, and discuss future prospects. However, they typically contain over 25,000 words across all sections, large amounts of industry and legal jargon, and a high percentage of boilerplate content that does not change much year-to-year. These unique characteristics present challenges for many generic pretrained language models because it is likely that only a small percentage of the long report that reflects salient information contains meaningful signal about the future prospects of the company. In this work, we curate a large-scale dataset of paired financial reports and introduce two novel, challenging tasks of predicting long-horizon company risk and correlation that evaluate the ability of the model to recognize cross-document relationships with complex, nuanced signals. We explore and present a comprehensive set of methods and experiments, and establish strong baselines designed to learn to identify subtle similarities and differences between long documents. Furthermore, we demonstrate that it is possible to predict company risk and correlation solely from the text of their financial reports and further that modeling the cross-document interactions at a fine-grained level provides significant benefit. Finally, we probe the best performing model through quantitative and qualitative interpretability methods to reveal some insight into the underlying task signal.</abstract>
      <url hash="d6898770">2024.findings-eacl.34</url>
      <bibkey>koval-etal-2024-learning</bibkey>
    </paper>
    <paper id="35">
      <title>Arukikata Travelogue Dataset with Geographic Entity Mention, Coreference, and Link Annotation</title>
      <author><first>Shohei</first><last>Higashiyama</last><affiliation>Nara Institute of Science and Technology, Japan and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hiroki</first><last>Ouchi</last><affiliation>NAIST</affiliation></author>
      <author><first>Hiroki</first><last>Teranishi</last><affiliation>RIKEN</affiliation></author>
      <author><first>Hiroyuki</first><last>Otomo</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Yusuke</first><last>Ide</last></author>
      <author><first>Aitaro</first><last>Yamamoto</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hiroyuki</first><last>Shindo</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yuki</first><last>Matsuda</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Shoko</first><last>Wakamiya</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Naoya</first><last>Inoue</last><affiliation>RIKEN and Japan Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Ikuya</first><last>Yamada</last><affiliation>RIKEN and Studio Ousia</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>513-532</pages>
      <abstract>Geoparsing is a fundamental technique for analyzing geo-entity information in text, which is useful for geographic applications, e.g., tourist spot recommendation. We focus on document-level geoparsing that considers geographic relatedness among geo-entity mentions and present a Japanese travelogue dataset designed for training and evaluating document-level geoparsing systems. Our dataset comprises 200 travelogue documents with rich geo-entity information: 12,171 mentions, 6,339 coreference clusters, and 2,551 geo-entities linked to geo-database entries.</abstract>
      <url hash="feb0bd10">2024.findings-eacl.35</url>
      <bibkey>higashiyama-etal-2024-arukikata</bibkey>
    </paper>
    <paper id="36">
      <title>Knowledge Generation for Zero-shot Knowledge-based <fixed-case>VQA</fixed-case></title>
      <author><first>Rui</first><last>Cao</last></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Singapore Management University</affiliation></author>
      <pages>533-549</pages>
      <abstract>Previous solutions to knowledge-based visual question answering (K-VQA) retrieve knowledge from external knowledge bases and use supervised learning to train the K-VQA model.Recently pre-trained LLMs have been used as both a knowledge source and a zero-shot QA model for K-VQA and demonstrated promising results.However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability.Inspired by recent work on knowledge generation from LLMs for text-based QA, in this work we propose and test a similar knowledge-generation-based K-VQA method, which first generates knowledge from an LLM and then incorporates the generated knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA benchmarks and found that our method performs better than previous zero-shot K-VQA methods and our generated knowledge is generally relevant and helpful.</abstract>
      <url hash="fbd74890">2024.findings-eacl.36</url>
      <attachment type="software" hash="6164d06c">2024.findings-eacl.36.software.zip</attachment>
      <bibkey>cao-jiang-2024-knowledge</bibkey>
      <video href="2024.findings-eacl.36.mp4"/>
    </paper>
    <paper id="37">
      <title>Simple Temperature Cool-down in Contrastive Framework for Unsupervised Sentence Representation Learning</title>
      <author><first>Yoo Hyun</first><last>Jeong</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Myeong Soo</first><last>Han</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Dong-Kyu</first><last>Chae</last><affiliation>Hanyang University</affiliation></author>
      <pages>550-559</pages>
      <abstract>In this paper, we proposes a simple, tricky method to improve sentence representation of unsupervised contrastive learning. Even though contrastive learning has achieved great performances in both visual representation learning (VRL) and sentence representation learning (SRL) fields, we focus on the fact that there is a gap between characteristics and training dynamics of VRL and SRL. We first examine the role of temperature to bridge the gap between VRL and SRL, and find some temperature-dependent elements in SRL; <i>i.e.</i>, a higher temperature causes overfitting of the uniformity while improving the alignment in earlier phase of training. Then, we design a <i>temperature cool-down</i> technique based on this observation, which helps PLMs to be more suitable for contrastive learning via preparation of uniform representation space. Our experimental results on widely-utilized benchmarks demonstrate the effectiveness and extensiblity of our method.</abstract>
      <url hash="13d445d4">2024.findings-eacl.37</url>
      <bibkey>jeong-etal-2024-simple</bibkey>
      <video href="2024.findings-eacl.37.mp4"/>
    </paper>
    <paper id="38">
      <title>Bootstrap Your Own <fixed-case>PLM</fixed-case>: Boosting Semantic Features of <fixed-case>PLM</fixed-case>s for Unsuperivsed Contrastive Learning</title>
      <author><first>Yoo Hyun</first><last>Jeong</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Myeong Soo</first><last>Han</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Dong-Kyu</first><last>Chae</last><affiliation>Hanyang University</affiliation></author>
      <pages>560-569</pages>
      <abstract>This paper aims to investigate the possibility of exploiting original semantic features of PLMs (pre-trained language models) during contrastive learning in the context of SRL (sentence representation learning). In the context of feature modification, we identified a method called IFM (implicit feature modification), which reduces the tendency of contrastive models for VRL (visual representation learning) to rely on feature-suppressing shortcut solutions. We observed that IFM did not work well for SRL, which may be due to differences between the nature of VRL and SRL. We propose BYOP, which boosts well-represented features, taking the opposite idea of IFM, under the assumption that SimCSE’s dropout-noise-based augmentation may be too simple to modify high-level semantic features, and that the features learned by PLMs are semantically meaningful and should be boosted, rather than removed. Extensive experiments lend credence to the logic of BYOP, which considers the nature of SRL.</abstract>
      <url hash="10ca921c">2024.findings-eacl.38</url>
      <bibkey>jeong-etal-2024-bootstrap</bibkey>
      <video href="2024.findings-eacl.38.mp4"/>
    </paper>
    <paper id="39">
      <title>Personalized Abstractive Summarization by Tri-agent Generation Pipeline</title>
      <author><first>Wen</first><last>Xiao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yujia</first><last>Xie</last><affiliation>Microsoft</affiliation></author>
      <author><first>Giuseppe</first><last>Carenini</last><affiliation>, University of British Columbia</affiliation></author>
      <author><first>Pengcheng</first><last>He</last><affiliation>Microsoft</affiliation></author>
      <pages>570-581</pages>
      <abstract>Tailoring outputs from large language models, like ChatGPT, to implicit user preferences remains a challenge despite their impressive generative capabilities. In this paper, we propose a tri-agent generation pipeline comprising a generator, an instructor, and an editor to enhance output personalization. The generator produces an initial output, the instructor automatically generates editing instructions based on user preferences, and the editor refines the output to align with those preferences. The inference-only large language model (ChatGPT) serves as both the generator and editor, with a smaller model acting as the instructor to guide output generation. We train the instructor using editor-steered reinforcement learning, leveraging feedback from a large-scale editor model to optimize instruction generation. Experimental results on two abstractive summarization datasets demonstrate the effectiveness of our approach in generating outputs that better meet user expectations.</abstract>
      <url hash="17757746">2024.findings-eacl.39</url>
      <bibkey>xiao-etal-2024-personalized</bibkey>
      <video href="2024.findings-eacl.39.mp4"/>
    </paper>
    <paper id="40">
      <title>Revisiting the <fixed-case>M</fixed-case>arkov Property for Machine Translation</title>
      <author><first>Cunxiao</first><last>Du</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Singapore Management University</affiliation></author>
      <pages>582-588</pages>
      <abstract>In this paper, we re-examine the Markov property in the context of neural machine translation. We design a Markov Autoregressive Transformer (MAT) and undertake a comprehensive assessment of its performance across four WMT benchmarks. Our findings indicate that MAT with an order larger than 4 can generate translations with quality on par with that of conventional autoregressive transformers. In addition, counter-intuitively, we also find that the advantages of utilizing a higher-order MAT do not specifically contribute to the translation of longer sentences.</abstract>
      <url hash="df419ec5">2024.findings-eacl.40</url>
      <bibkey>du-etal-2024-revisiting</bibkey>
      <video href="2024.findings-eacl.40.mp4"/>
    </paper>
    <paper id="41">
      <title>Reward Engineering for Generating Semi-structured Explanation</title>
      <author><first>Jiuzhou</first><last>Han</last></author>
      <author><first>Wray</first><last>Buntine</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University and University of Cambridge</affiliation></author>
      <pages>589-602</pages>
      <abstract>Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation. This explanation highlights how available information in a specific query is utilised and supplemented with information a reasoner produces from its internal weights towards generating an answer. Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model’s true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL). In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge, and then introduce a carefully crafted reward engineering method in reinforcement learning (RL) to better address this problem. We investigate multiple reward aggregation methods and provide a detailed discussion which sheds light on the promising potential of RL for future research. Our proposed method on two semi-structured explanation generation benchmarks (ExplaGraph and COPA-SSE) achieves new state-of-the-art results.</abstract>
      <url hash="b3d3ccc3">2024.findings-eacl.41</url>
      <bibkey>han-etal-2024-reward</bibkey>
      <video href="2024.findings-eacl.41.mp4"/>
    </paper>
    <paper id="42">
      <title>Towards Context-Based Violence Detection: A <fixed-case>K</fixed-case>orean Crime Dialogue Dataset</title>
      <author><first>Minju</first><last>Kim</last></author>
      <author><first>Heuiyeen</first><last>Yeen</last></author>
      <author><first>Myoung-Wan</first><last>Koo</last><affiliation>Sogang University</affiliation></author>
      <pages>603-623</pages>
      <abstract>In order to enhance the security of society, there is rising interest in artificial intelligence (AI) to help detect and classify in advanced violence in daily life. The field of violence detection has introduced various datasets, yet context-based violence detection predominantly focuses on vision data, with a notable lack of NLP datasets. To overcome this, this paper presents the first Korean dialogue dataset for classifying violence that occurs in online settings: the Korean Crime Dialogue Dataset (KCDD). KCDD contains 22,249 dialogues created by crowd workers assuming offline scenarios. It has four criminal classes that meet international legal standards and one clean class (Serious Threats, Extortion or Blackmail, Harassment in the Workplace, Other Harassment, and Clean Dialogue). Plus, we propose a strong baseline for the proposed dataset, Relationship-Aware BERT. The model shows that understanding varying relationships among interlocutors improves the performance of crime dialogue classification. We hope that the proposed dataset will be used to detect cases of violence and aid people in danger. The KCDD dataset and corresponding baseline implementations can be found at the following link: <url>https://sites.google.com/view/kcdd</url>.</abstract>
      <url hash="f6301672">2024.findings-eacl.42</url>
      <attachment type="software" hash="e13f50ad">2024.findings-eacl.42.software.zip</attachment>
      <attachment type="note" hash="0af93109">2024.findings-eacl.42.note.zip</attachment>
      <bibkey>kim-etal-2024-towards</bibkey>
    </paper>
    <paper id="43">
      <title>Capturing the Relationship Between Sentence Triplets for <fixed-case>LLM</fixed-case> and Human-Generated Texts to Enhance Sentence Embeddings</title>
      <author><first>Na Min</first><last>An</last><affiliation>KAIST</affiliation></author>
      <author><first>Sania</first><last>Waheed</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>624-638</pages>
      <abstract>Deriving meaningful sentence embeddings is crucial in capturing the semantic relationship between texts. Recent advances in building sentence embedding models have centered on replacing traditional human-generated text datasets with those generated by LLMs. However, the properties of these widely used LLM-generated texts remain largely unexplored. Here, we evaluate the quality of the LLM-generated texts from four perspectives (Positive Text Repetition, Length Difference Penalty, Positive Score Compactness, and Negative Text Implausibility) and find that there exists an inherent difference between human and LLM-generated datasets. To further enhance sentence embeddings using both human and LLM-generated datasets, we propose a novel loss function that incorporates Positive-Negative sample Augmentation (PNA) within the contrastive learning objective. Our results demonstrate that PNA effectively mitigates the sentence anisotropy problem in Wikipedia corpus (-7% compared to CLHAIF) and simultaneously improves the Spearman’s correlation in standard Semantic Textual Similarity (STS) tasks (+1.47% compared to CLHAIF).</abstract>
      <url hash="5023b599">2024.findings-eacl.43</url>
      <bibkey>an-etal-2024-capturing</bibkey>
      <video href="2024.findings-eacl.43.mp4"/>
    </paper>
    <paper id="44">
      <title>Harmonizing Code-mixed Conversations: Personality-assisted Code-mixed Response Generation in Dialogues</title>
      <author><first>Shivani</first><last>Kumar</last><affiliation>Indraprastha Institute of Information Technology, Delhi, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>639-653</pages>
      <abstract>Code-mixing, the blending of multiple languages within a single conversation, introduces a distinctive challenge, particularly in the context of response generation. Capturing the intricacies of code-mixing proves to be a formidable task, given the wide-ranging variations influenced by individual speaking styles and cultural backgrounds. In this study, we explore response generation within code-mixed conversations. We introduce a novel approach centered on harnessing the Big Five personality traits acquired in an unsupervised manner from the conversations to bolster the performance of response generation. These inferred personality attributes are seamlessly woven into the fabric of the dialogue context, using a novel fusion mechanism, . It uses an effective two-step attention formulation to fuse the dialogue and personality information. This fusion not only enhances the contextual relevance of generated responses but also elevates the overall performance of the model. Our experimental results, grounded in a dataset comprising of multi-party Hindi-English code-mix conversations, highlight the substantial advantages offered by personality-infused models over their conventional counterparts. This is evident in the increase observed in ROUGE and BLUE scores for the response generation task when the identified personality is seamlessly integrated into the dialogue context. Qualitative assessment for personality identification and response generation aligns well with our quantitative results.</abstract>
      <url hash="6210c1f5">2024.findings-eacl.44</url>
      <bibkey>kumar-chakraborty-2024-harmonizing</bibkey>
      <video href="2024.findings-eacl.44.mp4"/>
    </paper>
    <paper id="45">
      <title>Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning</title>
      <author><first>Jeongwoo</first><last>Park</last></author>
      <author><first>Enrico</first><last>Liscio</last></author>
      <author><first>Pradeep</first><last>Murukannaiah</last><affiliation>Delft University of Technology</affiliation></author>
      <pages>654-673</pages>
      <abstract>Recent advances in NLP show that language models retain a discernible level of knowledge in deontological ethics and moral norms. However, existing works often treat morality as binary, ranging from right to wrong. This simplistic view does not capture the nuances of moral judgment. Pluralist moral philosophers argue that human morality can be deconstructed into a finite number of elements, respecting individual differences in moral judgment. In line with this view, we build a pluralist moral sentence embedding space via a state-of-the-art contrastive learning approach. We systematically investigate the embedding space by studying the emergence of relationships among moral elements, both quantitatively and qualitatively. Our results show that a pluralist approach to morality can be captured in an embedding space. However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels.</abstract>
      <url hash="7a5007bf">2024.findings-eacl.45</url>
      <attachment type="software" hash="935b473d">2024.findings-eacl.45.software.zip</attachment>
      <bibkey>park-etal-2024-morality</bibkey>
      <video href="2024.findings-eacl.45.mp4"/>
    </paper>
    <paper id="46">
      <title>Prosody in Cascade and Direct Speech-to-Text Translation: a case study on <fixed-case>K</fixed-case>orean Wh-Phrases</title>
      <author><first>Giulio</first><last>Zhou</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Tsz Kin</first><last>Lam</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <pages>674-683</pages>
      <abstract>Speech-to-Text Translation (S2TT) has typically been addressed with cascade systems, where speech recognition systems generate a transcription that is subsequently passed to a translation model. While there has been a growing interest in developing direct speech translation systems to avoid propagating errors and losing non-verbal content, prior work in direct S2TT has struggled to conclusively establish the advantages of integrating the acoustic signal directly into the translation process. This work proposes using contrastive evaluation to quantitatively measure the ability of direct S2TT systems to disambiguate utterances where prosody plays a crucial role. Specifically, we evaluated Korean-English translation systems on a test set containing wh-phrases, for which prosodic features are necessary to produce translations with the correct intent, whether it’s a statement, a yes/no question, a wh-question, and more. Our results clearly demonstrate the value of direct translation systems over cascade translation models, with a notable 12.9% improvement in overall accuracy in ambiguous cases, along with up to a 15.6% increase in F1 scores for one of the major intent categories. To the best of our knowledge, this work stands as the first to provide quantitative evidence that direct S2TT models can effectively leverage prosody. The code for our evaluation is openly accessible and freely available for review and utilisation.</abstract>
      <url hash="5135cf37">2024.findings-eacl.46</url>
      <bibkey>zhou-etal-2024-prosody</bibkey>
      <video href="2024.findings-eacl.46.mp4"/>
    </paper>
    <paper id="47">
      <title>Exploring the Potential of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations</title>
      <author><first>Chunkit</first><last>Chan</last></author>
      <author><first>Cheng</first><last>Jiayang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Weiqi</first><last>Wang</last></author>
      <author><first>Yuxin</first><last>Jiang</last></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>684-721</pages>
      <abstract>This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT’s promising performance across various tasks, we proceed to carry out thorough evaluations on the whole test sets of 11 datasets, including temporal and causal relations, PDTB2.0-based, and dialogue-based discourse relations. To ensure the reliability of our findings, we employ three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. Through our study, we discover that ChatGPT exhibits exceptional proficiency in detecting and reasoning about causal relations, albeit it may not possess the same level of expertise in identifying the temporal order between two events. While it is capable of identifying the majority of discourse relations with existing explicit discourse connectives, the implicit discourse relation remains a formidable challenge. Concurrently, ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.</abstract>
      <url hash="375a167d">2024.findings-eacl.47</url>
      <bibkey>chan-etal-2024-exploring</bibkey>
      <video href="2024.findings-eacl.47.mp4"/>
    </paper>
    <paper id="48">
      <title>Backtracing: Retrieving the Cause of the Query</title>
      <author><first>Rose</first><last>Wang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Pawan</first><last>Wirawarn</last></author>
      <author><first>Omar</first><last>Khattab</last></author>
      <author><first>Noah</first><last>Goodman</last><affiliation>Stanford University</affiliation></author>
      <author><first>Dorottya</first><last>Demszky</last><affiliation>Stanford University</affiliation></author>
      <pages>722-735</pages>
      <abstract>Many online content portals allow users to ask questions to supplement their understanding (e.g., of lectures). While information retrieval (IR) systems may provide answers for such user queries, they do not directly assist content creators—such as lecturers who want to improve their content—identify segments that caused a user to ask those questions.We introduce the task of backtracing, in which systems retrieve the text segment that most likely caused a user query.We formalize three real-world domains for which backtracing is important in improving content delivery and communication: understanding the cause of (a) student confusion in the Lecture domain, (b) reader curiosity in the News Article domain, and (c) user emotion in the Conversation domain.We evaluate the zero-shot performance of popular information retrieval methods and language modeling methods, including bi-encoder, re-ranking and likelihood-based methods and ChatGPT.While traditional IR systems retrieve semantically relevant information (e.g., details on “projection matrices” for a query “does projecting multiple times still lead to the same point?”), they often miss the causally relevant context (e.g., the lecturer states “projecting twice gets me the same answer as one projection”). Our results show that there is room for improvement on backtracing and it requires new retrieval approaches.We hope our benchmark serves to improve future retrieval systems for backtracing, spawning systems that refine content generation and identify linguistic triggers influencing user queries.</abstract>
      <url hash="b176d238">2024.findings-eacl.48</url>
      <bibkey>wang-etal-2024-backtracing</bibkey>
      <video href="2024.findings-eacl.48.mp4"/>
    </paper>
    <paper id="49">
      <title>Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling</title>
      <author><first>Chao-Wei</first><last>Huang</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chen-An</first><last>Li</last><affiliation>Department of computer science and informational engineering, National Taiwan University</affiliation></author>
      <author><first>Tsu-Yuan</first><last>Hsu</last></author>
      <author><first>Chen-Yu</first><last>Hsu</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Yun-Nung</first><last>Chen</last><affiliation>Department of Computer Science and Informational Engineering, National Taiwan University</affiliation></author>
      <pages>736-746</pages>
      <abstract>Dense retrieval methods have demonstrated promising performance in multilingual information retrieval, where queries and documents can be in different languages. However, dense retrievers typically require a substantial amount of paired data, which poses even greater challenges in multilingual scenarios. This paper introduces <tex-math>\textbf{UMR}</tex-math>, an <tex-math>\underline{U}</tex-math>nsupervised <tex-math>\underline{M}</tex-math>ultilingual dense <tex-math>\underline{R}</tex-math>etriever trained without any paired data. Our approach leverages the sequence likelihood estimation capabilities of multilingual language models to acquire pseudo labels for training dense retrievers. We propose a two-stage framework which iteratively improves the performance of multilingual dense retrievers. Experimental results on two benchmark datasets show that UMR outperforms supervised baselines, showcasing the potential of training multilingual retrievers without paired data, thereby enhancing their practicality. All of our source code, data, and models are available: https://github.com/MiuLab/UMR</abstract>
      <url hash="9afec971">2024.findings-eacl.49</url>
      <attachment type="software" hash="a6b3b82e">2024.findings-eacl.49.software.zip</attachment>
      <bibkey>huang-etal-2024-unsupervised</bibkey>
      <video href="2024.findings-eacl.49.mp4"/>
    </paper>
    <paper id="50">
      <title>Investigating grammatical abstraction in language models using few-shot learning of novel noun gender</title>
      <author><first>Priyanka</first><last>Sukumaran</last></author>
      <author><first>Conor</first><last>Houghton</last><affiliation>University of Bristol</affiliation></author>
      <author><first>Nina</first><last>Kazanina</last><affiliation>University of Bristol</affiliation></author>
      <pages>747-765</pages>
      <abstract>Humans can learn a new word and infer its grammatical properties from very few examples. They have an abstract notion of linguistic properties like grammatical gender and agreement rules that can be applied to novel syntactic contexts and words. Drawing inspiration from psycholinguistics, we conduct a noun learning experiment to assess whether an LSTM and a decoder-only transformer can achieve human-like abstraction of grammatical gender in French. Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context. We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts, albeit with a bias for the masculine gender category. Importantly, the few-shot updates were only applied to the embedding layers, demonstrating that models encode sufficient gender information within the word-embedding space. While the generalisation behaviour of models suggests that they represent grammatical gender as an abstract category, like humans, further work is needed to explore the details of how exactly this is implemented. For a comparative perspective with human behaviour, we conducted an analogous one-shot novel noun gender learning experiment, which revealed that native French speakers, like language models, also exhibited a masculine gender bias and are not excellent one-shot learners either.</abstract>
      <url hash="8b217871">2024.findings-eacl.50</url>
      <bibkey>sukumaran-etal-2024-investigating</bibkey>
    </paper>
    <paper id="51">
      <title>On-the-fly Denoising for Data Augmentation in Natural Language Understanding</title>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>Zoom</affiliation></author>
      <author><first>Fangyu</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>766-781</pages>
      <abstract>Data Augmentation (DA) is frequently used to provide additional training data without extra human annotation automatically.However, data augmentation may introduce noisy data that impairs training.To guarantee the quality of augmented data,existing methods either assume no noise exists in the augmented data and adopt consistency training or use simple heuristics such as training loss and diversity constraints to filter out “noisy” data.However, those filtered examples may still contain useful information, and dropping them completely causes a loss of supervision signals.In this paper, based on the assumption that the original dataset is cleaner than the augmented data, we propose an on-the-fly denoising technique for data augmentation that learns from soft augmented labels provided by an organic teacher model trained on the cleaner original data.To further prevent overfitting on noisy labels, a simple self-regularization module is applied to force the model prediction to be consistent across two distinct dropouts.Our method can be applied to general augmentation techniques and consistently improve the performance on both text classification and question-answering tasks.</abstract>
      <url hash="def6cbc2">2024.findings-eacl.51</url>
      <attachment type="software" hash="f0ea4f26">2024.findings-eacl.51.software.zip</attachment>
      <attachment type="note" hash="7205c223">2024.findings-eacl.51.note.zip</attachment>
      <bibkey>fang-etal-2024-fly</bibkey>
      <video href="2024.findings-eacl.51.mp4"/>
    </paper>
    <paper id="52">
      <title>Style Vectors for Steering Generative Large Language Models</title>
      <author><first>Kai</first><last>Konen</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Sophie</first><last>Jentzsch</last><affiliation>German Aerospace Center (DLR)</affiliation></author>
      <author><first>Diaoulé</first><last>Diallo</last><affiliation>German Aerospace Center (DLR)</affiliation></author>
      <author><first>Peer</first><last>Schütt</last><affiliation>German Aerospace Center</affiliation></author>
      <author><first>Oliver</first><last>Bensch</last><affiliation>German Aerospace Center (DLR)</affiliation></author>
      <author><first>Roxanne</first><last>El Baff</last><affiliation>German Aerospace Center and Bauhaus-University Weimar</affiliation></author>
      <author><first>Dominik</first><last>Opitz</last><affiliation>German Aerospace Center (DLR)</affiliation></author>
      <author><first>Tobias</first><last>Hecking</last><affiliation>German Aerospace Center</affiliation></author>
      <pages>782-802</pages>
      <abstract>This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.</abstract>
      <url hash="a02d4eb5">2024.findings-eacl.52</url>
      <attachment type="software" hash="29f876dd">2024.findings-eacl.52.software.zip</attachment>
      <bibkey>konen-etal-2024-style</bibkey>
      <video href="2024.findings-eacl.52.mp4"/>
    </paper>
    <paper id="53">
      <title>Consistent Joint Decision-Making with Heterogeneous Learning Models</title>
      <author><first>Hossein</first><last>Rajaby Faghihi</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>803-813</pages>
      <abstract>This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming(ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions’ prior probability, confidence (uncertainty), and the models’ expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.</abstract>
      <url hash="4dc934db">2024.findings-eacl.53</url>
      <bibkey>rajaby-faghihi-kordjamshidi-2024-consistent</bibkey>
      <video href="2024.findings-eacl.53.mp4"/>
    </paper>
    <paper id="54">
      <title>Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage</title>
      <author><first>Hanyin</first><last>Shao</last></author>
      <author><first>Jie</first><last>Huang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Shen</first><last>Zheng</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Kevin</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>814-825</pages>
      <abstract>The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts. These findings underscore the potential risk to PII confidentiality posed by the evolving capabilities of LLMs, especially as they continue to expand in scale and power.</abstract>
      <url hash="0e1d4951">2024.findings-eacl.54</url>
      <bibkey>shao-etal-2024-quantifying</bibkey>
    </paper>
    <paper id="55">
      <title>Probing Critical Learning Dynamics of <fixed-case>PLM</fixed-case>s for Hate Speech Detection</title>
      <author><first>Sarah</first><last>Masud</last><affiliation>Indraprastha Institute of Information Technology Delhi (IIIT-Delhi)</affiliation></author>
      <author><first>Mohammad Aflah</first><last>Khan</last></author>
      <author><first>Vikram</first><last>Goyal</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Md Shad</first><last>Akhtar</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>826-845</pages>
      <abstract>Despite the widespread adoption, there is a lack of research into how various critical aspects of pretrained language models (PLMs) affect their performance in hate speech detection. Through five research questions, our findings and recommendations lay the groundwork for empirically investigating different aspects of PLMs’ use in hate speech detection. We deep dive into comparing different pretrained models, evaluating their seed robustness, finetuning settings, and the impact of pretraining data collection time. Our analysis reveals early peaks for downstream tasks during pretraining, the limited benefit of employing a more recent pretraining corpus, and the significance of specific layers during finetuning. We further call into question the use of domain-specific models and highlight the need for dynamic datasets for benchmarking hate speech detection.</abstract>
      <url hash="9b9e33fb">2024.findings-eacl.55</url>
      <attachment type="software" hash="5e4d61a2">2024.findings-eacl.55.software.zip</attachment>
      <bibkey>masud-etal-2024-probing</bibkey>
      <video href="2024.findings-eacl.55.mp4"/>
    </paper>
    <paper id="56">
      <title>Embible: Reconstruction of <fixed-case>A</fixed-case>ncient <fixed-case>H</fixed-case>ebrew and <fixed-case>A</fixed-case>ramaic Texts Using Transformers</title>
      <author><first>Niv</first><last>Fono</last></author>
      <author><first>Harel</first><last>Moshayof</last></author>
      <author><first>Eldar</first><last>Karol</last></author>
      <author><first>Itai</first><last>Assraf</last></author>
      <author><first>Mark</first><last>Last</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <pages>846-852</pages>
      <abstract>Hebrew and Aramaic inscriptions serve as an essential source of information on the ancient history of the Near East. Unfortunately, some parts of the inscribed texts become illegible over time. Special experts, called epigraphists, use time-consuming manual procedures to estimate the missing content. This problem can be considered an extended masked language modeling task, where the damaged content can comprise single characters, character n-grams (partial words), single complete words, and multi-word n-grams.This study is the first attempt to apply the masked language modeling approach to corrupted inscriptions in Hebrew and Aramaic languages, both using the Hebrew alphabet consisting mostly of consonant symbols. In our experiments, we evaluate several transformer-based models, which are fine-tuned on the Biblical texts and tested on three different percentages of randomly masked parts in the testing corpus. For any masking percentage, the highest text completion accuracy is obtained with a novel ensemble of word and character prediction models.</abstract>
      <url hash="a4fcfdf5">2024.findings-eacl.56</url>
      <bibkey>fono-etal-2024-embible</bibkey>
      <video href="2024.findings-eacl.56.mp4"/>
    </paper>
    <paper id="57">
      <title>Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling</title>
      <author><first>Qingyang</first><last>Wu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>853-867</pages>
      <abstract>Transformer models have achieved great performance in dialogue generation tasks. However, their inability to process long dialogue history often leads to truncation of the context. To address this problem, we propose a novel memory-augmented transformer that is compatible with existing pre-trained encoder-decoder models and enables efficient preservation of the dialogue history information. The new model incorporates a separate memory module alongside the pre-trained transformer, which can effectively interchange information between the memory states and the current input context. We evaluate the efficiency of our model on three dialogue datasets and two language modeling datasets. Experimental results show that our method has achieved superior efficiency and performance compared to other pre-trained Transformer baselines.</abstract>
      <url hash="98a3579c">2024.findings-eacl.57</url>
      <attachment type="software" hash="83a1986c">2024.findings-eacl.57.software.zip</attachment>
      <bibkey>wu-yu-2024-stateful</bibkey>
    </paper>
    <paper id="58">
      <title>The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models</title>
      <author><first>Anton</first><last>Razzhigaev</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Matvey</first><last>Mikhalchuk</last><affiliation>Artificial Intelligence Research Institute (AIRI) and Lomonosov Moscow State University</affiliation></author>
      <author><first>Elizaveta</first><last>Goncharova</last></author>
      <author><first>Ivan</first><last>Oseledets</last><affiliation>Artificial Intelligence Research Institute and Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Denis</first><last>Dimitrov</last><affiliation>AIRI and Sber</affiliation></author>
      <author><first>Andrey</first><last>Kuznetsov</last><affiliation>Samara National Research University</affiliation></author>
      <pages>868-874</pages>
      <abstract>In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. This fact is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.</abstract>
      <url hash="3436b782">2024.findings-eacl.58</url>
      <bibkey>razzhigaev-etal-2024-shape</bibkey>
      <video href="2024.findings-eacl.58.mp4"/>
    </paper>
    <paper id="59">
      <title><fixed-case>MED</fixed-case>s for <fixed-case>PET</fixed-case>s: Multilingual Euphemism Disambiguation for Potentially Euphemistic Terms</title>
      <author><first>Patrick</first><last>Lee</last></author>
      <author><first>Alain</first><last>Chirino Trujillo</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Diana</first><last>Cuevas Plancarte</last></author>
      <author><first>Olumide</first><last>Ojo</last></author>
      <author><first>Xinyi</first><last>Liu</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Iyanuoluwa</first><last>Shode</last></author>
      <author><first>Yuan</first><last>Zhao</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Anna</first><last>Feldman</last><affiliation>Montclair State University</affiliation></author>
      <author><first>Jing</first><last>Peng</last><affiliation>Montclair State University</affiliation></author>
      <pages>875-881</pages>
      <abstract>Euphemisms are found across the world’s languages, making them a universal linguistic phenomenon. As such, euphemistic data may have useful properties for computational tasks across languages. In this study, we explore this premise by training a multilingual transformer model (XLM-RoBERTa) to disambiguate potentially euphemistic terms (PETs) in multilingual and cross-lingual settings. In line with current trends, we demonstrate that zero-shot learning across languages takes place. We also show cases where multilingual models perform better on the task compared to monolingual models by a statistically significant margin, indicating that multilingual data presents additional opportunities for models to learn about cross-lingual, computational properties of euphemisms. In a follow-up analysis, we focus on universal euphemistic “categories” such as death and bodily functions among others. We test to see whether cross-lingual data of the same domain is more important than within-language data of other domains to further understand the nature of the cross-lingual transfer.</abstract>
      <url hash="23037a84">2024.findings-eacl.59</url>
      <bibkey>lee-etal-2024-meds</bibkey>
    </paper>
    <paper id="60">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>E</fixed-case>xplainer: Explaining Language Models through Prompt-based Learning</title>
      <author><first>Zijian</first><last>Feng</last></author>
      <author><first>Hanzhang</first><last>Zhou</last></author>
      <author><first>Zixiao</first><last>Zhu</last></author>
      <author><first>Kezhi</first><last>Mao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>882-895</pages>
      <abstract>Pretrained language models have become workhorses for various natural language processing (NLP) tasks, sparking a growing demand for enhanced interpretability and transparency. However, prevailing explanation methods, such as attention-based and gradient-based strategies, largely rely on linear approximations, potentially causing inaccuracies such as accentuating irrelevant input tokens. To mitigate the issue, we develop PromptExplainer, a novel method for explaining language models through prompt-based learning. PromptExplainer aligns the explanation process with the masked language modeling (MLM) task of pretrained language models and leverages the prompt-based learning framework for explanation generation. It disentangles token representations into the explainable embedding space using the MLM head and extracts discriminative features with a verbalizer to generate class-dependent explanations. Extensive experiments demonstrate that PromptExplainer significantly outperforms state-of-the-art explanation methods.</abstract>
      <url hash="fd7afc75">2024.findings-eacl.60</url>
      <attachment type="software" hash="e7e35714">2024.findings-eacl.60.software.zip</attachment>
      <bibkey>feng-etal-2024-promptexplainer</bibkey>
      <video href="2024.findings-eacl.60.mp4"/>
    </paper>
    <paper id="61">
      <title>Do-Not-Answer: Evaluating Safeguards in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Xudong</first><last>Han</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>896-911</pages>
      <abstract>With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to identify potential risks through the evaluation of “dangerous capabilities” in order to responsibly deploy LLMs. Here we aim to facilitate this process. In particular, we collect an open-source dataset to evaluate the safeguards in LLMs, to facilitate the deployment of safer open-source LLMs at a low cost. Our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. We assess the responses of six popular LLMs to these instructions, and we find that simple BERT-style classifiers can achieve results that are comparable to GPT-4 on automatic safety evaluation. Our data and code are available at https://github.com/Libr-AI/do-not-answer</abstract>
      <url hash="3329c299">2024.findings-eacl.61</url>
      <bibkey>wang-etal-2024-answer</bibkey>
      <video href="2024.findings-eacl.61.mp4"/>
    </paper>
    <paper id="62">
      <title>Do Language Models Know When They’re Hallucinating References?</title>
      <author><first>Ayush</first><last>Agrawal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mirac</first><last>Suzgun</last><affiliation>Stanford University</affiliation></author>
      <author><first>Lester</first><last>Mackey</last><affiliation>Microsoft Research New England</affiliation></author>
      <author><first>Adam</first><last>Kalai</last></author>
      <pages>912-928</pages>
      <abstract>State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda. In this work, we focus on hallucinated book and article references and present them as the “model organism” of language model hallucination research, due to their frequent and easy-to-discern nature. We posit that if a language model cites a particular reference in its output, then it should ideally possess sufficient information about its authors and content, among other relevant details. Using this basic insight, we illustrate that one can identify hallucinated references without ever consulting any external resources, by asking a set of direct or indirect queries to the language model about the references. These queries can be considered as “consistency checks.” Our findings highlight that while LMs, including GPT-4, often produce inconsistent author lists for hallucinated references, they also often accurately recall the authors of real references. In this sense, the LM can be said to “know” when it is hallucinating references. Furthermore, these findings show how hallucinated references can be dissected to shed light on their nature.</abstract>
      <url hash="1d49754a">2024.findings-eacl.62</url>
      <attachment type="software" hash="2c355a56">2024.findings-eacl.62.software.zip</attachment>
      <attachment type="note" hash="35db54a9">2024.findings-eacl.62.note.zip</attachment>
      <bibkey>agrawal-etal-2024-language</bibkey>
      <video href="2024.findings-eacl.62.mp4"/>
    </paper>
    <paper id="63">
      <title>Bridging Cultural Nuances in Dialogue Agents through Cultural Value Surveys</title>
      <author><first>Yong</first><last>Cao</last></author>
      <author><first>Min</first><last>Chen</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Daniel</first><last>Hershcovich</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>929-945</pages>
      <abstract>The cultural landscape of interactions with dialogue agents is a compelling yet relatively unexplored territory. It’s clear that various sociocultural aspects—from communication styles and beliefs to shared metaphors and knowledge—profoundly impact these interactions. To delve deeper into this dynamic, we introduce cuDialog, a first-of-its-kind benchmark for dialogue generation with a cultural lens. We also develop baseline models capable of extracting cultural attributes from dialogue exchanges, with the goal of enhancing the predictive accuracy and quality of dialogue agents. To effectively co-learn cultural understanding and multi-turn dialogue predictions, we propose to incorporate cultural dimensions with dialogue encoding features. Our experimental findings highlight that incorporating cultural value surveys boosts alignment with references and cultural markers, demonstrating its considerable influence on personalization and dialogue quality. To facilitate further exploration in this exciting domain, we publish our benchmark publicly accessible at https://github.com/yongcaoplus/cuDialog.</abstract>
      <url hash="37d09e4b">2024.findings-eacl.63</url>
      <bibkey>cao-etal-2024-bridging</bibkey>
    </paper>
    <paper id="64">
      <title><fixed-case>CEO</fixed-case>: Corpus-based Open-Domain Event Ontology Induction</title>
      <author><first>Nan</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Jianshu</first><last>Chen</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>946-964</pages>
      <abstract>Existing event-centric NLP models often only apply to the pre-defined ontology, which significantly restricts their generalization capabilities.This paper presents CEO, a novel Corpus-based Event Ontology induction model to relax the restriction imposed by pre-defined event ontologies. Without direct supervision, CEO leverages distant supervision from available summary datasets to detect corpus-wise salient events and exploits external event knowledge to force events within a short distance to have close embeddings. Experiments on three popular event datasets show that the schema induced by CEO has better coverage and higher accuracy than previous methods. Moreover, CEO is the first event ontology induction model that can induce a hierarchical event ontology with meaningful names on eleven open-domain corpora, making the induced schema more trustworthy and easier to be further curated. We anonymously release our dataset, codes, and induced ontology.</abstract>
      <url hash="6a6fd189">2024.findings-eacl.64</url>
      <bibkey>xu-etal-2024-ceo</bibkey>
    </paper>
    <paper id="65">
      <title>Rethinking <fixed-case>STS</fixed-case> and <fixed-case>NLI</fixed-case> in Large Language Models</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Minghan</first><last>Wang</last><affiliation>Monash University</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>965-982</pages>
      <abstract>Recent years, have seen the rise of large language models (LLMs), where practitioners use task-specific prompts; this was shown to be effective for a variety of tasks. However, when applied to semantic textual similarity (STS) and natural language inference (NLI), the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements. With this in mind, here we try to rethink STS and NLI in the era of LLMs. We first evaluate the performance of STS and NLI in the clinical/biomedical domain, and then we assess LLMs’ predictive confidence and their capability of capturing collective human opinions. We find that these old problems are still to be properly addressed in the era of LLMs.</abstract>
      <url hash="4cbaa3d3">2024.findings-eacl.65</url>
      <bibkey>wang-etal-2024-rethinking</bibkey>
      <video href="2024.findings-eacl.65.mp4"/>
    </paper>
    <paper id="66">
      <title>Learning High-Quality and General-Purpose Phrase Representations</title>
      <author><first>Lihu</first><last>Chen</last></author>
      <author><first>Gael</first><last>Varoquaux</last><affiliation>INRIA</affiliation></author>
      <author><first>Fabian</first><last>Suchanek</last><affiliation>Telecom Paris</affiliation></author>
      <pages>983-994</pages>
      <abstract>Phrase representations play an important role in data science and natural language processing, benefiting various tasks like Entity Alignment, Record Linkage, Fuzzy Joins, and Paraphrase Classification.The current state-of-the-art method involves fine-tuning pre-trained language models for phrasal embeddings using contrastive learning. However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences.Second, leveraging the phrase type and morphology gives phrase representations that are both more precise and more flexible.We propose an improved framework to learn phrase representations in a context-free fashion.The framework employs phrase type classification as an auxiliary task and incorporates character-level information more effectively into the phrase representation.Furthermore, we design three granularities of data augmentation to increase the diversity of training samples.Our experiments across a wide range of tasks reveal that our approach generates superior phrase embeddings compared to previous methods while requiring a smaller model size.</abstract>
      <url hash="da034f83">2024.findings-eacl.66</url>
      <attachment type="software" hash="55439295">2024.findings-eacl.66.software.zip</attachment>
      <bibkey>chen-etal-2024-learning</bibkey>
      <video href="2024.findings-eacl.66.mp4"/>
    </paper>
    <paper id="67">
      <title>Explaining Language Model Predictions with High-Impact Concepts</title>
      <author><first>Ruochen</first><last>Zhao</last></author>
      <author><first>Tan</first><last>Wang</last></author>
      <author><first>Yongjie</first><last>Wang</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>995-1012</pages>
      <abstract>To encourage fairness and transparency, there exists an urgent demand for deriving reliable explanations for large language models (LLMs). One promising solution is concept-based explanations, i.e., human-understandable concepts from internal representations. However, due to the compositional nature of languages, current methods mostly discover correlational explanations instead of causal features. Therefore, we propose a novel framework to provide impact-aware explanations for users to understand the LLM’s behavior, which are robust to feature changes and influential to the model’s predictions. Specifically, we extract predictive high-level features (concepts) from the model’s hidden layer activations. Then, we innovatively optimize for features whose existence causes the output predictions to change substantially. Extensive experiments on real and synthetic tasks demonstrate that our method achieves superior results on predictive impact, explainability, and faithfulness compared to the baselines, especially for LLMs.</abstract>
      <url hash="5970a8fc">2024.findings-eacl.67</url>
      <bibkey>zhao-etal-2024-explaining</bibkey>
    </paper>
    <paper id="68">
      <title>Understanding and Mitigating Spurious Correlations in Text Classification with Neighborhood Analysis</title>
      <author><first>Oscar</first><last>Chew</last></author>
      <author><first>Hsuan-Tien</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Kuan-Hao</first><last>Huang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <pages>1013-1025</pages>
      <abstract>Recent research has revealed that machine learning models have a tendency to leverage spurious correlations that exist in the training set but may not hold true in general circumstances. For instance, a sentiment classifier may erroneously learn that the token “performances” is commonly associated with positive movie reviews.Relying on these spurious correlations degrades the classifier’s performance when it deploys on out-of-distribution data.In this paper, we examine the implications of spurious correlations through a novel perspective called neighborhood analysis. The analysis uncovers how spurious correlations lead unrelated words to erroneously cluster together in the embedding space. Driven by the analysis, we design a metric to detect spurious tokens and also propose a family of regularization methods, NFL (doN’t Forget your Language) to mitigate spurious correlations in text classification.Experiments show that NFL can effectively prevent erroneous clusters and significantly improve the robustness of classifiers without auxiliary data. The code is publicly available at https://github.com/oscarchew/doNt-Forget-your-Language.</abstract>
      <url hash="ac912e09">2024.findings-eacl.68</url>
      <bibkey>chew-etal-2024-understanding</bibkey>
      <video href="2024.findings-eacl.68.mp4"/>
    </paper>
    <paper id="69">
      <title>On the Intractability to Synthesize Factual Inconsistencies in Summarization</title>
      <author><first>Ge</first><last>Luo</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Weisi</first><last>Fan</last></author>
      <author><first>Miaoran</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Youbiao</first><last>He</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Yinfei</first><last>Yang</last><affiliation>Apple</affiliation></author>
      <author><first>Forrest</first><last>Bao</last><affiliation>Iowa State University, Iowa State University and Iowa State University</affiliation></author>
      <pages>1026-1037</pages>
      <abstract>Factual consistency detection has gotten raised attention in the task of abstractive summarization. Many existing works rely on synthetic training data, which may not accurately reflect or match the inconsistencies produced by summarization models. In this paper, we first systematically analyze the shortcomings of the current methods in synthesizing inconsistent summaries. Current synthesis methods may fail to produce inconsistencies of coreference errors and discourse errors, per our quantitative and qualitative study. Then, employing the parameter-efficient finetuning (PEFT) technique, we discover that a competitive factual consistency detector can be achieved using thousands of real model-generated summaries with human annotations. Our study demonstrates the importance of real machine-generated texts with human annotation in NLG evaluation as our model outperforms the SOTA on the CoGenSumm, FactCC, Frank, and SummEval datasets.</abstract>
      <url hash="814be2d6">2024.findings-eacl.69</url>
      <bibkey>luo-etal-2024-intractability</bibkey>
    </paper>
    <paper id="70">
      <title><fixed-case>I</fixed-case>ndi<fixed-case>V</fixed-case>ec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators</title>
      <author><first>Luyang</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lingzhi</first><last>Wang</last></author>
      <author><first>Xiaoyan</first><last>Zhao</last><affiliation>Chinese University of Hong Kong, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>1038-1050</pages>
      <abstract>This study focuses on media bias detection, crucial in today’s era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input’s bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions). Experimental results on four political bias datasets highlight IndiVec’s significant superiority over baselines. Furthermore, additional experiments and analysis provide profound insights into the framework’s effectiveness.</abstract>
      <url hash="05911d09">2024.findings-eacl.70</url>
      <bibkey>lin-etal-2024-indivec</bibkey>
      <video href="2024.findings-eacl.70.mp4"/>
    </paper>
    <paper id="71">
      <title>Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?</title>
      <author><first>Rishav</first><last>Hada</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>Varun</first><last>Gumma</last><affiliation>Microsoft</affiliation></author>
      <author><first>Adrian</first><last>Wynter</last></author>
      <author><first>Harshita</first><last>Diddee</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Mohamed</first><last>Ahmed</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Kalika</first><last>Bali</last><affiliation>Microsoft Research Labs</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>1051-1070</pages>
      <abstract>Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations. Employing LLMs as evaluators to rank or score other models’ outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks. In this study, we explore the potential of LLM-based evaluators in enhancing multilingual evaluation by calibrating them against 20K human judgments across three text-generation tasks, five metrics, and eight languages. Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.</abstract>
      <url hash="24bd4a12">2024.findings-eacl.71</url>
      <bibkey>hada-etal-2024-large</bibkey>
      <video href="2024.findings-eacl.71.mp4"/>
    </paper>
    <paper id="72">
      <title>Computational Morphology and Lexicography Modeling of <fixed-case>M</fixed-case>odern <fixed-case>S</fixed-case>tandard <fixed-case>A</fixed-case>rabic Nominals</title>
      <author><first>Christian</first><last>Khairallah</last><affiliation>New York University</affiliation></author>
      <author><first>Reham</first><last>Marzouk</last><affiliation>Alexandria University and New York University, Abu Dhabi</affiliation></author>
      <author><first>Salam</first><last>Khalifa</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Mayar</first><last>Nassar</last><affiliation>Ain Shams University</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <pages>1071-1084</pages>
      <abstract>Modern Standard Arabic (MSA) nominals present many morphological and lexical modeling challenges that have not been consistently addressed previously. This paper attempts to define the space of such challenges, and leverage a recently proposed morphological framework to build a comprehensive and extensible model for MSA nominals. Our model design addresses the nominals’ intricate morphotactics, as well as their paradigmatic irregularities. Our implementation showcases enhanced accuracy and consistency compared to a commonly used MSA morphological analyzer and generator. We make our models publicly available.</abstract>
      <url hash="4372130f">2024.findings-eacl.72</url>
      <bibkey>khairallah-etal-2024-computational</bibkey>
      <video href="2024.findings-eacl.72.mp4"/>
    </paper>
    <paper id="73">
      <title>Relabeling Minimal Training Subset to Flip a Prediction</title>
      <author><first>Jinghan</first><last>Yang</last></author>
      <author><first>Linjie</first><last>Xu</last></author>
      <author><first>Lequan</first><last>Yu</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>1085-1098</pages>
      <abstract>When facing an unsatisfactory prediction from a machine learning model, users can be interested in investigating the underlying reasons and exploring the potential for reversing the outcome. We ask: To flip the prediction on a test point <tex-math>x_t</tex-math>, how to identify the smallest training subset <tex-math>\mathcal{S}_t</tex-math> that we need to relabel?We propose an efficient algorithm to identify and relabel such a subset via an extended influence function for binary classification models with convex loss.We find that relabeling fewer than 2% of the training points can always flip a prediction.This mechanism can serve multiple purposes: (1) providing an approach to challenge a model prediction by altering training points; (2) evaluating model robustness with the cardinality of the subset (i.e., <tex-math>|\mathcal{S}_t|</tex-math>); we show that <tex-math>|\mathcal{S}_t|</tex-math> is highly related to the noise ratio in the training set and <tex-math>|\mathcal{S}_t|</tex-math> is correlated with but complementary to predicted probabilities; and (3) revealing training points lead to group attribution bias. To the best of our knowledge, we are the first to investigate identifying and relabeling the minimal training subset required to flip a given prediction.</abstract>
      <url hash="57da3035">2024.findings-eacl.73</url>
      <bibkey>yang-etal-2024-relabeling</bibkey>
      <video href="2024.findings-eacl.73.mp4"/>
    </paper>
    <paper id="74">
      <title>Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models</title>
      <author><first>Sachin</first><last>Pawar</last></author>
      <author><first>Nitin</first><last>Ramrakhiyani</last><affiliation>International Institute of Information Technology, Hyderabad and Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Anubhav</first><last>Sinha</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Manoj</first><last>Apte</last></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <pages>1099-1114</pages>
      <abstract>In this paper, we propose a novel two-step technique for text classification using autoregressive Language Models (LM). In the first step, a set of perplexity and log-likelihood based numeric features are elicited from an LM for a text instance to be classified. Then, in the second step, a classifier based on these features is trained to predict the final label. The classifier used is usually a simple machine learning classifier like Support Vector Machine (SVM) or Logistic Regression (LR) and it is trained using a small set of training examples. We believe, our technique presents a whole new way of exploiting the available training instances, in addition to the existing ways like fine-tuning LMs or in-context learning. Our approach stands out by eliminating the need for parameter updates in LMs, as required in fine-tuning, and does not impose limitations on the number of training examples faced while building prompts for in-context learning. We evaluate our technique across 5 different datasets and compare with multiple competent baselines.</abstract>
      <url hash="c83c1003">2024.findings-eacl.74</url>
      <bibkey>pawar-etal-2024-generate</bibkey>
      <video href="2024.findings-eacl.74.mp4"/>
    </paper>
    <paper id="75">
      <title>Autism Detection in Speech – A Survey</title>
      <author><first>Nadine</first><last>Probol</last></author>
      <author><first>Margot</first><last>Mieskes</last><affiliation>University of Applied Sciences Darmstadt</affiliation></author>
      <pages>1115-1125</pages>
      <abstract>There has been a range of studies of how autism is displayed in voice, speech, and language. We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues. Our survey looks at all three domains. We define autism and which comorbidities might influence the correct detection of the disorder. We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate. We also show word-based approaches and describe machine learning and transformer-based approaches both on the audio data as well as the transcripts. Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched. Also, most NLP research focuses on traditional machine learning methods instead of transformers. Additionally, we were unable to find research combining both features from audio and transcripts.</abstract>
      <url hash="51af6a1c">2024.findings-eacl.75</url>
      <bibkey>probol-mieskes-2024-autism</bibkey>
      <video href="2024.findings-eacl.75.mp4"/>
    </paper>
    <paper id="76">
      <title>Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary Tasks</title>
      <author><first>Danae</first><last>Sanchez Villegas</last></author>
      <author><first>Daniel</first><last>Preotiuc-Pietro</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Nikolaos</first><last>Aletras</last><affiliation>University of Sheffield, University of Sheffield and Amazon</affiliation></author>
      <pages>1126-1137</pages>
      <abstract>Effectively leveraging multimodal information from social media posts is essential to various downstream tasks such as sentiment analysis, sarcasm detection or hate speech classification. Jointly modeling text and images is challenging because cross-modal semantics might be hidden or the relation between image and text is weak. However, prior work on multimodal classification of social media posts has not yet addressed these challenges. In this work, we present an extensive study on the effectiveness of using two auxiliary losses jointly with the main task during fine-tuning multimodal models. First, Image-Text Contrastive (ITC) is designed to minimize the distance between image-text representations within a post, thereby effectively bridging the gap between posts where the image plays an important role in conveying the post’s meaning. Second, Image-Text Matching (ITM) enhances the model’s ability to understand the semantic relationship between images and text, thus improving its capacity to handle ambiguous or loosely related posts. We combine these objectives with five multimodal models, demonstrating consistent improvements of up to 2.6 F1 score across five diverse social media datasets. Our comprehensive analysis shows the specific scenarios where each auxiliary task is most effective.</abstract>
      <url hash="43ecdf60">2024.findings-eacl.76</url>
      <bibkey>sanchez-villegas-etal-2024-improving</bibkey>
    </paper>
    <paper id="77">
      <title>What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition</title>
      <author><first>Carolin</first><last>Holtermann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Markus</first><last>Frohmann</last></author>
      <author><first>Navid</first><last>Rekabsaz</last></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>1138-1157</pages>
      <abstract>The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge composition strategies. In particular, we test two module combination methods and five selection and weighting strategies for their effectiveness and efficiency in an extensive experimental setup. Our results highlight the efficacy of ensembling but also hint at the power of simple though often-ignored weighting methods. Further in-depth analyses allow us to understand the role of weighting vs. top-k selection, and show that, to a certain extent, the performance of adapter composition can even be predicted.</abstract>
      <url hash="1265d531">2024.findings-eacl.77</url>
      <attachment type="software" hash="a178fa2c">2024.findings-eacl.77.software.zip</attachment>
      <bibkey>holtermann-etal-2024-weight</bibkey>
      <video href="2024.findings-eacl.77.mp4"/>
    </paper>
    <paper id="78">
      <title><fixed-case>I</fixed-case>ndi<fixed-case>F</fixed-case>ood<fixed-case>VQA</fixed-case>: Advancing Visual Question Answering and Reasoning with a Knowledge-Infused Synthetic Data Generation Pipeline</title>
      <author><first>Pulkit</first><last>Agarwal</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Settaluri</first><last>Sravanthi</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>1158-1176</pages>
      <abstract>Large Vision Language Models (VLMs) like GPT-4, LLaVA, and InstructBLIP exhibit extraordinary capabilities for both knowledge understanding and reasoning. However, the reasoning capabilities of such models on sophisticated problems that require external knowledge of a specific domain have not been assessed well, due to the unavailability of necessary datasets. In this work, we release a first-of-its-kind dataset called IndiFoodVQA with around 16.7k data samples, consisting of explicit knowledge-infused questions, answers, and reasons. We also release IndiFoodKG, a related Knowledge Graph (KG) with 79k triples. The data has been created with minimal human intervention via an automated pipeline based on InstructBlip and GPT-3.5. We also present a methodology to extract knowledge from the KG and use it to both answer and reason upon the questions. We employ different models to report baseline zero-shot and fine-tuned results. Fine-tuned VLMs on our data showed an improvement of ~25% over the corresponding base model, highlighting the fact that current VLMs need domain-specific fine-tuning to excel in specialized settings. Our findings reveal that (1) explicit knowledge infusion during question generation helps in making questions that have more grounded knowledge, and (2) proper knowledge retrieval can often lead to better-answering potential in such cases. The data and code is available at https://github.com/SLSravanthi/IndifoodVQA.</abstract>
      <url hash="f3aa97e4">2024.findings-eacl.78</url>
      <attachment type="note" hash="c8051c81">2024.findings-eacl.78.note.zip</attachment>
      <bibkey>agarwal-etal-2024-indifoodvqa</bibkey>
    </paper>
    <paper id="79">
      <title><fixed-case>MAPLE</fixed-case>: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verification</title>
      <author><first>Xia</first><last>Zeng</last><affiliation>Queen Mary University London</affiliation></author>
      <author><first>Arkaitz</first><last>Zubiaga</last><affiliation>Queen Mary University of London</affiliation></author>
      <pages>1177-1196</pages>
      <abstract>Claim verification is an essential step in the automated fact-checking pipeline which assesses the veracity of a claim against a piece of evidence. In this work, we explore the potential of few-shot claim verification, where only very limited data is available for supervision. We propose MAPLE (Micro Analysis of Pairwise Language Evolution), a pioneering approach that explores the alignment between a claim and its evidence with a small seq2seq model and a novel semantic measure. Its innovative utilization of micro language evolution path leverages unlabelled pairwise data to facilitate claim verification while imposing low demand on data annotations and computing resources. MAPLE demonstrates significant performance improvements over SOTA baselines SEED, PET and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and SciFact. Data and code are available.</abstract>
      <url hash="07de59b9">2024.findings-eacl.79</url>
      <attachment type="software" hash="19d09dc0">2024.findings-eacl.79.software.zip</attachment>
      <attachment type="note" hash="60b3c1f3">2024.findings-eacl.79.note.zip</attachment>
      <bibkey>zeng-zubiaga-2024-maple</bibkey>
      <video href="2024.findings-eacl.79.mp4"/>
    </paper>
    <paper id="80">
      <title>Leveraging Open Information Extraction for More Robust Domain Transfer of Event Trigger Detection</title>
      <author><first>David</first><last>Dukić</last><affiliation>Faculty of Electrical Engineering and Computing, University of Zagreb</affiliation></author>
      <author><first>Kiril</first><last>Gashteovski</last><affiliation>NEC Laboratories Europe, St.Cyril and Methodius University and NEC Laboratories Europe</affiliation></author>
      <author><first>Goran</first><last>Glavaš</last><affiliation>Julius-Maximilians-Universität Würzburg</affiliation></author>
      <author><first>Jan</first><last>Snajder</last><affiliation>UniZg-FER, University of Zagreb</affiliation></author>
      <pages>1197-1213</pages>
      <abstract>Event detection is a crucial information extraction task in many domains, such as Wikipedia or news. The task typically relies on trigger detection (TD) – identifying token spans in the text that evoke specific events. While the notion of triggers should ideally be universal across domains, domain transfer for TD from high- to low-resource domains results in significant performance drops. We address the problem of negative transfer in TD by coupling triggers between domains using subject-object relations obtained from a rule-based open information extraction (OIE) system. We demonstrate that OIE relations injected through multi-task training can act as mediators between triggers in different domains, enhancing zero- and few-shot TD domain transfer and reducing performance drops, in particular when transferring from a high-resource source domain (Wikipedia) to a low(er)-resource target domain (news). Additionally, we combine this improved transfer with masked language modeling on the target domain, observing further TD transfer gains. Finally, we demonstrate that the gains are robust to the choice of the OIE system.</abstract>
      <url hash="32442344">2024.findings-eacl.80</url>
      <bibkey>dukic-etal-2024-leveraging</bibkey>
      <video href="2024.findings-eacl.80.mp4"/>
    </paper>
    <paper id="81">
      <title>Exploring efficient zero-shot synthetic dataset generation for Information Retrieval</title>
      <author><first>Tiago</first><last>Almeida</last></author>
      <author><first>Sérgio</first><last>Matos</last><affiliation>Universidade de Aveiro</affiliation></author>
      <pages>1214-1231</pages>
      <abstract>The broad integration of neural retrieval models into Information Retrieval (IR) systems is significantly impeded by the high cost and laborious process associated with the manual labelling of training data. Similarly, synthetic training data generation, a potential workaround, often requires expensive computational resources due to the reliance on large language models. This work explored the potential of small language models for efficiently creating high-quality synthetic datasets to train neural retrieval models. We aim to identify an optimal method to generate synthetic datasets, enabling training neural reranking models in document collections where annotated data is unavailable. We introduce a novel methodology, grounded in the principles of information theory, to select the most appropriate documents to be used as context for question generation. Then, we employ a small language model for zero-shot conditional question generation, supplemented by a filtering mechanism to ensure the quality of generated questions. Extensive evaluation on five datasets unveils the potential of our approach, outperforming unsupervised retrieval methods such as BM25 and pretrained monoT5. Our findings indicate that an efficiently generated “silver-standard” dataset allows effective training of neural rerankers in unlabeled scenarios. To ensure reproducibility and facilitate wider application, we will release a code repository featuring an accessible API for zero-shot synthetic question generation.</abstract>
      <url hash="a7485aef">2024.findings-eacl.81</url>
      <bibkey>almeida-matos-2024-exploring</bibkey>
    </paper>
    <paper id="82">
      <title>Clustering-based Sampling for Few-Shot Cross-Domain Keyphrase Extraction</title>
      <author><first>Prakamya</first><last>Mishra</last><affiliation>AMD AI</affiliation></author>
      <author><first>Lincy</first><last>Pattanaik</last></author>
      <author><first>Arunima</first><last>Sundar</last></author>
      <author><first>Nishant</first><last>Yadav</last><affiliation>Department of Computer Science, University of Massachusetts, Amherst</affiliation></author>
      <author><first>Mayank</first><last>Kulkarni</last><affiliation>Amazon</affiliation></author>
      <pages>1232-1250</pages>
      <abstract>Keyphrase extraction is the task of identifying a set of keyphrases present in a document that captures its most salient topics. Scientific domain-specific pre-training has led to achieving state-of-the-art keyphrase extraction performance with a majority of benchmarks being within the domain. In this work, we explore how to effectively enable the cross-domain generalization capabilities of such models without requiring the same scale of data. We primarily focus on the few-shot setting in non-scientific domain datasets such as OpenKP from the Web domain &amp; StackEx from the StackExchange forum. We propose to leverage topic information intrinsically available in the data, to build a novel clustering-based sampling approach that facilitates selecting a few samples to label from the target domain facilitating building robust and performant models. This approach leads to large gains in performance of up to 26.35 points in F1 when compared to selecting few-shot samples uniformly at random. We also explore the setting where we have access to labeled data from the model’s pretraining domain corpora and perform gradual training which involves slowly folding in target domain data to the source domain data. Here we demonstrate further improvements in the model performance by up to 12.76 F1 points.</abstract>
      <url hash="91306589">2024.findings-eacl.82</url>
      <bibkey>mishra-etal-2024-clustering</bibkey>
      <video href="2024.findings-eacl.82.mp4"/>
    </paper>
    <paper id="83">
      <title>Random Smooth-based Certified Defense against Text Adversarial Attack</title>
      <author><first>Zeliang</first><last>Zhang</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Wei</first><last>Yao</last></author>
      <author><first>Susan</first><last>Liang</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Chenliang</first><last>Xu</last><affiliation>University of Rochester, University of Rochester and University of Rochester</affiliation></author>
      <pages>1251-1265</pages>
      <abstract>Certified defense methods have identified their effectiveness against textual adversarial examples, which train models on the worst-case text generated by substituting words in original texts with synonyms. However, due to the discrete word embedding representations, the large search space hinders the robust training efficiency, resulting in significant time consumption. To overcome this challenge, motivated by the observation that synonym embedding has a small distance, we propose to treat the word substitution as a continuous perturbation on the word embedding representation. The proposed method Text-RS applies random smooth techniques to approximate the word substitution operation, offering a computationally efficient solution that outperforms conventional discrete methods and improves the robustness in training. The evaluation results demonstrate its effectiveness in defending against multiple textual adversarial attacks.</abstract>
      <url hash="563c62a2">2024.findings-eacl.83</url>
      <bibkey>zhang-etal-2024-random</bibkey>
      <video href="2024.findings-eacl.83.mp4"/>
    </paper>
    <paper id="84">
      <title>Clarifying the Path to User Satisfaction: An Investigation into Clarification Usefulness</title>
      <author><first>Hossein A.</first><last>Rahmani</last></author>
      <author><first>Xi</first><last>Wang</last></author>
      <author><first>Mohammad</first><last>Aliannejadi</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Mohammadmehdi</first><last>Naghiaei</last></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>1266-1277</pages>
      <abstract>Clarifying questions are an integral component of modern information retrieval systems, directly impacting user satisfaction and overall system performance. Poorly formulated questions can lead to user frustration and confusion, negatively affecting the system’s performance. This research addresses the urgent need to identify and leverage key features that contribute to the classification of clarifying questions, enhancing user satisfaction. To gain deeper insights into how different features influence user satisfaction, we conduct a comprehensive analysis, considering a broad spectrum of lexical, semantic, and statistical features, such as question length and sentiment polarity. Our empirical results provide three main insights into the qualities of effective query clarification: (1) specific questions are more effective than generic ones; (2) the subjectivity and emotional tone of a question play a role; and (3) shorter and more ambiguous queries benefit significantly from clarification. Based on these insights, we implement feature-integrated user satisfaction prediction using various classifiers, both traditional and neural-based, including random forest, BERT, and large language models. Our experiments show a consistent and significant improvement, particularly in traditional classifiers, with a minimum performance boost of 45%. This study presents invaluable guidelines for refining the formulation of clarifying questions and enhancing both user satisfaction and system performance.</abstract>
      <url hash="25b6e579">2024.findings-eacl.84</url>
      <bibkey>rahmani-etal-2024-clarifying</bibkey>
    </paper>
    <paper id="85">
      <title>Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning</title>
      <author><first>Lifu</first><last>Tu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Jin</first><last>Qu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Semih</first><last>Yavuz</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Wenhao</first><last>Liu</last><affiliation>Faire</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Yingbo</first><last>Zhou</last><affiliation>Salesforce Research</affiliation></author>
      <pages>1278-1294</pages>
      <abstract>Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce for cross-lingual alignment pretraining, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains about 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model’s cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent classification. Our results demonstrate strong and efficient modeling ability of NLI-based classifiers and the large cross-lingual transfer improvements achieved by our aligned prompts, particularly in few-shot settings. We also conduct studies on large language models (LLMs) such as text-davinci-003 and ChatGPT in both zero- and few-shot settings. While LLMs exhibit impressive performance in English, their cross-lingual capabilities in other languages, particularly low-resource ones, are limited.</abstract>
      <url hash="26a32271">2024.findings-eacl.85</url>
      <bibkey>tu-etal-2024-efficiently</bibkey>
      <video href="2024.findings-eacl.85.mp4"/>
    </paper>
    <paper id="86">
      <title>Correcting Language Model Outputs by Editing Salient Layers</title>
      <author><first>Kshitij</first><last>Mishra</last><affiliation>Indian Institute of Technology, Patna</affiliation></author>
      <author><first>Tamer</first><last>Soliman</last><affiliation>Amazon</affiliation></author>
      <author><first>Anil</first><last>Ramakrishna</last><affiliation>Amazon</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Anoop</first><last>Kumar</last><affiliation>Amazon</affiliation></author>
      <pages>1295-1305</pages>
      <abstract>Large language models can accumulate incorrect or outdated knowledge as the real world evolves. Compared to typical solutions such as retraining, retrieval augmented generation, model editing offers an effective yet low cost solution to address this issue. However, existing model editing algorithms employ manual selection of edit layers, which requires prior domain knowledge or expensive architecture-specific empirical layer selection methods, such as causal tracing. In this work, we propose SaLEM (Salient Layers Editing Model), an efficient solution for data driven layer selection for the model editing task. Our solution utilizes layer-wise saliency maps for layer selection, and matches the accuracy of prior approaches but with only 1/3 of their edits, enabling efficient updates to the parametric knowledge in large language models.</abstract>
      <url hash="d27cffde">2024.findings-eacl.86</url>
      <bibkey>mishra-etal-2024-correcting</bibkey>
    </paper>
    <paper id="87">
      <title>Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback</title>
      <author><first>Nikhil</first><last>Mehta</last></author>
      <author><first>Milagro</first><last>Teruel</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xin</first><last>Deng</last></author>
      <author><first>Sergio</first><last>Figueroa Sanz</last></author>
      <author><first>Ahmed</first><last>Awadallah</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Julia</first><last>Kiseleva</last><affiliation>Research, Microsoft</affiliation></author>
      <pages>1306-1321</pages>
      <abstract>Many approaches to Natural Language Processing tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaboration.In this paper, we investigate these directions using the challenging task established by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We delve into multiple types of help players can give to the AI to guide it and analyze the impact of this help on behavior, resulting in performance improvements and an end-to-end interactive system.</abstract>
      <url hash="49597c49">2024.findings-eacl.87</url>
      <bibkey>mehta-etal-2024-improving</bibkey>
      <video href="2024.findings-eacl.87.mp4"/>
    </paper>
    <paper id="88">
      <title>Goodhart’s Law Applies to <fixed-case>NLP</fixed-case>’s Explanation Benchmarks</title>
      <author><first>Jennifer</first><last>Hsia</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Danish</first><last>Pruthi</last><affiliation>Indian Institute of Science, Bangalore</affiliation></author>
      <author><first>Aarti</first><last>Singh</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zachary</first><last>Lipton</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>1322-1335</pages>
      <abstract>Despite the rising popularity of saliency-based explanations, the research community remains at an impasse, facing doubts concerning their purpose, efficacy, and tendency to contradict each other. Seeking to unite the community’s efforts around common goals, several recent works have proposed evaluation metrics. In this paper, we critically examine two sets of metrics: the ERASER metrics (comprehensiveness and sufficiency) and the EVAL-X metrics, focusing our inquiry on natural language processing. First, we show that we can inflate a model’s comprehensiveness and sufficiency scores dramatically without altering its predictions or explanations on in-distribution test inputs. Our strategy exploits the tendency for extracted explanations and their complements to be “out-of-support” relative to each other and in-distribution inputs. Next, we demonstrate that the EVAL-X metrics can be inflated arbitrarily by a simple method that encodes the label, even though EVAL-X is precisely motivated to address such exploits. Our results raise doubts about the ability of current metrics to guide explainability research, underscoring the need for a broader reassessment of what precisely these metrics are intended to capture.</abstract>
      <url hash="e6cc8838">2024.findings-eacl.88</url>
      <bibkey>hsia-etal-2024-goodharts</bibkey>
      <video href="2024.findings-eacl.88.mp4"/>
    </paper>
    <paper id="89">
      <title>Syllable-level lyrics generation from melody exploiting character-level language model</title>
      <author><first>Zhe</first><last>Zhang</last><affiliation>NII, SOKENDAI</affiliation></author>
      <author><first>Karol</first><last>Lasocki</last></author>
      <author><first>Yi</first><last>Yu</last><affiliation>NII, National Institute of Informatics</affiliation></author>
      <author><first>Atsuhiro</first><last>Takasu</last><affiliation>SOKENDAI</affiliation></author>
      <pages>1336-1346</pages>
      <abstract>The generation of lyrics tightly connected to accompanying melodies involves establishing a mapping between musical notes and syllables of lyrics. This process requires a deep understanding of music constraints and semantic patterns at syllable-level, word-level, and sentence-level semantic meanings. However, pre-trained language models specifically designed at the syllable level are publicly unavailable. To solve these challenging issues, we propose to exploit fine-tuning character-level language models for syllable-level lyrics generation from symbolic melody. In particular, our method aims to fine-tune a character-level pre-trained language model, allowing to incorporation of linguistic knowledge of the language model into the beam search process of a syllable-level Transformer generator network. Besides, by exploring ChatGPT-based evaluation of generated lyrics in addition to human subjective evaluation, we prove that our approach improves the coherence and correctness of generated lyrics, without the need to train expensive new language models.</abstract>
      <url hash="5a393d93">2024.findings-eacl.89</url>
      <bibkey>zhang-etal-2024-syllable</bibkey>
      <video href="2024.findings-eacl.89.mp4"/>
    </paper>
    <paper id="90">
      <title>Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca</title>
      <author><first>Pinzhen</first><last>Chen</last><affiliation>University of Edinburgh and University of Edinburgh</affiliation></author>
      <author><first>Shaoxiong</first><last>Ji</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Nikolay</first><last>Bogoychev</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Andrey</first><last>Kutuzov</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Barry</first><last>Haddow</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>1347-1356</pages>
      <abstract>Foundational large language models (LLMs) can be instruction-tuned to perform open-domain question answering, facilitating applications like chat assistants. While such efforts are often carried out in a single language, we empirically analyze cost-efficient strategies for multilingual scenarios. Our study employs the Alpaca dataset and machine translations of it to form multilingual data, which is then used to tune LLMs through either low-rank adaptation or full-parameter training. Under a controlled computation budget, comparisons show that multilingual tuning is on par or better than tuning a model for each language. Furthermore, multilingual tuning with downsampled data can be as powerful and more robust. Our findings serve as a guide for expanding language support through instruction tuning.</abstract>
      <url hash="f30017a2">2024.findings-eacl.90</url>
      <bibkey>chen-etal-2024-monolingual</bibkey>
      <video href="2024.findings-eacl.90.mp4"/>
    </paper>
    <paper id="91">
      <title>Prompt Perturbation Consistency Learning for Robust Language Models</title>
      <author><first>Yao</first><last>Qiang</last><affiliation>Wayne State University</affiliation></author>
      <author><first>Subhrangshu</first><last>Nandi</last><affiliation>Amazon</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Greg</first><last>Ver Steeg</last><affiliation>University of California, Riverside, Amazon and USC/ISI</affiliation></author>
      <author><first>Anoop</first><last>Kumar</last><affiliation>Amazon</affiliation></author>
      <author><first>Anna</first><last>Rumshisky</last><affiliation>University of Massachusetts, Lowell, University of Massachusetts at Lowell and Amazon</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California and Amazon Alexa</affiliation></author>
      <pages>1357-1370</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbation Consistency Learning (PPCL), which works by regularizing the divergence between losses from clean and perturbed samples. Our experiments show that PPCL can recover on an average 59% and 69% of the performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats data augmentation approach while using ten times fewer augmented data samples.</abstract>
      <url hash="90de46fe">2024.findings-eacl.91</url>
      <bibkey>qiang-etal-2024-prompt</bibkey>
      <video href="2024.findings-eacl.91.mp4"/>
    </paper>
    <paper id="92">
      <title>Enhancing Society-Undermining Disinformation Detection through Fine-Grained Sentiment Analysis Pre-Finetuning</title>
      <author><first>Tsung-Hsuan</first><last>Pan</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chung-Chi</first><last>Chen</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hen-Hsen</first><last>Huang</last><affiliation>Institute of Information Science, Academia Sinica</affiliation></author>
      <author><first>Hsin-Hsi</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>1371-1377</pages>
      <abstract>In the era of the digital world, while freedom of speech has been flourishing, it has also paved the way for disinformation, causing detrimental effects on society. Legal and ethical criteria are insufficient to address this concern, thus necessitating technological intervention. This paper presents a novel method leveraging pre-finetuning concept for efficient detection and removal of disinformation that may undermine society, as deemed by judicial entities. We argue the importance of detecting this type of disinformation and validate our approach with real-world data derived from court orders. Following a study that highlighted four areas of interest for rumor analysis, our research proposes the integration of a fine-grained sentiment analysis task in the pre-finetuning phase of language models, using the GoEmotions dataset. Our experiments validate the effectiveness of our approach in enhancing performance significantly. Furthermore, we explore the application of our approach across different languages using multilingual language models, showing promising results. To our knowledge, this is the first study that investigates the role of sentiment analysis pre-finetuning in disinformation detection.</abstract>
      <url hash="64499bf9">2024.findings-eacl.92</url>
      <bibkey>pan-etal-2024-enhancing</bibkey>
      <video href="2024.findings-eacl.92.mp4"/>
    </paper>
    <paper id="93">
      <title>Minimal Distillation Schedule for Extreme Language Model Compression</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Jiahao</first><last>Liu</last></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Wei</first><last>Wu</last><affiliation>Ant Research</affiliation></author>
      <author><first>Dawei</first><last>Song</last><affiliation>Beijing Institute of Technology and Open University</affiliation></author>
      <pages>1378-1394</pages>
      <abstract>Recent studies have revealed that language model distillation can become less effective when there is a significant capacity gap between the teacher and the student models. In order to bridge the gap, teacher assistant-based distillation has been introduced, in which the selection of the teacher assistant plays a crucial role in transferring knowledge from the teacher to the student. However, existing approaches for teacher assistant-based distillation require numerous trials to find the optimal teacher assistant.In this paper, we propose a novel approach called Minimal Distillation Schedule (MiniDisc), which enables the scheduling of an optimal teacher assistant in just one trial for extreme model compression (e.g, to 5% scale). In particular, we empirically show that the performance of the student is positively correlated with the scale-performance tradeoff of the teacher assistant. We then introduce a new <tex-math>\lambda</tex-math>-tradeoff metric that quantifies the optimality of the teacher assistant without the need for trial distillation to the student. By employing a sandwich framework, MiniDisc can select the optimal teacher assistant with the best <tex-math>\lambda</tex-math>-tradeoff.We extensively evaluate MiniDisc through a series of experiments on the GLUE benchmark. The results demonstrate that our approach achieved an improved efficiency compared to various state-of-the-art baselines. Furthermore, we showcase the scalability of MiniDisc by applying it to a language model with billions of parameters.</abstract>
      <url hash="036b40bb">2024.findings-eacl.93</url>
      <bibkey>zhang-etal-2024-minimal</bibkey>
    </paper>
    <paper id="94">
      <title>Event Semantic Classification in Context</title>
      <author><first>Haoyu</first><last>Wang</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Kaiqiang</first><last>Song</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <pages>1395-1407</pages>
      <abstract>In this work, we focus on a fundamental yet underexplored problem, event semantic classification in context, to help machines gain a deeper understanding of events. We classify events from six perspectives: modality, affirmation, specificity, telicity, durativity, and kinesis. These properties provide essential cues regarding the occurrence and grounding of events, changes of status that events can bring about, and the connection between events and time. To this end, this paper introduces a novel dataset collected for the semantic classification tasks and several effective models. By incorporating these event properties into downstream tasks, we demonstrate that understanding the fine-grained event semantics benefits downstream event understanding and reasoning via experiments on event extraction, temporal relation extraction, and subevent relation extraction.</abstract>
      <url hash="a9fb504c">2024.findings-eacl.94</url>
      <bibkey>wang-etal-2024-event</bibkey>
    </paper>
    <paper id="95">
      <title>Local and Global Contexts for Conversation</title>
      <author><first>Zuoquan</first><last>Lin</last><affiliation>Peking University</affiliation></author>
      <author><first>Xinyi</first><last>Shen</last><affiliation>Peking University</affiliation></author>
      <pages>1408-1418</pages>
      <abstract>The context in conversation is the dialog history crucial for multi-turn dialogue. Learning from the relevant contexts in dialog history for grounded conversation is a challenging problem. Local context is the most neighbor and more sensitive to the subsequent response, and global context is relevant to a whole conversation far beyond neighboring utterances. Currently, pretrained transformer models for conversation challenge capturing the correlation and connection between local and global contexts. We introduce a local and global conversation model (LGCM) for general-purpose conversation in open domain. It is a local-global hierarchical transformer model that excels at accurately discerning and assimilating the relevant contexts necessary for generating responses. It employs a local encoder to grasp the local context at the level of individual utterances and a global encoder to understand the broader context at the dialogue level. The seamless fusion of these locally and globally contextualized encodings ensures a comprehensive comprehension of the conversation. Experiments on popular datasets show that LGCM outperforms the existing conversation models on the performance of automatic metrics with significant margins.</abstract>
      <url hash="070044b6">2024.findings-eacl.95</url>
      <bibkey>lin-shen-2024-local</bibkey>
    </paper>
    <paper id="96">
      <title>Aspect-based Key Point Analysis for Quantitative Summarization of Reviews</title>
      <author><first>An</first><last>Tang</last></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Minh</first><last>Dinh</last></author>
      <pages>1419-1433</pages>
      <abstract>Key Point Analysis (KPA) is originally for summarizing arguments, where short sentences containing salient viewpoints are extracted as key points (KPs) and quantified for their prevalence as salience scores. Recently, KPA was applied to summarize reviews, but the study still relies on sentence-based KP extraction and matching, which leads to two issues: sentence-based extraction can result in KPs of overlapping opinions on the same aspects, and sentence-based matching of KP to review comment can be inaccurate, resulting in inaccurate salience scores. To address the above issues, in this paper, we propose Aspect-based Key Point Analysis (ABKPA), a novel framework for quantitative review summarization. Leveraging the readily available aspect-based sentiment analysis (ABSA) resources of reviews to automatically annotate silver labels for matching aspect-sentiment pairs, we propose a contrastive learning model to effectively match KPs to reviews and quantify KPs at the aspect level. Especially, the framework ensures extracting KP of distinct aspects and opinions, leading to more accurate opinion quantification. Experiments on five business categories of the popular Yelp review dataset show that ABKPA outperforms state-of-the-art baselines. Source code and data are available at: https://github.com/antangrocket1312/ABKPA</abstract>
      <url hash="a8a88d1d">2024.findings-eacl.96</url>
      <bibkey>tang-etal-2024-aspect</bibkey>
    </paper>
    <paper id="97">
      <title>Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders</title>
      <author><first>Yingji</first><last>Zhang</last></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Ian</first><last>Pratt-Hartmann</last><affiliation>University of Manchester, University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>University of Manchester</affiliation></author>
      <pages>1434-1450</pages>
      <abstract>Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoder (VQVAE) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAE to guide the self-attention mechanism in T5, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of control and preservation of semantic information across different tasks such as auto-encoding of sentences and mathematical expressions, text transfer, and inference. Moreover, T5VQVAE exhibits improved reasoning capabilities, suggesting potential applications for downstream natural language and symbolic inference tasks.</abstract>
      <url hash="170b94f0">2024.findings-eacl.97</url>
      <bibkey>zhang-etal-2024-improving</bibkey>
    </paper>
    <paper id="98">
      <title>High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models</title>
      <author><first>Michela</first><last>Lorandi</last><affiliation>Dublin City University</affiliation></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University and University of Aberdeen</affiliation></author>
      <pages>1451-1461</pages>
      <abstract>The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages. We explore the extent to which pretrained large language models (LLMs) can bridge this gap, via the example of data-to-text generation for Irish, Welsh, Breton and Maltese. We test LLMs on these under-resourced languages and English, in a range of scenarios. We find that LLMs easily set the state of the art for the under-resourced languages by substantial margins, as measured by both automatic and human evaluations. For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric’s suitability for evaluating non-task-specific systems. Overall, our results demonstrate the great potential of LLMs to bridge the performance gap for under-resourced languages.</abstract>
      <url hash="6a16680d">2024.findings-eacl.98</url>
      <bibkey>lorandi-belz-2024-high</bibkey>
      <video href="2024.findings-eacl.98.mp4"/>
    </paper>
    <paper id="99">
      <title>Antonym vs Synonym Distinction using <fixed-case>I</fixed-case>nterla<fixed-case>C</fixed-case>ed Encoder <fixed-case>NET</fixed-case>works (<fixed-case>ICE</fixed-case>-<fixed-case>NET</fixed-case>)</title>
      <author><first>Muhammad</first><last>Ali</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Yan</first><last>Hu</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Jianbin</first><last>Qin</last><affiliation>Shenzhen University</affiliation></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <pages>1462-1473</pages>
      <abstract>Antonyms vs synonyms distinction is a core challenge in lexico-semantic analysis and automated lexical resource construction. These pairs share a similar distributional context which makes it harder to distinguish them. Leading research in this regard attempts to capture the properties of the relation pairs, i.e., symmetry, transitivity, and trans-transitivity. However, the inability of existing research to appropriately model the relation-specific properties limits their end performance. In this paper, we propose InterlaCed Encoder NETworks (i.e., ICE-NET) for antonym vs synonym distinction, that aim to capture and model the relation-specific properties of the antonyms and synonyms pairs in order to perform the classification task in a performance-enhanced manner. Experimental evaluation using the benchmark datasets shows that ICE-NET outperforms the existing research by a relative score of upto 1.8% in F1-measure.</abstract>
      <url hash="a7dca423">2024.findings-eacl.99</url>
      <bibkey>ali-etal-2024-antonym</bibkey>
    </paper>
    <paper id="100">
      <title>Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity</title>
      <author><first>Eric</first><last>Khiu</last></author>
      <author><first>Hasti</first><last>Toossi</last></author>
      <author><first>David</first><last>Anugraha</last></author>
      <author><first>Jinyu</first><last>Liu</last></author>
      <author><first>Jiaxu</first><last>Li</last></author>
      <author><first>Juan</first><last>Flores</last><affiliation>Universidad de Guanajuato</affiliation></author>
      <author><first>Leandro</first><last>Roman</last></author>
      <author><first>A. Seza</first><last>Doğruöz</last><affiliation>Ghent University</affiliation></author>
      <author><first>En-Shiun</first><last>Lee</last></author>
      <pages>1474-1486</pages>
      <abstract>Fine-tuning and testing a multilingual large language model is a challenge for low-resource languages (LRLs) since it is an expensive process. While previous studies have predicted the performance of natural language processing (NLP) tasks using machine learning methods, they primarily focus on high-resource languages, overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate three factors (the size of the fine-tuning corpus, domain similarity between fine-tuning and testing corpora, and language similarity between source and target languages), which can potentially impact the model performance by using classical regression models. Our results indicate that domain similarity has the most important impact on predicting the performance of Machine Translation models.</abstract>
      <url hash="93d00c41">2024.findings-eacl.100</url>
      <attachment type="software" hash="6c430bf7">2024.findings-eacl.100.software.zip</attachment>
      <attachment type="note" hash="6b714bab">2024.findings-eacl.100.note.zip</attachment>
      <bibkey>khiu-etal-2024-predicting</bibkey>
      <revision id="1" href="2024.findings-eacl.100v1" hash="02411aaa"/>
      <revision id="2" href="2024.findings-eacl.100v2" hash="144d271f" date="2024-03-25">Include authors email address.</revision>
      <video href="2024.findings-eacl.100.mp4"/>
      <revision id="3" href="2024.findings-eacl.100v3" hash="93d00c41" date="2024-06-10">Update affiliation and watermark.</revision>
    </paper>
    <paper id="101">
      <title>Does <fixed-case>CLIP</fixed-case> Bind Concepts? Probing Compositionality in Large Image Models</title>
      <author><first>Martha</first><last>Lewis</last><affiliation>University of Bristol and University of Bristol</affiliation></author>
      <author><first>Nihal</first><last>Nayak</last><affiliation>Brown University</affiliation></author>
      <author><first>Peilin</first><last>Yu</last><affiliation>Brown University</affiliation></author>
      <author><first>Jack</first><last>Merullo</last><affiliation>Brown University</affiliation></author>
      <author><first>Qinan</first><last>Yu</last></author>
      <author><first>Stephen</first><last>Bach</last><affiliation>Computer Science Department, Brown University and Snorkel AI</affiliation></author>
      <author><first>Ellie</first><last>Pavlick</last><affiliation>Brown University</affiliation></author>
      <pages>1487-1500</pages>
      <abstract>Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ‘red cube’ by reasoning over the constituents ‘red’ and ‘cube’. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ‘cube behind sphere’ from ‘sphere behind cube’). To inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We benchmark them on three synthetic datasets – single-object, two-object, and relational – designed to test concept binding. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically. At the same time, CDSMs also perform poorly, with best performance at chance level.</abstract>
      <url hash="88b32c5a">2024.findings-eacl.101</url>
      <attachment type="software" hash="88b0c508">2024.findings-eacl.101.software.zip</attachment>
      <bibkey>lewis-etal-2024-clip</bibkey>
      <video href="2024.findings-eacl.101.mp4"/>
    </paper>
    <paper id="102">
      <title>Code-Switching and Back-Transliteration Using a Bilingual Model</title>
      <author><first>Daniel</first><last>Weisberg Mitelman</last><affiliation>Reichman University</affiliation></author>
      <author><first>Nachum</first><last>Dershowitz</last><affiliation>Tel Aviv University, Technion</affiliation></author>
      <author><first>Kfir</first><last>Bar</last><affiliation>Reichman University</affiliation></author>
      <pages>1501-1511</pages>
      <abstract>The challenges of automated transliteration and code-switching–detection in Judeo-Arabic texts are addressed. We introduce two novel machine-learning models, one focused on transliterating Judeo-Arabic into Arabic, and another aimed at identifying non-Arabic words, predominantly Hebrew and Aramaic. Unlike prior work, our models are based on a bilingual Arabic-Hebrew language model, providing a unique advantage in capturing shared linguistic nuances. Evaluation results show that our models outperform prior solutions for the same tasks. As a practical contribution, we present a comprehensive pipeline capable of taking Judeo-Arabic text, identifying non-Arabic words, and then transliterating the Arabic portions into Arabic script. This work not only advances the state of the art but also offers a valuable toolset for making Judeo-Arabic texts more accessible to a broader Arabic-speaking audience.</abstract>
      <url hash="d7cc1bdf">2024.findings-eacl.102</url>
      <bibkey>weisberg-mitelman-etal-2024-code</bibkey>
    </paper>
    <paper id="103">
      <title>Tsetlin Machine Embedding: Representing Words Using Logical Expressions</title>
      <author><first>Bimal</first><last>Bhattarai</last></author>
      <author><first>Ole-Christoffer</first><last>Granmo</last></author>
      <author><first>Lei</first><last>Jiao</last><affiliation>University of Agder</affiliation></author>
      <author><first>Rohan</first><last>Yadav</last></author>
      <author><first>Jivitesh</first><last>Sharma</last><affiliation>Norwegian Institute for Air Research</affiliation></author>
      <pages>1512-1522</pages>
      <abstract>Embedding words in vector space is a fundamental first step in state-of-the-art natural language processing (NLP). Typical NLP solutions employ pre-defined vector representations to improve generalization by co-locating similar words in vector space. For instance, Word2Vec is a self-supervised predictive model that captures the context of words using a neural network. Similarly, GLoVe is a popular unsupervised model incorporating corpus-wide word co-occurrence statistics. Such word embedding has significantly boosted important NLP tasks, including sentiment analysis, document classification, and machine translation. However, the embeddings are dense floating-point vectors, making them expensive to compute and difficult to interpret. In this paper, we instead propose to represent the semantics of words with a few defining words that are related using propositional logic. To produce such logical embeddings, we introduce a Tsetlin Machine-based autoencoder that learns logical clauses self-supervised. The clauses consist of contextual words like <tex-math>\textit{black}</tex-math>, <tex-math>\textit{cup}</tex-math>, and <tex-math>\textit{hot}</tex-math> to define other words like <tex-math>\textit{coffee}</tex-math>, thus being human-understandable. We evaluate our embedding approach on several intrinsic and extrinsic benchmarks, outperforming GLoVe on six classification tasks. Furthermore, we investigate the interpretability of our embedding using the logical representations acquired during training. We also visualize word clusters in vector space, demonstrating how our logical embedding co-locate similar words.</abstract>
      <url hash="e635de69">2024.findings-eacl.103</url>
      <attachment type="software" hash="fe2bb58c">2024.findings-eacl.103.software.zip</attachment>
      <bibkey>bhattarai-etal-2024-tsetlin</bibkey>
    </paper>
    <paper id="104">
      <title>Reading Between the Tweets: Deciphering Ideological Stances of Interconnected Mixed-Ideology Communities</title>
      <author><first>Zihao</first><last>He</last></author>
      <author><first>Ashwin</first><last>Rao</last></author>
      <author><first>Siyi</first><last>Guo</last></author>
      <author><first>Negar</first><last>Mokhberian</last></author>
      <author><first>Kristina</first><last>Lerman</last><affiliation>University of Southern California and USC Information Sciences Institute</affiliation></author>
      <pages>1523-1536</pages>
      <abstract>Recent advances in NLP have improved our ability to understand the nuanced worldviews of online communities. Existing research focused on probing ideological stances treats liberals and conservatives as separate groups. However, this fails to account for the nuanced views of the organically formed online communities and the connections between them. In this paper, we study discussions of the 2020 U.S. election on Twitter to identify complex interacting communities. Capitalizing on this interconnectedness, we introduce a novel approach that harnesses message passing when finetuning language models (LMs) to probe the nuanced ideologies of these communities. By comparing the responses generated by LMs and real-world survey results, our method shows higher alignment than existing baselines, highlighting the potential of using LMs in revealing complex ideologies within and across interconnected mixed-ideology communities.</abstract>
      <url hash="f8386119">2024.findings-eacl.104</url>
      <attachment type="software" hash="9c7db982">2024.findings-eacl.104.software.zip</attachment>
      <attachment type="note" hash="f185d42a">2024.findings-eacl.104.note.zip</attachment>
      <bibkey>he-etal-2024-reading</bibkey>
      <video href="2024.findings-eacl.104.mp4"/>
    </paper>
    <paper id="105">
      <title>Unified Embeddings for Multimodal Retrieval via Frozen <fixed-case>LLM</fixed-case>s</title>
      <author><first>Ziyang</first><last>Wang</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Heba</first><last>Elfardy</last><affiliation>Amazon</affiliation></author>
      <author><first>Markus</first><last>Dreyer</last><affiliation>Amazon</affiliation></author>
      <author><first>Kevin</first><last>Small</last><affiliation>Amazon</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>1537-1547</pages>
      <abstract>In this work, We present Unified Embeddings for Multimodal Retrieval (UniMuR), a simple but effective approach that embeds multimodal inputs and retrieves visual and textual outputs via frozen Large Language Models (LLMs). Specifically, UniMuR jointly retrieves multimodal outputs via a unified multimodal embedding and applies dual alignment training to account for both visual and textual semantics. Thus, unlike previous approaches, UniMuR significantly reduces LLM’s modality bias towards generating text-only outputs. Meanwhile, the proposed unified multimodal embedding mitigates the inconsistency between visual and textual outputs and provides coherent multimodal outputs. Furthermore, benefiting from the joint training of visual and textual semantics, UniMuR also achieves strong image/text retrieval ability. Compared to existing approaches, UniMuR achieves better zero-shot multimodal response retrieval performance on MMDialog, improving the overall R@1 by 6.5% while boosting the image retrieval rate and having better cross-modal consistency on multimodal outputs. UniMuR also achieves 2.4% and 3.9% improvement on context-based image retrieval tasks on MMDialog and VisDial respectively when compared to previous approaches, validating its generalization ability across multiple tasks.</abstract>
      <url hash="7ee77965">2024.findings-eacl.105</url>
      <bibkey>wang-etal-2024-unified</bibkey>
      <video href="2024.findings-eacl.105.mp4"/>
    </paper>
    <paper id="106">
      <title>Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods</title>
      <author><first>Mohammed</first><last>Mohammed</last></author>
      <author><first>Anya</first><last>Belz</last><affiliation>Dublin City University and University of Aberdeen</affiliation></author>
      <pages>1548-1556</pages>
      <abstract>As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning.In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module.We find that the ported modules far outperform the two alternatives tested, but that there are interesting differences between the four PEFT techniques tested.We conclude that task-specific knowledge in the form of structurally modular sets of parameters as produced by PEFT techniques is highly portable, but that degree of success depends on type of PEFT and on differences between originating and receiving pretrained models.</abstract>
      <url hash="4e5f93e8">2024.findings-eacl.106</url>
      <bibkey>mohammed-belz-2024-assessing</bibkey>
    </paper>
    <paper id="107">
      <title>Exploiting Class Probabilities for Black-box Sentence-level Attacks</title>
      <author><first>Raha</first><last>Moraffah</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Huan</first><last>Liu</last><affiliation>Arizona State University</affiliation></author>
      <pages>1557-1568</pages>
      <abstract>Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack’s success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.</abstract>
      <url hash="973ff83c">2024.findings-eacl.107</url>
      <bibkey>moraffah-liu-2024-exploiting</bibkey>
      <video href="2024.findings-eacl.107.mp4"/>
    </paper>
    <paper id="108">
      <title>Learning Label Hierarchy with Supervised Contrastive Learning</title>
      <author><first>Ruixue</first><last>Lian</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>William</first><last>Sethares</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Junjie</first><last>Hu</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <pages>1569-1581</pages>
      <abstract>Supervised contrastive learning (SCL) frameworks treat each class as independent and thus consider all classes to be equally important. This neglects the common scenario in which label hierarchy exists, where fine-grained classes under the same category show more similarity than very different ones. This paper introduces a family of Label-Aware SCL methods (LA-SCL) that incorporates hierarchical information to SCL by leveraging similarities between classes, resulting in creating a more well-structured and discriminative feature space. This is achieved by first adjusting the distance between instances based on measures of the proximity of their classes with the scaled instance-instance-wise contrastive. An additional instance-center-wise contrastive is introduced to move within-class examples closer to their centers, which are represented by a set of learnable label parameters. The learned label parameters can be directly used as a nearest neighbor classifier without further finetuning. In this way, a better feature representation is generated with improvements of intra-cluster compactness and inter-cluster separation. Experiments on three datasets show that the proposed LA-SCL works well on text classification of distinguishing a single label among multi-labels, outperforming the baseline supervised approaches. Our code is publicly available <tex-math>^1</tex-math>.</abstract>
      <url hash="2d0f13d7">2024.findings-eacl.108</url>
      <bibkey>lian-etal-2024-learning</bibkey>
      <video href="2024.findings-eacl.108.mp4"/>
    </paper>
    <paper id="109">
      <title><fixed-case>G</fixed-case>roun<fixed-case>D</fixed-case>ial: Human-norm Grounded Safe Dialog Response Generation</title>
      <author><first>Siwon</first><last>Kim</last></author>
      <author><first>Shuyang</first><last>Dai</last><affiliation>Amazon</affiliation></author>
      <author><first>Mohammad</first><last>Kachuee</last><affiliation>Amazon</affiliation></author>
      <author><first>Shayan</first><last>Ray</last></author>
      <author><first>Tara</first><last>Taghavi</last></author>
      <author><first>Sungroh</first><last>Yoon</last><affiliation>Seoul National University</affiliation></author>
      <pages>1582-1588</pages>
      <abstract>Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses agreeing to offensive user input or including toxic content. Previous research aimed to alleviate the toxicity by fine-tuning LLM with manually annotated safe dialogue histories. However, the dependency on additional tuning requires substantial costs. To remove the dependency, we propose GrounDial, where response safety is achieved by grounding responses to commonsense social rules without requiring fine-tuning. A hybrid approach of in-context learning and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer even without additional data or tuning.</abstract>
      <url hash="fd648be5">2024.findings-eacl.109</url>
      <bibkey>kim-etal-2024-groundial</bibkey>
    </paper>
    <paper id="110">
      <title>Trainable Hard Negative Examples in Contrastive Learning for Unsupervised Abstractive Summarization</title>
      <author><first>Haojie</first><last>Zhuang</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Wei Emma</first><last>Zhang</last><affiliation>The University of Adelaide</affiliation></author>
      <author><first>Chang</first><last>Dong</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Quan</first><last>Sheng</last><affiliation>Macquarie University</affiliation></author>
      <pages>1589-1600</pages>
      <abstract>Contrastive learning has demonstrated promising results in unsupervised abstractive summarization. However, existing methods rely on manually crafted negative examples, demanding substantial human effort and domain knowledge. Moreover, these human-generated negative examples may be poor in quality and lack adaptability during model training. To address these issues, we propose a novel approach that learns trainable negative examples for contrastive learning in unsupervised abstractive summarization, which eliminates the need for manual negative example design. Our framework introduces an adversarial optimization process between a negative example network and a representation network (including the summarizer and encoders). The negative example network is trained to synthesize hard negative examples that are close to the positive examples, driving the representation network to improve the quality of the generated summaries. We evaluate our method on two benchmark datasets for unsupervised abstractive summarization and observe significant performance improvements compared to strong baseline models.</abstract>
      <url hash="fc891a5c">2024.findings-eacl.110</url>
      <bibkey>zhuang-etal-2024-trainable</bibkey>
    </paper>
    <paper id="111">
      <title>Low-Resource Counterspeech Generation for <fixed-case>I</fixed-case>ndic Languages: The Case of <fixed-case>B</fixed-case>engali and <fixed-case>H</fixed-case>indi</title>
      <author><first>Mithun</first><last>Das</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Saurabh</first><last>Pandey</last></author>
      <author><first>Shivansh</first><last>Sethi</last></author>
      <author><first>Punyajoy</first><last>Saha</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <author><first>Animesh</first><last>Mukherjee</last><affiliation>Indian Institute of Technology Kharagpur</affiliation></author>
      <pages>1601-1614</pages>
      <abstract>With the rise of online abuse, the NLP community has begun investigating the use of neural architectures to generate counterspeech that can “counter” the vicious tone of such abusive speech and dilute/ameliorate their rippling effect over the social network. However, most of the efforts so far have been primarily focused on English. To bridge the gap for low-resource languages such as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive speech/counterspeech pairs, of which 2,460 pairs are in Bengali, and 2,602 pairs are in Hindi. We implement several baseline models considering various interlingual transfer mechanisms with different configurations to generate suitable counterspeech to set up an effective benchmark. We observe that the monolingual setup yields the best performance. Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family.</abstract>
      <url hash="419ea91d">2024.findings-eacl.111</url>
      <attachment type="note" hash="8bdbf9b3">2024.findings-eacl.111.note.zip</attachment>
      <bibkey>das-etal-2024-low</bibkey>
      <video href="2024.findings-eacl.111.mp4"/>
    </paper>
    <paper id="112">
      <title>Teaching Probabilistic Logical Reasoning to Transformers</title>
      <author><first>Aliakbar</first><last>Nafar</last></author>
      <author><first>K. Brent</first><last>Venable</last><affiliation>University of West Florida and Florida Institute of Human and Machine Cognition</affiliation></author>
      <author><first>Parisa</first><last>Kordjamshidi</last><affiliation>Michigan State University</affiliation></author>
      <pages>1615-1632</pages>
      <abstract>In this paper, we evaluate the capability of transformer-based language models in making inferences over uncertain text that includes uncertain rules of reasoning. We cover both Pre-trained Language Models (PLMs) and generative Large Language Models (LLMs). Our evaluation results show that both generations of language models struggle with reasoning over uncertain text. We propose a novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT), that utilizes probabilistic logical rules as constraints in the fine-tuning phase without relying on these rules in the inference stage. To assess the effectiveness of PCT, we utilize the related corpora and, additionally, create a new and more challenging benchmark that, unlike the previous ones, uses instance-specific rules. Our study demonstrates that PCT improves the transformer-based language model’s intrinsic reasoning and makes their probabilistic logical reasoning process more explicit and explainable. Furthermore, PCT equips these models to effectively handle novel situations, including higher reasoning depth, new domains, and complex probabilistic structures.</abstract>
      <url hash="e83b6dd5">2024.findings-eacl.112</url>
      <attachment type="software" hash="ea1e1ac9">2024.findings-eacl.112.software.zip</attachment>
      <attachment type="note" hash="ea47095c">2024.findings-eacl.112.note.zip</attachment>
      <bibkey>nafar-etal-2024-teaching</bibkey>
      <video href="2024.findings-eacl.112.mp4"/>
    </paper>
    <paper id="113">
      <title>On Measuring Context Utilization in Document-Level <fixed-case>MT</fixed-case> Systems</title>
      <author><first>Wafaa</first><last>Mohammed</last></author>
      <author><first>Vlad</first><last>Niculae</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>1633-1643</pages>
      <abstract>Document-level translation models are usually evaluated using general metrics such as BLEU, which are not informative about the benefits of context. Current work on context-aware evaluation, such as contrastive methods, only measure translation accuracy on words that need context for disambiguation. Such measures cannot reveal whether the translation model uses the correct supporting context. We propose to complement accuracy-based evaluation with measures of context utilization. We find that perturbation-based analysis (comparing models’ performance when provided with correct versus random context) is an effective measure of overall context utilization. For a finer-grained phenomenon-specific evaluation, we propose to measure how much the supporting context contributes to handling context-dependent discourse phenomena. We show that automatically-annotated supporting context gives similar conclusions to human-annotated context and can be used as alternative for cases where human annotations are not available. Finally, we highlight the importance of using discourse-rich datasets when assessing context utilization.</abstract>
      <url hash="d45369b7">2024.findings-eacl.113</url>
      <bibkey>mohammed-niculae-2024-measuring</bibkey>
      <video href="2024.findings-eacl.113.mp4"/>
    </paper>
    <paper id="114">
      <title>Solving <fixed-case>NLP</fixed-case> Problems through Human-System Collaboration: A Discussion-based Approach</title>
      <author><first>Masahiro</first><last>Kaneko</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Naoaki</first><last>Okazaki</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <pages>1644-1658</pages>
      <abstract>Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other.Similarly, if a system can have discussions with human partners when solving tasks, it has the potential to improve the system’s performance and reliability.In previous research on explainability, it has only been possible for systems to make predictions and for humans to ask questions about them, rather than having a mutual exchange of opinions.This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans, improving the accuracy by up to 25 points on a natural language inference task.</abstract>
      <url hash="e5d679ba">2024.findings-eacl.114</url>
      <attachment type="note" hash="e94e1232">2024.findings-eacl.114.note.zip</attachment>
      <bibkey>kaneko-etal-2024-solving</bibkey>
    </paper>
    <paper id="115">
      <title>Autoregressive Score Generation for Multi-trait Essay Scoring</title>
      <author><first>Heejin</first><last>Do</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Yunsu</first><last>Kim</last><affiliation>aiXplain, Inc.</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>1659-1666</pages>
      <abstract>Recently, encoder-only pre-trained models such as BERT have been successfully applied in automated essay scoring (AES) to predict a single overall score. However, studies have yet to explore these models in multi-trait AES, possibly due to the inefficiency of replicating BERT-based models for each trait. Breaking away from the existing sole use of *encoder*, we propose an autoregressive prediction of multi-trait scores (ArTS), incorporating a *decoding* process by leveraging the pre-trained T5. Unlike prior regression or classification methods, we redefine AES as a score-generation task, allowing a single model to predict multiple scores. During decoding, the subsequent trait prediction can benefit by conditioning on the preceding trait scores. Experimental results proved the efficacy of ArTS, showing over 5% average improvements in both prompts and traits.</abstract>
      <url hash="ae5946d1">2024.findings-eacl.115</url>
      <bibkey>do-etal-2024-autoregressive</bibkey>
      <video href="2024.findings-eacl.115.mp4"/>
    </paper>
    <paper id="116">
      <title><fixed-case>CMA</fixed-case>-<fixed-case>R</fixed-case>: Causal Mediation Analysis for Explaining Rumour Detection</title>
      <author><first>Lin</first><last>Tian</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Xiuzhen</first><last>Zhang</last><affiliation>Royal Melbourne Institute of Technology</affiliation></author>
      <author><first>Jey Han</first><last>Lau</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>1667-1675</pages>
      <abstract>We apply causal mediation analysis to explain the decision-making process of neural models for rumour detection on Twitter.Interventions at the input and network level reveal the causal impacts of tweets and words in the model output.We find that our approach CMA-R – Causal Mediation Analysis for Rumour detection – identifies salient tweets that explain model predictions and show strong agreement with human judgements for critical tweets determining the truthfulness of stories.CMA-R can further highlight causally impactful words in the salient tweets, providing another layer of interpretability and transparency into these blackbox rumour detection systems. Code is available at: https://github.com/ltian678/cma-r.</abstract>
      <url hash="c2bab0c1">2024.findings-eacl.116</url>
      <bibkey>tian-etal-2024-cma</bibkey>
    </paper>
    <paper id="117">
      <title>Morphology Aware Source Term Masking for Terminology-Constrained <fixed-case>NMT</fixed-case></title>
      <author><first>Ander</first><last>Corral</last><affiliation>Orai NLP Technologies</affiliation></author>
      <author><first>Xabier</first><last>Saralegi</last></author>
      <pages>1676-1688</pages>
      <abstract>Terminology-constrained NMT systems facilitate the forced translation of domain-specific vocabulary. A notable method in this context is the “copy-and-inflect” approach, which appends the target term lemmas of constraints to their corresponding source terms in the input sentence. In this work, we propose a novel adaptation of the “copy-and-inflect” method, referred to as “morph-masking”. Our method involves masking the source terms of the constraints from the input sentence while retaining essential grammatical information. Our approach is based on the hypothesis that “copy-and-inflect” systems have access to both source and target terms, allowing them to generate the correct surface form of the constraint by either translating the source term itself or properly inflecting the target term lemma. Through extensive validation of our method in two translation directions with different levels of source morphological complexity, Basque to Spanish and English to German, we have demonstrated that “morph-masking” is capable of providing a harder constraint signal, resulting in a notable improvement over the “copy-and-inflect” method (up to 38% in term accuracy), especially in challenging constraint scenarios.</abstract>
      <url hash="f037957d">2024.findings-eacl.117</url>
      <bibkey>corral-saralegi-2024-morphology</bibkey>
      <video href="2024.findings-eacl.117.mp4"/>
    </paper>
    <paper id="118">
      <title>Improving Backchannel Prediction Leveraging Sequential and Attentive Context Awareness</title>
      <author><first>Yo-Han</first><last>Park</last><affiliation>Chungnam National University and Chungnam National University</affiliation></author>
      <author><first>Wencke</first><last>Liermann</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Yong-Seok</first><last>Choi</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Kong Joo</first><last>Lee</last><affiliation>Chungnam National University</affiliation></author>
      <pages>1689-1694</pages>
      <abstract>Backchannels, which refer to short and often affirmative or empathetic responses from a listener during a conversation, play a crucial role in effective communication. In this paper, we introduce CABP(Context-Aware Backchannel Prediction), a sequential and attentive context approach aimed at enhancing backchannel prediction performance. Additionally, CABP leverages the pretrained wav2vec model for encoding audio signal. Experimental results show that CABP performs better than context-free models, with performance improvements of 1.3% and 1.8% in Korean and English datasets, respectively. Furthermore, when utilizing the pretrained wav2vec model, CABP consistently demonstrates the best performance, achieving performance improvements of 4.4% and 3.1% in Korean and English datasets.</abstract>
      <url hash="0120613a">2024.findings-eacl.118</url>
      <attachment type="software" hash="747f9bcc">2024.findings-eacl.118.software.zip</attachment>
      <bibkey>park-etal-2024-improving</bibkey>
      <video href="2024.findings-eacl.118.mp4"/>
    </paper>
    <paper id="119">
      <title><fixed-case>SENSE</fixed-case>-<fixed-case>LM</fixed-case> : A Synergy between a Language Model and Sensorimotor Representations for Auditory and Olfactory Information Extraction</title>
      <author><first>Cédric</first><last>Boscher</last></author>
      <author><first>Christine</first><last>Largeron</last><affiliation>Université Jean Monnet</affiliation></author>
      <author><first>Véronique</first><last>Eglin</last><affiliation>Institut National des Sciences Appliquées de Lyon</affiliation></author>
      <author><first>Elöd</first><last>Egyed-Zsigmond</last><affiliation>Institut National des Sciences Appliquées de Lyon</affiliation></author>
      <pages>1695-1711</pages>
      <abstract>The five human senses – vision, taste, smell, hearing, and touch – are key concepts that shape human perception of the world. The extraction of sensory references (i.e., expressions that evoke the presence of a sensory experience) in textual corpus is a challenge of high interest, with many applications in various areas. In this paper, we propose SENSE-LM, an information extraction system tailored for the discovery of sensory references in large collections of textual documents. Based on the novel idea of combining the strength of large language models and linguistic resources such as sensorimotor norms, it addresses the task of sensory information extraction at a coarse-grained (sentence binary classification) and fine-grained (sensory term extraction) level.Our evaluation of SENSE-LM for two sensory functions, Olfaction and Audition, and comparison with state-of-the-art methods emphasize a significant leap forward in automating these complex tasks.</abstract>
      <url hash="1b5a5d95">2024.findings-eacl.119</url>
      <bibkey>boscher-etal-2024-sense</bibkey>
      <video href="2024.findings-eacl.119.mp4"/>
    </paper>
    <paper id="120">
      <title>Analyzing the Role of Part-of-Speech in Code-Switching: A Corpus-Based Study</title>
      <author><first>Jie</first><last>Chi</last></author>
      <author><first>Peter</first><last>Bell</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>1712-1721</pages>
      <abstract>Code-switching (CS) is a common linguistic phenomenon wherein speakers fluidly transition between languages in conversation. While the cognitive processes driving CS remain a complex domain, earlier investigations have shed light on its multifaceted triggers. This study delves into the influence of Part-of-Speech (POS) on the propensity of bilinguals to engage in CS, employing a comprehensive analysis of Spanish-English and Mandarin-English corpora. Compared with prior research, our findings not only affirm the existence of a statistically significant connection between POS and the likelihood of CS across language pairs, but notably find this relationship exhibits its maximum strength in proximity to CS instances, progressively diminishing as tokens distance themselves from these CS points.</abstract>
      <url hash="cdc4e20a">2024.findings-eacl.120</url>
      <bibkey>chi-bell-2024-analyzing</bibkey>
      <video href="2024.findings-eacl.120.mp4"/>
    </paper>
    <paper id="121">
      <title>In-Contextual Gender Bias Suppression for Large Language Models</title>
      <author><first>Daisuke</first><last>Oba</last><affiliation>Institute of Industrial Science, The University of Tokyo</affiliation></author>
      <author><first>Masahiro</first><last>Kaneko</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <author><first>Danushka</first><last>Bollegala</last><affiliation>Amazon and University of Liverpool</affiliation></author>
      <pages>1722-1742</pages>
      <abstract>Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of LLMs, which are computationally costly. Moreover, one might not even have access to the model parameters for performing debiasing such as in the case of closed LLMs such as GPT-4. To address this challenge, we propose bias suppression that prevents biased generations of LLMs by simply providing textual preambles constructed from manually designed templates and real-world statistics, without accessing to model parameters. We show that, using CrowsPairs dataset, our textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2. Moreover, we find that gender-neutral descriptions of gender-biased objects can also suppress their gender biases. Moreover, we show that bias suppression has acceptable adverse effect on downstream task performance with HellaSwag and COPA.</abstract>
      <url hash="5dd0284b">2024.findings-eacl.121</url>
      <bibkey>oba-etal-2024-contextual</bibkey>
      <video href="2024.findings-eacl.121.mp4"/>
    </paper>
    <paper id="122">
      <title>Parameter-Efficient Fine-Tuning: Is There An Optimal Subset of Parameters to Tune?</title>
      <author><first>Max</first><last>Ploner</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>1743-1759</pages>
      <abstract>The ever-growing size of pretrained language models (PLM) presents a significant challenge for efficiently fine-tuning and deploying these models for diverse sets of tasks within memory-constrained environments.In light of this, recent research has illuminated the possibility of selectively updating only a small subset of a model’s parameters during the fine-tuning process.Since no new parameters or modules are added, these methods retain the inference speed of the original model and come at no additional computational cost. However, an open question pertains to which subset of parameters should best be tuned to maximize task performance and generalizability. To investigate, this paper presents comprehensive experiments covering a large spectrum of subset selection strategies. We comparatively evaluate their impact on model performance as well as the resulting model’s capability to generalize to different tasks.Surprisingly, we find that the gains achieved in performance by elaborate selection strategies are, at best, marginal when compared to the outcomes obtained by tuning a random selection of parameter subsets. Our experiments also indicate that selection-based tuning impairs generalizability to new tasks.</abstract>
      <url hash="da50d04e">2024.findings-eacl.122</url>
      <bibkey>ploner-akbik-2024-parameter</bibkey>
      <video href="2024.findings-eacl.122.mp4"/>
    </paper>
    <paper id="123">
      <title>Contextualized Topic Coherence Metrics</title>
      <author><first>Hamed</first><last>Rahimi</last></author>
      <author><first>David</first><last>Mimno</last><affiliation>Cornell University and Cornell University</affiliation></author>
      <author><first>Jacob</first><last>Hoover</last><affiliation>McGill University</affiliation></author>
      <author><first>Hubert</first><last>Naacke</last><affiliation>Sorbonne Université</affiliation></author>
      <author><first>Camelia</first><last>Constantin</last></author>
      <author><first>Bernd</first><last>Amann</last><affiliation>Sorbonne Université</affiliation></author>
      <pages>1760-1773</pages>
      <abstract>This article proposes a new family of LLM-based topic coherence metrics called Contextualized Topic Coherence (CTC) and inspired by standard human topic evaluation methods. CTC metrics simulate human-centered coherence evaluation while maintaining the efficiency of other automated methods. We compare the performance of our CTC metrics and five other baseline metrics on seven topic models and show that CTC metrics better reflect human judgment, particularly for topics extracted from short text collections by avoiding highly scored topics that are meaningless to humans.</abstract>
      <url hash="f7168365">2024.findings-eacl.123</url>
      <attachment type="software" hash="3868254e">2024.findings-eacl.123.software.zip</attachment>
      <bibkey>rahimi-etal-2024-contextualized</bibkey>
      <video href="2024.findings-eacl.123.mp4"/>
    </paper>
    <paper id="124">
      <title><fixed-case>P</fixed-case>ro<fixed-case>MIS</fixed-case>e: A Proactive Multi-turn Dialogue Dataset for Information-seeking Intent Resolution</title>
      <author><first>Yash</first><last>Butala</last></author>
      <author><first>Siddhant</first><last>Garg</last><affiliation>Meta</affiliation></author>
      <author><first>Pratyay</first><last>Banerjee</last><affiliation>Amazon</affiliation></author>
      <author><first>Amita</first><last>Misra</last><affiliation>Amazon</affiliation></author>
      <pages>1774-1789</pages>
      <abstract>Users of AI-based virtual assistants and search systems encounter challenges in articulating their intents while seeking information on unfamiliar topics, possibly due to complexity of the user’s intent or the lack of meta-information on the topic. We posit that an iterative suggested question-answering (SQA) conversation can improve the trade-off between the satisfaction of the user’s intent while keeping the information exchange natural and cognitive load of the interaction minimal on the users. In this paper, we evaluate a novel setting ProMISe by means of a sequence of interactions between a user, having a predefined information-seeking intent, and an agent that generates a set of SQA pairs at each step to aid the user to get closer to their intent. We simulate this two-player setting to create a multi-turn conversational dataset of SQAs and user choices (1025 dialogues comprising 4453 turns and 17812 SQAs) using human-feedback, chain-of-thought prompting and web-retrieval augmented large language models. We evaluate the quality of the SQs in the dataset on attributes such as diversity, specificity, grounding, etc, and benchmark the performance of different language models for the task of replicating user behavior.</abstract>
      <url hash="f2aec2e2">2024.findings-eacl.124</url>
      <attachment type="note" hash="551af5d6">2024.findings-eacl.124.note.zip</attachment>
      <bibkey>butala-etal-2024-promise</bibkey>
    </paper>
    <paper id="125">
      <title><fixed-case>CODET</fixed-case>: A Benchmark for Contrastive Dialectal Evaluation of Machine Translation</title>
      <author><first>Md Mahfuz Ibn</first><last>Alam</last></author>
      <author><first>Sina</first><last>Ahmadi</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>1790-1859</pages>
      <abstract>Neural machine translation (NMT) systems exhibit limited robustness in handling source-side linguistic variations. Their performance tends to degrade when faced with even slight deviations in language usage, such as different domains or variations introduced by second-language speakers. It is intuitive to extend this observation to encompass dialectal variations as well, but the work allowing the community to evaluate MT systems on this dimension is limited. To alleviate this issue, we compile and release CODET, a contrastive dialectal benchmark encompassing 891 different variations from twelve different languages. We also quantitatively demonstrate the challenges large MT models face in effectively translating dialectal variants. All the data and code have been released.</abstract>
      <url hash="9e3000f0">2024.findings-eacl.125</url>
      <bibkey>alam-etal-2024-codet</bibkey>
    </paper>
    <paper id="126">
      <title><fixed-case>QAEVENT</fixed-case>: Event Extraction as Question-Answer Pairs Generation</title>
      <author><first>Milind</first><last>Choudhary</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Xinya</first><last>Du</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>1860-1873</pages>
      <abstract>We propose a novel representation of document-level events as question and answer pairs (QAEVENT). Under this paradigm: (1) questions themselves can define argument roles without the need for predefined schemas, which will cover a comprehensive list of event arguments from the document; (2) it allows for more scalable and faster annotations from crowdworkers without linguistic expertise. Based on our new paradigm, we collect a novel and wide-coverage dataset. Our examinations show that annotations with the QA representations produce high-quality data for document-level event extraction, both in terms of human agreement level and high coverage of roles comparing to the pre-defined schema. We present and compare representative approaches for generating event question answer pairs on our benchmark.</abstract>
      <url hash="43e68930">2024.findings-eacl.126</url>
      <bibkey>choudhary-du-2024-qaevent</bibkey>
    </paper>
    <paper id="127">
      <title>Sequence Shortening for Context-Aware Machine Translation</title>
      <author><first>Paweł</first><last>Maka</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Yusuf</first><last>Semerci</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Jan</first><last>Scholtes</last><affiliation>Maastricht University</affiliation></author>
      <author><first>Gerasimos</first><last>Spanakis</last><affiliation>Maastricht University</affiliation></author>
      <pages>1874-1894</pages>
      <abstract>Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that the two methods achieve competitive BLEU and COMET scores and accuracies on the contrastive datasets to the other tested methods while potentially allowing for higher interpretability and reducing the growth of memory requirements with increased context size.</abstract>
      <url hash="774a45f4">2024.findings-eacl.127</url>
      <bibkey>maka-etal-2024-sequence</bibkey>
      <video href="2024.findings-eacl.127.mp4"/>
    </paper>
    <paper id="128">
      <title>Jigsaw Pieces of Meaning: Modeling Discourse Coherence with Informed Negative Sample Synthesis</title>
      <author><first>Shubhankar</first><last>Singh</last></author>
      <pages>1895-1908</pages>
      <abstract>Coherence in discourse is fundamental for comprehension and perception. Much research on coherence modeling has focused on better model architectures and training setups optimizing on the permuted document task, where random permutations of a coherent document are considered incoherent. However, there’s very limited work on creating “informed” synthetic incoherent samples that better represent or mimic incoherence. We source a diverse positive corpus for local coherence and propose six rule-based methods leveraging information from Constituency trees, Part-of-speech, semantic overlap and more, for “informed” negative sample synthesis for better representation of incoherence. We keep a straightforward training setup for local coherence modeling by fine-tuning popular transformer models, and aggregate local scores for global coherence. We evaluate on a battery of independent downstream tasks to assess the impact of improved negative sample quality. We assert that a step towards optimality for coherence modeling requires better negative sample synthesis in tandem with model improvements.</abstract>
      <url hash="7d14f3f1">2024.findings-eacl.128</url>
      <bibkey>singh-2024-jigsaw</bibkey>
    </paper>
    <paper id="129">
      <title>Non-Exchangeable Conformal Language Generation with Nearest Neighbors</title>
      <author><first>Dennis</first><last>Ulmer</last></author>
      <author><first>Chrysoula</first><last>Zerva</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico and Unbabel</affiliation></author>
      <pages>1909-1929</pages>
      <abstract>Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on *non-exchangeable* conformal prediction, which still ensures bounds on coverage. The result, *non-exchangeable conformal nucleus sampling*, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees.</abstract>
      <url hash="74ae0a2d">2024.findings-eacl.129</url>
      <bibkey>ulmer-etal-2024-non</bibkey>
      <video href="2024.findings-eacl.129.mp4"/>
    </paper>
    <paper id="130">
      <title>Evidentiality-aware Retrieval for Overcoming Abstractiveness in Open-Domain Question Answering</title>
      <author><first>Yongho</first><last>Song</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dahyun</first><last>Lee</last></author>
      <author><first>Myungha</first><last>Jang</last><affiliation>Pinterest Inc.</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Kyungjae</first><last>Lee</last></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <pages>1930-1943</pages>
      <abstract>The long-standing goal of dense retrievers in abtractive open-domain question answering (ODQA) tasks is to learn to capture evidence passages among relevant passages for any given query, such that the reader produce factually correct outputs from evidence passages. One of the key challenge is the insufficient amount of training data with the supervision of the answerability of the passages. Recent studies rely on iterative pipelines to annotate answerability using signals from the reader, but their high computational costs hamper practical applications. In this paper, we instead focus on a data-driven approach and propose Evidentiality-Aware Dense Passage Retrieval (EADPR), which leverages synthetic distractor samples to learn to discriminate evidence passages from distractors. We conduct extensive experiments to validate the effectiveness of our proposed method on multiple abstractive ODQA tasks.</abstract>
      <url hash="021372bf">2024.findings-eacl.130</url>
      <bibkey>song-etal-2024-evidentiality</bibkey>
      <video href="2024.findings-eacl.130.mp4"/>
    </paper>
    <paper id="131">
      <title>Self-training Strategies for Sentiment Analysis: An Empirical Study</title>
      <author><first>Haochen</first><last>Liu</last><affiliation>Fidelity Investments</affiliation></author>
      <author><first>Sai</first><last>Rallabandi</last></author>
      <author><first>Yijing</first><last>Wu</last></author>
      <author><first>Parag</first><last>Dakle</last></author>
      <author><first>Preethi</first><last>Raghavan</last><affiliation>Fidelity</affiliation></author>
      <pages>1944-1954</pages>
      <abstract>Sentiment analysis is a crucial task in natural language processing that involves identifying and extracting subjective sentiment from text. Self-training has recently emerged as an economical and efficient technique for developing sentiment analysis models by leveraging a small amount of labeled data and a large amount of unlabeled data. However, given a set of training data, how to utilize them to conduct self-training makes a significant difference in the final performance of the model. We refer to this methodology as the self-training strategy. In this paper, we present an empirical study of various self-training strategies for sentiment analysis. First, we investigate the influence of the self-training strategy and hyper-parameters on the performance of traditional small language models (SLMs) in various few-shot settings. Second, we also explore the feasibility of leveraging large language models (LLMs) to help self-training. We propose and empirically compare several self-training strategies with the intervention of LLMs. Extensive experiments are conducted on three real-world sentiment analysis datasets.</abstract>
      <url hash="10c2026e">2024.findings-eacl.131</url>
      <bibkey>liu-etal-2024-self</bibkey>
      <video href="2024.findings-eacl.131.mp4"/>
    </paper>
    <paper id="132">
      <title>Language is All a Graph Needs</title>
      <author><first>Ruosong</first><last>Ye</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Caiqi</first><last>Zhang</last></author>
      <author><first>Runhui</first><last>Wang</last></author>
      <author><first>Shuyuan</first><last>Xu</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <pages>1955-1973</pages>
      <abstract>The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data like images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, languages, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is available at https://github.com/agiresearch/InstructGLM.</abstract>
      <url hash="8ec9c9ff">2024.findings-eacl.132</url>
      <bibkey>ye-etal-2024-language</bibkey>
    </paper>
    <paper id="133">
      <title>Unraveling the Dynamics of Semi-Supervised Hate Speech Detection: The Impact of Unlabeled Data Characteristics and Pseudo-Labeling Strategies</title>
      <author><first>Florian</first><last>Ludwig</last><affiliation>Universität Duisburg-Essen</affiliation></author>
      <author><first>Klara</first><last>Dolos</last><affiliation>ZITiS</affiliation></author>
      <author><first>Ana</first><last>Alves-Pinto</last><affiliation>Central Office for Information Technology in the Security Sector</affiliation></author>
      <author><first>Torsten</first><last>Zesch</last><affiliation>Fernuniversität in Hagen</affiliation></author>
      <pages>1974-1986</pages>
      <abstract>Despite advances in machine learning based hate speech detection, the need for larges amounts of labeled training data for state-of-the-art approaches remains a challenge for their application. Semi-supervised learning addresses this problem by leveraging unlabeled data and thus reducing the amount of annotated data required. Underlying this approach is the assumption that labeled and unlabeled data follow similar distributions. This assumption however may not always hold, with consequences for real world applications. We address this problem by investigating the dynamics of pseudo-labeling, a commonly employed form of semi-supervised learning, in the context of hate speech detection. Concretely we analysed the influence of data characteristics and of two strategies for selecting pseudo-labeled samples: threshold- and ratio-based. The results show that the influence of data characteristics on the pseudo-labeling performances depends on other factors, such as pseudo-label selection strategies or model biases. Furthermore, the effectiveness of pseudo-labeling in classification performance is determined by the interaction between the number, hate ratio and accuracy of the selected pseudo-labels. Analysis of the results suggests an advantage of the threshold-based approach when labeled and unlabeled data arise from the same domain, whilst the ratio-based approach may be recommended in the opposite situation.</abstract>
      <url hash="e98ff5b9">2024.findings-eacl.133</url>
      <attachment type="software" hash="faa4add9">2024.findings-eacl.133.software.zip</attachment>
      <bibkey>ludwig-etal-2024-unraveling</bibkey>
    </paper>
    <paper id="134">
      <title>When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets</title>
      <author><first>Orion</first><last>Weller</last></author>
      <author><first>Kyle</first><last>Lo</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>David</first><last>Wadden</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Dawn</first><last>Lawrie</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>1987-2003</pages>
      <abstract>Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positives). Our results suggest the following recipe: use expansions for weaker models or when the target dataset significantly differs from training corpus in format; otherwise, avoid expansions to keep the relevance signal clear.</abstract>
      <url hash="74f3c4d7">2024.findings-eacl.134</url>
      <bibkey>weller-etal-2024-generative</bibkey>
      <video href="2024.findings-eacl.134.mp4"/>
    </paper>
    <paper id="135">
      <title>Can Large Language Models Understand Context?</title>
      <author><first>Yilun</first><last>Zhu</last></author>
      <author><first>Joel Ruben Antony</first><last>Moniz</last><affiliation>Apple</affiliation></author>
      <author><first>Shruti</first><last>Bhargava</last><affiliation>Apple</affiliation></author>
      <author><first>Jiarui</first><last>Lu</last><affiliation>Apple</affiliation></author>
      <author><first>Dhivya</first><last>Piraviperumal</last></author>
      <author><first>Site</first><last>Li</last></author>
      <author><first>Yuan</first><last>Zhang</last><affiliation>Apple</affiliation></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Apple</affiliation></author>
      <author><first>Bo-Hsiang</first><last>Tseng</last></author>
      <pages>2004-2018</pages>
      <abstract>Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models’ ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.</abstract>
      <url hash="400d0d77">2024.findings-eacl.135</url>
      <bibkey>zhu-etal-2024-large</bibkey>
      <video href="2024.findings-eacl.135.mp4"/>
    </paper>
    <paper id="136">
      <title>Let’s Negotiate! A Survey of Negotiation Dialogue Systems</title>
      <author><first>Haolan</first><last>Zhan</last><affiliation>Monash University</affiliation></author>
      <author><first>Yufei</first><last>Wang</last></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Monash University</affiliation></author>
      <author><first>Tao</first><last>Feng</last><affiliation>Monash University</affiliation></author>
      <author><first>Yuncheng</first><last>Hua</last></author>
      <author><first>Suraj</first><last>Sharma</last></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Zhaleh</first><last>Semnani Azad</last><affiliation>California State University, Northridge</affiliation></author>
      <author><first>Ingrid</first><last>Zukerman</last><affiliation>Monash University</affiliation></author>
      <author><first>Reza</first><last>Haf</last><affiliation>Monash University</affiliation></author>
      <pages>2019-2031</pages>
      <abstract>Negotiation is a crucial ability in human communication. Recently, there has been a resurgent research interest in negotiation dialogue systems, whose goal is to create intelligent agents that can assist people in resolving conflicts or reaching agreements. Although there have been many explorations into negotiation dialogue systems, a systematic review of this task has not been performed to date. We aim to fill this gap by investigating recent studies in the field of negotiation dialogue systems, and covering benchmarks, evaluations and methodologies within the literature. We also discuss potential future directions, including multi-modal, multi-party and cross-cultural negotiation scenarios. Our goal is to provide the community with a systematic overview of negotiation dialogue systems and to inspire future research.</abstract>
      <url hash="5722240a">2024.findings-eacl.136</url>
      <bibkey>zhan-etal-2024-lets</bibkey>
    </paper>
    <paper id="137">
      <title>Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models</title>
      <author><first>Younghun</first><last>Lee</last></author>
      <author><first>Dan</first><last>Goldwasser</last><affiliation>Purdue University</affiliation></author>
      <author><first>Laura Schwab</first><last>Reese</last><affiliation>Purdue University</affiliation></author>
      <pages>2032-2047</pages>
      <abstract>Understanding the dynamics of counseling conversations is an important task, yet it is a challenging NLP problem regardless of the recent advance of Transformer-based pre-trained language models. This paper proposes a systematic approach to examine the efficacy of domain knowledge and large language models (LLMs) in better representing conversations between a crisis counselor and a help seeker. We empirically show that state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome. To provide richer context to conversations, we incorporate human-annotated domain knowledge and LLM-generated features; simple integration of domain knowledge and LLM features improves the model performance by approximately 15%. We argue that both domain knowledge and LLM-generated features can be exploited to better characterize counseling conversations when they are used as an additional context to conversations.</abstract>
      <url hash="000711a7">2024.findings-eacl.137</url>
      <attachment type="software" hash="62e740c5">2024.findings-eacl.137.software.zip</attachment>
      <bibkey>lee-etal-2024-towards</bibkey>
    </paper>
    <paper id="138">
      <title>Better Explain Transformers by Illuminating Important Information</title>
      <author><first>Linxin</first><last>Song</last></author>
      <author><first>Yan</first><last>Cui</last></author>
      <author><first>Ao</first><last>Luo</last></author>
      <author><first>Freddy</first><last>Lecue</last><affiliation>INRIA</affiliation></author>
      <author><first>Irene</first><last>Li</last></author>
      <pages>2048-2062</pages>
      <abstract>Transformer-based models excel in various natural language processing (NLP) tasks, attracting countless efforts to explain their inner workings. Prior methods explain Transformers by focusing on the raw gradient and attention as token attribution scores, where non-relevant information is often considered during explanation computation, resulting in confusing results. In this work, we propose highlighting the important information and eliminating irrelevant information by a refined information flow on top of the layer-wise relevance propagation (LRP) method. Specifically, we consider identifying syntactic and positional heads as important attention heads and focus on the relevance obtained from these important heads. Experimental results demonstrate that irrelevant information does distort output attribution scores and then should be masked during explanation computation. Compared to eight baselines on both classification and question-answering datasets, our method consistently outperforms with over 3% to 33% improvement on explanation metrics, providing superior explanation performance. Our anonymous code repository is available at: https://anonymous.4open.science/r/MLRP-E676/</abstract>
      <url hash="ffb0def1">2024.findings-eacl.138</url>
      <bibkey>song-etal-2024-better</bibkey>
    </paper>
    <paper id="139">
      <title>Testing the Depth of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>’s Comprehension via Cross-Modal Tasks Based on <fixed-case>ASCII</fixed-case>-Art: <fixed-case>GPT</fixed-case>3.5’s Abilities in Regard to Recognizing and Generating <fixed-case>ASCII</fixed-case>-Art Are Not Totally Lacking</title>
      <author><first>David</first><last>Bayani</last><affiliation>Inpleo, Inc.</affiliation></author>
      <pages>2063-2077</pages>
      <abstract>In the months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche industry of papers have emerged examining the scope of capabilities these models possess, language — whether natural or stylized like code — has been the vehicle to exchange information with the network. Drawing inspiration from the multi-modal knowledge we’d expect an agent with true understanding to possess, we examine GPT3.5’s aptitude for visual tasks, where the inputs feature ASCII-art without overt distillation into a lingual summary. In particular, we scrutinize its performance on carefully designed image recognition and generation tasks. An extended version of this write-up is available at: https://arxiv.org/abs/2307.16806 .</abstract>
      <url hash="f92e7ca9">2024.findings-eacl.139</url>
      <bibkey>bayani-2024-testing</bibkey>
      <video href="2024.findings-eacl.139.mp4"/>
    </paper>
    <paper id="140">
      <title>Cross-lingual Editing in Multilingual Language Models</title>
      <author><first>Himanshu</first><last>Beniwal</last><affiliation>Indian Institute of Technology Gandhinagar</affiliation></author>
      <author><first>Kowsik</first><last>D</last></author>
      <author><first>Mayank</first><last>Singh</last><affiliation>Indian Institute of Technology Gandhinagar</affiliation></author>
      <pages>2078-2128</pages>
      <abstract>The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (XME) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: Latin (English, French, and Spanish) and Indic (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distinct script families. These findings highlight the need for further research and development of XME techniques to address these challenges. For more comprehensive information, the dataset used in this research and the associated code are publicly available at the following [URL](https://github.com/lingo-iitgn/XME).</abstract>
      <url hash="4343bb32">2024.findings-eacl.140</url>
      <bibkey>beniwal-etal-2024-cross</bibkey>
      <video href="2024.findings-eacl.140.mp4"/>
    </paper>
    <paper id="141">
      <title>Sorted <fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference</title>
      <author><first>Parsa</first><last>Kavehzadeh</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mojtaba</first><last>Valipour</last></author>
      <author><first>Marzieh</first><last>Tahaei</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>2129-2145</pages>
      <abstract>Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text. However, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference by leveraging the modularity in networks and sorting sub-models based on computation/accuracy in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any Pre-Training and by only replacing Standard Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that this approach can unlock the potential of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. The efficacy of our proposed method was demonstrated by applying it to tune LLaMA 2 13B on the Stanford Alpaca dataset for instruction following and TriviaQA for closed-book question answering. Our results show the superior performance of sub-models in comparison to Standard Fine-Tuning and SFT+ICT (Early-Exit), all achieved with very efficient tuning and without additional memory usage during inference.</abstract>
      <url hash="7cadd98a">2024.findings-eacl.141</url>
      <bibkey>kavehzadeh-etal-2024-sorted</bibkey>
      <video href="2024.findings-eacl.141.mp4"/>
    </paper>
    <paper id="142">
      <title><fixed-case>A</fixed-case>ccent<fixed-case>F</fixed-case>old: A Journey through <fixed-case>A</fixed-case>frican Accents for Zero-Shot <fixed-case>ASR</fixed-case> Adaptation to Target Accents</title>
      <author><first>Abraham</first><last>Owodunni</last><affiliation>Masakhane</affiliation></author>
      <author><first>Aditya</first><last>Yadavalli</last><affiliation>Karya Inc</affiliation></author>
      <author><first>Chris</first><last>Emezue</last></author>
      <author><first>Tobi</first><last>Olatunji</last></author>
      <author><first>Clinton</first><last>Mbataku</last></author>
      <pages>2146-2161</pages>
      <abstract>Despite advancements in speech recognition, accented speech remains challenging. While previous approaches have focused on modeling techniques or creating accented speech datasets, gathering sufficient data for the multitude of accents, particularly in the African context, remains impractical due to their sheer diversity and associated budget constraints. To address these challenges, we propose AccentFold, a method that exploits spatial relationships between learned accent embeddings to improve downstream Automatic Speech Recognition (ASR). Our exploratory analysis of speech embeddings representing 100+ African accents reveals interesting spatial accent relationships highlighting geographic and genealogical similarities, capturing consistent phonological, and morphological regularities, all learned empirically from speech. Furthermore, we discover accent relationships previously uncharacterized by the Ethnologue. Through empirical evaluation, we demonstrate the effectiveness of AccentFold by showing that, for out-of-distribution (OOD) accents, sampling accent subsets for training based on AccentFold information outperforms strong baselines a relative WER improvement of 4.6%. AccentFold presents a promising approach for improving ASR performance on accented speech, particularly in the context of African accents, where data scarcity and budget constraints pose significant challenges. Our findings emphasize the potential of leveraging linguistic relationships to improve zero-shot ASR adaptation to target accents.</abstract>
      <url hash="057ef8c0">2024.findings-eacl.142</url>
      <bibkey>owodunni-etal-2024-accentfold</bibkey>
    </paper>
    <paper id="143">
      <title>Hierarchical and Dynamic Prompt Compression for Efficient Zero-shot <fixed-case>API</fixed-case> Usage</title>
      <author><first>Yichen</first><last>Jiang</last></author>
      <author><first>Marco</first><last>Vecchio</last></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Anders</first><last>Johannsen</last></author>
      <pages>2162-2174</pages>
      <abstract>Long prompts present a significant challenge for practical LLM-based systems that need to operate with low latency and limited resources. We investigate prompt compression for zero-shot dialogue systems that learn to use unseen APIs directly in-context from their documentation, which may take up hundreds of prompt tokens per API. We start from a recently introduced approach (Mu et al., 2023) that learns to compress the prompt into a few “gist token” activations during finetuning. However, this simple idea is ineffective in compressing API documentation, resulting in low accuracy compared to the baseline using an uncompressed prompt. In this work, we introduce two major improvements. First, we specialize gist tokens for different hierarchies within an API: we use one <tex-math>\mathrm{Gist}_{\mathrm{arg}}</tex-math> token for compressing an argument and one <tex-math>\mathrm{Gist}_{\mathrm{value}}</tex-math> token for compressing an acceptable value of a categorical argument. We then dynamically reveal <tex-math>\mathrm{Gist}_{\mathrm{value}}</tex-math> tokens only when they are needed. Second, we add a reconstruction loss to predict the API documentation from the gist tokens. On multiple API-calling tasks, our proposed system keeps the simplicity, efficiency, and large compression factor (20x on SGD) of the gist token approach while achieving significantly better accuracy.</abstract>
      <url hash="8c1adec3">2024.findings-eacl.143</url>
      <bibkey>jiang-etal-2024-hierarchical</bibkey>
      <video href="2024.findings-eacl.143.mp4"/>
    </paper>
    <paper id="144">
      <title>Fine-tuning <fixed-case>CLIP</fixed-case> Text Encoders with Two-step Paraphrasing</title>
      <author><first>Hyunjae</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Trung</first><last>Bui</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Handong</first><last>Zhao</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Quan</first><last>Tran</last><affiliation>servicenow</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Jaewoo</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>2175-2184</pages>
      <abstract>Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, which we call ParaCLIP, exhibits significant improvements over baseline CLIP models across various tasks, including paraphrased retrieval (with rank similarity scores improved by up to 7.6% and 9.6%), Visual Genome Relation and Attribution, as well as seven semantic textual similarity tasks.</abstract>
      <url hash="25c91d2e">2024.findings-eacl.144</url>
      <bibkey>kim-etal-2024-fine</bibkey>
    </paper>
    <paper id="145">
      <title>Generative Interpretation: Toward Human-Like Evaluation for Educational Question-Answer Pair Generation</title>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Jaewook</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Sugyeong</first><last>Eo</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>2185-2196</pages>
      <abstract>Educational question-answer generation has been extensively researched owing to its practical applicability. However, we have identified a persistent challenge concerning the evaluation of such systems. Existing evaluation methods often fail to produce objective results and instead exhibit a bias towards favoring high similarity to the ground-truth question-answer pairs. In this study, we demonstrate that these evaluation methods yield low human alignment and propose an alternative approach called Generative Interpretation (GI) to achieve more objective evaluations. Through experimental analysis, we reveal that GI outperforms existing evaluation methods in terms of human alignment, and even shows comparable performance with GPT3.5, only with BART-large.</abstract>
      <url hash="7a40f6e9">2024.findings-eacl.145</url>
      <attachment type="software" hash="0e3e8735">2024.findings-eacl.145.software.zip</attachment>
      <bibkey>moon-etal-2024-generative</bibkey>
      <video href="2024.findings-eacl.145.mp4"/>
    </paper>
    <paper id="146">
      <title>Dive into the Chasm: Probing the Gap between In- and Cross-Topic Generalization</title>
      <author><first>Andreas</first><last>Waldis</last><affiliation>Technische Universität Darmstadt and Lucerne University of Applied Sciences and Arts</affiliation></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>2197-2214</pages>
      <abstract>Pre-trained language models (PLMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics. This paper analyzes various PLMs with three probing-based experiments to better understand the reasons behind such generalization gaps. For the first time, we demonstrate that the extent of these generalization gaps and the sensitivity to token-level interventions vary significantly across PLMs. By evaluating large language models (LLMs), we show the usefulness of our analysis for these recent models. Overall, we observe diverse pre-training objectives and architectural regularization contribute to more robust PLMs and mitigate generalization gaps. Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios.</abstract>
      <url hash="a4646e90">2024.findings-eacl.146</url>
      <bibkey>waldis-etal-2024-dive</bibkey>
      <video href="2024.findings-eacl.146.mp4"/>
    </paper>
    <paper id="147">
      <title><fixed-case>LLM</fixed-case>-<fixed-case>GE</fixed-case>m: Large Language Model-Guided Prediction of People’s Empathy Levels towards Newspaper Article</title>
      <author><first>Md Rakibul</first><last>Hasan</last><affiliation>Curtin University of Technology and BRAC University, Bangladesh</affiliation></author>
      <author><first>Md Zakir</first><last>Hossain</last><affiliation>CSIRO and Australian National University</affiliation></author>
      <author><first>Tom</first><last>Gedeon</last></author>
      <author><first>Shafin</first><last>Rahman</last><affiliation>North South University</affiliation></author>
      <pages>2215-2231</pages>
      <abstract>Empathy – encompassing the understanding and supporting others’ emotions and perspectives – strengthens various social interactions, including written communication in healthcare, education and journalism. Detecting empathy using AI models by relying on self-assessed ground truth through crowdsourcing is challenging due to the inherent noise in such annotations. To this end, we propose a novel system, named Large Language Model-Guided Empathy _(LLM-GEm)_ prediction system. It rectifies annotation errors based on our defined annotation selection threshold and makes the annotations reliable for conventional empathy prediction models, e.g., BERT-based pre-trained language models (PLMs). Previously, demographic information was often integrated numerically into empathy detection models. In contrast, our _LLM-GEm_ leverages GPT-3.5 LLM to convert numerical data into semantically meaningful textual sequences, enabling seamless integration into PLMs. We experiment with three _NewsEmpathy_ datasets involving people’s empathy levels towards newspaper articles and achieve state-of-the-art test performance using a RoBERTa-based PLM. Code and evaluations are publicly available at [https://github.com/hasan-rakibul/LLM-GEm](https://github.com/hasan-rakibul/LLM-GEm).</abstract>
      <url hash="ac47d4fc">2024.findings-eacl.147</url>
      <attachment type="software" hash="b41c778e">2024.findings-eacl.147.software.zip</attachment>
      <attachment type="note" hash="291dcb66">2024.findings-eacl.147.note.zip</attachment>
      <bibkey>hasan-etal-2024-llm</bibkey>
      <revision id="1" href="2024.findings-eacl.147v1" hash="d405c649"/>
      <revision id="2" href="2024.findings-eacl.147v2" hash="ac47d4fc" date="2024-05-02">Minor updates.</revision>
      <video href="2024.findings-eacl.147.mp4"/>
    </paper>
    <paper id="148">
      <title><fixed-case>ICE</fixed-case>-Score: Instructing Large Language Models to Evaluate Code</title>
      <author><first>Terry Yue</first><last>Zhuo</last></author>
      <pages>2232-2242</pages>
      <abstract>Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code intelligence tasks. Moreover, utilizing human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose ICE-Score, a new evaluation metric via instructing large language models (LLMs) for code assessments. Our metric addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references. We evaluate the efficacy of our metric on two different aspects (<i>human preference</i> and <i>execution success</i>) and four programming languages. Our results demonstrate that our metric surpasses state-of-the-art metrics for code generation, delivering high levels of accuracy and consistency across various programming languages and tasks. We also make our evaluation metric and datasets available to the public, encouraging further research in evaluating code intelligence tasks.</abstract>
      <url hash="f121a28a">2024.findings-eacl.148</url>
      <bibkey>zhuo-2024-ice</bibkey>
    </paper>
    <paper id="149">
      <title><fixed-case>CR</fixed-case>e<fixed-case>SE</fixed-case>: Benchmark Data and Automatic Evaluation Framework for Recommending Eligibility Criteria from Clinical Trial Information</title>
      <author><first>Siun</first><last>Kim</last></author>
      <author><first>Jung-Hyun</first><last>Won</last><affiliation>Seoul National University</affiliation></author>
      <author><first>David</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Renqian</first><last>Luo</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lijun</first><last>Wu</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Howard</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <pages>2243-2273</pages>
      <abstract>Eligibility criteria (EC) refer to a set of conditions an individual must meet to participate in a clinical trial, defining the study population and minimizing potential risks to patients. Previous research in clinical trial design has been primarily focused on searching for similar trials and generating EC within manual instructions, employing similarity-based performance metrics, which may not fully reflect human judgment. In this study, we propose a novel task of recommending EC based on clinical trial information, including trial titles, and introduce an automatic evaluation framework to assess the clinical validity of the EC recommendation model. Our new approach, known as CReSE (Contrastive learning and Rephrasing-based and Clinical Relevance-preserving Sentence Embedding), represents EC through contrastive learning and rephrasing via large language models (LLMs). The CReSE model outperforms existing language models pre-trained on the biomedical domain in EC clustering. Additionally, we have curated a benchmark dataset comprising 3.2M high-quality EC-title pairs extracted from 270K clinical trials available on ClinicalTrials.gov. The EC recommendation models achieve commendable performance metrics, with 49.0% precision@1 and 44.2% MAP@5 on our evaluation framework. We expect that our evaluation framework built on the CReSE model will contribute significantly to the development and assessment of the EC recommendation models in terms of clinical validity.</abstract>
      <url hash="9b808224">2024.findings-eacl.149</url>
      <attachment type="note" hash="a01f614e">2024.findings-eacl.149.note.zip</attachment>
      <bibkey>kim-etal-2024-crese</bibkey>
      <video href="2024.findings-eacl.149.mp4"/>
    </paper>
    <paper id="150">
      <title><fixed-case>BMX</fixed-case>: Boosting Natural Language Generation Metrics with Explainability</title>
      <author><first>Christoph</first><last>Leiter</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Hoa</first><last>Nguyen</last></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>Universität Bielefeld</affiliation></author>
      <pages>2274-2288</pages>
      <abstract>State-of-the-art natural language generation evaluation metrics are based on black-box language models. Hence, recent works consider their explainability with the goals of better understandability for humans and better metric analysis, including failure cases. In contrast, we explicitly leverage explanations to boost the metrics’ performance. In particular, we perceive feature importance explanations as word-level scores, which we convert, via power means, into a segment-level score. We then combine this segment-level score with the original metric to obtain a better metric. Our tests show improvements for multiple metrics across MT and summarization datasets. While improvements on machine translation are small, they are strong for summarization. Notably, BMX with the LIME explainer and preselected parameters achieves an average improvement of 0.087 points in Spearman correlation on the system-level evaluation of SummEval.</abstract>
      <url hash="98e48521">2024.findings-eacl.150</url>
      <attachment type="software" hash="bb92b528">2024.findings-eacl.150.software.zip</attachment>
      <attachment type="note" hash="8c28b8db">2024.findings-eacl.150.note.zip</attachment>
      <bibkey>leiter-etal-2024-bmx</bibkey>
      <video href="2024.findings-eacl.150.mp4"/>
    </paper>
    <paper id="151">
      <title>Joint Inference of Retrieval and Generation for Passage Re-ranking</title>
      <author><first>Wei</first><last>Fang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yung-Sung</first><last>Chuang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>2289-2298</pages>
      <abstract>Passage retrieval is a crucial component of modern open-domain question answering (QA) systems, providing information for downstream QA components to generate accurate and transparent answers. In this study we focus on passage re-ranking, proposing a simple yet effective method, Joint Passage Re-ranking (JPR), that optimizes the mutual information between query and passage distributions, integrating both cross-encoders and generative models in the re-ranking process. Experimental results demonstrate that JPR outperforms conventional re-rankers and language model scorers in both open-domain QA retrieval settings and diverse retrieval benchmarks under zero-shot settings.</abstract>
      <url hash="b861c24e">2024.findings-eacl.151</url>
      <attachment type="software" hash="45613d80">2024.findings-eacl.151.software.zip</attachment>
      <bibkey>fang-etal-2024-joint</bibkey>
      <video href="2024.findings-eacl.151.mp4"/>
    </paper>
    <paper id="152">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>S</fixed-case>tudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Jianguo</first><last>Zhang</last><affiliation>SalesForce AI Research</affiliation></author>
      <author><first>Kun</first><last>Qian</last></author>
      <author><first>Zhiwei</first><last>Liu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Shelby</first><last>Heinecke</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Rui</first><last>Meng</last><affiliation>SalesForce Research</affiliation></author>
      <author><first>Ye</first><last>Liu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <author><first>Huan</first><last>Wang</last></author>
      <author><first>Silvio</first><last>Savarese</last><affiliation>Salesforce and Stanford University</affiliation></author>
      <author><first>Caiming</first><last>Xiong</last><affiliation>Salesforce Research</affiliation></author>
      <pages>2299-2315</pages>
      <abstract>Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training.To further enhance the utility of DialogStudio, we identify the licenses for each dataset, design external knowledge and domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. To improve transparency and support dataset and task-based research, as well as language model pre-training, all datasets, licenses, codes, and models associated with DialogStudio will be made publicly accessible.</abstract>
      <url hash="152de025">2024.findings-eacl.152</url>
      <attachment type="note" hash="81f074a6">2024.findings-eacl.152.note.zip</attachment>
      <bibkey>zhang-etal-2024-dialogstudio</bibkey>
    </paper>
    <paper id="153">
      <title>Exploring hybrid approaches to readability: experiments on the complementarity between linguistic features and transformers</title>
      <author><first>Rodrigo</first><last>Wilkens</last><affiliation>UCL</affiliation></author>
      <author><first>Patrick</first><last>Watrin</last><affiliation>UCL</affiliation></author>
      <author><first>Rémi</first><last>Cardon</last><affiliation>Cental, ILC - UCLouvain</affiliation></author>
      <author><first>Alice</first><last>Pintard</last></author>
      <author><first>Isabelle</first><last>Gribomont</last><affiliation>UCLouvain</affiliation></author>
      <author><first>Thomas</first><last>François</last><affiliation>UCL</affiliation></author>
      <pages>2316-2331</pages>
      <abstract>Linguistic features have a strong contribution in the context of the automatic assessment of text readability (ARA). They have been one of the anchors between the computational and theoretical models. With the development in the ARA field, the research moved to Deep Learning (DL). In an attempt to reconcile the mixed results reported in this context, we present a systematic comparison of 6 hybrid approaches along with standard Machine Learning and DL approaches, on 4 corpora (different languages and target audiences). The various experiments clearly highlighted two rather simple hybridization methods (soft label and simple concatenation). They also appear to be the most robust on smaller datasets and across various tasks and languages. This study stands out as the first to systematically compare different architectures and approaches to feature hybridization in DL, as well as comparing performance in terms of two languages and two target audiences of the text, which leads to a clearer pattern of results.</abstract>
      <url hash="4f4857c4">2024.findings-eacl.153</url>
      <bibkey>wilkens-etal-2024-exploring</bibkey>
      <video href="2024.findings-eacl.153.mp4"/>
    </paper>
    <paper id="154">
      <title>Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models</title>
      <author><first>Maxime</first><last>Fily</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last><affiliation>LLF / Université Paris Cité</affiliation></author>
      <author><first>Severine</first><last>Guillaume</last><affiliation>CNRS</affiliation></author>
      <author><first>Gilles</first><last>Adda</last><affiliation>CNRS</affiliation></author>
      <author><first>Alexis</first><last>Michaud</last><affiliation>CNRS</affiliation></author>
      <pages>2332-2341</pages>
      <abstract>In the highly constrained context of low-resource language studies, we explore vector representations of speech from a pretrained model to determine their level of abstraction with regard to the audio signal. We propose a new unsupervised method using ABX tests on audio recordings with carefully curated metadata to shed light on the type of information present in the representations. ABX tests determine whether the representations computed by a multilingual speech model encode a given characteristic. Three experiments are devised: one on room acoustics aspects, one on linguistic genre, and one on phonetic aspects. The results confirm that the representations extracted from recordings with different linguistic/extra-linguistic characteristics differ along the same lines. Embedding more audio signal in one vector better discriminates extra-linguistic characteristics, whereas shorter snippets are better to distinguish segmental information. The method is fully unsupervised, potentially opening new research avenues for comparative work on under-documented languages.</abstract>
      <url hash="be4bc756">2024.findings-eacl.154</url>
      <bibkey>fily-etal-2024-establishing</bibkey>
      <video href="2024.findings-eacl.154.mp4"/>
    </paper>
    <paper id="155">
      <title>The Queen of <fixed-case>E</fixed-case>ngland is not <fixed-case>E</fixed-case>ngland’s Queen: On the Lack of Factual Coherency in <fixed-case>PLM</fixed-case>s</title>
      <author><first>Paul</first><last>Youssef</last><affiliation>Phillips-Universität Marburg</affiliation></author>
      <author><first>Jörg</first><last>Schlötterer</last><affiliation>Universität Mannheim and Phillips-Universität Marburg</affiliation></author>
      <author><first>Christin</first><last>Seifert</last><affiliation>Phillips-Universität Marburg and University of Twente</affiliation></author>
      <pages>2342-2354</pages>
      <abstract>Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an _object_ entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the _subject_ entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from their parameters in a coherent manner, and to be considered as knowledge bases.</abstract>
      <url hash="5a44cdb4">2024.findings-eacl.155</url>
      <bibkey>youssef-etal-2024-queen</bibkey>
      <video href="2024.findings-eacl.155.mp4"/>
    </paper>
    <paper id="156">
      <title><fixed-case>H</fixed-case>ierarchy<fixed-case>N</fixed-case>et: Learning to Summarize Source Code with Heterogeneous Representations</title>
      <author><first>Minh</first><last>Nguyen</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Nghi</first><last>Bui</last></author>
      <author><first>Truong Son</first><last>Hy</last><affiliation>Indiana State University</affiliation></author>
      <author><first>Long</first><last>Tran-Thanh</last><affiliation>The university of Warwick</affiliation></author>
      <author><first>Tien</first><last>Nguyen</last><affiliation>university of texas at dallas</affiliation></author>
      <pages>2355-2367</pages>
      <abstract>Code representation is important to machine learning models in the code-related applications. Existing code summarization approaches primarily leverage Abstract Syntax Trees (ASTs) and sequential information from source code to generate code summaries while often overlooking the critical consideration of the interplay of dependencies among code elements and code hierarchy. However, effective summarization necessitates a holistic analysis of code snippets from three distinct aspects: lexical, syntactic, and semantic information. In this paper, we propose a novel code summarization approach utilizing Heterogeneous Code Representations (HCRs) and our specially designed HierarchyNet. HCRs adeptly capture essential code features at lexical, syntactic, and semantic levels within a hierarchical structure. HierarchyNet processes each layer of the HCR separately, employing a Heterogeneous Graph Transformer, a Tree-based CNN, and a Transformer Encoder. In addition, HierarchyNet demonstrates superior performance compared to fine-tuned pre-trained models, including CodeT5, and CodeBERT, as well as large language models that employ zero/few-shot settings, such as CodeLlama, StarCoder, and CodeGen. Implementation details can be found at https://github.com/FSoft-AI4Code/HierarchyNet.</abstract>
      <url hash="3eef3fbe">2024.findings-eacl.156</url>
      <bibkey>nguyen-etal-2024-hierarchynet</bibkey>
      <video href="2024.findings-eacl.156.mp4"/>
    </paper>
    <paper id="157">
      <title>Understanding the effects of language-specific class imbalance in multilingual fine-tuning</title>
      <author><first>Vincent</first><last>Jung</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Lonneke</first><last>Plas</last><affiliation>Idiap Research Institute</affiliation></author>
      <pages>2368-2376</pages>
      <abstract>We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.</abstract>
      <url hash="a17b7cc2">2024.findings-eacl.157</url>
      <bibkey>jung-plas-2024-understanding</bibkey>
      <video href="2024.findings-eacl.157.mp4"/>
    </paper>
    <paper id="158">
      <title><fixed-case>NL</fixed-case>2<fixed-case>F</fixed-case>ormula: Generating Spreadsheet Formulas from Natural Language Queries</title>
      <author><first>Wei</first><last>Zhao</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Zhitao</first><last>Hou</last></author>
      <author><first>Siyuan</first><last>Wu</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Haoyu</first><last>Dong</last></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Hongyu</first><last>Zhang</last><affiliation>University of Newcastle, Australia</affiliation></author>
      <author><first>Yulei</first><last>Sui</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Haidong</first><last>Zhang</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>2377-2388</pages>
      <abstract>Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.</abstract>
      <url hash="310ef33c">2024.findings-eacl.158</url>
      <bibkey>zhao-etal-2024-nl2formula</bibkey>
    </paper>
  </volume>
  <volume id="naacl" ingest-date="2024-06-15" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics: NAACL 2024</booktitle>
      <editor><first>Kevin</first><last>Duh</last></editor>
      <editor><first>Helena</first><last>Gomez</last></editor>
      <editor><first>Steven</first><last>Bethard</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="245bf65c">2024.findings-naacl</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="a5d6802e">2024.findings-naacl.0</url>
      <bibkey>findings-2024-findings-association</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Structured Pruning for Large Language Models Using Coupled Components Elimination and Minor Fine-tuning</title>
      <author><first>Honghe</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>XiaolongShi</first><last>XiaolongShi</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jingwei</first><last>Sun</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Guangzhong</first><last>Sun</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>1-12</pages>
      <abstract>Large language models (LLMs) have demonstrated powerful capabilities in natural language processing, yet their vast number of parameters poses challenges for deployment and inference efficiency. Structured model pruning emerges as a viable approach to reduce model size and accelerate inference, without requiring specialized operators and libraries for deployment. However, structured pruning often severely weakens the model’s capability.Despite repetitive fine-tuning can restore the capability to a certain extent, it impairs LLMs’ utility as versatile problem solvers.To address this issue, we propose a novel structured pruning algorithm tailored for LLMs. It derives the importance of different components, namely rows and columns in parameter matrices, based on intermediate data dependencies. Then it removes coupled components across different layers simultaneously and preserves dependency relationships within remaining parameters, avoiding significant performance degradation. The pruned model requires only few epochs of fine-tuning to restore its performance, ensuring the model’s ability to generalize.Empirical evaluations on LLaMA, Vicuna, and ChatGLM3 demonstrate our algorithm’s efficacy, yielding 20% parameter reduction while retaining at least 94.4% of original performance metrics.</abstract>
      <url hash="1f64d6b4">2024.findings-naacl.1</url>
      <bibkey>zhang-etal-2024-structured</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.1</doi>
    </paper>
    <paper id="2">
      <title>Weight-Inherited Distillation for Task-Agnostic <fixed-case>BERT</fixed-case> Compression</title>
      <author><first>Taiqiang</first><last>Wu</last></author>
      <author><first>Cheng</first><last>Hou</last></author>
      <author><first>Shanshan</first><last>Lao</last></author>
      <author><first>Jiayi</first><last>Li</last></author>
      <author><first>Ngai</first><last>Wong</last><affiliation>The University of Hong Kong</affiliation></author>
      <author><first>Zhe</first><last>Zhao</last></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Graduate School at Shenzhen,Tsinghua University</affiliation></author>
      <pages>13-28</pages>
      <abstract>Knowledge Distillation (KD) is a predominant approach for BERT compression.Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model.These methods transfer the knowledge in an indirect way.In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher.WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation.Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization.Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines.Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.The code is available at https://github.com/wutaiqiang/WID-NAACL2024.</abstract>
      <url hash="29bdc519">2024.findings-naacl.2</url>
      <bibkey>wu-etal-2024-weight</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.2</doi>
    </paper>
    <paper id="3">
      <title>Ignore Me But Don’t Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain</title>
      <author><first>Eugene</first><last>Jang</last><affiliation>S2W Inc.</affiliation></author>
      <author><first>Jian</first><last>Cui</last></author>
      <author><first>Dayeon</first><last>Yim</last><affiliation>S2W Inc.</affiliation></author>
      <author><first>Youngjin</first><last>Jin</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jin-Woo</first><last>Chung</last></author>
      <author><first>Seungwon</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Yongjae</first><last>Lee</last><affiliation>S2W Inc.</affiliation></author>
      <pages>29-42</pages>
      <abstract>Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We experiment with different pretraining methodologies to account for non-linguistic elements (NLEs) and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy, a combination of selective MLM and jointly training NLE token classification, outperforms the commonly taken approach of replacing NLEs. We use our domain-customized methodology to train CyBERTuned, a cybersecurity domain language model that outperforms other cybersecurity PLMs on most tasks.</abstract>
      <url hash="167361fa">2024.findings-naacl.3</url>
      <bibkey>jang-etal-2024-ignore</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.3</doi>
    </paper>
    <paper id="4">
      <title>Extremely efficient online query encoding for dense retrieval</title>
      <author><first>Nachshon</first><last>Cohen</last><affiliation>Amazon</affiliation></author>
      <author><first>Yaron</first><last>Fairstein</last><affiliation>Amazon</affiliation></author>
      <author><first>Guy</first><last>Kushilevitz</last><affiliation>Amazon</affiliation></author>
      <pages>43-50</pages>
      <abstract>Existing dense retrieval systems utilize the same model architecture for encoding both the passages and the queries, even though queries are much shorter and simpler than passages. This leads to high latency of the query encoding, which is performed online and therefore might impact user experience. We show that combining a standard large passage encoder with a small efficient query encoder can provide significant latency drops with only a small decrease in quality. We offer a pretraining and training solution for multiple small query encoder architectures. Using a small transformer architecture we are able to decrease latency by up to <tex-math>\sim12\times</tex-math>, while <tex-math>MRR@10</tex-math> on the MS MARCO dev set only decreases from 38.2 to 36.2. If this solution does not reach the desired latency requirements, we propose an efficient RNN as the query encoder, which processes the query prefix incrementally and only infers the last word after the query is issued. This shortens latency by <tex-math>\sim38\times</tex-math> with only a minor drop in quality, reaching 35.5 <tex-math>MRR@10</tex-math> score.</abstract>
      <url hash="dc71300e">2024.findings-naacl.4</url>
      <bibkey>cohen-etal-2024-extremely</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>DIVKNOWQA</fixed-case>: Assessing the Reasoning Ability of <fixed-case>LLM</fixed-case>s via Open-Domain Question Answering over Knowledge Base and Text</title>
      <author><first>Wenting</first><last>Zhao</last></author>
      <author><first>Ye</first><last>Liu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Tong</first><last>Niu</last><affiliation>Salesforce AI Research</affiliation></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Philip</first><last>Yu</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Yingbo</first><last>Zhou</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Semih</first><last>Yavuz</last><affiliation>SalesForce.com</affiliation></author>
      <pages>51-68</pages>
      <abstract>Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrievalaugmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) Generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.</abstract>
      <url hash="129cb241">2024.findings-naacl.5</url>
      <bibkey>zhao-etal-2024-divknowqa</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.5</doi>
    </paper>
    <paper id="6">
      <title><fixed-case>S</fixed-case>peed<fixed-case>E</fixed-case>: <fixed-case>E</fixed-case>uclidean Geometric Knowledge Graph Embedding Strikes Back</title>
      <author><first>Aleksandar</first><last>Pavlović</last></author>
      <author><first>Emanuel</first><last>Sallinger</last><affiliation>TU Wien (Vienna University of Technology)</affiliation></author>
      <pages>69-92</pages>
      <abstract>Geometric knowledge graph embedding models (gKGEs) have shown great potential for knowledge graph completion (KGC), i.e., automatically predicting missing triples. However, contemporary gKGEs require high embedding dimensionalities or complex embedding spaces for good KGC performance, drastically limiting their space and time efficiency. Facing these challenges, we propose SpeedE, a lightweight Euclidean gKGE that (1) provides strong inference capabilities, (2) is competitive with state-of-the-art gKGEs, even significantly outperforming them on YAGO3-10 and WN18RR, and (3) dramatically increases their efficiency, in particular, needing solely a fifth of the training time and a fourth of the parameters of the state-of-the-art ExpressivE model on WN18RR to reach the same KGC performance.</abstract>
      <url hash="95561bcf">2024.findings-naacl.6</url>
      <bibkey>pavlovic-sallinger-2024-speede</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.6</doi>
    </paper>
    <paper id="7">
      <title>Language Guided Exploration for <fixed-case>RL</fixed-case> Agents in Text Environments</title>
      <author><first>Hitesh</first><last>Golchha</last></author>
      <author><first>Sahil</first><last>Yerawar</last></author>
      <author><first>Dhruvesh</first><last>Patel</last><affiliation>College of Information and Computer Science, University of Massachusetts, Amherst</affiliation></author>
      <author><first>Soham</first><last>Dan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Keerthiram</first><last>Murugesan</last><affiliation>International Business Machines</affiliation></author>
      <pages>93-102</pages>
      <abstract>Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like <tex-math>\textit{tabula rasa}</tex-math> reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts. In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER ). We observe that on ScienceWorld (Wang et al., 2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer.</abstract>
      <url hash="02bec4cd">2024.findings-naacl.7</url>
      <bibkey>golchha-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>GPT</fixed-case>-who: An Information Density-based Machine-Generated Text Detector</title>
      <author><first>Saranya</first><last>Venkatraman</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Adaku</first><last>Uchendu</last><affiliation>MIT Lincoln Laboratory, Massachusetts Institute of Technology</affiliation></author>
      <author><first>Dongwon</first><last>Lee</last><affiliation>The Pennsylvania State University</affiliation></author>
      <pages>103-115</pages>
      <abstract>The Uniform Information Density (UID) principle posits that humans prefer to spread information evenly during language production. We examine if this UID principle can help capture differences between Large Language Models (LLMs)-generated and human-generated texts. We propose GPT-who, the first psycholinguistically-inspired domain-agnostic statistical detector. This detector employs UID-based featuresto model the unique statistical signature of each LLM and human author for accurate detection. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- &amp; non-statistical) such as GLTR, GPTZero, DetectGPT, OpenAI detector, and ZeroGPT by over 20% across domains.In addition to better performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We find that GPT-who can distinguish texts generated by very sophisticated LLMs, even when the overlying text is indiscernible.UID-based measures for all datasets and code are available at https://github.com/saranya-venkatraman/gpt-who.</abstract>
      <url hash="966c5990">2024.findings-naacl.8</url>
      <bibkey>venkatraman-etal-2024-gpt</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>DEED</fixed-case>: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models</title>
      <author><first>Peng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Pengkai</first><last>Zhu</last><affiliation>Boston University</affiliation></author>
      <author><first>Tian</first><last>Li</last></author>
      <author><first>Srikar</first><last>Appalaraju</last><affiliation>Amazon</affiliation></author>
      <author><first>Vijay</first><last>Mahadevan</last><affiliation>Amazon</affiliation></author>
      <author><first>R.</first><last>Manmatha</last><affiliation>Amazon</affiliation></author>
      <pages>116-131</pages>
      <abstract>Encoder-decoder transformer models have achieved great success on various vision-language (VL) and language tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an approach of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer model which is trained with deep supervision so that each of its decoder layers is capable of generating plausible predictions. In addition, we leverage simple yet practical techniques, including shared generation head and adaptation modules, to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit model, we perform step-level dynamic early exit during inference, where the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step. Considering different number of decoder layers may be used at different decoding steps, we compute deeper-layer decoder features of previous decoding steps just-in-time, which ensures the features from different decoding steps are semantically aligned. We evaluate our approach with three state-of-the-art encoder-decoder transformer models on various VL and language tasks. We show our approach can reduce overall inference latency by 20%-74% with comparable or even higher accuracy compared to baselines.</abstract>
      <url hash="22b2d365">2024.findings-naacl.9</url>
      <bibkey>tang-etal-2024-deed</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.9</doi>
    </paper>
    <paper id="10">
      <title>Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation</title>
      <author><first>Ta-Chung</first><last>Chi</last></author>
      <author><first>Ting-Han</first><last>Fan</last></author>
      <author><first>Alexander</first><last>Rudnicky</last><affiliation>Carnegie Mellon University and Carnegie Mellon University</affiliation></author>
      <pages>132-148</pages>
      <abstract>An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any fine-tuning. Such long-context utilization capability relies heavily on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings show improvement on the long-context utilization capability of T5 on language modeling, retrieval, multi-document question answering, and code completion tasks without any fine-tuning. This suggests that a flexible positional embedding design and attention alignment can go a long way toward Transformer length extrapolation. The code is released at: <url>https://github.com/chijames/T5-Attention-Alignment</url></abstract>
      <url hash="c0a735b0">2024.findings-naacl.10</url>
      <bibkey>chi-etal-2024-attention</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.10</doi>
    </paper>
    <paper id="11">
      <title>Automatic Pair Construction for Contrastive Post-training</title>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Corby</first><last>Rosset</last></author>
      <author><first>Ethan</first><last>Chau</last><affiliation>Microsoft</affiliation></author>
      <author><first>Luciano</first><last>Corro</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Shweti</first><last>Mahajan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>Jennifer</first><last>Neville</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <author><first>Ahmed</first><last>Awadallah</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Nikhil</first><last>Rao</last><affiliation>Microsoft</affiliation></author>
      <pages>149-162</pages>
      <abstract>Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from “easier” pairs and transitioning to “harder” ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.</abstract>
      <url hash="696bc1db">2024.findings-naacl.11</url>
      <bibkey>xu-etal-2024-automatic</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.11</doi>
    </paper>
    <paper id="12">
      <title>Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models</title>
      <author><first>Miaoran</first><last>Li</last><affiliation>Iowa State University</affiliation></author>
      <author><first>Baolin</first><last>Peng</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Michel</first><last>Galley</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Zhu</first><last>Zhang</last></author>
      <pages>163-181</pages>
      <abstract>Fact-checking is an essential task in NLP that is commonly utilized to validate the factual accuracy of a piece of text. Previous approaches mainly involve the resource-intensive process of fine-tuning pre-trained language models on specific datasets. In addition, there is a notable gap in datasets that focus on fact-checking texts generated by large language models (LLMs). In this paper, we introduce Self-Checker, a plug-and-play framework that harnesses LLMs for efficient and rapid fact-checking in a few-shot manner. We also present the BingCheck dataset, specifically designed for fact-checking texts generated by LLMs. Empirical results demonstrate the potential of Self-Checker in the use of LLMs for fact-checking. Compared to state-of-the-art fine-tuned models, there is still significant room for improvement, indicating that adopting LLMs could be a promising direction for future fact-checking research.</abstract>
      <url hash="8419f64c">2024.findings-naacl.12</url>
      <bibkey>li-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.12</doi>
    </paper>
    <paper id="13">
      <title>Low-resource neural machine translation with morphological modeling</title>
      <author><first>Antoine</first><last>Nzeyimana</last></author>
      <pages>182-195</pages>
      <abstract>Morphological modeling in neural machine translation (NMT) is a promising approach to achieving open-vocabulary machine translation for morphologically-rich languages. However, existing methods such as sub-word tokenization and character-based models are limited to the surface forms of the words. In this work, we propose a framework-solution for modeling complex morphology in low-resource settings. A two-tier transformer architecture is chosen to encode morphological information at the inputs. At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve machine translation performance. An attention augmentation scheme to the transformer model is proposed in a generic form to allow integration of pre-trained language models and also facilitate modeling of word order relationships between the source and target languages. Several data augmentation techniques are evaluated and shown to increase translation performance in low-resource settings. We evaluate our proposed solution on Kinyarwanda <tex-math>\leftrightarrow</tex-math> English translation using public-domain parallel text. Our final models achieve competitive performance in relation to large multi-lingual models. We hope that our results will motivate more use of explicit morphological information and the proposed model and data augmentations in low-resource NMT.</abstract>
      <url hash="9ae0daf4">2024.findings-naacl.13</url>
      <bibkey>nzeyimana-2024-low</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.13</doi>
    </paper>
    <paper id="14">
      <title>Self-Cleaning: Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances</title>
      <author><first>Zhendong</first><last>Chu</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Rajiv</first><last>Jain</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Vlad</first><last>Morariu</last><affiliation>Adobe</affiliation></author>
      <author><first>Jiuxiang</first><last>Gu</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ani</first><last>Nenkova</last><affiliation>Adobe Research</affiliation></author>
      <pages>196-210</pages>
      <abstract>To achieve state-of-the-art performance, one still needs to train NER models on large-scale, high-quality annotated data, an asset that is both costly and time-intensive to accumulate. In contrast, real-world applications often resort to massive low-quality labeled data through non-expert annotators via crowdsourcing and external knowledge bases via distant supervision as a cost-effective alternative. However, these annotation methods result in noisy labels, which in turn lead to a notable decline in performance. Hence, we propose to denoise the noisy NER data with guidance from a small set of clean instances. Along with the main NER model we train a discriminator model and use its outputs to recalibrate the sample weights. The discriminator is capable of detecting both span and category errors with different discriminative prompts. Results on public crowdsourcing and distant supervision datasets show that the proposed method can consistently improve performance with a small guidance set.</abstract>
      <url hash="092120ad">2024.findings-naacl.14</url>
      <bibkey>chu-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.14</doi>
    </paper>
    <paper id="15">
      <title><fixed-case>VLUE</fixed-case>: A New Benchmark and Multi-task Knowledge Transfer Learning for <fixed-case>V</fixed-case>ietnamese Natural Language Understanding</title>
      <author><first>Phong</first><last>Do</last><affiliation>The UIT NLP Group and Zalo</affiliation></author>
      <author><first>Son</first><last>Tran</last></author>
      <author><first>Phu</first><last>Hoang</last></author>
      <author><first>Kiet</first><last>Nguyen</last><affiliation>University of Information Technology, VNU-HCM</affiliation></author>
      <author><first>Ngan</first><last>Nguyen</last></author>
      <pages>211-222</pages>
      <abstract>The success of Natural Language Understanding (NLU) benchmarks in various languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across a wide range of tasks. To establish a standardized set of benchmarks for Vietnamese NLU, we introduce the first Vietnamese Language Understanding Evaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets covering different NLU tasks, including text classification, span extraction, and natural language understanding. To provide an insightful overview of the current state of Vietnamese NLU, we then evaluate seven state-of-the-art pre-trained models, including both multilingual and Vietnamese monolingual models, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new state-of-the-art pre-trained model that achieves superior results across all tasks in the VLUE benchmark. Our model combines the proficiency of a multilingual pre-trained model with Vietnamese linguistic knowledge. CafeBERT is developed based on the XLM-RoBERTa model, with an additional pretraining step utilizing a significant amount of Vietnamese textual data to enhance its adaptation to the Vietnamese language. For the purpose of future research, CafeBERT is made publicly available for research purposes.</abstract>
      <url hash="bb7b2e7a">2024.findings-naacl.15</url>
      <bibkey>do-etal-2024-vlue</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>LETI</fixed-case>: Learning to Generate from Textual Interactions</title>
      <author><first>Xingyao</first><last>Wang</last><affiliation>Department of Computer Science, University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Hao</first><last>Peng</last><affiliation>Department of Computer Science, University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Reyhaneh</first><last>Jabbarvand</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>223-239</pages>
      <abstract>Fine-tuning pre-trained language models (LMs) is essential for enhancing their capabilities.Existing techniques commonly fine-tune on input-output pairs (e.g., instruction tuning) or with numerical rewards that gauge the output quality (e.g., RLHF). We explore LMs’ potential to **le**arn from **t**extual **i**nteractions (**LETI**) that not only check their correctness with *binary labels* but also pinpoint and explain errors in their outputs through *textual feedback*.Our focus is the code generation task, where the model produces code based on natural language instructions. This setting invites a natural and scalable way to acquire textual feedback: the error messages and stack traces from code execution using a Python interpreter. LETI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback. Prepended to this fine-tuning text, a binary reward token is used to differentiate correct and buggy solutions.LETI requires *no* ground-truth outputs for training and even outperforms a fine-tuned baseline that does. LETI not only improves the performance of LMs on a code generation dataset MBPP, but also generalizes to other datasets. Trained on MBPP, it achieves comparable or better performance than the base LMs on unseen problems in HumanEval. Furthermore, compared to binary feedback, we observe that textual feedback leads to improved generation quality and sample efficiency, achieving the same performance with fewer than half of the gradient steps.LETI is equally applicable in natural language tasks when they can be formulated as code generation, which we empirically verified on event argument extraction.</abstract>
      <url hash="38bf50e0">2024.findings-naacl.16</url>
      <bibkey>wang-etal-2024-leti</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.16</doi>
    </paper>
    <paper id="17">
      <title>Bilateral Masking with prompt for Knowledge Graph Completion</title>
      <author><first>Yonghui</first><last>Kong</last></author>
      <author><first>Cunhang</first><last>Fan</last><affiliation>School of Computer Science and Technology, Anhui University, Hefei 230601, China</affiliation></author>
      <author><first>Yujie</first><last>Chen</last></author>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Zhao</first><last>Lv</last><affiliation>School of Computer Science and Technology, Anhui University, Hefei 230601, China</affiliation></author>
      <author><first>Jianhua</first><last>Tao</last><affiliation>Tsinghua University</affiliation></author>
      <pages>240-249</pages>
      <abstract>The pre-trained language model (PLM) has achieved significant success in the field of knowledge graph completion (KGC) by effectively modeling entity and relation descriptions. In recent studies, the research in this field has been categorized into methods based on word matching and sentence matching, with the former significantly lags behind. However, there is a critical issue in word matching methods, which is that these methods fail to obtain satisfactory single embedding representations for entities.To address this issue and enhance entity representation, we propose the Bilateral Masking with prompt for Knowledge Graph Completion (BMKGC) approach.Our methodology employs prompts to narrow the distance between the predicted entity and the known entity. Additionally, the BMKGC model incorporates a bi-encoder architecture, enabling simultaneous predictions at both the head and tail. Furthermore, we propose a straightforward technique to augment positive samples, mitigating the problem of degree bias present in knowledge graphs and thereby improving the model’s robustness. Experimental results conclusively demonstrate that BMKGC achieves state-of-the-art performance on the WN18RR dataset.</abstract>
      <url hash="9bd03394">2024.findings-naacl.17</url>
      <bibkey>kong-etal-2024-bilateral</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.17</doi>
    </paper>
    <paper id="18">
      <title><fixed-case>M</fixed-case>i<fixed-case>L</fixed-case>e Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models</title>
      <author><first>Zhenpeng</first><last>Su</last></author>
      <author><first>Zijia</first><last>Lin</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Baixue</first><last>Baixue</last></author>
      <author><first>Hui</first><last>Chen</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <author><first>Wei</first><last>Zhou</last></author>
      <author><first>Guiguang</first><last>Ding</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xing</first><last>W</last></author>
      <pages>250-262</pages>
      <abstract>Generative language models are usually pre-trained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose a **MiLe Loss** function for **mi**tigating the bias of **le**arning difficulties with tokens. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 468M, 1.2B, and 6.7B parameters. Experiments reveal that models incorporating the proposed MiLe Loss can gain consistent performance improvement on downstream benchmarks.</abstract>
      <url hash="69b7c705">2024.findings-naacl.18</url>
      <bibkey>su-etal-2024-mile</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.18</doi>
    </paper>
    <paper id="19">
      <title><fixed-case>GOLD</fixed-case>: Geometry Problem Solver with Natural Language Description</title>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>University of Strathclyde</affiliation></author>
      <author><first>Yashar</first><last>Moshfeghi</last><affiliation>University of Strathclyde</affiliation></author>
      <pages>263-278</pages>
      <abstract>Addressing the challenge of automated geometry math problem-solving in artificial intelligence (AI) involves understanding multi-modal information and mathematics. blackCurrent methods struggle with accurately interpreting geometry diagrams, which hinders effective problem-solving. To tackle this issue, we present the <b>G</b>eometry problem s<b>O</b>lver with natural <b>L</b>anguage <b>D</b>escription (GOLD) model. GOLD enhances the extraction of geometric relations by separately processing symbols and geometric primitives within the diagram. Subsequently, it converts the extracted relations into natural language descriptions, efficiently utilizing large language models to solve geometry math problems. Experiments show that the GOLD model outperforms the Geoformer model, the previous best method on the UniGeo dataset, by achieving accuracy improvements of 12.7% and 42.1% in calculation and proving subsets. Additionally, it surpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet, by obtaining accuracy enhancements of 1.8% and 3.2%, respectively.</abstract>
      <url hash="0904ecb2">2024.findings-naacl.19</url>
      <bibkey>zhang-moshfeghi-2024-gold</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>R</fixed-case>o<fixed-case>D</fixed-case>ia: A New Dataset for <fixed-case>R</fixed-case>omanian Dialect Identification from Speech</title>
      <author><first>Rotaru</first><last>Codruț</last></author>
      <author><first>Nicolae</first><last>Ristea</last></author>
      <author><first>Radu</first><last>Ionescu</last><affiliation>Universitatea Bucuresti</affiliation></author>
      <pages>279-286</pages>
      <abstract>We introduce RoDia, the first dataset for Romanian dialect identification from speech. The RoDia dataset includes a varied compilation of speech samples from five distinct regions of Romania, covering both urban and rural environments, totaling 2 hours of manually annotated speech data. Along with our dataset, we introduce a set of competitive models to be used as baselines for future research. The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging. We thus believe that RoDia is a valuable resource that will stimulate research aiming to address the challenges of Romanian dialect identification. We release our dataset at https://github.com/codrut2/RoDia.</abstract>
      <url hash="0f2d6f23">2024.findings-naacl.20</url>
      <bibkey>codrut-etal-2024-rodia</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.20</doi>
    </paper>
    <paper id="21">
      <title>Examining Modularity in Multilingual <fixed-case>LM</fixed-case>s via Language-Specialized Subnetworks</title>
      <author><first>Rochelle</first><last>Choenni</last></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Dan</first><last>Garrette</last><affiliation>Google DeepMind</affiliation></author>
      <pages>287-301</pages>
      <abstract>Recent work has proposed explicitly inducing language-wise modularity in multilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a means of better guiding cross-lingual sharing. In this paper, we investigate (1) the degree to which language-wise modularity *naturally* arises within models with no special modularity interventions, and (2) how cross-lingual sharing and interference differ between such models and those with explicit SFT-guided subnetwork modularity. In order to do so, we use XLM-R as our multilingual LM. Moreover, to quantify language specialization and cross-lingual interaction, we use a Training Data Attribution method that estimates the degree to which a model’s predictions are influenced by in-language or cross-language training examples. Our results show that language-specialized subnetworks do naturally arise, and that SFT, rather than always increasing modularity, can decrease language specialization of subnetworks in favor of more cross-lingual sharing.</abstract>
      <url hash="6f28ec96">2024.findings-naacl.21</url>
      <bibkey>choenni-etal-2024-examining</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.21</doi>
    </paper>
    <paper id="22">
      <title>Reverse Chain: A Generic-Rule for <fixed-case>LLM</fixed-case>s to Master Multi-<fixed-case>API</fixed-case> Planning</title>
      <author><first>Yinger</first><last>Zhang</last></author>
      <author><first>Hui</first><last>Cai</last></author>
      <author><first>Xierui</first><last>Song</last></author>
      <author><first>Yicheng</first><last>Chen</last></author>
      <author><first>Rui</first><last>Sun</last></author>
      <author><first>Jing</first><last>Zheng</last><affiliation>Ant Group</affiliation></author>
      <pages>302-325</pages>
      <abstract>While enabling large language models to implement function calling (known as APIs) can greatly enhance the performance of Large Language Models (LLMs), function calling is still a challenging task due to the complicated relations between different APIs, especially in a context-learning setting without fine-tuning. This paper introduces “Reverse Chain”, a controllable, target-driven approach designed to empower LLMs with the capability to operate external APIs only via prompts. Recognizing that most LLMs have limited tool-use capabilities, Reverse Chain limits LLMs to executing simple tasks, e.g., API Selection and Argument Completion. Furthermore, to manage a controllable multi-function calling, Reverse Chain adopts a generic rule-based on a backward reasoning process. This rule determines when to do API selection or Argument completion. To evaluate the multi-tool-use capability of LLMs, we have released a compositional multi-tool task dataset, available at https://github.com/zhangyingerjelly/reverse-chain. Extensive numerical experiments validate the remarkable proficiency of Reverse Chain in managing multiple API calls.</abstract>
      <url hash="8281e51a">2024.findings-naacl.22</url>
      <bibkey>zhang-etal-2024-reverse</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.22</doi>
    </paper>
    <paper id="23">
      <title>Incorporating Exponential Smoothing into <fixed-case>MLP</fixed-case>: a Simple but Effective Sequence Model</title>
      <author><first>JiqunChu</first><last>JiqunChu</last></author>
      <author><first>Zuoquan</first><last>Lin</last><affiliation>Peking University</affiliation></author>
      <pages>326-337</pages>
      <abstract>Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.</abstract>
      <url hash="3cf1b316">2024.findings-naacl.23</url>
      <bibkey>jiqunchu-lin-2024-incorporating</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.23</doi>
    </paper>
    <paper id="24">
      <title><fixed-case>O</fixed-case>pen<fixed-case>FMN</fixed-case>av: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models</title>
      <author><first>Yuxuan</first><last>Kuang</last></author>
      <author><first>Hai</first><last>Lin</last><affiliation>University of Notre Dame and University of Notre Dame</affiliation></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>338-351</pages>
      <abstract>Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose **OpenFMNav**, an **Open**-set **F**oundation **M**odel based framework for zero-shot object **Nav**igation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user’s demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a *Versatile Semantic Score Map (VSSM)*. Then, by conducting common sense reasoning on *VSSM*, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method’s effectiveness. Furthermore, we perform real robot demonstrations to validate our method’s open-set-ness and generalizability to real-world environments.</abstract>
      <url hash="bf4a04f2">2024.findings-naacl.24</url>
      <bibkey>kuang-etal-2024-openfmnav</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.24</doi>
    </paper>
    <paper id="25">
      <title>Comparing Two Model Designs for Clinical Note Generation; Is an <fixed-case>LLM</fixed-case> a Useful Evaluator of Consistency?</title>
      <author><first>Nathan</first><last>Brake</last><affiliation>3M</affiliation></author>
      <author><first>Thomas</first><last>Schaaf</last></author>
      <pages>352-363</pages>
      <abstract>Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.</abstract>
      <url hash="b9736a92">2024.findings-naacl.25</url>
      <bibkey>brake-schaaf-2024-comparing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.25</doi>
    </paper>
    <paper id="26">
      <title><fixed-case>VOLTA</fixed-case>: Improving Generative Diversity by Variational Mutual Information Maximizing Autoencoder</title>
      <author><first>Yueen</first><last>Ma</last></author>
      <author><first>DaFeng</first><last>Chi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jingjing</first><last>Li</last></author>
      <author><first>Kai</first><last>Song</last></author>
      <author><first>Yuzheng</first><last>Zhuang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Irwin</first><last>King</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>364-378</pages>
      <abstract>The natural language generation domain has witnessed great success thanks to Transformer models. Although they have achieved state-of-the-art generative quality, they often neglect generative diversity. Prior attempts to tackle this issue suffer from either low model capacity or over-complicated architectures. Some recent methods employ the VAE framework to enhance diversity, but their latent variables fully depend on the input context, restricting exploration of the latent space. In this paper, we introduce VOLTA, a framework that elevates generative diversity by bridging Transformer with VAE via a more effective cross-attention-based connection, departing from conventional embedding concatenation or summation. Additionally, we propose integrating InfoGAN-style latent codes to enable input-independent variability, further diversifying the generation. Moreover, our framework accommodates discrete inputs alongside its existing support for continuous inputs. We perform comprehensive experiments with two types of Transformers on six datasets from three different NLG tasks to show that our approach can significantly improve generative diversity while maintaining generative quality.</abstract>
      <url hash="209ed79c">2024.findings-naacl.26</url>
      <bibkey>ma-etal-2024-volta</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.26</doi>
    </paper>
    <paper id="27">
      <title><fixed-case>E</fixed-case>co<fixed-case>S</fixed-case>peak: Cost-Efficient Bias Mitigation for Partially Cross-Lingual Speaker Verification</title>
      <author><first>Divya</first><last>Sharma</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>379-394</pages>
      <abstract>Linguistic bias is a critical problem concerning the diversity, equity, and inclusiveness of Natural Language Processing tools. The severity of this problem intensifies in security systems, such as speaker verification, where fairness is paramount. Speaker verification systems are biometric systems that determine whether two speech recordings are of the same speaker. Such user-centric systems should be inclusive to bilingual speakers. However, Deep neural network models are linguistically biased. Linguistic bias can be full or partial. Partially cross-lingual bias occurs when one test trial pair recording is in the training set’s language, and the other is in an unseen target language. Such linguistic mismatch influences the speaker verification model’s decision, dissuading bilingual speakers from using the system. Domain adaptation can mitigate this problem. However, adapting to each existing language is expensive. This paper explores cost-efficient bias mitigation techniques for partially cross-lingual speaker verification. We study the behavior of five baselines in five partially cross-lingual scenarios. Using our baseline behavioral insights, we propose EcoSpeak, a low-cost solution to partially cross-lingual speaker verification. EcoSpeak incorporates contrastive linguistic (CL) attention. CL attention utilizes linguistic differences in trial pairs to emphasize relevant speaker verification embedding parts. Experimental results demonstrate EcoSpeak’s robustness to partially cross-lingual testing.</abstract>
      <url hash="06f8c08f">2024.findings-naacl.27</url>
      <bibkey>sharma-2024-ecospeak</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.27</doi>
    </paper>
    <paper id="28">
      <title>Leveraging Contextual Information for Effective Entity Salience Detection</title>
      <author><first>Rajarshi</first><last>Bhowmik</last><affiliation>Bloomberg L.P.</affiliation></author>
      <author><first>Marco</first><last>Ponza</last></author>
      <author><first>Atharva</first><last>Tendle</last></author>
      <author><first>Anant</first><last>Gupta</last></author>
      <author><first>Rebecca</first><last>Jiang</last></author>
      <author><first>Xingyu</first><last>Lu</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Qian</first><last>Zhao</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Daniel</first><last>Preotiuc-Pietro</last><affiliation>Bloomberg</affiliation></author>
      <pages>395-408</pages>
      <abstract>In text documents such as news articles, the content and key events usually revolve around a subset of all the entities mentioned in a document. These entities, often deemed as salient entities, provide useful cues of the aboutness of a document to a reader. Identifying the salience of entities was found helpful in several downstream applications such as search, ranking, and entity-centric summarization, among others. Prior work on salient entity detection mainly focused on machine learning models that require heavy feature engineering. We show that fine-tuning medium-sized language models with a cross-encoder style architecture yields substantial performance gains over feature engineering approaches. To this end, we conduct a comprehensive benchmarking of four publicly available datasets using models representative of the medium-sized pre-trained language model family. Additionally, we show that zero-shot prompting of instruction-tuned language models yields inferior results, indicating the task’s uniqueness and complexity.</abstract>
      <url hash="cc38b18c">2024.findings-naacl.28</url>
      <bibkey>bhowmik-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.28</doi>
    </paper>
    <paper id="29">
      <title><fixed-case>LLM</fixed-case>-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be Detected?</title>
      <author><first>Qihui</first><last>Zhang</last></author>
      <author><first>Chujie</first><last>Gao</last></author>
      <author><first>Dongping</first><last>Chen</last></author>
      <author><first>Yue</first><last>Huang</last></author>
      <author><first>Yixin</first><last>Huang</last></author>
      <author><first>Zhenyang</first><last>Sun</last></author>
      <author><first>Shilin</first><last>Zhang</last></author>
      <author><first>Weiye</first><last>Li</last></author>
      <author><first>Zhengyan</first><last>Fu</last></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Lichao</first><last>Sun</last><affiliation>Lehigh University</affiliation></author>
      <pages>409-436</pages>
      <abstract>With the rapid development and widespread application of Large Language Models (LLMs), the use of Machine-Generated Text (MGT) has become increasingly common, bringing with it potential risks, especially in terms of quality and integrity in fields like news, education, and science. Current research mainly focuses on purely MGT detection, without adequately addressing mixed scenarios including AI-revised Human-Written Text (HWT) or human-revised MGT. To tackle this challenge, we define mixtext, a form of mixed text involving both AI and human-generated content. Then we introduce MixSet, the first dataset dedicated to studying these mixtext scenarios. Leveraging MixSet, we executed comprehensive experiments to assess the efficacy of prevalent MGT detectors in handling mixtext situations, evaluating their performance in terms of effectiveness, robustness, and generalization. Our findings reveal that existing detectors struggle to identify mixtext, particularly in dealing with subtle modifications and style adaptability. This research underscores the urgent need for more fine-grain detectors tailored for mixtext, offering valuable insights for future research. Code and Models are available at https://github.com/Dongping-Chen/MixSet.</abstract>
      <url hash="a248e99b">2024.findings-naacl.29</url>
      <bibkey>zhang-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.29</doi>
    </paper>
    <paper id="30">
      <title>A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection</title>
      <author><first>Ivo</first><last>Verhoeven</last></author>
      <author><first>Pushkar</first><last>Mishra</last><affiliation>Meta AI</affiliation></author>
      <author><first>Rahel</first><last>Beloch</last></author>
      <author><first>Helen</first><last>Yannakoudakis</last><affiliation>Computer Laboratory, University of Cambridge and King’s College London</affiliation></author>
      <author><first>Ekaterina</first><last>Shutova</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>437-463</pages>
      <abstract>Community models for malicious content detection, which take into account the context from a social graph alongside the content itself, have shown remarkable performance on benchmark datasets. Yet, misinformation and hate speech continue to propagate on social media networks. This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online content and the underlying social graph. In this paper, we propose a novel evaluation setup for model generalisation based on our few-shot subgraph sampling approach. This setup tests for generalisation through few labelled examples in local explorations of a larger graph, emulating more realistic application settings. We show this to be a challenging inductive setup, wherein strong performance on the training graph is not indicative of performance on unseen tasks, domains, or graph structures. Lastly, we show that graph meta-learners trained with our proposed few-shot subgraph sampling outperform standard community models in the inductive setup.</abstract>
      <url hash="4bec243e">2024.findings-naacl.30</url>
      <bibkey>verhoeven-etal-2024-realistic</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.30</doi>
    </paper>
    <paper id="31">
      <title>Citation: A Key to Building Responsible and Accountable Large Language Models</title>
      <author><first>Jie</first><last>Huang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Kevin</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>464-473</pages>
      <abstract>Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify “citation”—the acknowledgement or reference to a source or evidence—as a crucial yet missing component in LLMs. Incorporating citation could enhance content transparency and verifiability, thereby confronting the IP and ethical issues in the deployment of LLMs. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable LLMs.</abstract>
      <url hash="f2d4307f">2024.findings-naacl.31</url>
      <bibkey>huang-chang-2024-citation</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.31</doi>
    </paper>
    <paper id="32">
      <title>Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational <fixed-case>A</fixed-case>uto<fixed-case>E</fixed-case>ncoders</title>
      <author><first>Yingji</first><last>Zhang</last></author>
      <author><first>Marco</first><last>Valentino</last></author>
      <author><first>Danilo</first><last>Carvalho</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Ian</first><last>Pratt-Hartmann</last><affiliation>University of Opole and University of Manchester</affiliation></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>474-489</pages>
      <abstract>The injection of syntactic information in Variational AutoEncoders (VAEs) can result in an overall improvement of performances and generalisation. An effective strategy to achieve such a goal is to separate the encoding of distributional semantic features and syntactic structures into heterogeneous latent spaces via multi-task learning or dual encoder architectures. However, existing works employing such techniques are limited to LSTM-based VAEs. This work investigates latent space separation methods for structural syntactic injection in Transformer-based VAE architectures (i.e., Optimus) through the integration of graph-based models. Our empirical evaluation reveals that the proposed end-to-end VAE architecture can improve theoverall organisation of the latent space, alleviating the information loss occurring in standard VAE setups, and resulting in enhanced performances on language modelling and downstream generation tasks.</abstract>
      <url hash="4193dedb">2024.findings-naacl.32</url>
      <bibkey>zhang-etal-2024-graph</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.32</doi>
    </paper>
    <paper id="33">
      <title>Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles</title>
      <author><first>Weiting</first><last>Tan</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Haoran</first><last>Xu</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Lingfeng</first><last>Shen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Shuyue Stella</first><last>Li</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Kenton</first><last>Murray</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Philipp</first><last>Koehn</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Yunmo</first><last>Chen</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>490-502</pages>
      <abstract>Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting. In this paper, we investigate the factors contributing to this gap and find that this gap can largely be closed (for about 70%) by matching the writing styles of the target corpus. Additionally, we explore potential approaches to enhance zero-shot baselines without the need for parallel demonstration examples, providing valuable insights into how these methods contribute to improving translation metrics.</abstract>
      <url hash="555e7ab6">2024.findings-naacl.33</url>
      <bibkey>tan-etal-2024-narrowing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.33</doi>
    </paper>
    <paper id="34">
      <title>Which Modality should <fixed-case>I</fixed-case> use - Text, Motif, or Image? : Understanding Graphs with Large Language Models</title>
      <author><first>Debarati</first><last>Das</last></author>
      <author><first>Ishaan</first><last>Gupta</last></author>
      <author><first>Jaideep</first><last>Srivastava</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Dongyeop</first><last>Kang</last><affiliation>University of Minnesota</affiliation></author>
      <pages>503-519</pages>
      <abstract>Our research integrates graph data with Large Language Models (LLMs), which, despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints. This paper introduces a new approach to encoding a graph with diverse modalities, such as text, image, and motif, coupled with prompts to approximate a graph’s global connectivity, thereby enhancing LLMs’ efficiency in processing complex graph structures. The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty. Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and comes close to prior graph neural net (GNN) encoders. Furthermore, the research assesses how various factors affect the performance of each encoding modality and outlines the existing challenges and potential future developments for LLMs in graph understanding and reasoning tasks. Our code and data are publicly available on our project page - https://minnesotanlp.github.io/GraphLLM/</abstract>
      <url hash="68657766">2024.findings-naacl.34</url>
      <bibkey>das-etal-2024-modality</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.34</doi>
    </paper>
    <paper id="35">
      <title>On-the-Fly Fusion of Large Language Models and Machine Translation</title>
      <author><first>Hieu</first><last>Hoang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Huda</first><last>Khayrallah</last><affiliation>Microsoft</affiliation></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last><affiliation>Microsoft</affiliation></author>
      <pages>520-532</pages>
      <abstract>We propose on-the-fly ensembling of a neural machine translation (NMT) model with a large language model (LLM), prompted on the same task and input. Through experiments on 4 language directions with varying data amounts, we find that a slightly weaker-at-translation LLM can improve translations of a NMT model, and such an ensemble can produce better translations than ensembling two stronger NMT models.We demonstrate that our ensemble method can be combined with various techniques from LLM prompting, such as in context learning and translation context.</abstract>
      <url hash="19fe9a14">2024.findings-naacl.35</url>
      <bibkey>hoang-etal-2024-fly</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>READ</fixed-case>: Improving Relation Extraction from an <fixed-case>AD</fixed-case>versarial Perspective</title>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>William</first><last>Hogan</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>533-548</pages>
      <abstract>Recent works in relation extraction (RE) have achieved promising benchmark accuracy; however, our adversarial attack experiments show that these works excessively rely on entities, making their generalization capability questionable. To address this issue, we propose an adversarial training method specifically designed for RE. Our approach introduces both sequence- and token-level perturbations to the sample and uses a separate perturbation vocabulary to improve the search for entity and context perturbations.Furthermore, we introduce a probabilistic strategy for leaving clean tokens in the context during adversarial training. This strategy enables a larger attack budget for entities and coaxes the model to leverage relational patterns embedded in the context. Extensive experiments show that compared to various adversarial training methods, our method significantly improves both the accuracy and robustness of the model. Additionally, experiments on different data availability settings highlight the effectiveness of our method in low-resource scenarios.We also perform in-depth analyses of our proposed method and provide further hints.We will release our code at https://github.com/David-Li0406/READ.</abstract>
      <url hash="ca0eed0f">2024.findings-naacl.36</url>
      <bibkey>li-etal-2024-read</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.36</doi>
    </paper>
    <paper id="37">
      <title><fixed-case>REQUAL</fixed-case>-<fixed-case>LM</fixed-case>: Reliability and Equity through Aggregation in Large Language Models</title>
      <author><first>Sana</first><last>Ebrahimi</last></author>
      <author><first>Nima</first><last>Shahbazi</last></author>
      <author><first>Abolfazl</first><last>Asudeh</last><affiliation>University of Illinois Chicago</affiliation></author>
      <pages>549-560</pages>
      <abstract>The extensive scope of large language models (LLMs) across various domains underscores the critical importance of responsibility in their application, beyond natural language processing. In particular, the randomized nature of LLMs, coupled with inherent biases and historical stereotypes in data, raises critical concerns regarding reliability and equity. Addressing these challenges are necessary before using LLMs for applications with societal impact. Towards addressing this gap, we introduce REQUAL-LM, a novel method for finding reliable and equitable LLM outputs through aggregation. Specifically, we develop a Montecarlo method based on repeated sampling to find a reliable output close to the mean of the underlying distribution of possible outputs. We formally define the terms such as reliability and bias, and design an equity-aware aggregation to minimize harmful bias while finding a highly reliable output. REQUAL-LM does not require specialized hardware, does not impose a significant computing load, and uses LLMs as a blackbox. This design choice enables seamless scalability alongside the rapid advancement of LLM technologies. Our system does not require retraining the LLMs, which makes it deployment ready and easy to adapt. Our comprehensive experiments using various tasks and datasets demonstrate that REQUAL-LM effectively mitigates bias and selects a more equitable response, specifically the outputs that properly represents minority groups.</abstract>
      <url hash="38391e43">2024.findings-naacl.37</url>
      <bibkey>ebrahimi-etal-2024-requal</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.37</doi>
    </paper>
    <paper id="38">
      <title>Addressing Both Statistical and Causal Gender Fairness in <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Hannah</first><last>Chen</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Yangfeng</first><last>Ji</last><affiliation>University of Virginia</affiliation></author>
      <author><first>David</first><last>Evans</last><affiliation>University of Virginia</affiliation></author>
      <pages>561-582</pages>
      <abstract>Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.</abstract>
      <url hash="fd6fcc8a">2024.findings-naacl.38</url>
      <bibkey>chen-etal-2024-addressing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.38</doi>
    </paper>
    <paper id="39">
      <title><fixed-case>LLM</fixed-case>-Rec: Personalized Recommendation via Prompting Large Language Models</title>
      <author><first>Hanjia</first><last>Lyu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Song</first><last>Jiang</last></author>
      <author><first>Hanqing</first><last>Zeng</last><affiliation>Meta AI</affiliation></author>
      <author><first>Yinglong</first><last>Xia</last><affiliation>Meta</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Si</first><last>Zhang</last><affiliation>Meta</affiliation></author>
      <author><first>Ren</first><last>Chen</last></author>
      <author><first>Chris</first><last>Leung</last><affiliation>Meta AI and College of Computing, Georgia Institute of Technology</affiliation></author>
      <author><first>Jiajie</first><last>Tang</last></author>
      <author><first>Jiebo</first><last>Luo</last><affiliation>University of Rochester, University of Rochester, University of Rochester and University of Rochester</affiliation></author>
      <pages>583-612</pages>
      <abstract>Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-Rec, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model’s comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.</abstract>
      <url hash="21050f40">2024.findings-naacl.39</url>
      <bibkey>lyu-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.39</doi>
    </paper>
    <paper id="40">
      <title>A Robust Semantics-based Watermark for Large Language Model against Paraphrasing</title>
      <author><first>Jie</first><last>Ren</last><affiliation>Baidu and Michigan State University</affiliation></author>
      <author><first>Han</first><last>Xu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yiding</first><last>Liu</last><affiliation>Baidu</affiliation></author>
      <author><first>Yingqian</first><last>Cui</last></author>
      <author><first>Shuaiqiang</first><last>Wang</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>613-625</pages>
      <abstract>Large language models (LLMs) have show their remarkable ability in various natural language tasks. However, there are concerns that LLMs are possible to be used improperly or even illegally. To prevent the malicious usage of LLMs, detecting LLM-generated text becomes crucial in the deployment of LLM applications. Watermarking is an effective strategy to detect the LLM-generated content by encoding a pre-defined secret watermark to facilitate the detection process. However, the majority of existing watermark methods leverage the simple hashes of precedent tokens to partition vocabulary. Such watermarks can be easily eliminated by paraphrase and, correspondingly, the detection effectiveness will be greatly compromised. Thus, to enhance the robustness against paraphrase, we propose a semantics-based watermark framework, SemaMark. It leverages the semantics as an alternative to simple hashes of tokens since the semantic meaning of the sentences will be likely preserved under paraphrase and the watermark can remain robust. Comprehensive experiments are conducted to demonstrate the effectiveness and robustness of SemaMark under different paraphrases.</abstract>
      <url hash="38bb39d8">2024.findings-naacl.40</url>
      <bibkey>ren-etal-2024-robust</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.40</doi>
    </paper>
    <paper id="41">
      <title>Solving Data-centric Tasks using Large Language Models</title>
      <author><first>Shraddha</first><last>Barke</last></author>
      <author><first>Christian</first><last>Poelitz</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Carina</first><last>Negreanu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Benjamin</first><last>Zorn</last></author>
      <author><first>José</first><last>Cambronero</last><affiliation>Microsoft</affiliation></author>
      <author><first>Andrew</first><last>Gordon</last><affiliation>University of Edinburgh and Microsoft Research</affiliation></author>
      <author><first>Vu</first><last>Le</last><affiliation>Microsoft</affiliation></author>
      <author><first>Elnaz</first><last>Nouri</last></author>
      <author><first>Nadia</first><last>Polikarpova</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Advait</first><last>Sarkar</last><affiliation>Microsoft</affiliation></author>
      <author><first>Brian</first><last>Slininger</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Neil</first><last>Toronto</last></author>
      <author><first>Jack</first><last>Williams</last><affiliation>Microsoft</affiliation></author>
      <pages>626-638</pages>
      <abstract>Large language models are rapidly replacing help forums like StackOverflow, and are especially helpful to non-professional programmers and end users. These users are often interested in <i>data-centric tasks</i>, like spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including data. But how do we decide how much data and which data to include in the prompt?This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a novel <i>cluster-then-select</i> prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table,our cluster-then-select technique outperforms a random selection baseline.</abstract>
      <url hash="6a239c21">2024.findings-naacl.41</url>
      <bibkey>barke-etal-2024-solving</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.41</doi>
    </paper>
    <paper id="42">
      <title>A Novel Paradigm Boosting Translation Capabilities of Large Language Models</title>
      <author><first>Jiaxin</first><last>Guo</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Zongyao</first><last>Li</last></author>
      <author><first>Daimeng</first><last>Wei</last></author>
      <author><first>Hengchao</first><last>Shang</last></author>
      <author><first>Xiaoyu</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>639-649</pages>
      <abstract>This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs’ cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results conducted using the Llama2(CITATION)model, particularly on Chinese-Llama2(CITATION) after monolingual augmentation, demonstrate the improved translation capabilities of LLMs. A significant contribution of our approach lies in Stage2: Continual Pre-training with Interlinear Text Format Documents, which requires less than 1B training data, making our method highly efficient. Additionally, in Stage3, we observed that setting instructions consistent with the source language benefits the supervised fine-tuning process. Experimental results demonstrate that our approach surpasses previous work and achieves superior performance compared to models such as NLLB-54B(CITATION) and GPT3.5-text-davinci-003, despite having a significantly smaller parameter count of only 7B or 13B. This achievement establishes our method as a pioneering strategy in the field of machine translation.</abstract>
      <url hash="2abc6eb2">2024.findings-naacl.42</url>
      <bibkey>guo-etal-2024-novel</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.42</doi>
    </paper>
    <paper id="43">
      <title>Measuring Social Norms of Large Language Models</title>
      <author><first>Ye</first><last>Yuan</last></author>
      <author><first>Kexin</first><last>Tang</last><affiliation>Peking University</affiliation></author>
      <author><first>Jianhao</first><last>Shen</last></author>
      <author><first>Ming</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Chenguang</first><last>Wang</last><affiliation>Washington University, Saint Louis</affiliation></author>
      <pages>650-699</pages>
      <abstract>We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models’ ability to understand social norms. This method further improves large language models to be on par with humans. Given the increasing adoption of large language models in real-world applications, our finding is particularly important and presents a unique direction for future improvements.</abstract>
      <url hash="a43f9da7">2024.findings-naacl.43</url>
      <bibkey>yuan-etal-2024-measuring</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.43</doi>
    </paper>
    <paper id="44">
      <title>Source-Free Unsupervised Domain Adaptation for Question Answering via Prompt-Assisted Self-learning</title>
      <author><first>Maxwell</first><last>Yin</last></author>
      <author><first>Boyu</first><last>Wang</last><affiliation>University of Western Ontario</affiliation></author>
      <author><first>Charles</first><last>Ling</last><affiliation>Western University</affiliation></author>
      <pages>700-713</pages>
      <abstract>This work addresses source-free domain adaptation (SFDA) for Question Answering (QA), wherein a model trained on a source domain is adapted to unlabeled target domains without additional source data. Existing SFDA methods only focus on the adaptation phase, overlooking the impact of source domain training on model generalizability. In this paper, we argue that source model training itself is also critical for improving the adaptation performance and stability. To this end, we investigate the role of prompt learning as an effective method to internalize domain-agnostic QA knowledge, which can be integrated into source training. After source training, an interactive self-learning strategy is proposed to further fine tune both model and prompt in the model adaptation phase. This leads to the Prompt-Assisted Self-Adaptive Learning (PASAL), an innovative SFDA approach for QA. Empirical evaluation on four benchmark datasets shows that PASAL surpasses existing methods in managing domain gaps and demonstrates greater stability across various target domains, validating the significance of source domain training for effective domain adaptation.</abstract>
      <url hash="06007d60">2024.findings-naacl.44</url>
      <bibkey>yin-etal-2024-source</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.44</doi>
    </paper>
    <paper id="45">
      <title>Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level</title>
      <author><first>Chenlong</first><last>Zhao</last></author>
      <author><first>Xiwen</first><last>Zhou</last></author>
      <author><first>Xiaopeng</first><last>Xie</last></author>
      <author><first>Yong</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>714-726</pages>
      <abstract>Scientific document summarization has been a challenging task due to the long structure of the input text. The long input hinders the simultaneous effective modeling of both global high-order relations between sentences and local intra-sentence relations which is the most critical step in extractive summarization. However, existing methods mostly focus on one type of relation, neglecting the simultaneous effective modeling of both relations, which can lead to insufficient learning of semantic representations. In this paper, we propose HAESum, a novel approach utilizing graph neural networks to locally and globally model documents based on their hierarchical discourse structure. First, intra-sentence relations are learned using a local heterogeneous graph. Subsequently, a novel hypergraph self-attention layer is introduced to further enhance the characterization of high-order inter-sentence relations. We validate our approach on two benchmark datasets, and the experimental results demonstrate the effectiveness of HAESum and the importance of considering hierarchical structures in modeling long scientific documents.</abstract>
      <url hash="7899932a">2024.findings-naacl.45</url>
      <bibkey>zhao-etal-2024-hierarchical</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.45</doi>
    </paper>
    <paper id="46">
      <title><fixed-case>LEEET</fixed-case>s-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems</title>
      <author><first>Nalin</first><last>Kumar</last></author>
      <author><first>Ondrej</first><last>Dusek</last><affiliation>Charles University, Prague</affiliation></author>
      <pages>727-735</pages>
      <abstract>Linguistic entrainment, or alignment, represents a phenomenon where linguistic patterns employed by conversational participants converge to one another. While entrainment has been shown to produce a more natural user experience, most dialogue systems do not have any provisions for it. In this work, we introduce methods for achieving dialogue entrainment in a GPT-2-based end-to-end task-oriented dialogue system through the utilization of shared vocabulary. We experiment with training instance weighting, entrainment-specific loss, and additional conditioning to generate responses that align with the user. We demonstrate that all three approaches produce significantly better entrainment than the base, non-entrainment-optimized model, as confirmed by both automated and manual evaluation metrics.</abstract>
      <url hash="2b129e4d">2024.findings-naacl.46</url>
      <bibkey>kumar-dusek-2024-leeets</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.46</doi>
    </paper>
    <paper id="47">
      <title>Efficient Dependency Tree Sampling Without Replacement</title>
      <author><first>Bogdan</first><last>Dobre</last><affiliation>University of Bucharest</affiliation></author>
      <pages>736-741</pages>
      <abstract>In the context of computational models of dependency syntax, most dependency treebanks have the restriction that any valid dependency tree must have exactly one edge coming out of the root node in addition to respecting the spanning tree constraints. Many algorithms for dependency tree sampling were recently proposed, both for sampling with and without replacement.In this paper we propose a new algorithm called Wilson Reject SWOR for the case of sampling without replacement by adapting the Wilson Reject algorithm originally created for sampling with replacement and combining it with a Trie data structure. Experimental results indicate the efficiency of our approach in the scenario of sampling without replacement from dependency graphs with random weights.</abstract>
      <url hash="a13a6100">2024.findings-naacl.47</url>
      <bibkey>dobre-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.47</doi>
    </paper>
    <paper id="48">
      <title>Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization</title>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Revanth</first><last>Gangi Reddy</last></author>
      <author><first>Kevin</first><last>Small</last><affiliation>Amazon</affiliation></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>742-753</pages>
      <abstract>Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader’s over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generalizing to a new knowledge corpus. We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training. Extensive experimental results on multiple OpenQA benchmarks show that CIT achieves significantly better generalizability without compromising the model’s performance in its original corpus and domain.</abstract>
      <url hash="00ae9525">2024.findings-naacl.48</url>
      <bibkey>zhang-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.48</doi>
    </paper>
    <paper id="49">
      <title><fixed-case>GEE</fixed-case>! Grammar Error Explanation with Large Language Models</title>
      <author><first>Yixiao</first><last>Song</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Kalpesh</first><last>Krishna</last><affiliation>Google</affiliation></author>
      <author><first>Rajesh</first><last>Bhatt</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Kevin</first><last>Gimpel</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <pages>754-781</pages>
      <abstract>Existing grammatical error correction tools do not provide natural language explanations of the errors that they correct in user-written text. However, such explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006).To address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences. The task is not easily solved by prompting LLMs: we find that, using one-shot prompting, GPT-4 only explains 40.6% of the errors and does not even attempt to explain 39.8% of the errors.Since LLMs struggle to identify grammar errors, we develop a two-step pipeline that leverages fine-tuned and prompted large language models to perform structured atomic token edit extraction, followed by prompting GPT-4 to explain each edit. We evaluate our pipeline on German, Chinese, and English grammar error correction data. Our atomic edit extraction achieves an F1 of 0.93 on German, 0.91 on Chinese, and 0.891 on English. Human evaluation of generated explanations reveals that 93.9% of German errors, 96.4% of Chinese errors, and 92.20% of English errors are correctly detected and explained. To encourage further research, we open-source our data and code.</abstract>
      <url hash="bba32b65">2024.findings-naacl.49</url>
      <bibkey>song-etal-2024-gee</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.49</doi>
    </paper>
    <paper id="50">
      <title><fixed-case>A</fixed-case>da<fixed-case>R</fixed-case>efiner: Refining Decisions of Language Models with Adaptive Feedback</title>
      <author><first>Wanpeng</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zongqing</first><last>Lu</last><affiliation>Peking University</affiliation></author>
      <pages>782-799</pages>
      <abstract>Large Language Models (LLMs) have demonstrated significant success across various domains. However, their application in complex decision-making tasks frequently necessitates intricate prompt engineering or fine-tuning, leading to challenges in unseen downstream tasks and heavy demands on computational resources. Meanwhile, Reinforcement Learning (RL) has been recognized as effective in decision-making problems but struggles in environments with sparse rewards, such as open-world games. To overcome these challenges, we introduce AdaRefiner, a novel framework designed to enhance the synergy between LLMs and RL feedback. The key component of AdaRefiner is a lightweight Adapter Language Model (LM), which automatically refines task comprehension based on feedback from RL agents. This method mitigates the need for intricate prompt engineering and intensive LLM fine-tuning while maintaining the LLMs’ generalization abilities and enhancing their decision-making capabilities in downstream tasks. Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world game <i>Crafter</i> have demonstrated its superior effectiveness, especially in guiding agents towards higher-level and common-sense skills. Our work makes contributions to the automatic self-refinement of LLMs with RL feedback, offering a more adaptable and efficient solution for complex decision-making problems. The code is available at https://github.com/PKU-RL/AdaRefiner.</abstract>
      <url hash="bef6a625">2024.findings-naacl.50</url>
      <bibkey>zhang-lu-2024-adarefiner</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.50</doi>
    </paper>
    <paper id="51">
      <title><fixed-case>D</fixed-case>iv<fixed-case>TOD</fixed-case>: Unleashing the Power of <fixed-case>LLM</fixed-case>s for Diversifying Task-Oriented Dialogue Representations</title>
      <author><first>Weihao</first><last>Zeng</last></author>
      <author><first>Dayuan</first><last>Fu</last></author>
      <author><first>Keqing</first><last>He</last><affiliation>Meituan Group</affiliation></author>
      <author><first>Yejie</first><last>Wang</last></author>
      <author><first>Yukai</first><last>Xu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>800-813</pages>
      <abstract>Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context.In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.</abstract>
      <url hash="bda85c2f">2024.findings-naacl.51</url>
      <bibkey>zeng-etal-2024-divtod</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.51</doi>
    </paper>
    <paper id="52">
      <title>Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training</title>
      <author><first>Pavel</first><last>Denisov</last><affiliation>University of Stuttgart, University of Stuttgart</affiliation></author>
      <author><first>Thang</first><last>Vu</last><affiliation>University of Stuttgart, University of Stuttgart</affiliation></author>
      <pages>814-834</pages>
      <abstract>Recent advancements in language modeling have led to the emergenceof Large Language Models (LLMs) capable ofvarious natural language processing tasks.Despite their success in text-based tasks, applying LLMs to the speech domainremains limited and challenging. This paper presents BLOOMZMMS, a novel modelthat integrates a multilingual LLM with a multilingual speech encoder,aiming to harness the capabilities of LLMs for speech recognition and beyond.Utilizing a multi-instructional training approach, we demonstrate the transferabilityof linguistic knowledge from the text to the speech modality.Our experiments, conducted on 1900 hours of transcribed data from 139 languages,establish that a multilingual speech representation can be effectivelylearned and aligned with a multilingual LLM. While this learned representationinitially shows limitations in task generalization, we address this issue bygenerating synthetic targets in a multi-instructional style.Our zero-shot evaluation results confirm the robustness of our approach acrossmultiple tasks, including speech translation and multilingual spoken languageunderstanding, thereby opening new avenues for applying LLMs in the speech domain.</abstract>
      <url hash="aa88ddc2">2024.findings-naacl.52</url>
      <bibkey>denisov-vu-2024-teaching</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.52</doi>
    </paper>
    <paper id="53">
      <title><fixed-case>CLEAN</fixed-case>–<fixed-case>EVAL</fixed-case>: Clean Evaluation on Contaminated Large Language Models</title>
      <author><first>Wenhong</first><last>Zhu</last></author>
      <author><first>Hongkun</first><last>Hao</last></author>
      <author><first>Zhiwei</first><last>He</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yun-Ze</first><last>Song</last></author>
      <author><first>Jiao</first><last>Yueyang</last></author>
      <author><first>Yumeng</first><last>Zhang</last></author>
      <author><first>Hanxu</first><last>Hu</last></author>
      <author><first>Yiran</first><last>Wei</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hongyuan</first><last>Lu</last></author>
      <pages>835-847</pages>
      <abstract>We are currently in an era of fierce competition among various large language models (LLMs), continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination. In this paper, we propose a novel and valuable method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs more cleanly. Clean-Eval employs a neural-based model to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter those generated low-quality samples to narrow down this candidate set. Candidates with moderate BLEURT scores against the original samples are selected as the final evaluation set. According to human assessment, this set is almost semantically equivalent to the original contamination set but expressed differently. We conduct experiments on 20 existing benchmarks across diverse tasks, and results demonstrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.</abstract>
      <url hash="bf820348">2024.findings-naacl.53</url>
      <bibkey>zhu-etal-2024-clean</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.53</doi>
    </paper>
    <paper id="54">
      <title><fixed-case>R</fixed-case>-<fixed-case>BASS</fixed-case> : Relevance-aided Block-wise Adaptation for Speech Summarization</title>
      <author><first>Roshan</first><last>Sharma</last><affiliation>Google</affiliation></author>
      <author><first>Ruchira</first><last>Sharma</last></author>
      <author><first>Hira</first><last>Dhamyal</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Rita</first><last>Singh</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Bhiksha</first><last>Raj</last><affiliation>Carnegie Mellon University, Carnegie Mellon University and Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>848-857</pages>
      <abstract>End-to-end speech summarization on long recordings is challenging because of the high computational cost. Block-wise Adaptation for Speech Summarization (BASS) summarizes arbitrarily long sequences by sequentially processing abutting chunks of audio. Despite the benefits of BASS, it has higher compute time due to sequential processing of all blocks, regardless of whether they are relevant to the final summary. In this paper, we propose R-BASS, a new relevance-aware block-wise adaptation method. First, we introduce two approaches to automatically estimate block relevance based on lexical and semantic similarity between the block-level transcript and the summary. Experiments on the How2 dataset show that using ground truth relevance during inference improves efficiency by 63.9 % by dropping irrelevant blocks. Finally, we incorporate relevance scores into training using a novel relevance loss and relevance predictor, and the proposed R-BASS model makes it possible to drop 86.3 % of the blocks while retaining comparable performance, resulting in a 2.2x speedup over BASS.</abstract>
      <url hash="0f4fc0f3">2024.findings-naacl.54</url>
      <bibkey>sharma-etal-2024-r</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.54</doi>
    </paper>
    <paper id="55">
      <title><fixed-case>OVM</fixed-case>, Outcome-supervised Value Models for Planning in Mathematical Reasoning</title>
      <author><first>Fei</first><last>Yu</last></author>
      <author><first>Anningzhe</first><last>Gao</last><affiliation>ShenZhen research institute of big data</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>858-875</pages>
      <abstract>Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer.To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a <tex-math>\textit{value estimation}</tex-math> problem in planning.Inspired by the findings that <tex-math>\textit{outcome supervision for guided decoding essentially acts as a value model}</tex-math>, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions. Furthermore, the OVM eliminates the need for labor-intensive annotations of step-level correctness, thereby significantly enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our <tex-math>\textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters}</tex-math>; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training value models for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for guided decoding.</abstract>
      <url hash="fed336c7">2024.findings-naacl.55</url>
      <bibkey>yu-etal-2024-ovm</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.55</doi>
    </paper>
    <paper id="56">
      <title>The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation</title>
      <author><first>Lei</first><last>Wang</last><affiliation>SalesForce</affiliation></author>
      <author><first>Ee-Peng</first><last>Lim</last><affiliation>Singapore Management University</affiliation></author>
      <pages>876-895</pages>
      <abstract>Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.</abstract>
      <url hash="74c6cd93">2024.findings-naacl.56</url>
      <bibkey>wang-lim-2024-whole</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.56</doi>
    </paper>
    <paper id="57">
      <title>Bring Your Own <fixed-case>KG</fixed-case>: Self-Supervised Program Synthesis for Zero-Shot <fixed-case>KGQA</fixed-case></title>
      <author><first>Dhruv</first><last>Agarwal</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Rajarshi</first><last>Das</last><affiliation>AWS AI Labs</affiliation></author>
      <author><first>Sopan</first><last>Khosla</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Rashmi</first><last>Gangadharaiah</last><affiliation>Amazon</affiliation></author>
      <pages>896-919</pages>
      <abstract>We present BYOKG, a universal question-answering (QA) system that can operate on any knowledge graph (KG), requires no human-annotated training data, and can be ready to use within a day—attributes that are out-of-scope for current KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to comprehend information present in an unseen KG through exploration—starting at random nodes, inspecting the labels of adjacent nodes and edges, and combining them with their prior world knowledge. Exploration in BYOKG leverages an LLM-backed symbolic agent that generates a diverse set of query-program exemplars, which are then used to ground a retrieval-augmented reasoning procedure to synthesize programs for arbitrary questions. BYOKG is effective over both small- and large-scale graphs, showing dramatic gains in zero-shot QA accuracy of 27.89 and 59.88 F1 on GrailQA and MetaQA, respectively. We further find that performance of BYOKG reliably improves with continued exploration as well as improvements in the base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1 on a sub-sampled zero-shot split of GrailQA. Lastly, we verify our universality claim by evaluating BYOKG on a domain-specific materials science KG and show that it improves zero-shot performance by 46.33 F1.</abstract>
      <url hash="2745b84d">2024.findings-naacl.57</url>
      <bibkey>agarwal-etal-2024-bring</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.57</doi>
    </paper>
    <paper id="58">
      <title><fixed-case>G</fixed-case>ra<fixed-case>SAME</fixed-case>: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism</title>
      <author><first>Shuzhou</first><last>Yuan</last></author>
      <author><first>Michael</first><last>Färber</last><affiliation>Technische Universität Dresden</affiliation></author>
      <pages>920-933</pages>
      <abstract>Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks. However, bridging the modality gap between graph structures and text remains a significant challenge. Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes for integration into PLMs. In this work, we propose a novel graph-guided self-attention mechanism, GraSAME. GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts. As an end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs. Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million.</abstract>
      <url hash="5d77aa26">2024.findings-naacl.58</url>
      <bibkey>yuan-farber-2024-grasame</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.58</doi>
    </paper>
    <paper id="59">
      <title>Can Public Large Language Models Help Private Cross-device Federated Learning?</title>
      <author><first>Boxin</first><last>Wang</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Yibo</first><last>Zhang</last><affiliation>Stanford University and University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Yuan</first><last>Cao</last><affiliation>Google DeepMind</affiliation></author>
      <author id="bo-li"><first>Bo</first><last>Li</last><affiliation>University of Illinois, Urbana Champaign and University of California Berkeley</affiliation></author>
      <author><first>Hugh</first><last>McMahan</last><affiliation>Google</affiliation></author>
      <author><first>Sewoong</first><last>Oh</last><affiliation>University of Washington, University of Illinois at Urbana-Champaign and University of Washington, Seattle</affiliation></author>
      <author><first>Zheng</first><last>Xu</last><affiliation>Google</affiliation></author>
      <author><first>Manzil</first><last>Zaheer</last><affiliation>Zaheer and DeepMind</affiliation></author>
      <pages>934-949</pages>
      <abstract>We study (differentially) private federated learning (FL) of language models. The language models in cross-device FL are relatively small, which can be trained with meaningful formal user-level differential privacy (DP) guarantees when massive parallelism in training is enabled by the participation of a moderate size of users. Recently, public data has been used to improve privacy-utility trade-offs for both large and small language models. In this work, we provide a systematic study of using large-scale public data and LLMs to help differentially private training of on-device FL models, and further improve the privacy-utility tradeoff by techniques of distillation. Moreover, we propose a novel distribution matching algorithm with theoretical grounding to sample public data close to private data distribution, which significantly improves the sample efficiency of (pre-)training on public data. The proposed method is efficient and effective for training private models by taking advantage of public data, especially for customized on-device architectures that do not have ready-touse pre-trained models.</abstract>
      <url hash="277d72a6">2024.findings-naacl.59</url>
      <bibkey>wang-etal-2024-public</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.59</doi>
    </paper>
    <paper id="60">
      <title><fixed-case>L</fixed-case>ang<fixed-case>N</fixed-case>av: Language as a Perceptual Representation for Navigation</title>
      <author><first>Bowen</first><last>Pan</last></author>
      <author><first>Rameswar</first><last>Panda</last><affiliation>MIT-IBM Watson AI Lab</affiliation></author>
      <author><first>SouYoung</first><last>Jin</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Rogerio</first><last>Feris</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Aude</first><last>Oliva</last><affiliation>Massachusetts Institute of Technology, Massachusetts Institute of Technology and Massachusetts Institute of Technology</affiliation></author>
      <author><first>Phillip</first><last>Isola</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>950-974</pages>
      <abstract>We explore the use of language as a perceptual representation for vision-and-language navigation (VLN), with a focus on low-data settings. Our approach uses off-the-shelf vision systems for image captioning and object detection to convert an agent’s egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore several use cases of our language-based navigation (LangNav) approach on the R2R VLN benchmark: generating synthetic trajectories from a prompted language model (GPT-4) with which to finetune a smaller language model; domain transfer where we transfer a policy learned on one simulated environment (ALFRED) to another (more realistic) environment (R2R); and combining both vision- and language-based representations for VLN. Our approach is found to improve upon baselines that rely on visual features in settings where only a few expert trajectories (10-100) are available, demonstrating the potential of language as a perceptual representation for navigation.</abstract>
      <url hash="324764ee">2024.findings-naacl.60</url>
      <bibkey>pan-etal-2024-langnav</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.60</doi>
    </paper>
    <paper id="61">
      <title>Planning and Editing What You Retrieve for Enhanced Tool Learning</title>
      <author><first>Tenghao</first><last>Huang</last></author>
      <author><first>Dongwon</first><last>Jung</last></author>
      <author><first>Vaibhav</first><last>Kumar</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Mohammad</first><last>Kachuee</last><affiliation>Amazon</affiliation></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Puyang</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>975-988</pages>
      <abstract>Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel PLUTO (Planning, Learning, and Understanding for TOols) approach, encompassing “Plan-and-Retrieve (P&amp;R)” and “Edit-and-Ground (E&amp;G)” paradigms. The P&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E&amp;G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall and NDCG in tool retrieval tasks, significantly surpassing current state-of-the-art models.</abstract>
      <url hash="484ac7c2">2024.findings-naacl.61</url>
      <bibkey>huang-etal-2024-planning</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.61</doi>
    </paper>
    <paper id="62">
      <title>Chart-based Reasoning: Transferring Capabilities from <fixed-case>LLM</fixed-case>s to <fixed-case>VLM</fixed-case>s</title>
      <author><first>Victor</first><last>Carbune</last><affiliation>Google</affiliation></author>
      <author><first>Hassan</first><last>Mansoor</last><affiliation>Google</affiliation></author>
      <author><first>Fangyu</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Rahul</first><last>Aralikatte</last><affiliation>Mila, McGill University</affiliation></author>
      <author><first>Gilles</first><last>Baechler</last><affiliation>Research, Google</affiliation></author>
      <author><first>Jindong</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Abhanshu</first><last>Sharma</last><affiliation>Research, Google</affiliation></author>
      <pages>989-1004</pages>
      <abstract>Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We pro-pose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-artperformance when applied on the PaLI3-5B VLM by Chen et al. (2023c), while also enabling much better performance on PlotQA and FigureQA.We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by Liu et al. (2023a). We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model is fine-tuned using the multitask loss introduced by Hsieh et al. (2023).Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream OCR system, while keeping inference time constant compared to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought prompt (Chen et al., 2023a), our model outperforms the recently introduced Gemini Ultra and GPT-4V.</abstract>
      <url hash="ba101fab">2024.findings-naacl.62</url>
      <bibkey>carbune-etal-2024-chart</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.62</doi>
    </paper>
    <paper id="63">
      <title><fixed-case>SL</fixed-case>i<fixed-case>M</fixed-case>: Speculative Decoding with Hypothesis Reduction</title>
      <author><first>Chi-Heng</first><last>Lin</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Shikhar</first><last>Tuli</last></author>
      <author><first>James</first><last>Smith</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yen-Chang</first><last>Hsu</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Yilin</first><last>Shen</last><affiliation>Samsung Research America</affiliation></author>
      <author><first>Hongxia</first><last>Jin</last><affiliation>Samsung Research America AI center</affiliation></author>
      <pages>1005-1017</pages>
      <abstract>Speculative decoding has emerged as a prominent alternative to autoregressive decoding for expediting inference in large language models (LLMs). However, prevailing assumptions often focus solely on latency reduction, neglecting the computational expenses. In this paper, we present <b>S</b>peculate <b>L</b>ess, val<b>i</b>date <b>M</b>ore (SLiM), a speculative decoding enhancement to reduce the speculation set while validating more effective tokens. SLiM is designed to mitigate LLMs’ computation costs associated with the token verification by introducing hypothesis reduction based on a fast posterior estimation. It consistently surpasses counterparts lacking cost reduction across a spectrum from CPU to GPU. Our evaluation with diverse conversational datasets shows that SLiM can achieve a substantial 70% reduction in FLOPs while generating more effective predictions on top of prior arts.</abstract>
      <url hash="ca8029b7">2024.findings-naacl.63</url>
      <bibkey>lin-etal-2024-slim</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.63</doi>
    </paper>
    <paper id="64">
      <title><fixed-case>REMATCH</fixed-case>: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity</title>
      <author><first>Zoher</first><last>Kachwala</last><affiliation>Indiana University</affiliation></author>
      <author><first>Jisun</first><last>An</last><affiliation>Indiana University</affiliation></author>
      <author><first>Haewoon</first><last>Kwak</last><affiliation>Indiana University</affiliation></author>
      <author><first>Filippo</first><last>Menczer</last><affiliation>Indiana University</affiliation></author>
      <pages>1018-1028</pages>
      <abstract>Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1–5 percentage points on the STS-B and SICK-R benchmarks. Rematch is also five times faster than the next most efficient metric.</abstract>
      <url hash="d54da481">2024.findings-naacl.64</url>
      <bibkey>kachwala-etal-2024-rematch</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.64</doi>
    </paper>
    <paper id="65">
      <title>Modeling the Sacred: Considerations when Using Religious Texts in Natural Language Processing</title>
      <author><first>Ben</first><last>Hutchinson</last><affiliation>Google</affiliation></author>
      <pages>1029-1043</pages>
      <abstract>This position paper concerns the use of religious texts in Natural Language Processing (NLP), which is of special interest to the Ethics of NLP. Religious texts are expressions of culturally important values, and machine learned models have a propensity to reproduce cultural values encoded in their training data. Furthermore, translations of religious texts are frequently used by NLP researchers when language data is scarce. This repurposes the translations from their original uses and motivations, which often involve attracting new followers. This paper argues that NLP’s use of such texts raises considerations that go beyond model biases, including data provenance, cultural contexts, and their use in proselytism. We argue for more consideration of researcher positionality, and of the perspectives of marginalized linguistic and religious communities.</abstract>
      <url hash="f574efc3">2024.findings-naacl.65</url>
      <bibkey>hutchinson-2024-modeling</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.65</doi>
    </paper>
    <paper id="66">
      <title>Testing the Effect of Code Documentation on Large Language Model Code Understanding</title>
      <author><first>William</first><last>Macke</last><affiliation>MITRE</affiliation></author>
      <author><first>Michael</first><last>Doyle</last><affiliation>MITRE</affiliation></author>
      <pages>1044-1050</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive abilities in recent years with regards to code generation and understanding. However, little work has investigated how documentation and other code properties affect an LLM’s ability to understand and generate code or documentation. We present an empirical analysis of how underlying properties of code or documentation can affect an LLM’s capabilities. We show that providing an LLM with “incorrect” documentation can greatly hinder code understanding, while incomplete or missing documentation does not seem to significantly affect an LLM’s ability to understand code.</abstract>
      <url hash="9b5f3a4b">2024.findings-naacl.66</url>
      <bibkey>macke-doyle-2024-testing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.66</doi>
    </paper>
    <paper id="67">
      <title>Aligning Large Language Models with Recommendation Knowledge</title>
      <author><first>Yuwei</first><last>Cao</last></author>
      <author><first>Nikhil</first><last>Mehta</last><affiliation>Research, Google</affiliation></author>
      <author><first>Xinyang</first><last>Yi</last><affiliation>Google</affiliation></author>
      <author><first>Raghunandan</first><last>Hulikal Keshavan</last></author>
      <author><first>Lukasz</first><last>Heldt</last></author>
      <author><first>Lichan</first><last>Hong</last><affiliation>Google</affiliation></author>
      <author><first>Ed</first><last>Chi</last><affiliation>Google</affiliation></author>
      <author><first>Maheswaran</first><last>Sathiamoorthy</last></author>
      <pages>1051-1066</pages>
      <abstract>Large language models (LLMs) have recently been used as backbones for recommender systems. However, their performance often lags behind conventional methods in standard tasks like retrieval. We attribute this to a mismatch between LLMs’ knowledge and the knowledge crucial for effective recommendations. While LLMs excel at natural language reasoning, they cannot model complex user-item interactions inherent in recommendation tasks. We propose bridging the knowledge gap and equipping LLMs with recommendation-specific knowledge to address this. Operations such as Masked Item Modeling (MIM) and Bayesian Personalized Ranking (BPR) have found success in conventional recommender systems. Inspired by this, we simulate these operations through natural language to generate auxiliary-task data samples that encode item correlations and user preferences. Fine-tuning LLMs on such auxiliary-task data samples and incorporating more informative recommendation-task data samples facilitates the injection of recommendation-specific knowledge into LLMs. Extensive experiments across retrieval, ranking, and rating prediction tasks on LLMs such as FLAN-T5-Base and FLAN-T5-XL show the effectiveness of our technique in domains such as Amazon Toys &amp; Games, Beauty, and Sports &amp; Outdoors. Notably, our method outperforms conventional and LLM-based baselines, including the current SOTA, by significant margins in retrieval, showcasing its potential for enhancing recommendation quality.</abstract>
      <url hash="bbd572fa">2024.findings-naacl.67</url>
      <bibkey>cao-etal-2024-aligning</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.67</doi>
    </paper>
    <paper id="68">
      <title><fixed-case>OFA</fixed-case>: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining</title>
      <author><first>Yihong</first><last>Liu</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Peiqin</first><last>Lin</last><affiliation>Institut für Informatik</affiliation></author>
      <author><first>Mingyang</first><last>Wang</last></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>1067-1097</pages>
      <abstract>Instead of pretraining multilingual language models from scratch, a more efficient method is to adapt existing pretrained language models (PLMs) to new languages via vocabulary extension and continued pretraining. However, this method usually randomly initializes the embeddings of new subwords and introduces substantially more embedding parameters to the model, thus weakening the efficiency. To address these issues, we propose a novel framework: <tex-math>\textbf{O}</tex-math>ne <tex-math>\textbf{F}</tex-math>or <tex-math>\textbf{A}</tex-math>ll (<tex-math>\textbf{OFA}</tex-math>), which wisely initializes the embeddings of unseen subwords and thus can adapt a PLM to multiple languages efficiently and effectively. OFA takes advantage of external well-aligned multilingual static word vectors and injects the alignment knowledge into the subword embeddings. In addition, OFA applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which largely reduces the number of parameters. We show OFA accelerates the convergence of continued pretraining, which is environmentally friendly as much fewer carbon footprints are generated. Through extensive experiments, we demonstrate OFA can achieve competitive or better performance than default continued pretraining baselines on a wide range of crosslingual downstream tasks. We make our code and models publicly available.</abstract>
      <url hash="eea02159">2024.findings-naacl.68</url>
      <bibkey>liu-etal-2024-ofa</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.68</doi>
    </paper>
    <paper id="69">
      <title><fixed-case>SELF</fixed-case>-<fixed-case>EXPERTISE</fixed-case>: Knowledge-based Instruction Dataset Augmentation for a Legal Expert Language Model</title>
      <author><first>Minju</first><last>Kim</last></author>
      <author><first>Haein</first><last>Jung</last></author>
      <author><first>Myoung-Wan</first><last>Koo</last><affiliation>Sogang University</affiliation></author>
      <pages>1098-1112</pages>
      <abstract>The advent of instruction-tuned large language models (LLMs) has significantly advanced the field of automatic instruction dataset augmentation. However, the method of generating instructions and outputs from inherent knowledge of LLM can unintentionally produce hallucinations — instances of generating factually incorrect or misleading information. To overcome this, we propose SELF-EXPERTISE, automatically generating instruction dataset in the legal domain from a seed dataset. SELF-EXPERTISE extracts knowledge from the outputs of the seed dataset, and generates new instructions, inputs, and outputs. In this way, the proposed method reduces hallucination in automatic instruction augmentation. We trained an SELF-EXPERTISE augmented instruction dataset on the LLaMA-2 7B model to construct Korean legal specialized model, called LxPERT. LxPERT has demonstrated performance surpassing GPT-3.5-turbo in both in-domain and out-of-domain datasets. The SELF-EXPERTISE augmentation pipeline is not only applicable to the legal field but is also expected to be extendable to various domains, potentially advancing domain-specialized LLMs.</abstract>
      <url hash="988c593d">2024.findings-naacl.69</url>
      <bibkey>kim-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.69</doi>
    </paper>
    <paper id="70">
      <title>Re-evaluating the Need for Visual Signals in Unsupervised Grammar Induction</title>
      <author><first>Boyi</first><last>Li</last><affiliation>NVIDIA Research and University of California, Berkeley</affiliation></author>
      <author><first>Rodolfo</first><last>Corona</last></author>
      <author><first>Karttikeya</first><last>Mangalam</last></author>
      <author id="catherine-chen-bu"><first>Catherine</first><last>Chen</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Daniel</first><last>Flaherty</last></author>
      <author><first>Serge</first><last>Belongie</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Kilian</first><last>Weinberger</last><affiliation>Cornell University, Cornell University and Cornell University</affiliation></author>
      <author><first>Jitendra</first><last>Malik</last><affiliation>Facebook and University of California Berkeley</affiliation></author>
      <author><first>Trevor</first><last>Darrell</last><affiliation>Electrical Engineering &amp; Computer Science Department</affiliation></author>
      <author><first>Dan</first><last>Klein</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>1113-1123</pages>
      <abstract>Are multimodal inputs necessary for grammar induction? Recent work has shown that multimodal training inputs can improve grammar induction. However, these improvements are based on comparisons to weak text-only baselines that were trained on relatively little textual data. To determine whether multimodal inputs are needed in regimes with large amounts of textual training data, we design a stronger text-only baseline, which we refer to as LC-PCFG. LC-PCFG is a C-PFCG that incorporates embeddings from text-only large language models (LLMs). We use a fixed grammar family to directly compare LC-PCFG to various multimodal grammar induction methods. We compare performance on four benchmark datasets. LC-PCFG provides an up to 17% relative improvement in Corpus-F1 compared to state-of-the-art multimodal grammar induction methods. LC-PCFG is also more computationally efficient, providing an up to 85% reduction in parameter count and <tex-math>8.8\times</tex-math> reduction in training time compared to multimodal approaches. These results suggest that multimodal inputs may not be necessary for grammar induction, and emphasize the importance of strong vision-free baselines for evaluating the benefit of multimodal approaches.</abstract>
      <url hash="a6e6dd2d">2024.findings-naacl.70</url>
      <bibkey>li-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.70</doi>
    </paper>
    <paper id="71">
      <title><fixed-case>EDE</fixed-case>ntail: An Entailment-based Few-shot Text Classification with Extensional Definition</title>
      <author><first>Zixiao</first><last>Zhu</last></author>
      <author><first>Junlang</first><last>Qian</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Zijian</first><last>Feng</last></author>
      <author><first>Hanzhang</first><last>Zhou</last></author>
      <author><first>Kezhi</first><last>Mao</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>1124-1137</pages>
      <abstract>Few-shot text classification has seen significant advancements, particularly with entailment-based methods, which typically use either class labels or intensional definitions of class labels in hypotheses for label semantics expression. In this paper, we propose EDEntail, a method that employs extensional definition (EDef) of class labels in hypotheses, aiming to express the semantics of class labels more explicitly. To achieve the above goal, we develop an algorithm to gather and select extensional descriptive words of class labels and then order and format them into a sequence to form hypotheses. Our method has been evaluated and compared with state-of-the-art models on five classification datasets. The results demonstrate that our approach surpasses the supervised-learning methods and prompt-based methods under the few-shot setting, which underlines the potential of using an extensional definition of class labels for entailment-based few-shot text classification. Our code is available at https://github.com/MidiyaZhu/EDEntail.</abstract>
      <url hash="086d995d">2024.findings-naacl.71</url>
      <bibkey>zhu-etal-2024-edentail</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.71</doi>
    </paper>
    <paper id="72">
      <title>What Makes Math Word Problems Challenging for <fixed-case>LLM</fixed-case>s?</title>
      <author><first>Kv Aditya</first><last>Srivatsa</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Ekaterina</first><last>Kochmar</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>1138-1148</pages>
      <abstract>This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs.</abstract>
      <url hash="c96243b2">2024.findings-naacl.72</url>
      <bibkey>srivatsa-kochmar-2024-makes</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.72</doi>
    </paper>
    <paper id="73">
      <title><fixed-case>SMILE</fixed-case>: Multimodal Dataset for Understanding Laughter in Video with Language Models</title>
      <author><first>Lee</first><last>Hyun</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Kim</first><last>Sung-Bin</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Seungju</first><last>Han</last><affiliation>Allen Institute for Artificial Intelligence and Seoul National University</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Tae-Hyun</first><last>Oh</last><affiliation>POSTECH</affiliation></author>
      <pages>1149-1167</pages>
      <abstract>Despite the recent advances in artificial intelligence, building social intelligence remains a challenge.Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans.In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning.We introduce this new task to explain why people laugh in a particular video and a dataset for this task.Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model checkpoints on https://github.com/postech-ami/SMILE-Dataset.</abstract>
      <url hash="3adde58b">2024.findings-naacl.73</url>
      <bibkey>hyun-etal-2024-smile</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.73</doi>
    </paper>
    <paper id="74">
      <title><fixed-case>T</fixed-case>3<fixed-case>M</fixed-case>: Text Guided 3<fixed-case>D</fixed-case> Human Motion Synthesis from Speech</title>
      <author><first>Wenshuo</first><last>Peng</last></author>
      <author><first>Kaipeng</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Sai Qian</first><last>Zhang</last><affiliation>Harvard University, Harvard University, University of Toronto and Facebook</affiliation></author>
      <pages>1168-1177</pages>
      <abstract>Speech-driven 3D motion synthesis seeks to create lifelike animations based on human speech, with potential uses in virtual reality, gaming, and the film production. Existing approaches reply solely on speech audio for motion generation, leading to inaccurate and inflexible synthesis results. To mitigate this problem, we introduce a novel text-guided 3D human motion synthesis method, termed T3M. Unlike traditional approaches, T3M allows precise control over motion synthesis via textual input, enhancing the degree of diversity and user customization. The experiment results demonstrate that T3M can greatly outperform the state-of-the-art methods in both quantitative metrics and qualitative evaluations. We have publicly released our code at https://github.com/Gloria2tt/naacl2024.git</abstract>
      <url hash="1ee4debf">2024.findings-naacl.74</url>
      <bibkey>peng-etal-2024-t3m</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.74</doi>
    </paper>
    <paper id="75">
      <title>Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning</title>
      <author><first>Miao</first><last>Peng</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Ben</first><last>Liu</last></author>
      <author><first>Wenjie</first><last>Xu</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Zihao</first><last>Jiang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jiahui</first><last>Zhu</last></author>
      <author><first>Min</first><last>Peng</last><affiliation>Wuhan University</affiliation></author>
      <pages>1178-1191</pages>
      <abstract>Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via contrastive estimation between queries and candidates. By introducing virtual time prefix tokens, it applies a prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks under different settings. We evaluate ChapTER on four transductive and three few-shot inductive TKGR benchmarks, and experimental results demonstrate that ChapTER achieves superior performance compared to competitive baselines with only 0.17% tuned parameters. We conduct thorough analysis to verify the effectiveness, flexibility and efficiency of ChapTER.</abstract>
      <url hash="499b4237">2024.findings-naacl.75</url>
      <bibkey>peng-etal-2024-deja</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.75</doi>
    </paper>
    <paper id="76">
      <title>Explanation Extraction from Hierarchical Classification Frameworks for Long Legal Documents</title>
      <author><first>Nishchal</first><last>Prasad</last></author>
      <author><first>Taoufiq</first><last>Dkaki</last><affiliation>Institut de Recherche en Informatique de Toulouse</affiliation></author>
      <author><first>Mohand</first><last>Boughanem</last><affiliation>Université de Toulouse</affiliation></author>
      <pages>1192-1201</pages>
      <abstract>Hierarchical classification frameworks have been widely used to process long sequences, especially in the legal domain for predictions from long legal documents. But being black-box models they are unable to explain their predictions making them less reliable for practical applications, more so in the legal domain. In this work, we develop an extractive explanation algorithm for hierarchical frameworks for long sequences based on the sensitivity of the trained model to its input perturbations. We perturb using occlusion and develop Ob-HEx; an Occlusion-based Hierarchical Explanation-extractor. We adapt Ob-HEx to Hierarchical Transformer models trained on long Indian legal texts. And use Ob-HEx to analyze them and extract their explanations for the ILDC-Expert dataset, achieving a minimum gain of 1 point over the previous benchmark on most of our performance evaluation metrics.</abstract>
      <url hash="646fbf1e">2024.findings-naacl.76</url>
      <bibkey>prasad-etal-2024-explanation</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.76</doi>
    </paper>
    <paper id="77">
      <title>Low-Rank Adaptation for Multilingual Summarization: An Empirical Study</title>
      <author><first>Chenxi</first><last>Whitehouse</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Fantine</first><last>Huot</last><affiliation>Google</affiliation></author>
      <author><first>Jasmijn</first><last>Bastings</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Mostafa</first><last>Dehghani</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Chu-Cheng</first><last>Lin</last><affiliation>Google</affiliation></author>
      <author><first>Mirella</first><last>Lapata</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>1202-1228</pages>
      <abstract>Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning outperforms full fine-tuning and the dynamic composition of language-specific LoRA modules.</abstract>
      <url hash="782c1b07">2024.findings-naacl.77</url>
      <bibkey>whitehouse-etal-2024-low</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.77</doi>
    </paper>
    <paper id="78">
      <title>A Tree-of-Thoughts to Broaden Multi-step Reasoning across Languages</title>
      <author><first>Leonardo</first><last>Ranaldi</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Giulia</first><last>Pucci</last></author>
      <author><first>Federico</first><last>Ranaldi</last><affiliation>University of Roma “Tor Vergata”</affiliation></author>
      <author><first>Elena Sofia</first><last>Ruzzetti</last><affiliation>Università degli Studi di Roma Tor Vergata</affiliation></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last><affiliation>University of Rome Tor Vergata</affiliation></author>
      <pages>1229-1241</pages>
      <abstract>Reasoning methods, best exemplified by the well-known Chain-of-Thought (CoT), empower the reasoning abilities of Large Language Models (LLMs) by eliciting them to solve complex tasks in a step-by-step manner. Although they are achieving significant success, the ability to deliver multi-step reasoning remains limited to English because of the imbalance in the distribution of pre-training data, which makes other languages a barrier. In this paper, we propose Cross-lingual Tree-of-Thoughts (Cross-ToT), a method for aligning Cross-lingual CoT reasoning across languages. The proposed method, through a self-consistent cross-lingual prompting mechanism inspired by the Tree-of-Thoughts approach, provides multi-step reasoning paths in different languages that, during the steps, lead to the final solution. Experimental evaluations show that our method significantly outperforms existing prompting methods by reducing the number of interactions and achieving state-of-the-art performance.</abstract>
      <url hash="7578feb0">2024.findings-naacl.78</url>
      <bibkey>ranaldi-etal-2024-tree</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.78</doi>
    </paper>
    <paper id="79">
      <title>Emergent Abilities in Reduced-Scale Generative Language Models</title>
      <author><first>Sherin</first><last>Muckatira</last><affiliation>University of Massachusetts at Lowell</affiliation></author>
      <author><first>Vijeta</first><last>Deshpande</last></author>
      <author><first>Vladislav</first><last>Lialin</last><affiliation>University of Massachusetts, Lowell</affiliation></author>
      <author><first>Anna</first><last>Rumshisky</last><affiliation>University of Massachusetts, Lowell, University of Massachusetts at Lowell and Amazon</affiliation></author>
      <pages>1242-1257</pages>
      <abstract>Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size.Additionally, we find that these smaller models pre-trained on simplified data demonstrate a power law relationship between the evaluation loss and the three scaling factors: compute, dataset size, and model size.</abstract>
      <url hash="ec7f28bf">2024.findings-naacl.79</url>
      <bibkey>muckatira-etal-2024-emergent</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.79</doi>
    </paper>
    <paper id="80">
      <title>Context Does Matter: Implications for Crowdsourced Evaluation Labels in Task-Oriented Dialogue Systems</title>
      <author><first>Clemencia</first><last>Siro</last></author>
      <author><first>Mohammad</first><last>Aliannejadi</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Maarten</first><last>Rijke</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>1258-1273</pages>
      <abstract>Crowdsourced labels play a crucial role in evaluating task-oriented dialogue systems (TDSs). Obtaining high-quality and consistent ground-truth labels from annotators presents challenges. When evaluating a TDS, annotators must fully comprehend the dialogue before providing judgments. Previous studies suggest using only a portion of the dialogue context in the annotation process. However, the impact of this limitation on label quality remains unexplored. This study investigates the influence of dialogue context on annotation quality, considering the truncated context for relevance and usefulness labeling. We further propose to use large language models ( LLMs) to summarize the dialogue context to provide a rich and short description of the dialogue context and study the impact of doing so on the annotator’s performance. Reducing context leads to more positive ratings. Conversely, providing the entire dialogue context yields higher-quality relevance ratings but introduces ambiguity in usefulness ratings. Using the first user utterance as context leads to consistent ratings, akin to those obtained using the entire dialogue, with significantly reduced annotation effort. Our findings show how task design, particularly the availability of dialogue context, affects the quality and consistency of crowdsourced evaluation labels.</abstract>
      <url hash="874ad2d5">2024.findings-naacl.80</url>
      <bibkey>siro-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.80</doi>
    </paper>
    <paper id="81">
      <title>Matching Varying-Length Texts via Topic-Informed and Decoupled Sentence Embeddings</title>
      <author><first>Xixi</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chunbin</first><last>Gu</last></author>
      <author><first>Xin</first><last>Jie</last></author>
      <author><first>Jiajun</first><last>Bu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haishuai</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>1274-1280</pages>
      <abstract>Measuring semantic similarity between texts is a crucial task in natural language processing. While existing semantic text matching focuses on pairs of similar-length sequences, matching texts with non-comparable lengths has broader applications in specific domains, such as comparing professional document summaries and content. Current approaches struggle with text pairs of non-comparable lengths due to truncation issues. To address this, we split texts into natural sentences and decouple sentence representations using supervised contrastive learning (SCL). Meanwhile, we adopt the embedded topic model (ETM) for specific domain data. Our experiments demonstrate the effectiveness of our model, based on decoupled and topic-informed sentence embeddings, in matching texts of significantly different lengths across three well-studied datasets.</abstract>
      <url hash="3bb3bb06">2024.findings-naacl.81</url>
      <bibkey>zhou-etal-2024-matching</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.81</doi>
    </paper>
    <paper id="82">
      <title>Instruction Tuning with Human Curriculum</title>
      <author><first>Bruce W</first><last>Lee</last></author>
      <author><first>Hyunsoo</first><last>Cho</last><affiliation>Ewha Women’s University</affiliation></author>
      <author><first>Kang Min</first><last>Yoo</last><affiliation>NAVER</affiliation></author>
      <pages>1281-1309</pages>
      <abstract>In this work, we (1) introduce Curriculum Instruction Tuning, (2) explore the potential advantages of employing diverse curriculum strategies, and (3) delineate a synthetic instruction-response generation framework that complements our theoretical approach. Distinct from the existing instruction tuning dataset, our generation pipeline is systematically structured to emulate the sequential and orderly characteristic of human learning. Additionally, we describe a methodology for generating instruction-response datasets that extensively span the various stages of human education, from middle school through the graduate level, utilizing educational subject catalogs.Before training, we meticulously organize the instruction data to ensure that questions escalate in difficulty regarding (A) the subject matter and (B) the intricacy of the instructions. The findings of our study reveal that substantial improvements in performance can be achieved through the mere application of curriculum ordering to instruction data—achieving gains of +4.76 on TruthfulQA, +2.98 on MMLU, +2.8 on OpenbookQA, and +1.28 on ARC-hard—compared to random shuffling. This enhancement is achieved without incurring additional computational expenses. Through comprehensive experimentation, we observe that the advantages of our proposed method are consistently evident across nine benchmarks.</abstract>
      <url hash="803771cb">2024.findings-naacl.82</url>
      <bibkey>lee-etal-2024-instruction</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.82</doi>
    </paper>
    <paper id="83">
      <title>Natural Language-based State Representation in Deep Reinforcement Learning</title>
      <author><first>Md Masudur</first><last>Rahman</last><affiliation>Purdue University</affiliation></author>
      <author><first>Yexiang</first><last>Xue</last><affiliation>Purdue University, Purdue University and Purdue University</affiliation></author>
      <pages>1310-1319</pages>
      <abstract>This paper investigates the potential of using natural language descriptions as an alternative to direct image-based observations for learning policies in reinforcement learning. Due to the inherent challenges in managing image-based observations, which include abundant information and irrelevant features, we propose a method that compresses images into a natural language form for state representation. This approach allows better interpretability and leverages the processing capabilities of large-language models. We conducted several experiments involving tasks that required image-based observation. The results demonstrated that policies trained using natural language descriptions of images yield better generalization than those trained directly from images, emphasizing the potential of this approach in practical settings.</abstract>
      <url hash="345b241a">2024.findings-naacl.83</url>
      <bibkey>rahman-xue-2024-natural</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.83</doi>
    </paper>
    <paper id="84">
      <title>Learning Cross-Architecture Instruction Embeddings for Binary Code Analysis in Low-Resource Architectures</title>
      <author><first>Junzhe</first><last>Wang</last></author>
      <author><first>Qiang</first><last>Zeng</last><affiliation>George Mason University</affiliation></author>
      <author><first>Lannan</first><last>Luo</last><affiliation>George Mason University</affiliation></author>
      <pages>1320-1332</pages>
      <abstract>Binary code analysis is indispensable for a variety of software security tasks. Applying deep learning to binary code analysis has drawn great attention because of its notable performance. Today, source code is frequently compiled for various Instruction Set Architectures (ISAs). It is thus critical to expand binary analysis capabilities to multiple ISAs. Given a binary analysis task, the scale of available data on different ISAs varies. As a result, the rich datasets (e.g., malware) for certain ISAs, such as x86, lead to a disproportionate focus on these ISAs and a negligence of other ISAs, such as PowerPC, which suffer from the “data scarcity” problem. To address the problem, we propose to learn cross-architecture instruction embeddings (CAIE), where semantically-similar instructions, regardless of their ISAs, have close embeddings in a shared space. Consequently, we can transfer a model trained on a data-rich ISA to another ISA with less available data. We consider four ISAs (x86, ARM, MIPS, and PowerPC) and conduct both intrinsic and extrinsic evaluations (including malware detection and function similarity comparison). The results demonstrate the effectiveness of our approach to generate high-quality CAIE with good transferability.</abstract>
      <url hash="c0e15083">2024.findings-naacl.84</url>
      <bibkey>wang-etal-2024-learning-cross</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.84</doi>
    </paper>
    <paper id="85">
      <title><fixed-case>R</fixed-case>e<fixed-case>E</fixed-case>val: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks</title>
      <author><first>Xiaodong</first><last>Yu</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <author><first>Hao</first><last>Cheng</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last><affiliation>Microsoft Research</affiliation></author>
      <pages>1333-1351</pages>
      <abstract>Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs’ reliability in using new evidence for answering.We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection ofLLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.</abstract>
      <url hash="2eede47c">2024.findings-naacl.85</url>
      <bibkey>yu-etal-2024-reeval</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.85</doi>
    </paper>
    <paper id="86">
      <title>An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution</title>
      <author><first>Tien-Hong</first><last>Lo</last><affiliation>National Taiwan Normal University</affiliation></author>
      <author><first>Fu-An</first><last>Chao</last><affiliation>National Taiwan Normal University</affiliation></author>
      <author><first>Tzu-i</first><last>Wu</last></author>
      <author><first>Yao-Ting</first><last>Sung</last></author>
      <author><first>Berlin</first><last>Chen</last><affiliation>National Taiwan Normal University</affiliation></author>
      <pages>1352-1362</pages>
      <abstract>Automated speaking assessment (ASA) typically involves automatic speech recognition (ASR) and hand-crafted feature extraction from the ASR transcript of a learner’s speech. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss re-weighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.</abstract>
      <url hash="64957e4d">2024.findings-naacl.86</url>
      <bibkey>lo-etal-2024-effective</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.86</doi>
    </paper>
    <paper id="87">
      <title><fixed-case>GPT</fixed-case>-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards <fixed-case>GPT</fixed-case>-4 and Beyond</title>
      <author><first>Shen</first><last>Zheng</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Yuyu</first><last>Zhang</last><affiliation>ByteDance</affiliation></author>
      <author><first>Yijie</first><last>Zhu</last></author>
      <author><first>Chenguang</first><last>Xi</last></author>
      <author><first>Pengyang</first><last>Gao</last></author>
      <author><first>Zhou</first><last>Xun</last></author>
      <author><first>Kevin</first><last>Chang</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>1363-1382</pages>
      <abstract>With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI’s legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI’s earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM’s reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of these questions, aiming to improve the transparency of advanced LLMs.</abstract>
      <url hash="051f95b5">2024.findings-naacl.87</url>
      <bibkey>zheng-etal-2024-gpt</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.87</doi>
    </paper>
    <paper id="88">
      <title>Subword Attention and Post-Processing for Rare and Unknown Contextualized Embeddings</title>
      <author><first>Raj</first><last>Patel</last><affiliation>George Mason University</affiliation></author>
      <author><first>Carlotta</first><last>Domeniconi</last><affiliation>George Mason University and George Mason University</affiliation></author>
      <pages>1383-1389</pages>
      <abstract>Word representations are an important aspect of Natural Language Processing (NLP). Representations are trained using large corpora, either as independent static embeddings or as part of a deep contextualized model. While word embeddings are useful, they struggle on rare and unknown words. As such, a large body of work has been done on estimating rare and unknown words. However, most of the methods focus on static embeddings, with few models focused on contextualized representations. In this work, we propose SPRUCE, a rare/unknown embedding architecture that focuses on contextualized representations. This architecture uses subword attention and embedding post-processing combined with the contextualized model to produce high quality embeddings. We then demonstrate these techniques lead to improved performance in most intrinsic and downstream tasks.</abstract>
      <url hash="3ad4cce8">2024.findings-naacl.88</url>
      <bibkey>patel-domeniconi-2024-subword</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.88</doi>
    </paper>
    <paper id="89">
      <title><fixed-case>UGIF</fixed-case>-<fixed-case>D</fixed-case>ata<fixed-case>S</fixed-case>et: A New Dataset for Cross-lingual, Cross-modal Sequential actions on the <fixed-case>UI</fixed-case></title>
      <author><first>Sagar</first><last>Gubbi Venkatesh</last></author>
      <author><first>Partha</first><last>Talukdar</last><affiliation>Google Research and Indian Institute of Science, Bangalore</affiliation></author>
      <author><first>Srini</first><last>Narayanan</last><affiliation>Google research</affiliation></author>
      <pages>1390-1399</pages>
      <abstract>Help documents are supposed to aid smartphone users in resolving queries such as “How to block calls from unknown numbers?”. However, given a query, identifying the right help document, understanding instructions from the document, and using them to resolve the issue at hand is challenging. The user experience may be enhanced by converting the instructions in the help document to a step-by-step tutorial overlaid on the phone UI. Successful execution of this task requires overcoming research challenges in retrieval, parsing, and grounding in the multilingual-multimodal setting. For example, user queries in one language may have to be matched against instructions in another language, which in turn needs to be grounded in a multimodal UI in yet another language. Moreover, there isn’t any relevant dataset for such a task. In order to bridge this gap, we introduce UGIF-DataSet, a multi-lingual, multi-modal UI grounded dataset for step-by-step task completion on the smartphone, containing 4,184 tasks across 8 languages. The instruction steps in UGIF-DataSet are available only in English, so the challenge involves operations in the cross-modal, cross-lingual setting. We compare the performance of different large language models for this task and find that the end-to-end task completion rate drops from 48% in English to 32% for other languages, demonstrating significant overall headroom for improvement. We are hopeful that UGIF-DataSet and our analysis will aid further research on the important problem of sequential task completion in the multilingual and multimodal setting.</abstract>
      <url hash="734d8018">2024.findings-naacl.89</url>
      <bibkey>gubbi-venkatesh-etal-2024-ugif</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.89</doi>
    </paper>
    <paper id="90">
      <title><fixed-case>S</fixed-case>im<fixed-case>SCOOD</fixed-case>: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models</title>
      <author><first>Hossein</first><last>Hajipour</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Ning</first><last>Yu</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Cristian-Alexandru</first><last>Staicu</last></author>
      <author><first>Mario</first><last>Fritz</last><affiliation>CISPA Helmholtz Center for Information Security and Saarland University</affiliation></author>
      <pages>1400-1416</pages>
      <abstract>Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet.In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on four state-of-the-art pretrained models and applied to two code generation tasks, exposes multiple failure modes attributed to OOD generalization issues.</abstract>
      <url hash="57cd3b2e">2024.findings-naacl.90</url>
      <bibkey>hajipour-etal-2024-simscood</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.90</doi>
    </paper>
    <paper id="91">
      <title>Pruning as a Domain-specific <fixed-case>LLM</fixed-case> Extractor</title>
      <author><first>Nan</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Yanchi</first><last>Liu</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Xujiang</first><last>Zhao</last><affiliation>NEC Labs America</affiliation></author>
      <author><first>Wei</first><last>Cheng</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Runxue</first><last>Bao</last><affiliation>NEC Labs America</affiliation></author>
      <author><first>Rui</first><last>Zhang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Prasenjit</first><last>Mitra</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Haifeng</first><last>Chen</last><affiliation>NEC-Labs</affiliation></author>
      <pages>1417-1428</pages>
      <abstract>Large Language Models (LLMs) have exhibited remarkable proficiency across a wide array of NLP tasks. However, the escalation in model size also engenders substantial deployment costs. While few efforts have explored model pruning techniques to reduce the size of LLMs, they mainly center on general or task-specific weights. This leads to suboptimal performance due to lacking specificity on the target domain or generality on different tasks when applied to domain-specific challenges. This work introduces an innovative unstructured dual-pruning methodology, D-Pruner, for domain-specific compression on LLM. It extracts a compressed, domain-specific, and task- agnostic LLM by identifying LLM weights that are pivotal for general capabilities, like linguistic capability and multi-task solving, and domain-specific knowledge. More specifically, we first assess general weight importance by quantifying the error incurred upon their removal with the help of an open-domain calibration dataset. Then, we utilize this general weight importance to refine the training loss, so that it preserves generality when fitting into a specific domain. Moreover, by efficiently approximating weight importance with the refined training loss on a domain-specific calibration dataset, we obtain a pruned model emphasizing generality and specificity. Our comprehensive experiments across various tasks in healthcare and legal domains show the effectiveness of D-Pruner in domain-specific compression. Our code is available at https://github.com/psunlpgroup/D-Pruner.</abstract>
      <url hash="580a584f">2024.findings-naacl.91</url>
      <bibkey>zhang-etal-2024-pruning</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.91</doi>
    </paper>
    <paper id="92">
      <title><fixed-case>LLMR</fixed-case>efine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback</title>
      <author><first>Wenda</first><last>Xu</last></author>
      <author><first>Daniel</first><last>Deutsch</last><affiliation>Google</affiliation></author>
      <author><first>Mara</first><last>Finkelstein</last><affiliation>Google</affiliation></author>
      <author><first>Juraj</first><last>Juraska</last><affiliation>Google</affiliation></author>
      <author><first>Biao</first><last>Zhang</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Zhongtao</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Markus</first><last>Freitag</last><affiliation>Google</affiliation></author>
      <pages>1429-1445</pages>
      <abstract>Recent large language models (LLM) areleveraging human feedback to improve theirgeneration quality. However, human feedbackis costly to obtain, especially during inference.In this work, we propose LLMRefine, aninference time optimization method to refineLLM’s output. The core idea is to usea learned fine-grained feedback model topinpoint defects and guide LLM to refinethem iteratively. Using original LLM as aproposal of edits, LLMRefine searches fordefect-less text via simulated annealing, tradingoff the exploration and exploitation. Weconduct experiments on three text generationtasks, including machine translation, long-form question answering (QA), and topicalsummarization. LLMRefine consistentlyoutperforms all baseline approaches, achievingimprovements up to 1.7 MetricX points ontranslation tasks, 8.1 ROUGE-L on ASQA, 2.2ROUGE-L on topical summarization.</abstract>
      <url hash="9b6bf8a8">2024.findings-naacl.92</url>
      <bibkey>xu-etal-2024-llmrefine</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.92</doi>
    </paper>
    <paper id="93">
      <title>Noisy Multi-Label Text Classification via Instance-Label Pair Correction</title>
      <author><first>Pengyu</first><last>Xu</last></author>
      <author><first>Mingyang</first><last>Song</last><affiliation>Tencent</affiliation></author>
      <author><first>Linkaida</first><last>Liu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Hongjian</first><last>Sun</last></author>
      <author><first>Liping</first><last>Jing</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jian</first><last>Yu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>1446-1458</pages>
      <abstract>In noisy label learning, instance selection based on small-loss criteria has been proven to be highly effective. However, in the case of noisy multi-label text classification (NMLTC), the presence of noise is not limited to the instance-level but extends to the (instance-label) pair-level.This gives rise to two main challenges.(1) The loss information at the pair-level fails to capture the variations between instances. (2) There are two types of noise at the pair-level: false positives and false negatives. Identifying false negatives from a large pool of negative pairs presents an exceedingly difficult task. To tackle these issues, we propose a novel approach called instance-label pair correction (iLaCo), which aims to address the problem of noisy pair selection and correction in NMLTC tasks.Specifically, we first introduce a holistic selection metric that identifies noisy pairs by simultaneously considering global loss information and instance-specific ranking information.Secondly, we employ a filter guided by label correlation to focus exclusively on negative pairs with label relevance. This filter significantly reduces the difficulty of identifying false negatives.Experimental analysis indicates that our framework effectively corrects noisy pairs in NMLTC datasets, leading to a significant improvement in model performance.</abstract>
      <url hash="610003a2">2024.findings-naacl.93</url>
      <bibkey>xu-etal-2024-noisy</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.93</doi>
    </paper>
    <paper id="94">
      <title>Composite Backdoor Attacks Against Large Language Models</title>
      <author><first>Hai</first><last>Huang</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Zhengyu</first><last>Zhao</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Michael</first><last>Backes</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <author><first>Yun</first><last>Shen</last><affiliation>NetApp</affiliation></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>CISPA Helmholtz Center for Information Security</affiliation></author>
      <pages>1459-1472</pages>
      <abstract>Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with 3% poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a 100% Attack Success Rate (ASR) with a False Triggered Rate (FTR) below 2.06% and negligible model accuracy degradation. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.</abstract>
      <url hash="02f970fe">2024.findings-naacl.94</url>
      <bibkey>huang-etal-2024-composite</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.94</doi>
    </paper>
    <paper id="95">
      <title>Adapting Fake News Detection to the Era of Large Language Models</title>
      <author><first>Jinyan</first><last>Su</last><affiliation>Cornell University</affiliation></author>
      <author><first>Claire</first><last>Cardie</last><affiliation>Cornell University</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>1473-1490</pages>
      <abstract>In the age of large language models (LLMs) and the widespread adoption of AI-driven content creation, the landscape of information dissemination has witnessed a paradigm shift. With the proliferation of both human-written and machine-generated real and fake news, robustly and effectively discerning the veracity of news articles has become an intricate challenge. While substantial research has been dedicated to fake news detection, it has either assumed that all news articles are human-written or has abruptly assumed that all machine-generated news was fake. Thus, a significant gap exists in understanding the interplay between machine-paraphrased real news, machine-generated fake news, human-written fake news, and human-written real news. In this paper, we study this gap by conducting a comprehensive evaluation of fake news detectors trained in various scenarios. Our primary objectives revolve around the following pivotal question: How can we adapt fake news detectors to the era of LLMs?Our experiments reveal an interesting pattern that detectors trained exclusively on human-written articles can indeed perform well at detecting machine-generated fake news, but not vice versa. Moreover, due to the bias of detectors against machine-generated texts (CITATION), they should be trained on datasets with a lower machine-generated news ratio than the test set. Building on our findings, we provide a practical strategy for the development of robust fake news detectors.</abstract>
      <url hash="6a2c83da">2024.findings-naacl.95</url>
      <bibkey>su-etal-2024-adapting</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.95</doi>
    </paper>
    <paper id="96">
      <title><fixed-case>MCAD</fixed-case>: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval</title>
      <author><first>Youbo</first><last>Lei</last></author>
      <author><first>Feifei</first><last>He</last><affiliation>Researcher at OPPO Research Institute</affiliation></author>
      <author><first>Chen</first><last>Chen</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Yingbin</first><last>Mo</last></author>
      <author><first>Sijia</first><last>Li</last><affiliation>OPPO Research Institute</affiliation></author>
      <author><first>Defeng</first><last>Xie</last></author>
      <author><first>Haonan</first><last>Lu</last><affiliation>OPPO Guangdong Mobile Telecommunications Co., Ltd.</affiliation></author>
      <pages>1491-1503</pages>
      <abstract>Due to the success of large-scale visual-language pretraining (VLP) models and the widespread use of image-text retrieval in industry areas, it is now critically necessary to reduce the model size and streamline their mobile-device deployment. Single- and dual-stream model structures are commonly used in image-text retrieval with the goal of closing the semantic gap between textual and visual modalities. While single-stream models use deep feature fusion to achieve more accurate cross-model alignment, dual-stream models are better at offline indexing and fast inference. We propose a Multi-teacher Cross-modality Alignment Distillation (MCAD) technique to integrate the advantages of single- and dual-stream models. By incorporating the fused single-stream features into the image and text features of the dual-stream model, we formulate new modified teacher similarity distributions and features. Then, we conduct both distribution and feature distillation to boost the capability of the student dual-stream model, achieving high retrieval performance without increasing inference complexity. Extensive experiments demonstrate the remarkable performance and high efficiency of MCAD on image-text retrieval tasks. Furthermore, we implement a lightweight CLIP model on Snapdragon/Dimensity chips with only ~100M running memory and ~8.0ms search latency, achieving the mobile-device application of VLP models.</abstract>
      <url hash="f677b35f">2024.findings-naacl.96</url>
      <bibkey>lei-etal-2024-mcad</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.96</doi>
    </paper>
    <paper id="97">
      <title>Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting</title>
      <author><first>Zhen</first><last>Qin</last><affiliation>Google</affiliation></author>
      <author><first>Rolf</first><last>Jagerman</last><affiliation>Google</affiliation></author>
      <author><first>Kai</first><last>Hui</last><affiliation>Google</affiliation></author>
      <author><first>Honglei</first><last>Zhuang</last><affiliation>Google Research</affiliation></author>
      <author><first>Junru</first><last>Wu</last><affiliation>Google Research</affiliation></author>
      <author><first>Le</first><last>Yan</last><affiliation>Google</affiliation></author>
      <author><first>Jiaming</first><last>Shen</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Tianqi</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Jialu</first><last>Liu</last><affiliation>Google Research</affiliation></author>
      <author><first>Donald</first><last>Metzler</last><affiliation>Google</affiliation></author>
      <author><first>Xuanhui</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <pages>1504-1518</pages>
      <abstract>Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets.We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP).Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&amp;2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10.Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.</abstract>
      <url hash="b36bdbc0">2024.findings-naacl.97</url>
      <bibkey>qin-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.97</doi>
    </paper>
    <paper id="98">
      <title><fixed-case>F</fixed-case>ed<fixed-case>LFC</fixed-case>: Towards Efficient Federated Multilingual Modeling with <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>-based Language Family Clustering</title>
      <author><first>Zhihan</first><last>Guo</last></author>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong</affiliation></author>
      <author><first>Zhuo</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zenglin</first><last>Xu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Irwin</first><last>King</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>1519-1528</pages>
      <abstract>Federated Multilingual Modeling (FMM) plays a crucial role in the applications of natural language processing due to the increasing diversity of languages and the growing demand for data privacy. However, FMM faces limitations stemming from (1) the substantial communication costs in networking and (2) the conflicts arising from parameter interference between different languages. To address these challenges, we introduce a communication-efficient federated learning framework with low-rank adaptation and language family clustering for Multilingual Modeling (MM). In this framework, we maintain the weights of the base model, exclusively updating the lightweight Low-rank adaptation (LoRA) parameters to minimize communication costs. Additionally, we mitigate parameter conflicts by grouping languages based on their language family affiliations, as opposed to aggregating all LoRA parameters. Experiments demonstrate that our proposed model not only surpasses the baseline models in performance but also reduces the communication overhead. Our code is available at https://github.com/zhihan-guo/FedLFC.</abstract>
      <url hash="978ba75b">2024.findings-naacl.98</url>
      <bibkey>guo-etal-2024-fedlfc</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.98</doi>
    </paper>
    <paper id="99">
      <title><fixed-case>G</fixed-case>aussian Process Optimization for Adaptable Multi-Objective Text Generation using Linearly-Weighted Language Models</title>
      <author><first>Mohammad Mahdi</first><last>Abdollah Pour</last></author>
      <author><first>Ali</first><last>Pesaranghader</last><affiliation>LG Electronics</affiliation></author>
      <author><first>Eldan</first><last>Cohen</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Scott</first><last>Sanner</last><affiliation>Department of Mechanical and Industrial Engineering, University of Toronto and Department of Computer Science</affiliation></author>
      <pages>1529-1536</pages>
      <abstract>In multi-objective text generation, we aim to optimize over multiple weighted aspects (e.g., toxicity, semantic preservation, fluency) of the generated text. However, multi-objective weighting schemes may change dynamically in practice according to deployment requirements, evolving business needs, personalization requirements on edge devices, or the availability of new language models and/or objective requirements. Ideally, we need an efficient method to adapt to the dynamic requirements of the overall objective. To address these requirements, we propose a linear combination of objective-specific language models to <b>efficiently</b> adapt the decoding process and optimize for the desired objective <b>without</b> the significant computational overhead of retraining one or more language models. We show empirically that we can leverage Gaussian Process black box optimization to adapt the language model decoder weights to outperform other fixed weighting schemes and standard baselines of the task in only a few iterations of decoding. Overall this approach enables highly efficient adaptation of controllable language models via multi-objective weighting schemes that may evolve dynamically in practical deployment situations.</abstract>
      <url hash="ef661749">2024.findings-naacl.99</url>
      <bibkey>abdollah-pour-etal-2024-gaussian</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.99</doi>
    </paper>
    <paper id="100">
      <title>Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study</title>
      <author><first>Alessandro</first><last>Stolfo</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <pages>1537-1552</pages>
      <abstract>We present an empirical study of groundedness in long-form question answering (LFQA) by retrieval-augmented large language models (LLMs).In particular, we evaluate whether every generated sentence is grounded in the retrieved documents or the model’s pre-training data.Across 3 datasets and 4 model families, our findings reveal that a significant fraction of generated sentences are consistently ungrounded, even when those sentences contain correct ground-truth answers.Additionally, we examine the impacts of factors such as model size, decoding strategy, and instruction tuning on groundedness. Our results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations. This study provides novel insights into the groundedness challenges in LFQA and underscores the necessity for more robust mechanisms in LLMs to mitigate the generation of ungrounded content.</abstract>
      <url hash="bf117162">2024.findings-naacl.100</url>
      <bibkey>stolfo-2024-groundedness</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.100</doi>
    </paper>
    <paper id="101">
      <title><fixed-case>T</fixed-case>ag<fixed-case>D</fixed-case>ebias: Entity and Concept Tagging for Social Bias Mitigation in Pretrained Language Models</title>
      <author><first>Mehrnaz</first><last>Moslemi</last></author>
      <author><first>Amal</first><last>Zouaq</last><affiliation>Polytechnique Montreal</affiliation></author>
      <pages>1553-1567</pages>
      <abstract>Pre-trained language models (PLMs) play a crucial role in various applications, including sensitive domains such as the hiring process. However, extensive research has unveiled that these models tend to replicate social biases present in their pre-training data, raising ethical concerns. In this study, we propose the TagDebias method, which proposes debiasing a dataset using type tags. It then proceeds to fine-tune PLMs on this debiased dataset. Experiments show that our proposed TagDebias model, when applied to a ranking task, exhibits significant improvements in bias scores.</abstract>
      <url hash="7a02ded3">2024.findings-naacl.101</url>
      <bibkey>moslemi-zouaq-2024-tagdebias</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.101</doi>
    </paper>
    <paper id="102">
      <title>Improving Absent Keyphrase Generation with Diversity Heads</title>
      <author><first>Edwin</first><last>Thomas</last></author>
      <author><first>Sowmya</first><last>Vajjala</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>1568-1584</pages>
      <abstract>Keyphrase Generation (KPG) is the task of automatically generating appropriate keyphrases for a given text, with a wide range of real-world applications such as document indexing and tagging, information retrieval, and text summarization. NLP research makes a distinction between present and absent keyphrases based on whether a keyphrase is directly present as a sequence of words in the document during evaluation. However, present and absent keyphrases are treated together in a text-to-text generation framework during training. We treat present keyphrase extraction as a sequence labeling problem and propose a new absent keyphrase generation model that uses a modified cross-attention layer with additional heads to capture diverse views for the same context encoding in this paper. Our experiments show improvements over the state-of-the-art for four datasets for present keyphrase extraction and five datasets for absent keyphrase generation among the six English datasets we explored, covering long and short documents.</abstract>
      <url hash="dd9e9f24">2024.findings-naacl.102</url>
      <bibkey>thomas-vajjala-2024-improving</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.102</doi>
    </paper>
    <paper id="103">
      <title>m<fixed-case>O</fixed-case>thello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?</title>
      <author><first>Tianze</first><last>Hua</last><affiliation>Brown University</affiliation></author>
      <author><first>Tian</first><last>Yun</last><affiliation>Brown University</affiliation></author>
      <author><first>Ellie</first><last>Pavlick</last><affiliation>Brown University and Brown University</affiliation></author>
      <pages>1585-1598</pages>
      <abstract>Many pretrained multilingual models exhibit cross-lingual transfer ability, which is often attributed to a learned language-neutral representation during pretraining. However, it remains unclear what factors contribute to the learning of a language-neutral representation, and whether the learned language-neutral representation suffices to facilitate cross-lingual transfer. We propose a synthetic task, Multilingual Othello (mOthello), as a testbed to delve into these two questions. We find that: (1) models trained with naive multilingual pretraining fail to learn a language-neutral representation across all input languages; (2) the introduction of “anchor tokens” (i.e., lexical items that are identical across languages) helps cross-lingual representation alignment; and (3) the learning of a language-neutral representation alone is not sufficient to facilitate cross-lingual transfer. Based on our findings, we propose a novel approach – multilingual pretraining with unified output space – that both induces the learning of language-neutral representation and facilitates cross-lingual transfer.</abstract>
      <url hash="e11e2a5e">2024.findings-naacl.103</url>
      <bibkey>hua-etal-2024-mothello</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.103</doi>
    </paper>
    <paper id="104">
      <title>Discovering and Mitigating Indirect Bias in Attention-Based Model Explanations</title>
      <author><first>Farsheed</first><last>Haque</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <author><first>Depeng</first><last>Xu</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <author><first>Shuhan</first><last>Yuan</last><affiliation>Utah State University</affiliation></author>
      <pages>1599-1614</pages>
      <abstract>As the field of Natural Language Processing (NLP) increasingly adopts transformer-based models, the issue of bias becomes more pronounced. Such bias, manifesting through stereotypes and discriminatory practices, can disadvantage certain groups. Our study focuses on direct and indirect bias in the model explanations, where the model makes predictions relying heavily on identity tokens or associated contexts. We present a novel analysis of bias in model explanation, especially the subtle indirect bias, underlining the limitations of traditional fairness metrics. We first define direct and indirect bias in model explanations, which is complementary to fairness in predictions. We then develop an indirect bias discovery algorithm for quantitatively evaluating indirect bias in transformer models using their in-built self-attention matrix. We also propose an indirect bias mitigation algorithm to ensure fairness in transformer models by leveraging attention explanations. Our evaluation shows the significance of indirect bias and the effectiveness of our indirect bias discovery and mitigation.</abstract>
      <url hash="b1f6d994">2024.findings-naacl.104</url>
      <bibkey>haque-etal-2024-discovering</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.104</doi>
    </paper>
    <paper id="105">
      <title>i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data</title>
      <author><first>Ziyi</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mahmoud</first><last>Khademi</last><affiliation>Microsoft and University of British Columbia</affiliation></author>
      <author><first>Yichong</first><last>Xu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Reid</first><last>Pryzant</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Yuwei</first><last>Fang</last><affiliation>Snap Inc.</affiliation></author>
      <author><first>Chenguang</first><last>Zhu</last><affiliation>Zoom</affiliation></author>
      <author><first>Dongdong</first><last>Chen</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Yao</first><last>Qian</last></author>
      <author><first>Xuemei</first><last>Gao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yi-Ling</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Robert</first><last>Gmyr</last><affiliation>Paderborn University and Microsoft</affiliation></author>
      <author><first>Naoyuki</first><last>Kanda</last><affiliation>Microsoft</affiliation></author>
      <author><first>Noel</first><last>Codella</last><affiliation>Microsoft</affiliation></author>
      <author><first>Bin</first><last>Xiao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yu</first><last>Shi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Lu</first><last>Yuan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Takuya</first><last>Yoshioka</last><affiliation>Microsoft</affiliation></author>
      <author><first>Michael</first><last>Zeng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xuedong</first><last>Huang</last></author>
      <pages>1615-1627</pages>
      <abstract>The convergence of text, visual, and audio data is crucial towards human-like artificial intelligence, however the current Vision-Language-Speech landscape is dominated by encoder-only models that lack generative abilities. We propose closing this gap with i-Code V2, one of the first models capable of generating natural language from any combination of Vision, Language, and Speech data. i-Code V2 leverages state-of-the-art single-modality encoders, combining their outputs with a new modality-fusing encoder to project combinations of modalities into a shared representational space. Language tokens are generated from these representations via an autoregressive decoder. i-Code V2 is pretrained end-to-end on a large collection of dual- and single-modality datasets with a novel text completion objective that can be generalized across arbitrary combinations of modalities. i-Code V2 matches or outperforms state-of-the-art single- and dual-modality baselines on 7 multimodal tasks, demonstrating the power of generative multimodal pretraining across a diversity of tasks and signals.</abstract>
      <url hash="d4695178">2024.findings-naacl.105</url>
      <bibkey>yang-etal-2024-code</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.105</doi>
    </paper>
    <paper id="106">
      <title>Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation</title>
      <author><first>Yifu</first><last>Qiu</last></author>
      <author><first>Varun</first><last>Embar</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <author><first>Shay</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Benjamin</first><last>Han</last><affiliation>Apple</affiliation></author>
      <pages>1628-1644</pages>
      <abstract>Knowledge-to-text generators often struggle to faithfully generate descriptions for the input facts: they may produce hallucinations that contradict the input, or describe facts not present in the input. To reduce hallucinations, we propose a decoding-only method, TWEAK (Think While Effectively Articulating Knowledge), which can be integrated with any generator without retraining. TWEAK treats the generated sequences at each decoding step and its future sequences as hypotheses, and ranks each generation candidate based on the extent to which their hypotheses are supported by the input facts using a Hypothesis Verification Model (HVM). We first demonstrate the effectiveness of TWEAK by using a Natural Language Inference (NLI) model as the HVM and report improved faithfulness with a minimal impact on the quality. We then replace the NLI model with a task-specific HVM trained with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which pairs input facts with their original and perturbed descriptions. We test TWEAK with two generators, and the best TWEAK variants improve on average for the two models by 2.24/7.17 points in faithfulness (FactKB) in in/out-of-distribution evaluations, respectively, and with only a 0.14/0.32-point decline in quality (BERTScore).</abstract>
      <url hash="a12fd500">2024.findings-naacl.106</url>
      <bibkey>qiu-etal-2024-think</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.106</doi>
    </paper>
    <paper id="107">
      <title>It’s All Relative! – A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction</title>
      <author><first>Aditi</first><last>Chaudhary</last><affiliation>Google</affiliation></author>
      <author><first>Karthik</first><last>Raman</last><affiliation>Google</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <pages>1645-1664</pages>
      <abstract>Large language models (LLMs) have shown promising ability to generate synthetic query-document pairs by prompting with as few as 8 demonstrations. This has enabled building better IR models, especially for tasks with no training data. Typically, such synthetic query generation (QGen) approaches condition on an input context (e.g. a text document) and generate a query relevant to that context, or condition the QGen additionally on the relevance label (e.g. relevant vs irrelevant) to generate queries across relevance buckets. However, we find that such QGen approaches are sub-optimal as they require the model to reason about the desired label and the input from a handful of examples. In this work, we propose to reduce this burden of LLMs by generating queries simultaneously for different labels. We hypothesize that instead of asking the model to generate, say, an irrelevant query given an input context, asking the model to generate an irrelevant query relative to a relevant query is a much simpler task. Extensive experimentation across nine IR datasets shows that synthetic queries generated in such a fashion translates to better downstream performance.</abstract>
      <url hash="90485f6c">2024.findings-naacl.107</url>
      <bibkey>chaudhary-etal-2024-relative</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.107</doi>
    </paper>
    <paper id="108">
      <title><fixed-case>RS</fixed-case>-<fixed-case>DPO</fixed-case>: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models</title>
      <author><first>Saeed</first><last>Khaki</last><affiliation>Amazon</affiliation></author>
      <author><first>JinJin</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Lan</first><last>Ma</last><affiliation>Amazon</affiliation></author>
      <author><first>Liu</first><last>Yang</last></author>
      <author><first>Prathap</first><last>Ramachandra</last><affiliation>Amazon</affiliation></author>
      <pages>1665-1680</pages>
      <abstract>Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO often relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.</abstract>
      <url hash="60e8b520">2024.findings-naacl.108</url>
      <bibkey>khaki-etal-2024-rs</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.108</doi>
    </paper>
    <paper id="109">
      <title>Hypernetwork-Assisted Parameter-Efficient Fine-Tuning with Meta-Knowledge Distillation for Domain Knowledge Disentanglement</title>
      <author><first>Changqun</first><last>Li</last></author>
      <author><first>Linlin</first><last>Wang</last></author>
      <author><first>Xin</first><last>Lin</last></author>
      <author><first>Shizhou</first><last>Huang</last></author>
      <author><first>Liang</first><last>He</last></author>
      <pages>1681-1695</pages>
      <abstract>Domain adaptation from labeled source domains to the target domain is important in practical summarization scenarios. However, the key challenge is domain knowledge disentanglement. In this work, we explore how to disentangle domain-invariant knowledge from source domains while learning specific knowledge of the target domain. Specifically, we propose a hypernetwork-assisted encoder-decoder architecture with parameter-efficient fine-tuning. It leverages a hypernetwork instruction learning module to generate domain-specific parameters from the encoded inputs accompanied by task-related instruction. Further, to better disentangle and transfer knowledge from source domains to the target domain, we introduce a meta-knowledge distillation strategy to build a meta-teacher model that captures domain-invariant knowledge across multiple domains and use it to transfer knowledge to students. Experiments on three dialogue summarization datasets show the effectiveness of the proposed model. Human evaluations also show the superiority of our model with regard to the summary generation quality.</abstract>
      <url hash="c0acbe10">2024.findings-naacl.109</url>
      <bibkey>li-etal-2024-hypernetwork</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.109</doi>
    </paper>
    <paper id="110">
      <title><fixed-case>MIC</fixed-case>o: Preventative Detoxification of Large Language Models through Inhibition Control</title>
      <author><first>Roy</first><last>Siegelmann</last><affiliation>Whiting School of Engineering</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Palash</first><last>Goyal</last><affiliation>Amazon</affiliation></author>
      <author><first>Prasoon</first><last>Goyal</last><affiliation>Amazon</affiliation></author>
      <author><first>Lisa</first><last>Bauer</last><affiliation>Amazon</affiliation></author>
      <author><first>Jwala</first><last>Dhamala</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <author><first>Reza</first><last>Ghanadan</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>1696-1703</pages>
      <abstract>Large Language Models (LLMs) are powerful tools which have been both dominant and commonplace in the field of Artificial Intelligence. Yet, LLMs have a tendency to devolve into toxic degeneration, wherein otherwise safe and unproblematic models begin generating toxic content. For the sake of social responsibility and inspired by the biological mechanisms of inhibition control, we introduce the paradigm of Education for Societal Norms (ESN). By collecting and labeling examples as acceptable and unacceptable (in this case toxic and non-toxic), and including a corresponding acceptable rewrite with every unacceptable example, we introduce a new mechanism for LLM detoxification. We annotate a dataset of 2,850 entries and use it to fine-tune a model, which we call a Model with Inhibition Control (MICo). Evaluating this model on toxicity detection capability, rewrite detoxification, meaning preservation, and overall toxicity reduction, we discover significant improvements over the baseline model. In our experiments we show that overall toxicity of this model is more than 60% reduced, with over 75% reduction in severe toxicity.</abstract>
      <url hash="d7c22c49">2024.findings-naacl.110</url>
      <bibkey>siegelmann-etal-2024-mico</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.110</doi>
    </paper>
    <paper id="111">
      <title>Reinforcement Learning with Token-level Feedback for Controllable Text Generation</title>
      <author><first>Wendi</first><last>Li</last></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Kaihe</first><last>Xu</last></author>
      <author><first>Wenfeng</first><last>Xie</last></author>
      <author><first>Dangyang</first><last>Chen</last><affiliation>Pingan Technology</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>1704-1719</pages>
      <abstract>To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a “first-quantize-then-noise” paradigm to enhance the robustness of the RL algorithm. Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our algorithm can achieve superior performance on both single-attribute and multi-attribute control tasks. We have released our codes at https://github.com/WindyLee0822/CTG.</abstract>
      <url hash="facc7d83">2024.findings-naacl.111</url>
      <bibkey>li-etal-2024-reinforcement</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.111</doi>
    </paper>
    <paper id="112">
      <title><fixed-case>C</fixed-case>o<fixed-case>MM</fixed-case>: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex Problem Solving</title>
      <author><first>Pei</first><last>Chen</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Shuai</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Boran</first><last>Han</last></author>
      <pages>1720-1738</pages>
      <abstract>Large Language Models (LLMs) have shown great ability in solving traditional natural language tasks and elementary reasoning tasks with appropriate prompting techniques. However, their ability is still limited in solving complicated science problems. In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.</abstract>
      <url hash="7d0727e8">2024.findings-naacl.112</url>
      <bibkey>chen-etal-2024-comm</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.112</doi>
    </paper>
    <paper id="113">
      <title>Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies</title>
      <author><first>Anaelia</first><last>Ovalle</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Ninareh</first><last>Mehrabi</last><affiliation>Amazon</affiliation></author>
      <author><first>Palash</first><last>Goyal</last><affiliation>Amazon</affiliation></author>
      <author><first>Jwala</first><last>Dhamala</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Richard</first><last>Zemel</last><affiliation>Department of Computer Science, Columbia University and Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Yuval</first><last>Pinter</last><affiliation>Ben-Gurion University of the Negev</affiliation></author>
      <author><first>Rahul</first><last>Gupta</last></author>
      <pages>1739-1756</pages>
      <abstract>Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae). While data scarcity is a known culprit, the precise mechanisms through which scarcity affects this behavior remain underexplored. We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLMs. Unlike binary pronouns, BPE overfragments neopronouns, a direct consequence of data scarcity during tokenizer training. This disparate tokenization mirrors tokenizer limitations observed in multilingual and low-resource NLP, unlocking new misgendering mitigation strategies. We propose two techniques: (1) pronoun tokenization parity, a method to enforce consistent tokenization across gendered pronouns, and (2) utilizing pre-existing LLM pronoun knowledge to improve neopronoun proficiency. Our proposed methods outperform finetuning with standard BPE, improving neopronoun accuracy from 14.1% to 58.4%. Our paper is the first to link LLM misgendering to tokenization and deficient neopronoun grammar, indicating that LLMs unable to correctly treat neopronouns as pronouns are more prone to misgender.</abstract>
      <url hash="cd543617">2024.findings-naacl.113</url>
      <bibkey>ovalle-etal-2024-tokenization</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.113</doi>
    </paper>
    <paper id="114">
      <title><fixed-case>A</fixed-case>da<fixed-case>PT</fixed-case>: A Set of Guidelines for Hyperbolic Multimodal Multilingual <fixed-case>NLP</fixed-case></title>
      <author><first>Ramit</first><last>Sawhney</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Shrey</first><last>Pandit</last></author>
      <author><first>Vishwa</first><last>Shah</last></author>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>1757-1771</pages>
      <abstract>The Euclidean space is the familiar space for training neural models and performing arithmetic operations.However, many data types inherently possess complex geometries, and model training methods involve operating over their latent representations, which cannot be effectively captured in the Euclidean space.The hyperbolic space provides a more generalized representative geometry to model the hierarchical complexities of the tree-like structure of natural language.We propose AdaPT a set of guidelines for initialization, parametrization, and training of neural networks, which adapts to the dataset and can be used with different manifolds. AdaPT can be generalized over any existing neural network training methodology and leads to more stable training without a substantial increase in training time.We apply AdaPT guidelines over two state-of-the-art deep learning approaches and empirically demonstrate its effectiveness through experiments on three tasks over 12 languages across speech and text.Through extensive qualitative analysis, we put forward the applicability of AdaPT as a set of guidelines optimally utilizing the manifold geometry, which can be extended to various downstream tasks across languages and modalities.</abstract>
      <url hash="d8a65cba">2024.findings-naacl.114</url>
      <bibkey>sawhney-etal-2024-adapt</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.114</doi>
    </paper>
    <paper id="115">
      <title>More Samples or More Prompts? Exploring Effective Few-Shot In-Context Learning for <fixed-case>LLM</fixed-case>s with In-Context Sampling</title>
      <author><first>Bingsheng</first><last>Yao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Guiming</first><last>Chen</last></author>
      <author><first>Ruishi</first><last>Zou</last></author>
      <author><first>Yuxuan</first><last>Lu</last></author>
      <author><first>Jiachen</first><last>Li</last></author>
      <author><first>Shao</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yisi</first><last>Sang</last><affiliation>Apple</affiliation></author>
      <author><first>Sijia</first><last>Liu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>James</first><last>Hendler</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Dakuo</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <pages>1772-1790</pages>
      <abstract>While most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM’s performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs’ performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM’s performance, which sheds light on a new yet promising future research direction.</abstract>
      <url hash="a8f45523">2024.findings-naacl.115</url>
      <bibkey>yao-etal-2024-samples</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.115</doi>
    </paper>
    <paper id="116">
      <title><fixed-case>ZSEE</fixed-case>: A Dataset based on Zeolite Synthesis Event Extraction for Automated Synthesis Platform</title>
      <author><first>Song</first><last>He</last></author>
      <author><first>Xin</first><last>Peng</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Yihan</first><last>Cai</last><affiliation>Shanghai Research Institute of Petrochemical Technology, Sinopec Corporation</affiliation></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Zhiqing</first><last>Yuan</last></author>
      <author><first>WenLi</first><last>Du</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Weimin</first><last>Yang</last><affiliation>East China University of Science and Technology</affiliation></author>
      <pages>1791-1808</pages>
      <abstract>Automated synthesis of zeolite, one of the most important catalysts in chemical industries, holds great significance for attaining economic and environmental benefits. Structural synthesis data extracted through NLP technologies from zeolite experimental procedures can significantly expedite automated synthesis owing to its machine readability. However, the utilization of NLP technologies in information extraction of zeolite synthesis remains restricted due to the lack of annotated datasets. In this paper, we formulate an event extraction task to mine structural synthesis actions from experimental narratives for modular automated synthesis. Furthermore, we introduce ZSEE, a novel dataset containing fine-grained event annotations of zeolite synthesis actions. Our dataset features 16 event types and 13 argument roles which cover all the experimental operational steps of zeolite synthesis. We explore current state-of-the-art event extraction methods on ZSEE, perform error analysis based on the experimental results, and summarize the challenges and corresponding research directions to further facilitate the automated synthesis of zeolites. The code is publicly available at https://github.com/Hi-0317/ZSEE.</abstract>
      <url hash="40bc291f">2024.findings-naacl.116</url>
      <bibkey>he-etal-2024-zsee</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.116</doi>
    </paper>
    <paper id="117">
      <title>Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information</title>
      <author><first>Kyubyung</first><last>Chae</last></author>
      <author><first>Jaepill</first><last>Choi</last></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Taesup</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>1809-1820</pages>
      <abstract>A primary challenge in abstractive summarization is hallucination—the phenomenon where a model generates plausible text that is absent in the source text. We hypothesize that the domain (or topic) of the source text triggers the model to generate text that is highly probable in the domain, neglecting the details of the source text. To alleviate this model bias, we introduce a decoding strategy based on domain-conditional pointwise mutual information. This strategy adjusts the generation probability of each token by comparing it with the token’s marginal probability within the domain of the source text. According to evaluation on the XSUM dataset, our method demonstrates improvement in terms of faithfulness and source relevance.</abstract>
      <url hash="7d981d93">2024.findings-naacl.117</url>
      <bibkey>chae-etal-2024-mitigating</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.117</doi>
    </paper>
    <paper id="118">
      <title>Adversarial <fixed-case>DPO</fixed-case>: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents</title>
      <author><first>San</first><last>Kim</last><affiliation>POSTECH</affiliation></author>
      <author><first>Gary</first><last>Lee</last></author>
      <pages>1821-1835</pages>
      <abstract>Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies. Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience. In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token. We demonstrate that ADPO enhances the model’s resilience against harmful conversations while minimizing performance degradation. Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO. To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data.</abstract>
      <url hash="9a730e0b">2024.findings-naacl.118</url>
      <bibkey>kim-lee-2024-adversarial</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.118</doi>
    </paper>
    <paper id="119">
      <title>Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models</title>
      <author><first>Fobo</first><last>Shi</last></author>
      <author><first>Peijun</first><last>Qing</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Dong</first><last>Yang</last></author>
      <author><first>Nan</first><last>Wang</last></author>
      <author><first>Youbo</first><last>Lei</last></author>
      <author><first>Haonan</first><last>Lu</last><affiliation>OPPO Guangdong Mobile Telecommunications Co., Ltd.</affiliation></author>
      <author><first>Xiaodong</first><last>Lin</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Duantengchuan</first><last>Li</last></author>
      <pages>1836-1862</pages>
      <abstract>Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt “Let’s think step by step”, Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and effective mathematical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs. Our code is publicly available at https://github.com/YouBLEI/Prompt-Space</abstract>
      <url hash="e1214822">2024.findings-naacl.119</url>
      <bibkey>shi-etal-2024-prompt</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.119</doi>
    </paper>
    <paper id="120">
      <title><fixed-case>DAGCN</fixed-case>: Distance-based and Aspect-oriented Graph Convolutional Network for Aspect-based Sentiment Analysis</title>
      <author><first>Zhihao</first><last>Wang</last></author>
      <author><first>Bo</first><last>Zhang</last><affiliation>Shanghai Normal University</affiliation></author>
      <author><first>Ru</first><last>Yang</last><affiliation>Shanghai Normal University</affiliation></author>
      <author><first>Chang</first><last>Guo</last><affiliation>Shanghai Normal University</affiliation></author>
      <author><first>Maozhen</first><last>Li</last><affiliation>Brunel University Uxbridge</affiliation></author>
      <pages>1863-1876</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) is a task that aims to determine the sentiment polarity of aspects by identifying opinion words. Recent advancements have predominantly been rooted either in semantic or syntactic methods. However, both of them tend to interference from local factors such as irrelevant words and edges, hindering the precise identification of opinion words. In this paper, we present Distance-based and Aspect-oriented Graph Convolutional Network (DAGCN) to address the aforementioned issue. Firstly, we introduce the Distance-based Syntactic Weight (DSW). It focuses on the local scope of aspects in the pruned dependency trees, thereby reducing the candidate pool of opinion words. Additionally, we propose Aspect-Fusion Attention (AF) to further filter opinion words within the local context and consider cases where opinion words are distant from the aspect. With the combination of DSW and AF, we achieve precise identification of corresponding opinion words. Extensive experiments on three public datasets demonstrate that the proposed model outperforms state-of-the-art models and verify the effectiveness of the proposed architecture.</abstract>
      <url hash="11f48da0">2024.findings-naacl.120</url>
      <bibkey>wang-etal-2024-dagcn</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.120</doi>
    </paper>
    <paper id="121">
      <title>Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs</title>
      <author><first>Zhuoyi</first><last>Peng</last></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>1877-1890</pages>
      <abstract>We study the patent phrase similarity inference task, which measures the semantic similarity between two patent phrases. As patent documents employ legal and highly technical language, existing semantic textual similarity methods that use localized contextual information do not perform satisfactorily in inferring patent phrase similarity. To address this, we introduce a graph-augmented approach to amplify the global contextual information of the patent phrases. For each patent phrase, we construct a phrase graph that links to its focal patents and a list of patents that are either cited by or cite these focal patents. The augmented phrase embedding is then derived from combining its localized contextual embedding with its global embedding within the phrase graph. We further propose a self-supervised learning objective that capitalizes on the retrieved topology to refine both the contextualized embedding and the graph parameters in an end-to-end manner. Experimental results from a unique patent phrase similarity dataset demonstrate that our approach significantly enhances the representation of patent phrases, resulting in marked improvements in similarity inference in a self-supervised fashion. Substantial improvements are also observed in the supervised setting, underscoring the potential benefits of leveraging retrieved phrase graph augmentation.</abstract>
      <url hash="e75a36f9">2024.findings-naacl.121</url>
      <bibkey>peng-yang-2024-connecting</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.121</doi>
    </paper>
    <paper id="122">
      <title>Self-Regulated Sample Diversity in Large Language Models</title>
      <author><first>Mingyue</first><last>Liu</last></author>
      <author><first>Jonathan</first><last>Frawley</last><affiliation>Durham University</affiliation></author>
      <author><first>Sarah</first><last>Wyer</last></author>
      <author><first>Hubert P. H.</first><last>Shum</last><affiliation>Durham University</affiliation></author>
      <author><first>Sara</first><last>Uckelman</last><affiliation>Durham University</affiliation></author>
      <author><first>Sue</first><last>Black</last><affiliation>Durham University</affiliation></author>
      <author><first>Chris</first><last>Willcocks</last><affiliation>Durham University and Durham University</affiliation></author>
      <pages>1891-1899</pages>
      <abstract>Sample diversity depends on the task; within mathematics, precision and determinism are paramount, while storytelling thrives on creativity and surprise. This paper presents a simple self-regulating approach where we adjust sample diversity inference parameters dynamically based on the input prompt—in contrast to existing methods that require expensive and inflexible setups, or maintain static values during inference. Capturing a broad spectrum of sample diversities can be formulated as a straightforward self-supervised inference task, which we find significantly improves the quality of responses generically without model retraining or fine-tuning. In particular, our method demonstrates significant improvement in all supercategories of the MMLU multitask benchmark (GPT-3.5: <tex-math>+4.4\%</tex-math>, GPT-4: <tex-math>+1.5\%</tex-math>), which captures a large variety of difficult tasks covering STEM, the humanities and social sciences.</abstract>
      <url hash="a9cdc0b4">2024.findings-naacl.122</url>
      <bibkey>liu-etal-2024-self-regulated</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.122</doi>
    </paper>
    <paper id="123">
      <title>Methods, Applications, and Directions of Learning-to-Rank in <fixed-case>NLP</fixed-case> Research</title>
      <author><first>Justin</first><last>Lee</last></author>
      <author><first>Gabriel</first><last>Bernier-Colborne</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Tegan</first><last>Maharaj</last><affiliation>Toronto University and Ecole Polytechnique de Montreal</affiliation></author>
      <author><first>Sowmya</first><last>Vajjala</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>1900-1917</pages>
      <abstract>Learning-to-rank (LTR) algorithms aim to order a set of items according to some criteria. They are at the core of applications such as web search and social media recommendations, and are an area of rapidly increasing interest, with the rise of large language models (LLMs) and the widespread impact of these technologies on society. In this paper, we survey the diverse use cases of LTR methods in natural language processing (NLP) research, looking at previously under-studied aspects such as multilingualism in LTR applications and statistical significance testing for LTR problems. We also consider how large language models are changing the LTR landscape. This survey is aimed at NLP researchers and practitioners interested in understanding the formalisms and best practices regarding the application of LTR approaches in their research.</abstract>
      <url hash="8b3e9dc9">2024.findings-naacl.123</url>
      <bibkey>lee-etal-2024-methods</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.123</doi>
    </paper>
    <paper id="124">
      <title>When Quantization Affects Confidence of Large Language Models?</title>
      <author><first>Irina</first><last>Proskurina</last></author>
      <author><first>Luc</first><last>Brun</last><affiliation>Laboratoire ERIC, Université Lumiére (Lyon II)</affiliation></author>
      <author><first>Guillaume</first><last>Metzler</last><affiliation>Université Lumiére (Lyon II)</affiliation></author>
      <author><first>Julien</first><last>Velcin</last><affiliation>ERIC</affiliation></author>
      <pages>1918-1928</pages>
      <abstract>Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation. Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs.This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss.Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models. Secondly, we observe fluctuations in the impact on confidence across different scales. Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place.We make our code and quantized models publicly available.</abstract>
      <url hash="36dd53da">2024.findings-naacl.124</url>
      <bibkey>proskurina-etal-2024-quantization</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.124</doi>
    </paper>
    <paper id="125">
      <title><fixed-case>M</fixed-case>ed<fixed-case>C</fixed-case>ycle: Unpaired Medical Report Generation via Cycle-Consistency</title>
      <author><first>Elad</first><last>Hirsch</last><affiliation>Technion, Technion</affiliation></author>
      <author><first>Gefen</first><last>Dawidowicz</last></author>
      <author><first>Ayellet</first><last>Tal</last><affiliation>Technion and Technion</affiliation></author>
      <pages>1929-1944</pages>
      <abstract>Generating medical reports for X-ray images presents a significant challenge, particularly in unpaired scenarios where access to paired image-report data for training is unavailable. Previous works have typically learned a joint embedding space for images and reports, necessitating a specific labeling schema for both. We introduce an innovative approach that eliminates the need for consistent labeling schemas, thereby enhancing data accessibility and enabling the use of incompatible datasets. This approach is based on cycle-consistent mapping functions that transform image embeddings into report embeddings, coupled with report auto encoding for medical report generation. Our model and objectives consider intricate local details and the overarching semantic context within images and reports. This approach facilitates the learning of effective mapping functions, resulting in the generation of coherent reports. It outperforms state-of-the-art results in unpaired chest X-ray report generation, demonstrating improvements in both language and clinical metrics.</abstract>
      <url hash="1ddb5636">2024.findings-naacl.125</url>
      <bibkey>hirsch-etal-2024-medcycle</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.125</doi>
    </paper>
    <paper id="126">
      <title>Beta-<fixed-case>LR</fixed-case>: Interpretable Logical Reasoning based on Beta Distribution</title>
      <author><first>Yizhuo</first><last>Ma</last></author>
      <author><first>Ke</first><last>Qin</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Shuang</first><last>Liang</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <pages>1945-1955</pages>
      <abstract>The logical information contained in text isof significant importance for logical reasoning.Previous approaches have relied on embeddingtext into a low-dimensional vector to capturelogical information and perform reasoning inEuclidean space. These methods involve constructing special graph architectures that matchlogical relations or designing data augmentation frameworks by extending texts based onsymbolic logic. However, it presents two obvious problems. 1) The logical informationreflected in the text exhibits uncertainty that isdifficult to represent using a vector. 2) Integrating logical information requires modeling logical operations (such as ∪, ∩, and ¬), while onlysimple arithmetic operations can be performedin Euclidean space. To address both the problems, we propose Beta-LR, a probabilistic embedding method to capture logical information.Specifically, we embed texts into beta distribution on each dimension to eliminate logical uncertainty. We also define neural operators thatenable interpretability and perform logical operations based on the characteristics of the betadistribution. We conduct experiments on twodatasets, ReClor and LogiQA, and our Beta-LRachieves competitive results. The experimentsdemonstrate that our method effectively captures the logical information in text for reasoning purposes. The source code is available athttps://github.com/myz12138/Beta-LR.</abstract>
      <url hash="defd2cb0">2024.findings-naacl.126</url>
      <bibkey>ma-etal-2024-beta</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.126</doi>
    </paper>
    <paper id="127">
      <title>Applications of <fixed-case>BERT</fixed-case> Models Towards Automation of Clinical Coding in <fixed-case>I</fixed-case>celandic</title>
      <author><first>Haraldur Orri</first><last>Hauksson</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Hafsteinn</first><last>Einarsson</last><affiliation>deCODE genetics and University of Iceland</affiliation></author>
      <pages>1956-1967</pages>
      <abstract>This study explores the potential of automating clinical coding in Icelandic, a language with limited digital resources, by leveraging over 25 years of electronic health records (EHR) from the Landspitali University Hospital. Traditionally a manual and error-prone task, clinical coding is essential for patient care, billing, and research. Our research delves into the effectiveness of Transformer-based models in automating this process. We investigate various model training strategies, including continued pretraining and model adaptation, under a constrained computational budget. Our findings reveal that the best-performing model achieves competitive results in both micro and macro F1 scores, with label attention contributing significantly to its success. The study also explores the possibility of training on unlabeled data. Our research provides valuable insights into the possibilities of using NLP for clinical coding in low-resource languages, demonstrating that small countries with unique languages and well-segmented healthcare records can achieve results comparable to those in higher-resourced languages.</abstract>
      <url hash="c3799806">2024.findings-naacl.127</url>
      <bibkey>hauksson-einarsson-2024-applications</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.127</doi>
    </paper>
    <paper id="128">
      <title>“Tell me who you are and <fixed-case>I</fixed-case> tell you how you argue”: Predicting Stances and Arguments for Stakeholder Groups</title>
      <author><first>Philipp</first><last>Heinisch</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Lorik</first><last>Dumani</last><affiliation>Trier University</affiliation></author>
      <author><first>Philipp</first><last>Cimiano</last><affiliation>Bielefeld University</affiliation></author>
      <author><first>Ralf</first><last>Schenkel</last><affiliation>Trier University</affiliation></author>
      <pages>1968-1982</pages>
      <abstract>Argument mining has focused so far mainly on the identification, extraction, and formalization of arguments. An important yet unaddressedtask consists in the prediction of the argumentative behavior of stakeholders in a debate. Predicting the argumentative behavior in advance can support foreseeing issues in public policy making or help recognize potential disagreements early on and help to resolve them. In this paper, we consider the novel task of predicting the argumentative behavior of individual stakeholders. We present ARGENST, a framework that relies on a recommender-based architecture to predict the stance and the argumentative main point on a specific controversial topic for a given stakeholder, which is described in terms of a profile including properties related to demographic attributes, religious and political orientation, socio-economic background, etc. We evaluate our approach on the well-known debate.org dataset in terms of accuracy for predicting stance as well as in terms of similarity of the generated arguments to the ground truth arguments using BERTScore. As part of a case study, we show how juries of members representing different stakeholder groups and perspectives can be assembled to simulate the public opinion on a given topic.</abstract>
      <url hash="584f0bc7">2024.findings-naacl.128</url>
      <bibkey>heinisch-etal-2024-tell</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.128</doi>
    </paper>
    <paper id="129">
      <title>Psychometric Predictive Power of Large Language Models</title>
      <author><first>Tatsuki</first><last>Kuribayashi</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>1983-2005</pages>
      <abstract>Instruction tuning aligns the response of large language models (LLMs) with human preferences.Despite such efforts in human–LLM alignment, we find that instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs.In addition, we explore prompting methodologies for simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve psychometric predictive power, but are still inferior to small base models.These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, pure next-word probability remains a strong predictor for human reading behavior, even in the age of LLMs.</abstract>
      <url hash="288b3653">2024.findings-naacl.129</url>
      <bibkey>kuribayashi-etal-2024-psychometric</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.129</doi>
    </paper>
    <paper id="130">
      <title>Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</title>
      <author><first>Pouya</first><last>Pezeshkpour</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>2006-2017</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions—commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 85% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model’s bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs’ predictions, leading to up to 8 percentage points improvement across different models and benchmarks.</abstract>
      <url hash="e3fccdbc">2024.findings-naacl.130</url>
      <bibkey>pezeshkpour-hruschka-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.130</doi>
    </paper>
    <paper id="131">
      <title><fixed-case>PEEB</fixed-case>: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck</title>
      <author><first>Thang</first><last>Pham</last></author>
      <author><first>Peijie</first><last>Chen</last></author>
      <author><first>Tin</first><last>Nguyen</last></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Trung</first><last>Bui</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Anh</first><last>Nguyen</last><affiliation>Auburn University</affiliation></author>
      <pages>2018-2053</pages>
      <abstract>CLIP-based classifiers rely on the prompt containing a class name that is known to the text encoder. Therefore, they perform poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB – an explainable and editable classifier to (1) express the class name into a set of text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a huge margin (∼10× in top-1 accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art (SOTA) on the supervised-learning setting (88.80% and 92.20% accuracy on CUB-200 and Stanford Dogs-120, respectively) but also the first to enable users to edit the text descriptors to form a new classifier without any re-training. Compared to concept bottleneck models, PEEB is also the SOTA in both zero-shot and supervised-learning settings.</abstract>
      <url hash="a543ea50">2024.findings-naacl.131</url>
      <bibkey>pham-etal-2024-peeb</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.131</doi>
    </paper>
    <paper id="132">
      <title>Ethos: Rectifying Language Models in Orthogonal Parameter Space</title>
      <author><first>Lei</first><last>Gao</last></author>
      <author><first>Yue</first><last>Niu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Tingting</first><last>Tang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Salman</first><last>Avestimehr</last><affiliation>University of Southern California, University of Southern California and University of Southern California</affiliation></author>
      <author><first>Murali</first><last>Annavaram</last><affiliation>University of Southern California</affiliation></author>
      <pages>2054-2068</pages>
      <abstract>Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos separates the principal components that encode general from those associated with undesired knowledge. Ethos performs forgetting or unlearning by only negating the task vector with undesired knowledge, thereby minimizing collateral damage on general model utility. We demonstrate the efficacy of our approach on three different tasks: bias, toxicity, and memorization unlearning. Evaluations show Ethos is more effective in removing undesired knowledge while maintaining the overall model performance compared to current task arithmetic methods.</abstract>
      <url hash="2b5f6a42">2024.findings-naacl.132</url>
      <bibkey>gao-etal-2024-ethos</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.132</doi>
    </paper>
    <paper id="133">
      <title>Crafting In-context Examples according to <fixed-case>LM</fixed-case>s’ Parametric Knowledge</title>
      <author><first>Yoonsang</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Pranav</first><last>Atreya</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Xi</first><last>Ye</last></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>University of Texas, Austin</affiliation></author>
      <pages>2069-2085</pages>
      <abstract>In-context learning can improve the performances of knowledge-rich tasks such as question answering. In such scenarios, in-context examples trigger a language model (LM) to surface information stored in its parametric knowledge. We study how to better construct in-context example sets, based on whether the model is aware of the in-context examples. We identify ‘known’ examples, where models can correctly answer from their parametric knowledge, and ‘unknown’ ones. Our experiments show that prompting with ‘unknown’ examples decreases the performance, potentially as it encourages hallucination rather than searching for its parametric knowledge. Constructing an in-context example set that presents both known and unknown information performs the best across diverse settings. We perform analysis on three multi-answer question answering datasets, which allows us to further study answer set ordering strategies based on the LM’s knowledge of each answer. Together, our study sheds light on how to best construct in-context example sets for knowledge-rich tasks.</abstract>
      <url hash="1f1f9189">2024.findings-naacl.133</url>
      <bibkey>lee-etal-2024-crafting</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.133</doi>
    </paper>
    <paper id="134">
      <title><fixed-case>ICXML</fixed-case>: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification</title>
      <author><first>Yaxin</first><last>Zhu</last></author>
      <author><first>Hamed</first><last>Zamani</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>2086-2098</pages>
      <abstract>This paper focuses on the task of Extreme Multi-Label Classification (XMC) whose goal is to predict multiple labels for each instance from an extremely large label space. While existing research has primarily focused on fully supervised XMC, real-world scenarios often lack supervision signals, highlighting the importance of zero-shot settings. Given the large label space, utilizing in-context learning approaches is not trivial. We address this issue by introducing In-Context Extreme Multi-label Learning (ICXML), a two-stage framework that cuts down the search space by generating a set of candidate labels through in-context learning and then reranks them. Extensive experiments suggest that ICXML advances the state of the art on two diverse public benchmarks.</abstract>
      <url hash="c6ba079f">2024.findings-naacl.134</url>
      <bibkey>zhu-zamani-2024-icxml</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.134</doi>
    </paper>
    <paper id="135">
      <title><fixed-case>CLGSI</fixed-case>: A Multimodal Sentiment Analysis Framework based on Contrastive Learning Guided by Sentiment Intensity</title>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Xunde</first><last>Dong</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Yupeng</first><last>Qiang</last></author>
      <pages>2099-2110</pages>
      <abstract>Recently, contrastive learning has begun to gain popularity in multimodal sentiment analysis (MSA). However, most of existing MSA methods based on contrastive learning lacks more detailed learning of the distribution of sample pairs with different sentiment intensity differences in the contrastive learning representation space. In addition, limited research has been conducted on the fusion of each modality representation obtained by contrastive learning training.In this paper, we propose a novel framework for multimodal sentiment analysis based on Contrastive Learning Guided by Sentiment Intensity (CLGSI). Firstly, the proposed contrastive learning guided by sentiment intensity selects positive and negative sample pairs based on the difference in sentiment intensity and assigns corresponding weights accordingly.Subsequently, we propose a new multimodal representation fusion mechanism, called Global-Local-Fine-Knowledge (GLFK), which extracts common features between different modalities’ representations. At the same time, each unimodal encoder output is separately processed by a Multilayer Perceptron (MLP) to extract specific features of each modality. Finally, joint learning of the common and specific features is used to predict sentiment intensity. The effectiveness of CLGSI is assessed on two English datasets, MOSI and MOSEI, as well as one Chinese dataset, SIMS. We achieve competitive experimental results, which attest to the strong generalization performance of our approach. The code for our approach will be released in https://github.com/AZYoung233/CLGSI</abstract>
      <url hash="96195c10">2024.findings-naacl.135</url>
      <bibkey>yang-etal-2024-clgsi</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.135</doi>
    </paper>
    <paper id="136">
      <title>Interpreting Answers to Yes-No Questions in Dialogues from Multiple Domains</title>
      <author><first>Zijie</first><last>Wang</last></author>
      <author><first>Farzana</first><last>Rashid</last></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <pages>2111-2128</pages>
      <abstract>People often answer yes-no questions without explicitly saying yes, no, or similar polar key-words. Figuring out the meaning of indirectanswers is challenging, even for large language models. In this paper, we investigate this problem working with dialogues from multiple domains. We present new benchmarks in three diverse domains: movie scripts, tennis interviews, and airline customer service. We present an approach grounded on distant supervision and blended training to quickly adapt to a new dialogue domain. Experimental results show that our approach is never detrimental and yields F1 improvements as high as 11-34%.</abstract>
      <url hash="1fd16b33">2024.findings-naacl.136</url>
      <bibkey>wang-etal-2024-interpreting</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.136</doi>
    </paper>
    <paper id="137">
      <title>Enhancing Perception: Refining Explanations of News Claims with <fixed-case>LLM</fixed-case> Conversations</title>
      <author><first>Yi-Li</first><last>Hsu</last><affiliation>National Tsinghua University and Academia Sinica</affiliation></author>
      <author><first>Jui-Ning</first><last>Chen</last><affiliation>National Taiwan University and , Academia Sinica</affiliation></author>
      <author><first>Yang</first><last>Fan Chiang</last><affiliation>Academia Sinica</affiliation></author>
      <author><first>Shang-Chien</first><last>Liu</last></author>
      <author><first>Aiping</first><last>Xiong</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Lun-Wei</first><last>Ku</last><affiliation>Academia Sinica</affiliation></author>
      <pages>2129-2147</pages>
      <abstract>We introduce Enhancing Perception, a framework for Large Language Models (LLMs) designed to streamline the time-intensive task typically undertaken by professional fact-checkers of crafting explanations for fake news. This study investigates the effectiveness of enhancing LLM explanations through conversational refinement. We compare various questioner agents, including state-of-the-art LLMs like GPT-4, Claude 2, PaLM 2, and 193 American participants acting as human questioners. Based on the histories of these refinement conversations, we further generate comprehensive summary explanations. We evaluated the effectiveness of these initial, refined, and summary explanations across 40 news claims by involving 2,797 American participants, measuring their self-reported belief change regarding both real and fake claims after receiving the explanations. Our findings reveal that, in the context of fake news, explanations that have undergone conversational refinement—whether by GPT-4 or human questioners, who ask more diverse and detail-oriented questions—were significantly more effective than both the initial unrefined explanations and the summary explanations. Moreover, these refined explanations achieved a level of effectiveness comparable to that of expert-written explanations. The results highlight the potential of automatic explanation refinement by LLMs in debunking fake news claims.</abstract>
      <url hash="9aa6291f">2024.findings-naacl.137</url>
      <bibkey>hsu-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.137</doi>
    </paper>
    <paper id="138">
      <title>How Interpretable are Reasoning Explanations from Prompting Large Language Models?</title>
      <author><first>Yeo</first><last>Wei Jie</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Ranjan</first><last>Satapathy</last></author>
      <author><first>Rick</first><last>Goh</last><affiliation>Institute of High Performance Computing, Singapore, A*STAR</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>2148-2164</pages>
      <abstract>Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability</abstract>
      <url hash="79cf9440">2024.findings-naacl.138</url>
      <bibkey>wei-jie-etal-2024-interpretable</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.138</doi>
    </paper>
    <paper id="139">
      <title>Plug-in Language Model: Controlling Text Generation with a Simple Regression Model</title>
      <author><first>Nai-Chi</first><last>Yang</last></author>
      <author><first>Wei-Yun</first><last>Ma</last><affiliation>Academia Sinica</affiliation></author>
      <author><first>Pu-Jen</first><last>Cheng</last><affiliation>National Taiwan University</affiliation></author>
      <pages>2165-2181</pages>
      <abstract>Large-scale pre-trained language models have displayed unrivaled capacity in generating text that closely resembles human-written text. Nevertheless, generating texts adhering to specific conditions without fine-tuning or adding new parameters can be challenging. Contemporary approaches commonly rely on either prompts or auxiliary models to avoid modifying the language models. These auxiliary models are designed to assess whether a generated token contributes to meeting the desired requirements. These approaches adjust the distribution of the next token during the inference phase by leveraging the prediction score of the desired attribute to calculate gradients. However, these auxiliary models typically require the language model’s latent states. This prerequisite challenges integrating various existing black box attribute models or tools. We present the Plug-in Language Model (PiLM) as a solution to address the limitations. PiLM leverages reinforcement learning to utilize black box tools directly, adjusting the latent state to control text generation. However, performing backpropagation during the inference phase is time-consuming for PiLM. By replacing backpropagation with a simple regression model, PiLM can achieve an inference time comparable to that of the original LLM. Experiment results show that our approaches in this paper outperform existing state-of-the-art methods that rely on gradient-based, weighted decoding, or prompt-based methodologies.</abstract>
      <url hash="f05565d4">2024.findings-naacl.139</url>
      <bibkey>yang-etal-2024-plug</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.139</doi>
    </paper>
    <paper id="140">
      <title>Signer Diversity-driven Data Augmentation for Signer-Independent Sign Language Translation</title>
      <author><first>Honghao</first><last>Fu</last></author>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Biao</first><last>Fu</last></author>
      <author><first>Rui</first><last>Zhao</last></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Xiaodong</first><last>Shi</last><affiliation>Xiamen University, Tsinghua University</affiliation></author>
      <author><first>Yidong</first><last>Chen</last></author>
      <pages>2182-2193</pages>
      <abstract>The primary objective of sign language translation (SLT) is to transform sign language videos into natural sentences.A crucial challenge in this field is developing signer-independent SLT systems which requires models to generalize effectively to signers not encountered during training.This challenge is exacerbated by the limited diversity of signers in existing SLT datasets, which often results in suboptimal generalization capabilities of current models.Achieving robustness to unseen signers is essential for signer-independent SLT.However, most existing method relies on signer identity labels, which is often impractical and costly in real-world applications.To address this issue, we propose the Signer Diversity-driven Data Augmentation (SDDA) method that can achieve good generalization without relying on signer identity labels. SDDA comprises two data augmentation schemes. The first is data augmentation based on adversarial training, which aims to utilize the gradients of the model to generate adversarial examples. The second is data augmentation based on diffusion model, which focuses on using the advanced diffusion-based text guided image editing method to modify the appearances of the signer in images. The combination of the two strategies significantly enriches the diversity of signers in the training process.Moreover, we introduce a consistency loss and a discrimination loss to enhance the learning of signer-independent features.Our experimental results demonstrate our model significantly enhances the performance of SLT in the signer-independent setting, achieving state-of-the-art results without relying on signer identity labels.</abstract>
      <url hash="f99f4b3d">2024.findings-naacl.140</url>
      <bibkey>honghaofu-etal-2024-signer</bibkey>
      <revision id="1" href="2024.findings-naacl.140v1" hash="c343b2c4"/>
      <revision id="2" href="2024.findings-naacl.140v2" hash="f99f4b3d" date="2024-07-21">Added author affiliations.</revision>
      <doi>10.18653/v1/2024.findings-naacl.140</doi>
    </paper>
    <paper id="141">
      <title>A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation</title>
      <author><first>Francois</first><last>Meyer</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Jan</first><last>Buys</last><affiliation>University of Cape Town</affiliation></author>
      <pages>2194-2200</pages>
      <abstract>Multilingual modelling can improve machine translation for low-resource languages, partly through shared subword representations. This paper studies the role of subword segmentation in cross-lingual transfer. We systematically compare the efficacy of several subword methods in promoting synergy and preventing interference across different linguistic typologies. Our findings show that subword regularisation boosts synergy in multilingual modelling, whereas BPE more effectively facilitates transfer during cross-lingual fine-tuning. Notably, our results suggest that differences in orthographic word boundary conventions (the morphological granularity of written words) may impede cross-lingual transfer more significantly than linguistic unrelatedness. Our study confirms that decisions around subword modelling can be key to optimising the benefits of multilingual modelling.</abstract>
      <url hash="4a5bce41">2024.findings-naacl.141</url>
      <bibkey>meyer-buys-2024-systematic</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.141</doi>
    </paper>
    <paper id="142">
      <title>Multi-Granularity Guided Fusion-in-Decoder</title>
      <author><first>Eunseong</first><last>Choi</last></author>
      <author><first>Hyeri</first><last>Lee</last></author>
      <author><first>Jongwuk</first><last>Lee</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>2201-2212</pages>
      <abstract>In Open-domain Question Answering (ODQA), it is essential to discern relevant contexts as evidence and avoid spurious ones among retrieved results. The model architecture that uses concatenated multiple contexts in the decoding phase, *i.e.*, Fusion-in-Decoder, demonstrates promising performance but generates incorrect outputs from seemingly plausible contexts. To address this problem, we propose the ***M**ulti-**G**ranularity guided **F**usion-**i**n-**D**ecoder (**MGFiD**)*, discerning evidence across multiple levels of granularity. Based on multi-task learning, MGFiD harmonizes passage re-ranking with sentence classification. It aggregates evident sentences into an *anchor vector* that instructs the decoder. Additionally, it improves decoding efficiency by reusing the results of passage re-ranking for *passage pruning*. Through our experiments, MGFiD outperforms existing models on the Natural Questions (NQ) and TriviaQA (TQA) datasets, highlighting the benefits of its multi-granularity solution.</abstract>
      <url hash="684fc457">2024.findings-naacl.142</url>
      <bibkey>choi-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.142</doi>
    </paper>
    <paper id="143">
      <title>Group Fairness in Multilingual Speech Recognition Models</title>
      <author><first>Anna</first><last>Zee</last></author>
      <author><first>Marc</first><last>Zee</last><affiliation>Research, Google</affiliation></author>
      <author><first>Anders</first><last>Søgaard</last><affiliation>Copenhagen University</affiliation></author>
      <pages>2213-2226</pages>
      <abstract>We evaluate the performance disparity of the Whisper and MMS families of ASR models across the VoxPopuli and Common Voice multilingual datasets, with an eye toward intersectionality. Our two most important findings are that model size, surprisingly, correlates logarithmically with worst-case performance disparities, meaning that larger (and better) models are less fair. We also observe the importance of intersectionality. In particular, models often exhibit significant performance disparity across binary gender for adolescents.</abstract>
      <url hash="96ca57ea">2024.findings-naacl.143</url>
      <bibkey>zee-etal-2024-group</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.143</doi>
    </paper>
    <paper id="144">
      <title>Rethinking Machine Ethics – Can <fixed-case>LLM</fixed-case>s Perform Moral Reasoning through the Lens of Moral Theories?</title>
      <author><first>Jingyan</first><last>Zhou</last></author>
      <author><first>Minda</first><last>Hu</last></author>
      <author><first>Junan</first><last>Li</last></author>
      <author><first>Xiaoying</first><last>Zhang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xixin</first><last>Wu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Irwin</first><last>King</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Helen</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>2227-2242</pages>
      <abstract>Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for potentially overgeneralizing a limited group of annotators’ moral stances and lacking explainability. This work proposes a flexible top-down framework to steer (Large) Language Models to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore, we show the alignment between different moral theories and existing morality datasets. Our analysis exhibits the potential and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.</abstract>
      <url hash="2c393a5d">2024.findings-naacl.144</url>
      <bibkey>zhou-etal-2024-rethinking</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.144</doi>
    </paper>
    <paper id="145">
      <title>Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models</title>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yi</first><last>Chen</last></author>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Qi</first><last>Zhu</last></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>2243-2255</pages>
      <abstract>The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt. The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains.Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities.</abstract>
      <url hash="1030cded">2024.findings-naacl.145</url>
      <bibkey>wang-etal-2024-role</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.145</doi>
    </paper>
    <paper id="146">
      <title><fixed-case>BERT</fixed-case>weet’s <fixed-case>TACO</fixed-case> Fiesta: Contrasting Flavors On The Path Of Inference And Information-Driven Argument Mining On <fixed-case>T</fixed-case>witter</title>
      <author><first>Marc</first><last>Feger</last></author>
      <author><first>Stefan</first><last>Dietze</last><affiliation>GESIS and Heinrich-Heine-University Düsseldorf</affiliation></author>
      <pages>2256-2266</pages>
      <abstract>Argument mining, dealing with the classification of text based on inference and information, denotes a challenging analytical task in the rich context of Twitter (now <tex-math>\mathbb{X}</tex-math>), a key platform for online discourse and exchange. Thereby, Twitter offers a diverse repository of short messages bearing on both of these elements. For text classification, transformer approaches, particularly BERT, offer state-of-the-art solutions. Our study delves into optimizing the embeddings of the understudied BERTweet transformer for argument mining on Twitter and broader generalization across topics.We explore the impact of pre-classification fine-tuning by aligning similar manifestations of inference and information while contrasting dissimilar instances. Using the TACO dataset, our approach augments tweets for optimizing BERTweet in a Siamese network, strongly improving classification and cross-topic generalization compared to standard methods.Overall, we contribute the transformer WRAPresentations and classifier WRAP, scoring 86.62% F1 for inference detection, 86.30% for information recognition, and 75.29% across four combinations of these elements, to enhance inference and information-driven argument mining on Twitter.</abstract>
      <url hash="44ed6303">2024.findings-naacl.146</url>
      <bibkey>feger-dietze-2024-bertweets</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.146</doi>
    </paper>
    <paper id="147">
      <title>Testing the limits of logical reasoning in neural and hybrid models</title>
      <author><first>Manuel</first><last>Vargas Guzmán</last></author>
      <author><first>Jakub</first><last>Szymanik</last><affiliation>University of Trento</affiliation></author>
      <author><first>Maciej</first><last>Malicki</last></author>
      <pages>2267-2279</pages>
      <abstract>We study the ability of neural and hybrid models to generalize logical reasoning patterns. We created a series of tests for analyzing various aspects of generalization in the context of language and reasoning, focusing on compositionality and recursiveness. We used them to study the syllogistic logic in hybrid models, where the network assists in premise selection. We analyzed feed-forward, recurrent, convolutional, and transformer architectures. Our experiments demonstrate that even though the models can capture elementary aspects of the meaning of logical terms, they learn to generalize logical reasoning only to a limited degree.</abstract>
      <url hash="20df89ce">2024.findings-naacl.147</url>
      <bibkey>guzman-etal-2024-testing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.147</doi>
    </paper>
    <paper id="148">
      <title><fixed-case>METAL</fixed-case>: Towards Multilingual Meta-Evaluation</title>
      <author><first>Rishav</first><last>Hada</last><affiliation>Microsoft Research India</affiliation></author>
      <author><first>Varun</first><last>Gumma</last><affiliation>Microsoft</affiliation></author>
      <author><first>Mohamed</first><last>Ahmed</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Kalika</first><last>Bali</last><affiliation>Microsoft Research Labs</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>2280-2298</pages>
      <abstract>With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evaluate LLM-based evaluators, which we refer to as meta-evaluation (METAL). We compare the performance of LLM-based evaluators created using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that LLM-based evaluators based on GPT-4 perform the best across languages, while GPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the reasoning provided by LLM-based evaluators and find that it often does not match the reasoning provided by human judges.</abstract>
      <url hash="d61b1cb1">2024.findings-naacl.148</url>
      <bibkey>hada-etal-2024-metal</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.148</doi>
    </paper>
    <paper id="149">
      <title><fixed-case>AGIE</fixed-case>val: A Human-Centric Benchmark for Evaluating Foundation Models</title>
      <author><first>Wanjun</first><last>Zhong</last></author>
      <author><first>Ruixiang</first><last>Cui</last></author>
      <author><first>Yiduo</first><last>Guo</last></author>
      <author><first>Yaobo</first><last>Liang</last></author>
      <author><first>Shuai</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yanlin</first><last>Wang</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Amin</first><last>Saied</last></author>
      <author><first>Weizhu</first><last>Chen</last><affiliation>Microsoft GenAI</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>2299-2314</pages>
      <abstract>Assessing foundation models’ abilities for human-level tasks is crucial for Artificial General Intelligence (AGI) development.Traditional benchmarks, which rely on artificial datasets, may not accurately represent these capabilities. In this paper, we introduce AGIEval, a novel bilingual benchmark designed to assess foundation models in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models on our benchmark. Impressively, we show that GPT-4 exceeds the average human performance in SAT, LSAT, and math contests, with 95% accuracy on SAT Math and 92.5% on the Chinese college entrance English exam. This demonstrates the exceptional performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks requiring complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal their strengths and limitations, providing valuable insights into future directions for enhancing general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a meaningful and robust evaluation of foundation models’ performance in real-world scenarios.</abstract>
      <url hash="949f687f">2024.findings-naacl.149</url>
      <bibkey>zhong-etal-2024-agieval</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.149</doi>
    </paper>
    <paper id="150">
      <title>Product Description and <fixed-case>QA</fixed-case> Assisted Self-Supervised Opinion Summarization</title>
      <author><first>Tejpalsingh</first><last>Siledar</last></author>
      <author><first>Rupasai</first><last>Rangaraju</last></author>
      <author><first>Sankara</first><last>Muddu</last></author>
      <author><first>Suman</first><last>Banerjee</last><affiliation>Flipkart</affiliation></author>
      <author><first>Amey</first><last>Patil</last></author>
      <author><first>Sudhanshu</first><last>Singh</last></author>
      <author><first>Muthusamy</first><last>Chelliah</last><affiliation>Flipkart</affiliation></author>
      <author><first>Nikesh</first><last>Garera</last></author>
      <author><first>Swaprava</first><last>Nath</last><affiliation>IIT Kanpur and Computer Science and Engineering, Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>2315-2332</pages>
      <abstract>In e-commerce, opinion summarization is the process of summarizing the consensus opinions found in product reviews. However, the potential of additional sources such as product description and question-answers (QA) has been considered less often. Moreover, the absence of any supervised training data makes this task challenging. To address this, we propose a novel synthetic dataset creation (SDC) strategy that leverages information from reviews as well as additional sources for selecting one of the reviews as a pseudo-summary to enable supervised training. Our Multi-Encoder Decoder framework for Opinion Summarization (MEDOS) employs a separate encoder for each source, enabling effective selection of information while generating the summary. For evaluation, due to the unavailability of test sets with additional sources, we extend the Amazon, Oposum+, and Flipkart test sets and leverage ChatGPT to annotate summaries. Experiments across nine test sets demonstrate that the combination of our SDC approach and MEDOS model achieves on average a 14.5% improvement in ROUGE-1 F1 over the SOTA. Moreover, comparative analysis underlines the significance of incorporating additional sources for generating more informative summaries. Human evaluations further indicate that MEDOS scores relatively higher in coherence and fluency with 0.41 and 0.5 (−1 to 1) respectively, compared to existing models. To the best of our knowledge, we are the first to generate opinion summaries leveraging additional sources in a self-supervised setting.</abstract>
      <url hash="cd9ad100">2024.findings-naacl.150</url>
      <bibkey>siledar-etal-2024-product</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.150</doi>
    </paper>
    <paper id="151">
      <title><fixed-case>COMEM</fixed-case>: In-Context Retrieval-Augmented Mass-Editing Memory in Large Language Models</title>
      <author><first>Shanbao</first><last>Qiao</last></author>
      <author><first>Xuebing</first><last>Liu</last></author>
      <author><first>Seung-Hoon</first><last>Na</last><affiliation>Chonbuk National University</affiliation></author>
      <pages>2333-2347</pages>
      <abstract>Noting that world knowledge continuously evolves over time, large language models (LLMs) need to be properly adjusted by performing the “knowledge editing”, which involves updating outdated information or correcting false information. To achieve reliable and “massive” editing capabilities in terms of <tex-math>\textit{generalization}</tex-math> and <tex-math>\textit{specificity}</tex-math>, this paper proposes a unified knowledge editing method called in-<tex-math>\textbf{CO}</tex-math>ntext retrieval-augmented <tex-math>\textbf{M}</tex-math>ass-<tex-math>\textbf{E}</tex-math>diting <tex-math>\textbf{M}</tex-math>emory (COMEM), which combines two types of editing approaches: parameter updating and in-context knowledge editing (IKE). In particular, COMEM incorporates <tex-math>\textit{retrieval-augmented IKE}</tex-math>, a novel extension of IKE designed for massive editing tasks, based on an <tex-math>\textit{updating}</tex-math>-aware demonstration construction.Experimental results on the zsRE and CounterFact datasets demonstrate that COMEM outperforms all existing methods, achieving state-of-the-art performance. Our code is available at https://github.com/JoveReCode/COMEM.git.</abstract>
      <url hash="a22a9d74">2024.findings-naacl.151</url>
      <bibkey>qiao-etal-2024-comem</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.151</doi>
    </paper>
    <paper id="152">
      <title>Content-Specific Humorous Image Captioning Using Incongruity Resolution Chain-of-Thought</title>
      <author><first>Kohtaro</first><last>Tanaka</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Kohei</first><last>Uehara</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Lin</first><last>Gu</last><affiliation>RIKEN</affiliation></author>
      <author><first>Yusuke</first><last>Mukuta</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Tatsuya</first><last>Harada</last><affiliation>RIKEN and The University of Tokyo</affiliation></author>
      <pages>2348-2367</pages>
      <abstract>Although automated image captioning methods have benefited considerably from the development of large language models (LLMs), generating humorous captions is still a challenging task. Humorous captions generated by humans are unique to the image and reflect the content of the image. However, captions generated using previous captioning models tend to be generic. Therefore, we propose incongruity-resolution chain-of-thought (IRCoT) as a novel prompting framework that creates content-specific resolutions from fine details extracted from an image. Furthermore, we integrate logit bias and negative sampling to suppress the output of generic resolutions. The results of experiments with GPT4-V demonstrate that our proposed framework effectively generated humorous captions tailored to the content of specific input images.</abstract>
      <url hash="f4a29b9b">2024.findings-naacl.152</url>
      <bibkey>tanaka-etal-2024-content</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.152</doi>
    </paper>
    <paper id="153">
      <title>Denoising Attention for Query-aware User Modeling</title>
      <author><first>Elias</first><last>Bassani</last><affiliation>European Commission, Joint Research Centre</affiliation></author>
      <author><first>Pranav</first><last>Kasela</last><affiliation>University of Milan - Bicocca</affiliation></author>
      <author><first>Gabriella</first><last>Pasi</last><affiliation>University of Milan - Bicocca</affiliation></author>
      <pages>2368-2380</pages>
      <abstract>Personalization of search results has gained increasing attention in the past few years, also thanks to the development of Neural Networks-based approaches for Information Retrieval. Recent works have proposed to build user models at query time by leveraging the Attention mechanism, which allows weighing the contribution of the user-related information w.r.t. the current query.This approach allows giving more importance to the user’s interests related to the current search performed by the user.In this paper, we discuss some shortcomings of the Attention mechanism when employed for personalization and introduce a novel Attention variant, the Denoising Attention, to solve them.Denoising Attention adopts a robust normalization scheme and introduces a filtering mechanism to better discern among the user-related data those helpful for personalization.Experimental evaluation shows improvements in MAP, MRR, and NDCG above 15% w.r.t. other Attention variants at the state-of-the-art.</abstract>
      <url hash="183e2fca">2024.findings-naacl.153</url>
      <bibkey>bassani-etal-2024-denoising</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.153</doi>
    </paper>
    <paper id="154">
      <title>A Lightweight Mixture-of-Experts Neural Machine Translation Model with Stage-wise Training Strategy</title>
      <author><first>Fan</first><last>Zhang</last><affiliation>Communication University of China and Samsung</affiliation></author>
      <author><first>Mei</first><last>Tu</last></author>
      <author><first>Song</first><last>Liu</last></author>
      <author><first>Jinyao</first><last>Yan</last><affiliation>Communication University of China</affiliation></author>
      <pages>2381-2392</pages>
      <abstract>Dealing with language heterogeneity has always been one of the challenges in neural machine translation (NMT).The idea of using mixture-of-experts (MoE) naturally excels in addressing this issue by employing different experts to take responsibility for different problems.However, the parameter-inefficiency problem in MoE results in less performance improvement when boosting the number of parameters.Moreover, most of the MoE models are suffering from the training instability problem.This paper proposes MoA (Mixture-of-Adapters), a lightweight MoE-based NMT model that is trained via an elaborately designed stage-wise training strategy.With the standard Transformer as the backbone model, we introduce lightweight adapters as experts for easy expansion.To improve the parameter efficiency, we explicitly model and distill the language heterogeneity into the gating network with clustering.After freezing the gating network, we adopt the Gumbel-Max sampling as the routing scheme when training experts to balance the knowledge of generalization and specialization while preventing expert over-fitting.Empirical results show that MoA achieves stable improvements in different translation tasks by introducing much fewer extra parameters compared to other MoE baselines.Additionally, the performance evaluations on a multi-domain translation task illustrate the effectiveness of our training strategy.</abstract>
      <url hash="b658de64">2024.findings-naacl.154</url>
      <bibkey>zhang-etal-2024-lightweight</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.154</doi>
    </paper>
    <paper id="155">
      <title><fixed-case>BEAR</fixed-case>: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models</title>
      <author><first>Jacek</first><last>Wiland</last><affiliation>Department of Computer Science, Humboldt University Berlin, Humboldt Universität Berlin</affiliation></author>
      <author><first>Max</first><last>Ploner</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>2393-2411</pages>
      <abstract>Knowledge probing assesses to which degree a language model (LM) has successfully learned relational knowledge during pre-training. Probing is an inexpensive way to compare LMs of different sizes and training configurations. However, previous approaches rely on the objective function used in pre-training LMs and are thus applicable only to masked or causal LMs. As a result, comparing different types of LMs becomes impossible. To address this, we propose an approach that uses an LM’s inherent ability to estimate the log-likelihood of any given textual statement. We carefully design an evaluation dataset of 7,731 instances (40,916 in a larger variant) from which we produce alternative statements for each relational fact, one of which is correct. We then evaluate whether an LM correctly assigns the highest log-likelihood to the correct statement. Our experimental evaluation of 22 common LMs shows that our proposed framework, BEAR, can effectively probe for knowledge across different LM types. We release the BEAR datasets and an open-source framework that implements the probing approach to the research community to facilitate the evaluation and development of LMs.</abstract>
      <url hash="f77284cd">2024.findings-naacl.155</url>
      <bibkey>wiland-etal-2024-bear</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.155</doi>
    </paper>
    <paper id="156">
      <title>Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition</title>
      <author><first>Floris</first><last>Hengst</last></author>
      <author><first>Ralf</first><last>Wolter</last><affiliation>ING Bank</affiliation></author>
      <author><first>Patrick</first><last>Altmeyer</last></author>
      <author><first>Arda</first><last>Kaygan</last></author>
      <pages>2412-2432</pages>
      <abstract>We present Conformal Intent Classification and Clarification (CICC), a framework for fast and accurate intent classification for task-oriented dialogue systems. The framework turns heuristic uncertainty scores of any intent classifier into a clarification question that is guaranteed to contain the true intent at a pre-defined confidence level.By disambiguating between a small number of likely intents, the user query can be resolved quickly and accurately. Additionally, we propose to augment the framework for out-of-scope detection.In a comparative evaluation using seven intent recognition datasets we find that CICC generates small clarification questions and is capable of out-of-scope detection.CICC can help practitioners and researchers substantially in improving the user experience of dialogue agents with specific clarification questions.</abstract>
      <url hash="0968ffa5">2024.findings-naacl.156</url>
      <bibkey>hengst-etal-2024-conformal</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.156</doi>
    </paper>
    <paper id="157">
      <title>Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models in Court Decisions</title>
      <author><first>Alex</first><last>Nyffenegger</last></author>
      <author><first>Matthias</first><last>Stürmer</last><affiliation>BFH - Bern University of Applied Sciences and Universität Bern</affiliation></author>
      <author><first>Joel</first><last>Niklaus</last><affiliation>University of Bern, Universität Bern</affiliation></author>
      <pages>2433-2462</pages>
      <abstract>Anonymity in court rulings is a critical aspect of privacy protection in the European Union and Switzerland but with the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland (FSCS), we study re-identification risks using actual legal data. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. In addition to the datasets, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. We demonstrate that for now, the risk of re-identifications using LLMs is minimal in the vast majority of cases. We hope that our system can help enhance the confidence in the security of anonymized decisions, thus leading the courts to publish more decisions.</abstract>
      <url hash="c42b9d9f">2024.findings-naacl.157</url>
      <bibkey>nyffenegger-etal-2024-anonymity</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.157</doi>
    </paper>
    <paper id="158">
      <title><fixed-case>X</fixed-case>-<fixed-case>LL</fixed-case>a<fixed-case>VA</fixed-case>: Optimizing Bilingual Large Vision-Language Alignment</title>
      <author><first>DongJae</first><last>Shin</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>HyeonSeok</first><last>Lim</last><affiliation>Hanbat National University</affiliation></author>
      <author><first>Inho</first><last>Won</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>ChangSu</first><last>Choi</last></author>
      <author><first>Minjun</first><last>Kim</last></author>
      <author><first>SeungWoo</first><last>Song</last><affiliation>Hanbat National University</affiliation></author>
      <author><first>HanGyeol</first><last>Yoo</last></author>
      <author><first>SangMin</first><last>Kim</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <author><first>KyungTae</first><last>Lim</last><affiliation>Seoul National University of Science and Technology</affiliation></author>
      <pages>2463-2473</pages>
      <abstract>The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.</abstract>
      <url hash="b30034b3">2024.findings-naacl.158</url>
      <bibkey>shin-etal-2024-x</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.158</doi>
    </paper>
    <paper id="159">
      <title>Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise</title>
      <author><first>Giwon</first><last>Hong</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Junmo</first><last>Kang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <author><first>Joyce</first><last>Whang</last><affiliation>KAIST</affiliation></author>
      <pages>2474-2495</pages>
      <abstract>Most existing retrieval-augmented language models (LMs) assume a naive dichotomy within a retrieved document set: query-relevance and irrelevance. Our work investigates a more challenging scenario in which even the “relevant” documents may contain misleading or incorrect information, causing conflict among the retrieved documents and thereby negatively influencing model decisions as noise. We observe that existing LMs are highly brittle to the presence of conflicting information in both the fine-tuning and in-context few-shot learning scenarios. We propose approaches for handling knowledge conflicts among retrieved documents by explicitly fine-tuning a discriminator or prompting GPT-3.5 to elicit its discriminative capability. Our empirical results on open-domain QA show that these approaches significantly enhance model robustness. We also provide our findings on incorporating the fine-tuned discriminator’s decision into the in-context learning process, proposing a way to exploit the benefits of two disparate learning schemes. Alongside our findings, we provide MacNoise, a machine-generated, conflict-induced dataset to further encourage research in this direction.</abstract>
      <url hash="2b21f10d">2024.findings-naacl.159</url>
      <bibkey>hong-etal-2024-gullible</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.159</doi>
    </paper>
    <paper id="160">
      <title>Heterogeneity over Homogeneity: Investigating Multilingual Speech Pre-Trained Models for Detecting Audio Deepfake</title>
      <author><first>Orchid</first><last>Chetia Phukan</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Gautam</first><last>Kashyap</last></author>
      <author><first>Arun Balaji</first><last>Buduru</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Rajesh</first><last>Sharma</last><affiliation>institute of computer science, University of Tartu</affiliation></author>
      <pages>2496-2506</pages>
      <abstract>In this work, we investigate multilingual speech Pre-Trained models (PTMs) for Audio deepfake detection (ADD). We hypothesize thatmultilingual PTMs trained on large-scale diverse multilingual data gain knowledge about diverse pitches, accents, and tones, during theirpre-training phase and making them more robust to variations. As a result, they will be more effective for detecting audio deepfakes. To validate our hypothesis, we extract representations from state-of-the-art (SOTA) PTMs including monolingual, multilingual as well as PTMs trained for speaker and emotion recognition, and evaluated them on ASVSpoof 2019 (ASV), In-the-Wild (ITW), and DECRO benchmark databases. We show that representations from multilingual PTMs, with simple downstream networks, attain the best performance for ADD compared to other PTM representations, which validates our hypothesis. We also explore the possibility of fusion of selected PTM representations for further improvements in ADD, and we propose a framework, MiO (Merge into One) for this purpose. With MiO, we achieve SOTA performance on ASV and ITW and comparable performance on DECRO with current SOTA works.</abstract>
      <url hash="a8e90956">2024.findings-naacl.160</url>
      <bibkey>chetia-phukan-etal-2024-heterogeneity</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.160</doi>
    </paper>
    <paper id="161">
      <title>Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts</title>
      <author><first>Chenghao</first><last>Yang</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Tuhin</first><last>Chakrabarty</last><affiliation>SalesForce Research</affiliation></author>
      <author><first>Karli</first><last>Hochstatter</last></author>
      <author><first>Melissa</first><last>Slavin</last><affiliation>Columbia University</affiliation></author>
      <author><first>Nabila</first><last>El-Bassel</last><affiliation>Columbia University and Columbia University</affiliation></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Amazon and Columbia University</affiliation></author>
      <pages>2507-2521</pages>
      <abstract>In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids making it a national public health emergency (USDHHS, 2017). Medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors. We present a moderate size corpus of 2500 opioid-related posts from various subreddits labeled with six different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of opioid use disorder is highly contextual and challenging. However, we find that using explanations during modeling leads to a significant boost in classification accuracy demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum.</abstract>
      <url hash="8e73f5b6">2024.findings-naacl.161</url>
      <bibkey>yang-etal-2024-identifying</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.161</doi>
    </paper>
    <paper id="162">
      <title>Self-Adaptive Sampling for Accurate Video Question Answering on Image Text Models</title>
      <author><first>Wei</first><last>Han</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Hui</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>2522-2534</pages>
      <abstract>Image–text models (ITMs) is the prevalent architecture to solve video question–answering tasks, which requires only a few input frames to save huge computational cost compared to video–language models.However, we find existent ITM video question–answering solutions either 1) adopt simplistic and unintentional sampling strategies, which may miss key frames to offer the answer clues; or 2) sample a large number of frames into divided groups, which the computational sources can not accommodate. In this work, we aim at an efficient sampling method towards the few-frame situations.We first summarize a family of prior sampling methods based on question–frame correlation into a unified one, dubbed *Most Implied Frames* (MIF). Through some primary results and analysis, Through analysis, we form a hypothesis that question-aware sampling is not necessary, from which we further propose the other method *Most Dominant Frames* (MDF).Experimental results on four public datasets and three advanced ITMs demonstrate that our proposed strategies can boost the performance for image–text pretrained models, and have a wide application scenario in terms of model architectures and dataset types. Our code is available at https://github.com/declare-lab/Sealing<url>https://github.com/declare-lab/Sealing</url>.</abstract>
      <url hash="914cf66a">2024.findings-naacl.162</url>
      <bibkey>han-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.162</doi>
    </paper>
    <paper id="163">
      <title>Towards an On-device Agent for Text Rewriting</title>
      <author><first>Yun</first><last>Zhu</last><affiliation>Google</affiliation></author>
      <author><first>Yinxiao</first><last>Liu</last></author>
      <author><first>Felix</first><last>Stahlberg</last><affiliation>Google</affiliation></author>
      <author><first>Shankar</first><last>Kumar</last></author>
      <author><first>Yu-Hui</first><last>Chen</last></author>
      <author><first>Liangchen</first><last>Luo</last><affiliation>Google</affiliation></author>
      <author><first>Lei</first><last>Shu</last><affiliation>Google</affiliation></author>
      <author><first>Renjie</first><last>Liu</last></author>
      <author><first>Jindong</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Lei</first><last>Meng</last></author>
      <pages>2535-2552</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. However creating a smaller yet potent language model for text rewriting presents two formidable challenges: costly data collection and absence of emergent capabilities.In this paper we present solutions to address the above challenges.We propose an new instruction tuning method to develop a mo-bile text rewriting model that leverages LLM-generated data and heuristic reinforcement learning, eliminating the need for human data collection. Moreover, to bridge the performance gap from the constraint size, we pro-pose a cascading approach based on the confidence levels which are distilled from the large server model’s critiques. To evaluate the text rewriting tasks for mobile scenarios, we introduce MessageRewriteEval, a human-labeled benchmark that focuses on text rewriting of messages through natural language instructions. Through empirical experiments, we demonstrate that our on-device model surpasses the current state-of-the-art LLMs in text rewriting while maintaining a significantly reduced model size using public benchmark EditEval and our new benchmark. We also demonstrate that our proposed cascading approach improves model performance further.</abstract>
      <url hash="f33f1447">2024.findings-naacl.163</url>
      <bibkey>zhu-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.163</doi>
    </paper>
    <paper id="164">
      <title>Tailoring Vaccine Messaging with Common-Ground Opinions</title>
      <author><first>Rickard</first><last>Stureborg</last><affiliation>Duke University</affiliation></author>
      <author><first>Sanxing</first><last>Chen</last><affiliation>Duke University</affiliation></author>
      <author><first>Roy</first><last>Xie</last></author>
      <author><first>Aayushi</first><last>Patel</last></author>
      <author><first>Christopher</first><last>Li</last></author>
      <author><first>Chloe</first><last>Zhu</last></author>
      <author><first>Tingnan</first><last>Hu</last></author>
      <author><first>Jun</first><last>Yang</last><affiliation>Department of Computer Science, Duke University</affiliation></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>2553-2575</pages>
      <abstract>One way to personalize chatbot interactions is by establishing common ground with the intended reader. A domain where establishing mutual understanding could be particularly impactful is vaccine concerns and misinformation. Vaccine interventions are forms of messaging which aim to answer concerns expressed about vaccination. Tailoring responses in this domain is difficult, since opinions often have seemingly little ideological overlap. We define the task of tailoring vaccine interventions to a Common-Ground Opinion (CGO). Tailoring responses to a CGO involves meaningfully improving the answer by relating it to an opinion or belief the reader holds. In this paper we introduce Tailor-CGO, a dataset for evaluating how well responses are tailored to provided CGOs. We benchmark several major LLMs on this task; finding GPT-4-Turbo performs significantly better than others. We also build automatic evaluation metrics, including an efficient and accurate BERT model that outperforms finetuned LLMs, investigate how to successfully tailor vaccine messaging to CGOs, and provide actionable recommendations from this investigation.Tailor-CGO dataset and code available at: https://github.com/rickardstureborg/tailor-cgo</abstract>
      <url hash="692c0f07">2024.findings-naacl.164</url>
      <bibkey>stureborg-etal-2024-tailoring</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.164</doi>
    </paper>
    <paper id="165">
      <title>Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification</title>
      <author><first>Robert</first><last>Vacareanu</last></author>
      <author><first>Fahmida</first><last>Alam</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Md Asiful</first><last>Islam</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Haris</first><last>Riaz</last></author>
      <author><first>Mihai</first><last>Surdeanu</last><affiliation>University of Arizona</affiliation></author>
      <pages>2576-2594</pages>
      <abstract>This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching.Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data.Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher.In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29. We show that our proposed method outperforms previous state-of-the-art models in three out of four settings, despite not seeing any human-annotated training data.Further, we show that our approach remains modular and pliable, i.e., the corresponding rules can be locally modified to improve the overall model. Human interventions to the rules for the TACRED relation org:parents boost the performance on that relation by as much as 26% relative improvement, without negatively impacting the other relations, and without retraining the semantic matching component.</abstract>
      <url hash="02a0a30b">2024.findings-naacl.165</url>
      <bibkey>vacareanu-etal-2024-best</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.165</doi>
    </paper>
    <paper id="166">
      <title><fixed-case>Q</fixed-case>-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language Learning</title>
      <author><first>Yanhui</first><last>Guo</last></author>
      <author><first>Shaoyuan</first><last>Xu</last><affiliation>Amazon</affiliation></author>
      <author><first>Jinmiao</first><last>Fu</last></author>
      <author><first>Jia</first><last>Liu</last><affiliation>The Ohio State University</affiliation></author>
      <author><first>Chaosheng</first><last>Dong</last></author>
      <author><first>Bryan</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <pages>2595-2622</pages>
      <abstract>This paper introduces Q-tuning, a novel approach for continual prompt tuning that enables the lifelong learning of a pre-trained language model. When learning a new task, Q-tuning trains a task-specific prompt by adding it to a prompt queue consisting of the prompts from older tasks. To better transfer the knowledge of old tasks, we design an adaptive knowledge aggregation technique that reweighs previous prompts in the queue with a learnable low-rank matrix. Once the prompt queue reaches its maximum capacity, we leverage a PCA-based eviction rule to reduce the queue’s size, allowing the newly trained prompt to be added while preserving the primary knowledge of old tasks. In order to mitigate the accumulation of information loss caused by the eviction, we additionally propose a globally shared prefix prompt and a memory retention regularization based on information theory. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods substantially on continual prompt tuning benchmarks. Moreover, our approach enables lifelong learning on linearly growing task sequences while requiring constant complexity for training and inference.</abstract>
      <url hash="b6908519">2024.findings-naacl.166</url>
      <bibkey>guo-etal-2024-q</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.166</doi>
    </paper>
    <paper id="167">
      <title>In-Context Example Ordering Guided by Label Distributions</title>
      <author><first>Zhichao</first><last>Xu</last><affiliation>University of Utah</affiliation></author>
      <author><first>Daniel</first><last>Cohen</last><affiliation>Brown University</affiliation></author>
      <author><first>Bei</first><last>Wang</last><affiliation>University of Utah</affiliation></author>
      <author><first>Vivek</first><last>Srikumar</last><affiliation>University of Utah</affiliation></author>
      <pages>2623-2640</pages>
      <abstract>By allowing models to predict without task-specific training, in-context learning (ICL) with pretrained LLMs has enormous potential in NLP. However, a number of problems persist in ICL. In particular, its performance is sensitive to the choice and order of in-context examples. Given the same set of in-context examples with different orderings, model performance may vary from near random to near state-of-the-art. In this work, we formulate in-context example ordering as an optimization problem. We examine three problem settings that differ in the assumptions they make about what is known about the task. Inspired by the idea of learning from label proportions, we propose two principles for in-context example ordering guided by model’s probability predictions. We apply our proposed principles to thirteen text classification datasets and nine different autoregressive LLMs with 700M to 13B parameters. We demonstrate that our approach outperforms the baselines by improving the classification accuracy, reducing model miscalibration, and also by selecting better in-context examples.</abstract>
      <url hash="6dd051e8">2024.findings-naacl.167</url>
      <bibkey>xu-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.167</doi>
    </paper>
    <paper id="168">
      <title>Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives</title>
      <author><first>Jiaxin</first><last>Liu</last></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Kar Yan</first><last>Tam</last></author>
      <pages>2641-2652</pages>
      <abstract>In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives. These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons. Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company’s financial and operational situations, which is critical for financial decision-making. We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts. To address this gap, we propose an LLM-augmented pipeline specifically designed for the Financial-STS task. Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and generic LLM embeddings.</abstract>
      <url hash="924dc4ae">2024.findings-naacl.168</url>
      <bibkey>liu-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.168</doi>
    </paper>
    <paper id="169">
      <title>Laying Anchors: Semantically Priming Numerals in Language Modeling</title>
      <author><first>Mandar</first><last>Sharma</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Rutuja</first><last>Taware</last></author>
      <author><first>Pravesh</first><last>Koirala</last><affiliation>Vanderbilt University</affiliation></author>
      <author><first>Nikhil</first><last>Muralidhar</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Naren</first><last>Ramakrishnan</last><affiliation>Virginia Tech</affiliation></author>
      <pages>2653-2660</pages>
      <abstract>Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.</abstract>
      <url hash="868d0e81">2024.findings-naacl.169</url>
      <bibkey>sharma-etal-2024-laying</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.169</doi>
    </paper>
    <paper id="170">
      <title><fixed-case>UEGP</fixed-case>: Unified Expert-Guided Pre-training for Knowledge Rekindle</title>
      <author><first>Yutao</first><last>Mou</last></author>
      <author><first>Kexiang</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jianhe</first><last>Lin</last></author>
      <author><first>Dehong</first><last>Ma</last></author>
      <author><first>Jun</first><last>Fan</last></author>
      <author><first>Daiting</first><last>Shi</last></author>
      <author><first>Zhicong</first><last>Cheng</last></author>
      <author><first>Gu</first><last>Simiu</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>2661-2673</pages>
      <abstract>Pre-training and fine-tuning framework has become the standard training paradigm for NLP tasks and is also widely used in industrial-level applications. However, there are still a limitation with this paradigm: simply fine-tuning with task-specific objectives tends to converge to local minima, resulting in a sub-optimal performance. In this paper, we first propose a new paradigm: knowledge rekindle, which aims to re-incorporate the fine-tuned expert model into the training cycle and break through the performance upper bounds of experts without introducing additional annotated data. Then we further propose a unified expert-guided pre-training (UEGP) framework for knowledge rekindle. Specifically, we reuse fine-tuned expert models for various downstream tasks as knowledge sources and inject task-specific prior knowledge to pre-trained language models (PLMs) by means of knowledge distillation. In this process, we perform multi-task learning with knowledge distillation and masked language modeling (MLM) objectives. We also further explored whether mixture-of-expert guided pre-training (MoEGP) can further enhance the effect of knowledge rekindle. Experiments and analysis on eight datasets in GLUE benchmark and a industrial-level search re-ranking dataset show the effectiveness of our method.</abstract>
      <url hash="4d84bbd9">2024.findings-naacl.170</url>
      <bibkey>mou-etal-2024-uegp</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.170</doi>
    </paper>
    <paper id="171">
      <title><fixed-case>L</fixed-case>attice<fixed-case>G</fixed-case>en: Hiding Generated Text in a Lattice for Privacy-Aware Large Language Model Generation on Cloud</title>
      <author><first>Mengke</first><last>Zhang</last></author>
      <author><first>Tianxing</first><last>He</last></author>
      <author><first>Tianle</first><last>Wang</last></author>
      <author><first>Lu</first><last>Mi</last><affiliation>University of Washington and Allen Institute</affiliation></author>
      <author><first>Niloofar</first><last>Mireshghallah</last><affiliation>University of Washington</affiliation></author>
      <author><first>Binyi</first><last>Chen</last></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>2674-2690</pages>
      <abstract>In the current user-server interaction paradigm of prompted generation with large language models (LLMs) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text private to themselves. For privacy-aware text generation on cloud, we propose LatticeGen, a cooperative protocol in which the server still handles most of the computation while the client controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the client and hidden in a noised lattice. Only the client knows which tokens are the true ones. Considering potential attacks from a hypothetically malicious server and how the client can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as measured by BERTScore).</abstract>
      <url hash="2e8b364c">2024.findings-naacl.171</url>
      <bibkey>zhang-etal-2024-latticegen</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.171</doi>
    </paper>
    <paper id="172">
      <title><fixed-case>H</fixed-case>ate<fixed-case>M</fixed-case>oderate: Testing Hate Speech Detectors against Content Moderation Policies</title>
      <author><first>Jiangrui</first><last>Zheng</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Xueqing</first><last>Liu</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Mirazul</first><last>Haque</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Xing</first><last>Qian</last><affiliation>Stevens Institute of Technology</affiliation></author>
      <author><first>Guanqun</first><last>Yang</last></author>
      <author><first>Wei</first><last>Yang</last><affiliation>University of Texas, Dallas</affiliation></author>
      <pages>2691-2710</pages>
      <abstract>To protect users from massive hateful content, existing works studied automated hate speech detection. Despite the existing efforts, one question remains: Do automated hate speech detectors conform to social media content policies? A platform’s content policies are a checklist of content moderated by the social media platform. Because content moderation rules are often uniquely defined, existing hate speech datasets cannot directly answer this question. This work seeks to answer this question by creating HateModerate, a dataset for testing the behaviors of automated content moderators against content policies. First, we engage 28 annotators and GPT in a six-step annotation process, resulting in a list of hateful and non-hateful test suites matching each of Facebook’s 41 hate speech policies. Second, we test the performance of state-of-the-art hate speech detectors against HateModerate, revealing substantial failures these models have in their conformity to the policies. Third, using HateModerate, we augment the training data of a top-downloaded hate detector on HuggingFace. We observe significant improvement in the models’ conformity to content policies while having comparable scores on the original test data. Our dataset and code can be found on https://github.com/stevens-textmining/HateModerate.</abstract>
      <url hash="18c2c677">2024.findings-naacl.172</url>
      <bibkey>zheng-etal-2024-hatemoderate</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.172</doi>
    </paper>
    <paper id="173">
      <title>Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other</title>
      <author><first>Yifei</first><last>Gao</last></author>
      <author><first>Jie</first><last>Ou</last></author>
      <author><first>Lei</first><last>Wang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yuting</first><last>Xiao</last></author>
      <author><first>Xiangzhiyuan</first><last>Xiangzhiyuan</last></author>
      <author><first>Ruiting</first><last>Dai</last></author>
      <author><first>Jun</first><last>Cheng</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>2711-2722</pages>
      <abstract>Emergent Large Language Models (LLMs) use their extraordinary performance and powerful deduction capacity to discern from traditional language models. However, the expenses of computational resources and storage for these LLMs are stunning, quantization then arises as a trending conversation. To address accuracy decay caused by quantization, two streams of works in post-training quantization methods stand out. One uses other weights to compensate existing quantization error, while the other transfers the quantization difficulty to other parts in the model. Combining both merits, we introduce Learnable Singular value Increment (LSI) as an advanced solution. LSI uses Singular Value Decomposition to extract singular values of the weights and make them learnable to help weights compensate each other conditioned on activation. Incorporating LSI with existing techniques, we achieve state-of-the-art performance in diverse quantization settings, no matter in weight-only, weight-activation or extremely low bit scenarios. By unleashing the potential of LSI, efficient finetuning on quantized model is no longer a prohibitive problem.</abstract>
      <url hash="5e45dd1f">2024.findings-naacl.173</url>
      <bibkey>gao-etal-2024-compensate</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.173</doi>
    </paper>
    <paper id="174">
      <title>Contrastive Preference Learning for Neural Machine Translation</title>
      <author><first>Jianfei</first><last>He</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Shichao</first><last>Sun</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Sen</first><last>Peng</last></author>
      <author><first>Jie</first><last>Xu</last></author>
      <author><first>Xiaohua</first><last>Jia</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>2723-2735</pages>
      <abstract>There exists a discrepancy between the token-level objective during training and the overall sequence-level quality that is expected from the model. This discrepancy leads to issues like exposure bias.To align the model with human expectations, sequence-level objectives are often used to fine-tune pre-trained models.In this paper, we introduce a contrastive preference model that enhances the traditional Plackett-Luce model by incorporating an indicator function. Building upon this novel preference model, we propose Contrastive Preference Learning (CPL), which uses offline samples with list-wise preferences to fine-tune a pre-trained model in Neural Machine Translation. Our experiments, conducted on three language pairs, demonstrate that CPL outperforms not only the vanilla Transformer model but also other token-level and sequence-level baselines. Furthermore, the ablation study highlights the essential role of the proposed indicator function in achieving this improvement.</abstract>
      <url hash="9ff3da6d">2024.findings-naacl.174</url>
      <bibkey>he-etal-2024-contrastive</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.174</doi>
    </paper>
    <paper id="175">
      <title><fixed-case>S</fixed-case>oc<fixed-case>RE</fixed-case>val: Large Language Models with the Socratic Method for Reference-free Reasoning Evaluation</title>
      <author><first>Hangfeng</first><last>He</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Dan</first><last>Roth</last><affiliation>Amazon and University of Pennsylvania</affiliation></author>
      <pages>2736-2764</pages>
      <abstract>To comprehensively gauge the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains. However, such “gold-standard” human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning evaluation metrics, while eliminating the need for human-crafted reasoning chains as references, often require fine-tuning with human-derived chains before evaluation, complicating the process and questioning their adaptability to other datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes. Leveraging the Socratic method, we develop SocREval (**Soc**ratic Method-Inspired **R**easoning **Eval**uation), a novel approach for prompt design in reference-free reasoning evaluation. Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4’s performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.</abstract>
      <url hash="a49bc7a4">2024.findings-naacl.175</url>
      <bibkey>he-etal-2024-socreval</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.175</doi>
    </paper>
    <paper id="176">
      <title>Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis</title>
      <author><first>Wenhao</first><last>Zhu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Hongyi</first><last>Liu</last></author>
      <author><first>Qingxiu</first><last>Dong</last></author>
      <author><first>Jingjing</first><last>Xu</last></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>2765-2781</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs’ performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually involving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system like Google Translate, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages. Second, instruction semantics can surprisingly be ignored when given in-context exemplars. Third, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Code will be released at: https://github.com/NJUNLP/MMT-LLM.</abstract>
      <url hash="321e3b0c">2024.findings-naacl.176</url>
      <bibkey>zhu-etal-2024-multilingual</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.176</doi>
    </paper>
    <paper id="177">
      <title>Unleashing the Power of <fixed-case>LLM</fixed-case>s in Court View Generation by Stimulating Internal Knowledge and Incorporating External Knowledge</title>
      <author><first>Yifei</first><last>Liu</last></author>
      <author><first>Yiquan</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Yating</first><last>Zhang</last></author>
      <author><first>Changlong</first><last>Sun</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>2782-2792</pages>
      <abstract>Court View Generation (CVG) plays a vital role in the realm of legal artificial intelligence, which aims to support judges in crafting legal judgment documents. The court view consists of three essential judgment parts: the charge-related, law article-related, and prison term-related parts, each requiring specialized legal knowledge, rendering CVG a challenging task.Although Large Language Models (LLMs) have made remarkable strides in language generation, they encounter difficulties in the knowledge-intensive legal domain.Actually, there can be two types of knowledge: internal knowledge stored within LLMs’ parameters and external knowledge sourced from legal documents outside the models.In this paper, we decompose court views into different parts, stimulate internal knowledge, and incorporate external information to unleash the power of LLMs in the CVG task.To validate our method, we conduct a series of experiment results on two real-world datasets LAIC2021 and CJO2022. The experiments demonstrate that our method is capable of generating more accurate and reliable court views.</abstract>
      <url hash="0b3d703a">2024.findings-naacl.177</url>
      <bibkey>liu-etal-2024-unleashing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.177</doi>
    </paper>
    <paper id="178">
      <title>Prompting Vision-Language Models For Aspect-Controlled Generation of Referring Expressions</title>
      <author><first>Danfeng</first><last>Guo</last></author>
      <author><first>Sanchit</first><last>Agarwal</last><affiliation>Amazon</affiliation></author>
      <author><first>Arpit</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Jiun-Yu</first><last>Kao</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Emre</first><last>Barut</last><affiliation>Amazon</affiliation></author>
      <author><first>Tagyoung</first><last>Chung</last><affiliation>Amazon</affiliation></author>
      <author><first>Jing</first><last>Huang</last><affiliation>Amazon Alexa AI</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <pages>2793-2807</pages>
      <abstract>Referring Expression Generation (REG) is the task of generating a description that unambiguously identifies a given target in the scene. Different from Image Captioning (IC), REG requires learning fine-grained characteristics of not only the scene objects but also their surrounding context. Referring expressions are usually not singular; an object can often be uniquely referenced in numerous ways, for instance, by color, by location, or by relationship with other objects. Most prior works, however, have not explored this ‘aspect-based multiplicity’ of referring expressions. Hence, in this work, we focus on the Aspect-Controlled REG task, which requires generating a referring expression conditioned on the input aspect(s), where an aspect captures a style of reference. By changing the input aspect such as color, location, action etc., one can generate multiple distinct expressions per target region. To solve this new task, we first modify BLIP for aligning image-regions and text-expressions. We achieve this through a novel approach for feeding the input by drawing a bounding box around the target image-region and prompting the model to generate the referring expression. Our base REG model already beats all prior works in CIDEr score. To tackle Aspect-Controlled REG, we append ‘aspect tokens’ to the prompt and show that distinct expressions can be generated by just changing the prompt. Finally, to prove the high-quality and diversity of the data generated by our proposed aspect-controlled REG model, we also perform data-augmentation-based evaluation on the downstream Referring Expression Comprehension (REC) task. With just half of the real data augmented with the generated synthetic data, we achieve performance comparable to training with 100% of real data, using a SOTA REC model.</abstract>
      <url hash="9c8f2dab">2024.findings-naacl.178</url>
      <bibkey>guo-etal-2024-prompting</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.178</doi>
    </paper>
    <paper id="179">
      <title>Task-Agnostic Detector for Insertion-Based Backdoor Attacks</title>
      <author><first>Weimin</first><last>Lyu</last></author>
      <author><first>Xiao</first><last>Lin</last><affiliation>SRI International</affiliation></author>
      <author><first>Songzhu</first><last>Zheng</last><affiliation>Morgan Stanley</affiliation></author>
      <author><first>Lu</first><last>Pang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Haibin</first><last>Ling</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Susmit</first><last>Jha</last><affiliation>SRI International</affiliation></author>
      <author><first>Chao</first><last>Chen</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <pages>2808-2822</pages>
      <abstract>Textual backdoor attacks pose significant security threats. Current detection approaches, typically relying on intermediate feature representation or reconstructing potential triggers, are task-specific and less effective beyond sentence classification, struggling with tasks like question answering and named entity recognition. We introduce TABDet (Task-Agnostic Backdoor Detector), a pioneering task-agnostic method for backdoor detection. TABDet leverages final layer logits combined with an efficient pooling technique, enabling unified logit representation across three prominent NLP tasks. TABDet can jointly learn from diverse task-specific models, demonstrating superior detection efficacy over traditional task-specific methods.</abstract>
      <url hash="42668dd3">2024.findings-naacl.179</url>
      <bibkey>lyu-etal-2024-task</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.179</doi>
    </paper>
    <paper id="180">
      <title>Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission</title>
      <author><first>Jianfeng</first><last>He</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Linlin</first><last>Yu</last></author>
      <author><first>Shuo</first><last>Lei</last><affiliation>Sony Coporation of America</affiliation></author>
      <author><first>Chang-Tien</first><last>Lu</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Feng</first><last>Chen</last><affiliation>University of Texas, Dallas</affiliation></author>
      <pages>2823-2835</pages>
      <abstract>Sequential labeling is a task predicting labels for each token in a sequence, such as Named Entity Recognition (NER). NER tasks aim to extract entities and predict their labels given a text, which is important in information extraction. Although previous works have shown great progress in improving NER performance, uncertainty estimation on NER (UE-NER) is still underexplored but essential. This work focuses on UE-NER, which aims to estimate uncertainty scores for the NER predictions. Previous uncertainty estimation models often overlook two unique characteristics of NER: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on three datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset. Our code is available at <url>https://github.com/he159ok/UncSeqLabeling_SLPN</url>.</abstract>
      <url hash="845f8ced">2024.findings-naacl.180</url>
      <bibkey>he-etal-2024-uncertainty</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.180</doi>
    </paper>
    <paper id="181">
      <title>Exploring Language Model’s Code Generation Ability with Auxiliary Functions</title>
      <author><first>Seonghyeon</first><last>Lee</last></author>
      <author><first>Sanghwan</first><last>Jang</last><affiliation>POSTECH</affiliation></author>
      <author><first>Seongbo</first><last>Jang</last><affiliation>Pohang University of Science and Technology</affiliation></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Hwanjo</first><last>Yu</last><affiliation>POSTECH</affiliation></author>
      <pages>2836-2848</pages>
      <abstract>Auxiliary function is a helpful component to improve language model’s code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other.With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models’ various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models’ promising ability to utilize auxiliary functions including their self-improving behavior by implementing the two functions step-by-step. However, our analysis also reveals the model’s underutilized behavior to call the auxiliary function, suggesting the future direction to enhance their implementation by eliciting the auxiliary function call ability encoded in the models. We release our code and dataset to facilitate this research direction.</abstract>
      <url hash="9b6f5287">2024.findings-naacl.181</url>
      <bibkey>lee-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.181</doi>
    </paper>
    <paper id="182">
      <title>Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of <fixed-case>V</fixed-case>ietnamese Large Language Models</title>
      <author><first>Sang</first><last>Truong</last></author>
      <author><first>Duc</first><last>Nguyen</last></author>
      <author><first>Toan</first><last>Nguyen</last></author>
      <author><first>Dong</first><last>Le</last></author>
      <author><first>Nhi</first><last>Truong</last></author>
      <author><first>Tho</first><last>Quan</last></author>
      <author><first>Sanmi</first><last>Koyejo</last><affiliation>Stanford University and Google</affiliation></author>
      <pages>2849-2900</pages>
      <abstract>Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 tasks and 31 metrics. We observe that finetuning can help LLMs transfer knowledge across languages, serving as an efficient way to bolster their capabilities in non-English languages. Moreover, our analysis indicates that larger models can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or finetuning datasets. These insights underscore the significance of meticulous finetuning with high-quality datasets in enhancing LLM performance.</abstract>
      <url hash="886bcd8e">2024.findings-naacl.182</url>
      <bibkey>truong-etal-2024-crossing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.182</doi>
    </paper>
    <paper id="183">
      <title><fixed-case>G</fixed-case>o<fixed-case>T</fixed-case>: Effective Graph-of-Thought Reasoning in Language Models</title>
      <author id="yao-yao"><first>Yao</first><last>Yao</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>2901-2921</pages>
      <abstract>With the widespread use of language models (LMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. GoT adopts a two-stage framework with an additional GoT encoder for thought graph representation and fuses the graph representation with the original input representation through a gated fusion mechanism. We evaluate GoT’s performance on a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59% using the T5-base model over the state-of-the-art Multimodal-CoT on the ScienceQA test set. Our code is publicly available at https://github.com/Zoeyyao27/Graph-of-Thought</abstract>
      <url hash="b241783f">2024.findings-naacl.183</url>
      <bibkey>yao-etal-2024-got</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.183</doi>
    </paper>
    <paper id="184">
      <title>Enhancing the General Agent Capabilities of Low-Paramter <fixed-case>LLM</fixed-case>s through Tuning and Multi-Branch Reasoning</title>
      <author><first>Qinhao</first><last>Zhou</last></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Xiang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Ke</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuchuan</first><last>Wu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>2922-2931</pages>
      <abstract>Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.</abstract>
      <url hash="2c80a5f3">2024.findings-naacl.184</url>
      <bibkey>zhou-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.184</doi>
    </paper>
    <paper id="185">
      <title><fixed-case>M</fixed-case>u<fixed-case>M</fixed-case>ath: Multi-perspective Data Augmentation for Mathematical Reasoning in Large Language Models</title>
      <author><first>Weihao</first><last>You</last></author>
      <author><first>Shuo</first><last>Yin</last></author>
      <author><first>Xudong</first><last>Zhao</last></author>
      <author><first>Zhilong</first><last>Ji</last><affiliation>Tomorrow Advancing Life</affiliation></author>
      <author><first>Guoqiang</first><last>Zhong</last><affiliation>Ocean University of China</affiliation></author>
      <author><first>Jinfeng</first><last>Bai</last></author>
      <pages>2932-2958</pages>
      <abstract>Recently, the tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs. However, these models fall short in demonstrating the calculation process, which compromises user-friendliness and understanding of problem-solving steps. Conversely, while tool-free methods offer a clear display of the problem-solving process, their accuracy leaves room for improvement.These tool-free methods typically employ a somewhat narrow range of augmentation techniques such as rephrasing and difficulty enhancement to boost performance. In response to this issue, we have amalgamated and further refined these strengths while broadening the scope of augmentation methods to construct a **mu**lti-perspective augmentation dataset for **math**ematics—termed **MuMath** (<tex-math>\mu</tex-math>-Math) Dataset.Subsequently, we finetune LLaMA-2 on the MuMath dataset to derive the MuMath model. Our experiments indicate that our MuMath-70B model achieves new state-of-the-art performance among tool-free methods—achieving 88.3% on GSM8K and 34.5% on MATH .We release the MuMath dataset along with its corresponding models and code for public use.</abstract>
      <url hash="8ed741c2">2024.findings-naacl.185</url>
      <bibkey>you-etal-2024-mumath</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.185</doi>
    </paper>
    <paper id="186">
      <title>Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization</title>
      <author><first>Tong</first><last>Ye</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Lingfei</first><last>Wu</last><affiliation>Anytime AI and Pinterest</affiliation></author>
      <author><first>Tengfei</first><last>Ma</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Xuhong</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yangkai</first><last>Du</last></author>
      <author><first>Peiyu</first><last>Liu</last></author>
      <author><first>Shouling</first><last>Ji</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Wenhai</first><last>Wang</last></author>
      <pages>2959-2971</pages>
      <abstract>Automatically generating human-readable text describing the functionality of a program is the intent of source code summarization. Although neural language models achieve significant performance in this field, they are limited by their inability to access external knowledge. To address this limitation, an emerging trend is combining neural models with external knowledge through retrieval methods. Previous methods have relied on the sentence-level retrieval paradigm on the encoder side. However, this paradigm is coarse-grained, noise-filled and cannot directly take advantage of the high-quality retrieved summary tokens on the decoder side. In this paper, we propose a fine-grained Token-level retrieval-augmented mechanism (Tram) on the decoder side rather than the encoder side to enhance the performance of neural models and produce more low-frequency tokens in generating summaries. Furthermore, to overcome the challenge of token-level retrieval in capturing contextual code semantics, we also propose integrating code semantics into individual summary tokens. The results of extensive experiments and human evaluation show that our token-level retrieval-augmented approach significantly improves performance and is more interpretable.</abstract>
      <url hash="d4603b5f">2024.findings-naacl.186</url>
      <bibkey>ye-etal-2024-tram</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.186</doi>
    </paper>
    <paper id="187">
      <title><fixed-case>UNO</fixed-case>-<fixed-case>DST</fixed-case>: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking</title>
      <author><first>Chuang</first><last>Li</last></author>
      <author><first>Yan</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Min-Yen</first><last>Kan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>2972-2983</pages>
      <abstract>Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, but ignore unlabelled data in the target domain.We transform zero-shot DST into few-shot DST by utilising such unlabelled data via joint and self-training methods. Our method incorporates auxiliary tasks that generate slot types as inverse prompts for main tasks, creating slot values during joint training. Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning. This approach also facilitates automatic label creation, thereby optimizing the training and fine-tuning of DST models. We demonstrate this method’s effectiveness on general language models in zero-shot scenarios, improving average joint goal accuracy by 8% across all domains in MultiWOZ.</abstract>
      <url hash="954e50f3">2024.findings-naacl.187</url>
      <bibkey>li-etal-2024-uno</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.187</doi>
    </paper>
    <paper id="188">
      <title>Evaluating Step-by-Step Reasoning through Symbolic Verification</title>
      <author><first>YiFan</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Hanlin</first><last>Zhang</last><affiliation>Harvard University</affiliation></author>
      <author><first>Li</first><last>Li</last><affiliation>Amazon and Columbia University</affiliation></author>
      <author><first>Eric</first><last>Xing</last><affiliation>Mohamed bin Zayed Univeristy of AI and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>2984-3002</pages>
      <abstract>Pre-trained language models (LMs) have shown remarkable reasoning performance using explanations or chain-of-thoughts (CoT)) for in-context learning. On the other hand, these reasoning tasks are usually presumed to be more approachable for symbolic programming. To understand the mechanism of reasoning of LMs, we curate synthetic datasets containing equivalent (natural, symbolic) data pairs, where symbolic examples contain first-order logic rules and predicates from non-parametric knowledge bases (KBs), supporting automated verification of intermediate reasoning results. Then we revisit neuro-symbolic approaches and propose to learn from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog’s backward chaining algorithm and supporting automated verification of LMs’ outputs. Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more than 25% higher accuracy than CoT on length generalization benchmarks even with smaller model sizes.</abstract>
      <url hash="34aa5d25">2024.findings-naacl.188</url>
      <bibkey>zhang-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.188</doi>
    </paper>
    <paper id="189">
      <title>Multi-Review Fusion-in-Context</title>
      <author><first>Aviv</first><last>Slobodkin</last></author>
      <author><first>Ori</first><last>Shapira</last><affiliation>OriginAI</affiliation></author>
      <author><first>Ran</first><last>Levy</last><affiliation>Amazon</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>3003-3021</pages>
      <abstract>Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness.Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting. Concretely, we formalize Fusion-in-Context (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information.Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which strongly correlate to human judgment. Several baseline models exhibit promising outcomes and provide insightful analyses.This study lays the groundwork for further exploration of modular text generation in the multi-document setting, offering potential improvements in the quality and reliability of generated content. Our benchmark, FuseReviews, including the dataset, evaluation framework, and designated leaderboard, can be found at https://fusereviews.github.io/.</abstract>
      <url hash="694d0f72">2024.findings-naacl.189</url>
      <bibkey>slobodkin-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.189</doi>
    </paper>
    <paper id="190">
      <title>Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison</title>
      <author><first>Maxime</first><last>Bouthors</last></author>
      <author><first>Josep</first><last>Crego</last><affiliation>SYSTRAN</affiliation></author>
      <author><first>François</first><last>Yvon</last><affiliation>ISIR, Sorbonne Université &amp; CNRS</affiliation></author>
      <pages>3022-3039</pages>
      <abstract>Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process. While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored. In this paper, we study the effect of varying retrieval methods for several translation architectures to better understand the interplay between these two processes.We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning. Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures. We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board.</abstract>
      <url hash="c8131b19">2024.findings-naacl.190</url>
      <bibkey>bouthors-etal-2024-retrieving</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.190</doi>
    </paper>
    <paper id="191">
      <title>Extending Input Contexts of Language Models through Training on Segmented Sequences</title>
      <author><first>Petros</first><last>Karypis</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <author><first>George</first><last>Karypis</last><affiliation>University of Minnesota, Minneapolis</affiliation></author>
      <pages>3040-3052</pages>
      <abstract>Effectively training language models on longinputs poses many technical challenges. As acost consideration, languages models are pre-trained on a fixed sequence length before beingadapted to longer sequences. We explore var-ious methods for adapting models to longerinputs by training on segmented sequences andan interpolation-based method for extendingabsolute positional embeddings. We developa training procedure to extend the input con-text size of pretrained models with no architec-tural changes and no additional memory coststhan training on the original input lengths. Bysub-sampling segments from long inputs whilemaintaining their original position the model isable to learn new positional interactions. Ourmethod benefits both models trained with abso-lute positional embeddings, by extending theirinput contexts, as well as popular relative posi-tional embedding methods showing a reducedperplexity on sequences longer than they weretrained on. We demonstrate our method canextend input contexts by a factor of 4× whileimproving perplexity.</abstract>
      <url hash="b54791e1">2024.findings-naacl.191</url>
      <bibkey>karypis-etal-2024-extending</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.191</doi>
    </paper>
    <paper id="192">
      <title>Reason from Fallacy: Enhancing Large Language Models’ Logical Reasoning through Logical Fallacy Understanding</title>
      <author><first>Yanda</first><last>Li</last></author>
      <author><first>Dixuan</first><last>Wang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Guochao</first><last>Jiang</last></author>
      <author><first>Qianyu</first><last>He</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Deqing</first><last>Yang</last><affiliation>Fudan University</affiliation></author>
      <pages>3053-3066</pages>
      <abstract>Large Language Models (LLMs) have demonstrated good performance in many reasoning tasks, but they still struggle with some complicated reasoning tasks including logical reasoning. One non-negligible reason for LLMs’ suboptimal performance on logical reasoning is their overlooking of understanding logical fallacies correctly. To evaluate LLMs’ capability of logical fallacy understanding (LFU), we propose five concrete tasks from three cognitive dimensions of WHAT, WHY, and HOW in this paper. Towards these LFU tasks, we have successfully constructed a new dataset LFUD based on GPT-4 accompanied by a little human effort. Our extensive experiments justify that our LFUD can be used not only to evaluate LLMs’ LFU capability, but also to fine-tune LLMs to obtain significantly enhanced performance on logical reasoning.</abstract>
      <url hash="7ce6f73b">2024.findings-naacl.192</url>
      <bibkey>li-etal-2024-reason</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.192</doi>
    </paper>
    <paper id="193">
      <title>Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models</title>
      <author><first>Wanyong</first><last>Feng</last></author>
      <author><first>Jaewook</first><last>Lee</last></author>
      <author><first>Hunter</first><last>McNichols</last></author>
      <author><first>Alexander</first><last>Scarlatos</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Digory</first><last>Smith</last><affiliation>Eedi</affiliation></author>
      <author><first>Simon</first><last>Woodhead</last><affiliation>Eedi</affiliation></author>
      <author><first>Nancy</first><last>Ornelas</last></author>
      <author><first>Andrew</first><last>Lan</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>3067-3082</pages>
      <abstract>Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconceptions among real students.</abstract>
      <url hash="ff03c259">2024.findings-naacl.193</url>
      <bibkey>feng-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.193</doi>
    </paper>
    <paper id="194">
      <title>Aspect-based Sentiment Analysis with Context Denoising</title>
      <author><first>Yuanhe</first><last>Tian</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Yongdong</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>3083-3095</pages>
      <abstract>Given a sentence and a particular aspect term, aspect-based sentiment analysis (ABSA) aims to predict the sentiment polarity towards this aspect term, which provides fine-grained analysis on sentiment understanding and it has attracted much attention in recent years. In order to achieve a good performance on ABSA, it is important for a model to appropriately encode contextual information, especially identifying salient features and eliminating noise in the context. To make incorrect predictions, most existing approaches employ powerful text encoders to locate important context features, as well as noises that mislead ABSA models. These approaches determine the noise in the text for ABSA by assigning low weights to context features or directly removing them from model input, which runs the risk of computing wrong weights or eliminating important context information. In this paper, we propose to improve ABSA with context denoising, where three types of word-level information are regarded as noise, namely, lexicographic noise, bag-of-words noise, and syntax noise. We utilize diffusion networks to perform the denoising process to gradually eliminate them so as to better predict sentiment polarities for given aspect terms. Our approach uses task-specific noise rather than the standard stochastic Gaussian noise in the diffusion networks. The experimental results on five widely used ABSA datasets demonstrate the validity and effectiveness of our approach.</abstract>
      <url hash="331292d4">2024.findings-naacl.194</url>
      <bibkey>tian-etal-2024-aspect</bibkey>
      <revision id="1" href="2024.findings-naacl.194v1" hash="b9840e25"/>
      <revision id="2" href="2024.findings-naacl.194v2" hash="331292d4" date="2024-07-14">Typo fixes.</revision>
      <doi>10.18653/v1/2024.findings-naacl.194</doi>
    </paper>
    <paper id="195">
      <title><fixed-case>I</fixed-case>ru<fixed-case>M</fixed-case>ozhi: Automatically classifying diglossia in <fixed-case>T</fixed-case>amil</title>
      <author><first>Kabilan</first><last>Prasanna</last></author>
      <author><first>Aryaman</first><last>Arora</last></author>
      <pages>3096-3103</pages>
      <abstract>Tamil, a Dravidian language of South Asia, is a highly diglossic language with two very different registers in everyday use: Literary Tamil (preferred in writing and formal communication) and Spoken Tamil (confined to speech and informal media). Spoken Tamil is under-studied in modern NLP systems compared to Literary Tamil written in the Tamil script, as evidenced by a lack of datasets explicitly targetting the Spoken variety. In this paper, we release IruMozhi, a human-translated dataset of parallel text in Literary and Spoken Tamil. Using IruMozhi, we train classifiers on the task of identifying which Tamil variety a text belongs to. We use these models to gauge the availability of pretraining data in Spoken Tamil, to audit the composition of existing labelled datasets for Tamil, and to encourage future work on the variety.</abstract>
      <url hash="9fef6f47">2024.findings-naacl.195</url>
      <bibkey>prasanna-arora-2024-irumozhi</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.195</doi>
    </paper>
    <paper id="196">
      <title><fixed-case>RENOVI</fixed-case>: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations</title>
      <author><first>Haolan</first><last>Zhan</last><affiliation>Monash University</affiliation></author>
      <author><first>Zhuang</first><last>Li</last><affiliation>Monash University</affiliation></author>
      <author><first>Xiaoxi</first><last>Kang</last></author>
      <author><first>Tao</first><last>Feng</last><affiliation>Monash University</affiliation></author>
      <author><first>Yuncheng</first><last>Hua</last></author>
      <author><first>Lizhen</first><last>Qu</last><affiliation>Monash University</affiliation></author>
      <author><first>Yi</first><last>Ying</last><affiliation>Binus University</affiliation></author>
      <author><first>Mei Rianto</first><last>Chandra</last><affiliation>Binus University</affiliation></author>
      <author><first>Kelly</first><last>Rosalin</last></author>
      <author><first>Jureynolds</first><last>Jureynolds</last><affiliation>Binus University</affiliation></author>
      <author><first>Suraj</first><last>Sharma</last></author>
      <author><first>Shilin</first><last>Qu</last></author>
      <author><first>Linhao</first><last>Luo</last></author>
      <author><first>Ingrid</first><last>Zukerman</last><affiliation>Monash University</affiliation></author>
      <author><first>Lay-Ki</first><last>Soon</last><affiliation>Monash University</affiliation></author>
      <author><first>Zhaleh</first><last>Semnani Azad</last><affiliation>California State University, Northridge</affiliation></author>
      <author><first>Reza</first><last>Haf</last><affiliation>Monash University</affiliation></author>
      <pages>3104-3117</pages>
      <abstract>Norm violations occur when individuals fail to conform to culturally accepted behaviors, which may lead to potential conflicts. Remediating norm violations requires social awareness and cultural sensitivity of the nuances at play. To equip interactive AI systems with a remediation ability, we offer ReNoVi — a large-scale corpus of 9,258 multi-turn dialogues annotated with social norms, as well as define a sequence of tasks to help understand and remediate norm violations step by step. ReNoVi consists of two parts: 512 human-authored dialogues (real data), and 8,746 synthetic conversations generated by ChatGPT through prompt learning. While collecting sufficient human-authored data is costly, synthetic conversations provide suitable amounts of data to help mitigate the scarcity of training data, as well as the chance to assess the alignment between LLMs and humans in the awareness of social norms. We thus harness the power of ChatGPT to generate synthetic training data for our task. To ensure the quality of both human-authored and synthetic data, we follow a quality control protocol during data collection. Our experimental results demonstrate the importance of remediating norm violations in socio-cultural conversations, as well as the improvement in performance obtained from synthetic data.</abstract>
      <url hash="d428f192">2024.findings-naacl.196</url>
      <bibkey>zhan-etal-2024-renovi</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.196</doi>
    </paper>
    <paper id="197">
      <title>Human-in-the-Loop Synthetic Text Data Inspection with Provenance Tracking</title>
      <author><first>Hong Jin</first><last>Kang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Fabrice</first><last>Harel-Canada</last></author>
      <author><first>Muhammad Ali</first><last>Gulzar</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Miryung</first><last>Kim</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>3118-3129</pages>
      <abstract>Data augmentation techniques apply transformations to existing texts to generate additional data. The transformations may produce low-quality texts, where the meaning of the text is changed and the text may even be mangled beyond human comprehension. Analyzing the synthetically generated texts and their corresponding labels is slow and demanding. To winnow out texts with incorrect labels, we develop INSPECTOR, a human-in-the-loop data inspection technique. INSPECTOR combines the strengths of provenance tracking techniques with assistive labeling. INSPECTOR allows users to group related texts by their <tex-math>\textit{transformation provenance}</tex-math>, i.e., the transformations applied to the original text, or <tex-math>\textit{feature provenance}</tex-math>, the linguistic features of the original text. For assistive labeling, INSPECTOR computes metrics that approximate data quality, and allows users to compare the corresponding label of each text against the predictions of a large language model. In a user study, INSPECTOR increases the number of texts with correct labels identified by <tex-math>3\times</tex-math> on a sentiment analysis task and by <tex-math>4\times</tex-math> on a hate speech detection task. The participants found grouping the synthetically generated texts by their common transformation to be the most useful technique. Surprisingly, grouping texts by common linguistic features was perceived to be unhelpful. Contrary to prior work, our study finds that no single technique obviates the need for human inspection effort. This validates the design of INSPECTOR which combines both analysis of data provenance and assistive labeling to reduce human inspection effort.</abstract>
      <url hash="a8b17b09">2024.findings-naacl.197</url>
      <bibkey>kang-etal-2024-human</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.197</doi>
    </paper>
    <paper id="198">
      <title><fixed-case>COMMIT</fixed-case>: Code-Mixing <fixed-case>E</fixed-case>nglish-Centric Large Language Model for Multilingual Instruction Tuning</title>
      <author><first>Jaeseong</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>YeonJoon</first><last>Jung</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>3130-3137</pages>
      <abstract>Recently, instruction-tuned large language models (LLMs) are showing prominent performance on various tasks, such as question answering. However, the majority of instruction-tuned LLMs are English-centric, which hinders their application to low-resource language QA. In this paper, we propose COde-Mixed Multilingual Instruction Tuning (COMMIT) to adapt English-centric LLM to low-resource language QA. We point out two main causes of English-centricness: imbalance of unlabeled data, and English-centric instruction tuning datasets. To deviate from English-centric instruction tuning, we propose to specialize code-mixing for instruction tuning, which blocks code-mixing in English templates, to leverage the potential of its superiority. To overcome data imbalance, we perform cross-lingual alignment. The majority of cross-lingual alignment works focused on making representations similar, which is not desirable to decoder-based LLMs, such as LLaMA. Therefore, we propose code-mixed continual causal language modeling to align the decoder. COMMIT improves the exact match score of low-resourced language QA by up to 32x. Code is publicly available.</abstract>
      <url hash="c781ed14">2024.findings-naacl.198</url>
      <bibkey>lee-etal-2024-commit</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.198</doi>
    </paper>
    <paper id="199">
      <title><fixed-case>D</fixed-case>i<fixed-case>LM</fixed-case>: Distilling Dataset into Language Model for Text-level Dataset Distillation</title>
      <author><first>Aru</first><last>Maekawa</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Satoshi</first><last>Kosugi</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Kotaro</first><last>Funakoshi</last><affiliation>Institute of Innovative Research, Tokyo Institute of Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>3138-3153</pages>
      <abstract>Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and in-context learning of large language models. Our code will be available at https://github.com/arumaekawa/DiLM.</abstract>
      <url hash="44382ddd">2024.findings-naacl.199</url>
      <bibkey>maekawa-etal-2024-dilm</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.199</doi>
    </paper>
    <paper id="200">
      <title><fixed-case>M</fixed-case>ind<fixed-case>A</fixed-case>gent: Emergent Gaming Interaction</title>
      <author><first>Ran</first><last>Gong</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Qiuyuan</first><last>Huang</last><affiliation>Microsoft Research, Redmond</affiliation></author>
      <author><first>Xiaojian</first><last>Ma</last></author>
      <author><first>Yusuke</first><last>Noda</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zane</first><last>Durante</last></author>
      <author><first>Zilong</first><last>Zheng</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Demetri</first><last>Terzopoulos</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Li</first><last>Fei-Fei</last><affiliation>Stanford University and Stanford University</affiliation></author>
      <author><first>Jianfeng</first><last>Gao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Hoi</first><last>Vo</last></author>
      <pages>3154-3183</pages>
      <abstract>Large Foundation Models (LFMs) can perform complex scheduling in a multi-agent system and can coordinate agents to complete sophisticated tasks that require extensive collaboration.However, despite the introduction of numerous gaming frameworks, the community lacks adequate benchmarks that support the implementation of a general multi-agent infrastructure encompassing collaboration between LFMs and human-NPCs. We propose a novel infrastructure—Mindagent—for evaluating planning and coordination capabilities in the context of gaming interaction. In particular, our infrastructure leverages an existing gaming framework to (i) act as the coordinator for a multi-agent system, (ii) collaborate with human players via instructions, and (iii) enable in-context learning based on few-shot prompting with feedback.Furthermore, we introduce “Cuisineworld”, a new gaming scenario and its related benchmark that supervises multiple agents playing the game simultaneously and measures multi-agent collaboration efficiency. We have conducted comprehensive evaluations with a new auto-metric Collaboration Score: CoS for assessing the collaboration efficiency. Finally, Mindagent can be deployed in real-world gaming scenarios in a customized VR version of Cuisineworld and adapted in the “Minecraft” domain. Our work involving LFMs within our new infrastructure for general-purpose scheduling and coordination can elucidate how such skills may be obtained by learning from large language corpora.</abstract>
      <url hash="836cfdfc">2024.findings-naacl.200</url>
      <bibkey>gong-etal-2024-mindagent</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.200</doi>
    </paper>
    <paper id="201">
      <title><fixed-case>B</fixed-case>ot<fixed-case>C</fixed-case>hat: Evaluating <fixed-case>LLM</fixed-case>s’ Capabilities of Having Multi-Turn Dialogues</title>
      <author><first>Haodong</first><last>Duan</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jueqi</first><last>Wei</last></author>
      <author><first>Chonghua</first><last>Wang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Hongwei</first><last>Liu</last></author>
      <author><first>Yixiao</first><last>Fang</last><affiliation>Tencent</affiliation></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>3184-3200</pages>
      <abstract>In the realm of modern Large Language Models (LLMs), facilitating high-quality, multi-turn dialogues with humans represents a cornerstone feature. However, human-based evaluation of such a capability involves substantial manual effort. This study offers a formative assessment of current LLMs’ proficiency in emulating human-like, multi-turn conversations using an LLM-centric approach. The evaluation encompasses three key elements in the evaluation pipeline: utterance generation, evaluation protocol, and judgement, and we delve deeply into each aspect. GPT-4, both as an utterance generator and as a judge, exhibits exceptional performance. As a generator, GPT-4 crafts dialogues indistinguishable from human interactions in terms of style and flow. When judging, it shows a heightened alignment with human evaluative standards and consistency. Conversely, other LLMs face challenges in producing quality multi-turn dialogues, hindered by inadequate instruction-following abilities, a propensity for prolix utterances, and overall limited capabilities. Notably, generating extensive dialogues (e.g., spanning tens of turns) remains a formidable task for most LLMs, particularly in Chinese contexts. We hope that our work can serve as a valuable resource for evaluating the multi-turn chatting capabilities of LLMs. Related resources are available at https://github.com/open-compass/BotChat.</abstract>
      <url hash="295fe03e">2024.findings-naacl.201</url>
      <bibkey>duan-etal-2024-botchat</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.201</doi>
    </paper>
    <paper id="202">
      <title>Learning Mutually Informed Representations for Characters and Subwords</title>
      <author><first>Yilin</first><last>Wang</last><affiliation>School of Engineering and Applied Sciences, Harvard University and Carnegie Mellon University</affiliation></author>
      <author><first>Xinyi</first><last>Hu</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Matthew</first><last>Gormley</last><affiliation>Solventum and School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>3201-3213</pages>
      <abstract>Most pretrained language models rely on subword tokenization, which processes text as a sequence of subword tokens. However, different granularities of text, such as characters, subwords, and words, can contain different kinds of information. Previous studies have shown that incorporating multiple input granularities improves model generalization, yet very few of them outputs useful representations for each granularity. In this paper, we introduce the entanglement model, aiming to combine character and subword language models. Inspired by vision-language models, our model treats characters and subwords as separate modalities, and it generates mutually informed representations for both granularities as output. We evaluate our model on text classification, named entity recognition, POS-tagging, and character-level sequence labeling (intraword code-switching). Notably, the entanglement model outperforms its backbone language models, particularly in the presence of noisy texts and low-resource languages. Furthermore, the entanglement model even outperforms larger pre-trained models on all English sequence labeling tasks and classification tasks. We make our code publically available.</abstract>
      <url hash="2c4c8710">2024.findings-naacl.202</url>
      <bibkey>wang-etal-2024-learning-mutually</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.202</doi>
    </paper>
    <paper id="203">
      <title>A Novel Two-step Fine-tuning Framework for Transfer Learning in Low-Resource Neural Machine Translation</title>
      <author><first>Yuan</first><last>Gao</last></author>
      <author><first>Feng</first><last>Hou</last><affiliation>Massey University</affiliation></author>
      <author><first>Ruili</first><last>Wang</last><affiliation>Massey University</affiliation></author>
      <pages>3214-3224</pages>
      <abstract>Existing transfer learning methods for neural machine translation typically use a well-trained translation model (i.e., a parent model) of a high-resource language pair to directly initialize a translation model (i.e., a child model) of a low-resource language pair, and the child model is then fine-tuned with corresponding datasets. In this paper, we propose a novel two-step fine-tuning (TSFT) framework for transfer learning in low-resource neural machine translation. In the first step, we adjust the parameters of the parent model to fit the child language by using the child source data. In the second step, we transfer the adjusted parameters to the child model and fine-tune it with a proposed distillation loss for efficient optimization. Our experimental results on five low-resource translations demonstrate that our framework yields significant improvements over various strong transfer learning baselines. Further analysis demonstrated the effectiveness of different components in our framework.</abstract>
      <url hash="40882b78">2024.findings-naacl.203</url>
      <bibkey>gao-etal-2024-novel</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.203</doi>
    </paper>
    <paper id="204">
      <title>Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment</title>
      <author><first>Zhongtao</first><last>Miao</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Qiyu</first><last>Wu</last><affiliation>The University of Tokyo, Tokyo Institute of Technology and Peking University</affiliation></author>
      <author><first>Kaiyan</first><last>Zhao</last></author>
      <author><first>Zilong</first><last>Wu</last></author>
      <author><first>Yoshimasa</first><last>Tsuruoka</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>3225-3236</pages>
      <abstract>The field of cross-lingual sentence embeddings has recently experienced significant advancements, but research concerning low-resource languages has lagged due to the scarcity of parallel corpora. This paper shows that cross-lingual word representation in low-resource languages is notably under-aligned with that in high-resource languages in current models. To address this, we introduce a novel framework that explicitly aligns words between English and eight low-resource languages, utilizing off-the-shelf word alignment models. This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking. We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on sentence embeddings in low-resource languages. In addition, the competitive performance of the proposed model across a broader range of tasks in high-resource languages underscores its practicality.</abstract>
      <url hash="d6ff2cb3">2024.findings-naacl.204</url>
      <bibkey>miao-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.204</doi>
    </paper>
    <paper id="205">
      <title>C<tex-math>^{3}</tex-math><fixed-case>LPGCN</fixed-case>:Integrating Contrastive Learning and Cooperative Learning with Prompt into Graph Convolutional Network for Aspect-based Sentiment Analysis</title>
      <author><first>Ye</first><last>He</last><affiliation>Chongqing University of Technology</affiliation></author>
      <author><first>Shihao</first><last>Zou</last><affiliation>Chongqing University of Technology</affiliation></author>
      <author><first>YuzheChen</first><last>YuzheChen</last></author>
      <author><first>Xianying</first><last>Huang</last><affiliation>Chongqing University of Technology</affiliation></author>
      <pages>3237-3247</pages>
      <abstract>Aspect-based Sentiment Analysis (ABSA) is a fine-grained task. Recently, using graph convolutional networks (GCNs) to model syntactic information has become a popular topic. In addition, a growing consensus exists to enhance sentence representation using contrastive learning. However, when modeling syntactic information, incorrect syntactic structure may introduce additional noise. Meanwhile, we believe that contrastive learning implicitly introduce label information as priori. Therefore, we propose C<tex-math>^{3}</tex-math>LPGCN, which integrates Contrastive Learning and Cooperative Learning with Prompt into GCN. Specifically, to alleviate the noise when modeling syntactic information, we propose mask-aware aspect information filter, which combines prompt information of template with aspect information to filter the syntactic information. Besides, we propose prompt-based contrastive learning and cooperative learning to utilise the label information further. On the one hand, we construct prompts containing labels for contrastive learning, by which the model can focus more on task-relevant features. On the other hand, cooperative learning further extracts label information by aligning input samples’ representation and output distribution with label samples. Extensive experiments on three datasets demonstrate that our method significantly improves the model’s performance compared to traditional contrastive learning methods. Moreover, our C<tex-math>^{3}</tex-math>LPGCN outperforms state-of-the-art methods. Our source code and final models are publicly available at github</abstract>
      <url hash="f6911b53">2024.findings-naacl.205</url>
      <bibkey>he-etal-2024-c3lpgcn</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.205</doi>
    </paper>
    <paper id="206">
      <title>Visual Enhanced Entity-Level Interaction Network for Multimodal Summarization</title>
      <author><first>Haolong</first><last>Yan</last></author>
      <author><first>Binghao</first><last>Tang</last></author>
      <author><first>Boda</first><last>Lin</last></author>
      <author><first>Gang</first><last>Zhao</last></author>
      <author><first>Si</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>3248-3260</pages>
      <abstract>MultiModal Summarization (MMS) aims to generate a concise summary based on multimodal data like texts and images and has wide application in multimodal fields.Previous works mainly focus on the coarse-level textual and visual features in which the overall features of the image interact with the whole sentence.However, the entities of the input text and the objects of the image may be underutilized, limiting the performance of current MMS models.In this paper, we propose a novel Visual Enhanced Entity-Level Interaction Network (VE-ELIN) to address the problem of underutilization of multimodal inputs at a fine-grained level in two ways.We first design a cross-modal entity interaction module to better fuse the entity information in text and the object information in vision.Then, we design an object-guided visual enhancement module to fully extract the visual features and enhance the focus of the image on the object area.We evaluate VE-ELIN on two MMS datasets and propose new metrics to measure the factual consistency of entities in the output.Finally, experimental results demonstrate that VE-ELIN is effective and outperforms previous methods under both traditional metrics and ours.The source code is available at https://github.com/summoneryhl/VE-ELIN.</abstract>
      <url hash="55230582">2024.findings-naacl.206</url>
      <bibkey>yan-etal-2024-visual</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.206</doi>
    </paper>
    <paper id="207">
      <title>Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning</title>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chuanqi</first><last>Tan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>3261-3280</pages>
      <abstract>Large language models (LLMs) enable in-context learning (ICL) by conditioning on a few labeled training examples as a text-based prompt, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets: the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL:1) injecting knowledge into LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples for ICL with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge.We evaluate the proposed approaches on autoregressive models (e.g., GPT-style LLMs) over multiple text classification and question-answering tasks. Experimental results demonstrate that KICT substantially outperforms strong baselines and improves by more than 13% and 7% on text classification and question-answering tasks, respectively.</abstract>
      <url hash="64b9b8dd">2024.findings-naacl.207</url>
      <bibkey>wang-etal-2024-knowledgeable</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.207</doi>
    </paper>
    <paper id="208">
      <title>Time Machine <fixed-case>GPT</fixed-case></title>
      <author><first>Felix</first><last>Drinkall</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Eghbal</first><last>Rahimikia</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Janet</first><last>Pierrehumbert</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Stefan</first><last>Zohren</last><affiliation>University of Oxford</affiliation></author>
      <pages>3281-3292</pages>
      <abstract>Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata. This approach is not aligned with the evolving nature of language. Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data. This paper presents a new approach: a series of point-in-time LLMs called TimeMachineGPT (TiMaGPT), specifically designed to be nonprognosticative. This ensures they remain uninformed about future factual information and linguistic changes. This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic. We provide access to both the models and training datasets.</abstract>
      <url hash="0202dbd7">2024.findings-naacl.208</url>
      <bibkey>drinkall-etal-2024-time</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.208</doi>
    </paper>
    <paper id="209">
      <title>An End-to-End Submodular Framework for Data-Efficient In-Context Learning</title>
      <author><first>Lilly</first><last>Kumari</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Shengjie</first><last>Wang</last><affiliation>University of Washington, University of Illinois, Urbana Champaign and New York University, Shanghai</affiliation></author>
      <author><first>Arnav</first><last>Das</last><affiliation>University of Washington</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jeff</first><last>Bilmes</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>3293-3308</pages>
      <abstract>Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework <tex-math>\textit{Div-S3}</tex-math> for exemplar selection for ICL. The first stage focuses on data annotation and employs a pool-based active learning approach to select a set of <tex-math>\textit{Div}</tex-math>erse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (<tex-math>\textit{S3}</tex-math>) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show <tex-math>\textit{Div-S3}</tex-math> outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.</abstract>
      <url hash="e673d0c9">2024.findings-naacl.209</url>
      <bibkey>kumari-etal-2024-end</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.209</doi>
    </paper>
    <paper id="210">
      <title>Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer</title>
      <author><first>Hele-Andra</first><last>Kuulmets</last><affiliation>University of Tartu</affiliation></author>
      <author><first>Taido</first><last>Purason</last></author>
      <author><first>Agnes</first><last>Luhtaru</last><affiliation>institute of computer science, University of Tartu</affiliation></author>
      <author><first>Mark</first><last>Fishel</last><affiliation>University of Tartu</affiliation></author>
      <pages>3309-3325</pages>
      <abstract>This paper explores cost-efficient methods to adapt pretrained Large Language Models (LLMs) to new lower-resource languages, with a specific focus on Estonian. Leveraging the Llama 2 model, we investigate the impact of combining cross-lingual instruction-tuning with additional monolingual pretraining. Our results demonstrate that even a relatively small amount of additional monolingual pretraining followed by cross-lingual instruction-tuning significantly enhances results on Estonian. Furthermore, we showcase cross-lingual knowledge transfer from high-quality English instructions to Estonian, resulting in improvements in commonsense reasoning and multi-turn conversation capabilities. Our best model, named Llammas, represents the first open-source instruction-following LLM for Estonian. Additionally, we publish Alpaca-est, the first general task instruction dataset for Estonia. These contributions mark the initial progress in the direction of developing open-source LLMs for Estonian.</abstract>
      <url hash="0005ffd7">2024.findings-naacl.210</url>
      <bibkey>kuulmets-etal-2024-teaching</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.210</doi>
    </paper>
    <paper id="211">
      <title>Simulating Opinion Dynamics with Networks of <fixed-case>LLM</fixed-case>-based Agents</title>
      <author><first>Yun-Shiuan</first><last>Chuang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Agam</first><last>Goyal</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Nikunj</first><last>Harlalka</last></author>
      <author><first>Siddharth</first><last>Suresh</last></author>
      <author><first>Robert</first><last>Hawkins</last><affiliation>Princeton University</affiliation></author>
      <author><first>Sijia</first><last>Yang</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Dhavan</first><last>Shah</last><affiliation>University of Wisconsin - Madison</affiliation></author>
      <author><first>Junjie</first><last>Hu</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Timothy</first><last>Rogers</last></author>
      <pages>3326-3346</pages>
      <abstract>Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.</abstract>
      <url hash="1e5fa174">2024.findings-naacl.211</url>
      <bibkey>chuang-etal-2024-simulating</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.211</doi>
    </paper>
    <paper id="212">
      <title>Probing the Category of Verbal Aspect in Transformer Language Models</title>
      <author><first>Anisia</first><last>Katinskaia</last></author>
      <author><first>Roman</first><last>Yangarber</last><affiliation>University of Helsinki</affiliation></author>
      <pages>3347-3366</pages>
      <abstract>We investigate how pretrained language models (PLM) encode the grammatical category of verbal aspect in Russian. Encoding of aspect in transformer LMs has not been studied previously in any language. A particular challenge is posed by ”alternative contexts”: where either the perfective or the imperfective aspect is suitable grammatically and semantically. We perform probing using BERT and RoBERTa on alternative and non-alternative contexts. First, we assess the models’ performance on aspect prediction, via behavioral probing. Next, we examine the models’ performance when their contextual representations are substituted with counterfactual representations, via causal probing. These counterfactuals alter the value of the “boundedness” feature—a semantic feature, which characterizes the action in the context. Experiments show that BERT and RoBERTa do encode aspect—mostly in their final layers. The counterfactual interventions affect perfective and imperfective in opposite ways, which is consistent with grammar: perfective is positively affected by adding the meaning of boundedness, and vice versa. The practical implications of our probing results are that fine-tuning only the last layers of BERT on predicting aspect is faster and more effective than fine-tuning the whole model. The model has high predictive uncertainty about aspect in alternative contexts, which tend to lack explicit hints about the boundedness of the described action.</abstract>
      <url hash="5bce3c1a">2024.findings-naacl.212</url>
      <bibkey>katinskaia-yangarber-2024-probing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.212</doi>
    </paper>
    <paper id="213">
      <title>A Measure for Transparent Comparison of Linguistic Diversity in Multilingual <fixed-case>NLP</fixed-case> Data Sets</title>
      <author><first>Tanja</first><last>Samardzic</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Ximena</first><last>Gutierrez</last><affiliation>Universidad Nacional Autónoma de México</affiliation></author>
      <author><first>Christian</first><last>Bentz</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <author><first>Steven</first><last>Moran</last><affiliation>University of Miami</affiliation></author>
      <author><first>Olga</first><last>Pelloni</last></author>
      <pages>3367-3382</pages>
      <abstract>Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of languages that are not represented in a data set. Using our method, we analyse a range of popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to ranking these data sets, we find, for example, that (poly)synthetic languages are missing in almost all of them.</abstract>
      <url hash="63889b9e">2024.findings-naacl.213</url>
      <bibkey>samardzic-etal-2024-measure</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.213</doi>
    </paper>
    <paper id="214">
      <title>Beyond Read-Only: Crafting a Comprehensive <fixed-case>C</fixed-case>hinese Text-to-<fixed-case>SQL</fixed-case> Dataset for Database Manipulation and Query</title>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Jinguo</first><last>You</last><affiliation>Kunmimg University of Science and Technology</affiliation></author>
      <author><first>Likun</first><last>Likun</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <pages>3383-3393</pages>
      <abstract>Text-to-SQL aims to convert natural language into structured query language, which is a challenging task. Current research focuses mainly on read operations and ignores other aspects of database operations such as create, update, and delete operations. The benchmark datasets as well as models that have been proposed also fail to cover these operations, limiting the development and practical applications in the field. To bridge this gap, we propose CRUDSQL, a large-scale cross-domain single-table CRUD operations Chinese Text-to-SQL dataset. The dataset contains 10,000 question/SQL pairs involving 625 tables from different domains. To support further research on this dataset, we also propose a baseline method, CRUDParser, which employs a two-phase approach based on BERT and T5 for SQL generation and incorporates two strategies, value matching, and value prompting, for interacting with databases to further improve the performance. The experimental results show that the new operation types bring different challenges for future research, and our approach achieves 67.08% and 83.8% exact set matching accuracy under both read and delete operations in the test set, but only 49.6% and 61.8% under create and update operations. We believe that the proposal of CRUDSQL as well as CRUDParser can provide new directions and possibilities for research and practical applications in the field of Text-to-SQL. The dataset is published at https://github.com/bizard-lab/CRUDSQL.</abstract>
      <url hash="293cd4dd">2024.findings-naacl.214</url>
      <bibkey>chen-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.214</doi>
    </paper>
    <paper id="215">
      <title>Normalizing without Modernizing: Keeping Historical Wordforms of <fixed-case>M</fixed-case>iddle <fixed-case>F</fixed-case>rench while Reducing Spelling Variants</title>
      <author><first>Raphael</first><last>Rubino</last><affiliation>University of Geneva</affiliation></author>
      <author><first>Johanna</first><last>Gerlach</last><affiliation>University of Geneva</affiliation></author>
      <author><first>Jonathan</first><last>Mutal</last></author>
      <author><first>Pierrette</first><last>Bouillon</last><affiliation>University of Geneva</affiliation></author>
      <pages>3394-3402</pages>
      <abstract>Conservation of historical documents benefits from computational methods by alleviating the manual labor related to digitization and modernization of textual content. Languages usually evolve over time and keeping historical wordforms is crucial for diachronic studies and digital humanities. However, spelling conventions did not necessarily exist when texts were originally written and orthographic variations are commonly observed depending on scribes and time periods. In this study, we propose to automatically normalize orthographic wordforms found in historical archives written in Middle French during the 16th century without fully modernizing textual content. We leverage pre-trained models in a low resource setting based on a manually curated parallel corpus and produce additional resources with artificial data generation approaches. Results show that causal language models and knowledge distillation improve over a strong baseline, thus validating the proposed methods.</abstract>
      <url hash="d9353190">2024.findings-naacl.215</url>
      <bibkey>rubino-etal-2024-normalizing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.215</doi>
    </paper>
    <paper id="216">
      <title>Anti-<fixed-case>LM</fixed-case> Decoding for Zero-shot In-context Machine Translation</title>
      <author><first>Suzanna</first><last>Sia</last></author>
      <author><first>Alexandra</first><last>DeLucia</last></author>
      <author><first>Kevin</first><last>Duh</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>3403-3420</pages>
      <abstract>Zero-shot In-context learning is the phenomenon where models can perform a task given only the instructions. However, pre-trained large language models are known to be poorly calibrated for zero-shot tasks. One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on a context. This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation. We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search. The proposed method outperforms other state-of-the-art decoding objectives, with up to 20 BLEU point improvement from the default objective in some settings.</abstract>
      <url hash="b847d0f8">2024.findings-naacl.216</url>
      <bibkey>sia-etal-2024-anti</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.216</doi>
    </paper>
    <paper id="217">
      <title>Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning</title>
      <author><first>Shuai</first><last>Zhao</last><affiliation>Jinan University</affiliation></author>
      <author><first>Leilei</first><last>Gan</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Lingjuan</first><last>Lyu</last><affiliation>Sony Research</affiliation></author>
      <author><first>Meihuizi</first><last>Jia</last></author>
      <author><first>Jinming</first><last>Wen</last></author>
      <pages>3421-3438</pages>
      <abstract>Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean. We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods. Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks.</abstract>
      <url hash="029f7eec">2024.findings-naacl.217</url>
      <bibkey>zhao-etal-2024-defending</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.217</doi>
    </paper>
    <paper id="218">
      <title>Select and Summarize: Scene Saliency for Movie Script Summarization</title>
      <author><first>Rohit</first><last>Saxena</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Frank</first><last>Keller</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>3439-3455</pages>
      <abstract>Abstractive summarization for long-form narrative texts such as movie scripts is challenging due to the computational and memory constraints of current language models. A movie script typically comprises a large number of scenes; however, only a fraction of these scenes are salient, i.e., important for understanding the overall narrative. The salience of a scene can be operationalized by considering it as salient if it is mentioned in the summary. Automatically identifying salient scenes is difficult due to the lack of suitable datasets. In this work, we introduce a scene saliency dataset that consists of human-annotated salient scenes for 100 movies. We propose a two-stage abstractive summarization approach which first identifies the salient scenes in script and then generates a summary using only those scenes. Using QA-based evaluation, we show that our model outperforms previous state-of-the-art summarization methods and reflects the information content of a movie more accurately than a model that takes the whole movie script as input.</abstract>
      <url hash="f06c2e3f">2024.findings-naacl.218</url>
      <bibkey>saxena-keller-2024-select</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.218</doi>
    </paper>
    <paper id="219">
      <title>Don’t be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks</title>
      <author><first>Seunguk</first><last>Yu</last></author>
      <author><first>Juhwan</first><last>Choi</last></author>
      <author><first>YoungBin</first><last>Kim</last><affiliation>Chung-Ang University</affiliation></author>
      <pages>3456-3467</pages>
      <abstract>Offensive language detection is an important task for filtering out abusive expressions and improving online user experiences. However, malicious users often attempt to avoid filtering systems through the involvement of textual noises. In this paper, we propose these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language. Furthermore, we introduce simple yet effective pooling strategies in a layer-wise manner to defend against the proposed attacks, focusing on the preceding layers not just the last layer to capture both offensiveness and token embeddings. We demonstrate that these pooling strategies are more robust to performance degradation even when the attack rate is increased, without directly training of such patterns. Notably, we found that models pre-trained on clean texts could achieve a comparable performance in detecting attacked offensive language, to models pre-trained on noisy texts by employing these pooling strategies.</abstract>
      <url hash="3ab696b5">2024.findings-naacl.219</url>
      <bibkey>yu-etal-2024-dont</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.219</doi>
    </paper>
    <paper id="220">
      <title><fixed-case>Z</fixed-case>-<fixed-case>GMOT</fixed-case>: Zero-shot Generic Multiple Object Tracking</title>
      <author><first>Kim</first><last>Tran</last><affiliation>FPT Software</affiliation></author>
      <author><first>Anh Duy</first><last>Le Dinh</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Tien-Phat</first><last>Nguyen</last><affiliation>John von Neumann</affiliation></author>
      <author><first>Thinh</first><last>Phan</last></author>
      <author><first>Pha</first><last>Nguyen</last><affiliation>University of Arkansas - Fayetteville</affiliation></author>
      <author><first>Khoa</first><last>Luu</last><affiliation>University of Arkansas, Fayetteville</affiliation></author>
      <author><first>Donald</first><last>Adjeroh</last><affiliation>West Virginia University</affiliation></author>
      <author><first>Gianfranco</first><last>Doretto</last><affiliation>West Virginia University</affiliation></author>
      <author><first>Ngan</first><last>Le</last><affiliation>University of Arkansas, Fayetteville</affiliation></author>
      <pages>3468-3479</pages>
      <abstract>Despite recent significant progress, Multi-Object Tracking (MOT) faces limitations such as reliance on prior knowledge and predefined categories and struggles with unseen objects. To address these issues, Generic Multiple Object Tracking (GMOT) has emerged as an alternative approach, requiring less prior information. However, current GMOT methods often rely on initial bounding boxes and struggle to handle variations in factors such as viewpoint, lighting, occlusion, and scale, among others. Our contributions commence with the introduction of the Referring GMOT dataset a collection of videos, each accompanied by detailed textual descriptions of their attributes. Subsequently, we propose Z-GMOT, a cutting-edge tracking solution capable of tracking objects from never-seen categories without the need of initial bounding boxes or predefined categories. Within our Z-GMOT framework, we introduce two novel components: (i) iGLIP, an improved Grounded language-image pretraining, for accurately detecting unseen objects with specific characteristics. (ii) MA-SORT, a novel object association approach that adeptly integrates motion and appearance-based matching strategies to tackle the complex task of tracking objects with high similarity. Our contributions are benchmarked through extensive experiments conducted on the Referring GMOT dataset for GMOT task. Additionally, to assess the generalizability of the proposed Z-GMOT, we conduct ablation studies on the DanceTrack and MOT20 datasets for the MOT task. Our dataset, code, and models are released at: https://fsoft-aic.github.io/Z-GMOT</abstract>
      <url hash="414f9505">2024.findings-naacl.220</url>
      <bibkey>tran-etal-2024-z</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.220</doi>
    </paper>
    <paper id="221">
      <title><fixed-case>NLP</fixed-case> for Counterspeech against Hate: A Survey and How-To Guide</title>
      <author><first>Helena</first><last>Bonaldi</last><affiliation>Fondazione Bruno Kessler and University of Trento</affiliation></author>
      <author><first>Yi-Ling</first><last>Chung</last><affiliation>Alan Turing Institute</affiliation></author>
      <author><first>Gavin</first><last>Abercrombie</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Marco</first><last>Guerini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>3480-3499</pages>
      <abstract>In recent years, counterspeech has emerged as one of the most promising strategies to fight online hate. These non-escalatory responses tackle online abuse while preserving the freedom of speech of the users, and can have a tangible impact in reducing online and offline violence. Recently, there has been growing interest from the Natural Language Processing (NLP) community in addressing the challenges of analysing, collecting, classifying, and automatically generating counterspeech, to reduce the huge burden of manually producing it. In particular, researchers have taken different directions in addressing these challenges, thus providing a variety of related tasks and resources. In this paper, we provide a guide for doing research on counterspeech, by describing - with detailed examples - the steps to undertake, and providing best practices that can be learnt from the NLP studies on this topic. Finally, we discuss open challenges and future directions of counterspeech research in NLP.</abstract>
      <url hash="9c3ef585">2024.findings-naacl.221</url>
      <bibkey>bonaldi-etal-2024-nlp</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.221</doi>
    </paper>
    <paper id="222">
      <title><fixed-case>PRODIG</fixed-case>y: a <fixed-case>PRO</fixed-case>file-based <fixed-case>DI</fixed-case>alogue Generation dataset</title>
      <author><first>Daniela</first><last>Occhipinti</last></author>
      <author><first>Serra Sinem</first><last>Tekiroğlu</last></author>
      <author><first>Marco</first><last>Guerini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>3500-3514</pages>
      <abstract>Providing dialogue agents with a profile representation can improve their consistency and coherence, leading to better conversations. However, current profile-based dialogue datasets for training such agents contain either explicit profile representations that are simple and dialogue-specific, or implicit representations that are difficult to collect. In this work, we introduce the PRODIGy (PROfile-based DIalogue Generation) dataset, which brings diverse representations together, providing a more comprehensive profile dimension set for each speaker. This resource comprises more than 20k dialogues, sourced from movie scripts, aligned with speaker representations such as communication style, biography, personality and gender. Initial experiments with diverse baselines show that providing generative language models with these aspects of a profile, both separately and jointly, enhances models’ performance. This improvement holds true in both in-domain and cross-domain settings, for both fine-tuned and instruction-based LLMs.</abstract>
      <url hash="fe40e0e1">2024.findings-naacl.222</url>
      <bibkey>occhipinti-etal-2024-prodigy</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.222</doi>
    </paper>
    <paper id="223">
      <title><fixed-case>W</fixed-case>ater<fixed-case>J</fixed-case>udge: Quality-Detection Trade-off when Watermarking Large Language Models</title>
      <author><first>Piotr</first><last>Molenda</last></author>
      <author><first>Adian</first><last>Liusie</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <pages>3515-3525</pages>
      <abstract>Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating point that provides a well-balanced performance. This approach is applied to two different summarization systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis.</abstract>
      <url hash="930a9f29">2024.findings-naacl.223</url>
      <bibkey>molenda-etal-2024-waterjudge</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.223</doi>
    </paper>
    <paper id="224">
      <title>Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking</title>
      <author><first>Nan</first><last>Xu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ben</first><last>Zhou</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>Bangzheng</first><last>Li</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Chaowei</first><last>Xiao</last><affiliation>University of Wisconsin - Madison and NVIDIA</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>3526-3548</pages>
      <abstract>While large language models (LLMs) have demonstrated increasing power, they have also called upon studies on their vulnerabilities. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of 1) multilingual cognitive overload, 2) veiled expression, and 3) effect-to- cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overload. Motivated by cognitive psychology work on managing cognitive load, we further investigate defending cognitive overload attack from two perspectives. Empirical studies show that our cognitive overload from three perspectives can jailbreak all studied LLMs successfully, while existing defense strategies can hardly mitigate the caused malicious uses effectively.</abstract>
      <url hash="1602560a">2024.findings-naacl.224</url>
      <bibkey>xu-etal-2024-cognitive</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.224</doi>
    </paper>
    <paper id="225">
      <title><fixed-case>PAELLA</fixed-case>: Parameter-Efficient Lightweight Language-Agnostic Captioning Model</title>
      <author><first>Rita</first><last>Ramos</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Emanuele</first><last>Bugliarello</last><affiliation>Google</affiliation></author>
      <author><first>Bruno</first><last>Martins</last><affiliation>Instituto Superior Técnico</affiliation></author>
      <author><first>Desmond</first><last>Elliott</last><affiliation>and University of Copenhagen</affiliation></author>
      <pages>3549-3564</pages>
      <abstract>We introduce PAELLA, a Parameter-Efficient Lightweight Language-Agnostic image captioning model designed to be both parameter and data-efficient using retrieval augmentation. The model is trained by learning a small mapping network with 34M parameters between a pre-trained visual model and a multilingual language model that is conditioned on two types of input: (i) the image itself, and (ii) a set of retrieved captions in the target language. The retrieved examples play a key role in guiding the model to generate captions across languages. Through retrieval, the model can be lightweight in terms of the number of trainable parameters, which only exist in its mapping network, and also in the amount of multilingual training data that is required. Experiments on the XM3600 dataset, featuring 36 languages, show that PAELLA can outperform or compete against some models with 3–77<tex-math>\times</tex-math> more learned parameters and 35–863<tex-math>\times</tex-math> more data, particularly in low-resource languages. We also find that PAELLA can be trained on only monolingual data and still show strong zero-shot abilities in other languages.</abstract>
      <url hash="b88074a6">2024.findings-naacl.225</url>
      <bibkey>ramos-etal-2024-paella</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.225</doi>
    </paper>
    <paper id="226">
      <title><fixed-case>OSC</fixed-case>a<fixed-case>R</fixed-case>: Object State Captioning and State Change Representation</title>
      <author><first>Nguyen</first><last>Nguyen</last></author>
      <author><first>Jing</first><last>Bi</last></author>
      <author><first>Ali</first><last>Vosoughi</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Yapeng</first><last>Tian</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Pooyan</first><last>Fazli</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Chenliang</first><last>Xu</last><affiliation>University of Rochester, University of Rochester and University of Rochester</affiliation></author>
      <pages>3565-3576</pages>
      <abstract>The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating Multimodal Large Language Models (MLLMs). Our experiments demonstrate that while MLLMs show some skill, they lack a full understanding of object state changes. The benchmark includes a fine-tuned model that, despite initial capabilities, requires significant improvements in accuracy and generalization ability for effective understanding of these changes. Our code and dataset are available at https://github.com/nguyennm1024/OSCaR.</abstract>
      <url hash="f657d847">2024.findings-naacl.226</url>
      <bibkey>nguyen-etal-2024-oscar</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.226</doi>
    </paper>
    <paper id="227">
      <title><fixed-case>S</fixed-case>um<fixed-case>CSE</fixed-case>: Summary as a transformation for Contrastive Learning</title>
      <author><first>Raghuveer</first><last>Thirukovalluru</last></author>
      <author><first>Xiaolan</first><last>Wang</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Jun</first><last>Chen</last><affiliation>Meta Platform</affiliation></author>
      <author><first>Shuyang</first><last>Li</last><affiliation>Meta AI</affiliation></author>
      <author><first>Jie</first><last>Lei</last></author>
      <author><first>Rong</first><last>Jin</last><affiliation>Twitter</affiliation></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>3577-3588</pages>
      <abstract>Sentence embedding models are typically trained using contrastive learning (CL), either using human annotations directly or by repurposing other annotated datasets. In this work, we explore the recently introduced paradigm of generating CL data using generative language models (LM). In CL for computer vision (CV), compositional transformations (series of operations applied over an image. e.g. cropping + color distortion) which modify the input/image to retain minimal information were shown to be very effective. We show that composition of a ‘Summary’ transformation with diverse paraphrasing/contradicting transformations accomplishes the same and works very well in CL for sentence embeddings. Our final generated dataset (using Vicuna-13B) significantly outperforms the previous best unsupervised method (using ChatGPT) by 1.8 points, and SimCSE, a strong supervised baseline by 0.3 points on the semantic text similarity (STS) benchmark.</abstract>
      <url hash="303fcd3d">2024.findings-naacl.227</url>
      <bibkey>thirukovalluru-etal-2024-sumcse</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.227</doi>
    </paper>
    <paper id="228">
      <title>The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text</title>
      <author><first>Yanzhu</first><last>Guo</last></author>
      <author><first>Guokan</first><last>Shang</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Michalis</first><last>Vazirgiannis</last><affiliation>Ecole Polytechnique, France</affiliation></author>
      <author><first>Chloé</first><last>Clavel</last><affiliation>INRIA and Télécom Paris</affiliation></author>
      <pages>3589-3604</pages>
      <abstract>This study investigates the consequences of training language models on synthetic data generated by their predecessors, an increasingly prevalent practice given the prominence of powerful generative models. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time. To assess this, we adapt and develop a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive finetuning experiments across various natural language generation tasks in English. Our findings reveal a consistent decrease in the diversity of the model outputs through successive iterations, especially remarkable for tasks demanding high levels of creativity. This trend underscores the potential risks of training language models on synthetic text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of language models.</abstract>
      <url hash="475970bb">2024.findings-naacl.228</url>
      <bibkey>guo-etal-2024-curious</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.228</doi>
    </paper>
    <paper id="229">
      <title><fixed-case>P</fixed-case>ersona<fixed-case>LLM</fixed-case>: Investigating the Ability of Large Language Models to Express Personality Traits</title>
      <author><first>Hang</first><last>Jiang</last></author>
      <author><first>Xiajie</first><last>Zhang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Xubo</first><last>Cao</last></author>
      <author><first>Cynthia</first><last>Breazeal</last></author>
      <author><first>Deb</first><last>Roy</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jad</first><last>Kabbara</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>3605-3627</pages>
      <abstract>Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas’ self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas’ writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80%. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.</abstract>
      <url hash="87d15af8">2024.findings-naacl.229</url>
      <bibkey>jiang-etal-2024-personallm</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.229</doi>
    </paper>
    <paper id="230">
      <title><fixed-case>FIRE</fixed-case>: A Dataset for Financial Relation Extraction</title>
      <author><first>Hassan</first><last>Hamad</last></author>
      <author><first>Abhinav Kumar</first><last>Thakur</last></author>
      <author><first>Nijil</first><last>Kolleri</last></author>
      <author><first>Sujith</first><last>Pulikodan</last></author>
      <author><first>Keith</first><last>Chugg</last><affiliation>University of Southern California</affiliation></author>
      <pages>3628-3642</pages>
      <abstract>This paper introduces FIRE (**FI**nancial **R**elation **E**xtraction), a sentence-level dataset of named entities and relations within the financial sector. Comprising 3,025 instances, the dataset encapsulates 13 named entity types along with 18 relation types. Sourced from public financial reports and financial news articles, FIRE captures a wide array of financial information about a business including, but not limited to, corporate structure, business model, revenue streams, and market activities such as acquisitions. The full dataset was labeled by a single annotator to minimize labeling noise. The labeling time for each sentence was recorded during the labeling process. We show how this feature, along with curriculum learning techniques, can be used to improved a model’s performance. The FIRE dataset is designed to serve as a valuable resource for training and evaluating machine learning algorithms in the domain of financial information extraction. The dataset and the code to reproduce our experimental results are available at https://github.com/hmhamad/FIRE. The repository for the labeling tool can be found at https://github.com/abhinav-kumar-thakur/relation-extraction-annotator.</abstract>
      <url hash="6be5495d">2024.findings-naacl.230</url>
      <bibkey>hamad-etal-2024-fire</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.230</doi>
    </paper>
    <paper id="231">
      <title><fixed-case>M</fixed-case>usi<fixed-case>L</fixed-case>ingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response</title>
      <author><first>Zihao</first><last>Deng</last></author>
      <author><first>Yinghao</first><last>Ma</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Yudong</first><last>Liu</last></author>
      <author><first>Rongchen</first><last>Guo</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo and Google</affiliation></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Emmanouil</first><last>Benetos</last><affiliation>Queen Mary, University of London</affiliation></author>
      <pages>3643-3655</pages>
      <abstract>Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains not well-explored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT (CITATION) with a frozen LLM, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;A datasets, we created the MusicInstruct (MI) dataset from captions in the MusicCaps datasets, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.</abstract>
      <url hash="ba835227">2024.findings-naacl.231</url>
      <bibkey>deng-etal-2024-musilingo</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.231</doi>
    </paper>
    <paper id="232">
      <title>Investigating Acceleration of <fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case> Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with ‘<fixed-case>LITE</fixed-case>’</title>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Agneet</first><last>Chatterjee</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Mihir</first><last>Parmar</last></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University, Arizona State University and Arizona State University</affiliation></author>
      <pages>3656-3677</pages>
      <abstract>Large Language Models (LLMs) have achieved remarkable performance across a wide variety of tasks; however, their large size makes their inference slow and computationally expensive. Focusing on this problem, we study instruction tuning LLMs with additional explicit Losses from the Intermediate layers (LITE) and show that it enables these layers to acquire ‘good’ generation ability without affecting the generation ability of the final layer. We then perform ‘dynamic confidence-based early exiting’ at token level from the intermediate layers which improves the computational efficiency of text generation without sacrificing the quality of the generation. We conduct comprehensive experiments by instruction tuning LLaMA-2 models on the Alpaca dataset and evaluate on four different instruction test sets. We show that dynamic early exiting achieves consistent and considerable inference cost improvements (37.86% for 7B and 46.35% for 13B model) while maintaining the generation quality. We further conduct a thorough analysis of the results and dissect the efficiency improvements which reveals several important findings.</abstract>
      <url hash="2df42a85">2024.findings-naacl.232</url>
      <bibkey>varshney-etal-2024-investigating</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.232</doi>
    </paper>
    <paper id="233">
      <title>Instruction-following Evaluation through Verbalizer Manipulation</title>
      <author><first>Shiyang</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Jun</first><last>Yan</last></author>
      <author><first>Hai</first><last>Wang</last><affiliation>Samsung</affiliation></author>
      <author><first>Zheng</first><last>Tang</last><affiliation>Samsung</affiliation></author>
      <author><first>Xiang</first><last>Ren</last><affiliation>University of Southern California, University of Southern California and University of Southern California</affiliation></author>
      <author><first>Vijay</first><last>Srinivasan</last></author>
      <author><first>Hongxia</first><last>Jin</last><affiliation>Samsung Research America AI center</affiliation></author>
      <pages>3678-3692</pages>
      <abstract>While instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. Existing benchmarks primarily focus on common instructions that align well with what the model learned during training. However, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. In this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting “positive” for positive sentiment), to minimally aligned (e.g., outputting “negative” for positive sentiment). Verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model’s reliance on priors and its ability to override them to accurately follow the instructions. We conduct a comprehensive evaluation of four major model families across nine datasets, employing twelve sets of verbalizers for each of them. We observe that the instruction-following abilities of models, across different families and scales, are significantly distinguished by their performance on less natural verbalizers. Even the strongest GPT-4 model struggles to perform better than random guessing on the most challenging verbalizer, emphasizing the need for continued advancements to improve their instruction-following abilities.</abstract>
      <url hash="05cd7b01">2024.findings-naacl.233</url>
      <bibkey>li-etal-2024-instruction</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.233</doi>
    </paper>
    <paper id="234">
      <title><fixed-case>W</fixed-case>eb<fixed-case>WISE</fixed-case>: Unlocking Web Interface Control for <fixed-case>LLM</fixed-case>s via Sequential Exploration</title>
      <author><first>Heyi</first><last>Tao</last></author>
      <author><first>Sethuraman</first><last>T V</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Michal</first><last>Shlapentokh-Rothman</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Tanmay</first><last>Gupta</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Derek</first><last>Hoiem</last><affiliation>Department of Computer Science, Reconstruct and University of Illinois, Urbana Champaign</affiliation></author>
      <pages>3693-3711</pages>
      <abstract>This paper investigates using Large Language Models (LLMs) to automatically perform web software tasks using click, scroll, and text in- put operations. Previous approaches, such as reinforcement learning (RL) or imitation learning, are inefficient to train and task-specific. Our method uses filtered Document Object Model (DOM) elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations. We use in-context learning, either benefiting from a single manually provided example, or an automatically generated example based on a successful zero-shot trial. We evaluate our proposed method on the MiniWob++ benchmark. With only one in-context example, our WebWISE method using gpt-3.5-turbo achieves similar or better performance than other methods that require many demonstrations or trials.</abstract>
      <url hash="d31e9d09">2024.findings-naacl.234</url>
      <bibkey>tao-etal-2024-webwise</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.234</doi>
    </paper>
    <paper id="235">
      <title><fixed-case>C</fixed-case>odec<fixed-case>LM</fixed-case>: Aligning Language Models with Tailored Synthetic Data</title>
      <author><first>Zifeng</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Chun-Liang</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Vincent</first><last>Perot</last><affiliation>Google</affiliation></author>
      <author><first>Long</first><last>Le</last><affiliation>Google</affiliation></author>
      <author><first>Jin</first><last>Miao</last><affiliation>Google</affiliation></author>
      <author><first>Zizhao</first><last>Zhang</last><affiliation>Google</affiliation></author>
      <author><first>Chen-Yu</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>3712-3729</pages>
      <abstract>Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next-token prediction objective and users’ actual goals. To reduce the labor and time cost to collect or annotate data by humans, researchers start to explore the use of LLMs to generate instruction-aligned synthetic data. Recent works focus on generating diverse instructions and applying LLM to increase instruction complexity, often neglecting downstream use cases. It remains unclear how to tailor high-quality data to elicit better instruction-following abilities in different target instruction distributions and LLMs. To this end, we introduce CodecLM, a general framework for adaptively generating high-quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs. Drawing on the Encode-Decode principles, we use LLMs as codecs to guide the data generation process. We first encode seed instructions into metadata, which are concise keywords generated on-the-fly to capture the target instruction distribution, and then decode metadata to create tailored instructions. We also introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient samples. Extensive experiments on four open-domain instruction following benchmarks validate the effectiveness of CodecLM over the current state-of-the-arts.</abstract>
      <url hash="7e7e4e73">2024.findings-naacl.235</url>
      <bibkey>wang-etal-2024-codeclm</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.235</doi>
    </paper>
    <paper id="236">
      <title>Prompting Few-shot Multi-hop Question Generation via Comprehending Type-aware Semantics</title>
      <author><first>Zefeng</first><last>Lin</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Weidong</first><last>Chen</last></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yongdong</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>3730-3740</pages>
      <abstract>Given several documents, multi-hop question generation (MQG) is a task aims to generate complicated questions that require reasoning over multiple pieces of these documents to find the answer. To perform this task, existing studies focus on designing advanced architectures to locate essential keywords or sentences in multiple documents and then generate questions accordingly, where they normally do not note that question types could provide crucial hints for extracting key information from the documents for MQG. In general, supervised approaches are used that rely on large annotated data, which is not available in many low-resource scenarios and thus makes MQG hard in these domains. Consider the recent success of large language models (LLMs) on natural language processing tasks using limited labeled data under few-shot settings, in this paper, we propose an approach named type-aware semantics extraction-based chain-of-thought method (TASE-CoT) for few-shot MQG. Specifically, our approach firstly extracts question types and essential semantic phrases from the given documents and the answer. Then, we design a three-step CoT template to leverage the extracted question type and semantic phrases to predict multi-hop questions. Extensive experiments and the results demonstrate the effectiveness of our approach and the proposed modules.</abstract>
      <url hash="bc70ae10">2024.findings-naacl.236</url>
      <bibkey>lin-etal-2024-prompting</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.236</doi>
    </paper>
    <paper id="237">
      <title>When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models</title>
      <author><first>Yanhong</first><last>Li</last></author>
      <author><first>Chenghao</first><last>Yang</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Allyson</first><last>Ettinger</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>3741-3753</pages>
      <abstract>Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs’ ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA.We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models’ initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.</abstract>
      <url hash="1eafa3fa">2024.findings-naacl.237</url>
      <bibkey>li-etal-2024-hindsight</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.237</doi>
    </paper>
    <paper id="238">
      <title><fixed-case>C</fixed-case>o<fixed-case>D</fixed-case>a: Constrained Generation based Data Augmentation for Low-Resource <fixed-case>NLP</fixed-case></title>
      <author><first>Chandra Kiran</first><last>Evuru</last></author>
      <author><first>Sreyan</first><last>Ghosh</last></author>
      <author><first>Sonal</first><last>Kumar</last></author>
      <author><first>Ramaneswaran</first><last>S</last></author>
      <author><first>Utkarsh</first><last>Tyagi</last></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>3754-3769</pages>
      <abstract>We present CoDa (**Co**nstrained Generation based **Da**ta Augmentation), a controllable, effective, and *training-free* data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control over the augmentation generation process, thereby also allowing easy adaptation to several domains. We demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3 low-resource settings. CoDa outperforms all our baselines, qualitatively and quantitatively, with improvements of 0.12%-7.19%. Code is available.</abstract>
      <url hash="b04e3869">2024.findings-naacl.238</url>
      <bibkey>evuru-etal-2024-coda</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.238</doi>
    </paper>
    <paper id="239">
      <title>Synonym relations affect object detection learned on vision-language data</title>
      <author><first>Giacomo</first><last>Nebbia</last></author>
      <author><first>Adriana</first><last>Kovashka</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>3770-3776</pages>
      <abstract>We analyze whether object detectors trained on vision-language data learn effective visual representations for synonyms. Since many current vision-language models accept user-provided textual input, we highlight the need for such models to learn feature representations that are robust to changes in how such input is provided. Specifically, we analyze changes in synonyms used to refer to objects. Here, we study object detectors trained on vision-language data and investigate how to make their performance less dependent on whether synonyms are used to refer to an object. We propose two approaches to achieve this goal: data augmentation by back-translation and class embedding enrichment. We show the promise of such approaches, reporting improved performance on synonyms from mAP@0.5=33.87% to 37.93%.</abstract>
      <url hash="dde5d02e">2024.findings-naacl.239</url>
      <bibkey>nebbia-kovashka-2024-synonym</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.239</doi>
    </paper>
    <paper id="240">
      <title><fixed-case>CM</fixed-case>-<fixed-case>TTS</fixed-case>: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models</title>
      <author><first>Xiang</first><last>Li</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>FanBu</first><last>FanBu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Ambuj</first><last>Mehrish</last></author>
      <author><first>Yingting</first><last>Li</last></author>
      <author><first>Jiale</first><last>Han</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Bo</first><last>Cheng</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>3777-3794</pages>
      <abstract>Neural Text-to-Speech (TTS) systems find broad applications in voice assistants, e-learning, and audiobook creation. The pursuit of modern models, like Diffusion Models (DMs), holds promise for achieving high-fidelity, real-time speech synthesis. Yet, the efficiency of multi-step sampling in Diffusion Models presents challenges. Efforts have been made to integrate GANs with DMs, speeding up inference by approximating denoising distributions, but this introduces issues with model convergence due to adversarial training. To overcome this, we introduce CM-TTS, a novel architecture grounded in consistency models (CMs). Drawing inspiration from continuous-time diffusion models, CM-TTS achieves top-quality speech synthesis in fewer steps without adversarial training or pre-trained model dependencies. We further design weighted samplers to incorporate different sampling positions into model training with dynamic probabilities, ensuring unbiased learning throughout the entire training process. We present a real-time mel-spectrogram generation consistency model, validated through comprehensive evaluations. Experimental results underscore CM-TTS’s superiority over existing single-step speech synthesis systems, representing a significant advancement in the field.</abstract>
      <url hash="2d96c4bf">2024.findings-naacl.240</url>
      <bibkey>li-etal-2024-cm</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.240</doi>
    </paper>
    <paper id="241">
      <title><fixed-case>R</fixed-case>obust<fixed-case>S</fixed-case>ent<fixed-case>E</fixed-case>mbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning</title>
      <author><first>Javad</first><last>Rafiei Asl</last></author>
      <author><first>Prajwal</first><last>Panzade</last><affiliation>Georgia State University</affiliation></author>
      <author><first>Eduardo</first><last>Blanco</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Daniel</first><last>Takabi</last><affiliation>Old Dominion University</affiliation></author>
      <author><first>Zhipeng</first><last>Cai</last><affiliation>Georgia State University</affiliation></author>
      <pages>3795-3809</pages>
      <abstract>Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing the BERTAttack success rate by almost half (from 75.51% to 38.81%). The framework also yields improvements of 1.59% and 0.23% in semantic textual similarity tasks and various transfer tasks, respectively.</abstract>
      <url hash="0af59288">2024.findings-naacl.241</url>
      <bibkey>rafiei-asl-etal-2024-robustsentembed</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.241</doi>
    </paper>
    <paper id="242">
      <title>Characterizing Human and Zero-Shot <fixed-case>GPT</fixed-case>-3.5 Object-Similarity Judgments</title>
      <author><first>D</first><last>McKnight</last></author>
      <author><first>Alona</first><last>Fyshe</last><affiliation>University of Alberta</affiliation></author>
      <pages>3810-3828</pages>
      <abstract>Recent advancements in large language models’ (LLMs) capabilities have yielded few-shot, human-comparable performance on a range of tasks. At the same time, researchers expend significant effort and resources gathering human annotations. At some point, LLMs may be able to perform some simple annotation tasks, but studies of LLM annotation accuracy and behavior are sparse. In this paper, we characterize OpenAI’s GPT-3.5’s judgment on a behavioral task for implicit object categorization. We characterize the embedding spaces of models trained on human vs. GPT responses and give similarities and differences between them, finding many similar dimensions. We also find that despite these similar dimensions, augmenting humans’ responses with GPT ones drives model divergence across the sizes of datasets tested.</abstract>
      <url hash="eb32507c">2024.findings-naacl.242</url>
      <bibkey>mcknight-fyshe-2024-characterizing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.242</doi>
    </paper>
    <paper id="243">
      <title>Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models</title>
      <author><first>Wei</first><last>He</last><affiliation>Fudan University</affiliation></author>
      <author><first>Shichun</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Yiwen</first><last>Ding</last></author>
      <author><first>Yi</first><last>Lu</last></author>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>3829-3845</pages>
      <abstract>Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos’s generalization and provide more insights.</abstract>
      <url hash="1f7aafc7">2024.findings-naacl.243</url>
      <bibkey>he-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.243</doi>
    </paper>
    <paper id="244">
      <title>Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning</title>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>Zoom</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>3846-3868</pages>
      <abstract>Event temporal reasoning aims at identifying the temporal relations between two or more events from narratives. However, knowledge conflicts arise when there is a mismatch between the actual temporal relations of events in the context and the prior knowledge or biases learned by the model. In this paper, we propose to detect knowledge-conflict examples in event temporal reasoning using bias indicators, which include event relation prior bias, tense bias, narrative bias, and dependency bias. We define conflict examples as those where event relations are opposite to biased or prior relations. To mitigate event-related knowledge conflicts, we introduce a Counterfactual Data Augmentation (CDA) based method that can be applied to both Pre-trained Language Models (PLMs) and Large Language Models (LLMs) either as additional training data or demonstrations for In- Context Learning. Experiments suggest both PLMs and LLMs suffer from knowledge conflicts in event temporal reasoning, and CDA has the potential for reducing hallucination and improving model performance.</abstract>
      <url hash="df691a13">2024.findings-naacl.244</url>
      <bibkey>fang-etal-2024-getting</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.244</doi>
    </paper>
    <paper id="245">
      <title><fixed-case>MCECR</fixed-case>: A Novel Dataset for Multilingual Cross-Document Event Coreference Resolution</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Viet</first><last>Lai</last><affiliation>Kensho Technologies</affiliation></author>
      <author><first>Chien</first><last>Nguyen</last><affiliation>University of Oregon</affiliation></author>
      <author><first>Franck</first><last>Dernoncourt</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Thien</first><last>Nguyen</last><affiliation>, University of Oregon</affiliation></author>
      <pages>3869-3880</pages>
      <abstract>Event coreference resolution (ECR) is a critical task in information extraction of natural language processing, aiming to identify and link event mentions across multiple documents. Despite recent progress, existing datasets for ECR primarily focus on within-document event coreference and English text, lacking cross-document ECR datasets for multiple languages beyond English. To address this issue, this work presents the first multiligual dataset for cross-document ECR, called MCECR (Multilingual Cross-Document Event Coreference Resolution), that manually annotates a diverse collection of documents for event mentions and coreference in five languages, i.e., English, Spanish, Hindi, Turkish, and Ukrainian. Using sampled articles from Wikinews over various topics as the seeds, our dataset fetches related news articles from the Google search engine to increase the number of non-singleton event clusters. In total, we annotate 5,802 news articles, providing a substantial and varied dataset for multilingual ECR in both within-document and cross-document scenarios. Extensive analysis of the proposed dataset reveals the challenging nature of multilingual event coreference resolution tasks, promoting MCECR as a strong benchmark dataset for future research in this area.</abstract>
      <url hash="5256ffb1">2024.findings-naacl.245</url>
      <bibkey>pouran-ben-veyseh-etal-2024-mcecr</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.245</doi>
    </paper>
    <paper id="246">
      <title>Sentiment Analysis in the Era of Large Language Models: A Reality Check</title>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Yue</first><last>Deng</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Bing</first><last>Liu</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Sinno</first><last>Pan</last><affiliation>Nanyang Technological University and The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <pages>3881-3906</pages>
      <abstract>Sentiment analysis (SA) has been a long-standing research area in natural language processing. With the recent advent of large language models (LLMs), there is great potential for their employment on SA problems. However, the extent to which current LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring a deeper understanding of specific sentiment phenomena or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs’ SA abilities and propose a novel benchmark, SentiEval, for a more comprehensive and realistic evaluation. Data and code are available at <url>https://github.com/DAMO-NLP-SG/LLM-Sentiment</url>.</abstract>
      <url hash="fa430209">2024.findings-naacl.246</url>
      <bibkey>zhang-etal-2024-sentiment</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.246</doi>
    </paper>
    <paper id="247">
      <title>Tokenizer Choice For <fixed-case>LLM</fixed-case> Training: Negligible or Crucial?</title>
      <author><first>Mehdi</first><last>Ali</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Michael</first><last>Fromm</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Klaudia</first><last>Thellmann</last><affiliation>TU Dresden</affiliation></author>
      <author><first>Richard</first><last>Rutmann</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Max</first><last>Lübbering</last><affiliation>Fraunhofer IAIS</affiliation></author>
      <author><first>Johannes</first><last>Leveling</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Katrin</first><last>Klug</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Jan</first><last>Ebert</last><affiliation>Forschungszentrum Jülich GmbH</affiliation></author>
      <author><first>Niclas</first><last>Doll</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Jasper</first><last>Buschhoff</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Charvi</first><last>Jain</last></author>
      <author><first>Alexander</first><last>Weber</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Lena</first><last>Jurkschat</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Hammam</first><last>Abdelwahab</last><affiliation>Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <author><first>Chelsea</first><last>John</last><affiliation>Forschungszentrum Juelich GmbH</affiliation></author>
      <author><first>Pedro</first><last>Ortiz Suarez</last><affiliation>Common Crawl Foundation</affiliation></author>
      <author><first>Malte</first><last>Ostendorff</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Samuel</first><last>Weinbach</last><affiliation>Aleph Alpha GmbH</affiliation></author>
      <author><first>Rafet</first><last>Sifa</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Stefan</first><last>Kesselheim</last><affiliation>Forschungszentrum Jülich</affiliation></author>
      <author><first>Nicolas</first><last>Flores-Herr</last><affiliation>Max-Planck Institute and Fraunhofer Institute IAIS, Fraunhofer IAIS</affiliation></author>
      <pages>3907-3924</pages>
      <abstract>The recent success of large language models (LLMs) has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot.Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model’s downstream performance and training costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model’s downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English. While English-centric tokenizers have been applied to the training of multi-lingual LLMs in the past, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68%, due to an inefficient tokenization vocabulary.</abstract>
      <url hash="c626a4c0">2024.findings-naacl.247</url>
      <bibkey>ali-etal-2024-tokenizer</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.247</doi>
    </paper>
    <paper id="248">
      <title>Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue</title>
      <author><first>Junkai</first><last>Zhou</last></author>
      <author><first>Liang</first><last>Pang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <pages>3925-3951</pages>
      <abstract>The emergence of large language models (LLMs) further improves the capabilities of open-domain dialogue systems and can generate fluent, coherent, and diverse responses. However, LLMs still lack a crucial ability: communication skills. This limitation renders them more like information seeking tools rather than anthropomorphic chatbots. Communication skills, such as topic transition, proactively asking questions, concept guidance, empathy, and summarising often should be taken into consideration, to make LLMs more anthropomorphic and proactive during the conversation, thereby increasing the interest of users and attracting them to chat for longer. However, enabling these communication skills in black-box LLMs remains a key challenge because they do not have the same utterance formation mode as real people: think before speaking. Inspired by linguistics and cognitive science, we empower LLMs with communication skills through inner monologues. To evaluate various communication skills, we construct a benchmark named Cskills, which can also more comprehensively evaluate the dialogue generation ability of the model. Experimental results show that the proposed CSIM strategy improves the backbone models and outperforms the baselines.</abstract>
      <url hash="3c85bfcc">2024.findings-naacl.248</url>
      <bibkey>zhou-etal-2024-think</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.248</doi>
    </paper>
    <paper id="249">
      <title>The Impact of Differential Privacy on Group Disparity Mitigation</title>
      <author><first>Victor</first><last>Hansen</last></author>
      <author><first>Atula</first><last>Neerkaje</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Ramit</first><last>Sawhney</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Anders</first><last>Søgaard</last><affiliation>Copenhagen University</affiliation></author>
      <pages>3952-3965</pages>
      <abstract>The performance cost of differential privacy has, for some applications, been shown to be higher for minority groups; fairness, conversely, has been shown to disproportionally compromise the privacy of members of such groups. Most work in this area has been restricted to computer vision and risk assessment. In response, we evaluate the impact of differential privacy on fairness across four diverse tasks, focusing on how attempts to mitigate privacy violations and between-group performance differences interact: Does privacy inhibit attempts to ensure fairness? To this end, we train <tex-math>(\varepsilon,\delta)</tex-math>-differentially private models with empirical risk minimization and group distributionally robust training objectives. Consistent with previous findings, we find that differential privacy increases between-group performance differences in the baseline setting; more interestingly, differential privacy <i>reduces</i> between-group performance differences in the robust setting. We explain this by interpreting differential privacy as regularization.</abstract>
      <url hash="a3578e23">2024.findings-naacl.249</url>
      <bibkey>hansen-etal-2024-impact</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.249</doi>
    </paper>
    <paper id="250">
      <title>Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning</title>
      <author><first>Shivam</first><last>Mhaskar</last><affiliation>Rakuten Mobile, Inc.</affiliation></author>
      <author><first>Nirmesh</first><last>Shah</last><affiliation>Sony Research India</affiliation></author>
      <author><first>Mohammadi</first><last>Zaki</last><affiliation>Sony Research India, Bangalore</affiliation></author>
      <author><first>Ashishkumar</first><last>Gudmalwar</last></author>
      <author><first>Pankaj</first><last>Wasnik</last><affiliation>Sony Research India</affiliation></author>
      <author><first>Rajiv</first><last>Shah</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>3966-3976</pages>
      <abstract>Traditional Automatic Video Dubbing (AVD) pipeline consists of three key modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation (NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms are employed to regulate the length of the synthesized output text. This is done to guarantee synchronization with respect to the alignment of video and audio subsequent to the dubbing process. Previous approaches have focused on aligning the number of characters and words in the source and target language texts of Machine Translation models. However, our approach aims to align the number of phonemes instead, as they are closely associated with speech duration. In this paper, we present the development of an isometric NMT system using Reinforcement Learning (RL), with a focus on optimizing the alignment of phoneme counts in the source and target language sentence pairs. To evaluate our models, we propose the Phoneme Count Compliance (PCC) score, which is a measure of length compliance. Our approach demonstrates a substantial improvement of approximately 36% in the PCC score compared to the state-of-the-art models when applied to English-Hindi language pairs. Moreover, we propose a student-teacher architecture within the framework of our RL approach to maintain a trade-off between the phoneme count and translation quality.</abstract>
      <url hash="4e0cff82">2024.findings-naacl.250</url>
      <bibkey>mhaskar-etal-2024-isometric</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.250</doi>
    </paper>
    <paper id="251">
      <title>Read between the lines - Functionality Extraction From <fixed-case>README</fixed-case>s</title>
      <author><first>Prince</first><last>Kumar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Srikanth</first><last>Tamilselvam</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Dinesh</first><last>Garg</last></author>
      <pages>3977-3990</pages>
      <abstract>While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1 score against ChatGPT and Bard respectively.</abstract>
      <url hash="f6157b4b">2024.findings-naacl.251</url>
      <bibkey>kumar-etal-2024-read</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.251</doi>
    </paper>
    <paper id="252">
      <title><fixed-case>A</fixed-case>bs<fixed-case>P</fixed-case>yramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph</title>
      <author><first>Zhaowei</first><last>Wang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Haochen</first><last>Shi</last></author>
      <author><first>Weiqi</first><last>Wang</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Sehyun</first><last>Choi</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>Amazon</affiliation></author>
      <author><first>Yangqiu</first><last>Song</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <pages>3991-4010</pages>
      <abstract>Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.</abstract>
      <url hash="cbdb6ea0">2024.findings-naacl.252</url>
      <bibkey>wang-etal-2024-abspyramid</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.252</doi>
    </paper>
    <paper id="253">
      <title>Few-<fixed-case>TK</fixed-case>: A Dataset for Few-shot Scientific Typed Keyphrase Recognition</title>
      <author><first>Avishek</first><last>Lahiri</last><affiliation>Indian Association for the Cultivation of Science</affiliation></author>
      <author><first>Pratyay</first><last>Sarkar</last><affiliation>Indian Association for the Cultivation of Science</affiliation></author>
      <author><first>Medha</first><last>Sen</last></author>
      <author><first>Debarshi Kumar</first><last>Sanyal</last><affiliation>Indian Association for the Cultivation of Science</affiliation></author>
      <author><first>Imon</first><last>Mukherjee</last></author>
      <pages>4011-4025</pages>
      <abstract>Scientific texts are distinctive from ordinary texts in quite a few aspects like their vocabulary and discourse structure. Consequently, Information Extraction (IE) tasks for scientific texts come with their own set of challenges. The classical definition of Named Entities restricts the inclusion of all scientific terms under its hood, which is why previous works have used the terms Named Entities and Keyphrases interchangeably. We suggest the rechristening of Named Entities for the scientific domain as Typed Keyphrases (TK), broadening their scope. We advocate for exploring this task in the few-shot domain due to the scarcity of labeled scientific IE data. Currently, no dataset exists for few-shot scientific Typed Keyphrase Recognition. To address this gap, we develop an annotation schema and present Few-TK, a dataset in the AI/ML field that includes scientific Typed Keyphrase annotations on abstracts of 500 research papers. To the best of our knowledge, this is the introductory few-shot Typed Keyphrase recognition dataset and only the second dataset structured specifically for few-shot NER, after Few-NERD. We report the results of several few-shot sequence-labelling models applied to our dataset. The data and code are available at https://github.com/AvishekLahiri/Few_TK.git</abstract>
      <url hash="c4ea4323">2024.findings-naacl.253</url>
      <bibkey>lahiri-etal-2024-tk</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.253</doi>
    </paper>
    <paper id="254">
      <title>Language Models can be Deductive Solvers</title>
      <author><first>Jiazhan</first><last>Feng</last></author>
      <author><first>Ruochen</first><last>Xu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Junheng</first><last>Hao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hiteshi</first><last>Sharma</last></author>
      <author><first>Yelong</first><last>Shen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Weizhu</first><last>Chen</last><affiliation>Microsoft GenAI</affiliation></author>
      <pages>4026-4042</pages>
      <abstract>Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of external logical solvers and no answer to the logical questions. In this paper, we introduce LoGiPT, a novel language model that directly internalizes and emulates the reasoning processes of logical solvers and avoids parsing errors by learning strict adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning benchmarks show that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like GPT-4. This project is available in https://github.com/Cyril-JZ/LoGiPT.</abstract>
      <url hash="4342ba61">2024.findings-naacl.254</url>
      <bibkey>feng-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.254</doi>
    </paper>
    <paper id="255">
      <title>Interpreting User Requests in the Context of Natural Language Standing Instructions</title>
      <author><first>Nikita</first><last>Moghe</last></author>
      <author><first>Patrick</first><last>Xia</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology and Microsoft</affiliation></author>
      <author><first>Jason</first><last>Eisner</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Harsh</first><last>Jhamtani</last><affiliation>Microsoft</affiliation></author>
      <pages>4043-4060</pages>
      <abstract>Users of natural language interfaces, frequently powered by Large Language Models (LLMs), must often repeat their full set of preferences each time they make a similar request. We describe an approach to LLM-based dialogue modeling in which persistent user constraints and preferences – collectively termed standing instructions – are provided as additional context for such interfaces. For example, when a user states “I’m hungry”, a previously expressed preference for Persian food can be automatically added to the LLM prompt, influencing the search for relevant restaurants.We develop NLSI, a language-to-program dataset consisting of over 2.4K English dialogues spanning 17 domains, in which each dialogue is paired with a user profile (a set of user-specific standing instructions) and corresponding structured representations (a sequence of API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains diverse phenomena, from simple preferences to interdependent instructions such as triggering a hotel search whenever the user is booking tickets to an event. We conduct experiments on NLSI using prompting with large language models and various retrieval approaches, achieving a maximum of 46% exact match on API prediction. Our results demonstrate the challenges in identifying the relevant standing instructions and their interpretation into API calls</abstract>
      <url hash="73eec2d4">2024.findings-naacl.255</url>
      <bibkey>moghe-etal-2024-interpreting</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.255</doi>
    </paper>
    <paper id="256">
      <title>Secure Your Model: An Effective Key Prompt Protection Mechanism for Large Language Models</title>
      <author><first>Ruixiang</first><last>Tang</last></author>
      <author><first>Yu-Neng</first><last>Chuang</last><affiliation>Rice University</affiliation></author>
      <author><first>Xuanting</first><last>Cai</last></author>
      <author><first>Mengnan</first><last>Du</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author><first>Xia</first><last>Hu</last><affiliation>Rice University</affiliation></author>
      <pages>4061-4073</pages>
      <abstract>Large language models (LLMs) have notably revolutionized many domains within natural language processing due to their exceptional performance. Their security has become increasingly vital. This study is centered on protecting LLMs against unauthorized access and potential theft. We propose a simple yet effective protective measure wherein a unique key prompt is embedded within the LLM. This mechanism enables the model to respond only when presented with the correct key prompt; otherwise, LLMs will refuse to react to any input instructions. This key prompt protection offers a robust solution to prevent the unauthorized use of LLMs, as the model becomes unusable without the correct key. We evaluated the proposed protection on multiple LLMs and NLP tasks. Results demonstrate that our method can successfully protect the LLM without significantly impacting the model’s original function. Moreover, we demonstrate potential attacks that attempt to bypass the protection mechanism will adversely affect the model’s performance, further emphasizing the effectiveness of the proposed protection method.</abstract>
      <url hash="1993a00f">2024.findings-naacl.256</url>
      <bibkey>tang-etal-2024-secure</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.256</doi>
    </paper>
    <paper id="257">
      <title>Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models</title>
      <author><first>Jiashuo</first><last>Sun</last></author>
      <author><first>Yi</first><last>Luo</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Chen</first><last>Lin</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Yelong</first><last>Shen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jian</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>4074-4101</pages>
      <abstract>Large language models (LLMs) can achieve impressive performance on various reasoning tasks by incorporating chain-of-thought (CoT) prompting, where step-by-step reasoning is provided to guide LLMs to generate answers to questions, and the question-rationale-answer triplets are utilized as demonstration exemplars. However, the reasoning chains of demonstrations generated by LLMs are observed to be prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars, e.g., overly simplistic or complex exemplars depending on the question’s difficulty level, can affect the LLM’s performance. To address these issues, we introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts prompting). Iter-CoT has two advantages: (1) it adopts iterative bootstrapping that enables LLMs to rectify errors autonomously, resulting in more precise and comprehensive reasoning chains. (2) it selects exemplars of challenging yet answerable (i.e., the LLM has the potential to answer correctly) questions, enhancing the LLMs’ generalizability to answer questions with varying difficulty levels. Experimental results exhibit Iter-CoT superior performance on three distinct reasoning tasks on ten datasets.</abstract>
      <url hash="186937d2">2024.findings-naacl.257</url>
      <bibkey>sun-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.257</doi>
    </paper>
    <paper id="258">
      <title>Do Prompt Positions Really Matter?</title>
      <author><first>Junyu</first><last>Mao</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Stuart E.</first><last>Middleton</last><affiliation>University of Southampton</affiliation></author>
      <author><first>Mahesan</first><last>Niranjan</last><affiliation>University of Southampton</affiliation></author>
      <pages>4102-4130</pages>
      <abstract>Prompt-based models have gathered a lot of attention from researchers due to their remarkable advancements in the fields of zero-shot and few-shot learning. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary searching or embedding initialization within a predefined template with the prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position for diverse Natural Language Processing (NLP) tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt positions used in prior studies are often sub-optimal, and this observation is consistent even in widely used instruction-tuned models. These findings suggest prompt position optimisation as a valuable research direction to augment prompt engineering methodologies and prompt position-aware instruction tuning as a potential way to build more robust models in the future.</abstract>
      <url hash="c62126be">2024.findings-naacl.258</url>
      <bibkey>mao-etal-2024-prompt</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.258</doi>
    </paper>
    <paper id="259">
      <title>Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning</title>
      <author><first>Tianhua</first><last>Zhang</last></author>
      <author><first>Jiaxin</first><last>Ge</last></author>
      <author><first>Hongyin</first><last>Luo</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yung-Sung</first><last>Chuang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Mingye</first><last>Gao</last></author>
      <author><first>Yuan</first><last>Gong</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Xixin</first><last>Wu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Helen</first><last>Meng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>4131-4155</pages>
      <abstract>How can we perform computations over natural language representations to solve tasks that require symbolic and numeric reasoning? We propose natural language embedded programs (NLEP) as a unifying framework for addressing math/symbolic reasoning, natural language understanding, and instruction following tasks. Our approach prompts a language model to generate full Python programs that define functions over data structures which contain natural language representations of structured knowledge. A Python interpreter then executes the generated code and prints the output. Despite using a task-general prompt, we find that this approach can improve upon strong baselines across a range of different tasks including math and symbolic reasoning, text classification, question answering, and instruction following. We found that the generated programs are interpretable since they outline the exact reasoning process followed by the program interpreter.</abstract>
      <url hash="2d026d25">2024.findings-naacl.259</url>
      <bibkey>zhang-etal-2024-natural</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.259</doi>
    </paper>
    <paper id="260">
      <title>A Study on Scaling Up Multilingual News Framing Analysis</title>
      <author><first>Syeda Sabrina</first><last>Akter</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>4156-4173</pages>
      <abstract>Media framing is the study of strategically selecting and presenting specific aspects of political issues to shape public opinion. Despite its relevance to almost all societies around the world, research has been limited due to the lack of available datasets and other resources. This study explores the possibility of dataset creation through crowdsourcing, utilizing non-expert annotators to develop training corpora. We first extend framing analysis beyond English news to a multilingual context (12 typologically diverse languages) through automatic translation. We also present a novel benchmark in Bengali and Portuguese on the immigration and same-sex marriage domains.Additionally, we show that a system trained on our crowd-sourced dataset, combined with other existing ones, leads to a 5.32 percentage point increase from the baseline, showing that crowdsourcing is a viable option. Last, we study the performance of large language models (LLMs) for this task, finding that task-specific fine-tuning is a better approach than employing bigger non-specialized models.</abstract>
      <url hash="78acc39a">2024.findings-naacl.260</url>
      <bibkey>akter-anastasopoulos-2024-study</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.260</doi>
    </paper>
    <paper id="261">
      <title><fixed-case>V</fixed-case>i<fixed-case>GLUE</fixed-case>: A <fixed-case>V</fixed-case>ietnamese General Language Understanding Benchmark and Analysis of <fixed-case>V</fixed-case>ietnamese Language Models</title>
      <author><first>Minh-Nam</first><last>Tran</last></author>
      <author><first>Phu-Vinh</first><last>Nguyen</last></author>
      <author><first>Long</first><last>Nguyen</last><affiliation>Ho Chi Minh city University of Science, Vietnam National University</affiliation></author>
      <author><first>Dien</first><last>Dinh</last></author>
      <pages>4174-4189</pages>
      <abstract>As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to the difficulty in accessing natural language processing datasets or the scarcity of task-specific datasets. **ViGLUE**, the proposed dataset collection, is a **Vi**etnamese **G**eneral **L**anguage **U**nderstanding **E**valuation benchmark developed using three methods: translating an existing benchmark, generating new corpora, and collecting available datasets. ViGLUE contains twelve tasks and encompasses over ten areas and subjects, enabling it to evaluate models comprehensively over a broad spectrum of aspects. Baseline models utilizing multilingual language models are also provided for all tasks in the proposed benchmarks. In addition, the study of the available Vietnamese large language models is conducted to explore the language models’ ability in the few-shot learning framework, leading to the exploration of the relationship between specific tasks and the number of shots.</abstract>
      <url hash="1bde9a76">2024.findings-naacl.261</url>
      <bibkey>tran-etal-2024-viglue</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.261</doi>
    </paper>
    <paper id="262">
      <title>Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales</title>
      <author><first>Lucas</first><last>Resck</last><affiliation>Fundação Getulio Vargas</affiliation></author>
      <author><first>Marcos</first><last>M. Raimundo</last><affiliation>Universidade Estadual de Campinas</affiliation></author>
      <author><first>Jorge</first><last>Poco</last><affiliation>Fundação Getulio Vargas</affiliation></author>
      <pages>4190-4216</pages>
      <abstract>Saliency post-hoc explainability methods are important tools for understanding increasingly complex NLP models. While these methods can reflect the model’s reasoning, they may not align with human intuition, making the explanations not plausible. In this work, we present a methodology for incorporating rationales, which are text annotations explaining human decisions, into text classification models. This incorporation enhances the plausibility of post-hoc explanations while preserving their faithfulness. Our approach is agnostic to model architectures and explainability methods. We introduce the rationales during model training by augmenting the standard cross-entropy loss with a novel loss function inspired by contrastive learning. By leveraging a multi-objective optimization algorithm, we explore the trade-off between the two loss functions and generate a Pareto-optimal frontier of models that balance performance and plausibility. Through extensive experiments involving diverse models, datasets, and explainability methods, we demonstrate that our approach significantly enhances the quality of model explanations without causing substantial (sometimes negligible) degradation in the original model’s performance.</abstract>
      <url hash="570070ef">2024.findings-naacl.262</url>
      <bibkey>resck-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.262</doi>
    </paper>
    <paper id="263">
      <title>Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation</title>
      <author><first>Tong</first><last>Su</last></author>
      <author><first>Xin</first><last>Peng</last></author>
      <author><first>Sarubi</first><last>Thillainathan</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>David</first><last>Guzmán</last></author>
      <author><first>Surangika</first><last>Ranathunga</last><affiliation>Massey University</affiliation></author>
      <author><first>En-Shiun</first><last>Lee</last></author>
      <pages>4217-4225</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) methods are increasingly vital in adapting large-scale pre-trained language models for diverse tasks, offering a balance between adaptability and computational efficiency. They are important in Low-Resource Language (LRL) Neural Machine Translation (NMT) to enhance translation accuracy with minimal resources. However, their practical effectiveness varies significantly across different languages. We conducted comprehensive empirical experiments with varying LRL domains and sizes to evaluate the performance of 8 PEFT methods with in total of 15 architectures using the SacreBLEU score. We showed that 6 PEFT architectures outperform the baseline for both in-domain and out-domain tests and the Houlsby+Inversion adapter has the best performance overall, proving the effectiveness of PEFT methods.</abstract>
      <url hash="2500d003">2024.findings-naacl.263</url>
      <bibkey>su-etal-2024-unlocking</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.263</doi>
    </paper>
    <paper id="264">
      <title><fixed-case>AD</fixed-case>a<fixed-case>PT</fixed-case>: As-Needed Decomposition and Planning with Language Models</title>
      <author><first>Archiki</first><last>Prasad</last></author>
      <author><first>Alexander</first><last>Koller</last><affiliation>Saarland University</affiliation></author>
      <author><first>Mareike</first><last>Hartmann</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Ashish</first><last>Sabharwal</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Tushar</first><last>Khot</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>4226-4252</pages>
      <abstract>Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft – a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.</abstract>
      <url hash="6e5fe780">2024.findings-naacl.264</url>
      <bibkey>prasad-etal-2024-adapt</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.264</doi>
    </paper>
    <paper id="265">
      <title>Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations</title>
      <author><first>Dayeon</first><last>Ki</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Marine</first><last>Carpuat</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>4253-4273</pages>
      <abstract>Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation.</abstract>
      <url hash="2e29e9ce">2024.findings-naacl.265</url>
      <bibkey>ki-carpuat-2024-guiding</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.265</doi>
    </paper>
    <paper id="266">
      <title>Non-contrastive sentence representations via self-supervision</title>
      <author><first>Duccio</first><last>Pappadopulo</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Marco</first><last>Farina</last><affiliation>Bloomberg</affiliation></author>
      <pages>4274-4284</pages>
      <abstract>Sample contrastive methods, typically referred to simply as contrastive are the foundation of most unsupervised methods to learn text and sentence embeddings. On the other hand, a different class of self-supervised non-contrastive loss functions and methods have been considered in the computer vision community and referred to as dimension contrastive. In this paper, we thoroughly compare this class of methods with the standard baseline for contrastive sentence embeddings, SimCSE. We find that self-supervised embeddings trained using dimension contrastive objectives can outperform SimCSE on downstream tasks without needing auxiliary loss functions.</abstract>
      <url hash="edbbc2b1">2024.findings-naacl.266</url>
      <bibkey>pappadopulo-farina-2024-non</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.266</doi>
    </paper>
    <paper id="267">
      <title>Semantically-Prompted Language Models Improve Visual Descriptions</title>
      <author><first>Michael</first><last>Ogezi</last></author>
      <author><first>Bradley</first><last>Hauer</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Grzegorz</first><last>Kondrak</last><affiliation>University of Alberta</affiliation></author>
      <pages>4285-4302</pages>
      <abstract>Language-vision models like CLIP have made significant strides in vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive visual descriptions remains challenging; descriptions produced by current methods are often ambiguous and lacking in granularity. To tackle these issues, we propose V-GLOSS: Visual Glosses, a novel method built upon two key ideas. The first is Semantic Prompting, which conditions a language model on structured semantic knowledge. The second is a new contrastive algorithm that elicits fine-grained distinctions between similar concepts. With both ideas, we demonstrate that V-GLOSS improves visual descriptions and achieves strong results in the zero-shot setting on general and fine-grained image-classification datasets, including ImageNet, STL-10, FGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilities contribute to enhancing image-generation performance. Finally, we introduce a quality-tested silver dataset with descriptions generated with V-GLOSS for all ImageNet classes.</abstract>
      <url hash="677ebe1d">2024.findings-naacl.267</url>
      <bibkey>ogezi-etal-2024-semantically</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.267</doi>
    </paper>
    <paper id="268">
      <title><fixed-case>G</fixed-case>en<fixed-case>TKG</fixed-case>: Generative Forecasting on Temporal Knowledge Graph with Large Language Models</title>
      <author><first>Ruotong</first><last>Liao</last></author>
      <author><first>Xu</first><last>Jia</last></author>
      <author><first>Yangzhe</first><last>Li</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Yunpu</first><last>Ma</last><affiliation>Siemens Corporate Research</affiliation></author>
      <author><first>Volker</first><last>Tresp</last><affiliation>Ludwig Maximilian University of Munich and Siemens Corporate Research</affiliation></author>
      <pages>4303-4317</pages>
      <abstract>The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional embedding-based and rule-based methods dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval-augmented generation framework named GenTKG combining a temporal logical rule-based retrieval strategy and few-shot parameter-efficient instruction tuning to solve the above challenges, respectively. Extensive experiments have shown that GenTKG outperforms conventional methods of temporal relational forecasting with low computation resources using extremely limited training data as few as 16 samples. GenTKG also highlights remarkable cross-domain generalizability with outperforming performance on unseen datasets without re-training, and in-domain generalizability regardless of time split in the same dataset. Our work reveals the huge potential of LLMs in the tKG domain and opens a new frontier for generative forecasting on tKGs. The code and data are released here: <url>https://github.com/mayhugotong/GenTKG</url>.</abstract>
      <url hash="0090eddf">2024.findings-naacl.268</url>
      <bibkey>liao-etal-2024-gentkg</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.268</doi>
    </paper>
    <paper id="269">
      <title>A Transformer with Stack Attention</title>
      <author><first>Jiaoda</first><last>Li</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Jennifer</first><last>White</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>4318-4335</pages>
      <abstract>Natural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-basedattention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-freelanguages.</abstract>
      <url hash="347de55c">2024.findings-naacl.269</url>
      <bibkey>li-etal-2024-transformer</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.269</doi>
    </paper>
    <paper id="270">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>E</fixed-case>val: Systematic Evaluation of Instruction Selection Methods</title>
      <author><first>Anirudh</first><last>Ajith</last></author>
      <author><first>Chris</first><last>Pan</last></author>
      <author><first>Mengzhou</first><last>Xia</last></author>
      <author><first>Ameet</first><last>Deshpande</last></author>
      <author><first>Karthik</first><last>Narasimhan</last><affiliation>Princeton University</affiliation></author>
      <pages>4336-4350</pages>
      <abstract>In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that precise details of the inputs used in the ICL prompt significantly impact performance, which has incentivized instruction selection algorithms. The effect of instruction-choice however is severely underexplored, with existing analyses restricted to shallow subsets of models and tasks, limiting the generalizability of their insights. We develop InstructEval, an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from four model families, and covers nine tasks across three categories. Using the suite, we evaluate the relative performance of seven popular instruction selection methods over five metrics relevant to ICL. Our experiments reveal that using curated manually-written instructions or simple instructions without any task-specific descriptions often elicits superior ICL performance overall than that of automatic instruction-induction methods, pointing to a lack of generalizability among the latter. We release our evaluation suite (at https://github.com/princeton-nlp/InstructEval) for benchmarking instruction selection approaches and enabling more generalizable methods in this space.</abstract>
      <url hash="f80f29be">2024.findings-naacl.270</url>
      <bibkey>ajith-etal-2024-instructeval</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.270</doi>
    </paper>
    <paper id="271">
      <title><fixed-case>R</fixed-case>ec<fixed-case>M</fixed-case>ind: Large Language Model Powered Agent For Recommendation</title>
      <author><first>Yancheng</first><last>Wang</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Ziyan</first><last>Jiang</last><affiliation>Amazon</affiliation></author>
      <author><first>Zheng</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <author><first>Fan</first><last>Yang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yingxue</first><last>Zhou</last><affiliation>University of Minnesota, Minneapolis</affiliation></author>
      <author><first>Eunah</first><last>Cho</last></author>
      <author><first>Xing</first><last>Fan</last></author>
      <author><first>Yanbin</first><last>Lu</last></author>
      <author><first>Xiaojiang</first><last>Huang</last></author>
      <author><first>Yingzhen</first><last>Yang</last><affiliation>Arizona State University</affiliation></author>
      <pages>4351-4364</pages>
      <abstract>While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM “self-inspires” to consider all previously explored states to plan for the next step. This mechanism greatly improves the model’s ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind’s performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.</abstract>
      <url hash="f7672484">2024.findings-naacl.271</url>
      <bibkey>wang-etal-2024-recmind</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.271</doi>
    </paper>
    <paper id="272">
      <title><fixed-case>GOLD</fixed-case>: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation</title>
      <author><first>Mohsen</first><last>Gholami</last></author>
      <author><first>Mohammad</first><last>Akbari</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Tianxi</first><last>Hu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Vaden</first><last>Masrani</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Z.</first><last>Wang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Yong</first><last>Zhang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>4365-4380</pages>
      <abstract>Knowledge distillation from LLMs is essential for the efficient deployment of language models. Prior works have proposed data generation using LLMs for preparing distilled models. We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability). To this end, we propose GOLD, a task-agnostic data generation and knowledge distillation framework, which employs an iterative out-of-distribution-guided feedback mechanism for the LLM. As a result, the generated data improves the generalizability of distilled models. An energy-based OOD evaluation approach is also introduced to deal with noisy generated data. Our extensive experiments on 10 different classification and sequence-to-sequence tasks in NLP show that GOLD respectively outperforms prior arts and the LLM with an average improvement of 5% and 14%. We will also show that the proposed method is applicable to less explored and novel tasks. Code is available in the Appendix.</abstract>
      <url hash="d0fe27ef">2024.findings-naacl.272</url>
      <bibkey>gholami-etal-2024-gold</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.272</doi>
    </paper>
    <paper id="273">
      <title>How Lexical is Bilingual Lexicon Induction?</title>
      <author><first>Harsh</first><last>Kohli</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Helian</first><last>Feng</last><affiliation>Amazon</affiliation></author>
      <author><first>Nicholas</first><last>Dronen</last><affiliation>Lightmatter</affiliation></author>
      <author><first>Calvin</first><last>McCarter</last></author>
      <author><first>Sina</first><last>Moeini</last></author>
      <author><first>Ali</first><last>Kebarighotbi</last></author>
      <pages>4381-4386</pages>
      <abstract>In contemporary machine learning approaches to bilingual lexicon induction (BLI), a model learns a mapping between the embedding spaces of a language pair. Recently, retrieve-and-rank approach to BLI has achieved state of the art results on the task. However, the problem remains challenging in low-resource settings, due to the paucity of data. The task is complicated by factors such as lexical variation across languages. We argue that the incorporation of additional lexical information into the recent retrieve-and-rank approach should improve lexicon induction. We demonstrate the efficacy of our proposed approach on XLING, improving over the previous state of the art by an average of 2% across all language pairs.</abstract>
      <url hash="4cb02a02">2024.findings-naacl.273</url>
      <bibkey>kohli-etal-2024-lexical</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.273</doi>
    </paper>
    <paper id="274">
      <title>Fumbling in Babel: An Investigation into <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>’s Language Identification Ability</title>
      <author><first>Wei-Rui</first><last>Chen</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Ife</first><last>Adebara</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Khai</first><last>Doan</last></author>
      <author><first>Qisheng</first><last>Liao</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>4387-4413</pages>
      <abstract>ChatGPT has recently emerged as a powerful NLP tool that can carry out a variety of tasks. However, the range of languages ChatGPT can handle remains largely a mystery. To uncover which languages ChatGPT ‘knows’, we investigate its language identification (LID) abilities. For this purpose, we compile Babel-670, a benchmark comprising 670 languages representing 23 language families spoken in five continents. Languages in Babel-670 run the gamut from the very high-resource to the very low-resource. We then study ChatGPT’s (both GPT-3.5 and GPT-4) ability to (i) identify language names and language codes (ii) under zero- and few-shot conditions (iii) with and without provision of a label set. When compared to smaller finetuned LID tools, we find that ChatGPT lags behind. For example, it has poor performance on African languages. We conclude that current large language models would benefit from further development before they can sufficiently serve diverse communities.</abstract>
      <url hash="6d7517f1">2024.findings-naacl.274</url>
      <bibkey>chen-etal-2024-fumbling</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.274</doi>
    </paper>
    <paper id="275">
      <title>Targeted Augmentation for Low-Resource Event Extraction</title>
      <author><first>Sijia</first><last>Wang</last></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>4414-4428</pages>
      <abstract>Addressing the challenge of low-resource information extraction remains an ongoing issue due to the inherent information scarcity within limited training examples. Existing data augmentation methods, considered potential solutions, struggle to strike a balance between weak augmentation (e.g., synonym augmentation) and drastic augmentation (e.g., conditional generation without proper guidance). This paper introduces a novel paradigm that employs targeted augmentation and back validation to produce augmented examples with enhanced diversity, polarity, accuracy, and coherence. Extensive experimental results demonstrate the effectiveness of the proposed paradigm. Furthermore, identified limitations are discussed, shedding light on areas for future improvement.</abstract>
      <url hash="40e99951">2024.findings-naacl.275</url>
      <bibkey>wang-huang-2024-targeted</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.275</doi>
    </paper>
    <paper id="276">
      <title>Asking More Informative Questions for Grounded Retrieval</title>
      <author><first>Sedrick</first><last>Keh</last><affiliation>Toyota Research Institute</affiliation></author>
      <author><first>Justin</first><last>Chiu</last><affiliation>Cornell University</affiliation></author>
      <author><first>Daniel</first><last>Fried</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>4429-4442</pages>
      <abstract>When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions (White et al., 2021), limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations.</abstract>
      <url hash="2cf0baf6">2024.findings-naacl.276</url>
      <bibkey>keh-etal-2024-asking</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.276</doi>
    </paper>
    <paper id="277">
      <title>Efficient Citer: Tuning Large Language Models for Enhanced Answer Quality and Verification</title>
      <author><first>Marzieh</first><last>Tahaei</last></author>
      <author><first>Aref</first><last>Jafari</last><affiliation>University of Waterloo and Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ahmad</first><last>Rashid</last><affiliation>University of Waterloo</affiliation></author>
      <author><first>David</first><last>Alfonso-Hermelo</last></author>
      <author><first>Khalil</first><last>Bibi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yimeng</first><last>Wu</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>4443-4450</pages>
      <abstract>In recent years, there has been a growing interest in utilizing external knowledge to reduce hallucinations in large language models (LLMs) and provide them with updated information. Despite this improvement, a major challenge lies in the lack of explicit citations, which hampers the ability to verify the information generated by these models.This paper focuses on providing models with citation capabilities efficiently. By constructing a dataset of citations, we train two model architectures: an FID-style FLAN-T5 model for efficient answer composition and a 13B model known for its success in instruction following after tuning. Evaluation on fluency, correctness, and citation quality is conducted through human assessment and the newly introduced Automatic LLMs’ Citation Evaluation (ALCE) benchmark.Results demonstrate significant improvements in answer quality and efficiency, surpassing the performance of the popular ChatGPT on some of the metrics. The models exhibit exceptional out-of-domain generalization in both human and automatic evaluation. Notably, the FID-style FLAN-T5 model with only 3B parameters performs impressively compared to the 13B model.</abstract>
      <url hash="de5add53">2024.findings-naacl.277</url>
      <bibkey>tahaei-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.277</doi>
    </paper>
    <paper id="278">
      <title>Addressing Healthcare-related Racial and <fixed-case>LGBTQ</fixed-case>+ Biases in Pretrained Language Models</title>
      <author><first>Sean</first><last>Xie</last></author>
      <author><first>Saeed</first><last>Hassanpour</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>4451-4464</pages>
      <abstract>Recent studies have highlighted the issue of Pretrained Language Models (PLMs) inadvertently propagating social stigmas and stereotypes, a critical concern given their widespread use. This is particularly problematic in sensitive areas like healthcare, where such biases could lead to detrimental outcomes. Our research addresses this by adapting two intrinsic bias benchmarks to quantify racial and LGBTQ+ biases in prevalent PLMs. We also empirically evaluate the effectiveness of various debiasing methods in mitigating these biases. Furthermore, we assess the impact of debiasing on both Natural Language Understanding and specific biomedical applications. Our findings reveal that while PLMs commonly exhibit healthcare-related racial and LGBTQ+ biases, the applied debiasing techniques successfully reduce these biases without compromising the models’ performance in downstream tasks.</abstract>
      <url hash="d28c38b1">2024.findings-naacl.278</url>
      <bibkey>xie-etal-2024-addressing</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.278</doi>
    </paper>
    <paper id="279">
      <title><fixed-case>ATG</fixed-case>: Benchmarking Automated Theorem Generation for Generative Language Models</title>
      <author><first>Xiaohan</first><last>Lin</last></author>
      <author><first>Qingxing</first><last>Cao</last><affiliation>SUN YAT-SEN UNIVERSITY, Tsinghua University</affiliation></author>
      <author><first>Yinya</first><last>Huang</last></author>
      <author><first>Zhicheng</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <author><first>Zhengying</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zhenguo</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology and Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <pages>4465-4480</pages>
      <abstract>Humans can develop new theorems to explore broader and more complex mathematical results.While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to generate new or reusable theorems is still under-explored. Without the new theorems, current LMs struggle to prove harder theorems that are distant from the given hypotheses with the exponentially growing search space.More advanced theorem proving is if an agent (for instance, a generative LM) can leverage its creativity to generate new but also reasonable theorems that properly substitute part of a proof and also be saved as reusable knowledge for future theorem proving.Therefore, this paper proposes an Automated Theorem Generation (ATG) benchmark that evaluates whether an agent can automatically generate valuable (and possibly brand new) theorems that are applicable for downstream theorem proving as reusable knowledge. Specifically, we construct the ATG benchmark by splitting the Metamath library into three sets: axioms, library, and problem based on their proving depth.We conduct extensive experiments to investigate whether current LMs can generate theorems in the library and benefit the problem theorems proving. The results demonstrate that high-quality ATG data facilitates models’ performances on downstream ATP. However, there is still room for current LMs to develop better ATG and generate more advanced and human-like theorems. We hope the new ATG challenge can shed some light on advanced complex theorem proving.</abstract>
      <url hash="aba603d1">2024.findings-naacl.279</url>
      <bibkey>lin-etal-2024-atg</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.279</doi>
    </paper>
    <paper id="280">
      <title>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</title>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Alexander</first><last>Fabbri</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Jiawen</first><last>Chen</last></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Simeng</first><last>Han</last><affiliation>Yale University</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Dragomir</first><last>Radev</last><affiliation>Yale University</affiliation></author>
      <author><first>Chien-Sheng</first><last>Wu</last><affiliation>Salesforce AI</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>4481-4501</pages>
      <abstract>While large language models (LLMs) can already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluations of five LLM-based systems to assess their instruction-following capabilities in controllable summarization. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation capabilities. We make our collected benchmark InstruSum publicly available to facilitate future research in this direction.</abstract>
      <url hash="10dcec37">2024.findings-naacl.280</url>
      <bibkey>liu-etal-2024-benchmarking</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.280</doi>
    </paper>
    <paper id="281">
      <title><fixed-case>N</fixed-case>euro<fixed-case>C</fixed-case>omparatives: Neuro-Symbolic Distillation of Comparative Knowledge</title>
      <author><first>Phillip</first><last>Howard</last><affiliation>Intel</affiliation></author>
      <author><first>Junlin</first><last>Wang</last></author>
      <author><first>Vasudev</first><last>Lal</last><affiliation>Intel</affiliation></author>
      <author><first>Gadi</first><last>Singer</last></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Swabha</first><last>Swayamdipta</last><affiliation>University of Southern California</affiliation></author>
      <pages>4502-4520</pages>
      <abstract>Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we harvest the dramatic improvements in knowledge capabilities of language models into a large-scale comparative knowledge base. While the ease of acquisition of such comparative knowledge is much higher from extreme-scale models like GPT-4, compared to their considerably smaller and weaker counterparts such as GPT-2, not even the most powerful models are exempt from making errors. We thus ask: to what extent are models at different scales able to generate valid and diverse comparative knowledge?We introduce NeuroComparatives, a novel framework for comparative knowledge distillation overgenerated from language models such as GPT-variants and LLaMA, followed by stringent filtering of the generated knowledge. Our framework acquires comparative knowledge between everyday objects, producing a corpus of up to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% more diverse than existing resources. Moreover, human evaluations show that NeuroComparatives outperform existing resources in terms of validity (up to 32% absolute improvement). Our acquired NeuroComparatives leads to performance improvements on five downstream tasks.We find that neuro-symbolic manipulation of smaller models offers complementary benefits to the currently dominant practice of prompting extreme-scale language models for knowledge distillation.</abstract>
      <url hash="e6e84c0f">2024.findings-naacl.281</url>
      <bibkey>howard-etal-2024-neurocomparatives</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.281</doi>
    </paper>
    <paper id="282">
      <title>Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation</title>
      <author><first>Fangxu</first><last>Yu</last></author>
      <author><first>Junjie</first><last>Guo</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhen</first><last>Wu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Xinyu</first><last>Dai</last><affiliation>Nanjing University</affiliation></author>
      <pages>4521-4534</pages>
      <abstract>Emotion Recognition in Conversation (ERC) involves detecting the underlying emotion behind each utterance within a conversation. Effectively generating representations for utterances remains a significant challenge in this task. Recent works propose various models to address this issue, but they still struggle with differentiating similar emotions such as excitement and happiness. To alleviate this problem, We propose an Emotion-Anchored Contrastive Learning (EACL) framework that can generate more distinguishable utterance representations for similar emotions. To achieve this, we utilize label encodings as anchors to guide the learning of utterance representations and design an auxiliary loss to ensure the effective separation of anchors for similar emotions. Moreover, an additional adaptation process is proposed to adapt anchors to serve as effective classifiers to improve classification performance. Across extensive experiments, our proposed EACL achieves state-of-the-art emotion recognition performance and exhibits superior performance on similar emotions. Our code is available at https://github.com/Yu-Fangxu/EACL.</abstract>
      <url hash="27a140c4">2024.findings-naacl.282</url>
      <bibkey>yu-etal-2024-emotion</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.282</doi>
    </paper>
    <paper id="283">
      <title><fixed-case>SUQL</fixed-case>: Conversational Search over Structured and Unstructured Data with Large Language Models</title>
      <author><first>Shicheng</first><last>Liu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Jialiang</first><last>Xu</last></author>
      <author><first>Wesley</first><last>Tjangnaka</last></author>
      <author><first>Sina</first><last>Semnani</last><affiliation>Stanford University</affiliation></author>
      <author><first>Chen</first><last>Yu</last></author>
      <author><first>Monica</first><last>Lam</last><affiliation>Stanford University</affiliation></author>
      <pages>4535-4555</pages>
      <abstract>While most conversational agents are grounded on either free-text or structured knowledge, many knowledge corpora consist of hybrid sources.This paper presents the first conversational agent that supports the full generality of hybrid data access for large knowledge corpora, through a language we developed called SUQL (<tex-math>\textbf{S}</tex-math>tructured and <tex-math>\textbf{U}</tex-math>nstructured <tex-math>\textbf{Q}</tex-math>uery <tex-math>\textbf{L}</tex-math>anguage). Specifically, SUQL extends SQL with free-text primitives (<tex-math>{\small \text{SUMMARY}}</tex-math> and <tex-math>{\small \text{ANSWER}}</tex-math>), so information retrieval can be composed with structured data accesses arbitrarily in a formal, succinct, precise, and interpretable notation. With SUQL, we propose the first semantic parser, an LLM with in-context learning, that can handle hybrid data sources.Our in-context learning-based approach, when applied to the HybridQA dataset, comes within 8.9% Exact Match and 7.1% F1 of the SOTA, which was trained on 62K data samples. More significantly, unlike previous approaches, our technique is applicable to large databases and free-text corpora. We introduce a dataset consisting of crowdsourced questions and conversations on Yelp, a large, real restaurant knowledge base with structured and unstructured data. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 90.3% of the time, compared to 63.4% for a baseline based on linearization.</abstract>
      <url hash="73309326">2024.findings-naacl.283</url>
      <bibkey>liu-etal-2024-suql</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.283</doi>
    </paper>
    <paper id="284">
      <title>On Evaluating the Integration of Reasoning and Action in <fixed-case>LLM</fixed-case> Agents with Database Question Answering</title>
      <author><first>Linyong</first><last>Nan</last></author>
      <author><first>Ellen</first><last>Zhang</last></author>
      <author><first>Weijin</first><last>Zou</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Wenfei</first><last>Zhou</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>4556-4579</pages>
      <abstract>This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter. The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative. Our findings highlight that this task poses great challenges even for the state-of-the-art **GPT-4** model. We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction. A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries. To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations. This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.</abstract>
      <url hash="dd05fd2b">2024.findings-naacl.284</url>
      <bibkey>nan-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.284</doi>
    </paper>
    <paper id="285">
      <title><fixed-case>CARE</fixed-case>: Extracting Experimental Findings From Clinical Literature</title>
      <author><first>Aakanksha</first><last>Naik</last><affiliation>Allen Institute for Artificial Intelligence and National Institutes of Health</affiliation></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Erin</first><last>Bransom</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Doug</first><last>Downey</last><affiliation>Allen Institute for Artificial Intelligence and Northwestern University</affiliation></author>
      <author><first>Tom</first><last>Hope</last><affiliation>Allen Institute for Artificial Intelligence and Hebrew University, Hebrew University of Jerusalem</affiliation></author>
      <pages>4580-4596</pages>
      <abstract>Extracting fine-grained experimental findings from literature can provide dramatic utility for scientific applications. Prior work has developed annotation schemas and datasets for limited aspects of this problem, failing to capture the real-world complexity and nuance required. Focusing on biomedicine, this work presents CARE—a new IE dataset for the task of extracting clinical findings. We develop a new annotation schema capturing fine-grained findings as n-ary relations between entities and attributes, which unifies phenomena challenging for current IE systems such as discontinuous entity spans, nested relations, variable arity n-ary relations and numeric results in a single schema. We collect extensive annotations for 700 abstracts from two sources: clinical trials and case reports. We also demonstrate the generalizability of our schema to the computer science and materials science domains. We benchmark state-of-the-art IE systems on CARE, showing that even models such as GPT4 struggle. We release our resources to advance research on extracting and aggregating literature findings.</abstract>
      <url hash="ea1daefb">2024.findings-naacl.285</url>
      <bibkey>naik-etal-2024-care</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.285</doi>
    </paper>
    <paper id="286">
      <title>Personalized Federated Learning for Text Classification with Gradient-Free Prompt Tuning</title>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Tong</first><last>Yu</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Ruiyi</first><last>Zhang</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Sungchul</first><last>Kim</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ryan</first><last>Rossi</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Handong</first><last>Zhao</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Junda</first><last>Wu</last></author>
      <author><first>Subrata</first><last>Mitra</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Lina</first><last>Yao</last><affiliation>University of New South Wales and CSIRO’s Data61</affiliation></author>
      <author><first>Ricardo</first><last>Henao</last><affiliation>Duke University and King Abdullah University of Science and Technology</affiliation></author>
      <pages>4597-4612</pages>
      <abstract>In this paper, we study personalized federated learning for text classification with Pretrained Language Models (PLMs). We identify two challenges in efficiently leveraging PLMs for personalized federated learning: 1) Communication. PLMs are usually large in size, e.g., with hundreds of millions of parameters, inducing huge communication cost in a federated setting. 2) Local Training. Training with PLMs generally requires back-propagation, during which memory consumption can be several times that of the forward-propagation. This may not be affordable when the PLMs are trained locally on the clients that are resource constrained, e.g., mobile devices with limited access to memory resources. Additionally, the proprietary PLMs can be provided as concealed APIs, for which the back-propagation operations may not be available. In solving these, we propose a training framework that includes an approach of discrete local search for gradient-free local training, along with a compression mechanism inspired from the linear word analogy that allows communicating with discretely indexed tokens, thus significantly reducing the communication cost. Experiments show that our gradient-free framework achieves superior performance compared with baselines.</abstract>
      <url hash="9911fc8d">2024.findings-naacl.286</url>
      <bibkey>wang-etal-2024-personalized</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.286</doi>
    </paper>
    <paper id="287">
      <title><fixed-case>SGSH</fixed-case>: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation</title>
      <author><first>Shasha</first><last>Guo</last></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Yanling</first><last>Wang</last><affiliation>Zhongguancun Laboratory</affiliation></author>
      <author><first>Cuiping</first><last>Li</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Hong</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>4613-4625</pages>
      <abstract>Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH — a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates “skeleton heuristics”, which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise an automatic data construction strategy leveraging ChatGPT to construct a skeleton training dataset, based on which we employ a soft prompting approach to train a BART model dedicated to generating the skeleton associated with each input.Subsequently, skeleton heuristics are encoded into the prompt to incentivize GPT-3.5 to generate desired questions. Extensive experiments demonstrate that SGSH derives the new state-of-the-art performance on the KBQG tasks.</abstract>
      <url hash="0bc37101">2024.findings-naacl.287</url>
      <bibkey>guo-etal-2024-sgsh</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.287</doi>
    </paper>
    <paper id="288">
      <title>Biomedical Entity Representation with Graph-Augmented Multi-Objective Transformer</title>
      <author><first>Andrey</first><last>Sakhovskiy</last><affiliation>Kazan Federal University</affiliation></author>
      <author><first>Natalia</first><last>Semenova</last></author>
      <author><first>Artur</first><last>Kadurin</last><affiliation>Artificial Intelligence Research Institute and Kuban State University</affiliation></author>
      <author><first>Elena</first><last>Tutubalina</last><affiliation>Kazan Federal University</affiliation></author>
      <pages>4626-4643</pages>
      <abstract>Modern biomedical concept representations are mostly trained on synonymous concept names from a biomedical knowledge base, ignoring the inter-concept interactions and a concept’s local neighborhood in a knowledge base graph. In this paper, we introduce Biomedical Entity Representation with a Graph-Augmented Multi-Objective Transformer (BERGAMOT), which adopts the power of pre-trained language models (LMs) and graph neural networks to capture both inter-concept and intra-concept interactions from the multilingual UMLS graph. To obtain fine-grained graph representations, we introduce two additional graph-based objectives: (i) a node-level contrastive objective and (ii) the Deep Graph Infomax (DGI) loss, which maximizes the mutual information between a local subgraph and a high-level graph summary. We apply contrastive loss on textual and graph representations to make them less sensitive to surface forms and enable intermodal knowledge exchange. BERGAMOT achieves state-of-the-art results in zero-shot entity linking without task-specific supervision on 4 of 5 languages of the Mantra corpus and on 8 of 10 languages of the XL-BEL benchmark.</abstract>
      <url hash="e3394007">2024.findings-naacl.288</url>
      <bibkey>sakhovskiy-etal-2024-biomedical</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.288</doi>
    </paper>
    <paper id="289">
      <title>Cross-Lingual Summarization with Pseudo-Label Regularization</title>
      <author><first>Thang</first><last>Le</last><affiliation>VinAI Research</affiliation></author>
      <pages>4644-4677</pages>
      <abstract>Cross-Lingual Summarization (XLS) aims to summarize a document in the source language into a condensed version in the target language, effectively removing language barriers for non-native readers. Previous approaches, however, have the same limitation that only a single reference (gold summary) is exploited during model training, making the base model exposed to an underrepresented hypothesis space since the actual number of possible hypotheses is exponentially large. To alleviate this problem, we present a study adopting pseudo-labels in regularizing standard cross-lingual summarization training. We investigate several components leading to the gains in regularization training with verified experiments involving 8 diverse languages from different families. Conclusively, we show that pseudo-labeling is a simple and effective approach that significantly improves over standard gold reference training in XLS.</abstract>
      <url hash="c6ebcdfd">2024.findings-naacl.289</url>
      <bibkey>le-2024-cross</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.289</doi>
    </paper>
    <paper id="290">
      <title>On the Way to Gentle <fixed-case>AI</fixed-case> Counselor: Politeness Cause Elicitation and Intensity Tagging in Code-mixed <fixed-case>H</fixed-case>inglish Conversations for Social Good</title>
      <author><first>Priyanshu</first><last>Priya</last></author>
      <author><first>Gopendra</first><last>Singh</last></author>
      <author><first>Mauajama</first><last>Firdaus</last></author>
      <author><first>Jyotsna</first><last>Agrawal</last></author>
      <author><first>Asif</first><last>Ekbal</last><affiliation>Indian Institute of Technology, Jodhpur</affiliation></author>
      <pages>4678-4696</pages>
      <abstract>Politeness is a multifaceted concept influenced by individual perceptions of what is considered polite or impolite. With this objective, we introduce a novel task - Politeness Cause Elicitation and Intensity Tagging (PCEIT). This task focuses on conversations and aims to identify the underlying reasons behind the use of politeness and gauge the degree of politeness conveyed. To address this objective, we create HING-POEM, a new conversational dataset in Hinglish (a blend of Hindi and English) for mental health and legal counseling of crime victims. The rationale for the domain selection lies in the paramount importance of politeness in mental health and legal counseling of crime victims to ensure a compassionate and cordial atmosphere for them. We enrich the HING-POEM dataset by annotating it with politeness labels, politeness causal spans, and intensity values at the level of individual utterances. In the context of the introduced PCEIT task, we present PAANTH (Politeness CAuse ElicitAion and INtensity Tagging in Hinglish), a comprehensive framework based on Contextual Enhanced Attentive Convolution Transformer. We conduct extensive quantitative and qualitative evaluations to establish the effectiveness of our proposed approach using the newly constructed dataset. Our approach is compared against state-of-the-art baselines, and these analyses help demonstrate the superiority of our method.</abstract>
      <url hash="5b8188d5">2024.findings-naacl.290</url>
      <bibkey>priya-etal-2024-way</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.290</doi>
    </paper>
    <paper id="291">
      <title>Leveraging Summarization for Unsupervised Dialogue Topic Segmentation</title>
      <author><first>Aleksei</first><last>Artemiev</last></author>
      <author><first>Daniil</first><last>Parinov</last><affiliation>Yandex</affiliation></author>
      <author><first>Alexey</first><last>Grishanov</last></author>
      <author><first>Ivan</first><last>Borisov</last></author>
      <author><first>Alexey</first><last>Vasilev</last><affiliation>Sber, AI Lab</affiliation></author>
      <author><first>Daniil</first><last>Muravetskii</last></author>
      <author><first>Aleksey</first><last>Rezvykh</last></author>
      <author><first>Aleksei</first><last>Goncharov</last><affiliation>MIL Team</affiliation></author>
      <author><first>Andrey</first><last>Savchenko</last><affiliation>Sber AI Lab and HSE University</affiliation></author>
      <pages>4697-4704</pages>
      <abstract>Traditional approaches to dialogue segmentation perform reasonably well on synthetic or written dialogues but suffer when dealing with spoken, noisy dialogs. In addition, such methods require careful tuning of hyperparameters. We propose to leverage a novel approach that is based on dialogue summaries. Experiments on different datasets showed that the new approach outperforms popular state-of-the-art algorithms in unsupervised topic segmentation and requires less setup.</abstract>
      <url hash="180ee476">2024.findings-naacl.291</url>
      <bibkey>artemiev-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.291</doi>
    </paper>
    <paper id="292">
      <title><fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>-Rider: Spurring Large Language Models to Explore the Open World</title>
      <author><first>Yicheng</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <author><first>Yuxuan</first><last>Wang</last></author>
      <author><first>Jiazheng</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Sipeng</first><last>Zheng</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Zongqing</first><last>Lu</last><affiliation>Peking University</affiliation></author>
      <pages>4705-4724</pages>
      <abstract>Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments and try to align the LLMs’ knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain. In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities. In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment. This facilitates exploration and enhances the model’s performance. Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experiences. By evaluation in Minecraft, an open-ended sandbox world, we demonstrate that our approach LLaMA-Rider enhances the efficiency of the LLM in exploring the environment, and effectively improves the LLM’s ability to accomplish more tasks through fine-tuning with merely 1.3k instances of collected data, showing minimal training costs compared to the baseline using reinforcement learning. The code is available at https://github.com/PKU-RL/LLaMA-Rider.</abstract>
      <url hash="63d613dc">2024.findings-naacl.292</url>
      <bibkey>feng-etal-2024-llama</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.292</doi>
    </paper>
    <paper id="293">
      <title>Contrastive Learning as a Polarizer: Mitigating Gender Bias by Fair and Biased sentences</title>
      <author><first>Kyungmin</first><last>Park</last><affiliation>Hankuk University of Foreign Studies</affiliation></author>
      <author><first>Sihyun</first><last>Oh</last></author>
      <author><first>Daehyun</first><last>Kim</last></author>
      <author><first>Juae</first><last>Kim</last><affiliation>Hankuk University of Foreign Studies</affiliation></author>
      <pages>4725-4736</pages>
      <abstract>Recently, language models have accelerated the improvement in natural language processing. However, recent studies have highlighted a significant issue: social biases inherent in training data can lead models to learn and propagate these biases. In this study, we propose a contrastive learning method for bias mitigation, utilizing anchor points to push further negatives and pull closer positives within the representation space. This approach employs stereotypical data as negatives and stereotype-free data as positives, enhancing debiasing performance. Our model attained state-of-the-art performance in the ICAT score on the StereoSet, a benchmark for measuring bias in models. In addition, we observed that effective debiasing is achieved through an awareness of biases, as evidenced by improved hate speech detection scores. The implementation code and trained models are available at https://github.com/HUFS-NLP/CL_Polarizer.git.</abstract>
      <url hash="acb6efba">2024.findings-naacl.293</url>
      <bibkey>park-etal-2024-contrastive</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.293</doi>
    </paper>
    <paper id="294">
      <title><fixed-case>P</fixed-case>o<fixed-case>LLM</fixed-case>graph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics</title>
      <author><first>Derui</first><last>Zhu</last></author>
      <author><first>Dingfan</first><last>Chen</last><affiliation>CISPA, saarland university, saarland informatics campus</affiliation></author>
      <author><first>Qing</first><last>Li</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Zongxiong</first><last>Chen</last><affiliation>Fraunhofer FOKUS</affiliation></author>
      <author><first>Lei</first><last>Ma</last><affiliation>The University of Tokyo and University of Alberta</affiliation></author>
      <author><first>Jens</first><last>Grossklags</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Mario</first><last>Fritz</last><affiliation>CISPA Helmholtz Center for Information Security and Saarland University</affiliation></author>
      <pages>4737-4751</pages>
      <abstract>Despite tremendous advancements in large language models (LLMs) over recent years, a notably urgent challenge for their practical deployment is the phenomenon of "<tex-math>\textit{hallucination}</tex-math>”, where the model fabricates facts and produces non-factual statements. In response, we propose <tex-math>\texttt{PoLLMgraph}</tex-math>—a Polygraph for LLMs—as an effective model-based white-box detection and forecasting approach. <tex-math>\texttt{PoLLMgraph}</tex-math> distinctly differs from the large body of existing research that concentrates on addressing such challenges through black-box evaluations. In particular, we demonstrate that hallucination can be effectively detected by analyzing the LLM’s internal state transition dynamics during generation via tractable probabilistic models. Experimental results on various open-source LLMs confirm the efficacy of <tex-math>\texttt{PoLLMgraph}</tex-math>, outperforming state-of-the-art methods by a considerable margin, evidenced by over 20% improvement in AUC-ROC on common benchmarking datasets like TruthfulQA. Our work paves a new way for model-based white-box analysis of LLMs, motivating the research community to further explore, understand, and refine the intricate dynamics of LLM behaviors.</abstract>
      <url hash="0243566a">2024.findings-naacl.294</url>
      <bibkey>zhu-etal-2024-pollmgraph</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.294</doi>
    </paper>
    <paper id="295">
      <title>Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval</title>
      <author><first>Juraj</first><last>Vladika</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>4752-4763</pages>
      <abstract>In today’s digital world, seeking answers to health questions on the Internet is a common practice. However, existing question answering (QA) systems often rely on using pre-selected and annotated evidence documents, thus making them inadequate for addressing novel questions. Our study focuses on the open-domain QA setting, where the key challenge is to first uncover relevant evidence in large knowledge bases. By utilizing the common retrieve-then-read QA pipeline and PubMed as a trustworthy collection of medical research documents, we answer health questions from three diverse datasets. We modify different retrieval settings to observe their influence on the QA pipeline’s performance, including the number of retrieved documents, sentence selection process, the publication year of articles, and their number of citations. Our results reveal that cutting down on the amount of retrieved documents and favoring more recent and highly cited documents can improve the final macro F1 score up to 10%. We discuss the results, highlight interesting examples, and outline challenges for future research, like managing evidence disagreement and crafting user-friendly explanations.</abstract>
      <url hash="b9df5303">2024.findings-naacl.295</url>
      <bibkey>vladika-matthes-2024-improving</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.295</doi>
    </paper>
    <paper id="296">
      <title><fixed-case>D</fixed-case>ecoder<fixed-case>L</fixed-case>ens: Layerwise Interpretation of Encoder-Decoder Transformers</title>
      <author><first>Anna</first><last>Langedijk</last><affiliation>Utrecht University (ICS), Utrecht University and University of Amsterdam</affiliation></author>
      <author><first>Hosein</first><last>Mohebbi</last></author>
      <author><first>Gabriele</first><last>Sarti</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Willem</first><last>Zuidema</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Jaap</first><last>Jumelet</last></author>
      <pages>4764-4780</pages>
      <abstract>In recent years, several interpretability methods have been proposed to interpret the inner workings of Transformer models at different levels of precision and complexity.In this work, we propose a simple but effective technique to analyze encoder-decoder Transformers. Our method, which we name DecoderLens, allows the decoder to cross-attend representations of intermediate encoder activations instead of using the default final encoder output.The method thus maps uninterpretable intermediate vector representations to human-interpretable sequences of words or symbols, shedding new light on the information flow in this popular but understudied class of models.We apply DecoderLens to question answering, logical reasoning, speech recognition and machine translation models, finding that simpler subtasks are solved with high precision by low and intermediate encoder layers.</abstract>
      <url hash="ebb79566">2024.findings-naacl.296</url>
      <bibkey>langedijk-etal-2024-decoderlens</bibkey>
      <doi>10.18653/v1/2024.findings-naacl.296</doi>
    </paper>
  </volume>
  <volume id="acl" ingest-date="2024-08-12" type="proceedings">
    <meta>
      <booktitle>Findings of the Association for Computational Linguistics ACL 2024</booktitle>
      <editor><first>Lun-Wei</first><last>Ku</last><affiliation>Academia Sinica</affiliation></editor>
      <editor><first>Andre</first><last>Martins</last><affiliation>Instituto Superior Técnico / Instituto de Telecomunicações / Unbabel</affiliation></editor>
      <editor><first>Vivek</first><last>Srikumar</last><affiliation>University of Utah</affiliation></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bangkok, Thailand and virtual meeting</address>
      <month>August</month>
      <year>2024</year>
      <url hash="7ea27750">2024.findings-acl</url>
      <venue>findings</venue>
    </meta>
    <frontmatter>
      <url hash="035121fd">2024.findings-acl.0</url>
      <bibkey>findings-2024-acl</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Controllable Data Augmentation for Few-Shot Text Mining with Chain-of-Thought Attribute Manipulation</title>
      <author><first>Letian</first><last>Peng</last></author>
      <author><first>Yuwei</first><last>Zhang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>1-16</pages>
      <abstract>Prompting large language models (LLMs) for data augmentation has recently become a common practice in few-shot NLP tasks. In this paper, we propose Chain-of-Thought Attribute Manipulation (CoTAM), a novel approach that generates new data from existing examples by only tweaking in the user-provided, task-specific attribute, e.g., sentiment polarity or topic in movie reviews. Instead of conventional latent representation controlling, we leverage the chain-of-thought prompting to directly edit the text in three steps, (1) attribute decomposition, (2) manipulation proposal, and (3) sentence reconstruction. Extensive results on various tasks, such as text (pair) classification and aspect-based sentiment analysis, verify the superiority of CoTAM over other LLM-based augmentation methods with the same number of training examples for both fine-tuning and in-context learning. Remarkably, the 2D visualization of the augmented dataset using principle component analysis revealed a human-recognizable decision boundary that is likely hinted by the attribute manipulation, demonstrating the potential of our proposed approach.</abstract>
      <url hash="3eea4f96">2024.findings-acl.1</url>
      <bibkey>peng-etal-2024-controllable</bibkey>
      <doi>10.18653/v1/2024.findings-acl.1</doi>
    </paper>
    <paper id="2">
      <title>Match More, Extract Better! Hybrid Matching Model for Open Domain Web Keyphrase Extraction</title>
      <author><first>Mingyang</first><last>Song</last><affiliation>Tencent</affiliation></author>
      <author><first>Liping</first><last>Jing</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yi</first><last>Feng</last></author>
      <pages>17-27</pages>
      <abstract>Keyphrase extraction aims to automatically extract salient phrases representing the critical information in the source document. Identifying salient phrases is challenging because there is a lot of noisy information in the document, leading to wrong extraction. To address this issue, in this paper, we propose a hybrid matching model for keyphrase extraction, which combines representation-focused and interaction-based matching modules into a unified framework for improving the performance of the keyphrase extraction task. Specifically, HybridMatch comprises (1) a PLM-based Siamese encoder component that represents both candidate phrases and documents, (2) an interaction-focused matching (IM) component that estimates word matches between candidate phrases and the corresponding document at the word level, and (3) a representation-focused matching (RM) component captures context-aware semantic relatedness of each candidate keyphrase at the phrase level. Extensive experimental results on the OpenKP dataset demonstrate that the performance of the proposed model HybridMatch outperforms the recent state-of-the-art keyphrase extraction baselines. Furthermore, we discuss the performance of large language models in keyphrase extraction based on recent studies and our experiments.</abstract>
      <url hash="cca820a4">2024.findings-acl.2</url>
      <bibkey>song-etal-2024-match</bibkey>
      <doi>10.18653/v1/2024.findings-acl.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>AFPQ</fixed-case>: Asymmetric Floating Point Quantization for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Yijia</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Sicheng</first><last>Zhang</last></author>
      <author><first>Shijie</first><last>Cao</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>DaYou</first><last>Du</last><affiliation>HKUST(GZ)</affiliation></author>
      <author><first>Jianyu</first><last>Wei</last></author>
      <author><first>Ting</first><last>Cao</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Ningyi</first><last>Xu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>28-36</pages>
      <abstract>Large language models (LLMs) show great performance in various tasks, but face deployment challenges from limited memory capacity and bandwidth.Low-bit weight quantization can save memory and accelerate inference.Although floating-point (FP) formats show good performance in LLM quantization, they tend to perform poorly with small group sizes or sub-4 bits.We find the reason is that the absence of asymmetry in previous FP quantization makes it unsuitable for handling asymmetric value distribution of LLM weight tensors.In this work, we propose asymmetric FP quantization (AFPQ), which sets separate scales for positive and negative values.Our method leads to large accuracy improvements and can be easily plugged into other quantization methods, including GPTQ and AWQ, for better performance.Besides, no additional storage is needed compared with asymmetric integer (INT) quantization.The code is available at https://github.com/zhangsichengsjtu/AFPQ.</abstract>
      <url hash="9b2d8a3b">2024.findings-acl.3</url>
      <bibkey>zhang-etal-2024-afpq</bibkey>
      <doi>10.18653/v1/2024.findings-acl.3</doi>
    </paper>
    <paper id="4">
      <title>End-to-End Emotion Semantic Parsing</title>
      <author><first>Xiaotong</first><last>Jiang</last></author>
      <author><first>Zhongqing</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Guodong</first><last>Zhou</last><affiliation>Soochow University, China</affiliation></author>
      <pages>37-47</pages>
      <abstract>Emotion detection is the task of automatically associating one or more emotions with a text. The emotions are experienced, targeted, and caused by different semantic constituents. Therefore, it is necessary to incorporate these semantic constituents into the process of emotion detection. In this study, we propose a new task called emotion semantic parsing which aims to parse the emotion and semantic constituents into an abstract semantic tree structure. In particular, we design an end-to-end generation model to capture the relations between emotion and all the semantic constituents, and to generate them jointly. Furthermore, we employ a task decomposition strategy to capture the semantic relation among these constituents in a more cognitive and structural way. Experimental results demonstrate the importance of the proposed task, and indicate the proposed model gives superior performance compared to other models.</abstract>
      <url hash="84f4fbba">2024.findings-acl.4</url>
      <bibkey>jiang-etal-2024-end</bibkey>
      <doi>10.18653/v1/2024.findings-acl.4</doi>
    </paper>
    <paper id="5">
      <title>Overcoming Catastrophic Forgetting by Exemplar Selection in Task-oriented Dialogue System</title>
      <author><first>Chen</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Ruizhe</first><last>Li</last><affiliation>University of Aberdeen</affiliation></author>
      <author><first>Yuchen</first><last>Hu</last></author>
      <author><first>Yuanyuan</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Chengwei</first><last>Qin</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Qiang</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>48-61</pages>
      <abstract>Intelligent task-oriented dialogue systems (ToDs) are expected to continuously acquire new knowledge, also known as Continual Learning (CL), which is crucial to fit ever-changing user needs. However, catastrophic forgetting dramatically degrades the model performance in face of a long streamed curriculum. In this paper, we aim to overcome the forgetting problem in ToDs and propose a method (HESIT) with hyper-gradient-based exemplar strategy, which samples influential exemplars for periodic retraining. Instead of unilaterally observing data or models, HESIT adopts a profound exemplar selection strategy that considers the general performance of the trained model when selecting exemplars for each task domain. Specifically, HESIT analyzes the training data influence by tracing their hyper-gradient in the optimization process. Furthermore, HESIT avoids estimating Hessian to make it compatible for ToDs with a large pre-trained model. Experimental results show that HESIT effectively alleviates catastrophic forgetting by exemplar selection, and achieves state-of-the-art performance on the largest CL benchmark of ToDs in terms of all metrics.</abstract>
      <url hash="7dc1f2b6">2024.findings-acl.5</url>
      <bibkey>chen-etal-2024-overcoming</bibkey>
      <doi>10.18653/v1/2024.findings-acl.5</doi>
    </paper>
    <paper id="6">
      <title>Unveiling Imitation Learning: Exploring the impact of Data Falsity to Large Language Model</title>
      <author><first>Hyunsoo</first><last>Cho</last><affiliation>Ewha Women’s University</affiliation></author>
      <pages>62-73</pages>
      <abstract>Many recent studies endeavor to improve open-sourced language models through imitation learning, re-training on the synthetic instruction data from state-of-the-art proprietary models like ChatGPT and GPT-4.However, the innate nature of synthetic data inherently contains noisy data, giving rise to a substantial presence of low-quality data replete with misleading queries, erroneous responses, and flawed reasoning.Although we intuitively grasp the potential harm of noisy data, we lack a quantitative understanding of its impact.To this end, this paper explores correlation between the degree of noise and its impact on language models through instruction tuning.We first introduce the Falsity-Controllable () dataset, which comprises pairs of true answers and corresponding reasoning, as well as false pairs to manually control the factuality ratio of the dataset.Through our extensive experiments, we found multiple intriguing findings of the correlation between factuality and instruction tuning. Specifically, factuality can significantly impact various benchmark characteristics especially when benchmarks are related to knowledge domain, and initial data quality plays a critical role, whereas the number of learning steps has a lesser impact.Additionally, we noted that once the language model is trained with a dataset contaminated by noise, restoring its original performance becomes exceptionally challenging, verging on irreversible.</abstract>
      <url hash="069ce955">2024.findings-acl.6</url>
      <bibkey>cho-2024-unveiling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.6</doi>
    </paper>
    <paper id="7">
      <title>The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?</title>
      <author><first>Alex</first><last>Gu</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Wen-Ding</first><last>Li</last><affiliation>Cornell University</affiliation></author>
      <author><first>Naman</first><last>Jain</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Theo</first><last>Olausson</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Celine</first><last>Lee</last><affiliation>Cornell University</affiliation></author>
      <author><first>Koushik</first><last>Sen</last><affiliation>UC Berkeley, University of California, Berkeley</affiliation></author>
      <author><first>Armando</first><last>Solar-Lezama</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>74-117</pages>
      <abstract>While language models are increasingly more proficient at code generation, they still frequently generate incorrect programs. Many of these programs are obviously wrong, but others are more subtle and pass weaker correctness checks such as being able to compile. In this work, we focus on these counterfeit samples: programs sampled from a language model that 1) have a high enough log-probability to be generated at a moderate temperature and 2) pass weak correctness checks. Overall, we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit is often even lower than that of sampling a correct program from scratch. Counterfeits also have very unexpected properties: first, counterfeit programs for problems that are easier for a model to solve are not necessarily easier to detect and only slightly easier to execute and repair. Second, counterfeits from a given model are just as confusing to the model itself as they are to other models. Finally, both strong and weak models are able to generate counterfeit samples that equally challenge all models. In light of our findings, we recommend that care and caution be taken when relying on models to understand their own samples, especially when no external feedback is incorporated.</abstract>
      <url hash="e67ad5e5">2024.findings-acl.7</url>
      <bibkey>gu-etal-2024-counterfeit</bibkey>
      <doi>10.18653/v1/2024.findings-acl.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>CHIME</fixed-case>: <fixed-case>LLM</fixed-case>-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support</title>
      <author><first>Chao-Chun</first><last>Hsu</last><affiliation>University of Chicago</affiliation></author>
      <author><first>Erin</first><last>Bransom</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Jenna</first><last>Sparks</last></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Chenhao</first><last>Tan</last><affiliation>University of Chicago</affiliation></author>
      <author><first>David</first><last>Wadden</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Lucy</first><last>Wang</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Aakanksha</first><last>Naik</last><affiliation>Allen Institute for Artificial Intelligence and National Institutes of Health</affiliation></author>
      <pages>118-132</pages>
      <abstract>Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands. In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review. We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category. Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine. Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics. Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved. We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points. We release our dataset and models to encourage research on developing better assistive tools for literature review.</abstract>
      <url hash="0dc50954">2024.findings-acl.8</url>
      <bibkey>hsu-etal-2024-chime</bibkey>
      <doi>10.18653/v1/2024.findings-acl.8</doi>
    </paper>
    <paper id="9">
      <title>Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation</title>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Yuping</first><last>Wu</last></author>
      <author><first>Viktor</first><last>Schlegel</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Tharindu</first><last>Madusanka</last></author>
      <author><first>Iqra</first><last>Zahid</last></author>
      <author><first>Jiayan</first><last>Zeng</last></author>
      <author><first>Xiaochi</first><last>Wang</last></author>
      <author><first>Xinran</first><last>He</last></author>
      <author><first>Yizhi</first><last>Li</last><affiliation>University of Manchester and University of Sheffield</affiliation></author>
      <author><first>Goran</first><last>Nenadic</last><affiliation>University of Manchester</affiliation></author>
      <pages>133-150</pages>
      <abstract>With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments. Previous work attempted this task by integrating multiple components. In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are fully annotated with various properties supporting the aforementioned tasks. We evaluate multiple generative baselines for each of these tasks, including representative LLMs. We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation. This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation. The repository of this project is available at https://github.com/HarrywillDr/ArgSum-Datatset.</abstract>
      <url hash="4f5652f8">2024.findings-acl.9</url>
      <bibkey>li-etal-2024-side</bibkey>
      <doi>10.18653/v1/2024.findings-acl.9</doi>
    </paper>
    <paper id="10">
      <title>A Grounded Preference Model for <fixed-case>LLM</fixed-case> Alignment</title>
      <author><first>Tahira</first><last>Naseem</last><affiliation>IBM, International Business Machines</affiliation></author>
      <author><first>Guangxuan</first><last>Xu</last><affiliation>IBM Research</affiliation></author>
      <author><first>Sarathkrishna</first><last>Swaminathan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Asaf</first><last>Yehudai</last></author>
      <author><first>Subhajit</first><last>Chaudhury</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Radu</first><last>Florian</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ramón</first><last>Astudillo</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Asim</first><last>Munawar</last><affiliation>International Business Machines</affiliation></author>
      <pages>151-162</pages>
      <abstract>Despite LLMs’ recent advancements, they still suffer from factual inconsistency and hallucination. An often-opted remedy is retrieval-augmented generation – however, there is no guarantee that the model will strictly adhere to retrieved grounding. Fundamentally, LLMs need to be aligned to be more faithful to grounding, which will require high-quality preference annotations. This paper investigates whether we can create high-quality grounded preference data for model alignment without using annotations from humans or large proprietary models. We experimented with existing entailment data and proposed approaches to generate synthetic grounded preference data, with which we train a Grounded Preference Model(GPM). We demonstrate through Proximal Policy Optimization(PPO) training of Mistral-7B-Instruct that our GPM model can successfully align powerful LLMs to generate much better grounded responses as judged by GPT4. Moreover, we show that our GPM is also a great faithfulness classifier, achieving SoTA in dialogue sub-tasks of the TRUE faithfulness Benchmark. We will release our GPM under the Apache 2.0 license.</abstract>
      <url hash="79373acd">2024.findings-acl.10</url>
      <bibkey>naseem-etal-2024-grounded</bibkey>
      <doi>10.18653/v1/2024.findings-acl.10</doi>
    </paper>
    <paper id="11">
      <title>Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs</title>
      <author><first>Bowen</first><last>Jin</last></author>
      <author><first>Chulin</first><last>Xie</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Zhang</last></author>
      <author><first>Kashob Kumar</first><last>Roy</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Yu</first><last>Zhang</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Zheng</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Ruirui</first><last>Li</last></author>
      <author><first>Xianfeng</first><last>Tang</last><affiliation>Amazon</affiliation></author>
      <author><first>Suhang</first><last>Wang</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Yu</first><last>Meng</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>163-184</pages>
      <abstract>Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBench, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRBench, where Graph-CoT outperforms the baselines consistently. The code is available at https://github.com/PeterGriffinJin/Graph-CoT/.</abstract>
      <url hash="d3f2671f">2024.findings-acl.11</url>
      <bibkey>jin-etal-2024-graph</bibkey>
      <doi>10.18653/v1/2024.findings-acl.11</doi>
    </paper>
    <paper id="12">
      <title><fixed-case>T</fixed-case>ext2<fixed-case>DB</fixed-case>: Integration-Aware Information Extraction with Large Language Model Agents</title>
      <author><first>Yizhu</first><last>Jiao</last><affiliation>UIUC</affiliation></author>
      <author><first>Sha</first><last>Li</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Sizhe</first><last>Zhou</last></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>185-205</pages>
      <abstract>The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE, Text2DB, that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for <i>what to extract</i> and adapting to the given DB/KB schema for <i>how to extract</i> on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-Plan-Analyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation.</abstract>
      <url hash="31f38f2b">2024.findings-acl.12</url>
      <bibkey>jiao-etal-2024-text2db</bibkey>
      <doi>10.18653/v1/2024.findings-acl.12</doi>
    </paper>
    <paper id="13">
      <title>How Important is a Language Model for Low-resource <fixed-case>ASR</fixed-case>?</title>
      <author><first>Zoey</first><last>Liu</last><affiliation>University of Florida</affiliation></author>
      <author><first>Nitin</first><last>Venkateswaran</last></author>
      <author><first>Eric</first><last>Le Ferrand</last><affiliation>Boston College</affiliation></author>
      <author><first>Emily</first><last>Prud’hommeaux</last><affiliation>Boston College</affiliation></author>
      <pages>206-213</pages>
      <abstract>N-gram language models (LMs) are the innovation that first made large-vocabulary continuous automatic speech recognition (ASR) viable. With neural end-to-end ASR architectures, however, LMs have become an afterthought. While the effect on accuracy may be negligible for English and Mandarin, jettisoning the LM might not make sense for the world’s remaining 6000+ languages. In this paper, we investigate the role of the LM in low-resource ASR. First we ask: does using an n-gram LM in decoding in neural architectures help ASR performance? While it may seem obvious that it should, its absence in most implementations suggests otherwise. Second, we ask: when an n-gram LM is used in ASR, is there a relationship between the size of the LM and ASR accuracy? We have discovered that gut feelings on this question vary considerably, but there is little empirical work to support any particular claim. We explore these questions “in the wild” using a deliberately diverse set of 9 very small ASR corpora. The results show that: (1) decoding with an n-gram LM, regardless of its size, leads to lower word error rates; and (2) increasing the size of the LM appears to yield improvements only when the audio corpus itself is already relatively large. This suggests that collecting additional LM training text may benefit widely-spoken languages which typically have larger audio corpora. In contrast, for endangered languages where data of any kind will always be limited, efforts may be better spent collecting additional transcribed audio.</abstract>
      <url hash="c3f43d5c">2024.findings-acl.13</url>
      <bibkey>liu-etal-2024-important</bibkey>
      <doi>10.18653/v1/2024.findings-acl.13</doi>
    </paper>
    <paper id="14">
      <title><fixed-case>M</fixed-case>edi<fixed-case>S</fixed-case>wift: Efficient Sparse Pre-trained Biomedical Language Models</title>
      <author><first>Vithursan</first><last>Thangarasa</last><affiliation>Cerebras Systems, Inc</affiliation></author>
      <author><first>Mahmoud</first><last>Salem</last><affiliation>Cerebras Systems, Inc</affiliation></author>
      <author><first>Shreyas</first><last>Saxena</last><affiliation>Cerebras Systems, Inc</affiliation></author>
      <author><first>Chen-Yu</first><last>Leong</last></author>
      <author><first>Joel</first><last>Hestness</last><affiliation>Cerebras Systems, Inc</affiliation></author>
      <author><first>Sean</first><last>Lie</last><affiliation>Cerebras Systems, Inc</affiliation></author>
      <pages>214-230</pages>
      <abstract>Large language models (LLMs) are typically trained on general source data forvarious domains, but a recent surge in domain-specific LLMs has shown theirpotential to outperform general-purpose models in domain-specific tasks (e.g.,biomedicine). Although domain-specific pre-training enhances efficiency andleads to smaller models, the computational costs of training these LLMs remainhigh, posing budgeting challenges. We introduce MediSwift, a suite of biomedicalLMs that leverage sparse pre-training on domain-specific biomedical text data.By inducing up to 75% weight sparsity during the pre-training phase, MediSwiftachieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-trainingwas performed on the Cerebras CS-2 system, which is specifically designed torealize the acceleration benefits from unstructured weight sparsity, therebysignificantly enhancing the efficiency of the MediSwift models. Throughsubsequent dense fine-tuning and strategic soft prompting, MediSwift modelsoutperform existing LLMs up to 7B parameters on biomedical tasks, setting newbenchmarks w.r.t efficiency-accuracy on tasks such as PubMedQA. Our results showthat sparse pre-training, along with dense fine-tuning and soft prompting,offers an effective method for creating high-performing, computationallyefficient models in specialized domains.</abstract>
      <url hash="277228db">2024.findings-acl.14</url>
      <bibkey>thangarasa-etal-2024-mediswift</bibkey>
      <doi>10.18653/v1/2024.findings-acl.14</doi>
    </paper>
    <paper id="15">
      <title>Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling</title>
      <author><first>Chengxu</first><last>Zhuang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Evelina</first><last>Fedorenko</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology and Microsoft</affiliation></author>
      <pages>231-247</pages>
      <abstract>Today’s most accurate language models are trained on orders of magnitude more language data than human language learners receive— but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs’ representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive Grounding combines a next-token prediction strategy with a contrastive visual grounding objective, focusing on early-layerrepresentations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, LexiContrastiveGrounding not only outperforms standard language-only models in terms of learning efficiency in small and developmentally plausible data regimes, but also improves upon vision-and-language learning procedures including CLIP, GIT, Flamingo, and Vokenization.Moreover, LexiContrastive Grounding improves perplexity by around 5% on multiple language modeling tasks compared to other models trained on the same amount of text data. This work underscores the potential of incorporating visual grounding into language models, aligning more closely with the multimodal nature of human language acquisition.</abstract>
      <url hash="319d64af">2024.findings-acl.15</url>
      <bibkey>zhuang-etal-2024-lexicon</bibkey>
      <doi>10.18653/v1/2024.findings-acl.15</doi>
    </paper>
    <paper id="16">
      <title><fixed-case>P</fixed-case>-<fixed-case>TA</fixed-case>: Using Proximal Policy Optimization to Enhance Tabular Data Augmentation via Large Language Models</title>
      <author><first>Shuo</first><last>Yang</last></author>
      <author><first>Chenchen</first><last>Yuan</last></author>
      <author><first>Yao</first><last>Rong</last></author>
      <author><first>Felix</first><last>Steinbauer</last><affiliation>Department of Informatics, Technische Universität München</affiliation></author>
      <author><first>Gjergji</first><last>Kasneci</last><affiliation>Technische Universität München and University of Tuebingen</affiliation></author>
      <pages>248-264</pages>
      <abstract>A multitude of industries depend on accurate and reasonable tabular data augmentation for their business processes. Contemporary methodologies in generating tabular data revolve around utilizing Generative Adversarial Networks (GAN) or fine-tuning Large Language Models (LLM). However, GAN-based approaches are documented to produce samples with common-sense errors attributed to the absence of external knowledge. On the other hand, LLM-based methods exhibit a limited capacity to capture the disparities between synthesized and actual data distribution due to the absence of feedback from a discriminator during training. Furthermore, the decoding of LLM-based generation introduces gradient breakpoints, impeding the backpropagation of loss from a discriminator, thereby complicating the integration of these two approaches. To solve this challenge, we propose using proximal policy optimization (PPO) to apply GANs, guiding LLMs to enhance the probability distribution of tabular features. This approach enables the utilization of LLMs as generators for GANs in synthesizing tabular data. Our experiments demonstrate that PPO leads to an approximately 4% improvement in the accuracy of models trained on synthetically generated data over state-of-the-art across three real-world datasets.</abstract>
      <url hash="b3a69b1e">2024.findings-acl.16</url>
      <bibkey>yang-etal-2024-p</bibkey>
      <doi>10.18653/v1/2024.findings-acl.16</doi>
    </paper>
    <paper id="17">
      <title>Teaching-Assistant-in-the-Loop: Improving Knowledge Distillation from Imperfect Teacher Models in Low-Budget Scenarios</title>
      <author><first>Yuhang</first><last>Zhou</last></author>
      <author><first>Wei</first><last>Ai</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>265-282</pages>
      <abstract>There is increasing interest in distilling task-specific knowledge from large language models (LLM) to smaller student models.Nonetheless, LLM distillation presents a dual challenge: 1) there is a high cost associated with querying the teacher LLM, such as GPT-4, for gathering an ample number of demonstrations; 2) the teacher LLM might provide imperfect outputs with a negative impact on the student’s learning process. To enhance sample efficiency within resource-constrained, imperfect teacher scenarios, we propose a three-component framework leveraging three signal types. The first signal is the student’s self-consistency (consistency of student multiple outputs), which is a proxy of the student’s confidence. Specifically, we introduce a ”teaching assistant” (TA) model to assess the uncertainty of both the student’s and the teacher’s outputs via confidence scoring, which serves as another two signals for student training. Furthermore, we propose a two-stage training schema to first warm up the student with a small proportion of data to better utilize student’s signal. Experiments have shown the superiority of our proposed framework for four complex reasoning tasks. On average, our proposed two-stage framework brings a relative improvement of up to 20.79% compared to fine-tuning without any signals across datasets.</abstract>
      <url hash="888d3eb2">2024.findings-acl.17</url>
      <bibkey>zhou-ai-2024-teaching</bibkey>
      <doi>10.18653/v1/2024.findings-acl.17</doi>
    </paper>
    <paper id="18">
      <title>Small Models are Valuable Plug-ins for Large Language Models</title>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Yichong</first><last>Xu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <author><first>Chenguang</first><last>Zhu</last><affiliation>Zoom</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <pages>283-294</pages>
      <abstract>Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of supervised examples due to context length limits. In this paper, we propose Super In-Context Learning (SuperICL) which allows black-box LLMs to work with locally fine-tuned smaller models, resulting in superior performance on supervised tasks. Our experiments demonstrate that SuperICL can improve performance beyond state-of-the-art fine-tuned models while addressing the instability problem of in-context learning.</abstract>
      <url hash="8e7dcfef">2024.findings-acl.18</url>
      <bibkey>xu-etal-2024-small</bibkey>
      <doi>10.18653/v1/2024.findings-acl.18</doi>
    </paper>
    <paper id="19">
      <title>Are self-explanations from Large Language Models faithful?</title>
      <author><first>Andreas</first><last>Madsen</last><affiliation>Montreal Institute for Learning Algorithms, École Polytechnique de Montréal, Université de Montréal and Mila</affiliation></author>
      <author><first>Sarath</first><last>Chandar</last><affiliation>Polytechnique Montreal</affiliation></author>
      <author><first>Siva</first><last>Reddy</last><affiliation>Mila, McGill University and Mila, McGill University</affiliation></author>
      <pages>295-337</pages>
      <abstract>Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it’s important to measure if self-explanations truly reflect the model’s behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.</abstract>
      <url hash="49e0de62">2024.findings-acl.19</url>
      <bibkey>madsen-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.findings-acl.19</doi>
    </paper>
    <paper id="20">
      <title><fixed-case>I</fixed-case>mplicit<fixed-case>AVE</fixed-case>: An Open-Source Dataset and Multimodal <fixed-case>LLM</fixed-case>s Benchmark for Implicit Attribute Value Extraction</title>
      <author><first>Henry</first><last>Zou</last></author>
      <author><first>Vinay</first><last>Samuel</last></author>
      <author><first>Yue</first><last>Zhou</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Weizhi</first><last>Zhang</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Liancheng</first><last>Fang</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Zihe</first><last>Song</last></author>
      <author><first>Philip</first><last>Yu</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>338-354</pages>
      <abstract>Existing datasets for attribute value extraction (AVE) predominantly focus on explicit attribute values while neglecting the implicit ones, lack product images, are often not publicly available, and lack an in-depth human inspection across diverse domains. To address these limitations, we present ImplicitAVE, the first, publicly available multimodal dataset for implicit attribute value extraction. ImplicitAVE, sourced from the MAVE dataset, is carefully curated and expanded to include implicit AVE and multimodality, resulting in a refined dataset of 68k training and 1.6k testing data across five domains. We also explore the application of multimodal large language models (MLLMs) to implicit AVE, establishing a comprehensive benchmark for MLLMs on the ImplicitAVE dataset. Six recent MLLMs with eleven variants are evaluated across diverse settings, revealing that implicit value extraction remains a challenging task for MLLMs. The contributions of this work include the development and release of ImplicitAVE, and the exploration and benchmarking of various MLLMs for implicit AVE, providing valuable insights and potential future research directions. Dataset and code are available at https://github.com/HenryPengZou/ImplicitAVE.</abstract>
      <url hash="d75c8eae">2024.findings-acl.20</url>
      <bibkey>zou-etal-2024-implicitave</bibkey>
      <doi>10.18653/v1/2024.findings-acl.20</doi>
    </paper>
    <paper id="21">
      <title>Prompt Engineering a Prompt Engineer</title>
      <author><first>Qinyuan</first><last>Ye</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Mohamed</first><last>Ahmed</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Reid</first><last>Pryzant</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Fereshte</first><last>Khani</last><affiliation>Microsoft</affiliation></author>
      <pages>355-385</pages>
      <abstract>Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model’s errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform “let’s think step by step” by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks by 6.9%. Further, we show that PE2 can make targeted prompt edits, rectify erroneous prompts, and induce multi-step plans for complex tasks.</abstract>
      <url hash="ffe35e03">2024.findings-acl.21</url>
      <bibkey>ye-etal-2024-prompt</bibkey>
      <doi>10.18653/v1/2024.findings-acl.21</doi>
    </paper>
    <paper id="22">
      <title><fixed-case>ASPIRE</fixed-case>: Language-Guided Data Augmentation for Improving Robustness Against Spurious Correlations</title>
      <author><first>Sreyan</first><last>Ghosh</last></author>
      <author><first>Chandra Kiran</first><last>Evuru</last></author>
      <author><first>Sonal</first><last>Kumar</last></author>
      <author><first>Utkarsh</first><last>Tyagi</last></author>
      <author><first>S</first><last>Sakshi</last></author>
      <author><first>Sanjoy</first><last>Chowdhury</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>386-406</pages>
      <abstract>Neural image classifiers can often learn to make predictions by overly relying on non-predictive features that are spuriously correlated with the class labels in the training data. This leads to poor performance in real-world atypical scenarios where such features are absent. This paper presents ASPIRE (Language-guided Data Augmentation for SPurIous correlation REmoval), a simple yet effective solution for supplementing the training dataset with images without spurious features, for robust learning against spurious correlations via better generalization. ASPIRE, guided by language at various steps, can generate non-spurious images without requiring any group labeling or existing non-spurious images in the training set. Precisely, we employ LLMs to first extract foreground and background features from textual descriptions of an image, followed by advanced language-guided image editing to discover the features that are spuriously correlated with the class label. Finally, we personalize a text-to-image generation model using the edited images to generate diverse in-domain images without spurious features. ASPIRE is complementary to all prior robust training methods in literature, and we demonstrate its effectiveness across 4 datasets and 9 baselines and show that ASPIRE improves the worst-group classification accuracy of prior methods by 1% - 38%. We also contribute a novel test set for the challenging Hard ImageNet dataset.</abstract>
      <url hash="2d71bf17">2024.findings-acl.22</url>
      <bibkey>ghosh-etal-2024-aspire</bibkey>
      <doi>10.18653/v1/2024.findings-acl.22</doi>
    </paper>
    <paper id="23">
      <title>Tables as Texts or Images: Evaluating the Table Reasoning Ability of <fixed-case>LLM</fixed-case>s and <fixed-case>MLLM</fixed-case>s</title>
      <author><first>Naihao</first><last>Deng</last></author>
      <author><first>Zhenjie</first><last>Sun</last></author>
      <author><first>Ruiqi</first><last>He</last></author>
      <author><first>Aman</first><last>Sikka</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Yulong</first><last>Chen</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Lin</first><last>Ma</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <author><first>Rada</first><last>Mihalcea</last><affiliation>University of Michigan</affiliation></author>
      <pages>407-426</pages>
      <abstract>Tables contrast with unstructured text data by its structure to organize the information.In this paper, we investigate the efficiency of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We pioneer in the assessment of LLMs’ performance on image-based table representation. Specifically, we compare five text-based and three image-based table representations, revealing the influence of representation and prompting on LLM performance. We hope our study provides researchers insights into optimizing LLMs’ application in table-related tasks.</abstract>
      <url hash="ffc69db1">2024.findings-acl.23</url>
      <bibkey>deng-etal-2024-tables</bibkey>
      <doi>10.18653/v1/2024.findings-acl.23</doi>
    </paper>
    <paper id="24">
      <title>Biasly: An Expert-Annotated Dataset for Subtle Misogyny Detection and Mitigation</title>
      <author><first>Brooklyn</first><last>Sheppard</last></author>
      <author><first>Anna</first><last>Richter</last><affiliation>Mila - Quebec AI Institute</affiliation></author>
      <author><first>Allison</first><last>Cohen</last></author>
      <author><first>Elizabeth</first><last>Smith</last><affiliation>Université du Québec à Montréal</affiliation></author>
      <author><first>Tamara</first><last>Kneese</last></author>
      <author><first>Carolyne</first><last>Pelletier</last></author>
      <author><first>Ioana</first><last>Baldini</last></author>
      <author><first>Yue</first><last>Dong</last><affiliation>University of California, Riverside and McGill University</affiliation></author>
      <pages>427-452</pages>
      <abstract>Using novel approaches to dataset development, the Biasly dataset captures the nuance and subtlety of misogyny in ways that are unique within the literature. Built in collaboration with multi-disciplinary experts and annotators themselves, the dataset contains annotations of movie subtitles, capturing colloquial expressions of misogyny in North American film. The open-source dataset can be used for a range of NLP tasks, including binary and multi-label classification, severity score regression, and text generation for rewrites. In this paper, we discuss the methodology used, analyze the annotations obtained, provide baselines for each task using common NLP algorithms, and furnish error analyses to give insight into model behaviour when fine-tuned on the Biasly dataset.</abstract>
      <url hash="1dec80a9">2024.findings-acl.24</url>
      <bibkey>sheppard-etal-2024-biasly</bibkey>
      <doi>10.18653/v1/2024.findings-acl.24</doi>
    </paper>
    <paper id="25">
      <title><fixed-case>B</fixed-case>lend<fixed-case>SQL</fixed-case>: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra</title>
      <author><first>Parker</first><last>Glenn</last><affiliation>Fidelity Investments</affiliation></author>
      <author><first>Parag</first><last>Dakle</last></author>
      <author><first>Liang</first><last>Wang</last></author>
      <author><first>Preethi</first><last>Raghavan</last><affiliation>Fidelity</affiliation></author>
      <pages>453-466</pages>
      <abstract>Many existing end-to-end systems for hybrid question answering tasks can often be boiled down to a “prompt-and-pray” paradigm, where the user has limited control and insight into the intermediate reasoning steps used to achieve the final result. Additionally, due to the context size limitation of many transformer-based LLMs, it is often not reasonable to expect that the full structured and unstructured context will fit into a given prompt in a zero-shot setting, let alone a few-shot setting. We introduce BlendSQL, a superset of SQLite to act as a unified dialect for orchestrating reasoning across both unstructured and structured data. For hybrid question answering tasks involving multi-hop reasoning, we encode the full decomposed reasoning roadmap into a single interpretable BlendSQL query. Notably, we show that BlendSQL can scale to massive datasets and improve the performance of end-to-end systems while using 35% fewer tokens. Our code is available and installable as a package at https://github.com/parkervg/blendsql.</abstract>
      <url hash="95138996">2024.findings-acl.25</url>
      <bibkey>glenn-etal-2024-blendsql</bibkey>
      <doi>10.18653/v1/2024.findings-acl.25</doi>
    </paper>
    <paper id="26">
      <title><fixed-case>LLM</fixed-case>-<fixed-case>QAT</fixed-case>: Data-Free Quantization Aware Training for Large Language Models</title>
      <author><first>Zechun</first><last>Liu</last><affiliation>Meta Inc.</affiliation></author>
      <author><first>Barlas</first><last>Oguz</last><affiliation>Meta</affiliation></author>
      <author><first>Changsheng</first><last>Zhao</last><affiliation>Meta Inc.</affiliation></author>
      <author><first>Ernie</first><last>Chang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Pierre</first><last>Stock</last><affiliation>Facebook</affiliation></author>
      <author><first>Yashar</first><last>Mehdad</last><affiliation>Facebook</affiliation></author>
      <author><first>Yangyang</first><last>Shi</last><affiliation>Meta</affiliation></author>
      <author><first>Raghuraman</first><last>Krishnamoorthi</last><affiliation>Facebook</affiliation></author>
      <author><first>Vikas</first><last>Chandra</last><affiliation>Meta</affiliation></author>
      <pages>467-484</pages>
      <abstract>Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization-aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and supporting long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.</abstract>
      <url hash="fc96dcf9">2024.findings-acl.26</url>
      <bibkey>liu-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.26</doi>
    </paper>
    <paper id="27">
      <title><fixed-case>I</fixed-case>nfi<fixed-case>MM</fixed-case>: Advancing Multimodal Understanding with an Open-Sourced Visual Language Model</title>
      <author><first>Haogeng</first><last>Liu</last></author>
      <author><first>Quanzeng</first><last>You</last><affiliation>ByteDance</affiliation></author>
      <author><first>Yiqi</first><last>Wang</last></author>
      <author><first>Xiaotian</first><last>Han</last><affiliation>ByteDance</affiliation></author>
      <author><first>Bohan</first><last>Zhai</last><affiliation>Snowflake</affiliation></author>
      <author><first>Yongfei</first><last>Liu</last><affiliation>Bytedance</affiliation></author>
      <author><first>Wentao</first><last>Chen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yiren</first><last>Jian</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yunzhe</first><last>Tao</last><affiliation>ByteDance</affiliation></author>
      <author><first>Jianbo</first><last>Yuan</last><affiliation>Bytedance</affiliation></author>
      <author><first>Ran</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongxia</first><last>Yang</last></author>
      <pages>485-492</pages>
      <abstract>In this work, we present InfiMM, an advanced Multimodal Large Language Model that adapts to intricate vision-language tasks. InfiMM, inspired by the Flamingo architecture, distinguishes itself through the utilization of large-scale training data, comprehensive training strategies, and diverse large language models. This approach ensures the preservation of Flamingo’s foundational strengths while simultaneously introducing augmented capabilities. Empirical evaluations across a variety of benchmarks underscore InfiMM’s remarkable capability in multimodal understanding. The code can be found at: https://anonymous.4open.science/r/infimm-zephyr-F60C/.</abstract>
      <url hash="1f9fba68">2024.findings-acl.27</url>
      <bibkey>liu-etal-2024-infimm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.27</doi>
    </paper>
    <paper id="28">
      <title>Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution</title>
      <author><first>Xinze</first><last>Li</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Yubo</first><last>Ma</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Aixin</first><last>Sun</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>493-516</pages>
      <abstract>Although achieving great success, Large Language Models (LLMs) usually suffer from unreliable hallucinations. Although language attribution can be a potential solution, there are no suitable benchmarks and evaluation metrics to attribute LLMs to structured knowledge. In this paper, we define a new task of Knowledge-aware Language Model Attribution (KaLMA) that improves upon three core concerns with conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new “Conscious Incompetence” setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via evolutionary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs’ citation generation, emphasizing the importance of incorporating the “Conscious Incompetence” setting, and the critical role of retrieval accuracy.</abstract>
      <url hash="2696e462">2024.findings-acl.28</url>
      <bibkey>li-etal-2024-towards-verifiable</bibkey>
      <doi>10.18653/v1/2024.findings-acl.28</doi>
    </paper>
    <paper id="29">
      <title>Benchmarking Cognitive Biases in Large Language Models as Evaluators</title>
      <author><first>Ryan</first><last>Koo</last></author>
      <author><first>Minhwa</first><last>Lee</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Vipul</first><last>Raheja</last><affiliation>Columbia University, Grammarly and International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Jong Inn</first><last>Park</last></author>
      <author><first>Zae Myung</first><last>Kim</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <author><first>Dongyeop</first><last>Kang</last><affiliation>University of Minnesota</affiliation></author>
      <pages>517-545</pages>
      <abstract>Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 16 LLMs encompassing four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLer), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (40% of comparisons made by all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 44%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences.</abstract>
      <url hash="a8cb0ff1">2024.findings-acl.29</url>
      <bibkey>koo-etal-2024-benchmarking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.29</doi>
    </paper>
    <paper id="30">
      <title><fixed-case>X</fixed-case>-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions</title>
      <author><first>Chong</first><last>Li</last><affiliation>Institute of automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wen</first><last>Yang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jinliang</first><last>Lu</last><affiliation>Institute of automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Chengqing</first><last>Zong</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>546-566</pages>
      <abstract>Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples into these languages can be a solution but unreliable, leading to responses with translation errors and lacking language-specific or cultural knowledge. To address this issue, we propose a novel method to construct cross-lingual instruction following samples with instruction in English and response in low-resource languages. Specifically, the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses. The candidate cross-lingual instruction tuning samples are further refined and diversified. We have employed this method to build a large-scale cross-lingual instruction tuning dataset on 10 languages, namely X-Instruction. The instruction data built using our method incorporate more language-specific knowledge compared with the naive translation method. Experimental results have shown that the response quality of the model tuned on X-Instruction greatly exceeds the model distilled from a powerful teacher model, reaching or even surpassing the ones of ChatGPT. In addition, we find that models tuned on cross-lingual instruction following samples can follow the instruction in the output language without further tuning.</abstract>
      <url hash="b7435c57">2024.findings-acl.30</url>
      <bibkey>li-etal-2024-x</bibkey>
      <doi>10.18653/v1/2024.findings-acl.30</doi>
    </paper>
    <paper id="31">
      <title>Muffin: Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted <fixed-case>AI</fixed-case> Feedback</title>
      <author><first>Jiashuo</first><last>Wang</last></author>
      <author><first>Chunpu</first><last>Xu</last></author>
      <author><first>Chak Tou</first><last>Leong</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>567-585</pages>
      <abstract>Emotional support conversation systems are designed to alleviate users’ emotional distress and assist them in overcoming their challenges. While previous studies have made progress, their models occasionally generate unhelpful responses, which are intended to be supportive but instead have counterproductive effects. Since unhelpful responses can hinder the effectiveness of emotional support, it is crucial to mitigate them within conversations. Our solution is motivated by two principal considerations: (1) multiple facets of emotional support are expected to be considered when developing emotional support conversation models, and (2) directly reducing the probability of generating unhelpful responses can effectively mitigate their occurrence. Accordingly, we introduce a novel <tex-math>\textbf{model-agnostic}</tex-math> framework named <tex-math>\underline{M}</tex-math>itigating <tex-math>\underline{u}</tex-math>nhelpfulness with multifaceted AI <tex-math>\underline{f}</tex-math>eedback for emot<tex-math>\underline{i}</tex-math>o<tex-math>\underline{n}</tex-math>al support (<tex-math>\textit{Muffin}</tex-math>). It first employs a multifaceted AI feedback module designed to assess the helpfulness model responses across various facets of emotional support. Leveraging contrastive learning, Muffin then reduces the unhelpful responses’ likelihoods. To validate the effectiveness of our proposed framework, we apply Muffin to various previous emotional support generation models, including the state-of-the-art. Experimental results demonstrate that Muffin can significantly mitigate unhelpful response generation while enhancing response fluency and relevance.</abstract>
      <url hash="92cc28bb">2024.findings-acl.31</url>
      <bibkey>wang-etal-2024-muffin</bibkey>
      <doi>10.18653/v1/2024.findings-acl.31</doi>
    </paper>
    <paper id="32">
      <title>Resonance <fixed-case>R</fixed-case>o<fixed-case>PE</fixed-case>: Improving Context Length Generalization of Large Language Models</title>
      <author><first>Suyuchen</first><last>Wang</last></author>
      <author><first>Ivan</first><last>Kobyzev</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Peng</first><last>Lu</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Bang</first><last>Liu</last><affiliation>University of Montreal</affiliation></author>
      <pages>586-598</pages>
      <abstract>This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position better and more robustly. Our extensive LLM experiments also show superior performance after applying Resonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on both upstream language modeling tasks and a variety of downstream long-text applications.</abstract>
      <url hash="420f0f8e">2024.findings-acl.32</url>
      <bibkey>wang-etal-2024-resonance</bibkey>
      <doi>10.18653/v1/2024.findings-acl.32</doi>
    </paper>
    <paper id="33">
      <title><fixed-case>M</fixed-case>ed<fixed-case>A</fixed-case>gents: Large Language Models as Collaborators for Zero-shot Medical Reasoning</title>
      <author><first>Xiangru</first><last>Tang</last><affiliation>Yale University</affiliation></author>
      <author><first>Anni</first><last>Zou</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Ziming</first><last>Li</last></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Xingyao</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Mark</first><last>Gerstein</last><affiliation>Yale University</affiliation></author>
      <pages>599-621</pages>
      <abstract>Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents.</abstract>
      <url hash="df30c82f">2024.findings-acl.33</url>
      <bibkey>tang-etal-2024-medagents</bibkey>
      <doi>10.18653/v1/2024.findings-acl.33</doi>
    </paper>
    <paper id="34">
      <title>Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models</title>
      <author><first>Yiming</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Pei</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>622-643</pages>
      <abstract>Neural-symbolic methods have demonstrated efficiency in enhancing the reasoning abilities of large language models (LLMs). However, existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits. To broaden symbolic methods’ applicability and adaptability in the real world, we propose Meta-Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge. We conduct extensive experiments on more than ten datasets encompassing conventional reasoning tasks like arithmetic, symbolic, and logical reasoning, and the more complex interactive reasoning tasks like theory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning significantly enhances in-context reasoning accuracy, learning efficiency, out-of-domain generalization, and output stability compared to the Chain-of-Thought technique.</abstract>
      <url hash="c37412be">2024.findings-acl.34</url>
      <bibkey>wang-etal-2024-meta</bibkey>
      <doi>10.18653/v1/2024.findings-acl.34</doi>
    </paper>
    <paper id="35">
      <title><fixed-case>DPDLLM</fixed-case>: A Black-box Framework for Detecting Pre-training Data from Large Language Models</title>
      <author><first>Baohang</first><last>Zhou</last><affiliation>Nankai University</affiliation></author>
      <author><first>Zezhong</first><last>Wang</last></author>
      <author><first>Lingzhi</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Ying</first><last>Zhang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Kehui</first><last>Song</last></author>
      <author><first>Xuhui</first><last>Sui</last></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>644-653</pages>
      <abstract>The success of large language models (LLM) benefits from large-scale model parameters and large amounts of pre-training data. However, the textual data for training LLM can not be confirmed to be legal because they are crawled from different web sites. For example, there are copyrighted articles, personal reviews and information in the pre-training data for LLM which are illegal. To address the above issue and develop legal LLM, we propose to detect the pre-training data from LLM in a pure black-box way because the existing LLM services only return the generated text. The previous most related works are the membership inference attack (MIA) on machine learning models to detect the training data from them. But the existing methods are based on analyzing the output probabilities of models which are unrealistic to LLM services. To tackle the problem, we firstly construct the benchmark datasets by collecting textual data from different domains as the seen and unseen pre-training data for LLMs. Then, we investigate a black-box framework named DPDLLM, with the only access to the generated texts from LLM for detecting textual data whether was used to train it. In the proposed framework, we exploit GPT-2 as the reference model to fit the textual data and feed the generated text from LLM into it to acquire sequence probabilities as the significant feature for detection. The experimental results on the benchmark datasets demonstrate that DPDLLM is effective on different popular LLMs and outperforms the existing methods.</abstract>
      <url hash="73282b78">2024.findings-acl.35</url>
      <bibkey>zhou-etal-2024-dpdllm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.35</doi>
    </paper>
    <paper id="36">
      <title><fixed-case>PACIT</fixed-case>: Unlocking the Power of Examples for Better In-Context Instruction Tuning</title>
      <author><first>Tianci</first><last>Xue</last></author>
      <author><first>Ziqi</first><last>Wang</last></author>
      <author><first>Yixia</first><last>Li</last></author>
      <author><first>Yun</first><last>Chen</last><affiliation>Shanghai University of Finance and Economics</affiliation></author>
      <author><first>Guanhua</first><last>Chen</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>654-665</pages>
      <abstract>Instruction tuning enhances the instruction following ability of large language models by finetuning with supervised instruction data. Previous work proposes in-context instruction tuning (ICIT) where specific positive or negative examples are incorporated into the prompt for better performance. In this work, we propose PACIT, a simple and effective in-context instruction tuning method, inspired by the pedagogical concept of desirable difficulty. The PACIT method unlocks the power of examples by encouraging the model to actively learn to grasp the distinctions between the positive and negative examples instead of merely reading. The model is expected to first verify the correctness of the provided example according to the task description, which is then set as the condition for generating a better response to the task instance. Our extensive experiments prove the effectiveness of PACIT, outperforming ICIT baseline on both in-domain and out-domain tasks up to 9.16 and 3.14 average ROUGE-L scores, respectively. Moreover, PACIT can notably enhance the performance of instruction tuning even when all positive and negative examples are generated with a self-instruct method.</abstract>
      <url hash="da1d8388">2024.findings-acl.36</url>
      <bibkey>xue-etal-2024-pacit</bibkey>
      <doi>10.18653/v1/2024.findings-acl.36</doi>
    </paper>
    <paper id="37">
      <title>Listen Again and Choose the Right Answer: A New Paradigm for Automatic Speech Recognition with Large Language Models</title>
      <author><first>Yuchen</first><last>Hu</last></author>
      <author><first>Chen</first><last>Chen</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Chengwei</first><last>Qin</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Qiushi</first><last>Zhu</last></author>
      <author><first>EngSiong</first><last>Chng</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Ruizhe</first><last>Li</last><affiliation>University of Aberdeen</affiliation></author>
      <pages>666-679</pages>
      <abstract>Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which aims to predict the ground-truth transcription from the decoded N-best hypotheses. Thanks to the strong language generation ability of LLMs and rich information in the N-best list, GER shows great effectiveness in enhancing ASR results. However, it still suffers from two limitations: 1) LLMs are unaware of the source speech during GER, which may lead to results that are grammatically correct but violate the source speech content, 2) N-best hypotheses usually only vary in a few tokens, making it redundant to send all of them for GER, which could confuse LLM about which tokens to focus on and thus lead to increased miscorrection. In this paper, we propose ClozeGER, a new paradigm for ASR generative error correction. First, we introduce a multimodal LLM (i.e., SpeechGPT) to receive source speech as extra input to improve the fidelity of correction output. Then, we reformat GER as a cloze test with logits calibration to remove the input information redundancy and simplify GER with clear instructions. Experiments show that ClozeGER achieves a new breakthrough over vanilla GER on 9 popular ASR datasets.</abstract>
      <url hash="a30bc32a">2024.findings-acl.37</url>
      <bibkey>hu-etal-2024-listen</bibkey>
      <doi>10.18653/v1/2024.findings-acl.37</doi>
    </paper>
    <paper id="38">
      <title>Towards Better Graph-based Cross-document Relation Extraction via Non-bridge Entity Enhancement and Prediction Debiasing</title>
      <author><first>Hao</first><last>Yue</last></author>
      <author><first>Shaopeng</first><last>Lai</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Chengyi</first><last>Yang</last></author>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Junfeng</first><last>Yao</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>680-691</pages>
      <abstract>Cross-document Relation Extraction aims to predict the relation between target entities located in different documents. In this regard, the dominant models commonly retain useful information for relation prediction via bridge entities, which allows the model to elaborately capture the intrinsic interdependence between target entities. However, these studies ignore the non-bridge entities, each of which co-occurs with only one target entity and offers the semantic association between target entities for relation prediction. Besides, the commonly-used dataset–CodRED contains substantial NA instances, leading to the prediction bias during inference. To address these issues, in this paper, we propose a novel graph-based cross-document RE model with non-bridge entity enhancement and prediction debiasing. Specifically, we use a unified entity graph to integrate numerous non-bridge entities with target entities and bridge entities, modeling various associations between them, and then use a graph recurrent network to encode this graph. Finally, we introduce a novel debiasing strategy to calibrate the original prediction distribution. Experimental results on the closed and open settings show that our model significantly outperforms all baselines, including the GPT-3.5-turbo and InstructUIE, achieving state-of-the-art performance. Particularly, our model obtains 66.23% and 55.87% AUC points in the official leaderboard under the two settings, respectively,ranking the first place in all submissions since December 2023. Our code is available at https://github.com/DeepLearnXMU/CoRE-NEPD.</abstract>
      <url hash="68ea1dec">2024.findings-acl.38</url>
      <bibkey>yue-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-acl.38</doi>
    </paper>
    <paper id="39">
      <title>Large Language Models can Share Images, Too!</title>
      <author><first>Young-Jun</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Dokyong</first><last>Lee</last></author>
      <author><first>Joo Won</first><last>Sung</last></author>
      <author><first>Jonghwan</first><last>Hyeon</last><affiliation>KAIST</affiliation></author>
      <author><first>Ho-Jin</first><last>Choi</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <pages>692-713</pages>
      <abstract>This paper explores the image-sharing capability of Large Language Models (LLMs), such as GPT-4 and LLaMA 2, in a zero-shot setting. To facilitate a comprehensive evaluation of LLMs, we introduce the photochatplus dataset, which includes enriched annotations (ie intent, triggering sentence, image description, and salient information). Furthermore, we present the gradient-free and extensible Decide, Describe, and Retrieve () framework. With extensive experiments, we unlock the image-sharing capability of equipped with LLMs in zero-shot prompting, with ChatGPT achieving the best performance.Our findings also reveal the emergent image-sharing ability in LLMs under zero-shot conditions, validating the effectiveness of . We use this framework to demonstrate its practicality and effectiveness in two real-world scenarios: (1) human-bot interaction and (2) dataset augmentation. To the best of our knowledge, this is the first study to assess the image-sharing ability of various LLMs in a zero-shot setting. We make our source code and dataset publicly available at https://github.com/passing2961/DribeR.</abstract>
      <url hash="4f0fde32">2024.findings-acl.39</url>
      <bibkey>lee-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.39</doi>
    </paper>
    <paper id="40">
      <title><fixed-case>C</fixed-case>ode<fixed-case>M</fixed-case>: Less Data Yields More Versatility via Ability Matrix</title>
      <author><first>Daoguang</first><last>Zan</last></author>
      <author><first>Ailun</first><last>Yu</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <author><first>Bo</first><last>Shen</last></author>
      <author><first>Shaoxin</first><last>Lin</last></author>
      <author><first>Yongshun</first><last>Gong</last><affiliation>Shandong University</affiliation></author>
      <author><first>Yafen</first><last>Yao</last></author>
      <author><first>Yan</first><last>Liu</last></author>
      <author><first>Bei</first><last>Guan</last></author>
      <author><first>Weihua</first><last>Luo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongji</first><last>Wang</last></author>
      <author><first>Qianxiang</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Lizhen</first><last>Cui</last><affiliation>Shandong University</affiliation></author>
      <pages>714-729</pages>
      <abstract>In the era of code large language models (code LLMs), data engineering plays a pivotal role during the instruction fine-tuning phase. To train a versatile model, previous efforts devote tremendous efforts into crafting instruction data covering all the downstream scenarios. Nonetheless, this will incur significant expenses in constructing data and training model. Therefore, this paper introduces CodeM, a novel data construction strategy, which can efficiently train a versatile model using less data via our newly proposed ability matrix. CodeM uses ability matrix to decouple code LLMs’ abilities into two dimensions, constructing a lightweight training corpus that only covers a subset of target scenarios. Extensive experiments on HumanEvalPack and MultiPL-E imply that code LLMs can combine the single-dimensional abilities to master composed abilities, validating the effectiveness of CodeM.</abstract>
      <url hash="c874c12a">2024.findings-acl.40</url>
      <bibkey>zan-etal-2024-codem</bibkey>
      <doi>10.18653/v1/2024.findings-acl.40</doi>
    </paper>
    <paper id="41">
      <title>Do <fixed-case>LVLM</fixed-case>s Understand Charts? Analyzing and Correcting Factual Errors in Chart Captioning</title>
      <author><first>Kung-Hsiang</first><last>Huang</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Mingyang</first><last>Zhou</last></author>
      <author><first>Hou Pong</first><last>Chan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yi</first><last>Fung</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Lingyu</first><last>Zhang</last><affiliation>Duke University</affiliation></author>
      <author><first>Shih-Fu</first><last>Chang</last><affiliation>Columbia, Columbia University, Columbia University, Columbia University, Columbia University, Columbia University and Columbia University</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>730-749</pages>
      <abstract>Advances in large vision-language models (LVLMs) have led to significant progress in generating natural language descriptions for visual contents. These powerful models are known for producing texts that are factually inconsistent with the visual input. While some efforts mitigate such inconsistencies in natural image captioning, the factuality of generated captions for structured visuals, such as charts, has not received as much scrutiny. This work introduces a comprehensive typology of factual errors in generated chart captions. A large-scale human annotation effort provides insight into the error patterns in captions generated by various models, ultimately forming the foundation of a dataset, CHOCOLATE. Our analysis reveals that even advanced models like GPT-4V frequently produce captions laced with factual inaccuracies. To combat this, we establish the task of Chart Caption Factual Error Correction and introduce CHARTVE, a visual entailment model that outperforms current LVLMs in evaluating caption factuality. Furthermore, we propose C2TFEC, an interpretable two-stage framework that excels at correcting factual errors. This work inaugurates a new domain in factual error correction for chart captions, presenting a novel evaluation metric, and demonstrating an effective approach to ensuring the factuality of generated chart captions. The code and data as well as the continuously updated benchmark can be found at: https://khuangaf.github.io/CHOCOLATE/.</abstract>
      <url hash="3dba158e">2024.findings-acl.41</url>
      <bibkey>huang-etal-2024-lvlms</bibkey>
      <doi>10.18653/v1/2024.findings-acl.41</doi>
    </paper>
    <paper id="42">
      <title><fixed-case>BIDER</fixed-case>: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented <fixed-case>LLM</fixed-case>s via Key Supporting Evidence</title>
      <author><first>Jiajie</first><last>Jin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yutao</first><last>Zhu</last></author>
      <author><first>Yujia</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <pages>750-761</pages>
      <abstract>Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy.However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM’s answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM’s information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs’ answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential information for accurate question answering.</abstract>
      <url hash="f8eee1c5">2024.findings-acl.42</url>
      <bibkey>jin-etal-2024-bider</bibkey>
      <doi>10.18653/v1/2024.findings-acl.42</doi>
    </paper>
    <paper id="43">
      <title>Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions</title>
      <author><first>Wenxuan</first><last>Wang</last><affiliation>National Lab of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences and Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Yisi</first><last>Zhang</last><affiliation>University of Science and Technology Beijing</affiliation></author>
      <author><first>Xingjian</first><last>He</last><affiliation>, Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Yichen</first><last>Yan</last></author>
      <author><first>Zijia</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xinlong</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Jing</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>762-776</pages>
      <abstract>Visual grounding (VG) aims at locating the foreground entities that match the given natural language expression. Previous datasets and methods for classic VG task mainly rely on the prior assumption that the given expression must literally refer to the target object, which greatly impedes the practical deployment of agents in real-world scenarios. Since users usually prefer to provide the intention-based expressions for the desired object instead of covering all the details, it is necessary for the agents to interpret the intention-driven instructions. Thus, in this work, we take a step further to the intention-driven visual-language (V-L) understanding. To promote classic VG towards human intention interpretation, we propose a new intention-driven visual grounding (IVG) task and build a largest-scale IVG dataset named IntentionVG with free-form intention expressions. Considering that practical agents need to move and find specific targets among various scenarios to realize the grounding task, our IVG task and IntentionVG dataset have taken the crucial properties of both multi-scenario perception and egocentric view into consideration. Besides, various types of models are set up as the baselines to realize our IVG task. Extensive experiments on our IntentionVG dataset and baselines demonstrate the necessity and efficacy of our method for the V-L field. To foster future research in this direction, our newly built dataset and baselines will be publicly available at https://github.com/Rubics-Xuan/IVG.</abstract>
      <url hash="1420b71f">2024.findings-acl.43</url>
      <bibkey>wang-etal-2024-beyond-literal</bibkey>
      <doi>10.18653/v1/2024.findings-acl.43</doi>
    </paper>
    <paper id="44">
      <title>Incremental Sequence Labeling: A Tale of Two Shifts</title>
      <author><first>Shengjie</first><last>Qiu</last></author>
      <author><first>Junhao</first><last>Zheng</last></author>
      <author><first>Zhen</first><last>Liu</last></author>
      <author><first>Yicheng</first><last>Luo</last></author>
      <author><first>Qianli</first><last>Ma</last><affiliation>South China University of Technology</affiliation></author>
      <pages>777-791</pages>
      <abstract>The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model’s discriminative ability for old entities. Simultaneously, to tackle the O2E problem, we alleviate the model’s bias towards new entities through debiased loss and optimization levels.Our experimental evaluation, conducted on three datasets with various incremental settings, demonstrates the superior performance of IS3 compared to the previous state-of-the-art method by a significant margin.</abstract>
      <url hash="278815de">2024.findings-acl.44</url>
      <bibkey>qiu-etal-2024-incremental</bibkey>
      <doi>10.18653/v1/2024.findings-acl.44</doi>
    </paper>
    <paper id="45">
      <title>How Proficient Are Large Language Models in Formal Languages? An In-Depth Insight for Knowledge Base Question Answering</title>
      <author><first>Jinxin</first><last>Liu</last></author>
      <author><first>Shulin</first><last>Cao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Jiaxin</first><last>Shi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Tingjian</first><last>Zhang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Lunyiu</first><last>Nie</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Linmei</first><last>Hu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>792-815</pages>
      <abstract>Knowledge Base Question Answering (KBQA) aims to answer natural language questions based on facts in knowledge bases. A typical approach to KBQA is semantic parsing, which translates a question into an executable logical form in a formal language. Recent works leverage the capabilities of large language models (LLMs) for logical form generation to improve performance. However, although it is validated that LLMs are capable of solving some KBQA problems, there has been little discussion on the differences in LLMs’ proficiency in formal languages used in semantic parsing. In this work, we propose to evaluate the understanding and generation ability of LLMs to deal with differently structured logical forms by examining the inter-conversion of natural and formal language through in-context learning of LLMs. Extensive experiments with models of different sizes show that state-of-the-art LLMs can understand formal languages as well as humans, but generating correct logical forms given a few examples remains a challenge. Most importantly, our results also indicate that LLMs exhibit considerable sensitivity. In general, the formal language with a lower formalization level, i.e., the more similar it is to natural language, is more friendly to LLMs. Code and data can be found at https://github.com/Matthewlliu/structure_probe.</abstract>
      <url hash="ee184c70">2024.findings-acl.45</url>
      <bibkey>liu-etal-2024-proficient</bibkey>
      <doi>10.18653/v1/2024.findings-acl.45</doi>
    </paper>
    <paper id="46">
      <title><fixed-case>MELOV</fixed-case>: Multimodal Entity Linking with Optimized Visual Features in Latent Space</title>
      <author><first>Xuhui</first><last>Sui</last></author>
      <author><first>Ying</first><last>Zhang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Yu</first><last>Zhao</last><affiliation>Nankai University</affiliation></author>
      <author><first>Kehui</first><last>Song</last></author>
      <author><first>Baohang</first><last>Zhou</last><affiliation>Nankai University</affiliation></author>
      <author><first>Xiaojie</first><last>Yuan</last><affiliation>Nankai University</affiliation></author>
      <pages>816-826</pages>
      <abstract>Multimodal entity linking (MEL), which aligns ambiguous mentions within multimodal contexts to referent entities from multimodal knowledge bases, is essential for many natural language processing applications. Previous MEL methods mainly focus on exploring complex multimodal interaction mechanisms to better capture coherence evidence between mentions and entities by mining complementary information. However, in real-world social media scenarios, vision modality often exhibits low quality, low value, or low relevance to the mention. Integrating such information directly will backfire, leading to a weakened consistency between mentions and their corresponding entities. In this paper, we propose a novel latent space vision feature optimization framework MELOV, which combines inter-modality and intra-modality optimizations to address these challenges. For the inter-modality optimization, we exploit the variational autoencoder to mine shared information and generate text-based visual features. For the intra-modality optimization, we consider the relationships between mentions and build graph convolutional network to aggregate the visual features of semantic similar neighbors. Extensive experiments on three benchmark datasets demonstrate the superiority of our proposed framework.</abstract>
      <url hash="18de6be3">2024.findings-acl.46</url>
      <bibkey>sui-etal-2024-melov</bibkey>
      <doi>10.18653/v1/2024.findings-acl.46</doi>
    </paper>
    <paper id="47">
      <title>Unsupervised Distractor Generation via Large Language Model Distilling and Counterfactual Contrastive Decoding</title>
      <author><first>Fanyi</first><last>Qu</last></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>827-838</pages>
      <abstract>Within the context of reading comprehension, the task of Distractor Generation (DG) aims to generate several incorrect options to confuse readers. In recent years, the emergence of Large Language Models (LLMs) provides a potential for unsupervised DG without expensive human-annotated distractor labels. In this paper, we leverage LLMs as a cost-effective annotator to enhance the DG capability of smaller student models. To perform knowledge distilling, we propose a dual task training framework that integrates pseudo distractors from LLMs and answer information as the objective target with a two-stage training process. Moreover, we devise a counterfactual contrastive decoding mechanism for increasing the distracting capability of the DG model. Experiments show that our unsupervised generation method with Bart-base greatly surpasses GPT-3.5-turbo zero-shot performance with only 200<tex-math>\times</tex-math> fewer model parameters. Our proposed unsupervised DG method offers a cost-effective framework for practical reading comprehension applications, without the need of laborious distractor annotation and costly large-size models.</abstract>
      <url hash="8fca2d34">2024.findings-acl.47</url>
      <bibkey>qu-etal-2024-unsupervised</bibkey>
      <doi>10.18653/v1/2024.findings-acl.47</doi>
    </paper>
    <paper id="48">
      <title>Conversational Question Answering with Language Models Generated Reformulations over Knowledge Graph</title>
      <author><first>Lihui</first><last>Liu</last></author>
      <author><first>Blaine</first><last>Hill</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Boxin</first><last>Du</last><affiliation>Amazon</affiliation></author>
      <author><first>Fei</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Hanghang</first><last>Tong</last></author>
      <pages>839-850</pages>
      <abstract>Conversational question answering (ConvQA) over knowledge graphs (KGs) involves answering multi-turn natural language questions about information contained in a KG. State-of-the-art methods of ConvQA often struggle with inexplicit question-answer pairs. These inputs are easy for human beings to understand given a conversation history, but hard for a machine to interpret, which can degrade ConvQA performance. To address this problem, we propose a reinforcement learning (RL) based model, CoRnNet, which utilizes question reformulations generated by large language models (LLMs) to improve ConvQA performance. CoRnNet adopts a teacher-student architecture where a teacher model learns question representations using human writing reformulations, and a student model to mimic the teacher model’s output via reformulations generated by LLMs. The learned question representation is then used by a RL model to locate the correct answer in a KG. Extensive experimental results show that CoRnNet outperforms state-of-the-art ConvQA models.</abstract>
      <url hash="660600ca">2024.findings-acl.48</url>
      <bibkey>liu-etal-2024-conversational</bibkey>
      <doi>10.18653/v1/2024.findings-acl.48</doi>
    </paper>
    <paper id="49">
      <title>Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step by Step</title>
      <author><first>Li</first><last>Zhong</last></author>
      <author><first>Zilong</first><last>Wang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>851-870</pages>
      <abstract>Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections.</abstract>
      <url hash="e5fb9437">2024.findings-acl.49</url>
      <bibkey>zhong-etal-2024-debug</bibkey>
      <doi>10.18653/v1/2024.findings-acl.49</doi>
    </paper>
    <paper id="50">
      <title>Effective In-Context Example Selection through Data Compression</title>
      <author><first>ZhongXiang</first><last>Sun</last></author>
      <author><first>Kepu</first><last>Zhang</last></author>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Xiao</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Xu</last><affiliation>Renmin University of China</affiliation></author>
      <pages>871-877</pages>
      <abstract>In-context learning has been extensively validated in large language models. However, the mechanism and selection strategy for in-context example selection, which is a crucial ingredient in this approach, lacks systematic and in-depth research. In this paper, we propose a data compression approach to the selection of in-context examples. We introduce a two-stage method that can effectively choose relevant examples and retain sufficient information about the training dataset within the in-context examples. Our method shows a significant improvement of an average of 5.90% across five different real-world datasets using four language models.</abstract>
      <url hash="1cb2255b">2024.findings-acl.50</url>
      <bibkey>sun-etal-2024-effective</bibkey>
      <doi>10.18653/v1/2024.findings-acl.50</doi>
    </paper>
    <paper id="51">
      <title>Are <fixed-case>U</fixed-case> a Joke Master? Pun Generation via Multi-Stage Curriculum Learning towards a Humor <fixed-case>LLM</fixed-case></title>
      <author><first>Yang</first><last>Chen</last></author>
      <author><first>Chong</first><last>Yang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Tu</first><last>Hu</last></author>
      <author><first>Xinhao</first><last>Chen</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <author><first>Li</first><last>Cai</last><affiliation>Guizhou University</affiliation></author>
      <author><first>Xinlin</first><last>Zhuang</last></author>
      <author><first>Xuan</first><last>Lin</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xin</first><last>Lu</last></author>
      <author><first>Aimin</first><last>Zhou</last><affiliation>East China Normal University</affiliation></author>
      <pages>878-890</pages>
      <abstract>Although large language models (LLMs) acquire extensive world knowledge and some reasoning abilities, their proficiency in generating humorous sentences remains a challenge. Previous research has demonstrated that the humor generation capabilities of ChatGPT are confined to producing merely 25 unique jokes. In this work, we concentrate on endowing LLMs with the ability of generating puns, a particular category of humor by preference learning method. We propose a multi-stage curriculum preference learning framework to optimize both pun structure preferences and humor preferences. Specifically, we improve the Direct Preference Optimization (DPO) algorithm to address the challenge of multi-objective alignment problem. Besides, to facilitate further advancement in this field, we collect a Chinese Pun (ChinesePun) dataset, containing 2.1k puns and corresponding annotations. Experimental results on both Chinese and English benchmark datasets demonstrate that our method significantly outperforms all the baseline models.</abstract>
      <url hash="59f94ae5">2024.findings-acl.51</url>
      <bibkey>chen-etal-2024-u</bibkey>
      <doi>10.18653/v1/2024.findings-acl.51</doi>
    </paper>
    <paper id="52">
      <title>Knowledgeable Preference Alignment for <fixed-case>LLM</fixed-case>s in Domain-specific Question Answering</title>
      <author><first>Yichi</first><last>Zhang</last></author>
      <author><first>Zhuo</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yin</first><last>Fang</last></author>
      <author><first>Yanxi</first><last>Lu</last></author>
      <author><first>Li</first><last>Fangming</last></author>
      <author><first>Wen</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>891-904</pages>
      <abstract>Deploying large language models (LLMs) to real scenarios for domain-specific question answering (QA) is a key thrust for LLM applications, which poses numerous challenges, especially in ensuring that responses are both accommodating to user requirements and appropriately leveraging domain-specific knowledge bases. They are the two major difficulties for LLM application as vanilla fine-tuning falls short of addressing. Combining these requirements, we conceive of them as the requirement for the model’s preference to be harmoniously aligned with humans’. Thus, we introduce Knowledgeable Preference AlignmenT (KnowPAT), which constructs two kinds of preference sets to tackle the two issues. Besides, we design a new alignment objective to align the LLM preference with different human preferences uniformly, aiming to optimize LLM performance in real-world, domain-specific QA settings. Adequate experiments and comprehensive comparisons with 15 baseline methods illustrate that our KnowPAT is a superior pipeline for real-scenario domain-specific QA with LLMs.</abstract>
      <url hash="ef32855c">2024.findings-acl.52</url>
      <bibkey>zhang-etal-2024-knowledgeable</bibkey>
      <doi>10.18653/v1/2024.findings-acl.52</doi>
    </paper>
    <paper id="53">
      <title><fixed-case>MARIO</fixed-case>: <fixed-case>MA</fixed-case>th Reasoning with code Interpreter Output - A Reproducible Pipeline</title>
      <author><first>Minpeng</first><last>Liao</last></author>
      <author><first>Chengxi</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wei</first><last>Luo</last></author>
      <author><first>Wu</first><last>Jing</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Kai</first><last>Fan</last><affiliation>Alibaba Group</affiliation></author>
      <pages>905-924</pages>
      <abstract>Large language models (LLMs) have significantly improved in understanding natural language but still lack in mathematical reasoning, a hurdle on the path to true artificial general intelligence. The training of large language models, based on next-token prediction, struggles to capture the precise nature of mathematical reasoning, presenting both practical and theoretical challenges. In this paper, we address this challenge by enriching the data landscape and introducing a reasonable data format, enhanced the text analysis of the LLM with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT annotations, human review, and self-training processes. Additionally, we propose a tentative, easily replicable protocol for the fine-tuning of math-specific LLMs, which has led to a significant improvement in the performance of a 7B-parameter LLM on the GSM8K and MATH datasets. A solution generator and a value estimator are fine-tuned simultaneously in a multi-task fashion, while an outlier-free value model-based inference method is proposed to further boost the performance. We are committed to advancing the field of mathematical reasoning in LLMs and, to that end, we will make the source code and checkpoints publicly available.</abstract>
      <url hash="9f6b05f8">2024.findings-acl.53</url>
      <bibkey>liao-etal-2024-mario</bibkey>
      <doi>10.18653/v1/2024.findings-acl.53</doi>
    </paper>
    <paper id="54">
      <title><fixed-case>D</fixed-case>iffus<fixed-case>P</fixed-case>oll: Conditional Text Diffusion Model for Poll Generation</title>
      <author><first>Le</first><last>Cheng</last></author>
      <author><first>Shuangyin</first><last>Li</last></author>
      <pages>925-935</pages>
      <abstract>Online social media platforms often gather user feedback through polls to enhance user engagement. Automatically generating polls from social media and its context can decrease the labor expenses of media workers and enhance workplace productivity. However, on social media platforms, there are internet water armies that manipulate public opinion through sheer numbers and causing the comments to be biased, drowning out minority views. In such circumstances, polls created based on biased comments often have limited types of options and poor coverage. Therefore, it is crucial to diversify the poll options and try to listen to the voices of the minority. To achieve this, we introduce DiffusPoll, a novel paradigm for poll generation based on a non-autoregressive diffusion model that can generate diversified and high-quality samples. Under the new paradigm, we design a task-specific mask strategy tailored to the inherent logic of polls to optimize controlled generation. Furthermore, we also leverage additional attribute tags from comments to enhance the generation quality. Experimental results indicate that DiffusPoll has achieved state-of-the-art performance in both the quality and diversity of poll generation tasks, and is more likely to hit the voices of minority.</abstract>
      <url hash="8920310f">2024.findings-acl.54</url>
      <bibkey>cheng-li-2024-diffuspoll</bibkey>
      <doi>10.18653/v1/2024.findings-acl.54</doi>
    </paper>
    <paper id="55">
      <title>Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data</title>
      <author><first>Haolong</first><last>Li</last></author>
      <author><first>Yu</first><last>Ma</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Yinqi</first><last>Zhang</last><affiliation>ByteDance Inc. and East China Normal University</affiliation></author>
      <author><first>Chen</first><last>Ye</last><affiliation>Tongji University</affiliation></author>
      <author><first>Jie</first><last>Chen</last></author>
      <pages>936-946</pages>
      <abstract>While large language models (LLMs) have shown excellent capabilities in language understanding, text generation and many other tasks, they still struggle in complex multi-step reasoning problems such as mathematical reasoning. In this paper, through a newly proposed arithmetical puzzle problem, we show that the model can perform well on multi-step reasoning tasks via fine tuning on high-quality synthetic data. Experiments with the open-llama-3B model on three different test datasets show that not only the model can reach a zero-shot pass@1 at 0.44 on the in-domain dataset, it also demonstrates certain generalization capabilities on the out-of-domain datasets. Specifically, this paper has designed two out-of-domain datasets in the form of extending the numerical range and the composing components of the arithmetical puzzle problem separately. The fine-tuned model have shown encouraging performance on these two far more difficult tasks with the zero-shot pass@1 at 0.33 and 0.35 correspondingly.</abstract>
      <url hash="127a6f5b">2024.findings-acl.55</url>
      <bibkey>li-etal-2024-exploring-mathematical</bibkey>
      <doi>10.18653/v1/2024.findings-acl.55</doi>
    </paper>
    <paper id="56">
      <title>Implanting <fixed-case>LLM</fixed-case>’s Knowledge via Reading Comprehension Tree for Toxicity Detection</title>
      <author><first>Hankun</first><last>Kang</last></author>
      <author><first>Tieyun</first><last>Qian</last><affiliation>Wuhan University</affiliation></author>
      <pages>947-962</pages>
      <abstract>Toxicity detection plays a crucial role in maintaining the peace of the society. Existing methods can be roughly categorized as small language model (SLM) based and large language model (LLM) based. However, due to the limitation of SLMs on general knowledge and the potential embedded bias in LLMs despite their large amount of knowledge, it is not a good idea to detect toxicity only with either SLM or LLM based method.In this work, we propose to implant LLM’s knowledge into SLM based methods such that we can stick to both types of models’ strengths. To this end, we develop a reading comprehension (RC) tree to transfer knowledge between two models. Specifically, we first construct the RC tree, from an extensive to intensive reading perspective, to capture the local and global information in the text. We then model samples encoded by SLM and knowledge extracted from LLM as two distributions using the constructed RT tree. We finally transfer knowledge via optimal transportation between two distributions. Extensive experiments prove the effectiveness of our method on real-world and machine-generated datasets.</abstract>
      <url hash="3cd01d84">2024.findings-acl.56</url>
      <bibkey>kang-qian-2024-implanting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.56</doi>
    </paper>
    <paper id="57">
      <title><fixed-case>LLML</fixed-case>ingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</title>
      <author><first>Zhuoshi</first><last>Pan</last></author>
      <author><first>Qianhui</first><last>Wu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Huiqiang</first><last>Jiang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Menglin</first><last>Xia</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xufang</first><last>Luo</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Jue</first><last>Zhang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qingwei</first><last>Lin</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Victor</first><last>Rühle</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yuqing</first><last>Yang</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Chin-Yew</first><last>Lin</last><affiliation>Microsoft</affiliation></author>
      <author><first>H. Vicky</first><last>Zhao</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Lili</first><last>Qiu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft and Microsoft</affiliation></author>
      <pages>963-981</pages>
      <abstract>This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x.</abstract>
      <url hash="29761e8c">2024.findings-acl.57</url>
      <bibkey>pan-etal-2024-llmlingua</bibkey>
      <doi>10.18653/v1/2024.findings-acl.57</doi>
    </paper>
    <paper id="58">
      <title><fixed-case>E</fixed-case>con<fixed-case>NLI</fixed-case>: Evaluating Large Language Models on Economics Reasoning</title>
      <author><first>Yue</first><last>Guo</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>982-994</pages>
      <abstract>Large Language Models (LLMs) are widely used for writing economic analysis reports or providing financial advice, but their ability to understand economic knowledge and reason about potential results of specific economic events lacks systematic evaluation. To address this gap, we propose a new dataset, natural language inference on economic events (EconNLI), to evaluate LLMs’ knowledge and reasoning abilities in the economic domain. We evaluate LLMs on (1) their ability to correctly classify whether a premise event will cause a hypothesis event and (2) their ability to generate reasonable events resulting from a given premise. Our experiments reveal that LLMs are not sophisticated in economic reasoning and may generate wrong or hallucinated answers. Our study raises awareness of the limitations of using LLMs for critical decision-making involving economic reasoning and analysis. The dataset and codes are available at <url>https://github.com/Irenehere/EconNLI</url>.</abstract>
      <url hash="318807ba">2024.findings-acl.58</url>
      <bibkey>guo-yang-2024-econnli</bibkey>
      <doi>10.18653/v1/2024.findings-acl.58</doi>
    </paper>
    <paper id="59">
      <title>Better Late Than Never: Model-Agnostic Hallucination Post-Processing Framework Towards Clinical Text Summarization</title>
      <author><first>Songda</first><last>Li</last></author>
      <author><first>Yunqi</first><last>Zhang</last></author>
      <author><first>Chunyuan</first><last>Deng</last><affiliation>Rice University</affiliation></author>
      <author><first>Yake</first><last>Niu</last></author>
      <author><first>Hui</first><last>Zhao</last><affiliation>East China Normal University</affiliation></author>
      <pages>995-1011</pages>
      <abstract>Clinical text summarization has proven successful in generating concise and coherent summaries. However, these summaries may include unintended text with hallucinations, which can mislead clinicians and patients. Existing methods for mitigating hallucinations can be categorized into task-specific and task-agnostic approaches. Task-specific methods lack versatility for real-world applicability. Meanwhile, task-agnostic methods are not model-agnostic, so they require retraining for different models, resulting in considerable computational costs. To address these challenges, we propose MEDAL, a model-agnostic framework designed to post-process medical hallucinations. MEDAL can seamlessly integrate with any medical summarization model, requiring no additional computational overhead. MEDAL comprises a medical infilling model and a hallucination correction model. The infilling model generates non-factual summaries with common errors to train the correction model. The correction model is incorporated with a self-examination mechanism to activate its cognitive capability. We conduct comprehensive experiments using 11 widely accepted metrics on 7 baseline models across 3 medical text summarization tasks. MEDAL demonstrates superior performance in correcting hallucinations when applied to summaries generated by pre-trained language models and large language models.</abstract>
      <url hash="8cc4769a">2024.findings-acl.59</url>
      <bibkey>li-etal-2024-better</bibkey>
      <doi>10.18653/v1/2024.findings-acl.59</doi>
    </paper>
    <paper id="60">
      <title>Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers</title>
      <author><first>Haowen</first><last>Pan</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiaozhi</first><last>Wang</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Xun</first><last>Yang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Meng</first><last>Wang</last><affiliation>Hefei University of Technology</affiliation></author>
      <pages>1012-1037</pages>
      <abstract>Understanding the internal mechanisms by which multi-modal large language models (LLMs) interpret different modalities and integrate cross-modal representations is becoming increasingly critical for continuous improvements in both academia and industry. In this paper, we propose a novel method to identify key neurons for interpretability — how multi-modal LLMs bridge visual and textual concepts for captioning. Our method improves conventional works upon efficiency and applied range by removing needs of costly gradient computation. Based on those identified neurons, we further design a multi-modal knowledge editing method, beneficial to mitigate sensitive words or hallucination. For rationale of our design, we provide theoretical assumption. For empirical evaluation, we have conducted extensive quantitative and qualitative experiments. The results not only validate the effectiveness of our methods, but also offer insightful findings that highlight three key properties of multi-modal neurons: sensitivity, specificity and causal-effect, to shed light for future research.</abstract>
      <url hash="5b12e690">2024.findings-acl.60</url>
      <bibkey>pan-etal-2024-finding</bibkey>
      <doi>10.18653/v1/2024.findings-acl.60</doi>
    </paper>
    <paper id="61">
      <title>Realistic Evaluation of Toxicity in Large Language Models</title>
      <author><first>Tinh</first><last>Luong</last></author>
      <author><first>Thanh-Thien</first><last>Le</last></author>
      <author><first>Linh</first><last>Ngo</last><affiliation>Hanoi University of Science and Technology</affiliation></author>
      <author><first>Thien</first><last>Nguyen</last><affiliation>, University of Oregon</affiliation></author>
      <pages>1038-1047</pages>
      <abstract>Large language models (LLMs) have become integral to our professional workflows and daily lives. Nevertheless, these machine companions of ours have a critical flaw: the huge amount of data which endows them with vast and diverse knowledge, also exposes them to the inevitable toxicity and bias. While most LLMs incorporate defense mechanisms to prevent the generation of harmful content, these safeguards can be easily bypassed with minimal prompt engineering. In this paper, we introduce the new Thoroughly Engineered Toxicity (TET) dataset, comprising manually crafted prompts designed to nullify the protective layers of such models. Through extensive evaluations, we demonstrate the pivotal role of TET in providing a rigorous benchmark for evaluation of toxicity awareness in several popular LLMs: it highlights the toxicity in the LLMs that might remain hidden when using normal prompts, thus revealing subtler issues in their behavior.</abstract>
      <url hash="b02e433f">2024.findings-acl.61</url>
      <bibkey>luong-etal-2024-realistic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.61</doi>
    </paper>
    <paper id="62">
      <title>Controllable Text Generation with Residual Memory Transformer</title>
      <author><first>Hanqing</first><last>Zhang</last></author>
      <author><first>Si</first><last>Sun</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Haiming</first><last>Wu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Dawei</first><last>Song</last><affiliation>Beijing Institute of Technology and Open University</affiliation></author>
      <pages>1048-1066</pages>
      <abstract>Large-scale Causal Language Models (CLMs), e.g., GPT3 and ChatGPT, have brought great success in text generation. However, it is still an open challenge to effectively control the generation process of a CLM while balancing the flexibility, control granularity, and generation efficiency. In this paper, we provide a new alternative for controllable text generation (CTG), by designing a non-intrusive, lightweight control plugin, namely Residual Memory Transformer (RMT), to accompany the generation of CLM at arbitrary time steps. With an encoder-decoder setup, RMT can accept any types of control conditions and cooperate with the base CLM through a residual learning paradigm, to achieve a more flexible, general, and efficient CTG. Extensive experiments are carried out on various control tasks, in the form of both automatic and human evaluations. The results demonstrate the superiority of RMT over a wide range of state-of-the-art CTG approaches. The code implementation of our work is available at: https://github.com/Residual_Memory_Transformer.</abstract>
      <url hash="b5b5a015">2024.findings-acl.62</url>
      <bibkey>zhang-etal-2024-controllable</bibkey>
      <doi>10.18653/v1/2024.findings-acl.62</doi>
    </paper>
    <paper id="63">
      <title>Prompt-Based Length Controlled Generation with Multiple Control Types</title>
      <author><first>Renlong</first><last>Jie</last><affiliation>Northwest Polytechnical University Xi’an</affiliation></author>
      <author><first>Xiaojun</first><last>Meng</last><affiliation>Noah’s Ark Lab, Huawei Technologies Ltd.</affiliation></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <pages>1067-1085</pages>
      <abstract>Large language models (LLMs) have attracted great attention given their strong performance on a wide range of NLP tasks. In practice, users often expect generated texts to fall within a specific length range, making length controlled generation an important topic, especially for GPT-style models. Existing length control methods mostly focus on a simple control type of “equal to” a target length. Different from them, we propose a prompt-based method to achieve length controlled generation under different control types with high accuracy. In particular, we adopt reinforcement learning (RL) and sample filtering with the reward signal given by rule-based reward models, which enhances the length control ability of models by rewarding outputs that follow certain control instructions. In addition, we introduce a standard prompt extractor to parse arbitrary users’ input into standard control instructions. Experiments show that our method significantly improves the accuracy of prompt-based length control on popular summarization datasets like CNNDM and NYT under multiple control types. Moreover, both the standard prompt extractor and RL-tuned model show strong generalization to unseen control prompt templates.</abstract>
      <url hash="f90cb5aa">2024.findings-acl.63</url>
      <bibkey>jie-etal-2024-prompt</bibkey>
      <doi>10.18653/v1/2024.findings-acl.63</doi>
    </paper>
    <paper id="64">
      <title><fixed-case>PCA</fixed-case>-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</title>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Yichi</first><last>Zhang</last></author>
      <author><first>Shuhuai</first><last>Ren</last></author>
      <author><first>Haozhe</first><last>Zhao</last></author>
      <author><first>Zefan</first><last>Cai</last></author>
      <author><first>Yuchi</first><last>Wang</last></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Xiangdi</first><last>Meng</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <pages>1086-1104</pages>
      <abstract>We present PCA-Bench, a multimodal decision-making benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous benchmarks focusing on simplistic tasks and individual model capability, PCA-Bench introduces three complex scenarios: autonomous driving, domestic robotics, and open-world games. Given task instructions and diverse contexts, the model is required to seamlessly integrate multiple capabilities of Perception, Cognition, and Action in a reasoning chain to make accurate decisions. Moreover, PCA-Bench features error localization capabilities, scrutinizing model inaccuracies in areas such as perception, knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To balance accuracy and efficiency in evaluation, we propose PCA-Eval, an automatic evaluation protocol, and assess 10 prevalent MLLMs. The results reveal significant performance disparities between open-source models and powerful proprietary models like GPT-4 Vision. To address this, we introduce Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing instruction tuning examples in multimodal embodied environments. EIE generates 7,510 training examples in PCA-Bench and enhances the performance of open-source MLLMs, occasionally surpassing GPT-4 Vision (+3% in decision accuracy), thereby validating the effectiveness of EIE. Our findings suggest that robust MLLMs like GPT4-Vision show promise for decision-making in embodied agents, opening new avenues for MLLM research. All benchmark data and evaluation code are made public.</abstract>
      <url hash="f82b30f0">2024.findings-acl.64</url>
      <bibkey>chen-etal-2024-pca</bibkey>
      <doi>10.18653/v1/2024.findings-acl.64</doi>
    </paper>
    <paper id="65">
      <title>Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset</title>
      <author><first>Minjin</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Minju</first><last>Kim</last></author>
      <author><first>Hana</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Beong-woo</first><last>Kwak</last><affiliation>Yonsei University</affiliation></author>
      <author><first>SeongKu</first><last>Kang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Youngjae</first><last>Yu</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <pages>1105-1120</pages>
      <abstract>Conversational recommender systems are an emerging area that has garnered increasing interest in the community, especially with the advancements in large language models (LLMs) that enable sophisticated handling of conversational input. Despite the progress, the field still has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that PEARL contains more specific user preferences, show expertise in the target domain, and provides recommendations more relevant to the dialogue context than those in prior datasets. Furthermore, we demonstrate the utility of PEARL by showing that our downstream models outperform baselines in both human and automatic evaluations. We release our dataset and code.</abstract>
      <url hash="f0492116">2024.findings-acl.65</url>
      <bibkey>kim-etal-2024-pearl</bibkey>
      <doi>10.18653/v1/2024.findings-acl.65</doi>
    </paper>
    <paper id="66">
      <title><fixed-case>C</fixed-case>o<fixed-case>LL</fixed-case>a<fixed-case>VO</fixed-case>: Crayon Large Language and Vision m<fixed-case>O</fixed-case>del</title>
      <author><first>Byung-Kwan</first><last>Lee</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Beomchan</first><last>Park</last><affiliation>KAIST</affiliation></author>
      <author><first>Chae Won</first><last>Kim</last></author>
      <author><first>Yong Man</first><last>Ro</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>1121-1138</pages>
      <abstract>The remarkable success of Large Language Models (LLMs) and instruction tuning drives the evolution of Vision Language Models (VLMs) towards a versatile general-purpose model. Yet, it remains unexplored whether current VLMs genuinely possess quality object-level image understanding capabilities determined from ‘what objects are in the image?’ or ‘which object corresponds to a specified bounding box?’. Our findings reveal that the image understanding capabilities of current VLMs are strongly correlated with their zero-shot performance on vision language (VL) tasks. This suggests that prioritizing basic image understanding is crucial for VLMs to excel at VL tasks. To enhance object-level image understanding, we propose Crayon Large Language and Vision mOdel (CoLLaVO), which incorporates instruction tuning with Crayon Prompt as a new visual prompt tuning scheme based on panoptic color maps. Furthermore, we present a learning strategy of Dual QLoRA to preserve object-level image understanding without forgetting it during visual instruction tuning, thereby achieving a significant leap in numerous VL benchmarks in a zero-shot setting.</abstract>
      <url hash="eaa42eca">2024.findings-acl.66</url>
      <bibkey>lee-etal-2024-collavo</bibkey>
      <doi>10.18653/v1/2024.findings-acl.66</doi>
    </paper>
    <paper id="67">
      <title>Modelling Variability in Human Annotator Simulation</title>
      <author><first>Wen</first><last>Wu</last></author>
      <author><first>Wenlin</first><last>Chen</last><affiliation>University of Cambridge and Max Planck Institute for Intelligent Systems</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Tsinghua University and University College London</affiliation></author>
      <author><first>Phil</first><last>Woodland</last><affiliation>University of Cambridge</affiliation></author>
      <pages>1139-1157</pages>
      <abstract>Human annotator simulation (HAS) serves as a cost-effective substitute for human evaluation tasks such as data annotation and system assessment. It is important to incorporate the variability present in human evaluation into HAS, since it helps capture diverse subjective interpretations and mitigate potential biases and over-representation. This work introduces a novel framework for modelling variability in HAS. Conditional softmax flow (S-CNF) is proposed to model the distribution of subjective human annotations, which leverages diverse human annotations via meta-learning. This enables efficient generation of annotations that exhibit human variability for unlabelled input. In addition, a wide range of evaluation metrics are adopted to assess the capability and efficiency of HAS systems in predicting the aggregated behaviours of human annotators, matching the distribution of human annotations, and simulating the inter-annotator disagreements. Results demonstrate that the proposed method achieves state-of-the-art performance on two real-world human evaluation tasks: emotion recognition and toxic speech detection.</abstract>
      <url hash="685a368e">2024.findings-acl.67</url>
      <bibkey>wu-etal-2024-modelling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.67</doi>
    </paper>
    <paper id="68">
      <title><fixed-case>BE</fixed-case>n<fixed-case>QA</fixed-case>: A Question Answering Benchmark for <fixed-case>B</fixed-case>engali and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Sheikh</first><last>Shafayat</last><affiliation>KAIST</affiliation></author>
      <author><first>H</first><last>Hasan</last></author>
      <author><first>Minhajur</first><last>Mahim</last></author>
      <author><first>Rifki</first><last>Putri</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <author><first>Alice</first><last>Oh</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>1158-1177</pages>
      <abstract>In this study, we introduce BEnQA, a dataset comprising parallel Bengali and English exam questions for middle and high school levels in Bangladesh. Our dataset consists of approximately 5K questions covering several subjects in science with different types of questions, including factual, application, and reasoning-based questions. We benchmark several Large Language Models (LLMs) with our parallel dataset and observe a notable performance disparity between the models in Bengali and English. We also investigate some prompting methods, and find that Chain-of-Thought prompting is beneficial mostly on reasoning questions, but not so much on factual ones. We also find that appending English translation helps to answer questions in Bengali. Our findings point to promising future research directions for improving the performance of LLMs in Bengali and more generally in low-resource languages.</abstract>
      <url hash="afd3f278">2024.findings-acl.68</url>
      <bibkey>shafayat-etal-2024-benqa</bibkey>
      <doi>10.18653/v1/2024.findings-acl.68</doi>
    </paper>
    <paper id="69">
      <title><fixed-case>MORE</fixed-case>: Multi-m<fixed-case>O</fixed-case>dal <fixed-case>RE</fixed-case>trieval Augmented Generative Commonsense Reasoning</title>
      <author><first>Wanqing</first><last>Cui</last></author>
      <author><first>Keping</first><last>Bi</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <pages>1178-1192</pages>
      <abstract>Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge. Several studies have leveraged text retrieval to augment the models’ commonsense ability. Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel <tex-math>\textbf{M}</tex-math>ulti-m<tex-math>\textbf{O}</tex-math>dal <tex-math>\textbf{RE}</tex-math>trieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities.</abstract>
      <url hash="f1f46b27">2024.findings-acl.69</url>
      <bibkey>cui-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.69</doi>
    </paper>
    <paper id="70">
      <title>Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models</title>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongbang</first><last>Yuan</last></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Jiexin</first><last>Xu</last></author>
      <author><first>Huaijun</first><last>Li</last></author>
      <author><first>Xiaojian</first><last>Jiang</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>1193-1215</pages>
      <abstract>Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by the insights, we propose a novel method called Pruning Head via PatH PatcHing (PH3), which can efficiently mitigate knowledge conflicts by pruning conflicting attention heads without updating model parameters. PH3 can flexibly control eight LMs to use internal memory (<tex-math>\uparrow</tex-math> 44.0%) or external context (<tex-math>\uparrow</tex-math> 38.5%). Moreover, PH3 can also improve the performance of LMs on open-domain QA tasks. We also conduct extensive experiments to demonstrate the cross-model, cross-relation, and cross-format generalization of our method. Our code is publicly available at https://github.com/jinzhuoran/MConflict/.</abstract>
      <url hash="e3b88564">2024.findings-acl.70</url>
      <bibkey>jin-etal-2024-cutting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.70</doi>
    </paper>
    <paper id="71">
      <title><fixed-case>B</fixed-case>io<fixed-case>T</fixed-case>5+: Towards Generalized Biological Understanding with <fixed-case>IUPAC</fixed-case> Integration and Multi-task Tuning</title>
      <author><first>Qizhi</first><last>Pei</last></author>
      <author><first>Lijun</first><last>Wu</last></author>
      <author><first>Kaiyuan</first><last>Gao</last></author>
      <author><first>Xiaozhuan</first><last>Liang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yin</first><last>Fang</last></author>
      <author><first>Jinhua</first><last>Zhu</last></author>
      <author><first>Shufang</first><last>Xie</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>1216-1240</pages>
      <abstract>Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their textual descriptions, providing a more holistic understanding of biological entities, and largely improving the grounded reasoning of bio-text and bio-sequences. The model is pre-trained and fine-tuned with a large number of experiments, including <i>3 types of problems (classification, regression, generation), 15 kinds of tasks, and 21 total benchmark datasets</i>, demonstrating the remarkable performance and state-of-the-art results in most cases. BioT5+ stands out for its ability to capture intricate relationships in biological data, thereby contributing significantly to bioinformatics and computational biology. Our code is available at https://github.com/QizhiPei/BioT5.</abstract>
      <url hash="81277f81">2024.findings-acl.71</url>
      <bibkey>pei-etal-2024-biot5</bibkey>
      <doi>10.18653/v1/2024.findings-acl.71</doi>
    </paper>
    <paper id="72">
      <title><fixed-case>SIBO</fixed-case>: A Simple Booster for Parameter-Efficient Fine-Tuning</title>
      <author><first>Zhihao</first><last>Wen</last></author>
      <author><first>Jie</first><last>Zhang</last></author>
      <author><first>Yuan</first><last>Fang</last><affiliation>Singapore Management University</affiliation></author>
      <pages>1241-1257</pages>
      <abstract>Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straightforward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT methods on the arithmetic and commonsense reasoning tasks, respectively.</abstract>
      <url hash="48048a21">2024.findings-acl.72</url>
      <bibkey>wen-etal-2024-sibo</bibkey>
      <doi>10.18653/v1/2024.findings-acl.72</doi>
    </paper>
    <paper id="73">
      <title><fixed-case>G</fixed-case>eo<fixed-case>E</fixed-case>val: Benchmark for Evaluating <fixed-case>LLM</fixed-case>s and Multi-Modal Models on Geometry Problem-Solving</title>
      <author><first>Jiaxin</first><last>Zhang</last><affiliation>University of Strathclyde</affiliation></author>
      <author><first>Zhong-Zhi</first><last>Li</last></author>
      <author><first>Ming-Liang</first><last>Zhang</last></author>
      <author><first>Fei</first><last>Yin</last><affiliation>, Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Cheng-Lin</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yashar</first><last>Moshfeghi</last><affiliation>University of Strathclyde</affiliation></author>
      <pages>1258-1276</pages>
      <abstract>Recent advancements in large language models (LLMs) and multi-modal models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2,000 problems, a 750 problems subset focusing on backward reasoning, an augmented sub- set of 2,000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs in solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67% accuracy rate on the main subset but only a 6.00% accuracy on the hard subset. This highlights the critical need for testing models against datasets on which they have not been pre-trained. Additionally, our findings indicate that GPT-series models perform more effectively on problems they have rephrased, suggesting a promising method for enhancing model capabilities.</abstract>
      <url hash="3410166e">2024.findings-acl.73</url>
      <bibkey>zhang-etal-2024-geoeval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.73</doi>
    </paper>
    <paper id="74">
      <title>Boosting Textural <fixed-case>NER</fixed-case> with Synthetic Image and Instructive Alignment</title>
      <author><first>Jiahao</first><last>Wang</last></author>
      <author><first>Wenjun</first><last>Ke</last><affiliation>Southeast University</affiliation></author>
      <author><first>Peng</first><last>Wang</last></author>
      <author><first>Hang</first><last>Zhang</last></author>
      <author><first>Dong</first><last>Nie</last><affiliation>Meta Inc.</affiliation></author>
      <author><first>Jiajun</first><last>Liu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Guozheng</first><last>Li</last><affiliation>Southeast University</affiliation></author>
      <author><first>Ziyu</first><last>Shang</last></author>
      <pages>1277-1287</pages>
      <abstract>Named entity recognition (NER) is a pivotal task reliant on textual data, often impeding the disambiguation of entities due to the absence of context. To tackle this challenge, conventional methods often incorporate images crawled from the internet as auxiliary information. However, the images often lack sufficient entities or would introduce noise. Even with high-quality images, it is still challenging to efficiently use images as auxiliaries (i.e., fine-grained alignment with texts). We introduce a novel method named InstructNER to address these issues. Leveraging the rich real-world knowledge and image synthesis capabilities of a large pre-trained stable diffusion (SD) model, InstructNER transforms the text-only NER into a multimodal NER (MNER) task. A selection process automatically identifies the best synthetic image by comparing fine-grained similarities with internet-crawled images through a visual bag-of-words strategy. Note, during the image synthesis, a cross-attention matrix between synthetic images and raw text emerges, which inspires a soft attention guidance alignment (AGA) mechanism. AGA optimizes the MNER task and concurrently facilitates instructive alignment in MNER. Empirical experiments on prominent MNER datasets show that our method surpasses all text-only baselines, improving F1-score by 1.4% to 2.3%. Remarkably, even when compared to fully multimodal baselines, our approach maintains competitive. Furthermore, we open-source a comprehensive synthetic image dataset and the code to supplement existing raw dataset. The code and datasets are available in https://github.com/Heyest/InstructNER.</abstract>
      <url hash="204b74b5">2024.findings-acl.74</url>
      <bibkey>wang-etal-2024-boosting</bibkey>
      <revision id="1" href="2024.findings-acl.74v1" hash="09463288"/>
      <revision id="2" href="2024.findings-acl.74v2" hash="204b74b5" date="2024-08-22">Minor updates.</revision>
      <doi>10.18653/v1/2024.findings-acl.74</doi>
    </paper>
    <paper id="75">
      <title>Neurons in Large Language Models: Dead, N-gram, Positional</title>
      <author><first>Elena</first><last>Voita</last><affiliation>FAIR at Meta AI and University of Amsterdam</affiliation></author>
      <author><first>Javier</first><last>Ferrando</last></author>
      <author><first>Christoforos</first><last>Nalmpantis</last></author>
      <pages>1288-1301</pages>
      <abstract>We analyze a family of large language models in such a lightweight manner that can be done on a single GPU. Specifically, we focus on the OPT family of models ranging from 125m to 66b parameters and rely only on whether an FFN neuron is activated or not. First, we find that the early part of the network is sparse and represents many discrete features. Here, many neurons (more than in some layers of the 66b model) are “dead”, i.e. they never activate on a large collection of diverse data. At the same time, many of the alive neurons are reserved for discrete features and act as token and n-gram detectors. Interestingly, their corresponding FFN updates not only promote next token candidates as could be expected, but also explicitly focus on removing the information about triggering them tokens, i.e., current input. To the best of our knowledge, this is the first example of mechanisms specialized at removing (rather than adding) information from the residual stream. With scale, models become more sparse in a sense that they have more dead neurons and token detectors. Finally, some neurons are positional: them being activated or not depends largely (or solely) on position and less so (or not at all) on textual data. We find that smaller models have sets of neurons acting as position range indicators while larger models operate in a less explicit manner.</abstract>
      <url hash="9c26a1a0">2024.findings-acl.75</url>
      <bibkey>voita-etal-2024-neurons</bibkey>
      <doi>10.18653/v1/2024.findings-acl.75</doi>
    </paper>
    <paper id="76">
      <title><fixed-case>LLM</fixed-case>s as Bridges: Reformulating Grounded Multimodal Named Entity Recognition</title>
      <author><first>Jinyuan</first><last>Li</last></author>
      <author><first>Han</first><last>Li</last></author>
      <author><first>Di</first><last>Sun</last><affiliation>Tianjin University of Science and Technology</affiliation></author>
      <author><first>Jiahao</first><last>Wang</last></author>
      <author><first>Wenkun</first><last>Zhang</last></author>
      <author><first>Zan</first><last>Wang</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Gang</first><last>Pan</last><affiliation>Tianjin University</affiliation></author>
      <pages>1302-1318</pages>
      <abstract>Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two major limitations of existing GMNER methods. 2) The introduction of entity expansion expression and Visual Entailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG). It enables RiVEG to effortlessly inherit the Visual Entailment and Visual Grounding capabilities of any current or prospective multimodal pretraining models. Extensive experiments demonstrate that RiVEG outperforms state-of-the-art methods on the existing GMNER dataset and achieves absolute leads of 10.65%, 6.21%, and 8.83% in all three subtasks.</abstract>
      <url hash="71225597">2024.findings-acl.76</url>
      <bibkey>li-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.findings-acl.76</doi>
    </paper>
    <paper id="77">
      <title>Learning Job Title Representation from Job Description Aggregation Network</title>
      <author><first>Napat</first><last>Laosaengpha</last></author>
      <author><first>Thanit</first><last>Tativannarat</last></author>
      <author><first>Chawan</first><last>Piansaddhayanon</last><affiliation>Chulalongkorn University</affiliation></author>
      <author><first>Attapol</first><last>Rutherford</last><affiliation>Chulalongkorn University</affiliation></author>
      <author><first>Ekapol</first><last>Chuangsuwanich</last><affiliation>Chulalongkorn University</affiliation></author>
      <pages>1319-1329</pages>
      <abstract>Learning job title representation is a vital process for developing automatic human resource tools. To do so, existing methods primarily rely on learning the title representation through skills extracted from the job description, neglecting the rich and diverse content within. Thus, we propose an alternative framework for learning job titles through their respective job description (JD) and utilize a Job Description Aggregator component to handle the lengthy description and bidirectional contrastive loss to account for the bidirectional relationship between the job title and its description. We evaluated the performance of our method on both in-domain and out-of-domain settings, achieving a superior performance over the skill-based approach.</abstract>
      <url hash="0ddc14f2">2024.findings-acl.77</url>
      <bibkey>laosaengpha-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.77</doi>
    </paper>
    <paper id="78">
      <title><fixed-case>F</fixed-case>low<fixed-case>VQA</fixed-case>: Mapping Multimodal Logic in Visual Question Answering with Flowcharts</title>
      <author><first>Shubhankar</first><last>Singh</last></author>
      <author><first>Purvi</first><last>Chaurasia</last></author>
      <author><first>Yerram</first><last>Varun</last></author>
      <author><first>Pranshu</first><last>Pandya</last></author>
      <author><first>Vatsal</first><last>Gupta</last><affiliation>Indian Institute of Technology, Guwahati</affiliation></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>University of Pennsylvania, United States</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>1330-1350</pages>
      <abstract>Existing benchmarks for visual question answering lack in visual grounding and complexity, particularly in evaluating spatial reasoning skills. We introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of visual question-answering multimodal language models in reasoning with flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and human-verified flowchart images from three distinct content sources, along with 22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks, including information localization, decision-making, and logical progression. We conduct a thorough baseline evaluation on a suite of both open-source and proprietary multimodal language models using various strategies, followed by an analysis of directional bias. The results underscore the benchmark’s potential as a vital tool for advancing the field of multimodal modeling, providing a focused and challenging environment for enhancing model performance in visual and logical reasoning tasks.</abstract>
      <url hash="4c4eefda">2024.findings-acl.78</url>
      <bibkey>singh-etal-2024-flowvqa</bibkey>
      <doi>10.18653/v1/2024.findings-acl.78</doi>
    </paper>
    <paper id="79">
      <title>Flexible Weight Tuning and Weight Fusion Strategies for Continual Named Entity Recognition</title>
      <author><first>Yahan</first><last>Yu</last><affiliation>Kyoto University, Kyoto University</affiliation></author>
      <author><first>Duzhen</first><last>Zhang</last></author>
      <author><first>Xiuyi</first><last>Chen</last></author>
      <author><first>Chenhui</first><last>Chu</last><affiliation>Kyoto University</affiliation></author>
      <pages>1351-1358</pages>
      <abstract>Continual Named Entity Recognition (CNER) is dedicated to sequentially learning new entity types while mitigating catastrophic forgetting of old entity types. Traditional CNER approaches commonly employ knowledge distillation to retain old knowledge within the current model. However, because only the representations of old and new models are constrained to be consistent, the reliance solely on distillation in existing methods still suffers from catastrophic forgetting. To further alleviate the forgetting issue of old entity types, this paper introduces flexible Weight Tuning (WT) and Weight Fusion (WF) strategies for CNER. The WT strategy, applied at each training step, employs a learning rate schedule on the parameters of the current model. After learning the current task, the WF strategy dynamically integrates knowledge from both the current and previous models for inference. Notably, these two strategies are model-agnostic and seamlessly integrate with existing State-Of-The-Art (SOTA) models. Extensive experiments demonstrate that the WT and WF strategies consistently enhance the performance of previous SOTA methods across ten CNER settings in three datasets.</abstract>
      <url hash="1ca39e64">2024.findings-acl.79</url>
      <bibkey>yu-etal-2024-flexible</bibkey>
      <doi>10.18653/v1/2024.findings-acl.79</doi>
    </paper>
    <paper id="80">
      <title>Unveiling the Achilles’ Heel of <fixed-case>NLG</fixed-case> Evaluators: A Unified Adversarial Framework Driven by Large Language Models</title>
      <author><first>Yiming</first><last>Chen</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Chen</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Danqing</first><last>Luo</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Luis Fernando</first><last>D’Haro</last><affiliation>Universidad Politécnica de Madrid</affiliation></author>
      <author><first>Robby</first><last>Tan</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Haizhou</first><last>Li</last><affiliation>The Chinese University of Hong Kong (Shenzhen); National University of Singapore and National University of Singapore</affiliation></author>
      <pages>1359-1375</pages>
      <abstract>The automatic evaluation of natural language generation (NLG) systems presents a long-lasting challenge. Recent studies have highlighted various neural metrics that align well with human evaluations. Yet, the robustness of these evaluators against adversarial perturbations remains largely under-explored due to the unique challenges in obtaining adversarial data for different NLG evaluation tasks. To address the problem, we introduce AdvEval, a novel black-box adversarial framework against NLG evaluators. AdvEval is specially tailored to generate data that yield strong disagreements between human and victim evaluators. Specifically, inspired by the recent success of large language models (LLMs) in text generation and evaluation, we adopt strong LLMs as both the data generator and gold evaluator. Adversarial data are automatically optimized with feedback from the gold and victim evaluator. We conduct experiments on 12 victim evaluators and 11 NLG datasets, spanning tasks including dialogue, summarization, and question evaluation. The results show that AdvEval can lead to significant performance degradation of various victim metrics, thereby validating its efficacy.</abstract>
      <url hash="facd7198">2024.findings-acl.80</url>
      <bibkey>chen-etal-2024-unveiling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.80</doi>
    </paper>
    <paper id="81">
      <title>Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models</title>
      <author><first>Adian</first><last>Liusie</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Yassir</first><last>Fathullah</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <pages>1376-1387</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where LLMs’ outputs may significantly vary depending on the order of the input options. While debiasing techniques can mitigate these issues, and yield better performance and reliability, they often come with a high computational cost at inference. This paper addresses this inefficiency at inference time. The aim is to distill the capabilities of a computationally intensive, debiased, teacher model into a more compact student model. We explore two variants of student models: one based on pure distillation, and the other on an error-correction approach for more complex tasks, where the student corrects a single biased decision from the teacher to achieve a debiased output. Our approach is general and can be applied to both black-box and white-box LLMs. Furthermore, we demonstrate that our compact, encoder-only student models can outperform their larger, biased teacher counterparts, achieving better results with significantly fewer parameters.</abstract>
      <url hash="3e0e8921">2024.findings-acl.81</url>
      <bibkey>liusie-etal-2024-teacher</bibkey>
      <doi>10.18653/v1/2024.findings-acl.81</doi>
    </paper>
    <paper id="82">
      <title>Uncovering Limitations of Large Language Models in Information Seeking from Tables</title>
      <author><first>Chaoxu</first><last>Pang</last></author>
      <author><first>Yixuan</first><last>Cao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chunhao</first><last>Yang</last></author>
      <author><first>Ping</first><last>Luo</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>1388-1409</pages>
      <abstract>Tables are recognized for their high information density and widespread usage, serving as essential sources of information. Seeking information from tables (TIS) is a crucial capability for Large Language Models (LLMs), serving as the foundation of knowledge-based Q&amp;A systems. However, this field presently suffers from an absence of thorough and reliable evaluation. This paper introduces a more reliable benchmark for Table Information Seeking (TabIS). To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format. We establish an effective pipeline for generating options, ensuring their difficulty and quality. Experiments conducted on 12 LLMs reveal that while the performance of GPT-4-turbo is marginally satisfactory, both other proprietary and open-source models perform inadequately. Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems). These findings uncover the limitations and potential challenges of LLMs in seeking information from tables. We release our data and code to facilitate further research in this field.</abstract>
      <url hash="0b9ca2a8">2024.findings-acl.82</url>
      <bibkey>pang-etal-2024-uncovering</bibkey>
      <doi>10.18653/v1/2024.findings-acl.82</doi>
    </paper>
    <paper id="83">
      <title>An Ensemble-of-Experts Framework for Rehearsal-free Continual Relation Extraction</title>
      <author><first>Shen</first><last>Zhou</last></author>
      <author id="yongqi-li-hk"><first>Yongqi</first><last>Li</last></author>
      <author><first>Xin</first><last>Miao</last></author>
      <author><first>Tieyun</first><last>Qian</last><affiliation>Wuhan University</affiliation></author>
      <pages>1410-1423</pages>
      <abstract>Continual relation extraction (CRE) aims to continuously learn relations in new tasks without forgetting old relations in previous tasks.Current CRE methods are all rehearsal-based which need to store samples and thus may encounter privacy and security issues.This paper targets rehearsal-free continual relation extraction for the first time and decomposes it into task identification and within-task prediction sub-problems. Existing rehearsal-free methods focus on training a model (expert) for within-task prediction yet neglect to enhance models’ capability of task identification.In this paper, we propose an Ensemble-of-Experts (EoE) framework for rehearsal-free continual relation extraction. Specifically, we first discriminatively train each expert by augmenting analogous relations across tasks to enhance the expert’s task identification ability. We then propose a cascade voting mechanism to form an ensemble of experts for effectively aggregating their abilities.Extensive experiments demonstrate that our method outperforms current rehearsal-free methods and is even better than rehearsal-based CRE methods.</abstract>
      <url hash="a3f404a5">2024.findings-acl.83</url>
      <bibkey>zhou-etal-2024-ensemble</bibkey>
      <doi>10.18653/v1/2024.findings-acl.83</doi>
    </paper>
    <paper id="84">
      <title>Temporal Validity Change Prediction</title>
      <author><first>Georg</first><last>Wenzel</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <pages>1424-1446</pages>
      <abstract>Temporal validity is an important property of text that has many downstream applications, such as recommender systems, conversational AI, and user status tracking. Existing benchmarking tasks often require models to identify the temporal validity duration of a single statement. However, many data sources contain additional context, such as successive sentences in a story or posts on a social media profile. This context may alter the duration for which the originally collected statement is expected to be valid. We propose Temporal Validity Change Prediction, a natural language processing task benchmarking the capability of machine learning models to detect context statements that induce such change. We create a dataset consisting of temporal target statements sourced from Twitter and crowdsource corresponding context statements. We then benchmark a set of transformer-based language models on our dataset. Finally, we experiment with a multitasking approach to improve the state-of-the-art performance.</abstract>
      <url hash="058608fb">2024.findings-acl.84</url>
      <bibkey>wenzel-jatowt-2024-temporal</bibkey>
      <doi>10.18653/v1/2024.findings-acl.84</doi>
    </paper>
    <paper id="85">
      <title><fixed-case>RIFF</fixed-case>: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models</title>
      <author><first>Saeed</first><last>Najafi</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Alona</first><last>Fyshe</last><affiliation>University of Alberta</affiliation></author>
      <pages>1447-1466</pages>
      <abstract>Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods. To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective. Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone. The code used for our experiments can be found at https://github.com/SaeedNajafi/RIFF.</abstract>
      <url hash="b362ba60">2024.findings-acl.85</url>
      <bibkey>najafi-fyshe-2024-riff</bibkey>
      <doi>10.18653/v1/2024.findings-acl.85</doi>
    </paper>
    <paper id="86">
      <title>Modelling Commonsense Commonalities with Multi-Facet Concept Embeddings</title>
      <author><first>Hanane</first><last>Kteich</last></author>
      <author><first>Na</first><last>Li</last><affiliation>School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology</affiliation></author>
      <author><first>Usashi</first><last>Chatterjee</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Zied</first><last>Bouraoui</last><affiliation>CRIL Univ-Artois &amp; CNRS</affiliation></author>
      <author><first>Steven</first><last>Schockaert</last><affiliation>Cardiff University</affiliation></author>
      <pages>1467-1480</pages>
      <abstract>Concept embeddings offer a practical and efficient mechanism for injecting commonsense knowledge into downstream tasks. Their core purpose is often not to predict the commonsense properties of concepts themselves, but rather to identify commonalities, i.e. sets of concepts which share some property of interest. Such commonalities are the basis for inductive generalisation, hence high-quality concept embeddings can make learning easier and more robust. Unfortunately, standard embeddings primarily reflect basic taxonomic categories, making them unsuitable for finding commonalities that refer to more specific aspects (e.g. the colour of objects or the materials they are made of). In this paper, we address this limitation by explicitly modelling the different facets of interest when learning concept embeddings. We show that this leads to embeddings which capture a more diverse range of commonsense properties, and consistently improves results in downstream tasks such as ultra-fine entity typing and ontology completion.</abstract>
      <url hash="f2c31743">2024.findings-acl.86</url>
      <bibkey>kteich-etal-2024-modelling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.86</doi>
    </paper>
    <paper id="87">
      <title>Revisiting Multimodal Transformers for Tabular Data with Text Fields</title>
      <author><first>Thomas</first><last>Bonnier</last><affiliation>Centrale Lille Alumni</affiliation></author>
      <pages>1481-1500</pages>
      <abstract>Tabular data with text fields can be leveraged in applications such as financial risk assessment or medical diagnosis prediction. When employing multimodal approaches to make predictions based on these modalities, it is crucial to make the most appropriate modeling choices in terms of numerical feature encoding or fusion strategy. In this paper, we focus on multimodal classification tasks based on tabular datasets with text fields. We build on multimodal Transformers to propose the Tabular-Text Transformer (TTT), a tabular/text dual-stream Transformer network. This architecture includes a distance-to-quantile embedding scheme for numerical features and an overall attention module which concurrently considers self-attention and cross-modal attention. Further, we leverage the two well-informed modality streams to estimate whether a prediction is uncertain or not. To explain uncertainty in terms of feature values, we use a sampling-based approximation of Shapley values in a bimodal context, with two options for the value function. To show the efficacy and relevance of this approach, we compare it to six baselines and measure its ability to quantify and explain uncertainty against various methods. Our code is available at https://github.com/thomas-bonnier/TabularTextTransformer.</abstract>
      <url hash="7262c3fe">2024.findings-acl.87</url>
      <bibkey>bonnier-2024-revisiting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.87</doi>
    </paper>
    <paper id="88">
      <title>An Empirical Study on the Characteristics of Bias upon Context Length Variation for <fixed-case>B</fixed-case>angla</title>
      <author><first>Jayanta</first><last>Sadhu</last></author>
      <author><first>Ayan</first><last>Khan</last></author>
      <author><first>Abhik</first><last>Bhattacharjee</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <author><first>Rifat</first><last>Shahriyar</last><affiliation>Bangladesh University of Engineering and Technology</affiliation></author>
      <pages>1501-1520</pages>
      <abstract>Pretrained language models inherently exhibit various social biases, prompting a crucial examination of their social impact across various linguistic contexts due to their widespread usage. Previous studies have provided numerous methods for intrinsic bias measurements, predominantly focused on high-resource languages. In this work, we aim to extend these investigations to Bangla, a low-resource language. Specifically, in this study, we (1) create a dataset for intrinsic gender bias measurement in Bangla, (2) discuss necessary adaptations to apply existing bias measurement methods for Bangla, and (3) examine the impact of context length variation on bias measurement, a factor that has been overlooked in previous studies. Through our experiments, we demonstrate a clear dependency of bias metrics on context length, highlighting the need for nuanced considerations in Bangla bias analysis. We consider our work as a stepping stone for bias measurement in the Bangla Language and make all of our resources publicly available to support future research.</abstract>
      <url hash="33d272f8">2024.findings-acl.88</url>
      <bibkey>sadhu-etal-2024-empirical-study</bibkey>
      <doi>10.18653/v1/2024.findings-acl.88</doi>
    </paper>
    <paper id="89">
      <title><fixed-case>C</fixed-case>on<fixed-case>T</fixed-case>empo: A Unified Temporally Contrastive Framework for Temporal Relation Extraction</title>
      <author><first>Jingcheng</first><last>Niu</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <author><first>Saifei</first><last>Liao</last></author>
      <author><first>Victoria</first><last>Ng</last></author>
      <author><first>Simon</first><last>De Montigny</last></author>
      <author><first>Gerald</first><last>Penn</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <pages>1521-1533</pages>
      <abstract>The task of temporal relation extraction (TRE) involves identifying and extracting temporal relations between events from narratives. We identify two primary issues with TRE systems. First, by formulating TRE as a simple text classification task where every temporal relation is independent, it is hard to enhance the TRE model’s representation of meaning of temporal relations, and its facility with the underlying temporal calculus. We solve the issue by proposing a novel Temporally Contrastive learning model (ConTempo) that increase the model’s awareness of the meaning of temporal relations by leveraging their symmetric or antisymmetric properties. Second, the reusability of innovations has been limited due to incompatibilities in model architectures. Therefore, we propose a unified framework and show that ConTempo is compatible with all three main branches of TRE research. Our results demonstrate that the performance gains of ConTempo are more pronounced, with the total combination achieving state-of-the-art performance on the widely used MATRES and TBD corpora. We furthermore identified and corrected a large number of annotation errors present in the test set of MATRES, after which the performance increase brought by ConTempo becomes more apparent.</abstract>
      <url hash="b63464a9">2024.findings-acl.89</url>
      <bibkey>niu-etal-2024-contempo</bibkey>
      <doi>10.18653/v1/2024.findings-acl.89</doi>
    </paper>
    <paper id="90">
      <title><fixed-case>CHARP</fixed-case>: Conversation History <fixed-case>A</fixed-case>wa<fixed-case>R</fixed-case>eness Probing for Knowledge-grounded Dialogue Systems</title>
      <author><first>Abbas</first><last>Ghaddar</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>David</first><last>Alfonso-Hermelo</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Philippe</first><last>Langlais</last><affiliation>Université de Montréal</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Prasanna</first><last>Parthasarathi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>1534-1551</pages>
      <abstract>In this work, we dive deep into one of the popular knowledge-grounded dialogue benchmarks that focus on faithfulness, FaithDial. We show that a significant portion of the FaithDial data contains annotation artifacts, which may bias models towards completely ignoring the conversation history. We therefore introduce CHARP, a testbed, designed for evaluating supposedly non-hallucinatory models trained on the FaithDial dataset. Our extensive analysis reveals that models primarily exhibit poor performance on CHARP due to their inability to effectively attend to and reason over the conversation history. Furthermore, the evaluation methods of FaithDial fail to capture these shortcomings, neglecting the conversational history. Our findings indicate that there is substantial room for contribution in both dataset creation and hallucination evaluation for knowledge-grounded dialogue, and that CHARP can serve as a tool for monitoring the progress in this particular research area. Data, models, and source code will be publicly available upon acceptance.</abstract>
      <url hash="37fb045e">2024.findings-acl.90</url>
      <bibkey>ghaddar-etal-2024-charp</bibkey>
      <doi>10.18653/v1/2024.findings-acl.90</doi>
    </paper>
    <paper id="91">
      <title><fixed-case>C</fixed-case>ritic<fixed-case>B</fixed-case>ench: Benchmarking <fixed-case>LLM</fixed-case>s for Critique-Correct Reasoning</title>
      <author><first>Zicheng</first><last>Lin</last></author>
      <author><first>Zhibin</first><last>Gou</last></author>
      <author><first>Tian</first><last>Liang</last></author>
      <author><first>Ruilin</first><last>Luo</last></author>
      <author><first>Haowei</first><last>Liu</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Graduate School at Shenzhen,Tsinghua University</affiliation></author>
      <pages>1552-1587</pages>
      <abstract>The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs’ abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement.</abstract>
      <url hash="18fba52c">2024.findings-acl.91</url>
      <bibkey>lin-etal-2024-criticbench</bibkey>
      <doi>10.18653/v1/2024.findings-acl.91</doi>
    </paper>
    <paper id="92">
      <title><fixed-case>DAFN</fixed-case>et: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models</title>
      <author><first>Taolin</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qizhou</first><last>Chen</last></author>
      <author><first>Dongyang</first><last>Li</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xiaofeng</first><last>He</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Longtao</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hui</first><last>Xue’</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <pages>1588-1602</pages>
      <abstract>Recently, while large language models (LLMs) have demonstrated impressive results, they still suffer from hallucination, i.e., the generation of false information. Model editing is the task of fixing factual mistakes in LLMs; yet, most previous works treat it as a one-time task, paying little attention to ever-emerging mistakes generated by LLMs. We address the task of sequential model editing (SME) that aims to rectify mistakes continuously. A Dynamic Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic interaction among the factual knowledge within the entire sequence, preventing catastrophic forgetting during the editing process of multiple knowledge triples.Specifically, (1) for semantic fusion within a relation triple, we aggregate the intra-editing attention flow into auto-regressive self-attention with token-level granularity in LLMs. We further leverage multi-layer diagonal inter-editing attention flow to update the weighted representations of the entire sequence-level granularity. (2) Considering that auxiliary parameters are required to store the knowledge for sequential editing, we construct a new dataset named DAFSet, fulfilling recent, popular, long-tail and robust properties to enhance the generality of sequential editing. Experiments show DAFNet significantly outperforms strong baselines in single-turn and sequential editing. The usage of DAFSet also consistently improves the performance of other auxiliary network-based methods in various scenarios.</abstract>
      <url hash="c082d97b">2024.findings-acl.92</url>
      <bibkey>zhang-etal-2024-dafnet</bibkey>
      <doi>10.18653/v1/2024.findings-acl.92</doi>
    </paper>
    <paper id="93">
      <title>Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects - A Survey</title>
      <author><first>Ashok</first><last>Urlana</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Pruthwik</first><last>Mishra</last><affiliation>IIIT-Hyderabad</affiliation></author>
      <author><first>Tathagato</first><last>Roy</last></author>
      <author><first>Rahul</first><last>Mishra</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <pages>1603-1623</pages>
      <abstract>Generic text summarization approaches often fail to address the specific intent and needs of individual users. Recently, scholarly attention has turned to the development of summarization methods that are more closely tailored and controlled to align with specific objectives and user needs. Despite a growing corpus of controllable summarization research, there is no comprehensive survey available that thoroughly explores the diverse controllable attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable attributes according to their shared characteristics and objectives, and present a thorough examination of existing datasets and methods within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also exploring potential solutions and future directions for CTS. We release our detailed analysis of CTS papers at https://github.com/ashokurlana/controllable_text_summarization_survey.</abstract>
      <url hash="10c2eb5c">2024.findings-acl.93</url>
      <bibkey>urlana-etal-2024-controllable</bibkey>
      <doi>10.18653/v1/2024.findings-acl.93</doi>
    </paper>
    <paper id="94">
      <title>Benchmarking Large Language Models on Communicative Medical Coaching: A Dataset and a Novel System</title>
      <author><first>Hengguan</first><last>Huang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Songtao</first><last>Wang</last></author>
      <author><first>Hongfu</first><last>Liu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Ye</first><last>Wang</last></author>
      <pages>1624-1637</pages>
      <abstract>Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce “ChatCoach”, a human-AI cooperative framework designed to assist medical learners in practicing their communication skills during patient consultations. ChatCoach differentiates itself from conventional dialogue systems by offering a simulated environment where medical learners can practice dialogues with a patient agent, while a coach agent provides immediate, structured feedback. This is facilitated by our proposed Generalized Chain-of-Thought (GCoT) approach, which fosters the generation of structured feedback and enhances the utilization of external knowledge sources. Additionally, we have developed a dataset specifically for evaluating Large Language Models (LLMs) within the ChatCoach framework on communicative medical coaching tasks. Our empirical results validate the effectiveness of ChatCoach.</abstract>
      <url hash="eb01e453">2024.findings-acl.94</url>
      <bibkey>huang-etal-2024-benchmarking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.94</doi>
    </paper>
    <paper id="95">
      <title>Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation</title>
      <author><first>Ruomeng</first><last>Ding</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Chaoyun</first><last>Zhang</last></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yong</first><last>Xu</last></author>
      <author><first>Minghua</first><last>Ma</last><affiliation>Microsoft</affiliation></author>
      <author><first>Wei</first><last>Zhang</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Si</first><last>Qin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Saravan</first><last>Rajmohan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qingwei</first><last>Lin</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft and Microsoft</affiliation></author>
      <pages>1638-1662</pages>
      <abstract>This paper introduce a novel thought prompting approach called ”Everything of Thoughts” (XoT) for Large Language Models (LLMs) to defy the law of ”Penrose triangle” of existing thought paradigms, to achieve three key perspectives in thought generation simultaneously: performance, efficiency, and flexibility. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge and planning capability into thoughts, thereby enhancing LLMs’ decision-making capabilities. Through the MCTS-LLM collaborative thought revision framework, XoT autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to utilize flexible cognitive mappings for solving problems with multiple solutions.We evaluate XoT on several challenging problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our results demonstrate that XoT significantly outperforms existing approaches in various dimensions, showcasing its remarkable proficiency in addressing complex problems across diverse domains. The data and code are available at https://github.com/microsoft/Everything-of-Thoughts-XoT.</abstract>
      <url hash="963be159">2024.findings-acl.95</url>
      <bibkey>ding-etal-2024-everything</bibkey>
      <doi>10.18653/v1/2024.findings-acl.95</doi>
    </paper>
    <paper id="96">
      <title><fixed-case>SPAGHETTI</fixed-case>: Open-Domain Question Answering from Heterogeneous Data Sources with Retrieval and Semantic Parsing</title>
      <author><first>Heidi</first><last>Zhang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Sina</first><last>Semnani</last><affiliation>Stanford University</affiliation></author>
      <author><first>Farhad</first><last>Ghassemi</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <author><first>Jialiang</first><last>Xu</last></author>
      <author><first>Shicheng</first><last>Liu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Monica</first><last>Lam</last><affiliation>Stanford University</affiliation></author>
      <pages>1663-1678</pages>
      <abstract>We introduce SPAGHETTI: Semantic Parsing Augmented Generation for Hybrid English information from Text Tables and Infoboxes, a hybrid question-answering (QA) pipeline that utilizes information from heterogeneous knowledge sources, including knowledge base, text, tables, and infoboxes. Our LLM-augmented approach achieves state-of-the-art performance on the Compmix dataset, the most comprehensive heterogeneous open-domain QA dataset, with 56.5% exact match (EM) rate. More importantly, manual analysis on a sample of the dataset suggests that SPAGHETTI is more than 90% accurate, indicating that EM is no longer suitable for assessing the capabilities of QA systems today.</abstract>
      <url hash="a7bcf941">2024.findings-acl.96</url>
      <bibkey>zhang-etal-2024-spaghetti</bibkey>
      <doi>10.18653/v1/2024.findings-acl.96</doi>
    </paper>
    <paper id="97">
      <title>Data Augmentation using <fixed-case>LLM</fixed-case>s: Data Perspectives, Learning Paradigms and Challenges</title>
      <author><first>Bosheng</first><last>Ding</last></author>
      <author><first>Chengwei</first><last>Qin</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Ruochen</first><last>Zhao</last></author>
      <author><first>Tianze</first><last>Luo</last></author>
      <author><first>Xinze</first><last>Li</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Guizhen</first><last>Chen</last></author>
      <author><first>Wenhan</first><last>Xia</last></author>
      <author><first>Junjie</first><last>Hu</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>1679-1705</pages>
      <abstract>In the rapidly evolving field of large language models (LLMs), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives, we examine various strategies that utilize LLMs for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally, this paper highlights the primary open challenges faced in this domain, ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve as a comprehensive guide for researchers and practitioners.</abstract>
      <url hash="216c53ba">2024.findings-acl.97</url>
      <bibkey>ding-etal-2024-data</bibkey>
      <doi>10.18653/v1/2024.findings-acl.97</doi>
    </paper>
    <paper id="98">
      <title>k-<fixed-case>S</fixed-case>em<fixed-case>S</fixed-case>tamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text</title>
      <author><first>Abe</first><last>Hou</last></author>
      <author><first>Jingyu</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Yichen</first><last>Wang</last></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Tianxing</first><last>He</last></author>
      <pages>1706-1715</pages>
      <abstract>Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.</abstract>
      <url hash="87227024">2024.findings-acl.98</url>
      <bibkey>hou-etal-2024-k</bibkey>
      <doi>10.18653/v1/2024.findings-acl.98</doi>
    </paper>
    <paper id="99">
      <title><fixed-case>C</fixed-case>olor<fixed-case>S</fixed-case>wap: A Color and Word Order Dataset for Multimodal Evaluation</title>
      <author><first>Jirayu</first><last>Burapacheep</last></author>
      <author><first>Ishan</first><last>Gaur</last></author>
      <author><first>Agam</first><last>Bhatia</last></author>
      <author><first>Tristan</first><last>Thrush</last><affiliation>Stanford University</affiliation></author>
      <pages>1716-1726</pages>
      <abstract>This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a “color-swapped” pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to chance (at 12% and 30%, respectively), although the non-contrastive BLIP ITM model is stronger (87%). We also find that finetuning on fewer than 2,000 examples yields significant performance gains on this out-of-distribution word-order understanding task.</abstract>
      <url hash="a5965878">2024.findings-acl.99</url>
      <bibkey>burapacheep-etal-2024-colorswap</bibkey>
      <doi>10.18653/v1/2024.findings-acl.99</doi>
    </paper>
    <paper id="100">
      <title>Revisiting <fixed-case>OPRO</fixed-case>: The Limitations of Small-Scale <fixed-case>LLM</fixed-case>s as Optimizers</title>
      <author><first>Tuo</first><last>Zhang</last></author>
      <author><first>Jinyue</first><last>Yuan</last></author>
      <author><first>Salman</first><last>Avestimehr</last><affiliation>University of Southern California</affiliation></author>
      <pages>1727-1735</pages>
      <abstract>Numerous recent works aim to enhance the efficacy of Large Language Models (LLMs) through strategic prompting. In particular, the Optimization by PROmpting (OPRO) approach provides state-of-the-art performance by leveraging LLMs as optimizers where the optimization task is to find instructions that maximize the task accuracy. In this paper, we revisit OPRO for automated prompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral 7B. Our investigation reveals that OPRO shows limited effectiveness in small-scale LLMs, with limited inference capabilities constraining optimization ability. We suggest future automatic prompting engineering to consider both model capabilities and computational costs. Additionally, for small-scale LLMs, we recommend direct instructions that clearly outline objectives and methodologies as robust prompt baselines, ensuring efficient and effective prompt engineering in ongoing research.</abstract>
      <url hash="16875fb1">2024.findings-acl.100</url>
      <bibkey>zhang-etal-2024-revisiting-opro</bibkey>
      <doi>10.18653/v1/2024.findings-acl.100</doi>
    </paper>
    <paper id="101">
      <title><fixed-case>C</fixed-case>ee<fixed-case>BERT</fixed-case>: Cross-Domain Inference in Early Exit <fixed-case>BERT</fixed-case></title>
      <author><first>Divya Jyoti</first><last>Bajpai</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Manjesh</first><last>Hanawal</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <pages>1736-1748</pages>
      <abstract>Pre-trained Language Models (PLMs), like BERT, with self-supervision objectives exhibit remarkable performance and generalization across various tasks. However, they suffer in inference latency due to their large size. To address this issue, side branches are attached at intermediate layers, enabling early inference of samples without requiring them to pass through all layers. However, the challenge is to decide which layer to infer and exit each sample so that the accuracy and latency are balanced. Moreover, the distribution of the samples to be inferred may differ from that used for training necessitating cross-domain adaptation. We propose an online learning algorithm named Cross-Domain Inference in Early Exit BERT (CeeBERT) that dynamically determines early exits of samples based on the level of confidence at each exit point. CeeBERT learns optimal thresholds from domain-specific confidence observed at intermediate layers on the fly, eliminating the need for labeled data. Experimental results on five distinct datasets with BERT and ALBERT models demonstrate CeeBERT’s ability to improve latency by reducing unnecessary computations with minimal drop in performance. By adapting to the threshold values, CeeBERT can speed up the BERT/ALBERT models by <tex-math>2\times</tex-math> - <tex-math>3.1\times</tex-math> with minimal drop in accuracy. The anonymized source code is available at https://github.com/Div290/CeeBERT.</abstract>
      <url hash="9f28e796">2024.findings-acl.101</url>
      <bibkey>bajpai-hanawal-2024-ceebert</bibkey>
      <doi>10.18653/v1/2024.findings-acl.101</doi>
    </paper>
    <paper id="102">
      <title><fixed-case>UNIWIZ</fixed-case>: A Unified Large Language Model Orchestrated Wizard for Safe Knowledge Grounded Conversations</title>
      <author><first>Souvik</first><last>Das</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <author><first>Rohini</first><last>Srihari</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <pages>1749-1762</pages>
      <abstract>Large Language Models (LLMs) have made significant progress in integrating safety and knowledge alignment. However, adversarial actors can manipulate these models into generating unsafe responses, and excessive safety alignment can lead to unintended hallucinations. To address these challenges, we introduce UniWiz, a novel 2-step data orchestration framework that unifies safety and knowledge data generation. We propose a “safety-priming” method to generate synthetic safety data and overcome safety bottlenecks. We also inject relevant knowledge into conversations by retrieving factual information from curated sources. UniWiz dataset consists of 17,638 quality-controlled conversations and 10,000 augmented preference data. Pretrained models fine-tuned on UniWiz show improvements across various metrics and outperform state-of-the-art instruction-tuned models trained on much larger datasets.</abstract>
      <url hash="ffa98070">2024.findings-acl.102</url>
      <bibkey>das-srihari-2024-uniwiz</bibkey>
      <doi>10.18653/v1/2024.findings-acl.102</doi>
    </paper>
    <paper id="103">
      <title>A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism</title>
      <author><first>Brian</first><last>Thompson</last><affiliation>Amazon</affiliation></author>
      <author><first>Mehak</first><last>Dhaliwal</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Peter</first><last>Frisch</last><affiliation>Amazon</affiliation></author>
      <author><first>Tobias</first><last>Domhan</last><affiliation>Amazon</affiliation></author>
      <author><first>Marcello</first><last>Federico</last><affiliation>Amazon</affiliation></author>
      <pages>1763-1775</pages>
      <abstract>We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.</abstract>
      <url hash="cd62913d">2024.findings-acl.103</url>
      <bibkey>thompson-etal-2024-shocking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.103</doi>
    </paper>
    <paper id="104">
      <title><fixed-case>R</fixed-case>ank<fixed-case>M</fixed-case>ean: Module-Level Importance Score for Merging Fine-tuned <fixed-case>LLM</fixed-case> Models</title>
      <author><first>Gabriel</first><last>Perin</last></author>
      <author><first>Xuxi</first><last>Chen</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Shusen</first><last>Liu</last><affiliation>Lawrence Livermore National Labs</affiliation></author>
      <author><first>Bhavya</first><last>Kailkhura</last><affiliation>Lawrence Livermore National Laboratory</affiliation></author>
      <author><first>Zhangyang</first><last>Wang</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Brian</first><last>Gallagher</last><affiliation>Lawrence Livermore National Laboratory</affiliation></author>
      <pages>1776-1782</pages>
      <abstract>Traditionally, developing new language models (LMs) capable of addressing multiple tasks involves fine-tuning pre-trained LMs using a wide collection of datasets, a process that often incurs significant computational expenses. Model merging emerges as a cost-effective alternative, allowing the integration of existing models fine-tuned on different tasks into a single model that performs well across all tasks, eliminating the need for additional training. In this paper, we propose RankMean, an algorithm for merging fine-tuned LMs without requiring any downstream data. RankMean determines merging coefficients based on the relative rankings of weight change magnitudes and applies these coefficients for module-wise integration of various fine-tuned models. Our experimental results demonstrate that RankMean outperforms existing baseline methods on multiple benchmarks. The code is available at https://github.com/VITA-Group/RankMean.</abstract>
      <url hash="015ac774">2024.findings-acl.104</url>
      <bibkey>perin-etal-2024-rankmean</bibkey>
      <doi>10.18653/v1/2024.findings-acl.104</doi>
    </paper>
    <paper id="105">
      <title><fixed-case>VALOR</fixed-case>-<fixed-case>EVAL</fixed-case>: Holistic Coverage and Faithfulness Evaluation of Large Vision-Language Models</title>
      <author><first>Haoyi</first><last>Qiu</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Wenbo</first><last>Hu</last></author>
      <author><first>Zi-Yi</first><last>Dou</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>1783-1805</pages>
      <abstract>Large Vision-Language Models (LVLMs) suffer from hallucination issues, wherein the models generate plausible-sounding but factually incorrect outputs, undermining their reliability. A comprehensive quantitative evaluation is necessary to identify and understand the extent of hallucinations in these models. However, existing benchmarks are often limited in scope, focusing mainly on object hallucinations. Furthermore, current evaluation methods struggle to effectively address the subtle semantic distinctions between model outputs and reference data, as well as the balance between hallucination and informativeness. To address these issues, we introduce a multi-dimensional benchmark covering objects, attributes, and relations, with challenging images selected based on associative biases. Moreover, we propose a large language model (LLM)-based two-stage evaluation framework that generalizes the popular CHAIR metric and incorporates both faithfulness and coverage into the evaluation. Experiments on 10 established LVLMs demonstrate that our evaluation metric is more comprehensive and better correlated with humans than existing work when evaluating on our challenging human-annotated benchmark dataset. Our work also highlights the critical balance between faithfulness and coverage of model outputs, and encourages future works to address hallucinations in LVLMs while keeping their outputs informative.</abstract>
      <url hash="d3e0a78c">2024.findings-acl.105</url>
      <bibkey>qiu-etal-2024-valor</bibkey>
      <doi>10.18653/v1/2024.findings-acl.105</doi>
    </paper>
    <paper id="106">
      <title>Cyclical Contrastive Learning Based on Geodesic for Zero-shot Cross-lingual Spoken Language Understanding</title>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Zhihong</first><last>Zhu</last><affiliation>Tencent</affiliation></author>
      <author><first>Bang</first><last>Yang</last></author>
      <author><first>Xianwei</first><last>Zhuang</last></author>
      <author><first>Hongxiang</first><last>Li</last></author>
      <author><first>Yuexian</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <pages>1806-1816</pages>
      <abstract>Owing to the scarcity of labeled training data, Spoken Language Understanding (SLU) is still a challenging task in low-resource languages. Therefore, zero-shot cross-lingual SLU attracts more and more attention. Contrastive learning is widely applied to explicitly align representations of similar sentences across different languages. However, the vanilla contrastive learning method may face two problems in zero-shot cross-lingual SLU: (1) the consistency between different languages is neglected; (2) each utterance has two different kinds of SLU labels, i.e. slot and intent, the utterances with one different label are also pushed away without any discrimination, which limits the performance. In this paper, we propose Cyclical Contrastive Learning based on Geodesic (CCLG), which introduces cyclical contrastive learning to achieve the consistency between different languages and leverages geodesic to measure the similarity to construct the positive pairs and negative pairs. Experimental results demonstrate that our proposed framework achieves the new state-of-the-art performance on MultiATIS++ and MTOP datasets, and the model analysis further verifies that CCLG can effectively transfer knowledge between different languages.</abstract>
      <url hash="cbc33ebb">2024.findings-acl.106</url>
      <bibkey>cheng-etal-2024-cyclical</bibkey>
      <revision id="1" href="2024.findings-acl.106v1" hash="65a6282e"/>
      <revision id="2" href="2024.findings-acl.106v2" hash="cbc33ebb" date="2024-08-17">camera-ready version.</revision>
      <doi>10.18653/v1/2024.findings-acl.106</doi>
    </paper>
    <paper id="107">
      <title>Towards Safer Large Language Models through Machine Unlearning</title>
      <author><first>Zheyuan</first><last>Liu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Guangyao</first><last>Dou</last></author>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Yijun</first><last>Tian</last></author>
      <author><first>Meng</first><last>Jiang</last><affiliation>University of Notre Dame</affiliation></author>
      <pages>1817-1829</pages>
      <abstract>The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model’s performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.</abstract>
      <url hash="349c2912">2024.findings-acl.107</url>
      <bibkey>liu-etal-2024-towards-safer</bibkey>
      <doi>10.18653/v1/2024.findings-acl.107</doi>
    </paper>
    <paper id="108">
      <title>The Impact of Reasoning Step Length on Large Language Models</title>
      <author><first>Mingyu</first><last>Jin</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Qinkai</first><last>Yu</last></author>
      <author><first>Dong</first><last>Shu</last><affiliation>Northwestern University, Northwestern University</affiliation></author>
      <author><first>Haiyan</first><last>Zhao</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author><first>Wenyue</first><last>Hua</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Yanda</first><last>Meng</last><affiliation>University of Exeter</affiliation></author>
      <author><first>Yongfeng</first><last>Zhang</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Mengnan</first><last>Du</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <pages>1830-1842</pages>
      <abstract>Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs’ reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs’ potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences.</abstract>
      <url hash="133881c1">2024.findings-acl.108</url>
      <bibkey>jin-etal-2024-impact</bibkey>
      <doi>10.18653/v1/2024.findings-acl.108</doi>
    </paper>
    <paper id="109">
      <title>Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness</title>
      <author><first>Guangliang</first><last>Liu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Milad</first><last>Afshari</last></author>
      <author><first>Xitong</first><last>Zhang</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Zhiyu</first><last>Xue</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Avrajit</first><last>Ghosh</last></author>
      <author><first>Bidhan</first><last>Bashyal</last></author>
      <author><first>Rongrong</first><last>Wang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Kristen</first><last>Johnson</last><affiliation>Michigan State University</affiliation></author>
      <pages>1843-1856</pages>
      <abstract>While task-agnostic debiasing provides notable generalizability and reduced reliance on downstream data, its impact on language modeling ability and the risk of relearning social biases from downstream task-specific data remain as the two most significant challenges when debiasing Pretrained Language Models (PLMs). The impact on language modeling ability can be alleviated given a high-quality and long-contextualized debiasing corpus, but there remains a deficiency in understanding the specifics of relearning biases. We empirically ascertain that the effectiveness of task-agnostic debiasing hinges on the quantitative bias level of both the task-specific data used for downstream applications and the debiased model. We empirically show that the lower bound of the bias level of the downstream fine-tuned model can be approximated by the bias level of the debiased model, in most practical cases. To gain more in-depth understanding about how the parameters of PLMs change during fine-tuning due to the forgetting issue of PLMs, we propose a novel framework which can Propagate Socially-fair Debiasing to Downstream Fine-tuning, ProSocialTuning. Our proposed framework can push the fine-tuned model to approach the bias lower bound during downstream fine-tuning, indicating that the ineffectiveness of debiasing can be alleviated by overcoming the forgetting issue through regularizing successfully debiased attention heads based on the PLMs’ bias levels from stages of pretraining and debiasing.</abstract>
      <url hash="8b9cc5d7">2024.findings-acl.109</url>
      <bibkey>liu-etal-2024-towards-understanding</bibkey>
      <doi>10.18653/v1/2024.findings-acl.109</doi>
    </paper>
    <paper id="110">
      <title><fixed-case>SKGS</fixed-case>um: Structured Knowledge-Guided Document Summarization</title>
      <author><first>Qiqi</first><last>Wang</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Ruofan</first><last>Wang</last></author>
      <author><first>Kaiqi</first><last>Zhao</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Robert</first><last>Amor</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Benjamin</first><last>Liu</last></author>
      <author><first>Jiamou</first><last>Liu</last><affiliation>The University of Auckland</affiliation></author>
      <author><first>Xianda</first><last>Zheng</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Zijian</first><last>Huang</last><affiliation>University of Auckland</affiliation></author>
      <pages>1857-1871</pages>
      <abstract>A summary structure is inherent to certain types of texts according to the Genre Theory of Linguistics. Such structures aid readers in efficiently locating information within summaries. However, most existing automatic summarization methods overlook the importance of summary structure, resulting in summaries that emphasize the most prominent information while omitting essential details from other sections. While a few summarizers recognize the importance of summary structure, they rely heavily on the predefined labels of summary structures in the source document and ground truth summaries. To address these shortcomings, we developed a Structured Knowledge-Guided Summarization (SKGSum) and its variant, SKGSum-W, which do not require structure labels. Instead, these methods rely on a set of automatically extracted summary points to generate summaries. We evaluate the proposed methods using three real-world datasets. The results indicate that our methods not only improve the quality of summaries, in terms of ROUGE and BERTScore, but also broaden the types of documents that can be effectively summarized.</abstract>
      <url hash="26a2764c">2024.findings-acl.110</url>
      <bibkey>wang-etal-2024-skgsum</bibkey>
      <doi>10.18653/v1/2024.findings-acl.110</doi>
    </paper>
    <paper id="111">
      <title><fixed-case>C</fixed-case>hinese Spoken Named Entity Recognition in Real-world Scenarios: Dataset and Approaches</title>
      <author><first>Shilin</first><last>Zhou</last><affiliation>Soochow University</affiliation></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Chen</first><last>Gong</last></author>
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Yu</first><last>Hong</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>1872-1884</pages>
      <abstract>Spoken Named Entity Recognition (NER) aims to extract entities from speech. The extracted entities can help voice assistants better understand user’s questions and instructions. However, current Chinese Spoken NER datasets are laboratory-controlled data that are collected by reading existing texts in quiet environments, rather than natural spoken data, and the texts used for reading are also limited in topics. These limitations obstruct the development of Spoken NER in more natural and common real-world scenarios. To address this gap, we introduce a real-world Chinese Spoken NER dataset (RWCS-NER), encompassing open-domain daily conversations and task-oriented intelligent cockpit instructions. We compare several mainstream pipeline approaches on RWCS-NER. The results indicate that the current methods, affected by Automatic Speech Recognition (ASR) errors, do not perform satisfactorily in real settings. Aiming to enhance Spoken NER in real-world scenarios, we propose two approaches: self-training-asr and mapping then distilling (MDistilling). Experiments show that both approaches can achieve significant improvements, particularly MDistilling. Even compared with GPT4.0, MDistilling still reaches better results. We believe that our work will advance the field of Spoken NER in real-world settings.</abstract>
      <url hash="6b7a0d34">2024.findings-acl.111</url>
      <bibkey>zhou-etal-2024-chinese</bibkey>
      <doi>10.18653/v1/2024.findings-acl.111</doi>
    </paper>
    <paper id="112">
      <title><fixed-case>DEBATE</fixed-case>: Devil’s Advocate-Based Assessment and Text Evaluation</title>
      <author><first>Alex</first><last>Kim</last></author>
      <author><first>Keonwoo</first><last>Kim</last><affiliation>VRCREW</affiliation></author>
      <author><first>Sangwon</first><last>Yoon</last></author>
      <pages>1885-1897</pages>
      <abstract>As natural language generation (NLG) models have become prevalent, systematically assessing the quality of machine-generated texts has become increasingly important. Recent studies introduce LLM-based evaluators that operate as reference-free metrics, demonstrating their capability to adeptly handle novel tasks. However, these models generally rely on a single-agent approach, which, we argue, introduces an inherent limit to their performance. This is because there exist biases in LLM agent’s responses, including preferences for certain text structure or content. In this work, we propose DEBATE, an NLG evaluation framework based on multi-agent scoring system augmented with a concept of Devil’s Advocate. Within the framework, one agent is instructed to criticize other agents’ arguments, potentially resolving the bias in LLM agent’s answers. DEBATE substantially outperforms the previous state-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation, SummEval and TopicalChat. We also show that the extensiveness of debates among agents and the persona of an agent can influence the performance of evaluators.</abstract>
      <url hash="55cd437e">2024.findings-acl.112</url>
      <bibkey>kim-etal-2024-debate</bibkey>
      <doi>10.18653/v1/2024.findings-acl.112</doi>
    </paper>
    <paper id="113">
      <title>Can Large Multimodal Models Uncover Deep Semantics Behind Images?</title>
      <author><first>Yixin</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zheng</first><last>Li</last></author>
      <author><first>Qingxiu</first><last>Dong</last></author>
      <author><first>Heming</first><last>Xia</last></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <pages>1898-1912</pages>
      <abstract>Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models’ (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision). Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis reveals that LMM performance on DEEPEVAL varies according to the specific facets of deep semantics explored, indicating the fundamental challenges remaining in developing LMMs.</abstract>
      <url hash="5016cac6">2024.findings-acl.113</url>
      <bibkey>yang-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.113</doi>
    </paper>
    <paper id="114">
      <title>Harvesting Events from Multiple Sources: Towards a Cross-Document Event Extraction Paradigm</title>
      <author><first>Qiang</first><last>Gao</last></author>
      <author><first>Zixiang</first><last>Meng</last></author>
      <author><first>Bobo</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Jun</first><last>Zhou</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Chong</first><last>Teng</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <pages>1913-1927</pages>
      <abstract>Document-level event extraction aims to extract structured event information from unstructured text. However, a single document often contains limited event information and the roles of different event arguments may be biased due to the influence of the information source.This paper addresses the limitations of traditional document-level event extraction by proposing the task of cross-document event extraction (CDEE) to integrate event information from multiple documents and provide a comprehensive perspective on events. We construct a novel cross-document event extraction dataset, namely CLES, which contains 20,059 documents and 37,688 mention-level events, where over 70% of them are cross-document. To address the task, we propose a CDEE pipeline that includes 5 steps, namely event extraction, coreference resolution, entity normalization, role normalization and entity-role resolution. Our CDEE pipeline achieves about 72% F1 in end-to-end cross-document event extraction, suggesting the challenge of this task and setting up a benchmark for future research. Our work builds a new line of information extraction research and will attract new research attention.</abstract>
      <url hash="a0af815c">2024.findings-acl.114</url>
      <bibkey>gao-etal-2024-harvesting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.114</doi>
    </paper>
    <paper id="115">
      <title>A Graph per Persona: Reasoning about Subjective Natural Language Descriptions</title>
      <author><first>EunJeong</first><last>Hwang</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Dan</first><last>Gutfreund</last><affiliation>MIT-IBM Watson AI Lab</affiliation></author>
      <author><first>Veronika</first><last>Thost</last><affiliation>International Business Machines</affiliation></author>
      <pages>1928-1942</pages>
      <abstract>Reasoning about subjective natural language descriptions, such as opinions and preferences, is a challenging topic that largely remains unsolved to date. In particular, state-of-the-art large language models (LLMs) perform disappointingly in this task, show strong biases, and do not meet the interpretability requirements often needed in these kinds of applications. We propose a novel approach for reasoning about subjective knowledge that integrates potential and implicit meanings and explicitly models the relational nature of the information. We apply supervised graph learning, offer explanations for the model’s reasoning, and show that our model performs well across all 15 topics of OpinionQA, outperforming several prominent LLMs. Our detailed analysis further shows its unique advantages and the complementary nature it offers in comparison to LLMs.</abstract>
      <url hash="36fe3b2d">2024.findings-acl.115</url>
      <bibkey>hwang-etal-2024-graph</bibkey>
      <doi>10.18653/v1/2024.findings-acl.115</doi>
    </paper>
    <paper id="116">
      <title><fixed-case>M</fixed-case>ol<fixed-case>TC</fixed-case>: Towards Molecular Relational Modeling In Language Models</title>
      <author><first>Junfeng</first><last>Fang</last></author>
      <author><first>Shuai</first><last>Zhang</last></author>
      <author><first>Chang</first><last>Wu</last></author>
      <author><first>Zhengyi</first><last>Yang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Sihang</first><last>Li</last></author>
      <author><first>Kun</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Wenjie</first><last>Du</last></author>
      <author><first>Xiang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>1943-1958</pages>
      <abstract>Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the issue of insufficient data exploitation, as it hinders the sharing of interaction mechanism learned across various datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for molecular interaction modeling following Chain-of-Thought (CoT) theory, termed MolTC, which effectively integrate graphical information of two molecules in pair. To train this integrated framework efficiently, we introduce a *multi-hierarchical CoT theory* to refine its training paradigm, and conduct a comprehensive *Molecular Interactive Instructions* dataset for the development of biochemical LLMs involving MRL.Our experiments,conducted across various datasets involving over 4,000,000 molecular pairs, exhibit the superiority of our method over current GNN and LLM-based baselines. Code is available at https://github.com/MangoKiller/MolTC.</abstract>
      <url hash="c6ad4349">2024.findings-acl.116</url>
      <bibkey>fang-etal-2024-moltc</bibkey>
      <doi>10.18653/v1/2024.findings-acl.116</doi>
    </paper>
    <paper id="117">
      <title><fixed-case>KPE</fixed-case>val: Towards Fine-Grained Semantic-Based Keyphrase Evaluation</title>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>1959-1981</pages>
      <abstract>Despite the significant advancements in keyphrase extraction and keyphrase generation methods, the predominant approach for evaluation mainly relies on exact matching with human references. This scheme fails to recognize systems that generate keyphrases semantically equivalent to the references or diverse keyphrases that carry practical utility. To better assess the capability of keyphrase systems, we propose KPEval, a comprehensive evaluation framework consisting of four critical aspects: reference agreement, faithfulness, diversity, and utility. For each aspect, we design semantic-based metrics to reflect the evaluation objectives. Meta-evaluation studies demonstrate that our evaluation strategy correlates better with human preferences compared to a range of previously proposed metrics. Using KPEval, we re-evaluate 23 keyphrase systems and discover that (1) established model comparison results have blind-spots especially when considering reference-free evaluation; (2) large language models are underestimated by prior evaluation works; and (3) there is no single best model that can excel in all the aspects.</abstract>
      <url hash="ba30f8f8">2024.findings-acl.117</url>
      <bibkey>wu-etal-2024-kpeval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.117</doi>
    </paper>
    <paper id="118">
      <title>Learning Low-dimensional Multi-domain Knowledge Graph Embedding via Dual Archimedean Spirals</title>
      <author><first>Jiang</first><last>Li</last></author>
      <author><first>Xiangdong</first><last>Su</last><affiliation>Inner Mongolia University</affiliation></author>
      <author><first>Fujun</first><last>Zhang</last></author>
      <author><first>Guanglai</first><last>Gao</last><affiliation>Inner Mongolia University</affiliation></author>
      <pages>1982-1994</pages>
      <abstract>Knowledge graph embedding (KGE) is extensively employed for link prediction by representing entities and relations as low-dimensional vectors. In real-world scenarios, knowledge graphs (KGs) usually encompass diverse domains, which poses challenges to KG representations. However, existing KGE methods rarely make domain constraints on the embedding distribution of multi-domain KGs, leading to the embedding overlapping of different domains and performance degradation of link prediction. To address this challenge, we propose <b>Du</b>al <b>A</b>rchimedean <b>S</b>piral Knowledge Graph <b>E</b>mbedding (DuASE), a low-dimensional KGE model for multi-domain KGs. DuASE is inspired by our discovery that relation types can distinguish entities from different domains. Specifically, DuASE encodes entities with the same relation on the same Archimedean spiral, allowing it to differentiate the entities from different domains. To avoid embedding overlapping across domains, DuASE further makes the head and the tail spirals in the same triplet cluster to their respective domain space by a regularization function. Thus, DuASE can better capture the domain information and the dependencies between entities when modeling the multi-domain KGs, leading to improved KG representations. We validate the effectiveness of DuASE on the novel multi-domain dataset (<tex-math>n</tex-math>-MDKG) introduced in this study and three other benchmark datasets.</abstract>
      <url hash="3fd62358">2024.findings-acl.118</url>
      <bibkey>li-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.118</doi>
    </paper>
    <paper id="119">
      <title><fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Meets Dropout under a Unified Framework</title>
      <author><first>Sheng</first><last>Wang</last></author>
      <author><first>Liheng</first><last>Chen</last></author>
      <author><first>Jiyue</first><last>Jiang</last></author>
      <author><first>Boyang</first><last>Xue</last></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Chuan</first><last>Wu</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>1995-2008</pages>
      <abstract>With the remarkable capabilities, large language models (LLMs) have emergedas essential elements in numerous NLP applications, while parameter-efficientfinetuning, especially LoRA, has gained popularity as a lightweight approachfor model customization. Meanwhile, various dropout methods, initially designedfor full finetuning with all the parameters updated, alleviates overfittingassociated with excessive parameter redundancy. Hence, a possible contradictionarises from negligible trainable parameters of LoRA and the effectiveness ofprevious dropout methods, which has been largely overlooked. To fill this gap,we first confirm that parameter-efficient LoRA is also overfitting-prone. Wethen revisit transformer-specific dropout methods, and establish theirequivalence and distinctions mathematically and empirically. Building upon thiscomparative analysis, we introduce a unified framework for a comprehensiveinvestigation, which instantiates these methods based on dropping position,structural pattern and compensation measure. Through this framework, we revealthe new preferences and performance comparisons of them when involved withlimited trainable parameters. This framework also allows us to amalgamate themost favorable aspects into a novel dropout method named HiddenKey. Extensiveexperiments verify the remarkable superiority and sufficiency of HiddenKeyacross multiple models and tasks, which highlights it as the preferred approachfor high-performance and parameter-efficient finetuning of LLMs.</abstract>
      <url hash="48916584">2024.findings-acl.119</url>
      <bibkey>wang-etal-2024-lora</bibkey>
      <doi>10.18653/v1/2024.findings-acl.119</doi>
    </paper>
    <paper id="120">
      <title>Enhancing Text-to-<fixed-case>SQL</fixed-case> Parsing through Question Rewriting and Execution-Guided Refinement</title>
      <author><first>Wenxin</first><last>Mao</last></author>
      <author><first>Ruiqi</first><last>Wang</last></author>
      <author><first>Jiyu</first><last>Guo</last></author>
      <author><first>Jichuan</first><last>Zeng</last></author>
      <author><first>Cuiyun</first><last>Gao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Peiyi</first><last>Han</last><affiliation>Harbin Institute of Technology(ShenZhen)</affiliation></author>
      <author><first>Chuanyi</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>2009-2024</pages>
      <abstract>Large Language Model (LLM)-based approach has become the mainstream for Text-to-SQL task and achieves remarkable performance. In this paper, we augment the existing prompt engineering methods by exploiting the database content and execution feedback. Specifically, we introduce DART-SQL, which comprises two key components: (1) Question Rewriting: DART-SQL rewrites natural language questions by leveraging database content information to eliminate ambiguity. (2) Execution-Guided Refinement: DART-SQL incorporates database content information and utilizes the execution results of the generated SQL to iteratively refine the SQL. We apply this framework to the two LLM-based approaches (DAIL-SQL and C3) and test it on four widely used benchmarks (Spider-dev, Spider-test, Realistic and DK). Experiments show that our framework for DAIL-SQL and C3 achieves an average improvement of 12.41% and 5.38%, respectively, in terms of execution accuracy(EX) metric.</abstract>
      <url hash="dac55caf">2024.findings-acl.120</url>
      <bibkey>mao-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.120</doi>
    </paper>
    <paper id="121">
      <title>The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models</title>
      <author><first>Shuo</first><last>Zhang</last></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Junzhou</first><last>Zhao</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>2025-2038</pages>
      <abstract>Large language models often necessitate grounding on external knowledge to generate faithful and reliable answers. Yet even with the correct groundings in the reference, they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings. In this work, we formulate this knowledge alignment problem and introduce MixAlign, a framework that interacts with both the human user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic knowledge alignment and, if necessary, further enhances this alignment through human user clarifications. Experimental results highlight the crucial role of knowledge alignment in boosting model performance and mitigating hallucination, with improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the effectiveness of MixAlign in improving knowledge alignment by producing high-quality, user-centered clarifications.</abstract>
      <url hash="86892faa">2024.findings-acl.121</url>
      <bibkey>zhang-etal-2024-knowledge-alignment</bibkey>
      <doi>10.18653/v1/2024.findings-acl.121</doi>
    </paper>
    <paper id="122">
      <title><fixed-case>C</fixed-case>hat<fixed-case>KBQA</fixed-case>: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models</title>
      <author><first>Haoran</first><last>Luo</last></author>
      <author><first>Haihong</first><last>E</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <author><first>Zichen</first><last>Tang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Shiyao</first><last>Peng</last></author>
      <author><first>Yikai</first><last>Guo</last></author>
      <author><first>Wentai</first><last>Zhang</last></author>
      <author><first>Chenghao</first><last>Ma</last></author>
      <author><first>Guanting</first><last>Dong</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Meina</first><last>Song</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Wei</first><last>Lin</last></author>
      <author><first>Yifan</first><last>Zhu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>2039-2056</pages>
      <abstract>Knowledge Base Question Answering (KBQA) aims to answer natural language questions over large-scale knowledge bases (KBs), which can be summarized into two crucial steps: knowledge retrieval and semantic parsing. However, three core challenges remain: inefficient knowledge retrieval, mistakes of retrieval adversely impacting semantic parsing, and the complexity of previous KBQA methods. To tackle these challenges, we introduce ChatKBQA, a novel and simple generate-then-retrieve KBQA framework, which proposes first generating the logical form with fine-tuned LLMs, then retrieving and replacing entities and relations with an unsupervised retrieval method, to improve both generation and retrieval more directly. Experimental results show that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This work can also be regarded as a new paradigm for combining LLMs with knowledge graphs (KGs) for interpretable and knowledge-required question answering.</abstract>
      <url hash="12777d3a">2024.findings-acl.122</url>
      <bibkey>luo-etal-2024-chatkbqa</bibkey>
      <doi>10.18653/v1/2024.findings-acl.122</doi>
    </paper>
    <paper id="123">
      <title>Achilles-Bench: A Challenging Benchmark for Low-Resource Evaluation</title>
      <author><first>Yudong</first><last>Wang</last></author>
      <author><first>Chang</first><last>Ma</last></author>
      <author><first>Qingxiu</first><last>Dong</last></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <author><first>Jingjing</first><last>Xu</last></author>
      <pages>2057-2080</pages>
      <abstract>With promising yet saturated results in high-resource settings, low-resource datasets have gradually become crucial benchmarks (e.g., BigBench Hard, superGLUE) for evaluating the learning ability of advanced neural networks. In this work, we find that there exists a set of “hard examples” in low-resource settings that challenge neural networks but are not well evaluated, which causes over-estimated performance. We first give a theoretical analysis on which factors bring the difficulty of low-resource learning. It then motivates us to propose a challenging benchmark Achilles-Bench to better evaluate the learning ability, which covers 11 datasets, including 8 natural language process (NLP) datasets and 3 computer vision (CV) datasets. Experiments on a wide range of models show that neural networks, even pre-trained language models, have sharp performance drops on our benchmark, demonstrating the effectiveness of evaluating the weaknesses of neural networks. On NLP tasks, we surprisingly find that despite better results on traditional low-resource benchmarks, pre-trained networks, does not show performance improvements on our benchmarks. there is still a large robustness gap between existing models and human-level performance, highlighting the need for robust low-resource learning models.</abstract>
      <url hash="90e0827c">2024.findings-acl.123</url>
      <bibkey>wang-etal-2024-achilles</bibkey>
      <doi>10.18653/v1/2024.findings-acl.123</doi>
    </paper>
    <paper id="124">
      <title><fixed-case>INTERVENOR</fixed-case>: Prompting the Coding Ability of Large Language Models with the Interactive Chain of Repair</title>
      <author><first>Hanbin</first><last>Wang</last></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Ganqu</first><last>Cui</last></author>
      <author><first>Ning</first><last>Ding</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ge</first><last>Yu</last></author>
      <pages>2081-2107</pages>
      <abstract>This paper introduces INTERVENOR (INTERactiVE chaiN Of Repair), a system designed to emulate the interactive code repair processes observed in humans, encompassing both code diagnosis and code repair. INTERVENOR prompts Large Language Models (LLMs) to play distinct roles during the code repair process, functioning as both a Code Learner and a Code Teacher. Specifically, the Code Learner is tasked with adhering to instructions to generate or repair code, while the Code Teacher is responsible for crafting a Chain-of-Repair (CoR) to serve as guidance for the Code Learner. During generating the CoR, the Code Teacher needs to check the generated codes from Code Learner and reassess how to address code bugs based on error feedback received from compilers. Experimental results demonstrate that INTERVENOR surpasses baseline models, exhibiting improvements of approximately 18% and 4.3% over GPT-3.5 in code generation and code translation tasks, respectively. Our further analyses show that CoR is effective to illuminate the reasons behind bugs and outline solution plans in natural language. With the feedback of code compilers, INTERVENOR can accurately identify syntax errors and assertion errors and provide precise instructions to repair codes. All data and codes are available at [https://github.com/NEUIR/INTERVENOR](https://github.com/NEUIR/INTERVENOR).</abstract>
      <url hash="a781ce3e">2024.findings-acl.124</url>
      <bibkey>wang-etal-2024-intervenor</bibkey>
      <doi>10.18653/v1/2024.findings-acl.124</doi>
    </paper>
    <paper id="125">
      <title><fixed-case>S</fixed-case>ocial<fixed-case>B</fixed-case>ench: Sociality Evaluation of Role-Playing Conversational Agents</title>
      <author><first>Hongzhan</first><last>Chen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Hehong</first><last>Chen</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Wenshen</first><last>Xu</last></author>
      <author><first>Gao</first><last>Xing</last></author>
      <author><first>Weizhou</first><last>Shen</last></author>
      <author><first>Xiaojun</first><last>Quan</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Chenliang</first><last>Li</last></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>2108-2126</pages>
      <abstract>Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge and style of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce SocialBench, the first benchmark designed to systematically evaluate the sociality of role-playing agents at both individual and group levels of social interactions. SocialBench is constructed from various sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream LLMs. We find that agents excelling in individual level does not imply their proficiency in group level. Experimental results on SocialBench confirm its significance as a testbed for assessing the social interaction of role-playing agents. The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract.</abstract>
      <url hash="aa646ecc">2024.findings-acl.125</url>
      <bibkey>chen-etal-2024-socialbench</bibkey>
      <doi>10.18653/v1/2024.findings-acl.125</doi>
    </paper>
    <paper id="126">
      <title>From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in <fixed-case>LLM</fixed-case>s-based Applications</title>
      <author><first>Yongqiang</first><last>Ma</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Lizhi</first><last>Qing</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiawei</first><last>Liu</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Yangyang</first><last>Kang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wei</first><last>Lu</last></author>
      <author><first>Xiaozhong</first><last>Liu</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <author><first>Qikai</first><last>Cheng</last></author>
      <pages>2127-2137</pages>
      <abstract>Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications. Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications. Our proposed metric, termed “Revision Distance,” utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score. Our results show that for the easy-writing task, “Revision Distance” is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts. Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.</abstract>
      <url hash="c0a63db2">2024.findings-acl.126</url>
      <bibkey>ma-etal-2024-model</bibkey>
      <doi>10.18653/v1/2024.findings-acl.126</doi>
    </paper>
    <paper id="127">
      <title>Context-Aware Tracking and Dynamic Introduction for Incomplete Utterance Rewriting in Extended Multi-Turn Dialogues</title>
      <author><first>Xinnan</first><last>Guo</last></author>
      <author><first>Qian</first><last>Zhu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Qiuhui</first><last>Shi</last></author>
      <author><first>Xuan</first><last>Lin</last><affiliation>Ant Group</affiliation></author>
      <author><first>Liubin</first><last>Wang</last></author>
      <author><first>DaqianLi</first><last>DaqianLi</last></author>
      <author><first>Yongrui</first><last>Chen</last></author>
      <pages>2138-2148</pages>
      <abstract>Incomplete utterance rewriting (IUR) aims to reconstruct the utterance with omitted information and pronouns to be standalone and complete based on the context. The existing works predominantly focus on simple ellipsis and coreference problems in brief multi-turn dialogues. But in actual scenarios: 1) the context of the dialogues frequently comprises multiple similar candidates for ellipsis and coreference resolution, pouring to confuse. 2) the number of turns tends to be more extensive, while the content with various topics also grows more complex. This paper proposes a novel method called CaT to address these issues. In particular, we first devise a tacker model, distilled from GPT4-turbo, to adopt Context Tracking that dynamically updates a list of key phrases turn by turn, as accurate candidates for ellipsis and coreference resolution. Second, we further present the Dynamic Context Introduction mechanism to filter irrelevant preceding contexts that are not relied on by any element within the key phrase list to condense extended dialogues. Comprehensive experiments indicate that our solution provides a significant improvement over the existing baselines, and achieves state-of-the-art on three benchmarks.</abstract>
      <url hash="d6f4a987">2024.findings-acl.127</url>
      <bibkey>guo-etal-2024-context</bibkey>
      <doi>10.18653/v1/2024.findings-acl.127</doi>
    </paper>
    <paper id="128">
      <title><fixed-case>E</fixed-case>motion<fixed-case>Q</fixed-case>ueen: A Benchmark for Evaluating Empathy of Large Language Models</title>
      <author><first>Yuyan</first><last>Chen</last></author>
      <author><first>Songzhou</first><last>Yan</last></author>
      <author><first>Sijia</first><last>Liu</last></author>
      <author><first>Yueze</first><last>Li</last></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>2149-2176</pages>
      <abstract>Emotional intelligence in large language models (LLMs) is of great importance in Natural Language Processing. However, the previous research mainly focus on basic sentiment analysis tasks, such as emotion recognition, which is not enough to evaluate LLMs’ overall emotional intelligence. Therefore, this paper presents a novel framework named EmotionQueen for evaluating the emotional intelligence of LLMs. The framework includes four distinctive tasks: Key Event Recognition, Mixed Event Recognition, Implicit Emotional Recognition, and Intention Recognition. LLMs are requested to recognize important event or implicit emotions and generate empathetic response.We also design two metrics to evaluate LLMs’ capabilities in recognition and response for emotion-related statements. Experiments yield significant conclusions about LLMs’ capabilities and limitations in emotion intelligence.</abstract>
      <url hash="718bc439">2024.findings-acl.128</url>
      <bibkey>chen-etal-2024-emotionqueen</bibkey>
      <doi>10.18653/v1/2024.findings-acl.128</doi>
    </paper>
    <paper id="129">
      <title>Plum: Prompt Learning using Metaheuristics</title>
      <author><first>Rui</first><last>Pan</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shuo</first><last>Xing</last><affiliation>Texas A&amp;M University - College Station</affiliation></author>
      <author><first>Shizhe</first><last>Diao</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Wenhe</first><last>Sun</last></author>
      <author><first>Xiang</first><last>Liu</last></author>
      <author><first>KaShun</first><last>Shum</last></author>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Renjie</first><last>Pi</last></author>
      <author><first>Tong</first><last>Zhang</last><affiliation>UIUC</affiliation></author>
      <pages>2177-2197</pages>
      <abstract>Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly “general”, i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-box and black-box prompt learning. Furthermore, we show that these methods can be used to discover more human-understandable prompts that were previously unknown in both reasoning and image generation tasks, opening the door to a cornucopia of possibilities in prompt optimization.</abstract>
      <url hash="9a1fa691">2024.findings-acl.129</url>
      <bibkey>pan-etal-2024-plum</bibkey>
      <doi>10.18653/v1/2024.findings-acl.129</doi>
    </paper>
    <paper id="130">
      <title><fixed-case>HOTVCOM</fixed-case>: Generating Buzzworthy Comments for Videos</title>
      <author><first>Yuyan</first><last>Chen</last></author>
      <author><first>Songzhou</first><last>Yan</last></author>
      <author><first>Qingpei</first><last>Guo</last><affiliation>Ant Group</affiliation></author>
      <author><first>Jiyuan</first><last>Jia</last><affiliation>southern university of science and technology</affiliation></author>
      <author><first>Zhixu</first><last>Li</last></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>2198-2224</pages>
      <abstract>In the era of social media video platforms, popular “hot-comments” play a crucial role in attracting user impressions of short-form videos, making them vital for marketing and branding purpose. However, existing research predominantly focuses on generating descriptive comments or “danmaku” in English, offering immediate reactions to specific video moments. Addressing this gap, our study introduces HOTVCOM, the largest Chinese video hot-comment dataset, comprising 94k diverse videos and 137 million comments. We also present the ComHeat framework, which synergistically integrates visual, auditory, and textual data to generate influential hot-comments on the Chinese video dataset. Empirical evaluations highlight the effectiveness of our framework, demonstrating its excellence on both the newly constructed and existing datasets.</abstract>
      <url hash="ea351c3c">2024.findings-acl.130</url>
      <bibkey>chen-etal-2024-hotvcom</bibkey>
      <doi>10.18653/v1/2024.findings-acl.130</doi>
    </paper>
    <paper id="131">
      <title>Do Large Language Models have Problem-Solving Capability under Incomplete Information Scenarios?</title>
      <author><first>Yuyan</first><last>Chen</last></author>
      <author><first>Yueze</first><last>Li</last></author>
      <author><first>Songzhou</first><last>Yan</last></author>
      <author><first>Sijia</first><last>Liu</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>2225-2238</pages>
      <abstract>The evaluation of the problem-solving capability under incomplete information scenarios of Large Language Models (LLMs) is increasingly important, encompassing capabilities such as questioning, knowledge search, error detection, and path planning. Current research mainly focus on LLMs’ problem-solving capability such as “Twenty Questions”.However, these kinds of games do not require recognizing misleading cues which are necessary in the incomplete information scenario.Moreover, the existing game such as “Who is undercover” are highly subjective, making it challenging for evaluation.Therefore, in this paper, we introduce a novel game named BrainKing based on the “Who is undercover” and “Twenty Questions” for evaluating LLM capabilities under incomplete information scenarios. It requires LLMs to identify target entities with limited yes-or-no questions and potential misleading answers. By setting up easy, medium, and hard difficulty modes, we comprehensively assess the performance of LLMs across various aspects. Our results reveal the capabilities and limitations of LLMs in BrainKing, providing significant insights of LLM problem-solving levels.</abstract>
      <url hash="4b3873a0">2024.findings-acl.131</url>
      <bibkey>chen-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.131</doi>
    </paper>
    <paper id="132">
      <title>Distilling Robustness into Natural Language Inference Models with Domain-Targeted Augmentation</title>
      <author><first>Joe</first><last>Stacey</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Marek</first><last>Rei</last><affiliation>Imperial College London</affiliation></author>
      <pages>2239-2258</pages>
      <abstract>Knowledge distillation optimises a smaller student model to behave similarly to a larger teacher model, retaining some of the performance benefits. While this method can improve results on in-distribution examples, it does not necessarily generalise to out-of-distribution (OOD) settings. We investigate two complementary methods for improving the robustness of the resulting student models on OOD domains. The first approach augments the distillation with generated unlabeled examples that match the target distribution. The second method upsamples data points among the training set that are similar to the target distribution. When applied on the task of natural language inference (NLI), our experiments on MNLI show that distillation with these modifications outperforms previous robustness solutions. We also find that these methods improve performance on OOD domains even beyond the target domain.</abstract>
      <url hash="2f1ba68b">2024.findings-acl.132</url>
      <bibkey>stacey-rei-2024-distilling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.132</doi>
    </paper>
    <paper id="133">
      <title>Into the Unknown: Generating Geospatial Descriptions for New Environments</title>
      <author><first>Tzuf</first><last>Paz-Argaman</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>John</first><last>Palowitch</last><affiliation>Google</affiliation></author>
      <author><first>Sayali</first><last>Kulkarni</last><affiliation>Research, Google and Google</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <author><first>Jason</first><last>Baldridge</last><affiliation>Google</affiliation></author>
      <pages>2259-2273</pages>
      <abstract>Similar to vision-and-language navigation (VLN) tasks that focus on bridging the gap between vision and language for embodied navigation, the new Rendezvous (RVS) task requires reasoning over allocentric spatial relationships using non-sequential navigation instructions and maps. However, performance substantially drops in new environments with no training data.Using opensource descriptions paired with coordinates (e.g., Wikipedia) provides training data but suffers from limited spatially-oriented text resulting in low geolocation resolution. We propose a large-scale augmentation method for generating high-quality synthetic data for new environments using readily available geospatial data. Our method constructs a grounded knowledge-graph, capturing entity relationships. Sampled entities and relations (“shop north of school”) generate navigation instructions via (i) generating numerous templates using context-free grammar (CFG) to embed specific entities and relations; (ii) feeding the entities and relation into a large language model (LLM) for instruction generation. A comprehensive evaluation on RVS, showed that our approach improves the 100-meter accuracy by 45.83% on unseen environments. Furthermore, we demonstrate that models trained with CFG-based augmentation achieve superior performance compared with those trained with LLM-based augmentation, both in unseen and seen environments. These findings suggest that the potential advantages of explicitly structuring spatial information for text-based geospatial reasoning in previously unknown, can unlock data-scarce scenarios.</abstract>
      <url hash="5a6521f3">2024.findings-acl.133</url>
      <bibkey>paz-argaman-etal-2024-unknown</bibkey>
      <doi>10.18653/v1/2024.findings-acl.133</doi>
    </paper>
    <paper id="134">
      <title>Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance</title>
      <author><first>Omer</first><last>Goldman</last><affiliation>Bar Ilan University</affiliation></author>
      <author><first>Avi</first><last>Caciularu</last><affiliation>Google</affiliation></author>
      <author><first>Matan</first><last>Eyal</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Kris</first><last>Cao</last><affiliation>DeepMind</affiliation></author>
      <author><first>Idan</first><last>Szpektor</last><affiliation>Google</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>2274-2286</pages>
      <abstract>Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers’ compression and models’ downstream performance, suggesting that compression is a reliable intrinsic indicator of tokenization quality. These correlations are more pronounced for generation tasks (over classification) or for smaller models (over large ones). We replicated a representative part of our experiments on Turkish and found similar results, confirming that our results hold for languages with typological characteristics dissimilar to English. We conclude that building better compressing tokenizers is a fruitful avenue for further research and for improving overall model performance.</abstract>
      <url hash="20ee1a43">2024.findings-acl.134</url>
      <bibkey>goldman-etal-2024-unpacking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.134</doi>
    </paper>
    <paper id="135">
      <title>Length-aware Byte Pair Encoding for Mitigating Over-segmentation in <fixed-case>K</fixed-case>orean Machine Translation</title>
      <author><first>Jungseob</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyeonseok</first><last>Moon</last><affiliation>Korea University</affiliation></author>
      <author><first>Seungjun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>Sugyeong</first><last>Eo</last><affiliation>Korea University</affiliation></author>
      <author><first>Hyunwoong</first><last>Ko</last></author>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Seungyoon</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>2287-2303</pages>
      <abstract>Byte Pair Encoding is an effective approach in machine translation across several languages. However, our analysis indicates that BPE is prone to over-segmentation in the morphologically rich language, Korean, which can erode word semantics and lead to semantic confusion during training. This semantic confusion, stemming from over-segmentation, ultimately contributes to a degradation of overall translation quality. To address this issue, we introduce Length-aware Subword Vocabulary Construction (LeVoC), a novel approach strategically incorporating longer words into the vocabulary. By utilizing an external monolingual Korean corpus, LeVoC extracts and integrates long words, effectively preserving morphological information and reducing semantic confusion. Our experiments demonstrate that LeVoC not only significantly outperforms BPE, but also can be applied to and surpass current state-of-the-art morpheme-aware subword tokenization methods. We provide evidence that the difficulty in translating sentences with long words in Korean is associated with morphological compositionality, and LeVoC’s ability to reduce semantic confusion during training leads to improved translation quality.</abstract>
      <url hash="63786cdb">2024.findings-acl.135</url>
      <bibkey>lee-etal-2024-length</bibkey>
      <doi>10.18653/v1/2024.findings-acl.135</doi>
    </paper>
    <paper id="136">
      <title>Multilingual Instruction Tuning With Just a Pinch of Multilinguality</title>
      <author><first>Uri</first><last>Shaham</last><affiliation>Research, Google and Tel Aviv University</affiliation></author>
      <author><first>Jonathan</first><last>Herzig</last><affiliation>Research, Google</affiliation></author>
      <author><first>Roee</first><last>Aharoni</last><affiliation>Google</affiliation></author>
      <author><first>Idan</first><last>Szpektor</last><affiliation>Google</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <author><first>Matan</first><last>Eyal</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>2304-2317</pages>
      <abstract>As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages from the pre-training corpus. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples integrated in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in multiple languages compared to monolingually tuned models, despite training on 10x fewer examples in those languages. Finally, we find that diversifying the instruction tuning set with even just 2-4 languages significantly improves cross-lingual generalization. Our results suggest that building massively multilingual instruction-tuned models can be done with only a very small set of multilingual instruction-responses.</abstract>
      <url hash="fb52f2ad">2024.findings-acl.136</url>
      <bibkey>shaham-etal-2024-multilingual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.136</doi>
    </paper>
    <paper id="137">
      <title><fixed-case>M</fixed-case>3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</title>
      <author><first>Jianlyu</first><last>Chen</last></author>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Peitian</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Luo</last></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <pages>2318-2335</pages>
      <abstract>In this paper, we introduce a new embedding model called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3-Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross-lingual, and long-document retrieval benchmarks.</abstract>
      <url hash="cde5a7c2">2024.findings-acl.137</url>
      <bibkey>chen-etal-2024-m3</bibkey>
      <doi>10.18653/v1/2024.findings-acl.137</doi>
    </paper>
    <paper id="138">
      <title>Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback</title>
      <author><first>Zhangqian</first><last>Bi</last></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Zheng</first><last>Wang</last><affiliation>University of Leeds</affiliation></author>
      <author><first>Hongyu</first><last>Zhang</last><affiliation>University of Newcastle, Australia</affiliation></author>
      <author><first>Batu</first><last>Guan</last></author>
      <author><first>Fangxin</first><last>Lu</last></author>
      <author><first>Zili</first><last>Zhang</last></author>
      <author><first>Yulei</first><last>Sui</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Hai</first><last>Jin</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Xuanhua</first><last>Shi</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>2336-2353</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable progress in automated code generation. Yet, LLM-generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. We present CoCoGen, a new code generation approach that uses compiler feedback to improve the LLM-generated code. CoCoGen first leverages static analysis to identify mismatches between the generated code and the project’s context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on the project context and consistently outperforms the existing retrieval-based code generation baselines.</abstract>
      <url hash="40013528">2024.findings-acl.138</url>
      <bibkey>bi-etal-2024-iterative</bibkey>
      <doi>10.18653/v1/2024.findings-acl.138</doi>
    </paper>
    <paper id="139">
      <title>An Element is Worth a Thousand Words: Enhancing Legal Case Retrieval by Incorporating Legal Elements</title>
      <author><first>Chenlong</first><last>Deng</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Zhicheng</first><last>Dou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yujia</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Peitian</first><last>Zhang</last></author>
      <author><first>Kelong</first><last>Mao</last></author>
      <pages>2354-2365</pages>
      <abstract>Legal case retrieval plays an important role in promoting judicial justice and fairness. One of its greatest challenges is that the definition of relevance goes far beyond the common semantic relevance as in ad-hoc retrieval. In this paper, we reveal that the legal elements, which typically comprise key facts in a specialized legal context, can largely improve the relevance matching of legal case retrieval. To facilitate the use of legal elements, we construct a Chinese legal element dataset called LeCaRD-Elem based on the widely-used LeCaRD dataset, through a two-stage semi-automatic method with a minimized reliance on human labor. Meanwhile, we introduce two new models to enhance legal search using legal elements. The first, Elem4LCR-E, is a two-stage model that explicitly predicts legal elements from texts and then leverages them for improved ranking. Recognizing the potential benefits of more seamless integration, we further propose an end-to-end model called Elem4LCR-I, which internalizes the legal element knowledge into its model parameters using a tailored teacher-student training framework. Extensive experiments underscore the significant value of legal elements and demonstrate the superiority of our two proposed models in enhancing legal search over existing methods.</abstract>
      <url hash="4915df2c">2024.findings-acl.139</url>
      <bibkey>deng-etal-2024-element</bibkey>
      <doi>10.18653/v1/2024.findings-acl.139</doi>
    </paper>
    <paper id="140">
      <title><fixed-case>S</fixed-case>o<fixed-case>M</fixed-case>e<fixed-case>LVLM</fixed-case>: A Large Vision Language Model for Social Media Processing</title>
      <author><first>Xinnong</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Haoyu</first><last>Kuang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xinyi</first><last>Mou</last></author>
      <author><first>Hanjia</first><last>Lyu</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Kun</first><last>Wu</last></author>
      <author><first>Siming</first><last>Chen</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiebo</first><last>Luo</last><affiliation>University of Rochester and University of Rochester</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>2366-2389</pages>
      <abstract>The growth of social media, characterized by its multimodal nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks. The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed prompting methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks. In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge &amp; comprehension, application, analysis, evaluation, and creation. SoMeLVLM is designed to understand and generate realistic social media behavior. We have developed a 654k multimodal social media instruction-tuning dataset to support our cognitive framework and fine-tune our model. Our experiments demonstrate that SoMeLVLM achieves state-of-the-art performance in multiple social media tasks. Further analysis shows its significant advantages over baselines in terms of cognitive abilities.</abstract>
      <url hash="0760544b">2024.findings-acl.140</url>
      <bibkey>zhang-etal-2024-somelvlm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.140</doi>
    </paper>
    <paper id="141">
      <title><fixed-case>K</fixed-case>o<fixed-case>C</fixed-case>ommon<fixed-case>GEN</fixed-case> v2: A Benchmark for Navigating <fixed-case>K</fixed-case>orean Commonsense Reasoning Challenges in Large Language Models</title>
      <author><first>Jaehyung</first><last>Seo</last></author>
      <author><first>Jaewook</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Chanjun</first><last>Park</last><affiliation>Upstage</affiliation></author>
      <author><first>SeongTae</first><last>Hong</last><affiliation>Korea University</affiliation></author>
      <author><first>Seungjun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>2390-2415</pages>
      <abstract>The evolution of large language models (LLMs) has culminated in a multitask model paradigm where prompts drive the generation of user-specific outputs. However, this advancement has revealed a critical challenge: LLMs frequently produce outputs against socially acceptable commonsense standards in various scenarios. To address this gap in commonsense reasoning, we present KoCommonGEN v2, a fine-grained benchmark dataset focused on Korean commonsense reasoning. This dataset, enriched with human annotations, comprises multiple-choice questions across seven error categories. These categories include commonsense memorization, numerical commonsense, toxic speech, and more, which are vulnerable to undermining the reliability of LLMs’ commonsense reasoning capabilities. The empirical results present that LLMs struggle with Korean commonsense reasoning. With human accuracy benchmarked at approximately 85%, GPT-4’s performance lags at about 74%, and other LLMs demonstrate an average accuracy of around 42%. Our findings emphasize the need for targeted improvements in Korean commonsense reasoning within LLMs, paving the way for more socially and contextually sensitive AI models.</abstract>
      <url hash="8993de68">2024.findings-acl.141</url>
      <bibkey>seo-etal-2024-kocommongen</bibkey>
      <doi>10.18653/v1/2024.findings-acl.141</doi>
    </paper>
    <paper id="142">
      <title><fixed-case>N</fixed-case>euro<fixed-case>P</fixed-case>rune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models</title>
      <author><first>Amit</first><last>Dhurandhar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Tejaswini</first><last>Pedapati</last></author>
      <author><first>Ronny</first><last>Luss</last><affiliation>IBM</affiliation></author>
      <author><first>Soham</first><last>Dan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Aurelie</first><last>Lozano</last><affiliation>IBM Research</affiliation></author>
      <author><first>Payel</first><last>Das</last></author>
      <author><first>Georgios</first><last>Kollias</last><affiliation>International Business Machines</affiliation></author>
      <pages>2416-2430</pages>
      <abstract>Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite our sole objective not being optimizing performance. NeuroPrune is competitive with (or sometimes superior to) baselines on performance and can be up to 10x faster in terms of training time for a given level of sparsity, simultaneously exhibiting measurable improvements in inference time in many cases.</abstract>
      <url hash="9f28769e">2024.findings-acl.142</url>
      <bibkey>dhurandhar-etal-2024-neuroprune</bibkey>
      <doi>10.18653/v1/2024.findings-acl.142</doi>
    </paper>
    <paper id="143">
      <title>Ranking Large Language Models without Ground Truth</title>
      <author><first>Amit</first><last>Dhurandhar</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Rahul</first><last>Nair</last><affiliation>IBM Research Europe</affiliation></author>
      <author><first>Moninder</first><last>Singh</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Elizabeth</first><last>Daly</last><affiliation>IBM Research</affiliation></author>
      <author><first>Karthikeyan</first><last>Natesan Ramamurthy</last><affiliation>International Business Machines</affiliation></author>
      <pages>2431-2452</pages>
      <abstract>Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover true rankings without reference data. This points to a viable low-resource mechanism for practical use.</abstract>
      <url hash="9757ee04">2024.findings-acl.143</url>
      <bibkey>dhurandhar-etal-2024-ranking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.143</doi>
    </paper>
    <paper id="144">
      <title>Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback</title>
      <author><first>Chengfeng</first><last>Dou</last></author>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University and Peking University</affiliation></author>
      <author><first>Wenpin</first><last>Jiao</last><affiliation>Peking University</affiliation></author>
      <author><first>Haiyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Yongqiang</first><last>Zhao</last></author>
      <author><first>Zhengwei</first><last>Tao</last></author>
      <pages>2453-2473</pages>
      <abstract>The utilization of large language models for medical dialogue generation has attracted considerable attention due to its potential to enhance response richness and coherence. While previous studies have made strides in optimizing model performance, there is a pressing need to bolster the model’s capacity for diagnostic logic to ensure patient safety. In response to this need, we propose an approach termed preference learning from process feedback (PLPF), which involves integrating the doctor’s diagnostic logic into LLMs. PLPF encompasses three key components: rule modeling, preference data generation, and preference alignment. These components collectively serve to train the model to adhere to the diagnostic process. Our experimental results, utilizing Standardized Patient Testing, demonstrate that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, surpassing the performance of traditional approaches. Moreover, PLPF exhibits effectiveness in both multi-round and single-round dialogue tasks, thereby highlighting its potential in improving medical dialogue generation. Our dataset is available at https://github.com/Chengfeng-Dou/SpTesting.</abstract>
      <url hash="ee5c1d47">2024.findings-acl.144</url>
      <bibkey>dou-etal-2024-integrating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.144</doi>
    </paper>
    <paper id="145">
      <title><fixed-case>LM</fixed-case>-Cocktail: Resilient Tuning of Language Models via Model Merging</title>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Peitian</first><last>Zhang</last></author>
      <author><first>Xingrun</first><last>Xing</last></author>
      <pages>2474-2488</pages>
      <abstract>The pre-trained language models are continually fine-tuned to better support downstream applications. However, this operation may result in significant performance degeneration on general tasks beyond the targeted domain. To overcome this problem, we propose LM-Cocktail which enables the fine-tuned model to stay resilient in general perspectives. Our method is conducted in the form of model merging, where the fine-tuned language model is merged with the pre-trained base model or the peer models from other domains through weighted average. Despite simplicity, LM-Cocktail is surprisingly effective: the resulted model is able to achieve a strong empirical performance in the whole scope of general tasks while preserving a superior capacity in its targeted domain.</abstract>
      <url hash="a24a5d96">2024.findings-acl.145</url>
      <bibkey>xiao-etal-2024-lm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.145</doi>
    </paper>
    <paper id="146">
      <title>Episodic Memory Retrieval from <fixed-case>LLM</fixed-case>s: A Neuromorphic Mechanism to Generate Commonsense Counterfactuals for Relation Extraction</title>
      <author><first>Xin</first><last>Miao</last></author>
      <author id="yongqi-li-hk"><first>Yongqi</first><last>Li</last></author>
      <author><first>Shen</first><last>Zhou</last></author>
      <author><first>Tieyun</first><last>Qian</last><affiliation>Wuhan University</affiliation></author>
      <pages>2489-2511</pages>
      <abstract>Large language models (LLMs) have achieved satisfactory performance in counterfactual generation. However, confined by the stochastic generation process of LLMs, there often are misalignments between LLMs and humans which hinder LLMs from handling complex tasks like relation extraction. As a result, LLMs may generate commonsense-violated counterfactuals like ‘eggs were produced by a box’. To bridge this gap, we propose to mimick the episodic memory retrieval, the working mechanism of human hippocampus, to align LLMs’ generation process with that of humans. In this way, LLMs can derive experience from their extensive memory, which keeps in line with the way humans gain commonsense. We then implement two central functions in the hippocampus, i.e., pattern separation and pattern completion, to retrieve the episodic memory from LLMs and generate commonsense counterfactuals for relation extraction. Experimental results demonstrate the improvements of our framework over existing methods in terms of the quality of counterfactuals.</abstract>
      <url hash="53da2730">2024.findings-acl.146</url>
      <bibkey>miao-etal-2024-episodic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.146</doi>
    </paper>
    <paper id="147">
      <title><fixed-case>S</fixed-case>em<fixed-case>R</fixed-case>el2024: A Collection of Semantic Textual Relatedness Datasets for 13 Languages</title>
      <author><first>Nedjma</first><last>Ousidhoum</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Shamsuddeen</first><last>Muhammad</last><affiliation>Bayero University, Kano-Nigeria</affiliation></author>
      <author><first>Mohamed</first><last>Abdalla</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Idris</first><last>Abdulmumin</last><affiliation>Ahmadu Bello University</affiliation></author>
      <author><first>Ibrahim</first><last>Ahmad</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Sanchit</first><last>Ahuja</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Alham</first><last>Aji</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Amazon</affiliation></author>
      <author><first>Vladimir</first><last>Araujo</last><affiliation>KU Leuven</affiliation></author>
      <author><first>Abinew</first><last>Ayele</last><affiliation>Bahir Dar University, Universität Hamburg</affiliation></author>
      <author><first>Pavan</first><last>Baswani</last></author>
      <author><first>Meriem</first><last>Beloucif</last><affiliation>Uppsala University</affiliation></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <author><first>Sofia</first><last>Bourhim</last></author>
      <author><first>Christine</first><last>Kock</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Genet</first><last>Dekebo</last></author>
      <author><first>Oumaima</first><last>Hourrane</last></author>
      <author><first>Gopichand</first><last>Kanumolu</last></author>
      <author><first>Lokesh</first><last>Madasu</last></author>
      <author><first>Samuel</first><last>Rutunda</last></author>
      <author><first>Manish</first><last>Shrivastava</last><affiliation>International Institute of Information Technology Hyderabad, India</affiliation></author>
      <author><first>Thamar</first><last>Solorio</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of Houston</affiliation></author>
      <author><first>Nirmal</first><last>Surange</last><affiliation>International Institute of Information Technology Hyderabad</affiliation></author>
      <author><first>Hailegnaw</first><last>Tilaye</last><affiliation>Kotebe University of Education</affiliation></author>
      <author><first>Krishnapriya</first><last>Vishnubhotla</last></author>
      <author><first>Genta</first><last>Winata</last><affiliation>Capital One AI Foundations</affiliation></author>
      <author><first>Seid</first><last>Yimam</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Saif</first><last>Mohammad</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>2512-2530</pages>
      <abstract>Exploring and quantifying semantic relatedness is central to representing language and holds significant implications across various NLP tasks. While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present <i>SemRel</i>, a new semantic relatedness dataset collection annotated by native speakers across 13 languages: <i>Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Spanish,</i> and <i>Telugu</i>. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia – regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. The scores are obtained using a comparative annotation framework. We describe the data collection and annotation processes, challenges when building the datasets, baseline experiments, and their impact and utility in NLP.</abstract>
      <url hash="18c53f5f">2024.findings-acl.147</url>
      <bibkey>ousidhoum-etal-2024-semrel2024</bibkey>
      <doi>10.18653/v1/2024.findings-acl.147</doi>
    </paper>
    <paper id="148">
      <title>Alirector: Alignment-Enhanced <fixed-case>C</fixed-case>hinese Grammatical Error Corrector</title>
      <author><first>Haihui</first><last>Yang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Xiaojun</first><last>Quan</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <pages>2531-2546</pages>
      <abstract>Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model’s ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment knowledge from two alignment models to the correction model, instructing it on how to avoid overcorrection. Experimental results on three CGEC datasets demonstrate the effectiveness of our approach in alleviating overcorrection and improving overall performance. Our code has been made publicly available.</abstract>
      <url hash="a2ddd8ab">2024.findings-acl.148</url>
      <bibkey>yang-quan-2024-alirector</bibkey>
      <doi>10.18653/v1/2024.findings-acl.148</doi>
    </paper>
    <paper id="149">
      <title><fixed-case>VISP</fixed-case>ool: Enhancing Transformer Encoders with Vector Visibility Graph Neural Networks</title>
      <author><first>Tuna</first><last>Alikaşifoğlu</last></author>
      <author><first>Arda</first><last>Aras</last></author>
      <author><first>Aykut</first><last>Koc</last><affiliation>Bilkent University</affiliation></author>
      <pages>2547-2556</pages>
      <abstract>The emergence of transformers has revolutionized natural language processing (NLP), as evidenced in various NLP tasks. While graph neural networks (GNNs) show recent promise in NLP, they are not standalone replacements for transformers. Rather, recent research explores combining transformers and GNNs. Existing GNN-based approaches rely on static graph construction methods requiring excessive text processing, and most of them are not scalable with the increasing document and word counts. We address these limitations by proposing a novel dynamic graph construction method for text documents based on vector visibility graphs (VVGs) generated from transformer output. Then, we introduce visibility pooler (VISPool), a scalable model architecture that seamlessly integrates VVG convolutional networks into transformer pipelines. We evaluate the proposed model on the General Language Understanding Evaluation (GLUE) benchmark datasets. VISPool outperforms the baselines with less trainable parameters, demonstrating the viability of the visibility-based graph construction method for enhancing transformers with GNNs.</abstract>
      <url hash="325c77d1">2024.findings-acl.149</url>
      <bibkey>alikasifoglu-etal-2024-vispool</bibkey>
      <doi>10.18653/v1/2024.findings-acl.149</doi>
    </paper>
    <paper id="150">
      <title>The Emotion Dynamics of Literary Novels</title>
      <author><first>Krishnapriya</first><last>Vishnubhotla</last></author>
      <author><first>Adam</first><last>Hammond</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Graeme</first><last>Hirst</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Saif</first><last>Mohammad</last><affiliation>National Research Council Canada</affiliation></author>
      <pages>2557-2574</pages>
      <abstract>Stories are rich in the emotions they exhibit in their narratives and evoke in the readers. The emotional journeys of the various characters within a story are central to their appeal. Computational analysis of the emotions of novels, however, has rarely examined the variation in the emotional trajectories of the different characters within them, instead considering the entire novel to represent a single story arc. In this work, we use character dialogue to distinguish between the emotion arcs of the narration and the various characters. We analyze the emotion arcs of the various characters in a dataset of English literary novels using the framework of Utterance Emotion Dynamics. Our findings show that the narration and the dialogue largely express disparate emotions through the course of a novel, and that the commonalities or differences in the emotional arcs of stories are more accurately captured by those associated with individual characters.</abstract>
      <url hash="16d9355a">2024.findings-acl.150</url>
      <bibkey>vishnubhotla-etal-2024-emotion</bibkey>
      <doi>10.18653/v1/2024.findings-acl.150</doi>
    </paper>
    <paper id="151">
      <title>Accurate and Nuanced Open-<fixed-case>QA</fixed-case> Evaluation Through Textual Entailment</title>
      <author><first>Peiran</first><last>Yao</last></author>
      <author><first>Denilson</first><last>Barbosa</last><affiliation>University of Alberta</affiliation></author>
      <pages>2575-2587</pages>
      <abstract>Open-domain question answering (Open-QA) is a common task for evaluating large language models (LLMs). However, current Open-QA evaluations are criticized for the ambiguity in questions and the lack of semantic understanding in evaluators. Complex evaluators, powered by foundation models or LLMs and pertaining to semantic equivalence, still deviate from human judgments by a large margin. We propose to study the entailment relations of answers to identify more informative and more general system answers, offering a much closer evaluation to human judgment on both NaturalQuestions and TriviaQA while being learning-free. The entailment-based evaluation we propose allows the assignment of bonus or partial marks by quantifying the inference gap between answers, enabling a nuanced ranking of answer correctness that has higher AUC than current methods.</abstract>
      <url hash="29ea7104">2024.findings-acl.151</url>
      <bibkey>yao-barbosa-2024-accurate</bibkey>
      <doi>10.18653/v1/2024.findings-acl.151</doi>
    </paper>
    <paper id="152">
      <title>Dictionary-Aided Translation for Handling Multi-Word Expressions in Low-Resource Languages</title>
      <author><first>Antonios</first><last>Dimakis</last><affiliation>University of Athens</affiliation></author>
      <author><first>Stella</first><last>Markantonatou</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>Athena Research Center and George Mason University</affiliation></author>
      <pages>2588-2595</pages>
      <abstract>Multi-word expressions (MWEs) present unique challenges in natural language processing (NLP), particularly within the context of translation systems, due to their inherent scarcity, non-compositional nature, and other distinct lexical and morphosyntactic characteristics, issues that are exacerbated in low-resource settings.In this study, we elucidate and attempt to address these challenges by leveraging a substantial corpus of human-annotated Greek MWEs. To address the complexity of translating such phrases, we propose a novel method leveraging an available out-of-context lexicon.We assess the translation capabilities of current state-of-the-art systems on this task, employing both automated metrics and human evaluators.We find that by using our method when applicable, the performance of current systems can be significantly improved, however these models are still unable to produce translations comparable to those of a human speaker.</abstract>
      <url hash="f5959f43">2024.findings-acl.152</url>
      <bibkey>dimakis-etal-2024-dictionary</bibkey>
      <doi>10.18653/v1/2024.findings-acl.152</doi>
    </paper>
    <paper id="153">
      <title><fixed-case>LANS</fixed-case>: A Layout-Aware Neural Solver for Plane Geometry Problem</title>
      <author><first>Zhong-Zhi</first><last>Li</last></author>
      <author><first>Ming-Liang</first><last>Zhang</last></author>
      <author><first>Fei</first><last>Yin</last><affiliation>, Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Cheng-Lin</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>2596-2608</pages>
      <abstract>Geometry problem solving (GPS) is a challenging mathematical reasoning task requiring multi-modal understanding, fusion, and reasoning. Existing neural solvers take GPS as a vision-language task but are short in the representation of geometry diagrams that carry rich and complex layout information. In this paper, we propose a layout-aware neural solver named LANS, integrated with two new modules: multimodal layout-aware pre-trained language module (MLA-PLM) and layout-aware fusion attention (LA-FA). MLA-PLM adopts structural-semantic pre-training (SSP) to implement global relationship modeling, and point-match pre-training (PMP) to achieve alignment between visual points and textual points. LA-FA employs a layout-aware attention mask to realize point-guided cross-modal fusion for further boosting layout awareness of LANS. Extensive experiments on datasets Geometry3K and PGPS9K validate the effectiveness of the layout-aware modules and superior problem-solving performance of our LANS solver, over existing symbolic and neural solvers. We have made our code and data publicly available.</abstract>
      <url hash="f89499ff">2024.findings-acl.153</url>
      <bibkey>li-etal-2024-lans</bibkey>
      <doi>10.18653/v1/2024.findings-acl.153</doi>
    </paper>
    <paper id="154">
      <title>Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language Models</title>
      <author><first>Wenxuan</first><last>Ding</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shangbin</first><last>Feng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yuhan</first><last>Liu</last></author>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Vidhisha</first><last>Balachandran</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Tianxing</first><last>He</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <pages>2609-2636</pages>
      <abstract>We propose Knowledge Crosswords, a geometric knowledge reasoning benchmark consisting of incomplete knowledge networks bounded by structured factual constraints, where LLMs are tasked with inferring the missing facts to meet all constraints. The novel setting of geometric knowledge reasoning necessitates new LM abilities beyond existing atomic/linear multi-hop QA, such as backtracking, verifying facts and constraints, reasoning with uncertainty, and more. Knowledge Crosswords contains 2,101 individual problems, covering diverse knowledge domains, and is further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLMs and approaches on Knowledge Crosswords. Results demonstrate that baseline approaches struggle with larger knowledge networks and semantically-equivalent entity distractors. In light of their limitations, we propose two new approaches, Staged Prompting and Verify-All, to augment LLMs’ abilities for error-aware backtracking and constraint verification. Our Verify-All significantly outperforms prior methods and is more robust towards problems in the hard subset. Further analysis shows that geometric knowledge reasoning poses new challenges to LLMs’ knowledge abilities, particularly in robustness towards varying option orders, complex structural constraints in knowledge networks, “none of the above” scenarios, and more.</abstract>
      <url hash="294b6bba">2024.findings-acl.154</url>
      <bibkey>ding-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.findings-acl.154</doi>
    </paper>
    <paper id="155">
      <title><fixed-case>DELL</fixed-case>: Generating Reactions and Explanations for <fixed-case>LLM</fixed-case>-Based Misinformation Detection</title>
      <author><first>Herun</first><last>Wan</last></author>
      <author><first>Shangbin</first><last>Feng</last><affiliation>University of Washington</affiliation></author>
      <author><first>Zhaoxuan</first><last>Tan</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Heng</first><last>Wang</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Minnan</first><last>Luo</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <pages>2637-2667</pages>
      <abstract>Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount. In this work, we propose DELL that identifies three key stages in misinformation detection where LLMs could be incorporated as part of the pipeline: 1) LLMs could generate news reactions to represent diverse perspectives and simulate user-news interaction networks; 2) LLMs could generate explanations for proxy tasks (e.g., sentiment, stance) to enrich the contexts of news articles and produce experts specializing in various aspects of news understanding; 3) LLMs could merge task-specific experts and provide an overall prediction by incorporating the predictions and confidence scores of varying experts. Extensive experiments on seven datasets with three LLMs demonstrate that DELL outperforms state-of-the-art baselines by up to 16.8% in macro f1-score. Further analysis reveals that the generated reactions and explanations are greatly helpful in misinformation detection, while our proposed LLM-guided expert merging helps produce better-calibrated predictions.</abstract>
      <url hash="4024b96d">2024.findings-acl.155</url>
      <bibkey>wan-etal-2024-dell</bibkey>
      <doi>10.18653/v1/2024.findings-acl.155</doi>
    </paper>
    <paper id="156">
      <title>The Language Barrier: Dissecting Safety Challenges of <fixed-case>LLM</fixed-case>s in Multilingual Contexts</title>
      <author><first>Lingfeng</first><last>Shen</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Weiting</first><last>Tan</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Sihao</first><last>Chen</last></author>
      <author><first>Yunmo</first><last>Chen</last></author>
      <author><first>Jingyu</first><last>Zhang</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Haoran</first><last>Xu</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Boyuan</first><last>Zheng</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Philipp</first><last>Koehn</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Daniel</first><last>Khashabi</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>2668-2680</pages>
      <abstract>As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages,we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, training in lower-resource languages yields minimal improvement. This suggests that the bottleneck of cross-lingual alignment is rooted in the pretraining stage. Our findings highlight the challenges in cross-lingual LLM safety, and we hope they inform future research in this direction.</abstract>
      <url hash="ba0137df">2024.findings-acl.156</url>
      <bibkey>shen-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.156</doi>
    </paper>
    <paper id="157">
      <title>Self-Specialization: Uncovering Latent Expertise within Large Language Models</title>
      <author><first>Junmo</first><last>Kang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Hongyin</first><last>Luo</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yada</first><last>Zhu</last><affiliation>IBM Research</affiliation></author>
      <author><first>Jacob</first><last>Hansen</last></author>
      <author><first>James</first><last>Glass</last></author>
      <author><first>David</first><last>Cox</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Alan</first><last>Ritter</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Rogerio</first><last>Feris</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Leonid</first><last>Karlinsky</last><affiliation>IBM Research AI</affiliation></author>
      <pages>2681-2706</pages>
      <abstract>Recent works have demonstrated the effectiveness of self-alignment in which a large language model is aligned to follow general instructions using instructional data generated from the model itself starting from a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine, finance). As a preliminary, we quantitively show the marginal effect that generic instruction-following training has on downstream expert domains’ performance. To remedy this, we propose self-specialization - allowing for effective model specialization while achieving cross-task generalization by leveraging only a few labeled seeds. Self-specialization offers a data- and parameter-efficient way of “carving out” an expert model out of a generalist pre-trained LLM. Exploring a variety of popular open large models as a base for specialization, our experimental results in both biomedical and financial domains show that our self-specialized models outperform their base models by a large margin, and even larger models that are generally instruction-tuned or that have been adapted to the target domain by other means.</abstract>
      <url hash="c4dfbf1f">2024.findings-acl.157</url>
      <bibkey>kang-etal-2024-self</bibkey>
      <doi>10.18653/v1/2024.findings-acl.157</doi>
    </paper>
    <paper id="158">
      <title><fixed-case>FUSE</fixed-case>: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion</title>
      <author><first>Fred</first><last>Xu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Song</first><last>Jiang</last><affiliation>FAIR</affiliation></author>
      <author><first>Zijie</first><last>Huang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Xiao</first><last>Luo</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Shichang</first><last>Zhang</last><affiliation>Harvard Business School</affiliation></author>
      <author><first>Yuanzhou</first><last>Chen</last><affiliation>, University of California, Los Angeles</affiliation></author>
      <author><first>Yizhou</first><last>Sun</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>2707-2720</pages>
      <abstract>Taxonomy Expansion, which relies on modeling concepts and concept relations, can be formulated as a set representation learning task. The generalization of set, fuzzy set, incorporates uncertainty and measures the information within a semantic concept, making it suitable for concept modeling. Existing works usually model sets as vectors or geometric objects such as boxes, which are not closed under set operations. In this work, we propose a sound and efficient formulation of set representation learning based on its volume approximation as a fuzzy set. The resulting embedding framework, <i>Fuzzy Set Embedding</i>, satisfies all set operations and compactly approximates the underlying fuzzy set, hence preserving information while being efficient to learn, relying on minimum neural architecture. We empirically demonstrate the power of FUSE on the task of taxonomy expansion, where FUSE achieves remarkable improvements up to 23% compared with existing baselines. Our work marks the first attempt to understand and efficiently compute the embeddings of fuzzy sets.</abstract>
      <url hash="70553188">2024.findings-acl.158</url>
      <bibkey>xu-etal-2024-fuse</bibkey>
      <doi>10.18653/v1/2024.findings-acl.158</doi>
    </paper>
    <paper id="159">
      <title>Chain of Logic: Rule-Based Reasoning with Large Language Models</title>
      <author><first>Sergio</first><last>Servantez</last></author>
      <author><first>Joe</first><last>Barrow</last><affiliation>Pattern Data</affiliation></author>
      <author><first>Kristian</first><last>Hammond</last></author>
      <author><first>Rajiv</first><last>Jain</last><affiliation>Adobe Systems</affiliation></author>
      <pages>2721-2733</pages>
      <abstract>Rule-based reasoning, a fundamental type of legal reasoning, enables us to draw conclusions by accurately applying a rule to a set of facts. We explore causal language models as rule-based reasoners, specifically with respect to compositional rules - rules consisting of multiple elements which form a complex logical expression. Reasoning about compositional rules is challenging because it requires multiple reasoning steps, and attending to the logical relationships between elements. We introduce a new prompting method, Chain of Logic, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression). This method was inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers. We evaluate chain of logic across eight rule-based reasoning tasks involving three distinct compositional rules from the LegalBench benchmark and demonstrate it consistently outperforms other prompting methods, including chain of thought and self-ask, using open-source and commercial language models.</abstract>
      <url hash="5bcd0b37">2024.findings-acl.159</url>
      <bibkey>servantez-etal-2024-chain</bibkey>
      <doi>10.18653/v1/2024.findings-acl.159</doi>
    </paper>
    <paper id="160">
      <title>Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations</title>
      <author><first>Cheng-Han</first><last>Chiang</last></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>2734-2751</pages>
      <abstract>Long-form generations from large language models (LLMs) contain a mix of factual and non-factual claims, making evaluating factuality difficult.Prior works evaluate the factuality of a long paragraph by decomposing it into multiple facts, verifying those facts independently, and aggregating the results.Such methods assume that combining factual claims forms a factual paragraph.The above assumption can be violated: we show that strong open-source models like Llama-chat can generate paragraphs that contain verifiable facts, but the facts are combined into a non-factual paragraph due to entity ambiguity.We further reveal that existing factuality metrics, including FActScore and citation recall, cannot properly evaluate these non-factual paragraphs and overestimate their factuality.To address this, we introduce an enhanced metric, **D-FActScore**, specifically designed for content with ambiguous entities.We evaluate the D-FActScores of people biographies generated by retrieval-augmented LLMs.We show that D-FActScore can better assess the factuality of paragraphs with entity ambiguity than FActScore.We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs, making their D-FActScore much lower than FActScore by over 10%.</abstract>
      <url hash="38ee7981">2024.findings-acl.160</url>
      <bibkey>chiang-lee-2024-merging</bibkey>
      <doi>10.18653/v1/2024.findings-acl.160</doi>
    </paper>
    <paper id="161">
      <title>Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment</title>
      <author><first>William</first><last>Merrill</last><affiliation>New York University</affiliation></author>
      <author><first>Zhaofeng</first><last>Wu</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Norihito</first><last>Naka</last><affiliation>New York University</affiliation></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Tal</first><last>Linzen</last><affiliation>New York University and Google</affiliation></author>
      <pages>2752-2773</pages>
      <abstract>Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, sentence co-occurrence probabilities predicted by an optimal LM should reflect the entailment relationship of the constituent sentences, but it is unclear whether probabilities predicted by neural LMs encode entailment in this way because of strong assumptions made by Merrill et al. (namely, that humans always avoid redundancy). In this work, we investigate whether their theory can be used to decode entailment relations from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text. We argue that better accounting for redundancy related to *explanations* might derive the observed flipped test and, more generally, improve computational models of speakers in linguistics.</abstract>
      <url hash="31657083">2024.findings-acl.161</url>
      <bibkey>merrill-etal-2024-learn</bibkey>
      <doi>10.18653/v1/2024.findings-acl.161</doi>
    </paper>
    <paper id="162">
      <title>Simulated Misinformation Susceptibility (<fixed-case>SMISTS</fixed-case>): Enhancing Misinformation Research with Large Language Model Simulations</title>
      <author><first>Weicheng</first><last>Ma</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Chunyuan</first><last>Deng</last><affiliation>Rice University</affiliation></author>
      <author><first>Aram</first><last>Moossavi</last></author>
      <author><first>Lili</first><last>Wang</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>2774-2788</pages>
      <abstract>Psychological inoculation, a strategy designed to build resistance against persuasive misinformation, has shown efficacy in curbing its spread and mitigating its adverse effects at early stages. Despite its effectiveness, the design and optimization of these inoculations typically demand substantial human and financial resources, primarily due to the need for repeated experimental trials. To address these challenges, this paper introduces Simulated Misinformation Susceptibility Tests (SMISTs), leveraging Large Language Models (LLMs) to simulate participant responses in misinformation studies. SMIST employs a life experience-driven simulation methodology, which accounts for various aspects of participants’ backgrounds, to mitigate common issues of caricatures and stereotypes in LLM simulations and enhance response diversity. Our extensive experimentation demonstrates that SMIST, utilizing GPT-4 as the backend model, yields results that align closely with those obtained from human-subject studies in misinformation susceptibility. This alignment suggests that LLMs can effectively serve as proxies in evaluating the impact of psychological inoculations. Moreover, SMIST offers the critical benefit of being applicable to emerging or anticipated misinformation scenarios without exposing human participants to potentially harmful content. This characteristic of SMIST not only preserves participant safety but also expands the scope of misinformation research to include more sensitive or speculative topics.</abstract>
      <url hash="12a037f6">2024.findings-acl.162</url>
      <bibkey>ma-etal-2024-simulated</bibkey>
      <doi>10.18653/v1/2024.findings-acl.162</doi>
    </paper>
    <paper id="163">
      <title>Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future</title>
      <author><first>Minzhi</first><last>Li</last></author>
      <author><first>Weiyan</first><last>Shi</last></author>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>2789-2805</pages>
      <abstract>As Natural Language Processing (NLP) systems become increasingly integrated into human social life, these technologies will need to increasingly rely on social intelligence. Although there are many valuable datasets that benchmark isolated dimensions of social intelligence, there does not yet exist any body of work to join these threads into a cohesive subfield in which researchers can quickly identify research gaps and future directions. Towards this goal, we build a Social AI Data Infrastructure, which consists of a comprehensive social AI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows us to analyze existing dataset efforts, and also evaluate language models’ performance in different social intelligence aspects. Our analyses demonstrate its utility in enabling a thorough understanding of current data landscape and providing a holistic perspective on potential directions for future dataset development. We show there is a need for multifaceted datasets, increased diversity in language and culture, more long-tailed social situations, and more interactive data in future social intelligence data efforts.</abstract>
      <url hash="6482a4f7">2024.findings-acl.163</url>
      <bibkey>li-etal-2024-social</bibkey>
      <doi>10.18653/v1/2024.findings-acl.163</doi>
    </paper>
    <paper id="164">
      <title>Selective Prefix Tuning for Pre-trained Language Models</title>
      <author><first>Hongyi</first><last>Zhang</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>2806-2813</pages>
      <abstract>The prevalent approach for optimizing pre-trained language models in downstream tasks is fine-tuning. However, it is both time-consuming and memory-inefficient. In response, a more efficient method called Prefix Tuning, which insert learnable vectors into each Transformer layers, has been proposed and proven effective. Recent investigations reveal that prefix tokens carry context-specific information, prompting the hypothesis that enhancing their specialization can improve model performance. To address this, we propose Selective Prefix Tuning (SPT), integrating a selective mechanism inspired by selective self-attention. Additionally, we introduce Selective Loss (SL) to encourage diversity in prefix tokens. Extensive experiments validate the effectiveness of SPT in sentence and token classification tasks. We contribute insight into understanding the role of prefix in model adaptation.</abstract>
      <url hash="07cf69b2">2024.findings-acl.164</url>
      <bibkey>zhang-etal-2024-selective</bibkey>
      <doi>10.18653/v1/2024.findings-acl.164</doi>
    </paper>
    <paper id="165">
      <title><fixed-case>MODABS</fixed-case>: Multi-Objective Learning for Dynamic Aspect-Based Summarization</title>
      <author><first>Xiaobo</first><last>Guo</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <pages>2814-2827</pages>
      <abstract>The rapid proliferation of online content necessitates effective summarization methods, among which dynamic aspect-based summarization stands out. Unlike its traditional counterpart, which assumes a fixed set of known aspects, this approach adapts to the varied aspects of the input text. We introduce a novel multi-objective learning framework employing a Longformer-Encoder-Decoder for this task. The framework optimizes aspect number prediction, minimizes disparity between generated and reference summaries for each aspect, and maximizes dissimilarity across aspect-specific summaries. Extensive experiments show our method significantly outperforms baselines on three diverse datasets, largely due to the effective alignment of generated and reference aspect counts without sacrificing single-aspect summarization quality.</abstract>
      <url hash="45760470">2024.findings-acl.165</url>
      <bibkey>guo-vosoughi-2024-modabs</bibkey>
      <doi>10.18653/v1/2024.findings-acl.165</doi>
    </paper>
    <paper id="166">
      <title>Non-compositional Expression Generation and its Continual Learning</title>
      <author><first>Jianing</first><last>Zhou</last></author>
      <author><first>Suma</first><last>Bhat</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>2828-2839</pages>
      <abstract>Non-compositional expressions are an integral part of natural language and their meanings cannot be directly derived from the meanings of their component words. Recent work has shown how their processing remains a challenge for pre-trained language models. Here we consider the fact that prior knowledge of their component words is inadequate to infer their meaning as a whole and that these expressions constitute a long-tailed process in language (based on their occurrence in corpora and their coming into use as an idiomatic expression in a continual manner). Against this backdrop, this paper studies the ability of recent pre-trained language models to generate non-compositional expressions in English and their continual learning. Formulating this as a mask infilling task termed as CLoNE, the study uncovers the combined challenges of non-compositionality and their continual learning. Using a set of three diverse idiomatic expression datasets repurposed for this task, we benchmark different large pre-trained language models and different continual learning methods on the task of non-compositional expression generation. Our experiments on the CLoNE task show that large pre-trained language models are limited in their ability to generate non-compositional expressions and available continual learning methods are inadequate for our proposed CLoNE task which calls for more effective methods for continual learning of non-compositionality. Our datasets and code will be released publicly upon acceptance.</abstract>
      <url hash="8878bbef">2024.findings-acl.166</url>
      <bibkey>zhou-bhat-2024-non</bibkey>
      <doi>10.18653/v1/2024.findings-acl.166</doi>
    </paper>
    <paper id="167">
      <title>Medical Dialogue System: A Survey of Categories, Methods, Evaluation and Challenges</title>
      <author><first>Xiaoming</first><last>Shi</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Li</first><last>Du</last></author>
      <author><first>Yuxuan</first><last>Wang</last><affiliation>Zhejiang Lab, Zhejiang Lab</affiliation></author>
      <author><first>Hongru</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <author><first>Jie</first><last>Xu</last></author>
      <author><first>Xiaofan</first><last>Zhang</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Shaoting</first><last>Zhang</last><affiliation>University of North Carolina at Charlotte</affiliation></author>
      <pages>2840-2861</pages>
      <abstract>This paper surveys and organizes research works of medical dialog systems, which is an important yet challenging task. Although these systems have been surveyed in the medical community from an application perspective, a systematic review from a rigorous technical perspective has to date remained noticeably absent. As a result, an overview of the categories, methods, evaluation of medical dialogue systems remain limited and underspecified, hindering the further improvement of this area. To fill this gap, we investigate an initial pool of 325 papers from well-known computer science, natural language processing conferences and journals, and make an overview. Recently, large language models have shown strong model capacity on downstream tasks, which also reshape medical dialog systems’ foundation.Despite the alluring practical application value, current medical dialogue systems still suffer from problems. To this end, this paper lists grand challenges of medical dialog systems, especially of large language models.</abstract>
      <url hash="867d0e41">2024.findings-acl.167</url>
      <bibkey>shi-etal-2024-medical</bibkey>
      <doi>10.18653/v1/2024.findings-acl.167</doi>
    </paper>
    <paper id="168">
      <title>Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs</title>
      <author><first>Thi</first><last>Nguyen</last></author>
      <author><first>Linhao</first><last>Luo</last></author>
      <author><first>Fatemeh</first><last>Shiri</last><affiliation>Monash University</affiliation></author>
      <author><first>Dinh</first><last>Phung</last><affiliation>Monash University and Deakin University, Australia</affiliation></author>
      <author><first>Yuan-Fang</first><last>Li</last><affiliation>Monash University</affiliation></author>
      <author><first>Thuy-Trang</first><last>Vu</last><affiliation>Monash University</affiliation></author>
      <author><first>Gholamreza</first><last>Haffari</last><affiliation>Monash University</affiliation></author>
      <pages>2862-2883</pages>
      <abstract>Large language models (LLMs) have demonstrated strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs’ knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning.</abstract>
      <url hash="0311aed6">2024.findings-acl.168</url>
      <bibkey>nguyen-etal-2024-direct</bibkey>
      <doi>10.18653/v1/2024.findings-acl.168</doi>
    </paper>
    <paper id="169">
      <title>Comprehensive Abstractive Comment Summarization with Dynamic Clustering and Chain of Thought</title>
      <author><first>Longyin</first><last>Zhang</last><affiliation>A*STAR</affiliation></author>
      <author><first>Bowei</first><last>Zou</last><affiliation>A*STAR</affiliation></author>
      <author><first>Jacintha</first><last>Yi</last></author>
      <author><first>AiTi</first><last>Aw</last><affiliation>I2R</affiliation></author>
      <pages>2884-2896</pages>
      <abstract>Real-world news comments pose a significant challenge due to their noisy and ambiguous nature, which complicates their modeling for clustering and summarization tasks. Most previous research has predominantly focused on extractive summarization methods within specific constraints. This paper concentrates on Clustering and Abstractive Summarization of online news Comments (CASC). First, we introduce an enhanced fast clustering algorithm that maintains a dynamic similarity threshold to ensure the high density of each comment cluster being built. Moreover, we pioneer the exploration of tuning Large Language Models (LLMs) through a chain-of-thought strategy to generate summaries for each comment cluster. On the other hand, a notable challenge in CASC research is the scarcity of evaluation data. To address this problem, we design an annotation scheme and contribute a manual test suite tailored for CASC. Experimental results on the test suite demonstrate the effectiveness of our improvements to the baseline methods. In addition, the quantitative and qualitative analyses illustrate the adaptability of our approach to real-world news comment scenarios.</abstract>
      <url hash="3c221654">2024.findings-acl.169</url>
      <bibkey>zhang-etal-2024-comprehensive</bibkey>
      <doi>10.18653/v1/2024.findings-acl.169</doi>
    </paper>
    <paper id="170">
      <title>Self-Supervised Position Debiasing for Large Language Models</title>
      <author><first>Zhongkun</first><last>Liu</last></author>
      <author><first>Zheng</first><last>Chen</last></author>
      <author><first>Mengqi</first><last>Zhang</last><affiliation>Shandong University</affiliation></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <author><first>Pengjie</first><last>Ren</last><affiliation>Shandong University</affiliation></author>
      <author><first>Zhumin</first><last>Chen</last><affiliation>Shandong University</affiliation></author>
      <pages>2897-2917</pages>
      <abstract>Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Previous works have proven that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing debiasing methods for LLMs require external bias knowledge or annotated non-biased samples, which is lacking for position debiasing and impractical in reality. In this work, we propose a self-supervised position debiasing (SOD) framework to mitigate position bias for LLMs. SOD leverages unsupervised responses from pre-trained LLMs for debiasing without relying on any external knowledge. To improve the quality of unsupervised responses, we propose an objective alignment (OAM) module to prune these responses. Experiments on eight datasets and five tasks show that SOD consistently outperforms existing methods in mitigating three types of position biases. Besides, SOD achieves this by sacrificing only a small performance on biased samples, which is general and effective. To facilitate the reproducibility of the results, we share the code of all methods and datasets on https://github.com/LZKSKY/SOD.</abstract>
      <url hash="eace0f8d">2024.findings-acl.170</url>
      <bibkey>liu-etal-2024-self-supervised</bibkey>
      <doi>10.18653/v1/2024.findings-acl.170</doi>
    </paper>
    <paper id="171">
      <title><fixed-case>H</fixed-case>yper<fixed-case>CL</fixed-case>: A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology</title>
      <author><first>Yuhuan</first><last>Lu</last></author>
      <author><first>Weijian</first><last>Yu</last></author>
      <author><first>Xin</first><last>Jing</last></author>
      <author><first>Dingqi</first><last>Yang</last><affiliation>University of Macau</affiliation></author>
      <pages>2918-2929</pages>
      <abstract>Knowledge Graph (KG) embeddings are essential for link prediction over KGs. Compared to triplets, hyper-relational facts consisting of a base triplet and an arbitrary number of key-value pairs, can better characterize real-world facts and have aroused various hyper-relational embedding techniques recently. Nevertheless, existing works seldom consider the ontology of KGs, which is beneficial to link prediction tasks. A few studies attempt to incorporate the ontology information, by either utilizing the ontology as constraints on entity representations or jointly learning from hyper-relational facts and the ontology. However, existing approaches mostly overlook the ontology hierarchy and suffer from the dominance issue of facts over ontology, resulting in suboptimal performance. Against this background, we propose a universal contrastive learning framework for hyper-relational KG embeddings (<tex-math>\textbf{HyperCL}</tex-math>), which is flexible to integrate different hyper-relational KG embedding methods and effectively boost their link prediction performance. HyperCL designs relation-aware Graph Attention Networks to capture the hierarchical ontology and a concept-aware contrastive loss to alleviate the dominance issue. We evaluate HyperCL on three real-world datasets in different link prediction tasks. Experimental results show that HyperCL consistently boosts the performance of state-of-the-art baselines with an average improvement of 3.1-7.4% across the three datasets.</abstract>
      <url hash="6e0553c6">2024.findings-acl.171</url>
      <bibkey>lu-etal-2024-hypercl</bibkey>
      <doi>10.18653/v1/2024.findings-acl.171</doi>
    </paper>
    <paper id="172">
      <title>Encoding Hierarchical Schema via Concept Flow for Multifaceted Ideology Detection</title>
      <author><first>Songtao</first><last>Liu</last></author>
      <author><first>Bang</first><last>Wang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Wei</first><last>Xiang</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Han</first><last>Xu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Minghua</first><last>Xu</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>2930-2942</pages>
      <abstract>Multifaceted ideology detection (MID) aims to detect the ideological leanings of texts towards multiple facets. Previous studies on ideology detection mainly focus on one generic facet and ignore label semantics and explanatory descriptions of ideologies, which are a kind of instructive information and reveal the specific concepts of ideologies. In this paper, we develop a novel concept semantics-enhanced framework for the MID task. Specifically, we propose a bidirectional iterative concept flow (BICo) method to encode multifaceted ideologies. BICo enables the concepts to flow across levels of the schema tree and enriches concept representations with multi-granularity semantics. Furthermore, we explore concept attentive matching and concept-guided contrastive learning strategies to guide the model to capture ideology features with the learned concept semantics. Extensive experiments on the benchmark dataset show that our approach achieves state-of-the-art performance in MID, including in the cross-topic scenario.</abstract>
      <url hash="5702fc05">2024.findings-acl.172</url>
      <bibkey>liu-etal-2024-encoding</bibkey>
      <doi>10.18653/v1/2024.findings-acl.172</doi>
    </paper>
    <paper id="173">
      <title>Character-Level <fixed-case>C</fixed-case>hinese Dependency Parsing via Modeling Latent Intra-Word Structure</title>
      <author><first>Yang</first><last>Hou</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <pages>2943-2956</pages>
      <abstract>Revealing the syntactic structure of sentences in Chinese poses significant challenges for word-level parsers due to the absence of clear word boundaries. To facilitate a transition from word-level to character-level Chinese dependency parsing, this paper proposes modeling latent internal structures within words. In this way, each word-level dependency tree is interpreted as a forest of character-level trees. A constrained Eisner algorithm is implemented to ensure the compatibility of character-level trees, guaranteeing a single root for intra-word structures and establishing inter-word dependencies between these roots. Experiments on Chinese treebanks demonstrate the superiority of our method over both the pipeline framework and previous joint models. A detailed analysis reveals that a coarse-to-fine parsing strategy empowers the model to predict more linguistically plausible intra-word structures.</abstract>
      <url hash="ff4a6973">2024.findings-acl.173</url>
      <bibkey>hou-li-2024-character</bibkey>
      <doi>10.18653/v1/2024.findings-acl.173</doi>
    </paper>
    <paper id="174">
      <title><fixed-case>A</fixed-case>lign<fixed-case>RE</fixed-case>: An Encoding and Semantic Alignment Approach for Zero-Shot Relation Extraction</title>
      <author><first>Zehan</first><last>Li</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Fu</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jingwei</first><last>Cheng</last><affiliation>Northeastern University, China</affiliation></author>
      <pages>2957-2966</pages>
      <abstract>Zero-shot Relation Extraction (ZSRE) aims to predict unseen relations between entity pairs from input sentences. Existing prototype-based ZSRE methods encode relation descriptions into prototype embeddings and predict by measuring the similarity between sentence embeddings and prototype embeddings. However, these methods often overlook abundant side information of relations and suffer from a significant encoding gap between prototypes and sentences, limiting performance. To this end, we propose a framework named AlignRE, based on two Alignment methods for ZSRE. Specifically, we present a novel perspective centered on encoding schema alignment to enhance prototype-based ZSRE methods. We utilize well-designed prompt-tuning to bridge the encoding gap. To improve prototype quality, we explore and leverage multiple side information and propose a prototype aggregation method based on semantic alignment to create comprehensive relation prototype representations. We conduct experiments on FewRel and Wiki-ZSL datasets and consistently outperform state-of-the-art methods. Moreover, our method exhibits substantially faster performance and reduces the need for extensive manual labor in prototype construction. Code is available at https://github.com/lizehan1999/AlignRE.</abstract>
      <url hash="5c53545f">2024.findings-acl.174</url>
      <bibkey>li-etal-2024-alignre</bibkey>
      <doi>10.18653/v1/2024.findings-acl.174</doi>
    </paper>
    <paper id="175">
      <title>Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction</title>
      <author><first>Tingchen</first><last>Fu</last></author>
      <author><first>Deng</first><last>Cai</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Lemao</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>2967-2985</pages>
      <abstract>Supervised fine-tuning (SFT) on instruction-following corpus is a crucial approach toward the alignment of large language models (LLMs). However, the performance of LLMs on standard knowledge and reasoning benchmarks tends to suffer from deterioration at the latter stage of the SFT process, echoing the phenomenon of alignment tax. Through our pilot study, we put a hypothesis that the data biases are probably one cause behind the phenomenon. To address the issue, we introduce a simple disperse-then-merge framework. To be concrete, we disperse the instruction-following data into portions and then train multiple sub-models using different data portions. Lastly, we merge multiple models into a single one via model merging techniques. Despite its simplicity, our framework outperforms various sophisticated methods such as data curation and training regularization on a series of standard knowledge and reasoning benchmarks.</abstract>
      <url hash="4f20d8bc">2024.findings-acl.175</url>
      <bibkey>fu-etal-2024-disperse</bibkey>
      <doi>10.18653/v1/2024.findings-acl.175</doi>
    </paper>
    <paper id="176">
      <title>Efficient Knowledge Infusion via <fixed-case>KG</fixed-case>-<fixed-case>LLM</fixed-case> Alignment</title>
      <author><first>Zhouyu</first><last>Jiang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Ling</first><last>Zhong</last></author>
      <author><first>Mengshu</first><last>Sun</last></author>
      <author><first>Jun</first><last>Xu</last></author>
      <author><first>Rui</first><last>Sun</last></author>
      <author><first>Hui</first><last>Cai</last></author>
      <author><first>Shuhan</first><last>Luo</last></author>
      <author><first>Zhiqiang</first><last>Zhang</last><affiliation>Ant Group</affiliation></author>
      <pages>2986-2999</pages>
      <abstract>To tackle the problem of domain-specific knowledge scarcity within large language models (LLMs), knowledge graph-retrievalaugmented method has been proven to be an effective and efficient technique for knowledge infusion. However, existing approaches face two primary challenges: knowledge mismatch between public available knowledge graphs and the specific domain of the task at hand, and poor information compliance of LLMs with knowledge graphs. In this paper, we leverage a small set of labeled samples and a large-scale corpus to efficiently construct domain-specific knowledge graphs by an LLM, addressing the issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM alignment strategy to enhance the LLM’s capability to utilize information from knowledge graphs. We conduct experiments with a limited-sample setting on two biomedical question-answering datasets, and the results demonstrate that our approach outperforms existing baselines.</abstract>
      <url hash="b6ba4b41">2024.findings-acl.176</url>
      <bibkey>jiang-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.findings-acl.176</doi>
    </paper>
    <paper id="177">
      <title>Towards Precise Localization of Critical Errors in Machine Translation</title>
      <author><first>Dahyun</first><last>Jung</last><affiliation>Korea University</affiliation></author>
      <author><first>Sugyeong</first><last>Eo</last><affiliation>Korea University</affiliation></author>
      <author><first>Heuiseok</first><last>Lim</last><affiliation>Korea University</affiliation></author>
      <pages>3000-3012</pages>
      <abstract>The advent of large language models has experienced a remarkable improvement in the field of machine translation. However, machine translation is still vulnerable to critical meaning deviations, which may incur catastrophic issues in social or ethical contexts. In particular, existing critical error detection primarily focuses on identifying sentence-level errors, leaving the precise localization of such errors within the sentence unaddressed. In this paper, we introduce a new task, word-level critical error detection (WCED), to detect critical errors at a fine-grained level in machine translation sentences. The task aims to identify the parts of a machine translation that contain catastrophic meaning distortions. We hypothesize that the ability to determine errors at the sentence level will positively influence the detection of more granular errors. We propose a sentence-level error detection module to predict which words in a sentence have critical errors. Experimental results demonstrate that our method outperforms existing methodologies and LLM in En-De, Zh-En, En-Ru, and En-Ko. Our method is helpful for determining the fine-grained location of errors. We hope that such studies will improve the capacity to address critical errors adeptly.</abstract>
      <url hash="7fc040e5">2024.findings-acl.177</url>
      <bibkey>jung-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-acl.177</doi>
    </paper>
    <paper id="178">
      <title><fixed-case>L</fixed-case>o<fixed-case>RAP</fixed-case>rune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning</title>
      <author><first>Mingyang</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Hao</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Chunhua</first><last>Shen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhen</first><last>Yang</last></author>
      <author><first>Linlin</first><last>Ou</last></author>
      <author><first>Xinyi</first><last>Yu</last></author>
      <author><first>Bohan</first><last>Zhuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>3013-3026</pages>
      <abstract>Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead.To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather than the gradients of pre-trained weights for importance estimation. We subsequently integrate this criterion into an iterative pruning process, effectively removing redundant channels and heads. Extensive experimental results demonstrate the superior performance of our LoRAPrune over existing approaches on the LLaMA series models.At a 50% compression rate, LoRAPrune demonstrates superior performance over LLM-Pruner, achieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while also decreasing memory usage by 52.6%.Besides, LoRAPrune also matches semi-structural pruning across multiple LLMs, proving its wide applicability. The code is available at https://github.com/aim-uofa/LoRAPrune.</abstract>
      <url hash="25ac81d3">2024.findings-acl.178</url>
      <bibkey>zhang-etal-2024-loraprune</bibkey>
      <doi>10.18653/v1/2024.findings-acl.178</doi>
    </paper>
    <paper id="179">
      <title>Speculative Decoding via Early-exiting for Faster <fixed-case>LLM</fixed-case> Inference with <fixed-case>T</fixed-case>hompson Sampling Control Mechanism</title>
      <author><first>Jiahao</first><last>Liu</last><affiliation>Meituan</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last></author>
      <pages>3027-3043</pages>
      <abstract>The recent advancements in large language models (LLMs) have been extraordinary, yet the escalating inference costs associated with them present challenges in real-world applications. To address these challenges, we propose a novel approach called Early-exiting Speculative Decoding (EESD) with lossless acceleration. Specifically, EESD utilizes a segment of the LLM to generate draft tokens, incorporating Early-exiting structures after the first N layers. To enhance the quality of draft tokens, a self-distillation method is integrated. This early-exiting design not only reduces deployment and training costs but also significantly accelerates the token generation speed. Moreover, we introduce a novel sampling mechanism that leverages Thompson Sampling to regulate the generation processes, automatically determining the quantity of draft tokens in each round. The original LLM is then employed to validate these draft tokens through a single forward pass, and thus guarantees that the final output text maintains a distribution consistent with vanilla auto-regressive decoding. The experimental results on both 13B and 70B models demonstrate that our approach decodes tokens at a markedly accelerated rate compared to prior methods, showing the effectiveness of our approach.</abstract>
      <url hash="e8a6931a">2024.findings-acl.179</url>
      <bibkey>liu-etal-2024-speculative-decoding</bibkey>
      <doi>10.18653/v1/2024.findings-acl.179</doi>
    </paper>
    <paper id="180">
      <title>Towards Better Utilization of Multi-Reference Training Data for <fixed-case>C</fixed-case>hinese Grammatical Error Correction</title>
      <author><first>Yumeng</first><last>Liu</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Zhenghua</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>HaoChen</first><last>Jiang</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Bo</first><last>Zhang</last></author>
      <author><first>Chen</first><last>Li</last></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>3044-3052</pages>
      <abstract>For the grammatical error correction (GEC) task, there usually exist multiple correction ways for an erroneous input sentence, leading to multiple references. Observing the high proportion of multi-reference instances in Chinese GEC training data, we target a systematic study on how to better utilize multi-reference training data. We propose two new approaches and a simple two-stage training strategy. We compare them against previously proposed approaches, on two Chinese training datasets, i.e., Lang-8 for second language learner texts and FCGEC-Train for native speaker texts, and three test datasets. The experiments and analyses demonstrate the effectiveness of our proposed approaches and reveal interesting insights. Our code is available at https://github.com/ymliucs/MrGEC.</abstract>
      <url hash="5c4d0140">2024.findings-acl.180</url>
      <bibkey>liu-etal-2024-towards-better</bibkey>
      <doi>10.18653/v1/2024.findings-acl.180</doi>
    </paper>
    <paper id="181">
      <title><fixed-case>A</fixed-case>gent<fixed-case>T</fixed-case>uning: Enabling Generalized Agent Abilities for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Aohan</first><last>Zeng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Mingdao</first><last>Liu</last></author>
      <author><first>Rui</first><last>Lu</last></author>
      <author><first>Bowen</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>3053-3077</pages>
      <abstract>Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs’ agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://anonymous.4open.science/r/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.</abstract>
      <url hash="66fa1168">2024.findings-acl.181</url>
      <bibkey>zeng-etal-2024-agenttuning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.181</doi>
    </paper>
    <paper id="182">
      <title>Transition-based Opinion Generation for Aspect-based Sentiment Analysis</title>
      <author><first>Tianlai</first><last>Ma</last></author>
      <author><first>Zhongqing</first><last>Wang</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Guodong</first><last>Zhou</last><affiliation>Soochow University, China</affiliation></author>
      <pages>3078-3087</pages>
      <abstract>Recently, the use of pre-trained generation models for extracting sentiment elements has resulted in significant advancements in aspect-based sentiment analysis benchmarks. However, these approaches often overlook the importance of explicitly modeling structure among sentiment elements. To address this limitation, we present a study that aims to integrate general pre-trained sequence-to-sequence language models with a structure-aware transition-based approach. Therefore, we propose a transition system for opinion tree generation, designed to better exploit pre-trained language models for structured fine-tuning. Our proposed transition system ensures the structural integrity of the generated opinion tree. By leveraging pre-trained generation models and simplifying the transition set, we are able to maximize the accuracy of opinion tree generation. Extensive experiments show that our model significantly advances the state-of-the-art performance on several benchmark datasets. In addition, the empirical studies also indicate that the proposed opinion tree generation with transition system is more effective in capturing the sentiment structure than other generation models.</abstract>
      <url hash="4c5e498b">2024.findings-acl.182</url>
      <bibkey>ma-etal-2024-transition</bibkey>
      <doi>10.18653/v1/2024.findings-acl.182</doi>
    </paper>
    <paper id="183">
      <title>Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion</title>
      <author><first>Xiaobao</first><last>Wu</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xinshuai</first><last>Dong</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Thong</first><last>Nguyen</last></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>3088-3105</pages>
      <abstract>Dynamic topic models track the evolution of topics in sequential documents, which have derived various applications like trend analysis. However, existing models suffer from repetitive topic and unassociated topic issues, failing to reveal the evolution and hindering further applications. To address these issues, we break the tradition of simply chaining topics in existing work and propose a novel neural Chain-Free Dynamic Topic Model. We introduce a new evolution-tracking contrastive learning method that builds the similarity relations among dynamic topics. This not only tracks topic evolution but also maintains topic diversity, mitigating the repetitive topic issue. To avoid unassociated topics, we further present an unassociated word exclusion method that consistently excludes unassociated words from discovered topics. Extensive experiments demonstrate our model significantly outperforms state-of-the-art baselines, tracking topic evolution with high-quality topics, showing better performance on downstream tasks, and remaining robust to the hyperparameter for evolution intensities.</abstract>
      <url hash="5aab1262">2024.findings-acl.183</url>
      <bibkey>wu-etal-2024-modeling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.183</doi>
    </paper>
    <paper id="184">
      <title>A <fixed-case>C</fixed-case>hinese Dataset for Evaluating the Safeguards in Large Language Models</title>
      <author><first>Yuxia</first><last>Wang</last></author>
      <author><first>Zenan</first><last>Zhai</last></author>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Xudong</first><last>Han</last><affiliation>University of Melbourne</affiliation></author>
      <author><first>Shom</first><last>Lin</last></author>
      <author><first>Zhenxuan</first><last>Zhang</last></author>
      <author><first>Angela</first><last>Zhao</last></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>3106-3119</pages>
      <abstract>Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks. Previous studies have proposed comprehensive taxonomies of LLM risks, as well as corresponding prompts that can be used to examine LLM safety. However, the focus has been almost exclusively on English. We aim to broaden LLM safety research by introducing a dataset for the safety evaluation of Chinese LLMs, and extending it to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments over five LLMs show that region-specific risks are the prevalent risk type. Warning: this paper contains example data that may be offensive, harmful, or biased. Our data is available at https://github.com/Libr-AI/do-not-answer.</abstract>
      <url hash="6bf4e5f8">2024.findings-acl.184</url>
      <bibkey>wang-etal-2024-chinese</bibkey>
      <doi>10.18653/v1/2024.findings-acl.184</doi>
    </paper>
    <paper id="185">
      <title><fixed-case>LLMF</fixed-case>actor: Extracting Profitable Factors through Prompts for Explainable Stock Movement Prediction</title>
      <author><first>Meiyun</first><last>Wang</last></author>
      <author><first>Kiyoshi</first><last>Izumi</last></author>
      <author><first>Hiroki</first><last>Sakaji</last><affiliation>Hokkaido University</affiliation></author>
      <pages>3120-3131</pages>
      <abstract>Recently, Large Language Models (LLMs) have attracted significant attention for their exceptional performance across a broad range of tasks, particularly in text analysis. However, the finance sector presents a distinct challenge due to its dependence on time-series data for complex forecasting tasks. In this study, we introduce a novel framework called LLMFactor, which employs Sequential Knowledge-Guided Prompting (SKGP) to identify factors that influence stock movements using LLMs. Unlike previous methods that relied on keyphrases or sentiment analysis, this approach focuses on extracting factors more directly related to stock market dynamics, providing clear explanations for complex temporal changes. Our framework directs the LLMs to create background knowledge through a fill-in-the-blank strategy and then discerns potential factors affecting stock prices from related news. Guided by background knowledge and identified factors, we leverage historical stock prices in textual format to predict stock movement. An extensive evaluation of the LLMFactor framework across four benchmark datasets from both the U.S. and Chinese stock markets demonstrates its superiority over existing state-of-the-art methods and its effectiveness in financial time-series forecasting.</abstract>
      <url hash="0bfa35a7">2024.findings-acl.185</url>
      <bibkey>wang-etal-2024-llmfactor</bibkey>
      <doi>10.18653/v1/2024.findings-acl.185</doi>
    </paper>
    <paper id="186">
      <title>You Only Look at Screens: Multimodal Chain-of-Action Agents</title>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Aston</first><last>Zhang</last><affiliation>Meta</affiliation></author>
      <pages>3132-3149</pages>
      <abstract>Autonomous graphical user interface (GUI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, most existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-GUI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique—leveraging a series of intermediate previous action histories and future action plans—to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30<tex-math>K</tex-math> unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-GUI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%. Code is publicly available at https://github.com/cooelf/Auto-GUI.</abstract>
      <url hash="ec63b39b">2024.findings-acl.186</url>
      <bibkey>zhang-zhang-2024-look</bibkey>
      <doi>10.18653/v1/2024.findings-acl.186</doi>
    </paper>
    <paper id="187">
      <title><tex-math>\rm SP^3</tex-math>: Enhancing Structured Pruning via <fixed-case>PCA</fixed-case> Projection</title>
      <author><first>Yuxuan</first><last>Hu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Zhe</first><last>Zhao</last></author>
      <author><first>Chen</first><last>Zhao</last></author>
      <author><first>Xiaodong</first><last>Chen</last></author>
      <author><first>Cuiping</first><last>Li</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Hong</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>3150-3170</pages>
      <abstract>Structured pruning is a widely used technique for reducing the size of pre-trained language models (PLMs), but current methods often overlook the potential of compressing the hidden dimension <tex-math>d</tex-math> in PLMs, a dimension critical to model size and efficiency. This paper introduces a novel structured pruning approach, Structured Pruning with PCA Projection (<tex-math>\rm SP^3</tex-math>), targeting the effective reduction of <tex-math>d</tex-math> by projecting features into a space defined by principal components before masking. Extensive experiments on benchmarks (GLUE and SQuAD) show that can reduce <tex-math>d</tex-math> by 70%, compress 94% of the <tex-math>\rm BERT_{base}</tex-math> model, and maintain over 96% accuracy and outperform other methods that compress <tex-math>d</tex-math> by 6% in accuracy at the same compression ratio. <tex-math>\rm SP^3</tex-math> has also proven effective with other models, including OPT and Llama.Our data and code are available at https://github.com/hyx1999/SP3</abstract>
      <url hash="61e54a84">2024.findings-acl.187</url>
      <bibkey>hu-etal-2024-sp3</bibkey>
      <doi>10.18653/v1/2024.findings-acl.187</doi>
    </paper>
    <paper id="188">
      <title><fixed-case>GENDEX</fixed-case>: Generative Data Augmentation Strategy Leveraging External Data for Abstractive Dialogue Summarization</title>
      <author><first>Sangwon</first><last>Park</last><affiliation>Gwangju Institute of Science and Technology</affiliation></author>
      <author><first>Hongseok</first><last>Choi</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <author><first>Dongha</first><last>Choi</last><affiliation>Gwangju Institute of Science and Technology</affiliation></author>
      <author><first>Hyunju</first><last>Lee</last><affiliation>Gwangju Institute of Science and Technology</affiliation></author>
      <pages>3171-3185</pages>
      <abstract>With the proliferation of digital communication, dialogue summarization has become increasingly important. However, it still faces a shortage of data. To address this issue, we developed **Gen**erative **D**ata Augmentation Strategy Leveraging **Ex**ternal Data for Abstractive Dialogue Summarization (**GENDEX**), which is based on the hypothetical foundation that texts containing people and their interpersonal interactions can potentially serve as summaries of corresponding dialogues. We filter short texts containing people and resolve coreferences for better contextual analysis. We then identify the semantic roles of words within the texts and filter them based on the patterns observed in the dialogue summarization datasets. Using these texts, we generate synthetic dialogues through a controlled generation method. To better leverage the augmented data, we utilize noise-tolerant training to fine-tune the summarization model. The experimental results demonstrate the effectiveness of our proposed method, showing its robust performance, generalizability, and scalability. Moreover, performance improvements by *GENDEX* were observed regardless of complexity of dialogues. The code is available at https://github.com/DMCB-GIST/GENDEX.</abstract>
      <url hash="30d8cbe4">2024.findings-acl.188</url>
      <bibkey>park-etal-2024-gendex</bibkey>
      <doi>10.18653/v1/2024.findings-acl.188</doi>
    </paper>
    <paper id="189">
      <title>Concept-Best-Matching: Evaluating Compositionality In Emergent Communication</title>
      <author><first>Boaz</first><last>Carmeli</last><affiliation>Technion - Israel Institute of Technology and International Business Machines</affiliation></author>
      <author><first>Yonatan</first><last>Belinkov</last><affiliation>Technion, Technion</affiliation></author>
      <author><first>Ron</first><last>Meir</last><affiliation>Technion, Technion</affiliation></author>
      <pages>3186-3194</pages>
      <abstract>Artificial agents that learn to communicate in order to accomplish a given task acquire communication protocols that are typically opaque to a human. A large body of work has attempted to evaluate the emergent communication via various evaluation measures, with **compositionality** featuring as a prominent desired trait. However, current evaluation procedures do not directly expose the compositionality of the emergent communication. We propose a procedure to assess the compositionality of emergent communication by finding the best-match between emerged words and natural language concepts.The best-match algorithm provides both a global score and a translation-map from emergent words to natural language concepts. To the best of our knowledge, it is the first time that such direct and interpretable mapping between emergent words and human concepts is provided.</abstract>
      <url hash="d2738d5e">2024.findings-acl.189</url>
      <bibkey>carmeli-etal-2024-concept</bibkey>
      <doi>10.18653/v1/2024.findings-acl.189</doi>
    </paper>
    <paper id="190">
      <title>A Tale of Two Revisions: Summarizing Changes Across Document Versions</title>
      <author><first>Santosh</first><last>T.y.s.s</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Natwar</first><last>Modani</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Apoorv</first><last>Saxena</last><affiliation>Adobe Systems</affiliation></author>
      <pages>3195-3211</pages>
      <abstract>Document revision is a crucial aspect of the writing process, particularly in collaborative environments where multiple authors contribute simultaneously. However, current tools lack an efficient way to provide a comprehensive overview of changes between versions, leading to difficulties in understanding revisions. To address this, we propose a novel task of providing thematic summary of changes between document versions, organizing individual edits based on shared themes. We assess capabilities of LLMs on this task and further introduce three strategies to tackle this task: (i) representing the input of two documents along with edits in the ‘diff’ format (ii) a two-stage task decomposition with individual edit description generation as an intermediate task and (iii) clustering based chunking and subsequent merging techniques for handling longer documents. Our experiments demonstrate the effectiveness of our approach in improving the model’s capacity to handle this complex task. Additionally, we introduce ChangeSumm, a curated dataset comprising human-written thematic summaries for pairs of document versions, to facilitate evaluation and further research in this direction.</abstract>
      <url hash="b8886c5a">2024.findings-acl.190</url>
      <bibkey>t-y-s-s-etal-2024-tale</bibkey>
      <doi>10.18653/v1/2024.findings-acl.190</doi>
    </paper>
    <paper id="191">
      <title>Refine, Align, and Aggregate: Multi-view Linguistic Features Enhancement for Aspect Sentiment Triplet Extraction</title>
      <author><first>Guixin</first><last>Su</last></author>
      <author><first>Mingmin</first><last>Wu</last></author>
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <author><first>Yongcheng</first><last>Zhang</last></author>
      <author><first>Tongguan</first><last>Wang</last></author>
      <author><first>Yuxue</first><last>Hu</last></author>
      <author><first>Ying</first><last>Sha</last></author>
      <pages>3212-3228</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) aims to extract the triplets of aspect terms, their associated sentiment and opinion terms. Previous works based on different modeling paradigms have achieved promising results. However, these methods struggle to comprehensively explore the various specific relations between sentiment elements in multi-view linguistic features, which is the prior indication effect for facilitating sentiment triplets extraction, requiring to align and aggregate them to capture the complementary higher-order interactions. In this paper, we propose Multi-view Linguistic Features Enhancement (MvLFE) to explore the aforementioned prior indication effect in the “Refine, Align, and Aggregate” learning process. Specifically, we first introduce the relational graph attention network to encode the word-pair relations represented by each linguistic feature and refine them to pay more attention to the aspect-opinion pairs. Next, we employ the multi-view contrastive learning to align them at a fine-grained level in the contextual semantic space to maintain semantic consistency. Finally, we utilize the multi-semantic cross attention to capture and aggregate the complementary higher-order interactions between diverse linguistic features to enhance the aspect-opinion relations. Experimental results on several benchmark datasets show the effectiveness and robustness of our model, which achieves state-of-the-art performance.</abstract>
      <url hash="2ce02f08">2024.findings-acl.191</url>
      <bibkey>su-etal-2024-refine</bibkey>
      <doi>10.18653/v1/2024.findings-acl.191</doi>
    </paper>
    <paper id="192">
      <title>Pro-Woman, Anti-Man? Identifying Gender Bias in Stance Detection</title>
      <author><first>Yingjie</first><last>Li</last><affiliation>Westlake University</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>3229-3236</pages>
      <abstract>Gender bias has been widely observed in NLP models, which has the potential to perpetuate harmful stereotypes and discrimination. In this paper, we construct a dataset GenderStance of 36k samples to measure gender bias in stance detection, determining whether models consistently predict the same stance for a particular gender group. We find that all models are gender-biased and prone to classify sentences that contain male nouns as Against and those with female nouns as Favor. Moreover, extensive experiments indicate that sources of gender bias stem from the fine-tuning data and the foundation model itself. We will publicly release our code and dataset.</abstract>
      <url hash="cb065181">2024.findings-acl.192</url>
      <bibkey>li-zhang-2024-pro</bibkey>
      <doi>10.18653/v1/2024.findings-acl.192</doi>
    </paper>
    <paper id="193">
      <title>Likelihood-based Mitigation of Evaluation Bias in Large Language Models</title>
      <author><first>Masanari</first><last>Ohi</last></author>
      <author><first>Masahiro</first><last>Kaneko</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Ryuto</first><last>Koike</last></author>
      <author><first>Mengsay</first><last>Loem</last><affiliation>Sansan, Inc.</affiliation></author>
      <author><first>Naoaki</first><last>Okazaki</last><affiliation>Tokyo Institute of Technology</affiliation></author>
      <pages>3237-3245</pages>
      <abstract>Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics.However, the likelihood, a measure of LLM’s plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure.It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods.In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators.We also propose a method to mitigate the likelihood bias.Our method utilizes highly biased instances as few-shot examples for in-context learning.Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias.Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly.</abstract>
      <url hash="f5254ea6">2024.findings-acl.193</url>
      <bibkey>ohi-etal-2024-likelihood</bibkey>
      <doi>10.18653/v1/2024.findings-acl.193</doi>
    </paper>
    <paper id="194">
      <title>The Music Maestro or The Musically Challenged, A Massive Music Evaluation Benchmark for Large Language Models</title>
      <author><first>Jiajia</first><last>Li</last></author>
      <author><first>Lu</first><last>Yang</last></author>
      <author><first>Mingni</first><last>Tang</last></author>
      <author><first>Chenchong</first><last>Chenchong</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Ping</first><last>Wang</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>3246-3257</pages>
      <abstract>Benchmark plays a pivotal role in assessing the advancements of large language models (LLMs). While numerous benchmarks have been proposed to evaluate LLMs’ capabilities, there is a notable absence of a dedicated benchmark for assessing their musical abilities. To address this gap, we present ZIQI-Eval, a comprehensive and large-scale music benchmark specifically designed to evaluate the music-related capabilities of LLMs.ZIQI-Eval encompasses a wide range of questions, covering 10 major categories and 56 subcategories, resulting in over 14,000 meticulously curated data entries. By leveraging ZIQI-Eval, we conduct a comprehensive evaluation over 16 LLMs to evaluate and analyze LLMs’ performance in the domain of music.Results indicate that all LLMs perform poorly on the ZIQI-Eval benchmark, suggesting significant room for improvement in their musical capabilities.With ZIQI-Eval, we aim to provide a standardized and robust evaluation framework that facilitates a comprehensive assessment of LLMs’ music-related abilities. The dataset is available at GitHub and HuggingFace.</abstract>
      <url hash="0b20b82b">2024.findings-acl.194</url>
      <bibkey>li-etal-2024-music</bibkey>
      <doi>10.18653/v1/2024.findings-acl.194</doi>
    </paper>
    <paper id="195">
      <title><fixed-case>P</fixed-case>yramid<fixed-case>I</fixed-case>nfer: Pyramid <fixed-case>KV</fixed-case> Cache Compression for High-throughput <fixed-case>LLM</fixed-case> Inference</title>
      <author><first>Dongjie</first><last>Yang</last></author>
      <author><first>Xiaodong</first><last>Han</last><affiliation>xiaohongshu</affiliation></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Yao</first><last>Hu</last></author>
      <author><first>Shilin</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>3258-3270</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache.</abstract>
      <url hash="9948d24d">2024.findings-acl.195</url>
      <bibkey>yang-etal-2024-pyramidinfer</bibkey>
      <doi>10.18653/v1/2024.findings-acl.195</doi>
    </paper>
    <paper id="196">
      <title>From Role-Play to Drama-Interaction: An <fixed-case>LLM</fixed-case> Solution</title>
      <author><first>Weiqi</first><last>Wu</last></author>
      <author><first>Hongqiu</first><last>Wu</last></author>
      <author><first>Lai</first><last>Jiang</last></author>
      <author><first>Xingyuan</first><last>Liu</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>3271-3290</pages>
      <abstract>Drama is a form of storytelling inspired by human creativity, proceeding with a predefined storyline, carrying emotions and thoughts.This paper introduces LLM-based interactive drama, which endows traditional drama with an unprecedented immersion, where a person is allowed to walk into it and interact with the characters and scenes.We define this new artistic genre by 6 essential elements—plot, character, thought, diction, spectacle and interaction—and study the entire pipeline to forge a backbone drama LLM to drive the playing process, which is challenged by limited drama resources, uncontrollable narrative development, and complicated instruction following.We propose Narrative Chain to offer finer control over the narrative progression during interaction with players;Auto-Drama to synthesize drama scripts given arbitrary stories;Sparse Instruction Tuning to allow the model to follow sophisticated instructions.We manually craft 3 scripts, Detective Conan, Harry Potter, Romeo and Juliet, and design a 5-dimension principle to evaluate the drama LLM comprehensively.</abstract>
      <url hash="4784346b">2024.findings-acl.196</url>
      <bibkey>wu-etal-2024-role</bibkey>
      <doi>10.18653/v1/2024.findings-acl.196</doi>
    </paper>
    <paper id="197">
      <title><fixed-case>T</fixed-case>ime<fixed-case>C</fixed-case>hara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models</title>
      <author><first>Jaewoo</first><last>Ahn</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Taehyun</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Junyoung</first><last>Lim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jin-Hwa</first><last>Kim</last><affiliation>Seoul National University and NAVER</affiliation></author>
      <author><first>Sangdoo</first><last>Yun</last><affiliation>NAVER</affiliation></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>3291-3325</pages>
      <abstract>While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users’ narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters’ identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.</abstract>
      <url hash="dd253253">2024.findings-acl.197</url>
      <bibkey>ahn-etal-2024-timechara</bibkey>
      <doi>10.18653/v1/2024.findings-acl.197</doi>
    </paper>
    <paper id="198">
      <title>Red Teaming Visual Language Models</title>
      <author><first>Mukai</first><last>Li</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Yuwei</first><last>Yin</last></author>
      <author><first>Masood</first><last>Ahmed</last></author>
      <author><first>Zhenguang</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Hong Kong</affiliation></author>
      <pages>3326-3342</pages>
      <abstract>VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 12 subtasks (e.g., image misleading, multi-modal jailbreaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models’ performance with 10% in RTVLM test set, 13% in MM-hallu, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models in similar size with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-sourced.</abstract>
      <url hash="0268fc05">2024.findings-acl.198</url>
      <bibkey>li-etal-2024-red</bibkey>
      <doi>10.18653/v1/2024.findings-acl.198</doi>
    </paper>
    <paper id="199">
      <title>Enhancing Semantic Consistency of Large Language Models through Model Editing: An Interpretability-Oriented Approach</title>
      <author><first>Jingyuan</first><last>Yang</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Dapeng</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yajing</first><last>Sun</last></author>
      <author><first>Rongjun</first><last>Li</last></author>
      <author><first>Zhiyong</first><last>Feng</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Wei</first><last>Peng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>3343-3353</pages>
      <abstract>A Large Language Model (LLM) tends to generate inconsistent and sometimes contradictory outputs when presented with a prompt that has equivalent semantics but is expressed differently from the original prompt. To achieve semantic consistency of an LLM, one of the key approaches is to finetune the model with prompt-output pairs with semantically equivalent meanings. Despite its effectiveness, a data-driven finetuning method incurs substantial computation costs in data preparation and model optimization. In this regime, an LLM is treated as a “black box”, restricting our ability to gain deeper insights into its internal mechanism. In this paper, we are motivated to enhance the semantic consistency of LLMs through a more interpretable method (i.e., model editing) to this end. We first identify the model components (i.e., attention heads) that have a key impact on the semantic consistency of an LLM. We subsequently inject biases into the output of these model components along the semantic-consistency activation direction. It is noteworthy that these modifications are cost-effective, without reliance on mass manipulations of the original model parameters. Through comprehensive experiments on the constructed NLU and open-source NLG datasets, our method demonstrates significant improvements in the semantic consistency and task performance of LLMs. Additionally, our method exhibits promising generalization capabilities by performing well on tasks beyond the primary tasks.</abstract>
      <url hash="1a63391e">2024.findings-acl.199</url>
      <bibkey>yang-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.199</doi>
    </paper>
    <paper id="200">
      <title>Semantic Skill Grounding for Embodied Instruction-Following in Cross-Domain Environments</title>
      <author><first>Sangwoo</first><last>Shin</last><affiliation>Sungkyunkwan University</affiliation></author>
      <author><first>SeungHyun</first><last>Kim</last></author>
      <author><first>Youngsoo</first><last>Jang</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Moontae</first><last>Lee</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Honguk</first><last>Woo</last></author>
      <pages>3354-3376</pages>
      <abstract>In embodied instruction-following (EIF), the integration of pretrained language models (LMs) as task planners emerges as a significant branch, where tasks are planned at the skill level by prompting LMs with pretrained skills and user instructions. However, grounding these pretrained skills in different domains remains challenging due to their intricate entanglement with the domain-specific knowledge. To address this challenge, we present a semantic skill grounding (SemGro) framework that leverages the hierarchical nature of semantic skills. SemGro recognizes the broad spectrum of these skills, ranging from short-horizon low-semantic skills that are universally applicable across domains to long-horizon rich-semantic skills that are highly specialized and tailored for particular domains. The framework employs an iterative skill decomposition approach, starting from the higher levels of semantic skill hierarchy and then moving downwards, so as to ground each planned skill to an executable level within the target domain. To do so, we use the reasoning capabilities of LMs for composing and decomposing semantic skills, as well as their multi-modal extension for assessing the skill feasibility in the target domain. Our experiments in the VirtualHome benchmark show the efficacy of SemGro in 300 cross-domain EIF scenarios.</abstract>
      <url hash="525ef913">2024.findings-acl.200</url>
      <bibkey>shin-etal-2024-semantic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.200</doi>
    </paper>
    <paper id="201">
      <title><fixed-case>LIRE</fixed-case>: listwise reward enhancement for preference alignment</title>
      <author><first>Mingye</first><last>Zhu</last></author>
      <author><first>Yi</first><last>Liu</last><affiliation>State Key Laboratory of Communication Content Cognition</affiliation></author>
      <author><first>Lei</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Junbo</first><last>Guo</last><affiliation>People’s Daily Online</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>3377-3394</pages>
      <abstract>Recently, tremendous strides have been made to align the generation of Large Language Models (LLMs) with human values to mitigate toxic or unhelpful content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves effective and is widely adopted by researchers. However, implementing RLHF is complex, and its sensitivity to hyperparameters renders achieving stable performance and scalability challenging. Furthermore, prevailing approaches to preference alignment primarily concentrate on pairwise comparisons, with limited exploration into multi-response scenarios, thereby overlooking the potential richness within the candidate pool. For the above reasons, we propose a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a gradient-based reward optimization approach that incorporates the offline rewards of multiple responses into a streamlined listwise framework, thus eliminating the need for online sampling during training. LIRE is straightforward to implement, requiring minimal parameter tuning, and seamlessly aligns with the pairwise paradigm while naturally extending to multi-response scenarios. Moreover, we introduce a self-enhancement algorithm aimed at iteratively refining the reward during training. Our experiments demonstrate that LIRE consistently outperforms existing methods across several benchmarks on dialogue and summarization tasks, with good transferability to out-of-distribution data, assessed using proxy reward models and human annotators.</abstract>
      <url hash="f4aee39e">2024.findings-acl.201</url>
      <bibkey>zhu-etal-2024-lire</bibkey>
      <doi>10.18653/v1/2024.findings-acl.201</doi>
    </paper>
    <paper id="202">
      <title>See It All: Contextualized Late Aggregation for 3<fixed-case>D</fixed-case> Dense Captioning</title>
      <author><first>Minjung</first><last>Kim</last></author>
      <author><first>Hyung</first><last>Lim</last><affiliation>Diquest</affiliation></author>
      <author><first>Seung Hwan</first><last>Kim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Soonyoung</first><last>Lee</last></author>
      <author><first>Bumsoo</first><last>Kim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Gunhee</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <pages>3395-3405</pages>
      <abstract>3D dense captioning is a task to localize objects in a 3D scene and generate descriptive sentences for each object. Recent approaches in 3D dense captioning have adopted transformer encoder-decoder frameworks from object detection to build an end-to-end pipeline without hand-crafted components. However, these approaches struggle with contradicting objectives where a single query attention has to simultaneously view both the tightly localized object regions and contextual environment. To overcome this challenge, we introduce SIA (See-It-All), a transformer pipeline that engages in 3D dense captioning with a novel paradigm called late aggregation. SIA simultaneously decodes two sets of queries—context query and instance query. The instance query focuses on localization and object attribute descriptions, while the context query versatilely captures the region-of-interest of relationships between multiple objects or with the global scene, then aggregated afterwards (i.e., late aggregation) via simple distance-based measures. To further enhance the quality of contextualized caption generation, we design a novel aggregator to generate a fully informed caption based on the surrounding context, the global environment, and object instances. Extensive experiments on two of the most widely-used 3D dense captioning datasets demonstrate that our proposed method achieves a significant improvement over prior methods.</abstract>
      <url hash="205d10e9">2024.findings-acl.202</url>
      <bibkey>kim-etal-2024-see</bibkey>
      <doi>10.18653/v1/2024.findings-acl.202</doi>
    </paper>
    <paper id="203">
      <title><tex-math>\texttt{DARA}</tex-math>: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs</title>
      <author><first>Haishuo</first><last>Fang</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Xiaodan</first><last>Zhu</last><affiliation>Queen’s University</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <pages>3406-3432</pages>
      <abstract>Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning autonomous language agents in various real-life applications. To improve the neural-symbolic reasoning capabilities of language agents powered by Large Language Models (LLMs) in KGQA, we propose the Decomposition-Alignment-Reasoning Agent (DARA) framework. DARA effectively parses questions into formal queries through a dual mechanism: high-level iterative task decomposition and low-level task grounding. Importantly, DARA can be efficiently trained with a small number of high-quality reasoning trajectories. Our experimental results demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms both in-context learning-based agents with GPT-4 and alternative fine-tuned agents, across different benchmarks, making such models more accessible for real-life applications. We also show that DARA attains performance comparable to state-of-the-art enumerating-and-ranking-based methods for KGQA.</abstract>
      <url hash="cae34f96">2024.findings-acl.203</url>
      <bibkey>fang-etal-2024-dara</bibkey>
      <doi>10.18653/v1/2024.findings-acl.203</doi>
    </paper>
    <paper id="204">
      <title><fixed-case>GKT</fixed-case>: A Novel Guidance-Based Knowledge Transfer Framework For Efficient Cloud-edge Collaboration <fixed-case>LLM</fixed-case> Deployment</title>
      <author id="yao-yao"><first>Yao</first><last>Yao</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>3433-3446</pages>
      <abstract>The burgeoning size of Large Language Models (LLMs) has led to enhanced capabilities in generating responses, albeit at the expense of increased inference times and elevated resource demands. Existing methods of acceleration, predominantly hinged on knowledge distillation, generally necessitate fine-tuning of considerably large models, such as Llama-7B, posing a challenge for average users. Furthermore, present techniques for expediting inference and reducing costs operate independently. To address these issues, we introduce a novel and intuitive Guidance-based Knowledge Transfer (GKT) framework. This approach leverages a larger LLM as a ”teacher” to create guidance prompts, paired with a smaller ”student” model to finalize responses. Remarkably, GKT requires no fine-tuning and doesn’t necessitate the teacher and student models to have the same vocabulary, allowing for extensive batch generation to accelerate the process while ensuring user customization. GKT can be seamlessly integrated into cloud-edge collaboration architectures, and is versatile enough for plug-and-play application across various models. It excels in both efficiency and affordability, epitomizing a ”cheap and cheerful” solution. GKT achieves a maximum accuracy improvement of 14.18%, along with a 10.72 times speed-up on GSM8K and an accuracy improvement of 14.00 % along with a 7.73 times speed-up in CSQA. When utilizing ChatGPT as teacher model and Llama2-70B as the student model, we can achieve 95.00% of ChatGPT’s performance at 52% of the cost. The results highlight substantial enhancements in accuracy and processing speed on the GSM8K and CSQA datasets, surpassing the performance of using either the student or teacher models in isolation.</abstract>
      <url hash="7bdea6f4">2024.findings-acl.204</url>
      <bibkey>yao-etal-2024-gkt</bibkey>
      <doi>10.18653/v1/2024.findings-acl.204</doi>
    </paper>
    <paper id="205">
      <title>Compositional Generalization with Grounded Language Models</title>
      <author><first>Sondre</first><last>Wold</last></author>
      <author><first>Étienne</first><last>Simon</last></author>
      <author><first>Lucas</first><last>Charpentier</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Egor</first><last>Kostylev</last><affiliation>University of Oslo, Norway</affiliation></author>
      <author><first>Erik</first><last>Velldal</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Lilja</first><last>Øvrelid</last><affiliation>Dept. of Informatics, University of Oslo</affiliation></author>
      <pages>3447-3460</pages>
      <abstract>Grounded language models use external sources of information, such as knowledge graphs, to meet some of the general challenges associated with pre-training. By extending previous work on compositional generalization in semantic parsing, we allow for a controlled evaluation of the degree to which these models learn and generalize from patterns in knowledge graphs. We develop a procedure for generating natural language questions paired with knowledge graphs that targets different aspects of compositionality and further avoids grounding the language models in information already encoded implicitly in their weights. We evaluate existing methods for combining language models with knowledge graphs and find them to struggle with generalization to sequences of unseen lengths and to novel combinations of seen base components. While our experimental results provide some insight into the expressive power of these models, we hope our work and released datasets motivate future research on how to better combine language models with structured knowledge representations.</abstract>
      <url hash="da4a1de3">2024.findings-acl.205</url>
      <bibkey>wold-etal-2024-compositional</bibkey>
      <doi>10.18653/v1/2024.findings-acl.205</doi>
    </paper>
    <paper id="206">
      <title>Rethinking Negative Instances for Generative Named Entity Recognition</title>
      <author><first>Yuyang</first><last>Ding</last></author>
      <author><first>Juntao</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Pinzheng</first><last>Wang</last></author>
      <author><first>Zecheng</first><last>Tang</last><affiliation>Soochow University</affiliation></author>
      <author><first>Yan</first><last>Bowen</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>3461-3475</pages>
      <abstract>Large Language Models (LLMs) have demonstrated impressive capabilities for generalizing in unseen tasks. In the Named Entity Recognition (NER) task, recent advancements have seen the remarkable improvement of LLMs in a broad range of entity domains via instruction tuning, by adopting entity-centric schema. In this work, we explore the potential enhancement of the existing methods by incorporating negative instances into training. Our experiments reveal that negative instances contribute to remarkable improvements by (1) introducing contextual information, and (2) clearly delineating label boundaries. Furthermore, we introduce an efficient longest common subsequence (LCS) matching algorithm, which is tailored to transform unstructured predictions into structured entities. By integrating these components, we present GNER, a Generative NER system that shows improved zero-shot performance across unseen entity domains. Our comprehensive evaluation illustrates our system’s superiority, surpassing state-of-the-art (SoTA) methods by 9 <tex-math>F_1</tex-math> score in zero-shot evaluation.</abstract>
      <url hash="5193969d">2024.findings-acl.206</url>
      <bibkey>ding-etal-2024-rethinking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.206</doi>
    </paper>
    <paper id="207">
      <title><fixed-case>W</fixed-case>il<fixed-case>KE</fixed-case>: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing</title>
      <author><first>Chenhui</first><last>Hu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Pengfei</first><last>Cao</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yubo</first><last>Chen</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>3476-3503</pages>
      <abstract>Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named Wise-Layer Knowledge Editor (WilKE), which selects editing layer based on the pattern matching degree of editing knowledge across different layers in language models. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.</abstract>
      <url hash="efb6b70e">2024.findings-acl.207</url>
      <bibkey>hu-etal-2024-wilke</bibkey>
      <doi>10.18653/v1/2024.findings-acl.207</doi>
    </paper>
    <paper id="208">
      <title><fixed-case>DINER</fixed-case>: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference</title>
      <author><first>Jialong</first><last>Wu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Deyu</first><last>Zhou</last><affiliation>Southeast University</affiliation></author>
      <author><first>Guoqiang</first><last>Xu</last></author>
      <pages>3504-3518</pages>
      <abstract>Though notable progress has been made, neural-based aspect-based sentiment analysis (ABSA) models are prone to learn spurious correlations from annotation biases, resulting in poor robustness on adversarial data transformations. Among the debiasing solutions, causal inference-based methods have attracted much research attention, which can be mainly categorized into causal intervention methods and counterfactual reasoning methods. However, most of the present debiasing methods focus on single-variable causal inference, which is not suitable for ABSA with two input variables (the target aspect and the review). In this paper, we propose a novel framework based on multi-variable causal inference for debiasing ABSA. In this framework, different types of biases are tackled based on different causal intervention methods. For the review branch, the bias is modeled as indirect confounding from context, where backdoor adjustment intervention is employed for debiasing. For the aspect branch, the bias is described as a direct correlation with labels, where counterfactual reasoning is adopted for debiasing. Extensive experiments demonstrate the effectiveness of the proposed method compared to various baselines on the two widely used real-world aspect robustness test set datasets.</abstract>
      <url hash="6e987dc0">2024.findings-acl.208</url>
      <bibkey>wu-etal-2024-diner</bibkey>
      <doi>10.18653/v1/2024.findings-acl.208</doi>
    </paper>
    <paper id="209">
      <title><fixed-case>STAR</fixed-case>: Constraint <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models</title>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Jialong</first><last>Wu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Deyu</first><last>Zhou</last><affiliation>Southeast University</affiliation></author>
      <author><first>Guoqiang</first><last>Xu</last></author>
      <pages>3519-3532</pages>
      <abstract>Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the uncertainty gap, we introduce a dynamic uncertainty measurement that combines the uncertainty of the base model and the uncertainty of the full model during the iteration of active learning. For poor model calibration, we incorporate the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation. Experimental results show that the proposed approach outperforms existing baseline models on three complex reasoning tasks.</abstract>
      <url hash="06b7008e">2024.findings-acl.209</url>
      <bibkey>zhang-etal-2024-star</bibkey>
      <doi>10.18653/v1/2024.findings-acl.209</doi>
    </paper>
    <paper id="210">
      <title>How Much Does Nonverbal Communication Conform to Entropy Rate Constancy?: A Case Study on Listener Gaze in Interaction</title>
      <author><first>Yu</first><last>Wang</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Yang</first><last>Xu</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Gabriel</first><last>Skantze</last><affiliation>KTH Royal Institute of Technology, Stockholm, Sweden</affiliation></author>
      <author><first>Hendrik</first><last>Buschmeier</last><affiliation>Universität Bielefeld</affiliation></author>
      <pages>3533-3545</pages>
      <abstract>According to the Entropy Rate Constancy (ERC) principle, the information density of a text is approximately constant over its length. Whether this principle also applies to nonverbal communication signals is still under investigation. We perform empirical analyses of video-recorded dialogue data and investigate whether listener gaze, as an important nonverbal communication signal, adheres to the ERC principle. Results show (1) that the ERC principle holds for listener gaze; and (2) that the two linguistic factors syntactic complexity and turn transition potential are weakly correlated with local entropy of listener gaze.</abstract>
      <url hash="e8959744">2024.findings-acl.210</url>
      <bibkey>wang-etal-2024-much</bibkey>
      <revision id="1" href="2024.findings-acl.210v1" hash="0416e6e5"/>
      <revision id="2" href="2024.findings-acl.210v2" hash="e8959744" date="2024-08-29">Minor updates.</revision>
      <doi>10.18653/v1/2024.findings-acl.210</doi>
    </paper>
    <paper id="211">
      <title>Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation</title>
      <author><first>Xu</first><last>Huang</last></author>
      <author><first>Zhirui</first><last>Zhang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xiang</first><last>Geng</last></author>
      <author><first>Yichao</first><last>Du</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <pages>3546-3562</pages>
      <abstract>This study investigates how Large Language Models (LLMs) leverage source and reference data in machine translation evaluation task, aiming to better understand the mechanisms behind their remarkable performance in this task.We design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information.We find that reference information significantly enhances the evaluation accuracy, while surprisingly, source information sometimes is counterproductive, indicating LLMs’ inability to fully leverage the cross-lingual capability when evaluating translations.Further analysis of the fine-grained evaluation and fine-tuning experiments show similar results.These findings also suggest a potential research direction for LLMs that fully exploits the cross-lingual capability of LLMs to achieve better performance in machine translation evaluation tasks.</abstract>
      <url hash="60822c68">2024.findings-acl.211</url>
      <bibkey>huang-etal-2024-lost</bibkey>
      <doi>10.18653/v1/2024.findings-acl.211</doi>
    </paper>
    <paper id="212">
      <title>Chain-of-Verification Reduces Hallucination in Large Language Models</title>
      <author><first>Shehzaad</first><last>Dhuliawala</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Mojtaba</first><last>Komeili</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Jing</first><last>Xu</last><affiliation>Facebook AI Research</affiliation></author>
      <author><first>Roberta</first><last>Raileanu</last><affiliation>Facebook</affiliation></author>
      <author><first>Xian</first><last>Li</last><affiliation>Facebook AI</affiliation></author>
      <author><first>Asli</first><last>Celikyilmaz</last><affiliation>FAIR</affiliation></author>
      <author><first>Jason</first><last>Weston</last><affiliation>New York University and Facebook</affiliation></author>
      <pages>3563-3578</pages>
      <abstract>Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.</abstract>
      <url hash="d6cf66c6">2024.findings-acl.212</url>
      <bibkey>dhuliawala-etal-2024-chain</bibkey>
      <doi>10.18653/v1/2024.findings-acl.212</doi>
    </paper>
    <paper id="213">
      <title>Measuring Bargaining Abilities of <fixed-case>LLM</fixed-case>s: A Benchmark and A Buyer-Enhancement Method</title>
      <author><first>Tian</first><last>Xia</last></author>
      <author><first>Zhiwei</first><last>He</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Tong</first><last>Ren</last></author>
      <author><first>Yibo</first><last>Miao</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>3579-3602</pages>
      <abstract>Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents’ bargaining abilities remains an open problem.For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent’s performance in the Bargain task.We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents’ bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer’s performance.To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer’s offers, and an LLM Narrator to create natural language sentences for generated offers.Experimental results show that OG-Narrator improves the buyer’s deal rates from 26.67% to 88.88% and brings a ten times multiplication of profits on all baselines, even a model that has not been aligned.</abstract>
      <url hash="c61dead4">2024.findings-acl.213</url>
      <bibkey>xia-etal-2024-measuring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.213</doi>
    </paper>
    <paper id="214">
      <title><fixed-case>D</fixed-case>ev<fixed-case>E</fixed-case>val: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories</title>
      <author><first>Jia</first><last>Li</last></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University Shenzhen Graduate School</affiliation></author>
      <author><first>Yunfei</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Yongmin</first><last>Li</last></author>
      <author><first>Huanyu</first><last>Liu</last></author>
      <author><first>Hao</first><last>Zhu</last></author>
      <author><first>Lecheng</first><last>Wang</last></author>
      <author><first>Kaibo</first><last>Liu</last></author>
      <author><first>Zheng</first><last>Fang</last><affiliation>Peking University</affiliation></author>
      <author><first>Lanshen</first><last>Wang</last></author>
      <author><first>Jiazheng</first><last>Ding</last><affiliation>Peking University</affiliation></author>
      <author><first>Xuanming</first><last>Zhang</last></author>
      <author><first>Yuqi</first><last>Zhu</last></author>
      <author><first>Yihong</first><last>Dong</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University and Peking University</affiliation></author>
      <author><first>Binhua</first><last>Li</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Bin</first><last>Gu</last><affiliation>Beijing Institute of Control Engineering</affiliation></author>
      <author><first>Mengfei</first><last>Yang</last><affiliation>China Academy of Space Technology</affiliation></author>
      <pages>3603-3614</pages>
      <abstract>How to evaluate the coding abilities of Large Language Models (LLMs) remains an open question. We find that existing benchmarks are poorly aligned with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs.To address the knowledge gap, we propose a new benchmark named DevEval, which has three advances. (1) DevEval aligns with real-world repositories in multiple dimensions, e.g., code and dependency distributions. (2) DevEval is annotated by 13 developers and contains comprehensive annotations (e.g., requirements, original repositories, reference code, and reference dependencies). (3) DevEval comprises 1,825 testing samples from 115 repositories, covering 10 popular domains (e.g., Internet, Database). Based on DevEval, we propose repository-level code generation and evaluate 8 popular LLMs on DevEval (e.g., gpt-4, gpt-3.5, StarCoder 2, DeepSeek Coder, CodeLLaMa). Our experiments reveal these LLMs’ coding abilities in real-world code repositories. For example, the highest Pass@1 of gpt-4 only is 53.04% in our experiments. We also analyze LLMs’ failed cases and summarize their shortcomings. We hope DevEval can facilitate the development of LLMs in real code repositories. DevEval, prompts, and LLMs’ predictions have been released.</abstract>
      <url hash="6b56d728">2024.findings-acl.214</url>
      <bibkey>li-etal-2024-deveval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.214</doi>
    </paper>
    <paper id="215">
      <title><fixed-case>LPNL</fixed-case>: Scalable Link Prediction with Large Language Models</title>
      <author><first>Baolong</first><last>Bi</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shenghua</first><last>Liu</last></author>
      <author><first>Yiwei</first><last>Wang</last></author>
      <author><first>Lingrui</first><last>Mei</last></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <pages>3615-3625</pages>
      <abstract>Exploring the application of large language models (LLMs) to graph learning is an emerging endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to graph learning with LLMs. This work focuses on the link prediction task and introduces **LPNL** (Link Prediction via Natural Language), a framework based on large language models designed for scalable link prediction on large-scale heterogeneous graphs. We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from the graphs, and a divide-and-conquer strategy to control the input tokens within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for link prediction. Extensive experimental results demonstrate that LPNL outperforms multiple advanced baselines in link prediction tasks on large-scale graphs.</abstract>
      <url hash="8cc22c52">2024.findings-acl.215</url>
      <bibkey>bi-etal-2024-lpnl</bibkey>
      <doi>10.18653/v1/2024.findings-acl.215</doi>
    </paper>
    <paper id="216">
      <title>Aligning Speech Segments Beyond Pure Semantics</title>
      <author><first>Kevin</first><last>Heffernan</last><affiliation>Facebook</affiliation></author>
      <author><first>Artyom</first><last>Kozhevnikov</last></author>
      <author><first>Loic</first><last>Barrault</last></author>
      <author><first>Alexandre</first><last>Mourachko</last><affiliation>Research, Facebook</affiliation></author>
      <author><first>Holger</first><last>Schwenk</last></author>
      <pages>3626-3635</pages>
      <abstract>Multilingual parallel data for speech-to-speech translation is scarce and expensive to create from scratch. This is all the more true for expressive speech translation, which aims at preserving not only the semantics, but also the overall prosody (e.g. style, emotion, rate-of-speech). Existing corpora contain speech utterances with the same meaning, yet the overall prosody is typically different, as human annotators are not tasked with reproducing these aspects, or crowed-sourced efforts do not specifically target this kind of alignment in priority. In this paper, we propose a novel alignment algorithm, which automatically forms pairs of speech segments aligned not only in meaning, but also in expressivity. In order to validate our approach, we train an expressive multilingual speech-to-speech translation system on the automatically aligned data. Our experiments show that in comparison to semantic-only approaches, expressively aligned data yields large improvements in source expressivity preservation (e.g. 43% uplift in speech rate preservation on average), while still maintaining content translation quality. In some scenarios, results also indicate that this alignment algorithm can outperform standard, semantic-focused approaches even on content translation quality.</abstract>
      <url hash="3e213ae8">2024.findings-acl.216</url>
      <bibkey>heffernan-etal-2024-aligning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.216</doi>
    </paper>
    <paper id="217">
      <title>Video-Language Understanding: A Survey from Model Architecture, Model Training, and Data Perspectives</title>
      <author><first>Thong</first><last>Nguyen</last></author>
      <author><first>Yi</first><last>Bin</last></author>
      <author><first>Junbin</first><last>Xiao</last></author>
      <author><first>Leigang</first><last>Qu</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Yicong</first><last>Li</last></author>
      <author><first>Jay Zhangjie</first><last>Wu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Cong-Duy</first><last>Nguyen</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>See-Kiong</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Anh Tuan</first><last>Luu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>3636-3657</pages>
      <abstract>Humans use multiple senses to comprehend the environment. Vision and language are two of the most vital senses since they allow us to easily communicate our thoughts and perceive the world around us. There has been a lot of interest in creating video-language understanding systems with human-like senses since a video-language pair can mimic both our linguistic medium and visual environment with temporal dynamics. In this survey, we review the key tasks of these systems and highlight the associated challenges. Based on the challenges, we summarize their methods from model architecture, model training, and data perspectives. We also conduct performance comparison among the methods, and discuss promising directions for future research.</abstract>
      <url hash="c1d35d5c">2024.findings-acl.217</url>
      <bibkey>nguyen-etal-2024-video</bibkey>
      <doi>10.18653/v1/2024.findings-acl.217</doi>
    </paper>
    <paper id="218">
      <title>Generative Input: Towards Next-Generation Input Methods Paradigm</title>
      <author><first>Keyu</first><last>Ding</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yongcan</first><last>Wang</last><affiliation>iFLYTEK AI Research</affiliation></author>
      <author><first>Zihang</first><last>Xu</last><affiliation>iFLYTEK Research</affiliation></author>
      <author><first>Zhenzhen</first><last>Jia</last></author>
      <author><first>Enhong</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>3658-3669</pages>
      <abstract>Since the release of ChatGPT, generative models have achieved tremendous success and become the de facto approach for various NLP tasks. However, its application in the field of input methods remains under-explored. Many neural network approaches have been applied to the construction of Chinese input method engines (IMEs). Previous research often assumed that the input pinyin was correct and focused on Pinyin-to-character (P2C) task, which significantly falls short of meeting users’ demands. Moreover, previous research could not leverage user feedback to optimize the model and provide personalized results. In this study, we propose a novel Generative Input paradigm named GeneInput. It uses prompts to handle all input scenarios and other intelligent auxiliary input functions, optimizing the model with user feedback. The results demonstrate that we have achieved state-of-the-art performance for the first time in the Full-mode Key-sequence to Characters task. GeneInput also includes RLHF-IME, a novel RLHF application framework for input method, that eliminates the need for manual ranking annotations and the performance surpasses GPT-4. Relevant resources have been open-sourced.</abstract>
      <url hash="a391c281">2024.findings-acl.218</url>
      <bibkey>ding-etal-2024-generative</bibkey>
      <doi>10.18653/v1/2024.findings-acl.218</doi>
    </paper>
    <paper id="219">
      <title>A + <fixed-case>B</fixed-case>: A General Generator-Reader Framework for Optimizing <fixed-case>LLM</fixed-case>s to Unleash Synergy Potential</title>
      <author><first>Wei</first><last>Tang</last></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiahao</first><last>Ying</last></author>
      <author><first>Bo</first><last>Wang</last><affiliation>School of Computer Science &amp; Technology, Beijing Institute of Technology</affiliation></author>
      <author><first>Yuyue</first><last>Zhao</last></author>
      <author><first>Yong</first><last>Liao</last><affiliation>University of Science and Technology of China and China Academic of Electronics and Information Technology</affiliation></author>
      <author><first>Pengyuan</first><last>Zhou</last><affiliation>Aarhus University</affiliation></author>
      <pages>3670-3685</pages>
      <abstract>Retrieval-Augmented Generation (RAG) is an effective solution to supplement necessary knowledge to large language models (LLMs). Targeting its bottleneck of retriever performance, “generate-then-read” pipeline is proposed to replace the retrieval stage with generation from the LLM itself. Although promising, this research direction is underexplored and still cannot work in the scenario when source knowledge is given. In this paper, we formalize a general “A + B” framework with varying combinations of foundation models and types for systematic investigation. We explore the efficacy of the base and chat versions of LLMs and found their different functionalities suitable for generator A and reader B, respectively. Their combinations consistently outperform single models, especially in complex scenarios. Furthermore, we extend the application of the “A + B” framework to scenarios involving source documents through continuous learning, enabling the direct integration of external knowledge into LLMs. This approach not only facilitates effective acquisition of new knowledge but also addresses the challenges of safety and helpfulness post-adaptation. The paper underscores the versatility of the “A + B” framework, demonstrating its potential to enhance the practical application of LLMs across various domains.</abstract>
      <url hash="03717acb">2024.findings-acl.219</url>
      <bibkey>tang-etal-2024-b</bibkey>
      <doi>10.18653/v1/2024.findings-acl.219</doi>
    </paper>
    <paper id="220">
      <title>Functional Overlap Reranking for Neural Code Generation</title>
      <author><first>Hung</first><last>To</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Minh</first><last>Nguyen</last><affiliation>FPT Software AI Center</affiliation></author>
      <author><first>Nghi</first><last>Bui</last></author>
      <pages>3686-3704</pages>
      <abstract>Code Large Language Models (CodeLLMs) have ushered in a new era in code generation advancements. However, selecting the best code solutions from all possible CodeLLM outputs remains a challenge. Previous methods often overlooked the intricate functional similarities and interactions between solution clusters. We introduce SRank, a novel reranking strategy for selecting the best solutions from code generation, focusing on modeling the relationships between clusters of solutions. By quantifying the functional overlap between solution clusters, our approach provides a better ranking strategy for code solutions. Empirical results show that our method achieves remarkable results on the pass@1 score. For instance, on the Human-Eval benchmark, we achieve 69.66% in pass@1 with Codex002, 75.31% with WizardCoder, 53.99% with StarCoder, and 60.55% with CodeGen, surpassing state-of-the-art code generation reranking methods such as CodeT and Coder-Reviewer on the same CodeLLM by a significant margin approx 6.1% improvement on average. Even in scenarios with a limited number of sampled solutions and test cases, our approach demonstrates robustness and superiority, marking a new benchmark in code generation reranking. Our implementation can be found at https://github.com/FSoft-AI4Code/SRank-CodeRanker.</abstract>
      <url hash="9803ad2b">2024.findings-acl.220</url>
      <bibkey>to-etal-2024-functional</bibkey>
      <doi>10.18653/v1/2024.findings-acl.220</doi>
    </paper>
    <paper id="221">
      <title>Adversarial Preference Optimization: Enhancing Your Alignment via <fixed-case>RM</fixed-case>-<fixed-case>LLM</fixed-case> Game</title>
      <author><first>Pengyu</first><last>Cheng</last><affiliation>Tencent</affiliation></author>
      <author><first>Yifan</first><last>Yang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jian</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Yong</first><last>Dai</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Tianhao</first><last>Hu</last></author>
      <author><first>Peixin</first><last>Cao</last></author>
      <author><first>Nan</first><last>Du</last></author>
      <author><first>Xiaolong</first><last>Li</last></author>
      <pages>3705-3716</pages>
      <abstract>Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing alignment methods depend on manually annotated preference data to guide the LLM optimization directions. However, continuously updating LLMs for alignment raises a distribution gap between model-generated samples and human-annotated responses, hindering training effectiveness. To mitigate this issue, previous methods require additional preference annotation on newly generated samples to adapt to the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an Adversarial Preference Optimization (APO) framework, in which the LLM and the reward model update alternatively via a min-max game. Through adversarial training, the reward model can adapt to the shifted generation distribution of the LLM without any additional annotation. With comprehensive experiments, we find the proposed adversarial training framework further enhances existing alignment baselines in terms of LLM helpfulness and harmlessness. The code is at https://github.com/Linear95/APO.</abstract>
      <url hash="0a1f73f5">2024.findings-acl.221</url>
      <bibkey>cheng-etal-2024-adversarial</bibkey>
      <doi>10.18653/v1/2024.findings-acl.221</doi>
    </paper>
    <paper id="222">
      <title>Pinpointing Diffusion Grid Noise to Enhance Aspect Sentiment Quad Prediction</title>
      <author><first>Linan</first><last>Zhu</last><affiliation>Zhejiang University of Technology</affiliation></author>
      <author><first>Xiangfan</first><last>Chen</last></author>
      <author><first>Xiaolei</first><last>Guo</last></author>
      <author><first>Chenwei</first><last>Zhang</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Zhechao</first><last>Zhu</last></author>
      <author><first>Zehai</first><last>Zhou</last></author>
      <author><first>Xiangjie</first><last>Kong</last></author>
      <pages>3717-3726</pages>
      <abstract>Aspect sentiment quad prediction (ASQP) has garnered significant attention in aspect-based sentiment analysis (ABSA). Current ASQP research primarily relies on pre-trained generative language models to produce templated sequences, often complemented by grid-based auxiliary methods. Despite these efforts, the persistent challenge of generation instability remains unresolved and the effectiveness of grid methods remains underexplored in current studies. To this end, we introduce <b>G</b>rid Noise <b>D</b>iffusion <b>P</b>inpoint Network (<b>GDP</b>), a T5-based generative model aiming to tackle the issue of generation instability. The model consists of three novel modules, including Diffusion Vague Learning (DVL) to facilitate effective model learning and enhance overall robustness; Consistency Likelihood Learning (CLL) to discern the characteristics and commonalities of sentiment elements and thus reduce the impact of distributed noise; and GDP-FOR, a novel generation template, to enable models to generate outputs in a more natural way. Extensive experiments on four datasets demonstrate the remarkable effectiveness of our approach in addressing ASQP tasks.</abstract>
      <url hash="82257d4f">2024.findings-acl.222</url>
      <bibkey>zhu-etal-2024-pinpointing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.222</doi>
    </paper>
    <paper id="223">
      <title>Continual Contrastive Spoken Language Understanding</title>
      <author><first>Umberto</first><last>Cappellazzo</last></author>
      <author><first>Enrico</first><last>Fini</last><affiliation>Apple</affiliation></author>
      <author><first>Muqiao</first><last>Yang</last></author>
      <author><first>Daniele</first><last>Falavigna</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Alessio</first><last>Brutti</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Bhiksha</first><last>Raj</last><affiliation>Carnegie Mellon University, Carnegie Mellon University and Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>3727-3741</pages>
      <abstract>Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that helps the model learn more discriminative representations of the new data by aligning audio and text features. We also investigate different contrastive designs to combine the strengths of the contrastive loss with teacher-student architectures used for distillation. Experiments on two established SLU datasets reveal the effectiveness of our proposed approach and significant improvements over the baselines. We also show that COCONUT can be combined with methods that operate on the decoder side of the model, resulting in further metrics improvements.</abstract>
      <url hash="6de60823">2024.findings-acl.223</url>
      <bibkey>cappellazzo-etal-2024-continual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.223</doi>
    </paper>
    <paper id="224">
      <title><fixed-case>LLM</fixed-case> as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs</title>
      <author><first>Kai</first><last>Wang</last></author>
      <author><first>Yuwei</first><last>Xu</last></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Siqiang</first><last>Luo</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>3742-3759</pages>
      <abstract>Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting average performance improvements by 20%, 45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios.</abstract>
      <url hash="7a56f307">2024.findings-acl.224</url>
      <bibkey>wang-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.224</doi>
    </paper>
    <paper id="225">
      <title>Unsupervised Parsing by Searching for Frequent Word Sequences among Sentences with Equivalent Predicate-Argument Structures</title>
      <author><first>Junjie</first><last>Chen</last><affiliation>the University of Tokyo</affiliation></author>
      <author><first>Xiangheng</first><last>He</last></author>
      <author><first>Danushka</first><last>Bollegala</last><affiliation>Amazon and University of Liverpool</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>3760-3772</pages>
      <abstract>Unsupervised constituency parsing focuses on identifying word sequences that form a syntactic unit (i.e., constituents) in target sentences. Linguists identify the constituent by evaluating a set of Predicate-Argument Structure (PAS) equivalent sentences where we find the constituent appears more frequently than non-constituents (i.e., the constituent corresponds to a frequent word sequence within the sentence set). However, such frequency information is unavailable in previous parsing methods that identify the constituent by observing sentences with diverse PAS. In this study, we empirically show that constituents correspond to frequent word sequences in the PAS-equivalent sentence set. We propose a frequency-based parser, span-overlap, that (1) computes the span-overlap score as the word sequence’s frequency in the PAS-equivalent sentence set and (2) identifies the constituent structure by finding a constituent tree with the maximum span-overlap score. The parser achieves state-of-the-art level parsing accuracy, outperforming existing unsupervised parsers in eight out of ten languages. Additionally, we discover a multilingual phenomenon: participant-denoting constituents tend to have higher span-overlap scores than equal-length event-denoting constituents, meaning that the former tend to appear more frequently in the PAS-equivalent sentence set than the latter. The phenomenon indicates a statistical difference between the two constituent types, laying the foundation for future labeled unsupervised parsing research.</abstract>
      <url hash="4f48610b">2024.findings-acl.225</url>
      <bibkey>chen-etal-2024-unsupervised</bibkey>
      <doi>10.18653/v1/2024.findings-acl.225</doi>
    </paper>
    <paper id="226">
      <title>Data-Centric Explainable Debiasing for Improving Fairness in Pre-trained Language Models</title>
      <author><first>Yingji</first><last>Li</last><affiliation>Jilin University</affiliation></author>
      <author><first>Mengnan</first><last>Du</last><affiliation>New Jersey Institute of Technology</affiliation></author>
      <author><first>Rui</first><last>Song</last><affiliation>Jilin University</affiliation></author>
      <author><first>Xin</first><last>Wang</last><affiliation>Jilin University</affiliation></author>
      <author><first>Ying</first><last>Wang</last><affiliation>Jilin University</affiliation></author>
      <pages>3773-3786</pages>
      <abstract>Human-like social bias of pre-trained language models (PLMs) on downstream tasks have attracted increasing attention. The potential flaws in the training data are the main factor that causes unfairness in PLMs. Existing data-centric debiasing strategies mainly leverage explicit bias words (defined as sensitive attribute words specific to demographic groups) for counterfactual data augmentation to balance the training data. However, they lack consideration of implicit bias words potentially associated with explicit bias words in complex distribution data, which indirectly harms the fairness of PLMs. To this end, we propose a **Data**-Centric **Debias**ing method (named Data-Debias), which uses an explainability method to search for implicit bias words to assist in debiasing PLMs. Specifically, we compute the feature attributions of all tokens using the Integrated Gradients method, and then treat the tokens that have a large impact on the model’s decision as implicit bias words. To make the search results more precise, we iteratively train a biased model to amplify the bias with each iteration. Finally, we use the implicit bias words searched in the last iteration to assist in debiasing PLMs. Extensive experimental results on multiple PLMs debiasing on three different classification tasks demonstrate that Data-Debias achieves state-of-the-art debiasing performance and strong generalization while maintaining predictive abilities.</abstract>
      <url hash="5eee63e5">2024.findings-acl.226</url>
      <bibkey>li-etal-2024-data</bibkey>
      <doi>10.18653/v1/2024.findings-acl.226</doi>
    </paper>
    <paper id="227">
      <title>Knowledge-Driven Cross-Document Relation Extraction</title>
      <author><first>Monika</first><last>Jain</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Raghava</first><last>Mutharaju</last><affiliation>Indraprastha Institute of Information Technology, Delhi, India</affiliation></author>
      <author><first>Kuldeep</first><last>Singh</last><affiliation>Cerence GmbH</affiliation></author>
      <author><first>Ramakanth</first><last>Kavuluru</last><affiliation>University of Kentucky</affiliation></author>
      <pages>3787-3797</pages>
      <abstract>Relation extraction (RE) is a well-known NLP application often treated as a sentence or document-level task. However, a handful of recent efforts explore it across documents or in the cross-document setting (CrossDocRE). This is distinct from the single document case because different documents often focus on disparate themes, while text within a document tends to have a single goal.Current CrossDocRE efforts do not consider domain knowledge, which are often assumed to be known to the reader when documents are authored. Here, we propose a novel approach, KXDocRE, that embed domain knowledge of entities with input text for cross-document RE. Our proposed framework has three main benefits over baselines: 1) it incorporates domain knowledge of entities along with documents’ text; 2) it offers interpretability by producing explanatory text for predicted relations between entities 3) it improves performance over the prior methods. Code and models are available at <url>https://github.com/kracr/cross-doc-relation-extraction</url>.</abstract>
      <url hash="5a7a6749">2024.findings-acl.227</url>
      <bibkey>jain-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.findings-acl.227</doi>
    </paper>
    <paper id="228">
      <title>Injecting Salesperson’s Dialogue Strategies in Large Language Models with Chain-of-Thought Reasoning</title>
      <author><first>Wen</first><last>Chang</last></author>
      <author><first>Yun-Nung</first><last>Chen</last><affiliation>Department of Computer Science and Informational Engineering, National Taiwan University</affiliation></author>
      <pages>3798-3812</pages>
      <abstract>Recent research in dialogue systems focuses on two main categories: task-oriented (TOD) and open-domain (chit-chat) dialogues. TOD systems help users complete specific tasks, while open-domain systems aim to create engaging conversations. However, user intents often emerge during interactions. A recent study introduced SalesBot, simulating dialogues that transition from chit-chat to task-oriented scenarios to train sales agents. Unfortunately, the initial data lacked smooth transitions and coherent long dialogues, resulting in unnatural interactions. This paper presents SalesBot 2.0, an improved dataset leveraging commonsense knowledge from large language models (LLMs) through strategic prompting. Additionally, we introduce SalesAgent, a novel model trained on salesperson interactions using chain-of-thought (CoT) reasoning. This model excels in transitioning topics, understanding user intents, and selecting appropriate strategies.Experiments with diverse user simulations validate our method’s effectiveness in controlling dialogue strategies in LLMs. SalesBot 2.0 enhances coherence and reduces aggression, improving model learning for sales-customer interactions.</abstract>
      <url hash="0e7a651c">2024.findings-acl.228</url>
      <bibkey>chang-chen-2024-injecting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.228</doi>
    </paper>
    <paper id="229">
      <title><fixed-case>KG</fixed-case>-Adapter: Enabling Knowledge Graph Integration in Large Language Models through Parameter-Efficient Fine-Tuning</title>
      <author><first>Shiyu</first><last>Tian</last></author>
      <author><first>Yangyang</first><last>Luo</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Tianze</first><last>Xu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Caixia</first><last>Yuan</last></author>
      <author><first>Huixing</first><last>Jiang</last><affiliation>Li Auto</affiliation></author>
      <author><first>Chen</first><last>Wei</last></author>
      <author><first>Xiaojie</first><last>Wang</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <pages>3813-3828</pages>
      <abstract>Although large language models (LLMs) show remarkable capabilities and generalizability across various tasks, they are criticized for lack of expertise. One promising solution is to combine knowledge graphs (KGs) with LLMs, and recent studies focus on integrating KGs into LLMs through prompt-based methods. However, these approaches fail to use the structural information of the KGs, suffer from the problem of knowledge conflict, and over-reliance on super LLMs. To address these challenges, we propose KG-Adapter, a parameter-level KG integration method based on parameter-efficient fine-tuning (PEFT). Specifically, we introduce a novel adapter structure designed for decoder-only LLMs, which can encode KGs from both node-centered and relation-centered perspectives, and then perform joint reasoning with LLMs to generate responses end-to-end. Experiments with diverse models on four datasets for two different tasks all demonstrate significant improvements. With only 28M parameters trained, we make the 7B-parameter LLM outperform the previous full-parameter fine-tuned state-of-the-art method and comparable to the prompt-based ChatGPT methods.</abstract>
      <url hash="8595c766">2024.findings-acl.229</url>
      <bibkey>tian-etal-2024-kg</bibkey>
      <doi>10.18653/v1/2024.findings-acl.229</doi>
    </paper>
    <paper id="230">
      <title>Just Ask One More Time! Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios</title>
      <author><first>Lei</first><last>Lin</last><affiliation>kuaishou</affiliation></author>
      <author><first>Jiayi</first><last>Fu</last></author>
      <author><first>Pengli</first><last>Liu</last><affiliation>kuaishou</affiliation></author>
      <author><first>Qingyang</first><last>Li</last></author>
      <author><first>Yan</first><last>Gong</last></author>
      <author><first>Junchen</first><last>Wan</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Zhongyuan</first><last>Wang</last><affiliation>Kuaishou Inc. and Kuaishou</affiliation></author>
      <author><first>Di</first><last>Zhang</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Kun</first><last>Gai</last></author>
      <pages>3829-3852</pages>
      <abstract>Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as self-consistency, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose Self-Agreement, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input questions and the answer format of reasoning paths may be known or unknown. Self-agreement firstly samples from language model’s decoder to generate a diverse set of reasoning paths, and subsequently prompts the language model one more time to determine the optimal answer by selecting the most agreed answer among the sampled reasoning paths. Self-agreement simultaneously achieves remarkable performance on six public reasoning benchmarks and superior generalization capabilities.</abstract>
      <url hash="ed6f3ec2">2024.findings-acl.230</url>
      <bibkey>lin-etal-2024-just</bibkey>
      <doi>10.18653/v1/2024.findings-acl.230</doi>
    </paper>
    <paper id="231">
      <title>Evaluating <fixed-case>LLM</fixed-case>s’ Mathematical Reasoning in Financial Document Question Answering</title>
      <author><first>Pragya</first><last>Srivastava</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Manuj</first><last>Malik</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>University of Pennsylvania, United States</affiliation></author>
      <author><first>Tanuja</first><last>Ganu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dan</first><last>Roth</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>3853-3878</pages>
      <abstract>Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with a hybrid of structured tables and unstructured text remain uncertain. This study explores LLMs’ mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs’ capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique EEDP tailored to semi-structured documents, matching or outperforming baselines performance while providing a nuanced understanding of LLMs abilities.</abstract>
      <url hash="d5b82f88">2024.findings-acl.231</url>
      <bibkey>srivastava-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.231</doi>
    </paper>
    <paper id="232">
      <title>Improving In-Context Learning with Prediction Feedback for Sentiment Analysis</title>
      <author><first>Hongling</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Qianlong</first><last>Wang</last></author>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xi</first><last>Zeng</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>3879-3890</pages>
      <abstract>Large language models (LLMs) have achieved promising results in sentiment analysis through the in-context learning (ICL) paradigm. However, their ability to distinguish subtle sentiments still remains a challenge. Inspired by the human ability to adjust understanding via feedback, this paper enhances ICL by incorporating prior predictions and feedback, aiming to rectify sentiment misinterpretation of LLMs. Specifically, the proposed framework consists of three steps: (1) acquiring prior predictions of LLMs, (2) devising predictive feedback based on correctness, and (3) leveraging a feedback-driven prompt to refine sentiment understanding. Experimental results across nine sentiment analysis datasets demonstrate the superiority of our framework over conventional ICL methods, with an average F1 improvement of 5.95%.</abstract>
      <url hash="337ca60d">2024.findings-acl.232</url>
      <bibkey>xu-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.findings-acl.232</doi>
    </paper>
    <paper id="233">
      <title>Can Large Language Models Mine Interpretable Financial Factors More Effectively? A Neural-Symbolic Factor Mining Agent Model</title>
      <author><first>Zhiwei</first><last>Li</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ran</first><last>Song</last><affiliation>Kunmimg University of Science and Technology</affiliation></author>
      <author><first>Caihong</first><last>Sun</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Zhengtao</first><last>Yu</last><affiliation>Kunming University of Science and Technology</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>3891-3902</pages>
      <abstract>Finding interpretable factors for stock returns is the most vital issue in the empirical asset pricing domain. As data-driven methods, existing factor mining models can be categorized into symbol-based and neural-based models. Symbol-based models are interpretable but inefficient, while neural-based approaches are efficient but lack interpretability. Hence, mining interpretable factors effectively presents a significant challenge. Inspired by the success of Large Language Models (LLMs) in various tasks, we propose a FActor Mining Agent (FAMA) model that enables LLMs to integrate the strengths of both neural and symbolic models for factor mining. In this paper, FAMA consists of two main components: Cross-Sample Selection (CSS) and Chain-of-Experience (CoE). CSS addresses the homogeneity challenges in LLMs during factor mining by assimilating diverse factors as in-context samples, whereas CoE enables LLMs to leverage past successful mining experiences, expediting the mining of effective factors. Experimental evaluations on real-world stock market data demonstrate the effectiveness of our approach by surpassing the SOTA RankIC by 0.006 and RankICIR by 0.105 in predicting S&amp;P 500 returns. Furthermore, the investment simulation shows that our model can achieve superior performance with an annualized return of 38.4% and a Sharpe ratio of 667.2%.</abstract>
      <url hash="4c247768">2024.findings-acl.233</url>
      <bibkey>li-etal-2024-large-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.233</doi>
    </paper>
    <paper id="234">
      <title>Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint</title>
      <author><first>Xiaowei</first><last>Yuan</last></author>
      <author><first>Zhao</first><last>Yang</last></author>
      <author><first>Yequan</first><last>Wang</last><affiliation>Beijing Academy of Artificial Intelligence</affiliation></author>
      <author><first>Shengping</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <pages>3903-3922</pages>
      <abstract>Large language models (LLMs) internalize enormous <i>parametric knowledge</i> during pre-training. Concurrently, realistic applications necessitate external <i>contextual knowledge</i> to aid models on the underlying tasks. This raises a crucial dilemma known as <i>knowledge conflicts</i>, where the contextual knowledge clashes with the parametric knowledge. However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model’s faithfulness to conflicting context, and simultaneously maintain high performance among non-conflicting context. Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets.</abstract>
      <url hash="c04daa28">2024.findings-acl.234</url>
      <bibkey>yuan-etal-2024-discerning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.234</doi>
    </paper>
    <paper id="235">
      <title><fixed-case>SALAD</fixed-case>-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models</title>
      <author><first>Lijun</first><last>Li</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Bowen</first><last>Dong</last></author>
      <author><first>Ruohui</first><last>Wang</last></author>
      <author><first>Xuhao</first><last>Hu</last></author>
      <author><first>Wangmeng</first><last>Zuo</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>3923-3954</pages>
      <abstract>In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose SALAD-Bench, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. Data and evaluator are released under https://github.com/OpenSafetyLab/SALAD-BENCH</abstract>
      <url hash="497f060c">2024.findings-acl.235</url>
      <bibkey>li-etal-2024-salad</bibkey>
      <doi>10.18653/v1/2024.findings-acl.235</doi>
    </paper>
    <paper id="236">
      <title>Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation</title>
      <author><first>Pablo</first><last>Messina</last></author>
      <author><first>Rene</first><last>Vidal</last><affiliation>University of Pennsylvania and Amazon</affiliation></author>
      <author><first>Denis</first><last>Parra</last><affiliation>Pontificia Universidad Catolica de Chile</affiliation></author>
      <author><first>Alvaro</first><last>Soto</last></author>
      <author><first>Vladimir</first><last>Araujo</last><affiliation>KU Leuven</affiliation></author>
      <pages>3955-3986</pages>
      <abstract>Advancing representation learning in specialized fields like medicine remains challenging due to the scarcity of expert annotations for text and images. To tackle this issue, we present a novel two-stage framework designed to extract high-quality factual statements from free-text radiology reports in order to improve the representations of text encoders and, consequently, their performance on various downstream tasks.In the first stage, we propose a <i>Fact Extractor</i> that leverages large language models (LLMs) to identify factual statements from well-curated domain-specific datasets. In the second stage, we introduce a <i>Fact Encoder</i> (CXRFE) based on a BERT model fine-tuned with objective functions designed to improve its representations using the extracted factual data. Our framework also includes a new embedding-based metric (CXRFEScore) for evaluating chest X-ray text generation systems, leveraging both stages of our approach. Extensive evaluations show that our fact extractor and encoder outperform current state-of-the-art methods in tasks such as sentence ranking, natural language inference, and label extraction from radiology reports. Additionally, our metric proves to be more robust and effective than existing metrics commonly used in the radiology report generation literature. The code of this project is available at <url>https://github.com/PabloMessina/CXR-Fact-Encoder</url>.</abstract>
      <url hash="18cbb89e">2024.findings-acl.236</url>
      <bibkey>messina-etal-2024-extracting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.236</doi>
    </paper>
    <paper id="237">
      <title><fixed-case>GNN</fixed-case>avi: Navigating the Information Flow in Large Language Models by Graph Neural Network</title>
      <author><first>Shuzhou</first><last>Yuan</last></author>
      <author><first>Ercong</first><last>Nie</last></author>
      <author><first>Michael</first><last>Färber</last><affiliation>Technische Universität Dresden</affiliation></author>
      <author><first>Helmut</first><last>Schmid</last><affiliation>Center for Information and Language Processing</affiliation></author>
      <author><first>Hinrich</first><last>Schuetze</last></author>
      <pages>3987-4001</pages>
      <abstract>Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are used. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL’s information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 show GNNavi surpasses standard prompt-based fine-tuning methods in few-shot settings by updating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency. Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process.</abstract>
      <url hash="627550f1">2024.findings-acl.237</url>
      <bibkey>yuan-etal-2024-gnnavi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.237</doi>
    </paper>
    <paper id="238">
      <title><fixed-case>M</fixed-case>-<fixed-case>QALM</fixed-case>: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering</title>
      <author><first>Anand</first><last>Subramanian</last></author>
      <author><first>Viktor</first><last>Schlegel</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Abhinav</first><last>Ramesh Kashyap</last></author>
      <author><first>Thanh-Tung</first><last>Nguyen</last><affiliation>asus</affiliation></author>
      <author><first>Vijay Prakash</first><last>Dwivedi</last></author>
      <author><first>Stefan</first><last>Winkler</last><affiliation>National University of Singapore</affiliation></author>
      <pages>4002-4042</pages>
      <abstract>There is vivid research on adapting Large Language Models (LLMs) to perform a variety of tasks in high-stakes domains such as healthcare. Despite their popularity, there is a lack of understanding of the extent and contributing factors that allow LLMs to recall relevant knowledge and combine it with presented information in the clinical and biomedical domain: a fundamental pre-requisite for success on down-stream tasks.Addressing this gap, we use Multiple Choice and Abstractive Question Answering to conduct a large-scale empirical study on 22 datasets in three generalist and three specialist biomedical sub-domains. Our multifaceted analysis of the performance of 15 LLMs, further broken down by sub-domain, source of knowledge and model architecture, uncovers success factors such as instruction tuning that lead to improved recall and comprehension. We further show that while recently proposed domain-adapted models may lack adequate knowledge, directly fine-tuning on our collected medical knowledge datasets shows encouraging results, even generalising to unseen specialist sub-domains. We complement the quantitative results with a skill-oriented manual error analysis, which reveals a significant gap between the models’ capabilities to simply recall necessary knowledge and to integrate it with the presented context.To foster research and collaboration in this field we share M-QALM, our resources, standardised methodology, and evaluation results, with the research community to facilitate further advancements in clinical knowledge representation learning within language models.</abstract>
      <url hash="4027fd7e">2024.findings-acl.238</url>
      <bibkey>subramanian-etal-2024-qalm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.238</doi>
    </paper>
    <paper id="239">
      <title><fixed-case>M</fixed-case>ovie<fixed-case>S</fixed-case>um: An Abstractive Summarization Dataset for Movie Screenplays</title>
      <author><first>Rohit</first><last>Saxena</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Frank</first><last>Keller</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>4043-4050</pages>
      <abstract>Movie screenplay summarization is challenging, as it requires an understanding of long input contexts and various elements unique to movies. Large language models have shown significant advancements in document summarization, but they often struggle with processing long input contexts. Furthermore, while television transcripts have received attention in recent studies, movie screenplay summarization remains underexplored. To stimulate research in this area, we present a new dataset, MovieSum, for abstractive summarization of movie screenplays. This dataset comprises 2200 movie screenplays accompanied by their Wikipedia plot summaries. We manually formatted the movie screenplays to represent their structural elements. Compared to existing datasets, MovieSum possesses several distinctive features: 1) It includes movie screenplays which are longer than scripts of TV episodes. 2) It is twice the size of previous movie screenplay datasets. 3) It provides metadata with IMDb IDs to facilitate access to additional external knowledge. We also show the results of recently released large language models applied to summarization on our dataset to provide a detailed baseline.</abstract>
      <url hash="de929391">2024.findings-acl.239</url>
      <bibkey>saxena-keller-2024-moviesum</bibkey>
      <doi>10.18653/v1/2024.findings-acl.239</doi>
    </paper>
    <paper id="240">
      <title>Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality</title>
      <author><first>Jiahuan</first><last>Pei</last><affiliation>Centrum voor Wiskunde en Informatica</affiliation></author>
      <author><first>Irene</first><last>Viola</last></author>
      <author><first>Haochen</first><last>Huang</last></author>
      <author><first>Junxiao</first><last>Wang</last><affiliation>King Abdullah University of Science and Technology</affiliation></author>
      <author><first>Moonisa</first><last>Ahsan</last></author>
      <author><first>Fanghua</first><last>Ye</last></author>
      <author><first>Jiang</first><last>Yiming</last></author>
      <author><first>Yao</first><last>Sai</last></author>
      <author><first>Di</first><last>Wang</last><affiliation>KAUST</affiliation></author>
      <author><first>Zhumin</first><last>Chen</last><affiliation>Shandong University</affiliation></author>
      <author><first>Pengjie</first><last>Ren</last><affiliation>Shandong University</affiliation></author>
      <author><first>Pablo</first><last>Cesar</last><affiliation>Delft University of Technology and Centrum Wiskunde &amp; Informatica (CWI)</affiliation></author>
      <pages>4051-4066</pages>
      <abstract>Autonomous artificial intelligence (AI) agents have emerged as promising protocols for automatically understanding the language-based environment, particularly with the exponential development of large language models (LLMs). However, a fine-grained, comprehensive understanding of multimodal environments remains under-explored. This work designs an autonomous workflow tailored for integrating AI agents seamlessly into extended reality (XR) applications for fine-grained training. We present a demonstration of a multimodal fine-grained training assistant for LEGO brick assembly in a pilot XR environment. Specifically, we design a cerebral language agent that integrates LLM with memory, planning, and interaction with XR tools and a vision-language agent, enabling agents to decide their actions based on past experiences. Furthermore, we introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset synthesized automatically in the workflow served by a commercial LLM. This dataset comprises multimodal instruction manuals, conversations, XR responses, and vision question answering. Last, we present several prevailing open-resource LLMs as benchmarks, assessing their performance with and without fine-tuning on the proposed dataset. We anticipate that the broader impact of this workflow will advance the development of smarter assistants for seamless user interaction in XR environments, fostering research in both AI and HCI communities.</abstract>
      <url hash="5de62919">2024.findings-acl.240</url>
      <bibkey>pei-etal-2024-autonomous</bibkey>
      <doi>10.18653/v1/2024.findings-acl.240</doi>
    </paper>
    <paper id="241">
      <title>Perceptions of Language Technology Failures from <fixed-case>S</fixed-case>outh <fixed-case>A</fixed-case>sian <fixed-case>E</fixed-case>nglish Speakers</title>
      <author><first>Faye</first><last>Holt</last></author>
      <author><first>William</first><last>Held</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>4067-4081</pages>
      <abstract>English NLP systems have empirically worse performance for dialects other than Standard American English (SAmE). However, how these discrepancies impact use of language technology by speakers of non-SAmE global Englishes is not well understood. We focus on reducing this gap for South Asian Englishes (SAsE), a macro-group of regional varieties with cumulatively more speakers than SAmE, by surveying SAsE speakers about their interactions with language technology and compare their responses to a control survey of SAmE speakers. SAsE speakers are more likely to recall failures with language technology and more likely to reference specific issues with written language technology than their SAmE counterparts. Furthermore, SAsE speakers indicate that they modify both their lexicon and syntax to make technology work better, but that lexical issues are perceived as the most salient challenge. We then assess whether these issues are pervasive in more recently developed Large Language Models (LLMs), introducing two benchmarks for broader SAsE Lexical and Indian English Syntactic understanding and evaluating 11 families of LLMs on them.</abstract>
      <url hash="f1781a2c">2024.findings-acl.241</url>
      <bibkey>holt-etal-2024-perceptions</bibkey>
      <doi>10.18653/v1/2024.findings-acl.241</doi>
    </paper>
    <paper id="242">
      <title>A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task</title>
      <author><first>Jannik</first><last>Brinkmann</last><affiliation>University of Mannheim</affiliation></author>
      <author><first>Abhay</first><last>Sheshadri</last></author>
      <author><first>Victor</first><last>Levoso</last></author>
      <author><first>Paul</first><last>Swoboda</last><affiliation>Heinrich-Heine University Düsseldorf</affiliation></author>
      <author><first>Christian</first><last>Bartelt</last><affiliation>Universität Mannheim</affiliation></author>
      <pages>4082-4102</pages>
      <abstract>Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights into the broader operating principles of transformers and thus provide a basis for understanding more complex models.</abstract>
      <url hash="1e7ad06a">2024.findings-acl.242</url>
      <bibkey>brinkmann-etal-2024-mechanistic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.242</doi>
    </paper>
    <paper id="243">
      <title>Optimal Transport Guided Correlation Assignment for Multimodal Entity Linking</title>
      <author><first>Zefeng</first><last>Zhang</last></author>
      <author><first>Jiawei</first><last>Sheng</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhang</first><last>Chuang</last></author>
      <author><first>Liangyunzhi</first><last>Liangyunzhi</last></author>
      <author><first>Wenyuan</first><last>Zhang</last></author>
      <author><first>Siqi</first><last>Wang</last></author>
      <author><first>Tingwen</first><last>Liu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <pages>4103-4117</pages>
      <abstract>Multimodal entity linking (MEL) aims to link ambiguous mentions in multimodal contexts to entities in a multimodal knowledge graph. A pivotal challenge is to fully leverage multi-element correlations between mentions and entities to bridge modality gap and enable fine-grained semantic matching. Existing methods attempt several local correlative mechanisms, relying heavily on the automatically learned attention weights, which may over-concentrate on partial correlations. To mitigate this issue, we formulate the correlation assignment problem as an optimal transport (OT) problem, and propose a novel MEL framework, namely OT-MEL, with OT-guided correlation assignment. Thereby, we exploit the correlation between multimodal features to enhance multimodal fusion, and the correlation between mentions and entities to enhance fine-grained matching. To accelerate model prediction, we further leverage knowledge distillation to transfer OT assignment knowledge to attention mechanism. Experimental results show that our model significantly outperforms previous state-of-the-art baselines and confirm the effectiveness of the OT-guided correlation assignment.</abstract>
      <url hash="fbfabe4d">2024.findings-acl.243</url>
      <bibkey>zhang-etal-2024-optimal</bibkey>
      <doi>10.18653/v1/2024.findings-acl.243</doi>
    </paper>
    <paper id="244">
      <title>On Efficiently Representing Regular Languages as <fixed-case>RNN</fixed-case>s</title>
      <author><first>Anej</first><last>Svete</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Robin</first><last>Chan</last></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>4118-4135</pages>
      <abstract>Recent work by Hewitt et al. (2020) provides an interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs). It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.This suggests that RNNs’ success might be linked to their ability to model hierarchy. However, a closer inspection of hewitt-etal-2020-rnns construction shows that it is not inherently limited to hierarchical structures. This poses a natural question: What other classes of LMs RNNs can efficiently represent? To this end, we generalize Hewitt et al.’s (2020) construction and show that RNNs can efficiently represent a larger class of LMs than previously claimed—specifically, those that can be represented by a pushdown automaton with a bounded stack and a specific stack update function. Altogether, the efficiency of representing this diverse class of LMs with RNN LMs suggests novel interpretations of their inductive bias.</abstract>
      <url hash="8eadf4f7">2024.findings-acl.244</url>
      <bibkey>svete-etal-2024-efficiently</bibkey>
      <doi>10.18653/v1/2024.findings-acl.244</doi>
    </paper>
    <paper id="245">
      <title>A Survey on Modelling Morality for Text Analysis</title>
      <author><first>Ines</first><last>Reinig</last></author>
      <author><first>Maria</first><last>Becker</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Ines</first><last>Rehbein</last><affiliation>Universität Mannheim</affiliation></author>
      <author><first>Simone</first><last>Ponzetto</last><affiliation>University of Mannheim</affiliation></author>
      <pages>4136-4155</pages>
      <abstract>In this survey, we provide a systematic review of recent work on modelling morality in text, an area of research that has garnered increasing attention in recent years. Our survey is motivated by the importance of modelling decisions on the created resources, the models trained on these resources and the analyses that result from the models’ predictions. We review work at the interface of NLP, Computational Social Science and Psychology and give an overview of the different goals and research questions addressed in the papers, their underlying theoretical backgrounds and the methods that have been applied to pursue these goals. We then identify and discuss challenges and research gaps, such as the lack of a theoretical framework underlying the operationalisation of morality in text, the low IAA reported for manyhuman-annotated resulting resources and the lack of validation of newly proposed resources and analyses.</abstract>
      <url hash="3fe6208c">2024.findings-acl.245</url>
      <bibkey>reinig-etal-2024-survey</bibkey>
      <doi>10.18653/v1/2024.findings-acl.245</doi>
    </paper>
    <paper id="246">
      <title>Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection</title>
      <author><first>Ruibo</first><last>Chen</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Yihan</first><last>Wu</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Lichang</first><last>Chen</last></author>
      <author><first>Guodong</first><last>Liu</last></author>
      <author><first>Qi</first><last>He</last></author>
      <author><first>Tianyi</first><last>Xiong</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Chenxi</first><last>Liu</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Junfeng</first><last>Guo</last></author>
      <author><first>Heng</first><last>Huang</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>4156-4172</pages>
      <abstract>Data selection in instruction tuning emerges as a pivotal process for acquiring high-quality data and training instruction-following large language models (LLMs), but it is still a new and unexplored research area for vision-language models (VLMs). Existing data selection approaches on LLMs either rely on single unreliable scores, or use downstream tasks for selection, which is time-consuming and can lead to potential over-fitting on the chosen evaluation datasets. To address this challenge, we introduce a novel dataset selection method, Self-Filter, that utilizes the VLM itself as a filter. This approach is inspired by the observation that VLMs benefit from training with the most challenging instructions. Self-Filter operates in two stages. In the first stage, we devise a scoring network to evaluate the difficulty of training instructions, which is co-trained with the VLM. In the second stage, we use the trained score net to measure the difficulty of each instruction, select the most challenging samples, and penalize similar samples to encourage diversity. Comprehensive experiments on LLaVA and MiniGPT-4 show that Self-Filter can reach better results compared to full data settings with merely about 15% samples, and can achieve superior performance against competitive baselines.</abstract>
      <url hash="718cdfd5">2024.findings-acl.246</url>
      <bibkey>chen-etal-2024-vision</bibkey>
      <doi>10.18653/v1/2024.findings-acl.246</doi>
    </paper>
    <paper id="247">
      <title><fixed-case>D</fixed-case>ebug<fixed-case>B</fixed-case>ench: Evaluating Debugging Capability of Large Language Models</title>
      <author><first>Runchu</first><last>Tian</last></author>
      <author><first>Yining</first><last>Ye</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Xin</first><last>Cong</last></author>
      <author><first>Yankai</first><last>Lin</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yinxu</first><last>Pan</last></author>
      <author><first>Yesai</first><last>Wu</last></author>
      <author><first>Hui</first><last>Haotian</last></author>
      <author><first>Liu</first><last>Weichuan</last><affiliation>Siemens Corporate Research</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>4173-4198</pages>
      <abstract>Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs’ debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce ‘DebugBench’, an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.</abstract>
      <url hash="68a0f70d">2024.findings-acl.247</url>
      <bibkey>tian-etal-2024-debugbench</bibkey>
      <doi>10.18653/v1/2024.findings-acl.247</doi>
    </paper>
    <paper id="248">
      <title><fixed-case>POP</fixed-case>-<fixed-case>CEE</fixed-case>: Position-oriented Prompt-tuning Model for Causal Emotion Entailment</title>
      <author><first>Zhihan</first><last>Zhou</last><affiliation>Jilin University</affiliation></author>
      <author><first>Xue</first><last>Gu</last><affiliation>Universidade do Minho</affiliation></author>
      <author><first>Yujie</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Xu</last><affiliation>Jilin University</affiliation></author>
      <pages>4199-4210</pages>
      <abstract>The objective of the Causal Emotion Entailment (CEE) task is to identify the causes of the target emotional utterances in a given conversation. Most existing studies have focused on a fine-tuning paradigm based on a pretrained model, e.g., the BERT model. However, there are gaps between the pretrained task and the CEE task. Although a pretrained model enhances contextual comprehension to some extent, it cannot acquire specific knowledge that is relevant to the CEE task. In addition, in a typical CEE task, there are peculiarities in the distribution of the positions with different emotion types of emotion utterances and cause utterances in conversations. Existing methods employ a fixed-size window to capture the relationship between neighboring conversations; however, these methods ignore the specific semantic associations between emotions and cause utterances. To address these issues, we propose the Position-oriented Prompt-tuning (POP-CEE) model to solve the CEE task in an end-to-end manner. Specifically, we can model the CEE task by designing prompts with multiple unified goals and by exploring the positional relationship between emotion and cause utterances using a position constraint module. Experimental results demonstrate that the proposed POP-CEE model achieves state-of-the-art performance on a benchmark dataset. Ourcode and data can be found at: https://github.com/Zh0uzh/POP-CEE.</abstract>
      <url hash="85f54a17">2024.findings-acl.248</url>
      <bibkey>zhou-etal-2024-pop</bibkey>
      <doi>10.18653/v1/2024.findings-acl.248</doi>
    </paper>
    <paper id="249">
      <title>Context Length Extension via Generalized Extrapolation Scale</title>
      <author><first>Linhan</first><last>Li</last></author>
      <author><first>Zhang</first><last>Huaping</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>4211-4218</pages>
      <abstract>Context length expansion of transformer models is considered a key challenge, especially when handling context beyond the training length during inference stage. In this paper, we propose <tex-math>\textbf{Ge}</tex-math>eneralized extrapolatio<tex-math>\textbf{N}</tex-math> scal<tex-math>\textbf{E}</tex-math> (GeNE), a set of parameterized extrapolation functions applied to each layer and attention head to adaptively adjust its extrapolation scales. Experimental results show that GeNE provides a significant improvement on long context language modeling. By randomly scaling the extrapolation ratio during the finetuning, GeNE achieves stable extrapolation on 64k contexts by training on 16k length text. Further, the instruction following Llama2 model based on GeNE achieved competitive results compared with other open-source models of the same parameter scale.</abstract>
      <url hash="3939f931">2024.findings-acl.249</url>
      <bibkey>li-huaping-2024-context</bibkey>
      <doi>10.18653/v1/2024.findings-acl.249</doi>
    </paper>
    <paper id="250">
      <title>Selectively Answering Visual Questions</title>
      <author><first>Julian</first><last>Eisenschlos</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Hernán</first><last>Maina</last><affiliation>Universidad Nacional de Córdoba, Argentina</affiliation></author>
      <author><first>Guido</first><last>Ivetta</last><affiliation>Universidad Nacional de Córdoba</affiliation></author>
      <author><first>Luciana</first><last>Benotti</last><affiliation>Universidad nacional de Córdoba</affiliation></author>
      <pages>4219-4229</pages>
      <abstract>Recently, large multi-modal models (LMMs) have emerged with the capacity to perform vision tasks such as captioning and visual question answering (VQA) with unprecedented accuracy. Applications such as helping the blind or visually impaired have a critical need for precise answers. It is specially important for models to be well calibrated and be able to quantify their uncertainty in order to selectively decide when to answer and when to abstain or ask for clarifications. We perform the first in-depth analysis of calibration methods and metrics for VQA with in-context learning LMMs. Studying VQA on two answerability benchmarks, we show that the likelihood score of visually grounded models is better calibrated than in their text-only counterparts for in-context learning, where sampling based methods are generally superior, but no clear winner arises. We propose Avg BLEU, a calibration score combining the benefits of both sampling and likelihood methods across modalities.</abstract>
      <url hash="ea45c2ae">2024.findings-acl.250</url>
      <bibkey>eisenschlos-etal-2024-selectively</bibkey>
      <doi>10.18653/v1/2024.findings-acl.250</doi>
    </paper>
    <paper id="251">
      <title><fixed-case>W</fixed-case>av2<fixed-case>SQL</fixed-case>: Direct Generalizable Speech-To-<fixed-case>SQL</fixed-case> Parsing</title>
      <author><first>Huadai</first><last>Liu</last></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>FAIR</affiliation></author>
      <author><first>Jinzheng</first><last>He</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Gang</first><last>Sun</last></author>
      <author><first>Ran</first><last>Shen</last></author>
      <author><first>Xize</first><last>Cheng</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>4230-4242</pages>
      <abstract>We release a multi-accent dataset and propose speech-programming and gradient reversal classifier to improve the generalization.Abstract: Speech-to-SQL (S2SQL) aims to convert spoken questions into SQL queries given relational databases, which has been traditionally implemented in a cascaded manner while facing the following challenges: 1) model training is faced with the major issue of data scarcity, where limited parallel data is available; and 2) the systems should be robust enough to handle diverse out-of-domain speech samples that differ from the source data. In this work, we propose the direct generalizable speech-to-SQL parsing model Wav2SQL which avoids error compounding across cascaded systems. Specifically, 1) to accelerate speech-driven SQL parsing research in the community, we release a large-scale and multi-accent dataset MASpider; 2) leveraging the recent progress in the large-scale pre-training, we show that it alleviates the data scarcity issue and allow for direct speech-to-SQL parsing; and 3) we include the speech re-programming and gradient reversal classifier techniques to reduce acoustic variance and learned style-agnostic representation, improving generalization to unseen out-of-domain custom data. Experimental results demonstrate that Wav2SQL avoids error compounding and achieves state-of-the-art results by up to 4.7% accuracy improvement over the baseline.</abstract>
      <url hash="29480c7c">2024.findings-acl.251</url>
      <bibkey>liu-etal-2024-wav2sql</bibkey>
      <doi>10.18653/v1/2024.findings-acl.251</doi>
    </paper>
    <paper id="252">
      <title>E2-<fixed-case>LLM</fixed-case>: Efficient and Extreme Length Extension of Large Language Models</title>
      <author><first>Jiaheng</first><last>Liu</last></author>
      <author><first>ZhiqiBai</first><last>ZhiqiBai</last></author>
      <author><first>Yuanxing</first><last>Zhang</last></author>
      <author><first>Chenchen</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>YuangZh</first><last>YuangZh</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>JiakaiWang</first><last>JiakaiWang</last></author>
      <author><first>Haoran</first><last>Que</last></author>
      <author><first>Yukang</first><last>Chen</last></author>
      <author><first>Wenbo</first><last>Su</last></author>
      <author><first>Tiezheng</first><last>Ge</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo and Google</affiliation></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>4243-4253</pages>
      <abstract>Training Large Language Models (LLMs) to process extensive context lengths incurs prohibitive computational costs. Prevailing techniques for extending context capabilities in LLMs typically require not only additional training procedures but also access to datasets with long context (e.g., sequences of 32K tokens), presupposing substantial GPU expenditures. To address the aforementioned issues, we introduce a novel solution named Efficient and Extreme length extension for Large Language Models (E2-LLM). E2-LLM entails a singular training process over considerably short sequences (e.g., 4K tokens), which greatly mitigates the cost of continual-pretraining or fine-tuning. Within the training phase, we incorporate a dual augmentation strategy with Rotary Position Embeddings (RoPE) that adjusts the scale and position indices across distinct training samples. E 2 -LLM is meticulously designed to enhance the model’s robustness to diverse relative positions. The experimental results on multiple benchmark datasets demonstrate the superior performance of E 2 -LLM on demanding tasks of processing long contexts.</abstract>
      <url hash="2705ac85">2024.findings-acl.252</url>
      <bibkey>liu-etal-2024-e2</bibkey>
      <doi>10.18653/v1/2024.findings-acl.252</doi>
    </paper>
    <paper id="253">
      <title>Are Female Carpenters like Blue Bananas? A Corpus Investigation of Occupation Gender Typicality</title>
      <author><first>Da</first><last>Ju</last><affiliation>Facebook</affiliation></author>
      <author><first>Karen</first><last>Ullrich</last><affiliation>Meta AI</affiliation></author>
      <author><first>Adina</first><last>Williams</last><affiliation>FAIR (Meta Platforms Inc.)</affiliation></author>
      <pages>4254-4274</pages>
      <abstract>People tend to use language to mention surprising properties of events: for example, when a banana is blue, we are more likely to mention color than when it is yellow. This fact is taken to suggest that yellowness is somehow a typical feature of bananas, and blueness is exceptional. Similar to how a yellow color is typical of bananas, there may also be genders that are typical of occupations. In this work, we explore this question using information theoretic techniques coupled with corpus statistic analysis. In two distinct large corpora, we do not find strong evidence that occupations and gender display the same patterns of mentioning as do bananas and color. Instead, we find that gender mentioning is correlated with femaleness of occupation in particular, suggesting perhaps that woman-dominated occupations are seen as somehow “more gendered” than male-dominated ones, and thereby they encourage more gender mentioning overall.</abstract>
      <url hash="1469401a">2024.findings-acl.253</url>
      <bibkey>ju-etal-2024-female</bibkey>
      <doi>10.18653/v1/2024.findings-acl.253</doi>
    </paper>
    <paper id="254">
      <title>Call Me When Necessary: <fixed-case>LLM</fixed-case>s can Efficiently and Faithfully Reason over Structured Environments</title>
      <author><first>Sitao</first><last>Cheng</last></author>
      <author><first>Ziyuan</first><last>Zhuang</last></author>
      <author><first>Yong</first><last>Xu</last></author>
      <author><first>Fangkai</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chaoyun</first><last>Zhang</last></author>
      <author><first>Xiaoting</first><last>Qin</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xiang</first><last>Huang</last></author>
      <author><first>Ling</first><last>Chen</last></author>
      <author><first>Qingwei</first><last>Lin</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Dongmei</first><last>Zhang</last><affiliation>Microsoft and Microsoft</affiliation></author>
      <author><first>Saravan</first><last>Rajmohan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>4275-4295</pages>
      <abstract>Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graphs and tables. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous works adopt LLMs to incrementally build a reasoning path, where LLMs either invoke tools or pick up items by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA and two TableQA datasets show the effectiveness of Readi, significantly surpassing previous LLM-based methods (by 9.1% Hit@1 on WebQSP, 12.4% on MQA-3H and 9.5% on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ). Our code will be available on <url>https://aka.ms/readi</url>.</abstract>
      <url hash="c47bc373">2024.findings-acl.254</url>
      <bibkey>cheng-etal-2024-call</bibkey>
      <doi>10.18653/v1/2024.findings-acl.254</doi>
    </paper>
    <paper id="255">
      <title>Legal Judgment Reimagined: <fixed-case>P</fixed-case>red<fixed-case>E</fixed-case>x and the Rise of Intelligent <fixed-case>AI</fixed-case> Interpretation in <fixed-case>I</fixed-case>ndian Courts</title>
      <author><first>Shubham</first><last>Nigam</last><affiliation>IIT Kanpur</affiliation></author>
      <author><first>Anurag</first><last>Sharma</last><affiliation>IISER Kolkata</affiliation></author>
      <author><first>Danush</first><last>Khanna</last></author>
      <author><first>Noel</first><last>Shallum</last><affiliation>Symbiosis Law School Pune</affiliation></author>
      <author><first>Kripabandhu</first><last>Ghosh</last><affiliation>Indian Institute of Science Education and Research Kolkata</affiliation></author>
      <author><first>Arnab</first><last>Bhattacharya</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>4296-4315</pages>
      <abstract>In the era of Large Language Models (LLMs), predicting judicial outcomes poses significant challenges due to the complexity of legal proceedings and the scarcity of expert-annotated datasets. Addressing this, we introduce <b>Pred</b>iction with <b>Ex</b>planation (PredEx), the largest expert-annotated dataset for legal judgment prediction and explanation in the Indian context, featuring over 15,000 annotations. This groundbreaking corpus significantly enhances the training and evaluation of AI models in legal analysis, with innovations including the application of instruction tuning to LLMs. This method has markedly improved the predictive accuracy and explanatory depth of these models for legal judgments. We employed various transformer-based models, tailored for both general and Indian legal contexts. Through rigorous lexical, semantic, and expert assessments, our models effectively leverage PredEx to provide precise predictions and meaningful explanations, establishing it as a valuable benchmark for both the legal profession and the NLP community.</abstract>
      <url hash="46262b31">2024.findings-acl.255</url>
      <bibkey>nigam-etal-2024-legal</bibkey>
      <doi>10.18653/v1/2024.findings-acl.255</doi>
    </paper>
    <paper id="256">
      <title><fixed-case>R</fixed-case>ul<fixed-case>E</fixed-case>: Knowledge Graph Reasoning with Rule Embedding</title>
      <author><first>Xiaojuan</first><last>Tang</last></author>
      <author><first>Song-Chun</first><last>Zhu</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Yitao</first><last>Liang</last><affiliation>Peking University</affiliation></author>
      <author><first>Muhan</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>4316-4335</pages>
      <abstract>Knowledge graph reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called <b>RulE</b> (stands for Rule Embedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding methods, RulE learns rule embeddings from existing triplets and first-order rules by jointly representing <b>entities</b>, <b>relations</b> and <b>logical rules</b> in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct extensive experiments to verify each component of RulE.Results on multiple benchmarks reveal that our model outperforms the majority of existing embedding-based and rule-based approaches.</abstract>
      <url hash="76462353">2024.findings-acl.256</url>
      <bibkey>tang-etal-2024-rule</bibkey>
      <doi>10.18653/v1/2024.findings-acl.256</doi>
    </paper>
    <paper id="257">
      <title>Multi-Objective Linguistic Control of Large Language Models</title>
      <author><first>Dang</first><last>Nguyen</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jiuhai</first><last>Chen</last></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>4336-4347</pages>
      <abstract>Large language models (LLMs), despite their breakthroughs on many challenging benchmark tasks, prefer to generate verbose responses and lack the controllability of output complexity, which is usually preferred by human users in practice. In this paper, we study how to precisely control multiple linguistic complexities of LLM output by finetuning using off-the-shelf data. To this end, we propose multi-control tuning (MCTune), which includes multiple linguistic complexity values of ground-truth responses as controls in the input for instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM datasets. Evaluations on widely used benchmarks demonstrate that our method does not only improve LLMs’ multi-complexity controllability substantially but also retains or even enhances the quality of the responses as a side benefit.</abstract>
      <url hash="41013a42">2024.findings-acl.257</url>
      <bibkey>nguyen-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.257</doi>
    </paper>
    <paper id="258">
      <title>Evaluating the Smooth Control of Attribute Intensity in Text Generation with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Shang</first><last>Zhou</last></author>
      <author><first>Feng</first><last>Yao</last></author>
      <author><first>Chengyu</first><last>Dong</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>4348-4362</pages>
      <abstract>Controlling the attribute intensity of text generation is crucial across scenarios (e.g., writing conciseness, chatting emotion, and explanation clarity). The remarkable capabilities of large language models (LLMs) have revolutionized text generation, prompting us to explore such <i>smooth control</i> of LLM generation. Specifically, we propose metrics to assess the range, calibration, and consistency of the generated text’s attribute intensity in response to varying control values, as well as its relevance to the intended context. To quantify the attribute intensity and context relevance, we leverage an Elo rating system and GPT4, respectively, both renowned for their robust alignment with human judgment. We look into two viable training-free methods for achieving smooth control of LLMs: (1) Prompting with semantic shifters, and (2) Modifying internal model representations. The evaluations of these two methods are conducted on 5 different attributes with various models.</abstract>
      <url hash="6078929b">2024.findings-acl.258</url>
      <bibkey>zhou-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.258</doi>
    </paper>
    <paper id="259">
      <title>Planning, Creation, Usage: Benchmarking <fixed-case>LLM</fixed-case>s for Comprehensive Tool Utilization in Real-World Complex Scenarios</title>
      <author><first>Shijue</first><last>Huang</last></author>
      <author><first>Wanjun</first><last>Zhong</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Jianqiao</first><last>Lu</last></author>
      <author><first>Qi</first><last>Zhu</last></author>
      <author><first>Jiahui</first><last>Gao</last></author>
      <author><first>Weiwen</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yutai</first><last>Hou</last></author>
      <author><first>Xingshan</first><last>Zeng</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Lifeng</first><last>Shang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <pages>4363-4400</pages>
      <abstract>The recent trend of using Large Language Models (LLMs) as tool agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools. However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization. To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs’ ability in tool utilization within real-world scenarios. UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks. It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving. A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by mapping out the intermediate steps. Thus, unlike previous work, it eliminates the restriction of pre-defined toolset. Through extensive experiments on various LLMs, we offer novel insights into the evaluation of capabilities of LLMs in tool utilization, thereby contributing a fresh perspective to this rapidly evolving field. The benchmark is publicly available at https://github.com/JoeYing1019/UltraTool.</abstract>
      <url hash="5523fc06">2024.findings-acl.259</url>
      <bibkey>huang-etal-2024-planning-creation</bibkey>
      <doi>10.18653/v1/2024.findings-acl.259</doi>
    </paper>
    <paper id="260">
      <title>Do Androids Know They’re Only Dreaming of Electric Sheep?</title>
      <author><first>Sky</first><last>CH-Wang</last><affiliation>Columbia University</affiliation></author>
      <author><first>Benjamin</first><last>Van Durme</last><affiliation>Johns Hopkins University, Johns Hopkins University, Johns Hopkins University and Microsoft</affiliation></author>
      <author><first>Jason</first><last>Eisner</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Chris</first><last>Kedzie</last><affiliation>Rasa Technologies, Inc.</affiliation></author>
      <pages>4401-4420</pages>
      <abstract>We design probes trained on the internal representations of a transformer language model to predict its hallucinatory behavior on three grounded generation tasks. To train the probes, we annotate for span-level hallucination on both sampled (organic) and manually edited (synthetic) reference outputs. Our probes are narrowly trained and we find that they are sensitive to their training domain: they generalize poorly from one task to another or from synthetic to organic hallucinations. However, on in-domain data, they can reliably detect hallucinations at many transformer layers, achieving 95% of their peak performance as early as layer 4. Here, probing proves accurate for evaluating hallucination, outperforming several contemporary baselines and even surpassing an expert human annotator in response-level detection F1. Similarly, on span-level labeling, probes are on par or better than the expert annotator on two out of three generation tasks. Overall, we find that probing is a feasible and efficient alternative to language model hallucination evaluation when model states are available.</abstract>
      <url hash="d2115fcb">2024.findings-acl.260</url>
      <bibkey>ch-wang-etal-2024-androids</bibkey>
      <doi>10.18653/v1/2024.findings-acl.260</doi>
    </paper>
    <paper id="261">
      <title><fixed-case>URG</fixed-case>: A Unified Ranking and Generation Method for Ensembling Language Models</title>
      <author><first>Bo</first><last>Lv</last></author>
      <author><first>Chen</first><last>Tang</last></author>
      <author><first>Yanan</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Ping</first><last>Luo</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yue</first><last>Yu</last><affiliation>National University of Defense Technology and PengCheng Lab</affiliation></author>
      <pages>4421-4434</pages>
      <abstract>Prior research endeavors of the ensemble Large Language Models (LLMs) achieved great success by employing an individual language model (LM) rank before the text generation. However, the use of an individual LM ranker faces two primary challenges: (1) The time-intensive nature of the ranking process, stemming from the comparisons between models; (2) The issue of error propagation arising from the separate ranking and generation models within the framework. In order to overcome these challenges, we propose a novel ensemble framework, namely Unified Ranking and Generation (URG). URG represents an end-to-end framework that jointly ranks the outputs of LLMs and generates fine-grained fusion results, via utilizing a dedicated cross-attention-based module and noise mitigation training against irrelevant information stemming from bad ranking results. Through extensive experimentation and evaluation, we demonstrate the efficiency and effectiveness of our framework in both the ranking and generation tasks. With the close coordination of the ranking and generation modules, our end-to-end framework achieves the state-of-the-art (SOTA) performance on these tasks, and exhibits substantial enhancements to any of the ensembled models.</abstract>
      <url hash="524c3b47">2024.findings-acl.261</url>
      <bibkey>lv-etal-2024-urg</bibkey>
      <doi>10.18653/v1/2024.findings-acl.261</doi>
    </paper>
    <paper id="262">
      <title>Multi-Modal Retrieval For Large Language Model Based Speech Recognition</title>
      <author><first>Aditya</first><last>Gourav</last><affiliation>Amazon</affiliation></author>
      <author><first>Jari</first><last>Kolehmainen</last><affiliation>Amazon</affiliation></author>
      <author><first>Prashanth</first><last>Shivakumar</last><affiliation>Amazon</affiliation></author>
      <author><first>Yile</first><last>Gu</last></author>
      <author><first>Grant</first><last>Strimel</last><affiliation>Amazon</affiliation></author>
      <author><first>Ankur</first><last>Gandhe</last></author>
      <author><first>Ariya</first><last>Rastrow</last></author>
      <author><first>Ivan</first><last>Bulyko</last><affiliation>Amazon</affiliation></author>
      <pages>4435-4446</pages>
      <abstract>Retrieval is a widely adopted approach for improving language models leveraging external information. As the field moves towards multi-modal large language models, it is important to extend the pure text based methods to incorporate other modalities in retrieval as well for applications across the wide spectrum of machine learning tasks and data types. In this work, we propose multi-modal retrieval with two approaches: kNN-LM and cross-attention techniques. We demonstrate the effectiveness of our retrieval approaches empirically by applying them to automatic speech recognition tasks with access to external information. Under this setting, we show that speech-based multi-modal retrieval outperforms text based retrieval, and yields up to improvement in word error rate over the multi-modal language model baseline. Furthermore, we achieve state-of-the-art recognition results on the Spoken-Squad question answering dataset.</abstract>
      <url hash="acbc18b5">2024.findings-acl.262</url>
      <bibkey>gourav-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.262</doi>
    </paper>
    <paper id="263">
      <title><fixed-case>L</fixed-case>ora<fixed-case>R</fixed-case>etriever: Input-Aware <fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case> Retrieval and Composition for Mixed Tasks in the Wild</title>
      <author><first>Ziyu</first><last>Zhao</last></author>
      <author><first>Leilei</first><last>Gan</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Guoyin</first><last>Wang</last><affiliation>Bytedance</affiliation></author>
      <author><first>Wangchunshu</first><last>Zhou</last><affiliation>AIWaves Inc.</affiliation></author>
      <author><first>Hongxia</first><last>Yang</last></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Fei</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>4447-4462</pages>
      <abstract>Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLMs). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing efficient batch inference to accommodate heterogeneous requests. Experimental results indicate that LoraRetriever consistently outperforms the baselines, highlighting its practical effectiveness and versatility. Our code is available at https://github.com/StyxXuan/LoraRetriever.</abstract>
      <url hash="982f6d50">2024.findings-acl.263</url>
      <bibkey>zhao-etal-2024-loraretriever</bibkey>
      <doi>10.18653/v1/2024.findings-acl.263</doi>
    </paper>
    <paper id="264">
      <title><fixed-case>ELAD</fixed-case>: Explanation-Guided Large Language Models Active Distillation</title>
      <author><first>Yifei</first><last>Zhang</last><affiliation>Emory University</affiliation></author>
      <author><first>Bo</first><last>Pan</last></author>
      <author><first>Chen</first><last>Ling</last></author>
      <author><first>Yuntong</first><last>Hu</last><affiliation>Emory University</affiliation></author>
      <author><first>Liang</first><last>Zhao</last><affiliation>Emory University</affiliation></author>
      <pages>4463-4475</pages>
      <abstract>The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve the efficiency of sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in reasoning explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the student model’s reasoning. Our experiments across various reasoning datasets demonstrate that our framework significantly enhances the efficiency of LLMs knowledge distillation.</abstract>
      <url hash="02ef0b83">2024.findings-acl.264</url>
      <bibkey>zhang-etal-2024-elad</bibkey>
      <doi>10.18653/v1/2024.findings-acl.264</doi>
    </paper>
    <paper id="265">
      <title>Evaluating the Elementary Multilingual Capabilities of Large Language Models with <fixed-case>M</fixed-case>ulti<fixed-case>Q</fixed-case></title>
      <author><first>Carolin</first><last>Holtermann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Timm</first><last>Dill</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>4476-4494</pages>
      <abstract>Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages.Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use.For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e. whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages beyond their intended use. Most models are more accurate when they respond faithfully. However, differences across models are large, and there is a long tail of languages where models are neither accurate nor faithful. We explore differences in tokenization as a potential explanation for our findings, identifying possible correlations that warrant further investigation.</abstract>
      <url hash="060a0d9b">2024.findings-acl.265</url>
      <bibkey>holtermann-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.265</doi>
    </paper>
    <paper id="266">
      <title>Semantics or spelling? Probing contextual word embeddings with orthographic noise</title>
      <author><first>Jacob</first><last>Matthews</last><affiliation>Cornell University</affiliation></author>
      <author><first>John</first><last>Starr</last></author>
      <author><first>Marten</first><last>Schijndel</last><affiliation>Cornell University</affiliation></author>
      <pages>4495-4504</pages>
      <abstract>Pretrained language model (PLM) hidden states are frequently employed as contextual word embeddings (CWE): high-dimensional representations that encode semantic information given linguistic context. Across many areas of computational linguistics research, similarity between CWEs is interpreted as semantic similarity. However, it remains unclear exactly what information is encoded in PLM hidden states. We investigate this practice by probing PLM representations using minimal orthographic noise. We expect that if CWEs primarily encode semantic information, a single character swap in the input word will not drastically affect the resulting representation, given sufficient linguistic context. Surprisingly, we find that CWEs generated by popular PLMs are highly sensitive to noise in input data, and that this sensitivity is related to subword tokenization: the fewer tokens used to represent a word at input, the more sensitive its corresponding CWE. This suggests that CWEs capture information unrelated to word-level meaning and can be manipulated through trivial modifications of input data. We conclude that these PLM-derived CWEs may not be reliable semantic proxies, and that caution is warranted when interpreting representational similarity.</abstract>
      <url hash="76bf0d40">2024.findings-acl.266</url>
      <bibkey>matthews-etal-2024-semantics</bibkey>
      <doi>10.18653/v1/2024.findings-acl.266</doi>
    </paper>
    <paper id="267">
      <title>The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (<fixed-case>RAG</fixed-case>)</title>
      <author><first>Shenglai</first><last>Zeng</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Jiankun</first><last>Zhang</last></author>
      <author><first>Pengfei</first><last>He</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Yiding</first><last>Liu</last><affiliation>Baidu</affiliation></author>
      <author><first>Yue</first><last>Xing</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Han</first><last>Xu</last><affiliation>University of Arizona</affiliation></author>
      <author><first>Jie</first><last>Ren</last><affiliation>Baidu and Michigan State University</affiliation></author>
      <author><first>Yi</first><last>Chang</last><affiliation>Jilin University, China</affiliation></author>
      <author><first>Shuaiqiang</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Jiliang</first><last>Tang</last><affiliation>Michigan State University</affiliation></author>
      <pages>4505-4524</pages>
      <abstract>Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model generation with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. To this end, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risks brought by RAG on the retrieval data, we further discover that RAG can be used to mitigate the old risks, i.e., the leakage of the LLMs’ training data. In general, we reveal many new insights in this paper for privacy protection of retrieval-augmented LLMs, which could benefit both LLMs and RAG systems builders.</abstract>
      <url hash="2301acbe">2024.findings-acl.267</url>
      <bibkey>zeng-etal-2024-good</bibkey>
      <doi>10.18653/v1/2024.findings-acl.267</doi>
    </paper>
    <paper id="268">
      <title><fixed-case>E</fixed-case>mpathic<fixed-case>S</fixed-case>tories++: A Multimodal Dataset for Empathy Towards Personal Experiences</title>
      <author><first>Jocelyn</first><last>Shen</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yubin</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Mohit</first><last>Hulse</last></author>
      <author><first>Wazeer</first><last>Zulfikar</last></author>
      <author><first>Sharifa</first><last>Alghowinem</last></author>
      <author><first>Cynthia</first><last>Breazeal</last></author>
      <author><first>Hae</first><last>Park</last><affiliation>Amazon and Massachusetts Institute of Technology</affiliation></author>
      <pages>4525-4536</pages>
      <abstract>Modeling empathy is a complex endeavor that is rooted in interpersonal and experiential dimensions of human interaction, and remains an open problem within AI. Existing empathy datasets fall short in capturing the richness of empathy responses, often being confined to in-lab or acted scenarios, lacking longitudinal data, and missing self-reported labels. We introduce a new multimodal dataset for empathy during personal experience sharing: the EmpathicStories++ dataset containing 53 hours of video, audio, and text data of 41 participants sharing vulnerable experiences and reading empathically resonant stories with an AI agent. EmpathicStories++ is the first longitudinal dataset on empathy, collected over a month-long deployment of social robots in participants’ homes, as participants engage in natural, empathic storytelling interactions with AI agents. We then introduce a novel task of predicting individuals’ empathy toward others’ stories based on their personal experiences, evaluated in two contexts: participants’ own personal shared story context and their reflections on stories they read. We benchmark this task using state-of-the-art models to pave the way for future improvements in contextualized and longitudinal empathy modeling. Our work provides a valuable resource for further research in developing empathetic AI systems and understanding the intricacies of human empathy within genuine, real-world settings.</abstract>
      <url hash="aea7a2d5">2024.findings-acl.268</url>
      <bibkey>shen-etal-2024-empathicstories</bibkey>
      <doi>10.18653/v1/2024.findings-acl.268</doi>
    </paper>
    <paper id="269">
      <title><fixed-case>MRL</fixed-case> Parsing Without Tears: The Case of <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Shaltiel</first><last>Shmidman</last></author>
      <author><first>Avi</first><last>Shmidman</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Moshe</first><last>Koppel</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>4537-4550</pages>
      <abstract>Syntactic parsing remains a critical tool for relation extraction and information extraction, especially in resource-scarce languages where LLMs are lacking. Yet in morphologically rich languages (MRLs), where parsers need to identify multiple lexical units in each token, existing systems suffer in latency and setup complexity. Some use a pipeline to peel away the layers: first segmentation, then morphology tagging, and then syntax parsing; however, errors in earlier layers are then propagated forward. Others use a joint architecture to evaluate all permutations at once; while this improves accuracy, it is notoriously slow. In contrast, and taking Hebrew as a test case, we present a new “flipped pipeline”: decisions are made directly on the whole-token units by expert classifiers, each one dedicated to one specific task. The classifier predictions are independent of one another, and only at the end do we synthesize their predictions. This blazingly fast approach requires only a single huggingface call, without the need for recourse to lexicons or linguistic resources. When trained on the same training set used in previous studies, our model achieves near-SOTA performance on a wide array of Hebrew NLP tasks. Furthermore, when trained on a newly enlarged training corpus, our model achieves a new SOTA for Hebrew POS tagging and dependency parsing. We release this new SOTA model to the community. Because our architecture does not rely on any language-specific resources, it can serve as a model to develop similar parsers for other MRLs.</abstract>
      <url hash="c5d83741">2024.findings-acl.269</url>
      <bibkey>shmidman-etal-2024-mrl</bibkey>
      <doi>10.18653/v1/2024.findings-acl.269</doi>
    </paper>
    <paper id="270">
      <title><fixed-case>S</fixed-case>yntax<fixed-case>S</fixed-case>hap: Syntax-aware Explainability Method for Text Generation</title>
      <author><first>Kenza</first><last>Amara</last></author>
      <author><first>Rita</first><last>Sevastjanova</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Mennatallah</first><last>El-Assady</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>4551-4566</pages>
      <abstract>To harness the power of large language models in safety-critical domains, we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces *SyntaxShap*, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, coherency, and semantic alignment of the explanations to the model. We show that our syntax-aware method produces explanations that help build more faithful and coherent explanations for predictions by autoregressive models. Confronted with the misalignment of human and AI model reasoning, this paper also highlights the need for cautious evaluation strategies in explainable AI.</abstract>
      <url hash="36dd0798">2024.findings-acl.270</url>
      <bibkey>amara-etal-2024-syntaxshap</bibkey>
      <doi>10.18653/v1/2024.findings-acl.270</doi>
    </paper>
    <paper id="271">
      <title>Automated Detection and Analysis of Data Practices Using A Real-World Corpus</title>
      <author><first>Mukund</first><last>Srinath</last></author>
      <author><first>Pranav</first><last>Narayanan Venkit</last></author>
      <author><first>Maria</first><last>Badillo</last></author>
      <author><first>Florian</first><last>Schaub</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>C.</first><last>Giles</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Shomir</first><last>Wilson</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>4567-4574</pages>
      <abstract>Privacy policies are crucial for informing users about data practices, yet their length and complexity often deter users from reading them. In this paper, we propose an automated approach to identify and visualize data practices within privacy policies at different levels of detail. Leveraging crowd-sourced annotations from the ToS;DR platform, we experiment with various methods to match policy excerpts with predefined data practice descriptions. We further conduct a case study to evaluate our approach on a real-world policy, demonstrating its effectiveness in simplifying complex policies. Experiments show that our approach accurately matches data practice descriptions with policy excerpts, facilitating the presentation of simplified privacy information to users.</abstract>
      <url hash="b2e80fbf">2024.findings-acl.271</url>
      <bibkey>srinath-etal-2024-automated</bibkey>
      <doi>10.18653/v1/2024.findings-acl.271</doi>
    </paper>
    <paper id="272">
      <title>Enhancing Hyperbolic Knowledge Graph Embeddings via Lorentz Transformations</title>
      <author><first>Xiran</first><last>Fan</last><affiliation>VISA</affiliation></author>
      <author><first>Minghua</first><last>Xu</last></author>
      <author><first>Huiyuan</first><last>Chen</last><affiliation>VISA</affiliation></author>
      <author><first>Yuzhong</first><last>Chen</last></author>
      <author><first>Mahashweta</first><last>Das</last></author>
      <author><first>Hao</first><last>Yang</last><affiliation>Visa Research</affiliation></author>
      <pages>4575-4589</pages>
      <abstract>Knowledge Graph Embedding (KGE) is a powerful technique for predicting missing links in Knowledge Graphs (KGs) by learning the entities and relations. Hyperbolic space has emerged as a promising embedding space for KGs due to its ability to represent hierarchical data. Nevertheless, most existing hyperbolic KGE methods rely on tangent approximation and are not fully hyperbolic, resulting in distortions and inaccuracies. To overcome this limitation, we propose LorentzKG, a fully hyperbolic KGE method that represents entities as points in the Lorentz model and represents relations as the intrinsic transformation—the Lorentz transformations between entities. We demonstrate that the Lorentz transformation, which can be decomposed into Lorentz rotation/reflection and Lorentz boost, captures various types of relations including hierarchical structures. Experimental results show that our LorentzKG achieves state-of-the-art performance.</abstract>
      <url hash="9f4517d0">2024.findings-acl.272</url>
      <bibkey>fan-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.272</doi>
    </paper>
    <paper id="273">
      <title>Tell Me What’s Next: Textual Foresight for Generic <fixed-case>UI</fixed-case> Representations</title>
      <author><first>Andrea</first><last>Burns</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Kate</first><last>Saenko</last><affiliation>Boston University and Boston University</affiliation></author>
      <author><first>Bryan</first><last>Plummer</last><affiliation>Boston University</affiliation></author>
      <pages>4590-4611</pages>
      <abstract>Mobile app user interfaces (UIs) are rich with action, text, structure, and image content that can be utilized to learn generic UI representations for tasks like automating user commands, summarizing content, and evaluating the accessibility of user interfaces. Prior work has learned strong visual representations with local or global captioning losses, but fails to retain both granularities.To combat this, we propose Textual Foresight, a novel pretraining objective for learning UI screen representations. Textual Foresight generates global text descriptions of future UI states given a current UI and local action taken. Our approach requires joint reasoning over elements and entire screens, resulting in improved UI features: on generation tasks, UI agents trained with Textual Foresight outperform state-of-the-art by 2% with 28x fewer images. We train with our newly constructed mobile app dataset, OpenApp, which results in the first public dataset for app UI representation learning. OpenApp enables new baselines, and we find Textual Foresight improves average task performance over them by 5.7% while having access to 2x less data.</abstract>
      <url hash="49beed71">2024.findings-acl.273</url>
      <bibkey>burns-etal-2024-tell</bibkey>
      <doi>10.18653/v1/2024.findings-acl.273</doi>
    </paper>
    <paper id="274">
      <title>Probing the Uniquely Identifiable Linguistic Patterns of Conversational <fixed-case>AI</fixed-case> Agents</title>
      <author><first>Iqra</first><last>Zahid</last></author>
      <author><first>Tharindu</first><last>Madusanka</last></author>
      <author><first>Riza</first><last>Batista-Navarro</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Youcheng</first><last>Sun</last><affiliation>The University of Manchester</affiliation></author>
      <pages>4612-4628</pages>
      <abstract>The proliferation of Conversational AI agents (CAAs) has emphasised the need to distinguish between human and machine-generated texts, with implications spanning digital forensics and cybersecurity. While prior research primarily focussed on distinguishing human from machine-generated text, our study takes a more refined approach by analysing different CAAs. We construct linguistic profiles for five CAAs, aiming to identify Uniquely Identifiable Linguistic Patterns (UILPs) for each model using authorship attribution techniques. Authorship attribution (AA) is the task of identifying the author of an unknown text from a pool of known authors. Our research seeks to answer crucial questions about the existence of UILPs in CAAs, the linguistic overlap between various text types generated by these models, and the feasibility of Authorship Attribution (AA) for CAAs based on UILPs. Promisingly, we are able to attribute CAAs based on their original texts with a weighted F1-score of 96.94%. Further, we are able to attribute CAAs according to their writing style (as specified by prompts), yielding a weighted F1-score of 95.84%, which sets the baseline for this task. By employing principal component analysis (PCA), we identify the top 100 most informative linguistic features for each CAA, achieving a weighted F1-score ranging from 86.04% to 97.93%, and an overall weighted F1-score of 93.86%.</abstract>
      <url hash="8151f61d">2024.findings-acl.274</url>
      <bibkey>zahid-etal-2024-probing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.274</doi>
    </paper>
    <paper id="275">
      <title>The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance</title>
      <author><first>Abel</first><last>Salinas</last></author>
      <author><first>Fred</first><last>Morstatter</last><affiliation>University of Southern California and USC/ISI</affiliation></author>
      <pages>4629-4651</pages>
      <abstract>Large Language Models (LLMs) are regularly being used to label data across many domains and for myriad tasks. By simply asking the LLM for an answer, or “prompting,” practitioners are able to use LLMs to quickly get a response for an arbitrary task. This prompting is done through a series of decisions by the practitioner, from simple wording of the prompt, to requesting the output in a certain data format, to jailbreaking in the case of prompts that address more sensitive topics. In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM? We answer this using a series of prompt variations across a variety of text classification tasks. We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.</abstract>
      <url hash="b79cf0b1">2024.findings-acl.275</url>
      <bibkey>salinas-morstatter-2024-butterfly</bibkey>
      <doi>10.18653/v1/2024.findings-acl.275</doi>
    </paper>
    <paper id="276">
      <title><fixed-case>X</fixed-case>-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification</title>
      <author><first>Hanzi</first><last>Xu</last></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Slobodan</first><last>Vucetic</last><affiliation>Temple University and Temple University</affiliation></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <pages>4652-4665</pages>
      <abstract>In recent years, few-shot and zero-shot learning, which learn to predict labels with limited annotated instances, have garnered significant attention. Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios. Yet, in real-world settings, label occurrences vary greatly. Some of them might appear thousands of times, while others might only appear sporadically or not at all. For practical deployment, it is crucial that a system can adapt to any label occurrence. We introduce a novel classification challenge: **X-shot**, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels co-occur without predefined limits. Here, **X** can span from 0 to positive infinity. The crux of **X-shot** centers on open-domain generalization and devising a system versatile enough to manage various label scenarios. To solve **X-shot**, we propose **BinBin** (**B**inary **IN**ference **B**ased on **IN**struction following) that leverages the Indirect Supervision from a large collection of NLP tasks via instruction following, bolstered by Weak Supervision provided by large language models. **BinBin** surpasses previous state-of-the-art techniques on three benchmark datasets across multiple domains. To our knowledge, this is the first work addressing **X-shot** learning, where **X** remains variable.</abstract>
      <url hash="23c728bb">2024.findings-acl.276</url>
      <bibkey>xu-etal-2024-x</bibkey>
      <doi>10.18653/v1/2024.findings-acl.276</doi>
    </paper>
    <paper id="277">
      <title><fixed-case>SPIN</fixed-case>: Sparsifying and Integrating Internal Neurons in Large Language Models for Text Classification</title>
      <author><first>Difan</first><last>Jiao</last></author>
      <author><first>Yilun</first><last>Liu</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Zhenwei</first><last>Tang</last><affiliation>University of Toronto</affiliation></author>
      <author><first>Daniel</first><last>Matter</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Jürgen</first><last>Pfeffer</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Ashton</first><last>Anderson</last><affiliation>Department of Computer Science, University of Toronto</affiliation></author>
      <pages>4666-4682</pages>
      <abstract>Among the many tasks that Large Language Models (LLMs) have revolutionized is text classification. Current text classification paradigms, however, rely solely on the output of the final layer in the LLM, with the rich information contained in internal neurons largely untapped. In this study, we present SPIN: a model-agnostic framework that sparsifies and integrates internal neurons of intermediate layers of LLMs for text classification. Specifically, SPIN sparsifies internal neurons by linear probing-based salient neuron selection layer by layer, avoiding noise from unrelated neurons and ensuring efficiency. The cross-layer salient neurons are then integrated to serve as multi-layered features for the classification head. Extensive experimental results show our proposed SPIN significantly improves text classification accuracy, efficiency, and interpretability.</abstract>
      <url hash="b5f7c211">2024.findings-acl.277</url>
      <bibkey>jiao-etal-2024-spin</bibkey>
      <doi>10.18653/v1/2024.findings-acl.277</doi>
    </paper>
    <paper id="278">
      <title>Decomposing Co-occurrence Matrices into Interpretable Components as Formal Concepts</title>
      <author><first>Akihiro</first><last>Maeda</last><affiliation>Japan Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Takuma</first><last>Torii</last><affiliation>Tokyo Denki University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Shohei</first><last>Hidaka</last><affiliation>Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>4683-4700</pages>
      <abstract>This study addresses the interpretability of word representations through an investigation of a count-based co-occurrence matrix. Employing the mathematical methodology of Formal Concept Analysis, we reveal an underlying structure that is amenable to human interpretation. Furthermore, we unveil the emergence of hierarchical and geometrical structures within word vectors as consequences of word usage. Our experiments on the PPMI matrix demonstrate that the formal concepts that we identified align with interpretable categories, as shown in the category completion task.</abstract>
      <url hash="92f6d339">2024.findings-acl.278</url>
      <bibkey>maeda-etal-2024-decomposing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.278</doi>
    </paper>
    <paper id="279">
      <title>Two-Pronged Human Evaluation of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> Self-Correction in Radiology Report Simplification</title>
      <author><first>Ziyu</first><last>Yang</last></author>
      <author><first>Santhosh</first><last>Cherian</last><affiliation>Temple University</affiliation></author>
      <author><first>Slobodan</first><last>Vucetic</last><affiliation>Temple University and Temple University</affiliation></author>
      <pages>4701-4714</pages>
      <abstract>Radiology reports are highly technical documents aimed primarily at doctor-doctor communication. There has been an increasing interest in sharing those reports with patients, necessitating providing them patient-friendly simplifications of the original reports. This study explores the suitability of large language models in automatically generating those simplifications. We examine the usefulness of chain-of-thought and self-correction prompting mechanisms in this domain. We also propose a new evaluation protocol that employs radiologists and laypeople, where radiologists verify the factual correctness of simplifications, and laypeople assess simplicity and comprehension. Our experimental results demonstrate the effectiveness of self-correction prompting in producing high-quality simplifications. Our findings illuminate the preferences of radiologists and laypeople regarding text simplification, informing future research on this topic.</abstract>
      <url hash="c69a2c06">2024.findings-acl.279</url>
      <bibkey>yang-etal-2024-two</bibkey>
      <doi>10.18653/v1/2024.findings-acl.279</doi>
    </paper>
    <paper id="280">
      <title>Planning First, Question Second: An <fixed-case>LLM</fixed-case>-Guided Method for Controllable Question Generation</title>
      <author><first>Kunze</first><last>Li</last></author>
      <author><first>Yu</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>4715-4729</pages>
      <abstract>In the field of education, for better assessment of students’ abilities, generated questions often need to meet experts’ requirements, indicating the need for controllable question generation (CQG). However, current CQG methods mainly focus on difficulty control, neglecting the control of question content and assessed abilities, which are also crucial in educational QG. In this paper, we propose an LLM-guided method PFQS (for Planning First, Question Second), which utilizes Llama 2 to generate an answer plan and then generates questions based on it. The plan not only includes candidate answers but also integrates LLM’s understanding and multiple requirements, which make question generation simple and controllable. We evaluate our approach on the FairytaleQA dataset, a well-structured QA dataset derived from child-friendly storybooks. In the dataset, the attribute label represents content control, while the local_or_sum and ex_or_im labels denote difficulty control. Experimental results demonstrate that our approach outperforms previous state-of-the-art results and achieves better consistency with requirements compared to prompt-based method. Further application of our method to Llama 2 and Mistral also leads to improved requirement consistency in a zero-shot setting.</abstract>
      <url hash="a45afb73">2024.findings-acl.280</url>
      <bibkey>li-zhang-2024-planning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.280</doi>
    </paper>
    <paper id="281">
      <title><fixed-case>RA</fixed-case>-<fixed-case>ISF</fixed-case>: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback</title>
      <author><first>Yanming</first><last>Liu</last></author>
      <author><first>Xinyue</first><last>Peng</last></author>
      <author><first>Xuhong</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Weihao</first><last>Liu</last></author>
      <author><first>Jianwei</first><last>Yin</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jiannan</first><last>Cao</last></author>
      <author><first>Tianyu</first><last>Du</last><affiliation>Zhejiang University</affiliation></author>
      <pages>4730-4749</pages>
      <abstract>Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn’t previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model’s problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities and reducing hallucinations.</abstract>
      <url hash="3e9ba282">2024.findings-acl.281</url>
      <bibkey>liu-etal-2024-ra</bibkey>
      <doi>10.18653/v1/2024.findings-acl.281</doi>
    </paper>
    <paper id="282">
      <title><fixed-case>M</fixed-case>r<fixed-case>R</fixed-case>ank: Improving Question Answering Retrieval System through Multi-Result Ranking Model</title>
      <author><first>Danupat</first><last>Khamnuansin</last><affiliation>Chulalongkorn University and KASIKORN Business-Technology Group</affiliation></author>
      <author><first>Tawunrat</first><last>Chalothorn</last><affiliation>KASIKORN Business-Technology Group</affiliation></author>
      <author><first>Ekapol</first><last>Chuangsuwanich</last><affiliation>Chulalongkorn University</affiliation></author>
      <pages>4750-4762</pages>
      <abstract>Large Language Models (LLMs) often struggle with hallucinations and outdated information. To address this, Information Retrieval (IR) systems can be employed to augment LLMs with up-to-date knowledge. However, existing IR techniques contain deficiencies, posing a performance bottleneck. Given the extensive array of IR systems, combining diverse approaches presents a viable strategy. Nevertheless, prior attempts have yielded restricted efficacy. In this work, we propose an approach that leverages learning-to-rank techniques to combine heterogeneous IR systems. We demonstrate the method on two Retrieval Question Answering (ReQA) tasks. Our empirical findings exhibit a significant performance enhancement, outperforming previous approaches and achieving state-of-the-art results on ReQA SQuAD.</abstract>
      <url hash="b8f2df26">2024.findings-acl.282</url>
      <bibkey>khamnuansin-etal-2024-mrrank</bibkey>
      <doi>10.18653/v1/2024.findings-acl.282</doi>
    </paper>
    <paper id="283">
      <title>Chain-of-Question: A Progressive Question Decomposition Approach for Complex Knowledge Base Question Answering</title>
      <author><first>Peng</first><last>Yixing</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Quan</first><last>Wang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Licheng</first><last>Zhang</last></author>
      <author><first>Yi</first><last>Liu</last><affiliation>State Key Laboratory of Communication Content Cognition</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>4763-4776</pages>
      <abstract>Complex KBQA leverages the knowledge base (KB) to answer complex natural questions involving complicated semantics like multi-hop reasoning. Existing methods involve a question decomposition process, i.e., breaking a complex question into several simpler sub-questions, to assist obtaining logical forms for querying the KB. However, existing question decomposition process derives all sub-questions directly according to the original question, resulting in limitations when one sub-question relies on the answer from a previous one. In this work, we propose Chain-of-Question, a progressive question decomposition approach to address complex KBQA challenges. First, inspired by chain-of-thought, we design a prompt to guide LLM to sequentially decompose multiple semantically clear sub-questions and provide corresponding reference answers, where each step of the decomposition relies on the previous results. Next, we utilize the decomposition result to select relevant patterns (relation-entity pairs) as accurate and faithful auxiliary information for the following logical form generation. Finally, we jointly perform logical form generation and answer prediction, utilizing the predicted answer to supplement non-executable logical forms. Experimental results demonstrate that our method achieves state-of-the-art performance on multiple datasets.</abstract>
      <url hash="3340bc88">2024.findings-acl.283</url>
      <bibkey>yixing-etal-2024-chain</bibkey>
      <doi>10.18653/v1/2024.findings-acl.283</doi>
    </paper>
    <paper id="284">
      <title>Instruction Tuning with Retrieval-based Examples Ranking for Aspect-based Sentiment Analysis</title>
      <author><first>Guangmin</first><last>Zheng</last></author>
      <author><first>Jin</first><last>Wang</last><affiliation>Yunnan University</affiliation></author>
      <author><first>Liang-Chih</first><last>Yu</last><affiliation>Yuan Ze University</affiliation></author>
      <author><first>Xuejie</first><last>Zhang</last><affiliation>Yunnan University</affiliation></author>
      <pages>4777-4788</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) identifies sentiment information related to specific aspects and provides deeper market insights to businesses and organizations. With the emergence of large language models (LMs), recent studies have proposed using fixed examples for instruction tuning to reformulate ABSA as a generation task. However, the performance is sensitive to the selection of in-context examples; several retrieval methods are based on surface similarity and are independent of the LM generative objective. This study proposes an instruction learning method with retrieval-based example ranking for ABSA tasks. For each target sample, an LM was applied as a scorer to estimate the likelihood of the output given the input and a candidate example as the prompt, and training examples were labeled as positive or negative by ranking the scores. An alternating training schema is proposed to train both the retriever and LM. Instructional prompts can be constructed using high-quality examples. The LM is used for both scoring and inference, improving the generation efficiency without incurring additional computational costs or training difficulties. Extensive experiments on three ABSA subtasks verified the effectiveness of the proposed method, demonstrating its superiority over various strong baseline models. Code and data are released at https://github.com/zgMin/IT-RER-ABSA.</abstract>
      <url hash="8bea2ec6">2024.findings-acl.284</url>
      <bibkey>zheng-etal-2024-instruction</bibkey>
      <doi>10.18653/v1/2024.findings-acl.284</doi>
    </paper>
    <paper id="285">
      <title>Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation</title>
      <author><first>Xinyi</first><last>Mou</last></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>4789-4809</pages>
      <abstract>Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrate the effectiveness and flexibility of our method.</abstract>
      <url hash="39bef448">2024.findings-acl.285</url>
      <bibkey>mou-etal-2024-unveiling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.285</doi>
    </paper>
    <paper id="286">
      <title>Incorporating Syntax and Lexical Knowledge to Multilingual Sentiment Classification on Large Language Models</title>
      <author><first>Hiroshi</first><last>Kanayama</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Yang</first><last>Zhao</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ran</first><last>Iwamoto</last><affiliation>IBM Research - Tokyo, International Business Machines and Keio University</affiliation></author>
      <author><first>Takuya</first><last>Ohko</last><affiliation>IBM Research - Tokyo, International Business Machines</affiliation></author>
      <pages>4810-4817</pages>
      <abstract>This paper exploits a sentiment extractor supported by syntactic and lexical resources to enhance multilingual sentiment classification solved through the generative approach, without retraining LLMs. By adding external information of words and phrases that have positive/negative polarities, the multilingual sentiment classification error was reduced by up to 33 points, and the combination of two approaches performed best especially in high-performing pairs of LLMs and languages.</abstract>
      <url hash="3d0d7157">2024.findings-acl.286</url>
      <bibkey>kanayama-etal-2024-incorporating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.286</doi>
    </paper>
    <paper id="287">
      <title>Locating and Extracting Relational Concepts in Large Language Models</title>
      <author><first>Zijian</first><last>Wang</last></author>
      <author><first>Britney</first><last>Whyte</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Chang</first><last>Xu</last><affiliation>University of Sydney</affiliation></author>
      <pages>4818-4832</pages>
      <abstract>Relational concepts are indeed foundational to the structure of knowledge representation, as they facilitate the association between various entity concepts, allowing us to express and comprehend complex world knowledge.By expressing relational concepts in natural language prompts, people can effortlessly interact with large language models (LLMs) and recall desired factual knowledge. However, the process of knowledge recall lacks interpretability, and representations of relational concepts within LLMs remain unknown to us. In this paper, we identify hidden states that can express entity and relational concepts through causal mediation analysis in fact recall processes. Our finding reveals that at the last token position of the input prompt, there are hidden states that solely express the causal effects of relational concepts. Based on this finding, we assume that these hidden states can be treated as relational representations and we can successfully extract them from LLMs. The experimental results demonstrate high credibility of the relational representations: they can be flexibly transplanted into other fact recall processes, and can also be used as robust entity connectors. Moreover, we also show that the relational representations exhibit significant potential for controllable fact recall through relation rewriting.</abstract>
      <url hash="753dd161">2024.findings-acl.287</url>
      <bibkey>wang-etal-2024-locating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.287</doi>
    </paper>
    <paper id="288">
      <title>Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models</title>
      <author><first>Mingda</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Xinyu</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yifan</first><last>Chen</last></author>
      <author><first>Wenfeng</first><last>Xuan</last><affiliation>XVERSE</affiliation></author>
      <author><first>Weinan</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>4833-4850</pages>
      <abstract>Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example-level performance inconsistency exists not only between retrieval-augmented and retrieval-free LM but also among different retrievers. To understand this phenomenon, we investigate the degeneration behavior of RALMs and theoretically decompose it into four categories. Further analysis based on our decomposition reveals that the innate difference in knowledge sources and the unpredictable degeneration of the reader model contribute most to the inconsistency. Drawing from our analysis, we introduce Ensemble of Retrievers (EoR), a trainable framework that can adaptively retrieve from different knowledge sources and effectively decrease unpredictable reader errors. Our experiments on Open Domain Question Answering show that EoR substantially improves performance over the RALM with a single retriever by considerably reducing inconsistent behaviors.</abstract>
      <url hash="993ad49b">2024.findings-acl.288</url>
      <bibkey>li-etal-2024-unraveling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.288</doi>
    </paper>
    <paper id="289">
      <title><fixed-case>S</fixed-case>entic<fixed-case>V</fixed-case>ec: Toward Robust and Human-Centric Neurosymbolic Sentiment Analysis</title>
      <author><first>Xulang</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>4851-4863</pages>
      <abstract>The success of state-of-the-art Natural Language Processing (NLP) systems heavily depends on deep neural networks, which excel in various tasks through strong data fitting and latent feature modeling abilities. However, certain challenges linked to deep neural networks and supervised deep learning deserve considerations, e.g., extensive computing resources, knowledge forgetting, etc. Previous research attempted to tackle these challenges individually through irrelative techniques. However, they do not instigate fundamental shifts in the learning paradigm. In this work, we propose a novel neurosymbolic method for sentiment analysis to tackle these issues. We also propose a novel sentiment-pragmatic knowledge base that places emphasis on human subjectivity within varying domain annotations. We conducted extensive experiments to show that our neurosymbolic framework for sentiment analysis stands out for its lightweight nature, robustness across domains and languages, efficient few-shot training, and rapid convergence.</abstract>
      <url hash="65c429bc">2024.findings-acl.289</url>
      <bibkey>zhang-etal-2024-senticvec</bibkey>
      <doi>10.18653/v1/2024.findings-acl.289</doi>
    </paper>
    <paper id="290">
      <title>Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models</title>
      <author><first>Chen</first><last>Qian</last></author>
      <author><first>Jie</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Yao</last></author>
      <author><first>Dongrui</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhenfei</first><last>Yin</last><affiliation>University of Sydney and Shanghai AI Laboratory</affiliation></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Renmin University of China and Institute of information engineering, CAS</affiliation></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>4864-4888</pages>
      <abstract>Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs’ trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs’ trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that <i>LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension</i>. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM’s pre-training checkpoints to enhance the LLM’s trustworthiness. Finally, inspired by the theoretical result that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field.</abstract>
      <url hash="91ba51f8">2024.findings-acl.290</url>
      <bibkey>qian-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-acl.290</doi>
    </paper>
    <paper id="291">
      <title>Language Models can Evaluate Themselves via Probability Discrepancy</title>
      <author><first>Tingyu</first><last>Xia</last></author>
      <author><first>Bowen</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yuan</first><last>Wu</last><affiliation>Jilin University</affiliation></author>
      <author><first>Yi</first><last>Chang</last><affiliation>Jilin University, China</affiliation></author>
      <author><first>Chang</first><last>Zhou</last></author>
      <pages>4889-4901</pages>
      <abstract>In this paper, we begin by illustrating that, when presented with a query, Large Language Models (LLMs) capable of providing accurate responses tend to exhibit a more uniform probability distribution compared to their less proficient counterparts. Building upon this observation, we introduce a novel self-assessment criterion termed ProbDiff for evaluating the performance of diverse LLMs. This method eliminates the need for training an additional evaluation model or relying on external proprietary models such as GPT-4 as a judger. Instead, it solely relies on the LLMs under evaluation to compute the probability discrepancy between the original response generation and its revised versions. A higher discrepancy in two LLMs for the same query suggests a relatively weaker ability. We discover that ProbDiff yields comparable results to mainstream GPT-4-based evaluations on various scenarios including NLG tasks like translation and summarization, as well as LLM evaluation benchmarks such as AlignBench, MT-Bench, and AlpacaEval, across LLMs of different sizes.</abstract>
      <url hash="01d45e27">2024.findings-acl.291</url>
      <bibkey>xia-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.291</doi>
    </paper>
    <paper id="292">
      <title>Evaluating the Validity of Word-level Adversarial Attacks with Large Language Models</title>
      <author><first>Huichi</first><last>Zhou</last></author>
      <author><first>Zhaoyang</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hongtao</first><last>Wang</last><affiliation>North China Electric Power University</affiliation></author>
      <author><first>Dongping</first><last>Chen</last></author>
      <author><first>Wenhan</first><last>Mu</last></author>
      <author><first>Fangyuan</first><last>Zhang</last></author>
      <pages>4902-4922</pages>
      <abstract>Deep neural networks exhibit vulnerability to word-level adversarial attacks in natural language processing. Most of these attack methods adopt synonymous substitutions to perturb original samples for crafting adversarial examples while attempting to maintain semantic consistency with the originals. Some of them claim that they could achieve over 90% attack success rate, thereby raising serious safety concerns. However, our investigation reveals that many purportedly successful adversarial examples are actually invalid due to significant changes in semantic meanings compared to their originals. Even when equipped with semantic constraints such as BERTScore, existing attack methods can generate up to 87.9% invalid adversarial examples. Building on this insight, we first curate a 13K dataset for adversarial validity evaluation with the help of GPT-4. Then, an open-source large language model is fine-tuned to offer an interpretable validity score for assessing the semantic consistency between original and adversarial examples. Finally, this validity score can serve as a guide for existing adversarial attack methods to generate valid adversarial examples. Comprehensive experiments demonstrate the effectiveness of our method in evaluating and refining the quality of adversarial examples.</abstract>
      <url hash="282dfb71">2024.findings-acl.292</url>
      <bibkey>zhou-etal-2024-evaluating-validity</bibkey>
      <doi>10.18653/v1/2024.findings-acl.292</doi>
    </paper>
    <paper id="293">
      <title>On the Language Encoder of Contrastive Cross-modal Models</title>
      <author><first>Mengjie</first><last>Zhao</last><affiliation>Sony</affiliation></author>
      <author><first>Junya</first><last>Ono</last></author>
      <author><first>Zhi</first><last>Zhong</last><affiliation>Sony Group Corporation</affiliation></author>
      <author><first>Chieh-Hsin</first><last>Lai</last><affiliation>Sony AI</affiliation></author>
      <author><first>Yuhta</first><last>Takida</last><affiliation>Sony AI</affiliation></author>
      <author><first>Naoki</first><last>Murata</last><affiliation>Sony AI and Sony Group Corporation</affiliation></author>
      <author><first>Wei-Hsiang</first><last>Liao</last><affiliation>Sony Corporation</affiliation></author>
      <author><first>Takashi</first><last>Shibuya</last><affiliation>Sony AI</affiliation></author>
      <author><first>Hiromi</first><last>Wakaki</last><affiliation>Sony Group Corporation</affiliation></author>
      <author><first>Yuki</first><last>Mitsufuji</last><affiliation>Sony AI, Sony Group Corporation, Tokyo Institute of Technology, Tokyo Institute of Technology and Sony Group Corporation</affiliation></author>
      <pages>4923-4940</pages>
      <abstract>Contrastive cross-modal models such as CLIP and CLAP aid various vision-language (VL) and audio-language (AL) tasks. However, there has been limited investigation of and improvement in their language encoder – the central component of encoding natural language descriptions of image/audio into vector representations. We extensively evaluate how unsupervised and supervised sentence embedding training affect language encoder quality and cross-modal task performance. In VL pretraining, we found that sentence embedding training enhances language encoder quality and aids in cross-modal tasks, improving contrastive VL models such as CyCLIP. Sentence embedding training benefits AL tasks when the amount of training data is large. We analyze the representation spaces to understand the strengths of sentence embedding training, and find that it improves text-space uniformity, at the cost of decreased cross-modal alignment.</abstract>
      <url hash="c62dd811">2024.findings-acl.293</url>
      <bibkey>zhao-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.293</doi>
    </paper>
    <paper id="294">
      <title>Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World</title>
      <author><first>Guande</first><last>Wu</last></author>
      <author><first>Chen</first><last>Zhao</last><affiliation>New York University Shanghai</affiliation></author>
      <author><first>Claudio</first><last>Silva</last><affiliation>New York University</affiliation></author>
      <author><first>He</first><last>He</last><affiliation>New York University</affiliation></author>
      <pages>4941-4957</pages>
      <abstract>Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM’s ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner’s state and identify and correct execution errors. Both human-machine and machine-machine experiments show that LLM agents have strong grounding capacities, and our approach significantly improves the evaluation metric.</abstract>
      <url hash="ba9fd5b5">2024.findings-acl.294</url>
      <bibkey>wu-etal-2024-co</bibkey>
      <doi>10.18653/v1/2024.findings-acl.294</doi>
    </paper>
    <paper id="295">
      <title>Anchor-based Large Language Models</title>
      <author><first>Jianhui</first><last>Pang</last></author>
      <author><first>Fanghua</first><last>Ye</last></author>
      <author><first>Derek</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Xin</first><last>He</last></author>
      <author><first>Wanshun</first><last>Chen</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <pages>4958-4976</pages>
      <abstract>Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces Anchor-based LLMs (AnLLMs), which utilize an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments on question-answering benchmarks reveal that AnLLMs maintain similar accuracy levels while achieving up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the substantial enhancements of AnLLMs employing the AnSAN technique in resource utilization and computational efficiency underscore their potential for practical LLM applications.</abstract>
      <url hash="08ffe0c8">2024.findings-acl.295</url>
      <bibkey>pang-etal-2024-anchor</bibkey>
      <doi>10.18653/v1/2024.findings-acl.295</doi>
    </paper>
    <paper id="296">
      <title><fixed-case>ML</fixed-case>e<fixed-case>VLM</fixed-case>: Improve Multi-level Progressive Capabilities based on Multimodal Large Language Model for Medical Visual Question Answering</title>
      <author><first>Dexuan</first><last>Xu</last></author>
      <author><first>Yanyuan</first><last>Chen</last></author>
      <author><first>Jieyi</first><last>Wang</last></author>
      <author><first>Yue</first><last>Huang</last></author>
      <author><first>Hanpin</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University and Peking University</affiliation></author>
      <author><first>Hongxing</first><last>Wang</last><affiliation>Capital Medical University</affiliation></author>
      <author><first>Weihua</first><last>Yue</last></author>
      <author><first>Jing</first><last>He</last></author>
      <author><first>Hang</first><last>Li</last><affiliation>Peking University First Hospital</affiliation></author>
      <author><first>Yu</first><last>Huang</last><affiliation>Peking University</affiliation></author>
      <pages>4977-4997</pages>
      <abstract>Medical visual question answering (MVQA) requires in-depth understanding of medical images and questions to provide reliable answers. We summarize multi-level progressive capabilities that models need to focus on in MVQA: recognition, details, diagnosis, knowledge, and reasoning. Existing MVQA models tend to ignore the above capabilities due to unspecific data and plain architecture. To address these issues, this paper proposes Multi-level Visual Language Model (MLeVLM) for MVQA. On the data side, we construct a high-quality multi-level instruction dataset MLe-VQA via GPT-4, which covers multi-level questions and answers as well as reasoning processes from visual clues to semantic cognition. On the architecture side, we propose a multi-level feature alignment module, including attention-based token selector and context merger, which can efficiently align features at different levels from visual to semantic. To better evaluate the model’s capabilities, we manually construct a multi-level MVQA evaluation benchmark named MLe-Bench. Extensive experiments demonstrate the effectiveness of our constructed multi-level instruction dataset and the multi-level feature alignment module. It also proves that MLeVLM outperforms existing medical multimodal large language models.</abstract>
      <url hash="f80a0518">2024.findings-acl.296</url>
      <bibkey>xu-etal-2024-mlevlm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.296</doi>
    </paper>
    <paper id="297">
      <title>Disentangling Length from Quality in Direct Preference Optimization</title>
      <author><first>Ryan</first><last>Park</last></author>
      <author><first>Rafael</first><last>Rafailov</last><affiliation>Stanford University</affiliation></author>
      <author><first>Stefano</first><last>Ermon</last><affiliation>Stanford University</affiliation></author>
      <author><first>Chelsea</first><last>Finn</last><affiliation>Stanford University and Google</affiliation></author>
      <pages>4998-5017</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstrapping. We then develop a principled but simple regularization strategy that prevents length exploitation, while still maintaining improvements in model quality. We demonstrate these affects across datasets on summarization and dialogue, where we achieve up to 20% improvement in win rates when controlling for length, despite the GPT4 judge’s well-known verbosity bias.</abstract>
      <url hash="0da61a50">2024.findings-acl.297</url>
      <bibkey>park-etal-2024-disentangling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.297</doi>
    </paper>
    <paper id="298">
      <title><fixed-case>MIKE</fixed-case>: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing</title>
      <author><first>Jiaqi</first><last>Li</last><affiliation>Southeast University</affiliation></author>
      <author><first>Miaozeng</first><last>Du</last></author>
      <author><first>Chuanyi</first><last>Zhang</last><affiliation>Hohai University</affiliation></author>
      <author><first>Yongrui</first><last>Chen</last></author>
      <author><first>Nan</first><last>Hu</last><affiliation>Southeast University</affiliation></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <author><first>Haiyun</first><last>Jiang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Siyuan</first><last>Cheng</last></author>
      <author><first>Bozhong</first><last>Tian</last></author>
      <pages>5018-5029</pages>
      <abstract>Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations, we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring the complexity of FG knowledge editing in MLLMs. Our findings spotlight the urgent need for novel approaches in this domain, setting a clear agenda for future research and development efforts within the community.</abstract>
      <url hash="f897d861">2024.findings-acl.298</url>
      <bibkey>li-etal-2024-mike</bibkey>
      <doi>10.18653/v1/2024.findings-acl.298</doi>
    </paper>
    <paper id="299">
      <title>Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise: A Case Study on <fixed-case>C</fixed-case>hinese Legal Domain</title>
      <author><first>Zhen</first><last>Wan</last></author>
      <author><first>Yating</first><last>Zhang</last></author>
      <author><first>Yexiang</first><last>Wang</last></author>
      <author><first>Fei</first><last>Cheng</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>Kyoto University</affiliation></author>
      <pages>5030-5041</pages>
      <abstract>While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it’s not plausible to continue training LLMs of the GPT-4’s scale on in-domain data.This paper introduces a simple yet effective domain adaptation framework for GPT-4 by reformulating generation as an adapt-retrieve-revise process. The initial step is to adapt an affordable 7B LLM to the Chinese legal domain by continuing learning in-domain data. When solving an in-domain task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answer will be used to retrieve supporting evidence candidates from an external in-domain knowledge base. Finally, the draft answer and retrieved evidence are concatenated into a whole prompt to let GPT-4 assess the evidence and revise the draft answer to generate the final answer. Our proposal combines the advantages of the efficiency of adapting a smaller 7B model with the evidence-assessing capability of GPT-4 and effectively prevents GPT-4 from generating hallucinatory content. In the zero-shot setting of four Chinese legal tasks, our method improves the average score by +33.6 points, compared to GPT-4 direct generation. When compared to two stronger retrieval-based baselines, our method outperforms them by +17.0 and +23.5.</abstract>
      <url hash="85146a38">2024.findings-acl.299</url>
      <bibkey>wan-etal-2024-reformulating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.299</doi>
    </paper>
    <paper id="300">
      <title><fixed-case>M</fixed-case>eme<fixed-case>MQA</fixed-case>: Multimodal Question Answering for Memes via Rationale-Based Inferencing</title>
      <author><first>Siddhant</first><last>Agarwal</last></author>
      <author><first>Shivam</first><last>Sharma</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>5042-5078</pages>
      <abstract>Memes have evolved as a prevalent medium for diverse communication, ranging from humour to propaganda. With the rising popularity of image-focused content, there is a growing need to explore its potential harm from different aspects. Previous studies have analyzed memes in closed settings - detecting harm, applying semantic labels, and offering natural language explanations. To extend this research, we introduce MemeMQA, a multimodal question-answering framework aiming to solicit accurate responses to structured questions while providing coherent explanations. We curate MemeMQACorpus, a new dataset featuring 1,880 questions related to 1,122 memes with corresponding answer-explanation pairs. We further propose ARSENAL, a novel two-stage multimodal framework that leverages the reasoning capabilities of LLMs to address MemeMQA. We benchmark MemeMQA using competitive baselines and demonstrate its superiority - ~18% enhanced answer prediction accuracy and distinct text generation lead across various metrics measuring lexical and semantic alignment over the best baseline. We analyze ARSENAL’s robustness through diversification of question-set, confounder-based evaluation regarding MemeMQA’s generalizability, and modality-specific assessment, enhancing our understanding of meme interpretation in the multimodal communication landscape.</abstract>
      <url hash="4133926c">2024.findings-acl.300</url>
      <bibkey>agarwal-etal-2024-mememqa</bibkey>
      <doi>10.18653/v1/2024.findings-acl.300</doi>
    </paper>
    <paper id="301">
      <title>Improving Attributed Text Generation of Large Language Models via Preference Learning</title>
      <author><first>Dongfang</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zetian</first><last>Sun</last></author>
      <author><first>Baotian</first><last>Hu</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Zhenyu</first><last>Liu</last></author>
      <author><first>Xinshuo</first><last>Hu</last></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technolgy, Shenzhen</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>5079-5101</pages>
      <abstract>Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content. Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations). However, current attribution methods usually focus on the retrieval stage and automatic evaluation that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility. In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an Automatic Preference Optimization (APO) framework. First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets. Second, considering the high cost of labelling preference data, we further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs. Moreover, inspired by the human citation process, we further propose a progressive preference optimization method by leveraging fine-grained information. Extensive experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate that APO achieves state-of-the-art citation F1 with higher answer quality.</abstract>
      <url hash="81b3b69d">2024.findings-acl.301</url>
      <bibkey>li-etal-2024-improving-attributed</bibkey>
      <doi>10.18653/v1/2024.findings-acl.301</doi>
    </paper>
    <paper id="302">
      <title><fixed-case>KOMBO</fixed-case>: <fixed-case>K</fixed-case>orean Character Representations Based on the Combination Rules of Subcharacters</title>
      <author><first>SungHo</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Juhyeong</first><last>Park</last><affiliation>Korea University</affiliation></author>
      <author><first>Yeachan</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>SangKeun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>5102-5119</pages>
      <abstract>The Korean writing system, Hangeul, has a unique character representation rigidly following the invention principles recorded in Hunminjeongeum. However, existing pre-trained language models (PLMs) for Korean have overlooked these principles. In this paper, we introduce a novel framework for Korean PLMs called KOMBO, which firstly brings the invention principles of Hangeul to represent character. Our proposed method, KOMBO, exhibits notable experimental proficiency across diverse NLP tasks. In particular, our method outperforms the state-of-the-art Korean PLM by an average of 2.11% in five Korean natural language understanding tasks. Furthermore, extensive experiments demonstrate that our proposed method is suitable for comprehending the linguistic features of the Korean language. Consequently, we shed light on the superiority of using subcharacters over the typical subword-based approach for Korean PLMs. Our code is available at: https://github.com/SungHo3268/KOMBO.</abstract>
      <url hash="72fdbe43">2024.findings-acl.302</url>
      <bibkey>kim-etal-2024-kombo</bibkey>
      <doi>10.18653/v1/2024.findings-acl.302</doi>
    </paper>
    <paper id="303">
      <title>Tree-Planted Transformers: Unidirectional Transformer Language Models with Implicit Syntactic Supervision</title>
      <author><first>Ryo</first><last>Yoshida</last><affiliation>The University of Tokyo</affiliation></author>
      <author><first>Taiga</first><last>Someya</last></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>5120-5134</pages>
      <abstract>Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance; however, they have trouble with inference efficiency due to the explicit generation of syntactic structures. In this paper, we propose a new method dubbed tree-planting: instead of explicitly generating syntactic structures, we “plant” trees into attention weights of unidirectional Transformer LMs to implicitly reflect syntactic structures of natural language. Specifically, unidirectional Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which inherit the training efficiency from SLMs without changing the inference efficiency of their underlying Transformer LMs. Targeted syntactic evaluations on the SyntaxGym benchmark demonstrated that TPTs, despite the lack of explicit generation of syntactic structures, significantly outperformed not only vanilla Transformer LMs but also various SLMs that generate hundreds of syntactic structures in parallel. This result suggests that TPTs can learn human-like syntactic knowledge as data-efficiently as SLMs while maintaining the modeling space of Transformer LMs unchanged.</abstract>
      <url hash="3a2803c8">2024.findings-acl.303</url>
      <bibkey>yoshida-etal-2024-tree</bibkey>
      <doi>10.18653/v1/2024.findings-acl.303</doi>
    </paper>
    <paper id="304">
      <title>Play Guessing Game with <fixed-case>LLM</fixed-case>: Indirect Jailbreak Attack with Implicit Clues</title>
      <author><first>Zhiyuan</first><last>Chang</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Mingyang</first><last>Li</last><affiliation>ISCAS</affiliation></author>
      <author><first>Yi</first><last>Liu</last></author>
      <author><first>Junjie</first><last>Wang</last></author>
      <author><first>Qing</first><last>Wang</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>5135-5147</pages>
      <abstract>With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM’s defensive strategies and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of “When unable to attack, defend” from Sun Tzu’s Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. The experimental results indicate that the Query Success Rate of the Puzzler is 14.0%-82.7% higher than baselines on the most prominent LLMs. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines.</abstract>
      <url hash="0f720fbd">2024.findings-acl.304</url>
      <bibkey>chang-etal-2024-play</bibkey>
      <doi>10.18653/v1/2024.findings-acl.304</doi>
    </paper>
    <paper id="305">
      <title>Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes</title>
      <author><first>Sunjun</first><last>Kweon</last></author>
      <author><first>Junu</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jiyoun</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Sujeong</first><last>Im</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Eunbyeol</first><last>Cho</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Seongsu</first><last>Bae</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Jungwoo</first><last>Oh</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Gyubok</first><last>Lee</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Jong Hak</first><last>Moon</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Seng Chan</first><last>You</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Seungjin</first><last>Baek</last><affiliation>Yonsei university</affiliation></author>
      <author><first>Chang Hoon</first><last>Han</last></author>
      <author><first>Yoon Bin</first><last>Jung</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Yohan</first><last>Jo</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Edward</first><last>Choi</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>5148-5168</pages>
      <abstract>The development of large language models tailored for handling patients’ clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations.To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature.We then use these synthetic notes to train our specialized clinical large language model, Asclepius.While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes.We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructing high-performing clinical language models. This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals. All resources—including weights, codes, and data—used in the development of Asclepius will be made publicly accessible for future research.</abstract>
      <url hash="091fcd80">2024.findings-acl.305</url>
      <bibkey>kweon-etal-2024-publicly</bibkey>
      <doi>10.18653/v1/2024.findings-acl.305</doi>
    </paper>
    <paper id="306">
      <title>Extending Context Window of Large Language Models via Semantic Compression</title>
      <author><first>Weizhi</first><last>Fei</last><affiliation>The Department of Mathematics, Tsinghua University</affiliation></author>
      <author><first>Xueyan</first><last>Niu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Pingyi</first><last>Zhou</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Lu</first><last>Hou</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Bo</first><last>Bai</last></author>
      <author><first>Lei</first><last>Deng</last></author>
      <author><first>Wei</first><last>Han</last><affiliation>Huawei Tech. Investment Co., Limited</affiliation></author>
      <pages>5169-5181</pages>
      <abstract>Transformer based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses due to the quadratic complexity. These constraints restrict their applicability in long text scenarios. In this paper, we propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead.</abstract>
      <url hash="53f91980">2024.findings-acl.306</url>
      <bibkey>fei-etal-2024-extending</bibkey>
      <doi>10.18653/v1/2024.findings-acl.306</doi>
    </paper>
    <paper id="307">
      <title>Plausible Extractive Rationalization through Semi-Supervised Entailment Signal</title>
      <author><first>Yeo</first><last>Wei Jie</last><affiliation>School of Computer Science and Engineering, Nanyang Technological University</affiliation></author>
      <author><first>Ranjan</first><last>Satapathy</last></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>5182-5192</pages>
      <abstract>The increasing use of complex and opaque black box models requires the adoption of interpretable measures, one such option is extractive rationalizing models, which serve as a more interpretable alternative. These models, also known as Explain-Then-Predict models, employ an explainer model to extract rationales and subsequently condition the predictor with the extracted information. Their primary objective is to provide precise and faithful explanations, represented by the extracted rationales. In this paper, we take a semi-supervised approach to optimize for the plausibility of extracted rationales. We adopt a pre-trained natural language inference (NLI) model and further fine-tune it on a small set of supervised rationales (10%). The NLI predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment. We show that, by enforcing the alignment agreement between the explanation and answer in a question-answering task, the performance can be improved without access to ground truth labels. We evaluate our approach on the ERASER dataset and show that our approach achieves comparable results with supervised extractive models and outperforms unsupervised approaches by &gt; 100%.</abstract>
      <url hash="7cff551a">2024.findings-acl.307</url>
      <bibkey>wei-jie-etal-2024-plausible</bibkey>
      <doi>10.18653/v1/2024.findings-acl.307</doi>
    </paper>
    <paper id="308">
      <title>Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering</title>
      <author><first>ChaeHun</first><last>Park</last></author>
      <author><first>Koanho</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hyesu</first><last>Lim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jaeseok</first><last>Kim</last><affiliation>Korea Telecom Research</affiliation></author>
      <author><first>Junmo</first><last>Park</last><affiliation>Saltlux</affiliation></author>
      <author><first>Yu-Jung</first><last>Heo</last><affiliation>KT</affiliation></author>
      <author><first>Du-Seong</first><last>Chang</last></author>
      <author><first>Jaegul</first><last>Choo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>5193-5221</pages>
      <abstract>Building a reliable visual question answering (VQA) system across different languages is a challenging problem, primarily due to the lack of abundant samples for training. To address this challenge, recent studies have employed machine translation systems for the cross-lingual VQA task. This involves translating the evaluation samples into a source language (usually English) and using monolingual models (i.e., translate-test). However, our analysis reveals that translated texts contain unique characteristics distinct from human-written ones, referred to as translation artifacts. We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes. In light of this, we present a simple data augmentation strategy that can alleviate the adverse impacts of translation artifacts.</abstract>
      <url hash="442a8a8e">2024.findings-acl.308</url>
      <bibkey>park-etal-2024-translation</bibkey>
      <doi>10.18653/v1/2024.findings-acl.308</doi>
    </paper>
    <paper id="309">
      <title>Scented-<fixed-case>EAE</fixed-case>: Stage-Customized Entity Type Embedding for Event Argument Extraction</title>
      <author><first>Yu</first><last>Yang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jinyu</first><last>Guo</last></author>
      <author><first>Kai</first><last>Shuang</last></author>
      <author><first>Chenrui</first><last>Mao</last></author>
      <pages>5222-5235</pages>
      <abstract>Existing methods for incorporating entities into EAE rely on prompts or NER. They typically fail to explicitly explore the role of entity types, which results in shallow argument comprehension and often encounter three issues: (1) weak semantic associations due to missing role-entity correspondence cues; (2) compromised semantic integrity from abandoning context after recognizing entities regardless of their types; (3) one-sided semantic understanding relying solely on argument role semantics. To tackle these issues, we propose Scented-EAE, an EAE model with stage-customized entity type embedding to explicitly underscore and explore the role of entity types, thus intervening in argument selection. Specifically, at the input stage, we strengthen semantic associations by prompting role-entity correspondence after extending a non-autoregressive decoder as part of the encoder. At the intermediate stage, we preserve semantic integrity by optimizing our proposed BIO-aware NER and EAE via a novel IPE joint learning. At the output stage, we expand semantic understanding dimensions by determining arguments using span selectors from argument roles and entity types. Experiments show that our model achieves state-of-the-art performance on mainstream benchmarks. In addition, it also exhibits robustness in low-resource settings with the help of prompts and entity types.</abstract>
      <url hash="d99ea79c">2024.findings-acl.309</url>
      <bibkey>yang-etal-2024-scented</bibkey>
      <doi>10.18653/v1/2024.findings-acl.309</doi>
    </paper>
    <paper id="310">
      <title>Fast Randomized Low-Rank Adaptation of Pre-trained Language Models with <fixed-case>PAC</fixed-case> Regularization</title>
      <author><first>Zijian</first><last>Lei</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Dong</first><last>Qian</last><affiliation>Linköping University</affiliation></author>
      <author><first>William</first><last>Cheung</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <pages>5236-5249</pages>
      <abstract>Low-rank adaptation (LoRA) achieves parameter efficient fine-tuning for large language models (LLMs) by decomposing the model weight update into a pair of low-rank projection matrices. Yet, the memory overhead restricts it to scale up when the model size increases. We propose Randomized LoRA (RLoRA) which adopts Randomized Walsh-Hadamard Transform to achieve significant reduction in the size of trainable parameters compared to LoRA. At the same time, it allows a PAC-Bayes regularizer to be efficiently incorporated to improve generalization. We evaluate the effectiveness of RLoRA on LLMs RoBERTa, GPT-2 and LLaMA-7B using GLUE, E2E and math reasoning benchmarks. With a much lower memory requirement, RLoRA can give similar performance as the SOTA low-rank adaptation methods for these three tasks and significantly better performance under few-shot settings.</abstract>
      <url hash="1322b74d">2024.findings-acl.310</url>
      <bibkey>lei-etal-2024-fast</bibkey>
      <doi>10.18653/v1/2024.findings-acl.310</doi>
    </paper>
    <paper id="311">
      <title><fixed-case>SDA</fixed-case>: Semantic Discrepancy Alignment for Text-conditioned Image Retrieval</title>
      <author><first>Yuchen</first><last>Yang</last></author>
      <author><first>Yu</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>5250-5261</pages>
      <abstract>In the realm of text-conditioned image retrieval, models utilize a query composed of a reference image and modification text to retrieve corresponding images. Despite its significance, this task is fraught with challenges, including small-scale datasets due to labeling costs and the complexity of attributes in modification texts. These challenges often result in models learning a generalized representation of the query, thereby missing the semantic correlations of image and text attributes.In this paper, we introduce a general boosting framework designed to address these issues by employing semantic discrepancy alignment. Our framework first leverages the ChatGPT to augment text data by modifying the original modification text’s attributes. The augmented text is then combined with the original reference image to create an augmented composed query. Then we generate corresponding images using GPT-4 for the augmented composed query.We realize the cross-modal semantic discrepancy alignment by formulating distance consistency and neighbor consistency between the image and text domains. Through this novel approach, attribute in the text domain can be more effectively transferred to the image domain, enhancing retrieval performance. Extensive experiments on three prominent datasets validate the effectiveness of our approach, with state-of-the-art results on a majority of evaluation metrics compared to various baseline methods.</abstract>
      <url hash="d02354d3">2024.findings-acl.311</url>
      <bibkey>yang-etal-2024-sda</bibkey>
      <doi>10.18653/v1/2024.findings-acl.311</doi>
    </paper>
    <paper id="312">
      <title><tex-math>Se^2</tex-math>: Sequential Example Selection for In-Context Learning</title>
      <author><first>Haoyu</first><last>Liu</last></author>
      <author><first>Jianfeng</first><last>Liu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Yuefeng</first><last>Zhan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Weiwei</first><last>Deng</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>5262-5284</pages>
      <abstract>The remarkable capability of large language models(LLMs) for in-context learning(ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the “select then organize” paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a <tex-math>Se</tex-math>quential <tex-math>Se</tex-math>lection problem and introduce <tex-math>Se^2</tex-math>, a sequential-aware method that leverages the LLM’s feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that <tex-math>Se^2</tex-math> markedly surpasses competitive baselines and achieves 42% relative improvement over random selection. Further in-depth analysis shows the effectiveness of proposed strategies, highlighting <tex-math>Se^2</tex-math>‘s exceptional stability and adaptability across various scenarios. Code available at https://github.com/microsoft/LMOps.</abstract>
      <url hash="f1d64f74">2024.findings-acl.312</url>
      <bibkey>liu-etal-2024-se2</bibkey>
      <doi>10.18653/v1/2024.findings-acl.312</doi>
    </paper>
    <paper id="313">
      <title>Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding</title>
      <author><first>Hanling</first><last>Yi</last></author>
      <author><first>Feng</first><last>Lin</last><affiliation>IntelliFusion Co., Ltd</affiliation></author>
      <author><first>Hongbin</first><last>Li</last></author>
      <author><first>Ning</first><last>Peiyang</last><affiliation>Intellifusion Inc.</affiliation></author>
      <author><first>Xiaotian</first><last>Yu</last></author>
      <author><first>Rong</first><last>Xiao</last></author>
      <pages>5285-5299</pages>
      <abstract>This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose Smart Parallel Auto-Correct dEcoding (SPACE), an approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality.</abstract>
      <url hash="7a449f27">2024.findings-acl.313</url>
      <bibkey>yi-etal-2024-generation</bibkey>
      <doi>10.18653/v1/2024.findings-acl.313</doi>
    </paper>
    <paper id="314">
      <title><fixed-case>S</fixed-case>truct<fixed-case>E</fixed-case>val: Deepen and Broaden Large Language Model Assessment via Structured Evaluation</title>
      <author><first>Boxi</first><last>Cao</last></author>
      <author><first>Mengjie</first><last>Ren</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Feng</first><last>Zhang</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Junfeng</first><last>Zhan</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>5300-5318</pages>
      <abstract>Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggle to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, this paper proposes a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluations for large language models. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination, and reducing the interference of potential biases, thereby providing a more reliable and consistent conclusion regarding model capabilities. Our framework also sheds light on the design of future principled and trustworthy LLM evaluation protocols.</abstract>
      <url hash="9c8ca6d4">2024.findings-acl.314</url>
      <bibkey>cao-etal-2024-structeval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.314</doi>
    </paper>
    <paper id="315">
      <title>Mitigating Privacy Seesaw in Large Language Models: Augmented Privacy Neuron Editing via Activation Patching</title>
      <author><first>Xinwei</first><last>Wu</last></author>
      <author><first>Weilong</first><last>Dong</last></author>
      <author><first>Shaoyang</first><last>Xu</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>5319-5332</pages>
      <abstract>Protecting privacy leakage in large language models remains a paramount challenge. In this paper, we reveal Privacy Seesaw in LLM privacy safeguarding, a phenomenon where measures to secure specific private information inadvertently heighten exposure risks for other privacy. Through comprehensive analysis, we identify the amount of targeted privacy data and the volume of edited privacy neurons as the two central triggers to this issue. To mitigate privacy seesaw, we propose Augmented Privacy Neuron Editing via Activation Patching (APNEAP), a novel framework designed to well balance model performance with privacy protection. The proposed APNEAP augments collected private data by automatically synthesizing new private data, which deactivates the first trigger to the privacy seesaw issue. Additionally, it adapts activation patching to privacy neuron editing for switching off the second trigger to the privacy seesaw problem. Experimental results show that the proposed APNEAP is capable of alleviating the privacy seesaw phenomenon and offers a more stable and reliable approach to privacy protection in LLMs than previous methods.</abstract>
      <url hash="119a1cc5">2024.findings-acl.315</url>
      <bibkey>wu-etal-2024-mitigating-privacy</bibkey>
      <doi>10.18653/v1/2024.findings-acl.315</doi>
    </paper>
    <paper id="316">
      <title>Which Information Matters? Dissecting Human-written Multi-document Summaries with Partial Information Decomposition</title>
      <author><first>Laura</first><last>Mascarell</last></author>
      <author><first>Yan</first><last>LHomme</last></author>
      <author><first>Majed</first><last>El Helou</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <pages>5333-5338</pages>
      <abstract>Understanding the nature of high-quality summaries is crucial to further improve the performance of multi-document summarization. We propose an approach to characterize human-written summaries using partial information decomposition, which decomposes the mutual information provided by all source documents into union, redundancy, synergy, and unique information. Our empirical analysis on different MDS datasets shows that there is a direct dependency between the number of sources and their contribution to the summary.</abstract>
      <url hash="1eea0a80">2024.findings-acl.316</url>
      <bibkey>mascarell-etal-2024-information</bibkey>
      <doi>10.18653/v1/2024.findings-acl.316</doi>
    </paper>
    <paper id="317">
      <title><fixed-case>B</fixed-case>ad<fixed-case>A</fixed-case>cts: A Universal Backdoor Defense in the Activation Space</title>
      <author><first>Biao</first><last>Yi</last></author>
      <author><first>Sishuo</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yiming</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Tong</first><last>Li</last><affiliation>Nankai University</affiliation></author>
      <author><first>Baolei</first><last>Zhang</last></author>
      <author><first>Zheli</first><last>Liu</last></author>
      <pages>5339-5352</pages>
      <abstract>Backdoor attacks pose an increasingly severe security threat to Deep Neural Networks (DNNs) during their development stage. In response, backdoor sample purification has emerged as a promising defense mechanism, aiming to eliminate backdoor triggers while preserving the integrity of the clean content in the samples. However, existing approaches have been predominantly focused on the word space, which are ineffective against feature-space triggers and significantly impair performance on clean data. To address this, we introduce a universal backdoor defense that purifies backdoor samples in the activation space by drawing abnormal activations towards optimized minimum clean activation distribution intervals. The advantages of our approach are twofold: (1) By operating in the activation space, our method captures from surface-level information like words to higher-level semantic concepts such as syntax, thus counteracting diverse triggers; (2) the fine-grained continuous nature of the activation space allows for more precise preservation of clean content while removing triggers. Furthermore, we propose a detection module based on statistical information of abnormal activations, to achieve a better trade-off between clean accuracy and defending performance. Extensive experiments on diverse datasets and against diverse attacks (including syntax and style attacks) demonstrate that our defense achieves state-of-the-art performance.</abstract>
      <url hash="a0f95398">2024.findings-acl.317</url>
      <bibkey>yi-etal-2024-badacts</bibkey>
      <doi>10.18653/v1/2024.findings-acl.317</doi>
    </paper>
    <paper id="318">
      <title><fixed-case>R</fixed-case>eact<fixed-case>XT</fixed-case>: Understanding Molecular “Reaction-ship” via Reaction-Contextualized Molecule-Text Pretraining</title>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Yaorui</first><last>Shi</last></author>
      <author><first>An</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Sihang</first><last>Li</last></author>
      <author><first>Enzhi</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Wang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>5353-5377</pages>
      <abstract>Molecule-text modeling, which aims to facilitate molecule-relevant tasks with a textual interface and textual knowledge, is an emerging research direction. Beyond single molecules, studying reaction-text modeling holds promise for helping the synthesis of new materials and drugs. However, previous works mostly neglect reaction-text modeling: they primarily focus on modeling individual molecule-text pairs or learning chemical reactions without texts in context. Additionally, one key task of reaction-text modeling – experimental procedure prediction – is less explored due to the absence of an open-source dataset. The task is to predict step-by-step actions of conducting chemical experiments and is crucial to automating chemical synthesis. To resolve the challenges above, we propose a new pretraining method, ReactXT, for reaction-text modeling, and a new dataset, OpenExp, for experimental procedure prediction. Specifically, ReactXT features three types of input contexts to incrementally pretrain LMs. Each of the three input contexts corresponds to a pretraining task to improve the text-based understanding of either reactions or single molecules. ReactXT demonstrates consistent improvements in experimental procedure prediction and molecule captioning and offers competitive results in retrosynthesis. Our code is available at https://github.com/syr-cn/ReactXT.</abstract>
      <url hash="14714801">2024.findings-acl.318</url>
      <bibkey>liu-etal-2024-reactxt</bibkey>
      <doi>10.18653/v1/2024.findings-acl.318</doi>
    </paper>
    <paper id="319">
      <title>Multi-modal Concept Alignment Pre-training for Generative Medical Visual Question Answering</title>
      <author><first>Quan</first><last>Yan</last><affiliation>Central South University</affiliation></author>
      <author><first>Junwen</first><last>Duan</last><affiliation>Central South University, China</affiliation></author>
      <author><first>Jianxin</first><last>Wang</last><affiliation>Central South University</affiliation></author>
      <pages>5378-5389</pages>
      <abstract>Medical Visual Question Answering (Med-VQA) seeks to accurately respond to queries regarding medical images, a task particularly challenging for open-ended questions. This study unveils the Multi-modal Concept Alignment Pre-training (MMCAP) approach for generative Med-VQA, leveraging a knowledge graph sourced from medical image-caption datasets and the Unified Medical Language System. MMCAP advances the fusion of visual and textual medical knowledge via a graph attention network and a transformer decoder. Additionally, it incorporates a Type Conditional Prompt in the fine-tuning phase, markedly boosting the accuracy and relevance of answers to open-ended questions. Our tests on benchmark datasets illustrate MMCAP’s superiority over existing methods, demonstrating its high efficiency in data-limited settings and effective knowledge-image alignment capability.</abstract>
      <url hash="81ce4a42">2024.findings-acl.319</url>
      <bibkey>yan-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.319</doi>
    </paper>
    <paper id="320">
      <title>Exploring Ordinality in Text Classification: A Comparative Study of Explicit and Implicit Techniques</title>
      <author><first>Siva Rajesh</first><last>Kasa</last></author>
      <author><first>Aniket</first><last>Goel</last></author>
      <author><first>Karan</first><last>Gupta</last><affiliation>Amazon</affiliation></author>
      <author><first>Sumegh</first><last>Roychowdhury</last><affiliation>Amazon</affiliation></author>
      <author><first>Pattisapu</first><last>Priyatam</last></author>
      <author><first>Anish</first><last>Bhanushali</last></author>
      <author><first>Prasanna</first><last>Srinivasa Murthy</last><affiliation>Amazon</affiliation></author>
      <pages>5390-5404</pages>
      <abstract>Ordinal Classification (OC) is a widely encountered challenge in Natural Language Processing (NLP), with applications in various domains such as sentiment analysis, rating prediction, and more. Previous approaches to tackle OC have primarily focused on modifying existing or creating novel loss functions that explicitly account for the ordinal nature of labels. However, with the advent of Pre-trained Language Models (PLMs), it became possible to tackle ordinality through the implicit semantics of the labels as well. This paper provides a comprehensive theoretical and empirical examination of both these approaches. Furthermore, we also offer strategic recommendations regarding the most effective approach to adopt based on specific settings.</abstract>
      <url hash="fd86b76a">2024.findings-acl.320</url>
      <bibkey>kasa-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.320</doi>
    </paper>
    <paper id="321">
      <title>Evaluating Large Language Models on <fixed-case>W</fixed-case>ikipedia-Style Survey Generation</title>
      <author><first>Fan</first><last>Gao</last></author>
      <author><first>Hang</first><last>Jiang</last></author>
      <author><first>Rui</first><last>Yang</last></author>
      <author><first>Qingcheng</first><last>Zeng</last><affiliation>Northwestern University, Northwestern University</affiliation></author>
      <author><first>Jinghui</first><last>Lu</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Moritz</first><last>Blum</last></author>
      <author><first>Tianwei</first><last>She</last><affiliation>The University of Tokyo, Tokyo Institute of Technology</affiliation></author>
      <author><first>Yuang</first><last>Jiang</last></author>
      <author><first>Irene</first><last>Li</last></author>
      <pages>5405-5418</pages>
      <abstract>Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.</abstract>
      <url hash="4fab8f17">2024.findings-acl.321</url>
      <bibkey>gao-etal-2024-evaluating-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.321</doi>
    </paper>
    <paper id="322">
      <title>The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse</title>
      <author><first>Wanli</first><last>Yang</last></author>
      <author><first>Fei</first><last>Sun</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xinyu</first><last>Ma</last><affiliation>Baidu</affiliation></author>
      <author><first>Xun</first><last>Liu</last></author>
      <author><first>Dawei</first><last>Yin</last><affiliation>Baidu</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <pages>5419-5437</pages>
      <abstract>Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating changes in an edited model’s perplexity are strongly correlated with its downstream task performances. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse after only few edits. To facilitate further research, we have utilized GPT-3.5 to develop a new dataset, HardEdit, based on those hard cases. This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse. We hope this work can draw the community’s attention to the potential risks inherent in model editing practices.</abstract>
      <url hash="83364d13">2024.findings-acl.322</url>
      <bibkey>yang-etal-2024-butterfly</bibkey>
      <doi>10.18653/v1/2024.findings-acl.322</doi>
    </paper>
    <paper id="323">
      <title>Can We Continually Edit Language Models? On the Knowledge Attenuation in Sequential Model Editing</title>
      <author><first>Qi</first><last>Li</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xiaowen</first><last>Chu</last><affiliation>Hong Kong University of Science and Technology (Guangzhou)</affiliation></author>
      <pages>5438-5455</pages>
      <abstract>Model editing has become a promising method for precisely and effectively updating knowledge in language models. In this paper, we investigate knowledge attenuation, in which the retention of updated knowledge within the language model decreases as the number of edits increases after sequential editing. Through empirical study, we discovered that existing editing methods generally suffer from knowledge attenuation. We attribute this phenomenon to two aspects: (1) redundant parameters interference and (2) update weight disentanglement. To this end, we propose the AdaPLE method. It not only mitigates the knowledge attenuation issue but also improves the performance on existing benchmarks. To the best of our knowledge, we are the first to investigate the cause and mitigation of knowledge attenuation in sequential LLM editing.</abstract>
      <url hash="e3943e84">2024.findings-acl.323</url>
      <bibkey>li-chu-2024-continually</bibkey>
      <doi>10.18653/v1/2024.findings-acl.323</doi>
    </paper>
    <paper id="324">
      <title>Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-<fixed-case>SQL</fixed-case> Generation</title>
      <author><first>Ge</first><last>Qu</last></author>
      <author><first>Jinyang</first><last>Li</last></author>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Bowen</first><last>Qin</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Nan</first><last>Huo</last></author>
      <author><first>Chenhao</first><last>Ma</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <author><first>Reynold</first><last>Cheng</last></author>
      <pages>5456-5471</pages>
      <abstract>Large Language Models (LLMs) driven by In-Context Learning (ICL) have significantly improved the performance of text-to-SQL. Previous methods generally employ a two-stage reasoning framework, namely 1) schema linking and 2) logical synthesis, making the framework not only effective but also interpretable. Despite these advancements, the inherent bad nature of the generalization of LLMs often results in hallucinations, which limits the full potential of LLMs. In this work, we first identify and categorize the common types of hallucinations at each stage in text-to-SQL. We then introduce a novel strategy, Task Alignment (TA), designed to mitigate hallucinations at each stage. TA encourages LLMs to take advantage of experiences from similar tasks rather than starting the tasks from scratch. This can help LLMs reduce the burden of generalization, thereby mitigating hallucinations effectively. We further propose TA-SQL, a text-to-SQL framework based on this strategy. The experimental results and comprehensive analysis demonstrate the effectiveness and robustness of our framework. Specifically, it enhances the performance of the GPT-4 baseline by 21.23% relatively on BIRD dev and it yields significant improvements across six models and four mainstream, complex text-to-SQL benchmarks.</abstract>
      <url hash="3ef1c457">2024.findings-acl.324</url>
      <bibkey>qu-etal-2024-generation</bibkey>
      <doi>10.18653/v1/2024.findings-acl.324</doi>
    </paper>
    <paper id="325">
      <title>Translatotron-<fixed-case>V</fixed-case>(ison): An End-to-End Model for In-Image Machine Translation</title>
      <author><first>Zhibin</first><last>Lan</last></author>
      <author><first>Liqiang</first><last>Niu</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>5472-5485</pages>
      <abstract>In-image machine translation (IIMT) aims to translate an image containing texts in source language into an image containing translations in target language. In this regard, conventional cascaded methods suffer from issues such as error propagation, massive parameters, and difficulties in deployment and retaining visual characteristics of the input image.Thus, constructing end-to-end models has become an option, which, however, faces two main challenges: 1) the huge modeling burden, as it is required to simultaneously learn alignment across languages and preserve the visual characteristics of the input image; 2) the difficulties of directly predicting excessively lengthy pixel sequences.In this paper, we propose <tex-math>\textit{Translatotron-V(ision)}</tex-math>, an end-to-end IIMT model consisting of four modules. In addition to an image encoder, and an image decoder, our model contains a target text decoder and an image tokenizer. Among them, the target text decoder is used to alleviate the language alignment burden, and the image tokenizer converts long sequences of pixels into shorter sequences of visual tokens, preventing the model from focusing on low-level visual features. Besides, we present a two-stage training framework for our model to assist the model in learning alignment across modalities and languages. Finally, we propose a location-aware evaluation metric called Structure-BLEU to assess the translation quality of the generated images. Experimental results demonstrate that our model achieves competitive performance compared to cascaded models with only 70.9% of parameters, and significantly outperforms the pixel-level end-to-end IIMT model.</abstract>
      <url hash="c28a4b7c">2024.findings-acl.325</url>
      <bibkey>lan-etal-2024-translatotron</bibkey>
      <doi>10.18653/v1/2024.findings-acl.325</doi>
    </paper>
    <paper id="326">
      <title><fixed-case>S</fixed-case>tat<fixed-case>B</fixed-case>ot.<fixed-case>S</fixed-case>wiss: Bilingual Open Data Exploration in Natural Language</title>
      <author><first>Farhad</first><last>Nooralahzadeh</last><affiliation>University of Zurich and ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>University of Zurich and ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <author><first>Ellery</first><last>Smith</last></author>
      <author><first>Sabine</first><last>Maennel</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Cyril</first><last>Matthey-Doret</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Raphaël</first><last>De Fondeville</last><affiliation>Federal Office of Statistics</affiliation></author>
      <author><first>Kurt</first><last>Stockinger</last><affiliation>ZHAW - Zürcher Hochschule für Angewandte Wissenschaften</affiliation></author>
      <pages>5486-5507</pages>
      <abstract>The potential for improvements brought by Large Language Models (LLMs) in Text-to-SQL systems is mostly assessed on monolingual English datasets. However, LLMs’ performance for other languages remains vastly unexplored. In this work, we release the StatBot.Swiss dataset, the first bilingual benchmark for evaluating Text-to-SQL systems based on real-world applications. The StatBot.Swiss dataset contains 455 natural language/SQL-pairs over 35 big databases with varying level of complexity for both English and German.We evaluate the performance of state-of-the-art LLMs such as GPT-3.5-Turbo and mixtral-8x7b-instruct for the Text-to-SQL translation task using an in-context learning approach. Our experimental analysis illustrates that current LLMs struggle to generalize well in generating SQL queries on our novel bilingual dataset.</abstract>
      <url hash="17292046">2024.findings-acl.326</url>
      <bibkey>nooralahzadeh-etal-2024-statbot</bibkey>
      <doi>10.18653/v1/2024.findings-acl.326</doi>
    </paper>
    <paper id="327">
      <title>Subtle Signatures, Strong Shields: Advancing Robust and Imperceptible Watermarking in Large Language Models</title>
      <author><first>Yubing</first><last>Ren</last></author>
      <author><first>Ping</first><last>Guo</last></author>
      <author><first>Yanan</first><last>Cao</last></author>
      <author><first>Wei</first><last>Ma</last></author>
      <pages>5508-5519</pages>
      <abstract>The widespread adoption of Large Language Models (LLMs) has led to an increase in AI-generated text on the Internet, presenting a crucial challenge to differentiate AI-created content from human-written text. This challenge is critical to prevent issues of authenticity, trust, and potential copyright violations. Current research focuses on watermarking LLM-generated text, but traditional techniques struggle to balance robustness with text quality. We introduce a novel watermarking approach, Robust and Imperceptible Watermarking (RIW) for LLMs, which leverages token prior probabilities to improve detectability and maintain watermark imperceptibility. RIW methodically embeds watermarks by partitioning selected tokens into two distinct groups based on their prior probabilities and employing tailored strategies for each group. In the detection stage, the RIW method employs the ‘voted z-test’ to provide a statistically robust framework to identify the presence of a watermark accurately. The effectiveness of RIW is evaluated across three key dimensions: success rate, text quality, and robustness against removal attacks. Our experimental results on various LLMs, including GPT2-XL, OPT-1.3B, and LLaMA2-7B, indicate that RIW surpasses existing models, and also exhibits increased robustness against various attacks and good imperceptibility, thus promoting the responsible use of LLMs.</abstract>
      <url hash="f9eb8e01">2024.findings-acl.327</url>
      <bibkey>ren-etal-2024-subtle</bibkey>
      <doi>10.18653/v1/2024.findings-acl.327</doi>
    </paper>
    <paper id="328">
      <title>Thinking about how to extract: Energizing <fixed-case>LLM</fixed-case>s’ emergence capabilities for document-level event argument extraction</title>
      <author><first>Kai</first><last>Shuang</last></author>
      <author><first>Zhouji</first><last>Zhouji</last></author>
      <author><first>Wang</first><last>Qiwei</last></author>
      <author><first>Jinyu</first><last>Guo</last></author>
      <pages>5520-5532</pages>
      <abstract>There are two key challenges remaining for the document-level event argument extraction (D-EAE) tasks: key feature forgetting and cross-event argument confusion. The emergence capability of large language models (LLMs) holds promise for solving the above two challenges. In this paper, we propose a document-level event argument extraction method based on guided summarization and reasoning (EAESR), which leverages the emergence capabilities of LLMs to highlight key event information and to clarify the explicit and implicit association between multiple events. Specifically, we generate document summarization information that shorten the length of the event context while preserving the key event features. In addition, we generate inter-event reasoning information, which helps EAESR make sense of the correlations between events and reduces their dependence on the event context, especially to better cope with the few-shot D-EAE task. Then, we obtain named entity information to enable EAESR to learn argument boundary features to improve the sensitivity of its argument boundary recognition. Eventually, we fused the above features and sentence features to make EAESR have summarizing and reasoning capabilities simultaneously. Extensive experiments on WIKIEVENTS and RAMS have shown that EAESR achieves a new state-of-the-art that outperforms the baseline models by 1.3% F1 and 1.6% F1, respectively, and averages 11% F1 in few-shot settings.</abstract>
      <url hash="c370078b">2024.findings-acl.328</url>
      <bibkey>shuang-etal-2024-thinking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.328</doi>
    </paper>
    <paper id="329">
      <title>Improving the Robustness of Distantly-Supervised Named Entity Recognition via Uncertainty-Aware Teacher Learning and Student-Student Collaborative Learning</title>
      <author><first>Shuzheng</first><last>Si</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Helan</first><last>Hu</last></author>
      <author><first>Haozhe</first><last>Zhao</last></author>
      <author><first>Shuang</first><last>Zeng</last></author>
      <author><first>Kaikai</first><last>An</last></author>
      <author><first>Zefan</first><last>Cai</last></author>
      <author><first>Baobao</first><last>Chang</last><affiliation>Peking University</affiliation></author>
      <pages>5533-5546</pages>
      <abstract>Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates the burden of annotation, but meanwhile suffers from the label noise. Recent works attempt to adopt the teacher-student framework to gradually refine the training labels and improve the overall robustness. However, we argue that these teacher-student methods achieve limited performance because the poor calibration of the teacher network produces incorrectly pseudo-labeled samples, leading to error propagation. Therefore, we attempt to mitigate this issue by proposing: (1) Uncertainty-Aware Teacher Learning that leverages the prediction uncertainty to reduce the number of incorrect pseudo labels in the self-training stage; (2) Student-Student Collaborative Learning that allows the transfer of reliable labels between two student networks instead of indiscriminately relying on all pseudo labels from its teacher. This approach further enables a full exploration of mislabeled samples rather than simply filtering unreliable pseudo-labeled samples. We evaluate our proposed method on five DS-NER datasets, demonstrating that our method is superior to the state-of-the-art DS-NER denoising methods.</abstract>
      <url hash="c5f6c770">2024.findings-acl.329</url>
      <bibkey>si-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.findings-acl.329</doi>
    </paper>
    <paper id="330">
      <title>Predicting Narratives of Climate Obstruction in Social Media Advertising</title>
      <author><first>Harri</first><last>Rowlands</last><affiliation>InfluenceMap</affiliation></author>
      <author><first>Gaku</first><last>Morio</last><affiliation>Hitachi America, Ltd., Stanford University and Hitachi, ltd.</affiliation></author>
      <author><first>Dylan</first><last>Tanner</last></author>
      <author><first>Christopher</first><last>Manning</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <pages>5547-5558</pages>
      <abstract>Social media advertising offers a platform for fossil fuel value chain companies and their agents to reinforce their narratives, often emphasizing economic, labor market, and energy security benefits to promote oil and gas policy and products. Whether such narratives can be detected automatically and the extent to which the cost of human annotation can be reduced is our research question. We introduce a task of classifying narratives into seven categories, based on existing definitions and data.Experiments showed that RoBERTa-large outperforms other methods, while GPT-4 Turbo can serve as a viable annotator for the task, thereby reducing human annotation costs. Our findings and insights provide guidance to automate climate-related ad analysis and lead to more scalable ad scrutiny.</abstract>
      <url hash="e93caae2">2024.findings-acl.330</url>
      <bibkey>rowlands-etal-2024-predicting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.330</doi>
    </paper>
    <paper id="331">
      <title><fixed-case>SSS</fixed-case>: Editing Factual Knowledge in Language Models towards Semantic Sparse Space</title>
      <author><first>Huazheng</first><last>Wang</last></author>
      <author><first>Haifeng</first><last>Sun</last><affiliation>Beijing University of Posts and Telecommunications, Beijing University of Posts and Telecommunications and Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jingyu</first><last>Wang</last><affiliation>Beijing University of Post and Telecommunication, Tsinghua University</affiliation></author>
      <author><first>Qi</first><last>Qi</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Zixuan</first><last>Xia</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Menghao</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Jianxin</first><last>Liao</last></author>
      <pages>5559-5570</pages>
      <abstract>Language Models (LMs) acquire factual knowledge during pre-training and store it in the parameters, which can be valuable for downstream tasks. As world evolves, some facts may be incorrectly induced or become obsolete over time. Various model editing methods have been proposed to modify specific examples in LMs. However, existing training-based methods still suffer from sub-optimal locality, where irrelevant neighborhood examples can be adversely influenced. Model’s gradients are still struggling to identify the appropriate direction when updating the parameters. To address this issue, we find that directing the hidden state of the edit example towards spaces where semantics are sparse tends to help preserve the semantics of irrelevant neighborhood examples. Based on this hypothesis, we propose a novel metric, named SSS, to evaluate the degree of sparsity around a sentence embedding in the semantic space without any human or machine annotation. Subsequently, we incorporate SSS into the original loss function of the existing training-based methods to enhance locality. Experiments conducted on two datasets across various models demonstrate that SSS is effective in improving both locality and reasoning capability.</abstract>
      <url hash="0baa375d">2024.findings-acl.331</url>
      <bibkey>wang-etal-2024-sss</bibkey>
      <doi>10.18653/v1/2024.findings-acl.331</doi>
    </paper>
    <paper id="332">
      <title><tex-math>\textit{GeoHard}</tex-math>: Towards Measuring Class-wise Hardness through Modelling Class Semantics</title>
      <author><first>Fengyu</first><last>Cai</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Xinran</first><last>Zhao</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and Technical University of Darmstadt</affiliation></author>
      <author><first>Heinz</first><last>Koeppl</last></author>
      <pages>5571-5597</pages>
      <abstract>Recent advances in measuring hardness-wise properties of data guide language models in sample selection within low-resource scenarios. However, class-specific properties are overlooked for task setup and learning. How will these properties influence model learning and is it generalizable across datasets? To answer this question, this work formally initiates the concept of <tex-math>\textit{class-wise hardness}</tex-math>. Experiments across eight natural language understanding (NLU) datasets demonstrate a consistent hardness distribution across learning paradigms, models, and human judgment. Subsequent experiments unveil a notable challenge in measuring such class-wise hardness with instance-level metrics in previous works. To address this, we propose <tex-math>\textit{GeoHard}</tex-math> for class-wise hardness measurement by modeling class geometry in the semantic embedding space. <tex-math>\textit{GeoHard}</tex-math> surpasses instance-level metrics by over 59 percent on <tex-math>\textit{Pearson}</tex-math>‘s correlation on measuring class-wise hardness. Our analysis theoretically and empirically underscores the generality of <tex-math>\textit{GeoHard}</tex-math> as a fresh perspective on data diagnosis. Additionally, we showcase how understanding class-wise hardness can practically aid in improving task learning.</abstract>
      <url hash="d9c0c173">2024.findings-acl.332</url>
      <bibkey>cai-etal-2024-geohard</bibkey>
      <doi>10.18653/v1/2024.findings-acl.332</doi>
    </paper>
    <paper id="333">
      <title>Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models</title>
      <author><first>Sheng-Lun</first><last>Wei</last><affiliation>Department of computer science and informational engineering, National Taiwan University</affiliation></author>
      <author><first>Cheng-Kuang</first><last>Wu</last><affiliation>Appier</affiliation></author>
      <author><first>Hen-Hsen</first><last>Huang</last><affiliation>Institute of Information Science, Academia Sinica</affiliation></author>
      <author><first>Hsin-Hsi</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>5598-5621</pages>
      <abstract>In this paper, we investigate the phenomena of “selection biases” in Large Language Models (LLMs), focusing on problems where models are tasked with choosing the optimal option from an ordered sequence. We delve into biases related to option order and token usage, which significantly impact LLMs’ decision-making processes. We also quantify the impact of these biases through an extensive empirical analysis across multiple models and tasks. Furthermore, we propose mitigation strategies to enhance model performance. Our key contributions are threefold: 1) Precisely quantifying the influence of option order and token on LLMs, 2) Developing strategies to mitigate the impact of token and order sensitivity to enhance robustness, and 3) Offering a detailed analysis of sensitivity across models and tasks, which informs the creation of more stable and reliable LLM applications for selection problems.</abstract>
      <url hash="0735ccb9">2024.findings-acl.333</url>
      <bibkey>wei-etal-2024-unveiling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.333</doi>
    </paper>
    <paper id="334">
      <title><fixed-case>A</fixed-case>rabic<fixed-case>MMLU</fixed-case>: Assessing Massive Multitask Language Understanding in <fixed-case>A</fixed-case>rabic</title>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Sara</first><last>Shatnawi</last></author>
      <author><first>Jad</first><last>Doughman</last></author>
      <author><first>Abdelrahman</first><last>Sadallah</last></author>
      <author><first>Aisha</first><last>Alraeesi</last></author>
      <author><first>Khalid</first><last>Almubarak</last><affiliation>Prince Sattam bin Abdulaziz University</affiliation></author>
      <author><first>Zaid</first><last>Alyafeai</last></author>
      <author><first>Neha</first><last>Sengupta</last></author>
      <author><first>Shady</first><last>Shehata</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Nizar</first><last>Habash</last><affiliation>New York University Abu Dhabi</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>5622-5640</pages>
      <abstract>The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding benchmark for the Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA) and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and Falcon struggle to achieve a score of 50%, while even the top-performing Arabic-centric model only achieves a score of 62.3%.</abstract>
      <url hash="136d5033">2024.findings-acl.334</url>
      <bibkey>koto-etal-2024-arabicmmlu</bibkey>
      <doi>10.18653/v1/2024.findings-acl.334</doi>
    </paper>
    <paper id="335">
      <title>On the Relationship Between <fixed-case>RNN</fixed-case> Hidden-State Vectors and Semantic Structures</title>
      <author><first>Edi</first><last>Muskardin</last></author>
      <author><first>Martin</first><last>Tappler</last><affiliation>Technische Universität Wien</affiliation></author>
      <author><first>Ingo</first><last>Pill</last><affiliation>Technische Universität Graz</affiliation></author>
      <author><first>Bernhard</first><last>Aichernig</last><affiliation>Technische Universität Graz</affiliation></author>
      <author><first>Thomas</first><last>Pock</last><affiliation>Graz University of Technology</affiliation></author>
      <pages>5641-5658</pages>
      <abstract>We examine the assumption that hidden-state vectors of recurrent neural networks (RNNs) tend to form clusters of semantically similar vectors, which we dub the clustering hypothesis. While this hypothesis has been assumed in RNN analyses in recent years, its validity has not been studied thoroughly on modern RNN architectures. We first consider RNNs that were trained to recognize regular languages. This enables us to draw on perfect ground-truth automata in our evaluation, against which we can compare the RNN’s accuracy and the distribution of the hidden-state vectors. Then, we consider context-free languages to examine if RNN states form clusters for more expressive languages.For our analysis, we fit (generalized) linear models to classify RNN states into automata states and we apply different unsupervised clustering techniques. With a new ambiguity score, derived from information entropy, we measure how well an abstraction function maps the hidden state vectors to abstract clusters. Our evaluation supports the validity of the clustering hypothesis for regular languages, especially if RNNs are well-trained, i.e., clustering techniques succeed in finding clusters of similar state vectors. However, the clustering accuracy decreases substantially for context-free languages. This suggests that clustering is not a reliable abstraction technique for RNNs used in tasks like natural language processing.</abstract>
      <url hash="52e5609a">2024.findings-acl.335</url>
      <bibkey>muskardin-etal-2024-relationship</bibkey>
      <doi>10.18653/v1/2024.findings-acl.335</doi>
    </paper>
    <paper id="336">
      <title><fixed-case>XMC</fixed-case>-Agent : Dynamic Navigation over Scalable Hierarchical Index for Incremental Extreme Multi-label Classification</title>
      <author><first>Yanjiang</first><last>Liu</last></author>
      <author><first>Tianyun</first><last>Zhong</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Shuheng</first><last>Zhou</last><affiliation>Ant Group</affiliation></author>
      <author><first>Huijia</first><last>Zhu</last></author>
      <author><first>Weiqiang</first><last>Wang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Zhongyi</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>5659-5672</pages>
      <abstract>The eXtreme Multi-label Classification (XMC) aims at accurately assigning large-scale labels to instances, and is challenging for learning, managing, and predicting over the large-scale and rapidly growing set of labels. Traditional XMC methods, like one-vs-all and tree-based methods struggle with the growing set of labels due to their static label assumptions, and embedding-based methods struggle with the complex mapping relationships due to their late-interaction paradigm. In this paper, we propose a large language model (LLM) powered agent framework for extreme multi-label classification – XMC-Agent, which can effectively learn, manage and predict the extremely large and dynamically increasing set of labels. Specifically, XMC-Agent models the extreme multi-label classification task as a dynamic navigation problem, employing a scalable hierarchical label index to effectively manage the unified label space. Additionally, we propose two algorithms to enhance the dynamic navigation capabilities of XMC-Agent: a self-construction algorithm for building the scalable hierarchical index, and an iterative feedback learning algorithm for adjusting the agent to specific tasks. Experiments show that XMC-Agentachieves the state-of-the-art performance on three standard datasets.</abstract>
      <url hash="6d251ffc">2024.findings-acl.336</url>
      <bibkey>liu-etal-2024-xmc</bibkey>
      <doi>10.18653/v1/2024.findings-acl.336</doi>
    </paper>
    <paper id="337">
      <title>Benchmarking Large Language Models on <fixed-case>CFLUE</fixed-case> - A <fixed-case>C</fixed-case>hinese Financial Language Understanding Evaluation Dataset</title>
      <author><first>Jie</first><last>Zhu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Junhui</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Yalong</first><last>Wen</last></author>
      <author><first>Lifan</first><last>Guo</last></author>
      <pages>5673-5693</pages>
      <abstract>In light of recent breakthroughs in large language models (LLMs) that have revolutionized natural language processing (NLP), there is an urgent need for new benchmarks to keep pace with the fast development of LLMs. In this paper, we propose CFLUE, the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. Specifically, CFLUE provides datasets tailored for both knowledge assessment and application assessment. In knowledge assessment, it consists of 38K+ multiple-choice questions with associated solution explanations. These questions serve dual purposes: answer prediction and question reasoning. In application assessment, CFLUE features 16K+ test instances across distinct groups of NLP tasks such as text classification, machine translation, relation extraction, reading comprehension, and text generation. Upon CFLUE, we conduct a thorough evaluation of representative LLMs. The results reveal that only Qwen-72B, GPT-4, and GPT-4-turbo achieve an accuracy exceeding 60% in answer prediction for knowledge assessment, suggesting that there is still substantial room for improvement in current LLMs. In application assessment, while GPT-4 and GPT-4-turbo rank as the top two performers on average, their significant advantage over open-source LLMs is noticeably diminished, given that Qwen-72B achieves the best performance in 2 out of 5 tasks. The datasets and scripts associated with CFLUE are openly accessible at <url>https://github.com/aliyun/cflue</url>.</abstract>
      <url hash="15c222d8">2024.findings-acl.337</url>
      <bibkey>zhu-etal-2024-benchmarking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.337</doi>
    </paper>
    <paper id="338">
      <title>Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint</title>
      <author><first>Zhipeng</first><last>Chen</last></author>
      <author><first>Kun</first><last>Zhou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Junchen</first><last>Wan</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Di</first><last>Zhang</last><affiliation>Kuaishou Technology</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>5694-5711</pages>
      <abstract>Reinforcement learning (RL) has been widely used in training large language models (LLMs) for preventing unexpected outputs, e.g., reducing harmfulness and errors. However, existing RL methods mainly adopt instance-level reward, which cannot provide fine-grained supervision for complex reasoning tasks. As a result, the RL training cannot be fully aware of the specific part or step that actually leads to the incorrectness in model response. To address it, we propose a new RL method named RLMEC that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, which can produce token-level supervision for RL training. Based 0on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And these two objectives focus on the revision of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. Experiment results on 8 tasks have demonstrated the effectiveness of our approach. Our code and data will be publicly released.</abstract>
      <url hash="f36ca6b4">2024.findings-acl.338</url>
      <bibkey>chen-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.findings-acl.338</doi>
    </paper>
    <paper id="339">
      <title>Definition generation for lexical semantic change detection</title>
      <author><first>Mariia</first><last>Fedorova</last></author>
      <author><first>Andrey</first><last>Kutuzov</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Yves</first><last>Scherrer</last><affiliation>University of Oslo</affiliation></author>
      <pages>5712-5724</pages>
      <abstract>We use contextualized word definitions generated by large language models as semantic representations in the task of diachronic lexical semantic change detection (LSCD). In short, generated definitions are used as ‘senses’, and the change score of a target word is retrieved by comparing their distributions in two time periods under comparison. On the material of five datasets and three languages, we show that generated definitions are indeed specific and general enough to convey a signal sufficient to rank sets of words by the degree of their semantic change over time. Our approach is on par with or outperforms prior non-supervised sense-based LSCD methods. At the same time, it preserves interpretability and allows to inspect the reasons behind a specific shift in terms of discrete definitions-as-senses. This is another step in the direction of explainable semantic change modeling.</abstract>
      <url hash="74acbc37">2024.findings-acl.339</url>
      <bibkey>fedorova-etal-2024-definition</bibkey>
      <doi>10.18653/v1/2024.findings-acl.339</doi>
    </paper>
    <paper id="340">
      <title><fixed-case>M</fixed-case>u<fixed-case>T</fixed-case>ox: Universal <fixed-case>MU</fixed-case>ltilingual Audio-based <fixed-case>TOX</fixed-case>icity Dataset and Zero-shot Detector</title>
      <author><first>Marta</first><last>Costa-jussà</last><affiliation>Meta</affiliation></author>
      <author><first>Mariano</first><last>Meglioli</last><affiliation>Meta</affiliation></author>
      <author><first>Pierre</first><last>Andrews</last></author>
      <author><first>David</first><last>Dale</last><affiliation>FAIR at Meta</affiliation></author>
      <author><first>Prangthip</first><last>Hansanti</last></author>
      <author><first>Elahe</first><last>Kalbassi</last></author>
      <author><first>Alexandre</first><last>Mourachko</last><affiliation>Research, Facebook</affiliation></author>
      <author><first>Christophe</first><last>Ropers</last><affiliation>Meta and Syntexys Inc</affiliation></author>
      <author><first>Carleigh</first><last>Wood</last></author>
      <pages>5725-5734</pages>
      <abstract>Research in toxicity detection in natural language processing for the speech modality (audio-based) is quite limited, particularly for languages other than English. To address these limitations and lay the groundwork for truly multilingual audio-based toxicity detection, we introduce MuTox, the first highly multilingual audio-based dataset with toxicity labels which covers 14 different linguistic families. The dataset comprises 20,000 audio utterances for English and Spanish, and 4,000 for the other 28 languages. To demonstrate the quality of this dataset, we trained the MuTox audio-based toxicity classifier, which enables zero-shot toxicity detection across a wide range of languages. This classifier performs on par with existing text-based trainable classifiers, while expanding the language coverage more than tenfold. When compared to a wordlist-based classifier that covers a similar number of languages, MuTox improves F1-Score by an average of 100%. This significant improvement underscores the potential of MuTox in advancing the field of audio-based toxicity detection.</abstract>
      <url hash="85c65d62">2024.findings-acl.340</url>
      <bibkey>costa-jussa-etal-2024-mutox</bibkey>
      <doi>10.18653/v1/2024.findings-acl.340</doi>
    </paper>
    <paper id="341">
      <title>Phased Instruction Fine-Tuning for Large Language Models</title>
      <author><first>Wei</first><last>Pang</last></author>
      <author><first>Chuan</first><last>Zhou</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiao-Hua</first><last>Zhou</last></author>
      <author><first>Xiaojie</first><last>Wang</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <pages>5735-5748</pages>
      <abstract>Instruction Fine-Tuning, a method enhancing pre-trained language models’ capabilities from mere next-word prediction to complex instruction following, often employs a one-off training approach on diverse instruction dataset. However, this method may not effectively enhance models’ adherence to instructions due to the simultaneous handling of varying instruction complexities. To address this, we propose a novel phased instruction fine-tuning (Phased IFT) method, grounded in the hypothesis of progressive alignment, which posits that the transition of a pre-trained language model from simple next-word prediction to sophisticated instruction following is a gradual learning process. Specifically, we obtain the score of difficulty for each instruction via GPT-4, stratify the instruction data into subsets of increasing difficulty, and sequentially uptrain on these subsets using the standard supervised loss. Through extensive experiments on the pre-trained models Llama-2 7B/13B, and Mistral-7B using the 52K Alpaca instruction data, we demonstrate that Phased IFT significantly surpasses traditional one-off instruction fine-tuning (One-off IFT) method in win rate, empirically validating the progressive alignment hypothesis. Our findings suggest that Phased IFT offers a simple yet effective pathway for elevating the instruction-following capabilities of pre-trained language models.</abstract>
      <url hash="cec30244">2024.findings-acl.341</url>
      <bibkey>pang-etal-2024-phased</bibkey>
      <doi>10.18653/v1/2024.findings-acl.341</doi>
    </paper>
    <paper id="342">
      <title><fixed-case>TOREE</fixed-case>: Evaluating Topic Relevance of Student Essays for <fixed-case>C</fixed-case>hinese Primary and Middle School Education</title>
      <author><first>Xinlin</first><last>Zhuang</last></author>
      <author><first>Hongyi</first><last>Wu</last></author>
      <author><first>Xinshu</first><last>Shen</last></author>
      <author><first>Peimin</first><last>Yu</last></author>
      <author><first>Gaowei</first><last>Yi</last></author>
      <author><first>Xinhao</first><last>Chen</last></author>
      <author><first>Tu</first><last>Hu</last></author>
      <author><first>Yang</first><last>Chen</last></author>
      <author><first>Yupei</first><last>Ren</last></author>
      <author><first>Yadong</first><last>Zhang</last></author>
      <author><first>Youqi</first><last>Song</last></author>
      <author><first>Binxuan</first><last>Liu</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <pages>5749-5765</pages>
      <abstract>Topic relevance of an essay demands that the composition adheres to a clear theme and aligns well with the essay prompt requirements, a critical aspect of essay quality evaluation. However, existing research of Automatic Essay Scoring (AES) for Chinese essays has overlooked topic relevance and lacks detailed feedback, while Automatic Essay Comment Generation (AECG) faces much complexity and difficulty. Additionally, current Large Language Models, including GPT-4, often make incorrect judgments and provide overly impractical feedback when evaluating topic relevance. This paper introduces <b>TOREE</b> (<b>To</b>pic <b>Re</b>levance <b>E</b>valuation), a comprehensive dataset developed to assess topic relevance in Chinese primary and middle school students’ essays, which is beneficial for AES, AECG and other applications. Moreover, our proposed two-step method utilizes TOREE through a combination of Supervised Fine-tuning and Preference Learning. Experimental results demonstrate that TOREE is of high quality, and our method significantly enhances models’ performance on two designed tasks for topic relevance evaluation, improving both automatic and human evaluations across four diverse LLMs.</abstract>
      <url hash="721e2c53">2024.findings-acl.342</url>
      <bibkey>zhuang-etal-2024-toree</bibkey>
      <doi>10.18653/v1/2024.findings-acl.342</doi>
    </paper>
    <paper id="343">
      <title>Predicting the Unpredictable: Uncertainty-Aware Reasoning over Temporal Knowledge Graphs via Diffusion Process</title>
      <author><first>Yuxiang</first><last>Cai</last></author>
      <author><first>Qiao</first><last>Liu</last><affiliation>UESTC</affiliation></author>
      <author><first>Yanglei</first><last>Gan</last></author>
      <author><first>Changlin</first><last>Li</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Xueyi</first><last>Liu</last></author>
      <author><first>Run</first><last>Lin</last></author>
      <author><first>Da</first><last>Luo</last></author>
      <author><first>JiayeYang</first><last>JiayeYang</last></author>
      <pages>5766-5778</pages>
      <abstract>Temporal Knowledge Graph (TKG) reasoning seeks to predict future incomplete facts leveraging historical data. While existing approaches have shown effectiveness in addressing the task through various perspectives, such as graph learning and logic rules, they are limited in capturing the indeterminacy in future events, particularly in the case of rare/unseen facts. To tackle the highlighted issues, we introduce a novel approach by conceptualizing TKG reasoning as a sequence denoising process for future facts, namely DiffuTKG. Concretely, we first encodes the historical events as the conditional sequence. Then we gradually introduce Gaussian noise to corrupt target facts during the forward process and then employ a transformer-based conditional denoiser to restore them in the reverse phase. Moreover, we introduce an uncertainty regularization loss to mitigate the risk of prediction biases by favoring frequent scenarios over rare/unseen facts. Empirical results on four real-world datasets show that DiffuTKG outperforms state-of-the-art methods across multiple evaluation metrics.</abstract>
      <url hash="88892a81">2024.findings-acl.343</url>
      <bibkey>cai-etal-2024-predicting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.343</doi>
    </paper>
    <paper id="344">
      <title>Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks</title>
      <author><first>Haz</first><last>Shahgir</last></author>
      <author><first>Xianghao</first><last>Kong</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Greg</first><last>Ver Steeg</last><affiliation>University of California, Riverside, Amazon and USC/ISI</affiliation></author>
      <author><first>Yue</first><last>Dong</last><affiliation>University of California, Riverside and McGill University</affiliation></author>
      <pages>5779-5796</pages>
      <abstract>The widespread use of Text-to-Image (T2I) models in content generation requires careful examination of their safety, including their robustness to adversarial attacks. Despite extensive research on adversarial attacks, the reasons for their effectiveness remain underexplored. This paper presents an empirical study on adversarial attacks against T2I models, focusing on analyzing factors associated with attack success rates (ASR). We introduce a new attack objective - entity swapping using adversarial suffixes and two gradient-based attack algorithms. Human and automatic evaluations reveal the asymmetric nature of ASRs on entity swap: for example, it is easier to replace “human” with “robot” in the prompt “a human dancing in the rain.” with an adversarial suffix, but the reverse replacement is significantly harder. We further propose probing metrics to establish indicative signals from the model’s beliefs to the adversarial ASR. We identify conditions that result in a success probability of 60% for adversarial attacks and others where this likelihood drops below 5%. The code and data are available at https://github.com/Patchwork53/AsymmetricAttack</abstract>
      <url hash="15236d9e">2024.findings-acl.344</url>
      <bibkey>shahgir-etal-2024-asymmetric</bibkey>
      <doi>10.18653/v1/2024.findings-acl.344</doi>
    </paper>
    <paper id="345">
      <title>Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs</title>
      <author><first>Xun</first><last>Liang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Hanyu</first><last>Wang</last></author>
      <author><first>Shichao</first><last>Song</last></author>
      <author><first>Mengting</first><last>Hu</last><affiliation>Nankai University</affiliation></author>
      <author><first>Xunzhi</first><last>Wang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Zhiyu</first><last>Li</last></author>
      <author><first>Feiyu</first><last>Xiong</last><affiliation>Institute for Advanced Algorithms Research, Shanghai</affiliation></author>
      <author><first>Bo</first><last>Tang</last></author>
      <pages>5797-5814</pages>
      <abstract>Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant decrease in perplexity, markedly improving text fluency.</abstract>
      <url hash="94d92df6">2024.findings-acl.345</url>
      <bibkey>liang-etal-2024-controlled</bibkey>
      <doi>10.18653/v1/2024.findings-acl.345</doi>
    </paper>
    <paper id="346">
      <title>Coconut: Contextualized Commonsense Unified Transformers for Graph-Based Commonsense Augmentation of Language Models</title>
      <author><first>Jun-Hyung</first><last>Park</last></author>
      <author><first>Mingyu</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Junho</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>SangKeun</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <pages>5815-5830</pages>
      <abstract>In this paper, we introduce COCONUT to effectively guide the contextualization of structured commonsense knowledge based on largelanguage models. COCONUT employs a contextualized knowledge prompting scheme to gather high-quality contextualization examplesfrom a large language model. These examples are subsequently distilled into small language models to enhance their contextualization capability. Extensive evaluations show that COCONUT considerably improves commonsense reasoning performance across diverse benchmarks, models, and settings, exhibiting its flexibility and universality in generating contextualized commonsense knowledge. Notably,COCONUT consistently outperforms the state-of-the-art technique by an average of 5.8%.</abstract>
      <url hash="d0222861">2024.findings-acl.346</url>
      <bibkey>park-etal-2024-coconut</bibkey>
      <doi>10.18653/v1/2024.findings-acl.346</doi>
    </paper>
    <paper id="347">
      <title>Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge</title>
      <author><first>Daniel</first><last>Tamayo</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Aitor</first><last>Gonzalez-Agirre</last></author>
      <author><first>Javier</first><last>Hernando</last><affiliation>Barcelona Supercomputing Center and Universidad Politécnica de Cataluna</affiliation></author>
      <author><first>Marta</first><last>Villegas</last><affiliation>Barcelona Supercomputing Center, Universitat Pompeu Fabra and Universitat Autònoma de Barcelona</affiliation></author>
      <pages>5831-5847</pages>
      <abstract>Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at https://github.com/dtamayo-nlp/MEMAT.</abstract>
      <url hash="5936a0c3">2024.findings-acl.347</url>
      <bibkey>mela-etal-2024-mass</bibkey>
      <doi>10.18653/v1/2024.findings-acl.347</doi>
    </paper>
    <paper id="348">
      <title><fixed-case>B</fixed-case>io<fixed-case>M</fixed-case>istral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains</title>
      <author><first>Yanis</first><last>Labrak</last></author>
      <author><first>Adrien</first><last>Bazoge</last><affiliation>Nantes Université</affiliation></author>
      <author><first>Emmanuel</first><last>Morin</last></author>
      <author><first>Pierre-Antoine</first><last>Gourraud</last><affiliation>Université de Nantes</affiliation></author>
      <author><first>Mickael</first><last>Rouvier</last><affiliation>Université d’Avignon</affiliation></author>
      <author><first>Richard</first><last>Dufour</last><affiliation>Nantes University</affiliation></author>
      <pages>5848-5864</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges.In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral’s superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical LLMs, we automatically translated and evaluated this benchmark into 7 other languages. This marks the first large-scale multilingual evaluation of LLMs in the medical domain. Datasets, multilingual evaluation benchmarks, scripts, and all the models obtained during our experiments are freely released.</abstract>
      <url hash="12baba7f">2024.findings-acl.348</url>
      <bibkey>labrak-etal-2024-biomistral</bibkey>
      <doi>10.18653/v1/2024.findings-acl.348</doi>
    </paper>
    <paper id="349">
      <title>All Languages Matter: On the Multilingual Safety of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Wenxuan</first><last>Wang</last></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Chang</first><last>Chen</last></author>
      <author><first>Youliang</first><last>Yuan</last><affiliation>The Chinese University of Hong Kong-Shenzhen</affiliation></author>
      <author><first>Jen-tse</first><last>Huang</last></author>
      <author><first>Wenxiang</first><last>Jiao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Michael</first><last>Lyu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>5865-5877</pages>
      <abstract>Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open-source models. Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages. In addition, we propose a simple and effective prompting method to improve the multilingual safety of ChatGPT by enhancing cross-lingual generalization of safety alignment. Our prompting method can significantly reduce the ratio of unsafe responses by 42% for non-English queries. We will release all the data and results to facilitate future research on LLMs’ safety.</abstract>
      <url hash="31e6c56b">2024.findings-acl.349</url>
      <bibkey>wang-etal-2024-languages</bibkey>
      <doi>10.18653/v1/2024.findings-acl.349</doi>
    </paper>
    <paper id="350">
      <title><fixed-case>LJPC</fixed-case>heck: Functional Tests for Legal Judgment Prediction</title>
      <author><first>Yuan</first><last>Zhang</last></author>
      <author><first>Wanhong</first><last>Huang</last></author>
      <author><first>Yi</first><last>Feng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Chuanyi</first><last>Li</last><affiliation>nanjing university</affiliation></author>
      <author><first>Zhiwei</first><last>Fei</last><affiliation>Fudan University, Harbin Institute of Technology, Dalian University of Technology, Shanghai Jiaotong University, Shandong University, Peking University, Zhejiang University, University of Science and Technology of China, Hunan University, Beijing Institute of Technology, University of the Chinese Academy of Sciences, Southeast University, Sichuan University, Monash University, Malaysia Campus, Tianjin University, Beijing University of Aeronautics and Astronautics, Wuhan University of Technology, Yale University, Technische Universität München, Wuhan University, nanjing university, Tsinghua University and Wuhan University</affiliation></author>
      <author><first>Jidong</first><last>Ge</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Bin</first><last>Luo</last><affiliation>nanjing university</affiliation></author>
      <author><first>Vincent</first><last>Ng</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>5878-5894</pages>
      <abstract>Legal Judgment Prediction (LJP) refers to the task of automatically predicting judgment results (e.g., charges, law articles and term of penalty) given the fact description of cases. While SOTA models have achieved high accuracy and F1 scores on public datasets, existing datasets fail to evaluate specific aspects of these models (e.g., legal fairness, which significantly impact their applications in real scenarios). Inspired by functional testing in software engineering, we introduce LJPCHECK, a suite of functional tests for LJP models, to comprehend LJP models’ behaviors and offer diagnostic insights. We illustrate the utility of LJPCHECK on five SOTA LJP models. Extensive experiments reveal vulnerabilities in these models, prompting an in-depth discussion into the underlying reasons of their shortcomings.</abstract>
      <url hash="f9481970">2024.findings-acl.350</url>
      <bibkey>zhang-etal-2024-ljpcheck</bibkey>
      <doi>10.18653/v1/2024.findings-acl.350</doi>
    </paper>
    <paper id="351">
      <title><fixed-case>CMDL</fixed-case>: A Large-Scale <fixed-case>C</fixed-case>hinese Multi-Defendant Legal Judgment Prediction Dataset</title>
      <author><first>Wanhong</first><last>Huang</last></author>
      <author><first>Yi</first><last>Feng</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Chuanyi</first><last>Li</last><affiliation>nanjing university</affiliation></author>
      <author><first>Honghan</first><last>Wu</last></author>
      <author><first>Jidong</first><last>Ge</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Vincent</first><last>Ng</last><affiliation>University of Texas at Dallas</affiliation></author>
      <pages>5895-5906</pages>
      <abstract>Legal Judgment Prediction (LJP) has attracted significant attention in recent years. However, previous studies have primarily focused on cases involving only a single defendant, skipping multi-defendant cases due to complexity and difficulty. To advance research, we introduce CMDL, a large-scale real-world Chinese Multi-Defendant LJP dataset, which consists of over 393,945 cases with nearly 1.2 million defendants in total. For performance evaluation, we propose case-level evaluation metrics dedicated for the multi-defendant scenario. Experimental results on CMDL show existing SOTA approaches demonstrate weakness when applied to cases involving multiple defendants. We highlight several challenges that require attention and resolution.</abstract>
      <url hash="9db72ffb">2024.findings-acl.351</url>
      <bibkey>huang-etal-2024-cmdl</bibkey>
      <doi>10.18653/v1/2024.findings-acl.351</doi>
    </paper>
    <paper id="352">
      <title>Model Editing by Standard Fine-Tuning</title>
      <author><first>Govind Krishnan</first><last>Gangadhar</last><affiliation>Rutgers University</affiliation></author>
      <author><first>Karl</first><last>Stratos</last><affiliation>Rutgers University</affiliation></author>
      <pages>5907-5913</pages>
      <abstract>Standard fine-tuning is considered not as effective as specialized methods for model editing due to its comparatively poor performance. However, it is simple, agnostic to the architectural details of the model being edited, and able to leverage advances in standard training techniques with no additional work (e.g., black-box PEFT for computational efficiency), making it an appealing choice for a model editor. In this work, we show that standard fine-tuning alone can yield competitive model editing performance with two minor modifications. First, we optimize the conditional likelihood rather than the full likelihood. Second, in addition to the typical practice of training on randomly paraphrased edit prompts to encourage generalization, we also train on random or similar unedited facts to encourage locality. Our experiments on the ZsRE and CounterFact datasets demonstrate that these simple modifications allow standard fine-tuning to match or outperform highly specialized editors in terms of edit score.</abstract>
      <url hash="e17547d8">2024.findings-acl.352</url>
      <bibkey>gangadhar-stratos-2024-model</bibkey>
      <doi>10.18653/v1/2024.findings-acl.352</doi>
    </paper>
    <paper id="353">
      <title><fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation-Based Logic-Driven Data Augmentation for Logical Reasoning</title>
      <author><first>Qiming</first><last>Bao</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Alex</first><last>Peng</last></author>
      <author><first>Zhenyun</first><last>Deng</last></author>
      <author><first>Wanjun</first><last>Zhong</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Gael</first><last>Gendron</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Timothy</first><last>Pistotti</last></author>
      <author><first>Neset</first><last>Tan</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Nathan</first><last>Young</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Yang</first><last>Chen</last></author>
      <author><first>Yonghua</first><last>Zhu</last></author>
      <author><first>Paul</first><last>Denny</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Michael</first><last>Witbrock</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Jiamou</first><last>Liu</last><affiliation>The University of Auckland</affiliation></author>
      <pages>5914-5934</pages>
      <abstract>Combining large language models with logical reasoning enhances their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges when gathering reliable data from the web to build comprehensive training datasets, subsequently affecting performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logical structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into text to create augmented data. Notably, our methodology is architecture-agnostic and enhances both generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and discriminative large language models through contrastive learning with logic-driven data augmentation. Empirical evidence underscores the efficacy of our proposed method with improvement in performance across seven downstream tasks, such as reading comprehension requiring logical reasoning, textual entailment, and natural language inference. Furthermore, our method leads on the ReClor leaderboard. The source code and data are publicly available</abstract>
      <url hash="0d8520c6">2024.findings-acl.353</url>
      <bibkey>bao-etal-2024-abstract</bibkey>
      <doi>10.18653/v1/2024.findings-acl.353</doi>
    </paper>
    <paper id="354">
      <title><fixed-case>C</fixed-case>ode<fixed-case>I</fixed-case>nsight: A Curated Dataset of Practical Coding Solutions from <fixed-case>S</fixed-case>tack <fixed-case>O</fixed-case>verflow</title>
      <author><first>Nathanaël</first><last>Beau</last></author>
      <author><first>Benoit</first><last>Crabbé</last><affiliation>Université de Paris</affiliation></author>
      <pages>5935-5947</pages>
      <abstract>We introduce a novel dataset tailored for code generation, aimed at aiding developers in common tasks. Our dataset provides examples that include a clarified intent, code snippets associated, and an average of three related unit tests. It encompasses a range of libraries such as Pandas, Numpy, and Regex, along with more than 70 standard libraries in Python code derived from Stack Overflow. Comprising 3,402 crafted examples by Python experts, our dataset is designed for both model finetuning and standalone evaluation. To complete unit tests evaluation, we categorize examples in order to get more fine grained analysis, enhancing the understanding of models’ strengths and weaknesses in specific coding tasks. The examples have been refined to reduce data contamination, a process confirmed by the performance of three leading models: Mistral 7B, CodeLLAMA 13B, and Starcoder 15B. We further investigate data-contamination testing GPT-4 performance on a part of our dataset. The benchmark can be accessed at anonymized address.</abstract>
      <url hash="48cccfbe">2024.findings-acl.354</url>
      <bibkey>beau-crabbe-2024-codeinsight</bibkey>
      <revision id="1" href="2024.findings-acl.354v1" hash="1f2c9568"/>
      <revision id="2" href="2024.findings-acl.354v2" hash="48cccfbe" date="2024-09-17">Minor updates.</revision>
      <doi>10.18653/v1/2024.findings-acl.354</doi>
    </paper>
    <paper id="355">
      <title><fixed-case>V</fixed-case>i<fixed-case>H</fixed-case>ate<fixed-case>T</fixed-case>5: Enhancing Hate Speech Detection in <fixed-case>V</fixed-case>ietnamese With a Unified Text-to-Text Transformer Model</title>
      <author><first>Luan</first><last>Thanh Nguyen</last><affiliation>University of Information Technology, Vietnam National University Ho Chi Minh City</affiliation></author>
      <pages>5948-5961</pages>
      <abstract>Recent advancements in hate speech detection (HSD) in Vietnamese have made significant progress, primarily attributed to the emergence of transformer-based pre-trained language models, particularly those built on the BERT architecture. However, the necessity for specialized fine-tuned models has resulted in the complexity and fragmentation of developing a multitasking HSD system. Moreover, most current methodologies focus on fine-tuning general pre-trained models, primarily trained on formal textual datasets like Wikipedia, which may not accurately capture human behavior on online platforms. In this research, we introduce ViHateT5, a T5-based model pre-trained on our proposed large-scale domain-specific dataset named VOZ-HSD. By harnessing the power of a text-to-text architecture, ViHateT5 can tackle multiple tasks using a unified model and achieve state-of-the-art performance across all standard HSD benchmarks in Vietnamese. Our experiments also underscore the significance of label distribution in pre-training data on model efficacy. We provide our experimental materials for research purposes, including the VOZ-HSD dataset, pre-trained checkpoint, the unified HSD-multitask ViHateT5 model, and related source code on GitHub publicly.</abstract>
      <url hash="1c103e94">2024.findings-acl.355</url>
      <bibkey>thanh-nguyen-2024-vihatet5</bibkey>
      <doi>10.18653/v1/2024.findings-acl.355</doi>
    </paper>
    <paper id="356">
      <title>Bias in News Summarization: Measures, Pitfalls and Corpora</title>
      <author><first>Julius</first><last>Steen</last><affiliation>Institute for Computational Linguistics, Heidelberg University, Heidelberg University</affiliation></author>
      <author><first>Katja</first><last>Markert</last><affiliation>Heidelberg University</affiliation></author>
      <pages>5962-5983</pages>
      <abstract>Summarization is an important application of large language models (LLMs). Most previous evaluation of summarization models has focused on their content selection, faithfulness, grammaticality and coherence. However, it is well known that LLMs can reproduce and reinforce harmful social biases. This raises the question: Do biases affect model outputs in a constrained setting like summarization?To help answer this question, we first motivate and introduce a number of definitions for biased behaviours in summarization models, along with practical operationalizations. Since we find that biases inherent to input documents can confound bias analysis in summaries, we propose a method to generate input documents with carefully controlled demographic attributes. This allows us to study summarizer behavior in a controlled setting, while still working with realistic input documents.We measure gender bias in English summaries generated by both purpose-built summarization models and general purpose chat models as a case study. We find content selection in single document summarization to be largely unaffected by gender bias, while hallucinations exhibit evidence of bias.To demonstrate the generality of our approach, we additionally investigate racial bias, including intersectional settings.</abstract>
      <url hash="7ba5153c">2024.findings-acl.356</url>
      <bibkey>steen-markert-2024-bias</bibkey>
      <doi>10.18653/v1/2024.findings-acl.356</doi>
    </paper>
    <paper id="357">
      <title>When to Trust <fixed-case>LLM</fixed-case>s: Aligning Confidence with Response Quality</title>
      <author><first>Shuchang</first><last>Tao</last></author>
      <author><first>Liuyi</first><last>Yao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Hanxing</first><last>Ding</last></author>
      <author><first>Yuexiang</first><last>Xie</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Qi</first><last>Cao</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences, China</affiliation></author>
      <author><first>Fei</first><last>Sun</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jinyang</first><last>Gao</last></author>
      <author><first>Huawei</first><last>Shen</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Bolin</first><last>Ding</last><affiliation>Alibaba Group</affiliation></author>
      <pages>5984-5996</pages>
      <abstract>Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text. This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains. Existing methods often express reliability by confidence level, however, their effectiveness is limited by the lack of objective guidance. To address this, we propose CONfidence-Quality-ORDer-preserving alignment approach (CONQORD), which leverages reinforcement learning guided by a tailored dual-component reward function. This function integrates quality reward and order-preserving alignment reward functions. Specifically, the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality. Experiments demonstrate that CONQORD significantly improves the alignment performance between confidence and response accuracy, without causing over-cautious. Furthermore, the aligned confidence provided by CONQORD informs when to trust LLMs, and acts as a determinant for initiating the retrieval process of external knowledge. Aligning confidence with response quality ensures more transparent and reliable responses, providing better trustworthiness.</abstract>
      <url hash="dfedabe9">2024.findings-acl.357</url>
      <bibkey>tao-etal-2024-trust</bibkey>
      <doi>10.18653/v1/2024.findings-acl.357</doi>
    </paper>
    <paper id="358">
      <title>Zero-shot Cross-lingual Alignment for Embedding Initialization</title>
      <author><first>Xi</first><last>Ai</last></author>
      <author><first>Zhiyong</first><last>Huang</last><affiliation>NUS School of Computing</affiliation></author>
      <pages>5997-6007</pages>
      <abstract>For multilingual training, we present CrossInit, an initialization method that initializes embeddings into similar geometrical structures across languages in an unsupervised manner. CrossInit leverages a common cognitive linguistic mechanism, Zipf’s law, which indicates that similar concepts across languages have similar word ranks or frequencies in their monolingual corpora. Instead of considering point-to-point alignments based on ranks, CrossInit considers the same span of consecutive ranks in each language as the Positive pairs for alignment, while others out of the span are used as Negative pairs. CrossInit then employs Contrastive Learning to iteratively refine randomly initialized embeddings for similar geometrical structures across languages. Our experiments on Unsupervised NMT, XNLI, and MLQA showed significant gains in low-resource and dissimilar languages after applying CrossInit.</abstract>
      <url hash="109fd37e">2024.findings-acl.358</url>
      <bibkey>ai-huang-2024-zero</bibkey>
      <revision id="1" href="2024.findings-acl.358v1" hash="dbad09fc"/>
      <revision id="2" href="2024.findings-acl.358v2" hash="109fd37e" date="2024-08-17">Minor updates.</revision>
      <doi>10.18653/v1/2024.findings-acl.358</doi>
    </paper>
    <paper id="359">
      <title>Mitigating Hallucinations in Large Vision-Language Models (<fixed-case>LVLM</fixed-case>s) via Language-Contrastive Decoding (<fixed-case>LCD</fixed-case>)</title>
      <author><first>Avshalom</first><last>Manevich</last></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>6008-6022</pages>
      <abstract>Large Vision-Language Models (LVLMs) are an extension of Large Language Models (LLMs) that facilitate processing both image and text inputs, expanding AI capabilities. However, LVLMs struggle with object hallucinations due to their reliance on text cues and learned object co-occurrence biases. While most research quantifies these hallucinations, mitigation strategies are still lacking. Our study introduces a Language Contrastive Decoding (LCD) algorithm that adjusts LVLM outputs based on LLM distribution confidence levels, effectively reducing object hallucinations. We demonstrate the advantages of LCD in leading LVLMs, showing up to %4 improvement in POPE F1 scores and up to %36 reduction in CHAIR scores on the COCO validation set, while also improving captioning quality scores. Our method effectively improves LVLMs without needing complex post-processing or retraining, and is easily applicable to different models. Our findings highlight the potential of further exploration of LVLM-specific decoding algorithms.</abstract>
      <url hash="900f3a05">2024.findings-acl.359</url>
      <bibkey>manevich-tsarfaty-2024-mitigating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.359</doi>
    </paper>
    <paper id="360">
      <title>It takes two to borrow: a donor and a recipient. Who’s who?</title>
      <author><first>Liviu</first><last>Dinu</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Ana</first><last>Uban</last><affiliation>Universitatea Bucuresti</affiliation></author>
      <author><first>Anca</first><last>Dinu</last></author>
      <author><first>Ioan-Bogdan</first><last>Iordache</last></author>
      <author><first>Simona</first><last>Georgescu</last><affiliation>University of Bucharest</affiliation></author>
      <author><first>Laurentiu</first><last>Zoicas</last><affiliation>University of Bucharest</affiliation></author>
      <pages>6023-6035</pages>
      <abstract>We address the open problem of automatically identifying the direction of lexical borrowing, given word pairs in the donor and recipient languages. We propose strong benchmarks for this task, by applying a set of machine learning models. We extract and publicly release a comprehensive borrowings dataset from the recent RoBoCoP cognates and borrowings database for five Romance languages. We experiment on this dataset with both graphic and phonetic representations and with different features, models and architectures. We interpret the results, in terms of F1 score, commenting on the influence of features and model choice, of the imbalanced data and of the inherent difficulty of the task for particular language pairs. We show that automatically determining the direction of borrowing is a feasible task, and propose additional directions for future work.</abstract>
      <url hash="c2a945fb">2024.findings-acl.360</url>
      <bibkey>dinu-etal-2024-takes</bibkey>
      <doi>10.18653/v1/2024.findings-acl.360</doi>
    </paper>
    <paper id="361">
      <title>Advancing Post-<fixed-case>OCR</fixed-case> Correction: A Comparative Study of Synthetic Data</title>
      <author><first>Shuhao</first><last>Guan</last></author>
      <author><first>Derek</first><last>Greene</last><affiliation>University College Dublin</affiliation></author>
      <pages>6036-6047</pages>
      <abstract>This paper explores the application of synthetic data in the post-OCR domain on multiple fronts by conducting experiments to assess the impact of data volume, augmentation, and synthetic data generation methods on model performance. Furthermore, we introduce a novel algorithm that leverages computer vision feature detection algorithms to calculate glyph similarity for constructing post-OCR synthetic data. Through experiments conducted across a variety of languages, including several low-resource ones, we demonstrate that models like ByT5 can significantly reduce Character Error Rates (CER) without the need for manually annotated data, and our proposed synthetic data generation method shows advantages over traditional methods, particularly in low-resource languages.</abstract>
      <url hash="5e2341a7">2024.findings-acl.361</url>
      <bibkey>guan-greene-2024-advancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.361</doi>
    </paper>
    <paper id="362">
      <title><fixed-case>G</fixed-case>eo<fixed-case>A</fixed-case>gent: To Empower <fixed-case>LLM</fixed-case>s using Geospatial Tools for Address Standardization</title>
      <author><first>Chenghua</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Shisong</first><last>Chen</last></author>
      <author><first>Zhixu</first><last>Li</last></author>
      <author><first>Jianfeng</first><last>Qu</last><affiliation>Soochow University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiaxin</first><last>Liu</last></author>
      <author><first>Zhigang</first><last>Chen</last><affiliation>iFLYTEK Research</affiliation></author>
      <pages>6048-6063</pages>
      <abstract>This paper presents a novel solution to tackle the challenges that posed by the abundance of non-standard addresses, which input by users in modern applications such as navigation maps, ride-hailing apps, food delivery platforms, and logistics services. These manually entered addresses often contain irregularities, such as missing information, spelling errors, colloquial descriptions, and directional offsets, which hinder address-related tasks like address matching and linking. To tackle these challenges, we propose GeoAgent, a new framework comprising two main components: a large language model (LLM) and a suite of geographical tools. By harnessing the semantic understanding capabilities of the LLM and integrating specific geospatial tools, GeoAgent incorporates spatial knowledge into address texts and achieves efficient address standardization. Further, to verify the effectiveness and practicality of our approach, we construct a comprehensive dataset of complex non-standard addresses, which fills the gaps in existing datasets and proves invaluable for training and evaluating the performance of address standardization models in this community. Experimental results demonstrate the efficacy of GeoAgent, showcasing substantial improvements in the performance of address-related models across various downstream tasks.</abstract>
      <url hash="782c0ab8">2024.findings-acl.362</url>
      <bibkey>huang-etal-2024-geoagent</bibkey>
      <doi>10.18653/v1/2024.findings-acl.362</doi>
    </paper>
    <paper id="363">
      <title><fixed-case>HQP</fixed-case>: A Human-Annotated Dataset for Detecting Online Propaganda</title>
      <author><first>Abdurahman</first><last>Maarouf</last></author>
      <author><first>Dominik</first><last>Bär</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Dominique</first><last>Geissler</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Stefan</first><last>Feuerriegel</last><affiliation>LMU Munich</affiliation></author>
      <pages>6064-6089</pages>
      <abstract>Online propaganda poses a severe threat to the integrity of societies. However, existing datasets for detecting online propaganda have a key limitation: they were annotated using weak labels that can be noisy and even incorrect. To address this limitation, our work makes the following contributions: (1) We present HQP: a novel dataset (N=30000) for detecting online propaganda with high-quality labels. To the best of our knowledge, HQP is the first large-scale dataset for detecting online propaganda that was created through human annotation. (2) We show empirically that state-of-the-art language models fail in detecting online propaganda when trained with weak labels (AUC: 64.03). In contrast, state-of-the-art language models can accurately detect online propaganda when trained with our high-quality labels (AUC: 92.25), which is an improvement of 44%. (3) We show that prompt-based learning using a small sample of high-quality labels can still achieve a reasonable performance (AUC: 80.27) while significantly reducing the cost of labeling. (4) We extend HQP to HQP+ to test how well propaganda across different contexts can be detected. Crucially, our work highlights the importance of high-quality labels for sensitive NLP tasks such as propaganda detection.</abstract>
      <url hash="f9826b2f">2024.findings-acl.363</url>
      <bibkey>maarouf-etal-2024-hqp</bibkey>
      <doi>10.18653/v1/2024.findings-acl.363</doi>
    </paper>
    <paper id="364">
      <title>Teaching Language Models to Self-Improve by Learning from Language Feedback</title>
      <author><first>Chi</first><last>Hu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yimin</first><last>Hu</last></author>
      <author><first>Hang</first><last>Cao</last></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>JingBo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>6090-6101</pages>
      <abstract>Aligning Large Language Models (LLMs) with human intentions and values is crucial yet challenging. Current methods primarily rely on human preferences, which are costly and insufficient in capturing nuanced feedback expressed in natural language. In this paper, we present Self-Refinement Tuning (SRT), a method that leverages model feedback for alignment, thereby reducing reliance on human annotations. SRT uses a base language model (e.g., Tulu2) to generate initial responses, which are critiqued and refined by a more advanced model (e.g., GPT-4-Turbo). This process enables the base model to self-evaluate and improve its outputs, facilitating continuous learning. SRT further optimizes the model by learning from its self-generated feedback and refinements, creating a feedback loop that promotes model improvement. Our empirical evaluations demonstrate that SRT significantly outperforms strong baselines across diverse tasks and model sizes. When applied to a 70B parameter model, SRT increases the win rate from 9.6% to 25.8% on the AlpacaEval 2.0 benchmark, surpassing well-established systems such as GPT-4-0314, Claude 2, and Gemini. Our analysis highlights the crucial role of language feedback in the success of SRT, suggesting potential for further exploration in this direction.</abstract>
      <url hash="8249fcd5">2024.findings-acl.364</url>
      <bibkey>hu-etal-2024-teaching</bibkey>
      <doi>10.18653/v1/2024.findings-acl.364</doi>
    </paper>
    <paper id="365">
      <title>Exploring Spatial Schema Intuitions in Large Language and Vision Models</title>
      <author><first>Philipp</first><last>Wicke</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Lennart</first><last>Wachowiak</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>6102-6117</pages>
      <abstract>Despite the ubiquity of large language models (LLMs) in AI research, the question of embodiment in LLMs remains underexplored, distinguishing them from embodied systems in robotics where sensory perception directly informs physical action.Our investigation navigates the intriguing terrain of whether LLMs, despite their non-embodied nature, effectively capture implicit human intuitions about fundamental, spatial building blocks of language. We employ insights from spatial cognitive foundations developed through early sensorimotor experiences, guiding our exploration through the reproduction of three psycholinguistic experiments. Surprisingly, correlations between model outputs and human responses emerge, revealing adaptability without a tangible connection to embodied experiences. Notable distinctions include polarized language model responses and reduced correlations in vision language models. This research contributes to a nuanced understanding of the interplay between language, spatial experiences, and the computations made by large language models.Project Website: https://cisnlp.github.io/Spatial_Schemas/</abstract>
      <url hash="3ce71b2d">2024.findings-acl.365</url>
      <bibkey>wicke-wachowiak-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.365</doi>
    </paper>
    <paper id="366">
      <title>Efficient Detection of <fixed-case>LLM</fixed-case>-generated Texts with a <fixed-case>B</fixed-case>ayesian Surrogate Model</title>
      <author><first>Yibo</first><last>Miao</last></author>
      <author><first>Hongcheng</first><last>Gao</last></author>
      <author><first>Hao</first><last>Zhang</last><affiliation>University of California, San Diego, Petuum, Inc and Carnegie Mellon University</affiliation></author>
      <author><first>Zhijie</first><last>Deng</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <pages>6118-6130</pages>
      <abstract>The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.</abstract>
      <url hash="681035d3">2024.findings-acl.366</url>
      <bibkey>miao-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.findings-acl.366</doi>
    </paper>
    <paper id="367">
      <title>Decoding the Narratives: Analyzing Personal Drug Experiences Shared on <fixed-case>R</fixed-case>eddit</title>
      <author><first>Layla</first><last>Bouzoubaa</last><affiliation>Drexel University</affiliation></author>
      <author><first>Elham</first><last>Aghakhani</last></author>
      <author><first>Max</first><last>Song</last></author>
      <author><first>Quang</first><last>Trinh</last></author>
      <author><first>Shadi</first><last>Rezapour</last><affiliation>Drexel University</affiliation></author>
      <pages>6131-6148</pages>
      <abstract>Online communities such as drug-related subreddits serve as safe spaces for people who use drugs (PWUD), fostering discussions on substance use experiences, harm reduction, and addiction recovery. Users’ shared narratives on these forums provide insights into the likelihood of developing a substance use disorder (SUD) and recovery potential. Our study aims to develop a multi-level, multi-label classification model to analyze online user-generated texts about substance use experiences. For this purpose, we first introduce a novel taxonomy to assess the nature of posts, including their intended connections (Inquisition or Disclosure), subjects (e.g., Recovery, Dependency), and specific objectives (e.g., Relapse, Quality, Safety). Using various multi-label classification algorithms on a set of annotated data, we show that GPT-4, when prompted with instructions, definitions, and examples, outperformed all other models. We apply this model to label an additional 1,000 posts and analyze the categories of linguistic expression used within posts in each class. Our analysis shows that topics such as Safety, Combination of Substances, and Mental Health see more disclosure, while discussions about physiological Effects focus on harm reduction. Our work enriches the understanding of PWUD’s experiences and informs the broader knowledge base on SUD and drug use.</abstract>
      <url hash="cf6c48ed">2024.findings-acl.367</url>
      <bibkey>bouzoubaa-etal-2024-decoding</bibkey>
      <doi>10.18653/v1/2024.findings-acl.367</doi>
    </paper>
    <paper id="368">
      <title>Unveiling the Art of Heading Design: A Harmonious Blend of Summarization, Neology, and Algorithm</title>
      <author><first>Shaobo</first><last>Cui</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Yiyang</first><last>Feng</last></author>
      <author><first>Yisong</first><last>Mao</last></author>
      <author><first>Yifan</first><last>Hou</last><affiliation>Department of Computer Science, Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Boi</first><last>Faltings</last></author>
      <pages>6149-6174</pages>
      <abstract>Crafting an appealing heading is crucial for attracting readers and marketing work or products. A popular way is to summarize the main idea with a refined description and a memorable acronym. However, there lacks a systematic study and a formal benchmark including datasets and metrics. Motivated by this absence, we introduce LOgogram, a novel benchmark comprising 6,653 paper abstracts with corresponding descriptions and acronyms. To measure the quality of heading generation, we propose a set of evaluation metrics from three aspects: summarization, neology, and algorithm. Additionally, we explore three strategies for heading generation(generation ordering, tokenization of acronyms, and framework design) under various prevalent learning paradigms(supervised fine-tuning, in-context learning with Large Language Models(LLMs), and reinforcement learning) on our benchmark. Our experimental results indicate the difficulty in identifying a practice that excels across all summarization, neologistic, and algorithmic aspects.</abstract>
      <url hash="5160e534">2024.findings-acl.368</url>
      <bibkey>cui-etal-2024-unveiling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.368</doi>
    </paper>
    <paper id="369">
      <title>Understanding Fine-grained Distortions in Reports of Scientific Findings</title>
      <author><first>Amelie</first><last>Wuehrl</last><affiliation>University of Stuttgart, Universität Stuttgart</affiliation></author>
      <author><first>Dustin</first><last>Wright</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Roman</first><last>Klinger</last><affiliation>Otto-Friedrich Universität Bamberg</affiliation></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>6175-6191</pages>
      <abstract>Distorted science communication harms individuals and society as it can lead to unhealthy behavior change and decrease trust in scientific institutions. Given the rapidly increasing volume of science communication in recent years, a fine-grained understanding of how findings from scientific publications are reported to the general public, and methods to detect distortions from the original work automatically, are crucial. Prior work focused on individual aspects of distortions or worked with unpaired data. In this work, we make three foundational contributions towards addressing this problem: (1) annotating 1,600 instances of scientific findings from academic papers paired with corresponding findings as reported in news articles and tweets wrt. four characteristics: causality, certainty, generality and sensationalism; (2) establishing baselines for automatically detecting these characteristics; and (3) analyzing the prevalence of changes in these characteristics in both human-annotated and large-scale unlabeled data. Our results show that scientific findings frequently undergo subtle distortions when reported. Tweets distort findings more often than science news reports. Detecting fine-grained distortions automatically poses a challenging task. In our experiments, fine-tuned task-specific models consistently outperform few-shot LLM prompting.</abstract>
      <url hash="e5a315f5">2024.findings-acl.369</url>
      <bibkey>wuehrl-etal-2024-understanding</bibkey>
      <doi>10.18653/v1/2024.findings-acl.369</doi>
    </paper>
    <paper id="370">
      <title><fixed-case>MM</fixed-case>-<fixed-case>SOC</fixed-case>: Benchmarking Multimodal Large Language Models in Social Media Platforms</title>
      <author><first>Yiqiao</first><last>Jin</last></author>
      <author><first>Minje</first><last>Choi</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Gaurav</first><last>Verma</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Jindong</first><last>Wang</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Srijan</first><last>Kumar</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>6192-6210</pages>
      <abstract>Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs’ understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need for advancements in models’ social understanding capabilities. Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks. However, MLLMs demonstrate performance improvements post fine-tuning, suggesting potential pathways for improvement.</abstract>
      <url hash="08ba889b">2024.findings-acl.370</url>
      <bibkey>jin-etal-2024-mm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.370</doi>
    </paper>
    <paper id="371">
      <title>Instances Need More Care: Rewriting Prompts for Instances with <fixed-case>LLM</fixed-case>s in the Loop Yields Better Zero-Shot Performance</title>
      <author><first>Saurabh</first><last>Srivastava</last><affiliation>George Mason University</affiliation></author>
      <author><first>Chengyue</first><last>Huang</last></author>
      <author><first>Weiguo</first><last>Fan</last><affiliation>University of Iowa</affiliation></author>
      <author><first>Ziyu</first><last>Yao</last><affiliation>George Mason University</affiliation></author>
      <pages>6211-6232</pages>
      <abstract>Large language models (LLMs) have revolutionized zero-shot task performance, mitigating the need for task-specific annotations while enhancing task generalizability. Despite its advancements, current methods using trigger phrases such as “Let’s think step by step” remain limited. This study introduces PRomPTed, an approach that optimizes the zero-shot prompts for individual task instances following an innovative manner of “LLMs in the loop”.Our comprehensive evaluation across 13 datasets and 10 task types based on GPT-4 reveals that PRomPTed significantly outperforms both the naive zero-shot approaches and a strong baseline (i.e., “Output Refinement”) which refines the task output instead of the input prompt. Our experimental results also confirmed the generalization of this advantage to the relatively weaker GPT-3.5. Even more intriguingly, we found that leveraging GPT-3.5 to rewrite prompts for the stronger GPT-4 not only matches but occasionally exceeds the efficacy of using GPT-4 as the prompt rewriter. Our research thus presents a huge value in not only enhancing zero-shot LLM performance but also potentially enabling supervising LLMs with their weaker counterparts, a capability attracting much interest recently. Finally, our additional experiments confirm the generalization of the advantages to open-source LLMs such as Mistral 7B and Mixtral 8x7B.</abstract>
      <url hash="d36c2d87">2024.findings-acl.371</url>
      <bibkey>srivastava-etal-2024-instances</bibkey>
      <doi>10.18653/v1/2024.findings-acl.371</doi>
    </paper>
    <paper id="372">
      <title>Benchmarking Retrieval-Augmented Generation for Medicine</title>
      <author><first>Guangzhi</first><last>Xiong</last></author>
      <author><first>Qiao</first><last>Jin</last><affiliation>National Institutes of Health</affiliation></author>
      <author><first>Zhiyong</first><last>Lu</last><affiliation>National Institutes of Health</affiliation></author>
      <author><first>Aidong</first><last>Zhang</last></author>
      <pages>6233-6251</pages>
      <abstract>While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the “lost-in-the-middle” effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.</abstract>
      <url hash="02496f74">2024.findings-acl.372</url>
      <bibkey>xiong-etal-2024-benchmarking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.372</doi>
    </paper>
    <paper id="373">
      <title><fixed-case>C</fixed-case>hat<fixed-case>M</fixed-case>usician: Understanding and Generating Music Intrinsically with <fixed-case>LLM</fixed-case></title>
      <author><first>Ruibin</first><last>Yuan</last></author>
      <author><first>Hanfeng</first><last>Lin</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yi</first><last>Wang</last></author>
      <author><first>Zeyue</first><last>Tian</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shangda</first><last>Wu</last></author>
      <author><first>Tianhao</first><last>Shen</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Yuhang</first><last>Wu</last></author>
      <author><first>Cong</first><last>Liu</last></author>
      <author><first>Ziya</first><last>Zhou</last></author>
      <author><first>Liumeng</first><last>Xue</last></author>
      <author><first>Ziyang</first><last>Ma</last></author>
      <author><first>Qin</first><last>Liu</last></author>
      <author><first>Tianyu</first><last>Zheng</last></author>
      <author><first>Yizhi</first><last>Li</last><affiliation>University of Manchester and University of Sheffield</affiliation></author>
      <author><first>Yinghao</first><last>Ma</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Yiming</first><last>Liang</last></author>
      <author><first>Xiaowei</first><last>Chi</last></author>
      <author><first>Ruibo</first><last>Liu</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Zili</first><last>Wang</last></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Qifeng</first><last>Liu</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Tao</first><last>Jiang</last></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo and Google</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Emmanouil</first><last>Benetos</last></author>
      <author><first>Gus</first><last>Xia</last><affiliation>New York University</affiliation></author>
      <author><first>Roger</first><last>Dannenberg</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Wei</first><last>Xue</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Shiyin</first><last>Kang</last></author>
      <author><first>Yike</first><last>Guo</last></author>
      <pages>6252-6271</pages>
      <abstract>While LLMs demonstrate impressive capabilities in musical knowledge, we find that music reasoning is still an unsolved task.We introduce ChatMusician, an open-source large language model (LLM) that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language.ChatMusician can understand and generate music with a pure text tokenizer without external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score.ChatMusician is capable of composing well-structured, full-length music, condition on texts, chords, melodies, motifs, musical forms, etc.On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 by a noticeable margin. We show that ChatMusician preserves or even surpasses the original LLaMA2 7B’s language abilities by evaluating on MMLU benchmark.Our work reveals that LLMs can be an excellent compressor for music, which can be seen as humanity’s creative language, but there remains significant territory to be conquered.We release our 5B token music-language corpora MusicPiles, the collected MusicTheoryBench, code, model and demo.</abstract>
      <url hash="eb44cccb">2024.findings-acl.373</url>
      <bibkey>yuan-etal-2024-chatmusician</bibkey>
      <doi>10.18653/v1/2024.findings-acl.373</doi>
    </paper>
    <paper id="374">
      <title>Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop <fixed-case>QA</fixed-case> Dataset and Pseudo-Instruction Tuning</title>
      <author><first>Qingyu</first><last>Tan</last><affiliation>national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Hwee Tou</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <pages>6272-6286</pages>
      <abstract>Knowledge in the real world is being updated constantly. However, it is costly to frequently update large language models (LLMs). Therefore, it is crucial for LLMs to understand the concept of temporal knowledge. However, prior works on temporal question answering (TQA) did not emphasize multi-answer and multi-hop types of temporal reasoning. In this paper, we propose a complex temporal question-answering dataset Complex-TR that focuses on multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel data augmentation strategy to improve the complex temporal reasoning capability and robustness of LLMs. We conducted experiments on multiple temporal QA datasets. Experimental results show that our method is able to improve LLMs’ performance on temporal QA benchmarks by significant margins.</abstract>
      <url hash="0edfcb5b">2024.findings-acl.374</url>
      <bibkey>tan-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-acl.374</doi>
    </paper>
    <paper id="375">
      <title>Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements</title>
      <author><first>Anton</first><last>Voronov</last><affiliation>Higher School of Economics</affiliation></author>
      <author><first>Lena</first><last>Wolf</last><affiliation>Yandex</affiliation></author>
      <author><first>Max</first><last>Ryabinin</last><affiliation>Together AI</affiliation></author>
      <pages>6287-6310</pages>
      <abstract>Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples.The <tex-math>\textit{prompt template}</tex-math>, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning.In this work, we conduct a comprehensive study of the template format’s influence on the in-context learning performance.We evaluate the impact of the prompt template across 21 models (from 770M to 70B parameters) and 4 standard classification datasets. We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level.More importantly, the best templates do not transfer between different setups and even between models of the same family.Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works.As a first step towards mitigating this issue, we propose <tex-math>\textit{Template Ensembles}</tex-math> that aggregate model predictions across several templates.This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates.</abstract>
      <url hash="f79c65fd">2024.findings-acl.375</url>
      <bibkey>voronov-etal-2024-mind</bibkey>
      <doi>10.18653/v1/2024.findings-acl.375</doi>
    </paper>
    <paper id="376">
      <title>Knowledge Graph-Enhanced Large Language Models via Path Selection</title>
      <author><first>Haochen</first><last>Liu</last></author>
      <author><first>Song</first><last>Wang</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Yaochen</first><last>Zhu</last></author>
      <author><first>Yushun</first><last>Dong</last></author>
      <author><first>Jundong</first><last>Li</last><affiliation>University of Virginia</affiliation></author>
      <pages>6311-6321</pages>
      <abstract>Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.</abstract>
      <url hash="01f2f6d9">2024.findings-acl.376</url>
      <bibkey>liu-etal-2024-knowledge-graph</bibkey>
      <doi>10.18653/v1/2024.findings-acl.376</doi>
    </paper>
    <paper id="377">
      <title><fixed-case>OTTAWA</fixed-case>: Optimal <fixed-case>T</fixed-case>ranspor<fixed-case>T</fixed-case> Adaptive Word Aligner for Hallucination and Omission Translation Errors Detection</title>
      <author><first>Chenyang</first><last>Huang</last></author>
      <author><first>Abbas</first><last>Ghaddar</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Ivan</first><last>Kobyzev</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Osmar</first><last>Zaiane</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Boxing</first><last>Chen</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>6322-6334</pages>
      <abstract>Recently, there has been considerable attention on detecting hallucinations and omissions in Machine Translation (MT) systems. The two dominant approaches to tackle this task involve analyzing the MT system’s internal states or relying on the output of external tools, such as sentence similarity or MT quality estimators. In this work, we introduce OTTAWA, a novel Optimal Transport (OT)-based word aligner specifically designed to enhance the detection of hallucinations and omissions in MT systems. Our approach explicitly models the missing alignments by introducing a “null” vector, for which we propose a novel one-side constrained OT setting to allow an adaptive null alignment. Our approach yields competitive results compared to state-of-the-art methods across 18 language pairs on the HalOmi benchmark. In addition, it shows promising features, such as the ability to distinguish between both error types and perform word-level detection without accessing the MT system’s internal states.</abstract>
      <url hash="64a1738c">2024.findings-acl.377</url>
      <bibkey>huang-etal-2024-ottawa</bibkey>
      <doi>10.18653/v1/2024.findings-acl.377</doi>
    </paper>
    <paper id="378">
      <title><fixed-case>ONSEP</fixed-case>: A Novel Online Neural-Symbolic Framework for Event Prediction Based on Large Language Model</title>
      <author><first>Xuanqing</first><last>Yu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wangtao</first><last>Sun</last></author>
      <author><first>Jingwei</first><last>Li</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Chengbao</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jie</first><last>Tan</last></author>
      <pages>6335-6350</pages>
      <abstract>In the realm of event prediction, temporal knowledge graph forecasting (TKGF) stands as a pivotal technique. Previous approaches face the challenges of not utilizing experience during testing and relying on a single short-term history, which limits adaptation to evolving data. In this paper, we introduce the Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by integrating dynamic causal rule mining (DCRM) and dual history augmented generation (DHAG). DCRM dynamically constructs causal rules from real-time data, allowing for swift adaptation to new causal relationships. In parallel, DHAG merges short-term and long-term historical contexts, leveraging a bi-branch approach to enrich event prediction. Our framework demonstrates notable performance enhancements across diverse datasets, with significant Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language models (LLMs) for event prediction without necessitating extensive retraining. The ONSEP framework not only advances the field of TKGF but also underscores the potential of neural-symbolic approaches in adapting to dynamic data environments.</abstract>
      <url hash="03b0de24">2024.findings-acl.378</url>
      <bibkey>yu-etal-2024-onsep</bibkey>
      <doi>10.18653/v1/2024.findings-acl.378</doi>
    </paper>
    <paper id="379">
      <title>Speech-based Slot Filling using Large Language Models</title>
      <author><first>Guangzhi</first><last>Sun</last></author>
      <author><first>Shutong</first><last>Feng</last><affiliation>Heinrich-Heine Universität Düsseldorf</affiliation></author>
      <author><first>Dongcheng</first><last>Jiang</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Tsinghua University and University College London</affiliation></author>
      <author><first>Milica</first><last>Gasic</last><affiliation>Heinrich Heine University Duesseldorf</affiliation></author>
      <author><first>Phil</first><last>Woodland</last><affiliation>University of Cambridge</affiliation></author>
      <pages>6351-6362</pages>
      <abstract>Recently, advancements in large language models (LLMs) have shown an unprecedented ability across various language tasks. This paper investigates the potential application of LLMs to slot filling with noisy ASR transcriptions, via both in-context learning and task-specific fine-tuning. Dedicated prompt designs and noise-robust LoRA fine-tuning are proposed to improve the robustness of LLMs for slot filling with noisy ASR transcriptions. Moreover, a linearised knowledge injection (LKI) scheme is also proposed to integrate dynamic external knowledge into LLMs. Experiments were performed on SLURP to quantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B, LLaMA-2-13B and Vicuna-13B (v1.1 and v1.5) with different ASR error rates. The use of the noise-robust fine-tuning together with LKI for Vicuna-13B-v1.5 achieved 6.7% and 17.6% absolute SLU-F1 improvements compared to a fully fine-tuned Flan-T5-XL model on the limited data setup and the zero-shot setup respectively.</abstract>
      <url hash="38a3f5bb">2024.findings-acl.379</url>
      <bibkey>sun-etal-2024-speech</bibkey>
      <doi>10.18653/v1/2024.findings-acl.379</doi>
    </paper>
    <paper id="380">
      <title>Too Big to Fail: Larger Language Models are Disproportionately Resilient to Induction of Dementia-Related Linguistic Anomalies</title>
      <author><first>Changye</first><last>Li</last><affiliation>University of Washington</affiliation></author>
      <author><first>Zhecheng</first><last>Sheng</last></author>
      <author><first>Trevor</first><last>Cohen</last><affiliation>University of Washington</affiliation></author>
      <author><first>Serguei</first><last>Pakhomov</last><affiliation>University of Minnesota - Twin Cities</affiliation></author>
      <pages>6363-6377</pages>
      <abstract>As artificial neural networks grow in complexity, understanding their inner workings becomes increasingly challenging, which is particularly important in healthcare applications. The intrinsic evaluation metrics of autoregressive neural language models (NLMs), perplexity (PPL), can reflect how “surprised” an NLM model is at novel input. PPL has been widely used to understand the behavior of NLMs. Previous findings show that changes in PPL when masking attention layers in pre-trained transformer-based NLMs reflect linguistic anomalies associated with Alzheimer’s disease dementia. Building upon this, we explore a novel bidirectional attention head ablation method that exhibits properties attributed to the concepts of cognitive and brain reserve in human brain studies, which postulate that people with more neurons in the brain and more efficient processing are more resilient to neurodegeneration. Our results show that larger GPT-2 models require a disproportionately larger share of attention heads to be masked/ablated to display degradation of similar magnitude to masking in smaller models. These results suggest that the attention mechanism in transformer models may present an analogue to the notions of cognitive and brain reserve and could potentially be used to model certain aspects of the progression of neurodegenerative disorders and aging.</abstract>
      <url hash="6686045a">2024.findings-acl.380</url>
      <bibkey>li-etal-2024-big</bibkey>
      <doi>10.18653/v1/2024.findings-acl.380</doi>
    </paper>
    <paper id="381">
      <title><fixed-case>H</fixed-case>e<fixed-case>S</fixed-case>um: a Novel Dataset for Abstractive Text Summarization in <fixed-case>H</fixed-case>ebrew</title>
      <author><first>Tzuf</first><last>Paz-Argaman</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Itai</first><last>Mondshine</last></author>
      <author><first>Asaf</first><last>Achi Mordechai</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Reut</first><last>Tsarfaty</last><affiliation>Google and Bar-Ilan University, Technion</affiliation></author>
      <pages>6378-6388</pages>
      <abstract>While large language models (LLMs) excel in various natural language tasks in English, their performance in low-resource languages like Hebrew, especially for generative tasks such as abstractive summarization, remains unclear. The high morphological richness in Hebrew adds further challenges due to the ambiguity in sentence comprehension and the complexities in meaning construction.In this paper, we address this evaluation and resource gap by introducing HeSum, a novel benchmark dataset specifically designed for Hebrew abstractive text summarization. HeSum consists of 10,000 article-summary pairs sourced from Hebrew news websites written by professionals. Linguistic analysis confirms HeSum’s high abstractness and unique morphological challenges. We show that HeSum presents distinct difficulties even for state-of-the-art LLMs, establishing it as a valuable testbed for advancing generative language technology in Hebrew, and MRLs generative challenges in general.</abstract>
      <url hash="5ea495c3">2024.findings-acl.381</url>
      <bibkey>paz-argaman-etal-2024-hesum</bibkey>
      <doi>10.18653/v1/2024.findings-acl.381</doi>
    </paper>
    <paper id="382">
      <title><fixed-case>TRAM</fixed-case>: Benchmarking Temporal Reasoning for Large Language Models</title>
      <author><first>Yuqing</first><last>Wang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Yun</first><last>Zhao</last><affiliation>Meta Platforms, Inc</affiliation></author>
      <pages>6389-6415</pages>
      <abstract>Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the TeR capabilities of large language models (LLMs). We evaluate popular LLMs like GPT-4 and Llama2 in zero-shot and few-shot scenarios, and establish baselines with BERT-based and domain-specific models. Our findings indicate that the best-performing model lags significantly behind human performance. It is our aspiration that TRAM will spur further progress in enhancing the TeR capabilities of LLMs.</abstract>
      <url hash="6288e070">2024.findings-acl.382</url>
      <bibkey>wang-zhao-2024-tram</bibkey>
      <doi>10.18653/v1/2024.findings-acl.382</doi>
    </paper>
    <paper id="383">
      <title>Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models</title>
      <author><first>Alfonso</first><last>Amayuelas</last></author>
      <author><first>Kyle</first><last>Wong</last></author>
      <author><first>Liangming</first><last>Pan</last></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo and Google</affiliation></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <pages>6416-6432</pages>
      <abstract>This paper investigates the capabilities of Large Language Models (LLMs) in understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the models’ improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information.</abstract>
      <url hash="ab26efde">2024.findings-acl.383</url>
      <bibkey>amayuelas-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.findings-acl.383</doi>
    </paper>
    <paper id="384">
      <title>Exploring Defeasibility in Causal Reasoning</title>
      <author><first>Shaobo</first><last>Cui</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Lazar</first><last>Milikic</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Yiyang</first><last>Feng</last></author>
      <author><first>Mete</first><last>Ismayilzada</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Debjit</first><last>Paul</last><affiliation>EPFL - EPF Lausanne</affiliation></author>
      <author><first>Antoine</first><last>Bosselut</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Boi</first><last>Faltings</last></author>
      <pages>6433-6452</pages>
      <abstract>Defeasibility in causal reasoning implies that the causal relationship between cause and effect can be strengthened or weakened. Namely, the causal strength between cause and effect should increase or decrease with the incorporation of strengthening arguments (supporters) or weakening arguments (defeaters), respectively. However, existing works ignore defeasibility in causal reasoning and fail to evaluate existing causal strength metrics in defeasible settings. In this work, we present <tex-math>\delta</tex-math>-CAUSAL, the first benchmark dataset for studying defeasibility in causal reasoning. <tex-math>\delta</tex-math>-CAUSAL includes around 11K events spanning ten domains, featuring defeasible causality pairs, namely, cause-effect pairs accompanied by supporters and defeaters. We further show that current causal strength metrics fail to reflect the change of causal strength with the incorporation of supporters or defeaters in <tex-math>\delta</tex-math>-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation with Attention Rating), a metric that measures causal strength based on token-level causal relationships. CESAR achieves a significant 69.7% relative improvement over existing metrics, increasing from 47.2% to 80.1% in capturing the causal strength change brought by supporters and defeaters. We further demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and 10.7 points behind humans in generating supporters and defeaters, emphasizing the challenge posed by <tex-math>\delta</tex-math>-CAUSAL.</abstract>
      <url hash="0694bb00">2024.findings-acl.384</url>
      <bibkey>cui-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.384</doi>
    </paper>
    <paper id="385">
      <title>Better Synthetic Data by Retrieving and Transforming Existing Datasets</title>
      <author><first>Saumya</first><last>Gandhi</last></author>
      <author><first>Ritu</first><last>Gala</last></author>
      <author><first>Vijay</first><last>Viswanathan</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Tongshuang</first><last>Wu</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Graham</first><last>Neubig</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>6453-6466</pages>
      <abstract>Despite recent advances in large language models, building dependable and deployable NLP models typically requires abundant, high-quality training data. However, task-specific data is not available for many use cases, and manually curating task-specific data is labor-intensive. Recent work has studied prompt-driven synthetic data generation using large language models, but these generated datasets tend to lack complexity and diversity. To address these limitations, we introduce a method, _DataTune_, to make better use of existing, publicly available datasets to improve automatic dataset generation. DataTune performs _dataset transformation_, enabling the repurposing of publicly available datasets into a format that is directly aligned with the specific requirements of target tasks. On a diverse set of language-based tasks from the BIG-Bench benchmark, we find that finetuning language models via DataTune improves over a few-shot prompting baseline by 49% and improves over existing methods that use synthetic or retrieved training data by 34%. We find that dataset transformation significantly increases the diversity and difficulty of generated data on many tasks. We release a Python package and open-source repository to make this method accessible to the community (URL will be added upon acceptance).</abstract>
      <url hash="b7034ab2">2024.findings-acl.385</url>
      <bibkey>gandhi-etal-2024-better</bibkey>
      <doi>10.18653/v1/2024.findings-acl.385</doi>
    </paper>
    <paper id="386">
      <title>Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models</title>
      <author><first>Yanzheng</first><last>Xiang</last></author>
      <author><first>Hanqi</first><last>Yan</last></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>6467-6481</pages>
      <abstract>In-context learning has become a popular paradigm in natural language processing. However, its performance can be significantly influenced by the order of in-context demonstration examples. In this paper, we found that causal language models (CausalLMs) are more sensitive to this order compared to prefix language models (PrefixLMs). We attribute this phenomenon to the auto-regressive attention masks within CausalLMs, which restrict each token from accessing information from subsequent tokens. This results in different receptive fields for samples at different positions, thereby leading to representation disparities across positions. To tackle this challenge, we introduce an unsupervised fine-tuning method, termed the Information-Augmented and Consistency-Enhanced approach. This approach utilizes contrastive learning to align representations of in-context examples across different positions and introduces a consistency loss to ensure similar representations for inputs with different permutations. This enhances the model’s predictive consistency across permutations. Experimental results on five benchmarks suggest that our proposed method can reduce the sensitivity of CausalLMs to the order of in-context examples and exhibit robust generalizability, particularly when demonstrations are sourced from a candidate pool different from that used in the training phase, or when the number of in-context examples differs from what is used during training.</abstract>
      <url hash="58f993fc">2024.findings-acl.386</url>
      <bibkey>xiang-etal-2024-addressing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.386</doi>
    </paper>
    <paper id="387">
      <title>Perspective Taking through Generating Responses to Conflict Situations</title>
      <author><first>Joan</first><last>Plepi</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <author><first>Charles</first><last>Welch</last><affiliation>McMaster University</affiliation></author>
      <author><first>Lucie</first><last>Flek</last><affiliation>Rheinische Friedrich-Wilhelms Universität Bonn</affiliation></author>
      <pages>6482-6497</pages>
      <abstract>Although language model performance across diverse tasks continues to improve, these models still struggle to understand and explain the beliefs of other people. This skill requires perspective-taking, the process of conceptualizing the point of view of another person. Perspective taking becomes challenging when the text reflects more personal and potentially more controversial beliefs.We explore this task through natural language generation of responses to conflict situations. We evaluate novel modifications to recent architectures for conditioning generation on an individual’s comments and self-disclosure statements. Our work extends the Social-Chem-101 corpus, using 95k judgements written by 6k authors from English Reddit data, for each of whom we obtained 20-500 self-disclosure statements. Our evaluation methodology borrows ideas from both personalized generation and theory of mind literature. Our proposed perspective-taking models outperform recent work, especially the twin encoder model conditioned on self-disclosures with high similarity to the conflict situation.</abstract>
      <url hash="8c68da49">2024.findings-acl.387</url>
      <bibkey>plepi-etal-2024-perspective</bibkey>
      <doi>10.18653/v1/2024.findings-acl.387</doi>
    </paper>
    <paper id="388">
      <title><fixed-case>LLM</fixed-case>2<fixed-case>LLM</fixed-case>: Boosting <fixed-case>LLM</fixed-case>s with Novel Iterative Data Enhancement</title>
      <author><first>Nicholas</first><last>Lee</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Thanakul</first><last>Wattanawong</last></author>
      <author><first>Sehoon</first><last>Kim</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Karttikeya</first><last>Mangalam</last></author>
      <author><first>Sheng</first><last>Shen</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Gopala</first><last>Anumanchipalli</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Michael</first><last>Mahoney</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Kurt</first><last>Keutzer</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Amir</first><last>Gholami</last><affiliation>University of California Berkeley</affiliation></author>
      <pages>6498-6526</pages>
      <abstract>Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM. Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines. LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular fine-tuning in the low-data regime using a Llama-2-7B student model. Our code is available at https://github.com/SqueezeAILab/LLM2LLM.</abstract>
      <url hash="64183a4b">2024.findings-acl.388</url>
      <bibkey>lee-etal-2024-llm2llm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.388</doi>
    </paper>
    <paper id="389">
      <title>The Power of Summary-Source Alignments</title>
      <author><first>Ori</first><last>Ernst</last><affiliation>Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <author><first>Ori</first><last>Shapira</last><affiliation>OriginAI</affiliation></author>
      <author><first>Aviv</first><last>Slobodkin</last></author>
      <author><first>Sharon</first><last>Adar</last><affiliation>Amazon</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Jacob</first><last>Goldberger</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Ran</first><last>Levy</last><affiliation>Amazon</affiliation></author>
      <author><first>Ido</first><last>Dagan</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>6527-6548</pages>
      <abstract>Multi-document summarization (MDS) is a challenging task, often decomposed to subtasks of salience and redundancy detection, followed by text generation.In this context, alignment of corresponding sentences between a reference summary and its source documents has been leveraged to generate training data for some of the component tasks. Yet, this enabling alignment step has usually been applied heuristically on the sentence level on a limited number of subtasks.In this paper, we propose extending the summary-source alignment framework by (1) applying it at the more fine-grained proposition span level, (2) annotating alignment manually in a multi-document setup, and (3) revealing the great potential of summary-source alignments to yield several datasets for at least six different tasks. Specifically, for each of the tasks, we release a manually annotated test set that was derived automatically from the alignment annotation. We also release development and train sets in the same way, but from automatically derived alignments.Using the datasets, each task is demonstrated with baseline models and corresponding evaluation metrics to spur future research on this broad challenge.</abstract>
      <url hash="9571cec9">2024.findings-acl.389</url>
      <bibkey>ernst-etal-2024-power</bibkey>
      <doi>10.18653/v1/2024.findings-acl.389</doi>
    </paper>
    <paper id="390">
      <title>An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models</title>
      <author><first>Gantavya</first><last>Bhatt</last></author>
      <author><first>Yifang</first><last>Chen</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Arnav</first><last>Das</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jifan</first><last>Zhang</last><affiliation>University of Wisconsin, Madison</affiliation></author>
      <author><first>Sang</first><last>Truong</last></author>
      <author><first>Stephen</first><last>Mussmann</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yinglun</first><last>Zhu</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Jeff</first><last>Bilmes</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Simon</first><last>Du</last><affiliation>University of Washington</affiliation></author>
      <author><first>Kevin</first><last>Jamieson</last><affiliation>University of Washington</affiliation></author>
      <author><first>Jordan</first><last>Ash</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Robert</first><last>Nowak</last><affiliation>University of Wisconsin, Madison, Toyota Technological Institute at Chicago, Rice University, University of Wisconsin-Madison and University of Wisconsin - Madison</affiliation></author>
      <pages>6549-6560</pages>
      <abstract>Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimental design techniques and find that these methods consistently yield significant gains in label efficiency with little computational overhead. On generative tasks, to reach the same generalization performance, our methods save 50% of the annotation cost compared to random sampling.</abstract>
      <url hash="cdb105f0">2024.findings-acl.390</url>
      <bibkey>bhatt-etal-2024-experimental</bibkey>
      <doi>10.18653/v1/2024.findings-acl.390</doi>
    </paper>
    <paper id="391">
      <title>Learning Multimodal Contrast with Cross-modal Memory and Reinforced Contrast Recognition</title>
      <author><first>Yuanhe</first><last>Tian</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>6561-6573</pages>
      <abstract>In many practical scenarios, contents from different modalities are not semantically aligned; for instance, visual and textual information may conflict with each other, resulting in non-compositional expression effects such as irony or humor. Effective modeling and smooth integration of multimodal information are crucial for achieving good understanding of the contrast across modalities. Being focusing on image-text matching, most current studies face challenges in identifying such contrast, leading to limitations in exploring the extended semantics when images and texts do not match. In this paper, we propose an LLM-based approach for learning multimodal contrast following the encoding-decoding paradigm, enhanced by a memory module with reinforced contrast recognition, and use a series of tasks that have the nature of multimodal contrast to verify our approach. The memory module learns the integration between visual and textual features with trainable memory vectors and the reinforced contrast recognition uses self-rejection sampling to optimize the memory to further enhance learning multimodal contrast. The resulted information, accompanied with visual and text features, is finally fed into the LLM to predict corresponding labels. We experiment our approach on four English and Chinese benchmark datasets, where it outperforms strong baselines and state-of-the-art studies.</abstract>
      <url hash="94a15b17">2024.findings-acl.391</url>
      <bibkey>tian-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.391</doi>
    </paper>
    <paper id="392">
      <title>Text Simplification via Adaptive Teaching</title>
      <author><first>Seyed Ali</first><last>Bahrainian</last></author>
      <author><first>Jonathan</first><last>Dou</last></author>
      <author><first>Carsten</first><last>Eickhoff</last><affiliation>Eberhard-Karls-Universität Tübingen</affiliation></author>
      <pages>6574-6584</pages>
      <abstract>Text simplification is the process of rewriting a piece of text using simpler vocabulary and grammatical structure in order to make the text more accessible and understandable for a larger audience. In this paper, we introduce a new text simplification model based on the notion of adaptive teaching using a teacher network and a text generation network. We name this new model Simplification via Adaptive Teaching (SAT). Our proposed model sets a new state-of-the-art performance in terms of standard simplification metrics such as SARI and D-SARI with a significant improvement over the previous state of the art on the D-Wikipedia dataset and the Wiki-Doc benchmark dataset. Moreover, we conduct a human evaluation in terms of text simplicity, correctness, and fluency to substantiate SAT’s performance.</abstract>
      <url hash="a4a888e1">2024.findings-acl.392</url>
      <bibkey>bahrainian-etal-2024-text</bibkey>
      <doi>10.18653/v1/2024.findings-acl.392</doi>
    </paper>
    <paper id="393">
      <title>A multi-level multi-label text classification dataset of 19th century Ottoman and <fixed-case>R</fixed-case>ussian literary and critical texts</title>
      <author><first>Gokcen</first><last>Gokceoglu</last><affiliation>METU</affiliation></author>
      <author><first>Devrim</first><last>Çavuşoğlu</last></author>
      <author><first>Emre</first><last>Akbas</last><affiliation>Middle East Technical University</affiliation></author>
      <author><first>Özen</first><last>Dolcerocca</last><affiliation>University of Bologna</affiliation></author>
      <pages>6585-6596</pages>
      <abstract>This paper introduces a multi-level, multi-label text classification dataset comprising over 3000 documents. The dataset features literary and critical texts from 19th-century Ottoman Turkish and Russian. It is the first study to apply large language models (LLMs) to this dataset, sourced from prominent literary periodicals of the era. The texts have been meticulously organized and labeled. This was done according to a taxonomic framework that takes into account both their structural and semantic attributes. Articles are categorized and tagged with bibliometric metadata by human experts. We present baseline classification results using a classical bag-of-words (BoW) naive Bayes model and three modern LLMs: multilingual BERT, Falcon, and Llama-v2. We found that in certain cases, Bag of Words (BoW) outperforms Large Language Models (LLMs), emphasizing the need for additional research, especially in low-resource language settings. This dataset is expected to be a valuable resource for researchers in natural language processing and machine learning, especially for historical and low-resource languages. The dataset is publicly available.</abstract>
      <url hash="612cf830">2024.findings-acl.393</url>
      <bibkey>gokceoglu-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.393</doi>
    </paper>
    <paper id="394">
      <title>It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance</title>
      <author><first>Laura</first><last>Cabello</last><affiliation>Copenhagen University</affiliation></author>
      <author><first>Uchenna</first><last>Akujuobi</last><affiliation>Sony Research</affiliation></author>
      <pages>6597-6610</pages>
      <abstract>Aspect-Based Sentiment Analysis (ABSA) involves extracting opinions from textual data about specific entities and their corresponding aspects through various complementary subtasks. Several prior research has focused on developing ad hoc designs of varying complexities for these subtasks. In this paper, we build upon the instruction tuned model proposed by Scaria et al. (2023), who present an instruction-based model with task descriptions followed by in-context examples on ABSA subtasks. We propose PFInstruct, an extension to this instruction learning paradigm by appending an NLP-related task prefix to the task description. This simple approach leads to improved performance across all tested SemEval subtasks, surpassing previous state-of-the-art (SOTA) on the ATE subtask (Rest14) by +3.28 F1-score, and on the AOOE subtask by an average of +5.43 F1-score across SemEval datasets. Furthermore, we explore the impact of the prefix-enhanced prompt quality on the ABSA subtasks and find that even a noisy prefix enhances model performance compared to the baseline. Our method also achieves competitive results on a biomedical domain dataset (ERSA).</abstract>
      <url hash="8bc04f84">2024.findings-acl.394</url>
      <bibkey>cabello-akujuobi-2024-simple</bibkey>
      <doi>10.18653/v1/2024.findings-acl.394</doi>
    </paper>
    <paper id="395">
      <title>Whose Emotions and Moral Sentiments do Language Models Reflect?</title>
      <author><first>Zihao</first><last>He</last></author>
      <author><first>Siyi</first><last>Guo</last></author>
      <author><first>Ashwin</first><last>Rao</last></author>
      <author><first>Kristina</first><last>Lerman</last><affiliation>University of Southern California and USC Information Sciences Institute</affiliation></author>
      <pages>6611-6631</pages>
      <abstract>Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs’ emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages written by two ideological groups, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectives, the misalignment and liberal tendencies of the model persist, suggesting a systemic bias within LMs.</abstract>
      <url hash="5f824e2a">2024.findings-acl.395</url>
      <bibkey>he-etal-2024-whose</bibkey>
      <doi>10.18653/v1/2024.findings-acl.395</doi>
    </paper>
    <paper id="396">
      <title><fixed-case>LLM</fixed-case> can Achieve Self-Regulation via Hyperparameter Aware Generation</title>
      <author><first>Siyin</first><last>Wang</last></author>
      <author><first>Shimin</first><last>Li</last></author>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Qinyuan</first><last>Cheng</last></author>
      <author><first>Jiasheng</first><last>Ye</last></author>
      <author><first>Junjie</first><last>Ye</last></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>6632-6646</pages>
      <abstract>In the realm of Large Language Models (LLMs), users commonly employ diverse decoding strategies and adjust hyperparameters to control the generated text. However, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves? The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands. However, this process is typically cumbersome, and the decoding hyperparameters may not always be optimal for each sample. To address the aforementioned challenges, we propose a novel text generation paradigm termed Hyperparameter Aware Generation (HAG). By leveraging hyperparameter-aware instruction tuning, the LLM autonomously determines the optimal decoding strategy and configs based on the input samples, enabling self-regulation. Our approach eliminates the need for extensive manual tuning, offering a more autonomous, self-regulate model behavior. Experimental results spanning six datasets across reasoning, creativity, translation, and mathematics tasks demonstrate that hyperparameter-aware instruction tuning empowers the LLMs to self-regulate the decoding strategy and hyperparameter. HAG extends the current paradigm in the text generation process, highlighting the feasibility of endowing the LLMs with self-regulate decoding strategies.</abstract>
      <url hash="52344fca">2024.findings-acl.396</url>
      <bibkey>wang-etal-2024-llm-achieve</bibkey>
      <doi>10.18653/v1/2024.findings-acl.396</doi>
    </paper>
    <paper id="397">
      <title>Forward-Backward Reasoning in Large Language Models for Mathematical Verification</title>
      <author><first>Weisen</first><last>Jiang</last></author>
      <author><first>Han</first><last>Shi</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Longhui</first><last>Yu</last></author>
      <author><first>Zhengying</first><last>Liu</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Yu</first><last>Zhang</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Zhenguo</first><last>Li</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology and Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>James</first><last>Kwok</last><affiliation>Department of Computer Science and Engineering, The Hong Kong University of Science and Technology</affiliation></author>
      <pages>6647-6661</pages>
      <abstract>Self-Consistency samples diverse reasoning chains with answers and chooses the final answer by majority voting. It is based on forward reasoning and cannot further improve performance by sampling more reasoning chains when saturated. To further boost performance, we introduce backward reasoning to verify candidate answers. Specifically, for mathematical tasks, we mask a number in the question and ask the LLM to answer a backward question created by a simple template, i.e., to predict the masked number when a candidate answer is provided. Instead of using forward or backward reasoning alone, we propose **FOBAR** to combine **FO**rward and **BA**ckward **R**easoning for verification. Extensive experiments on six standard mathematical data sets and three LLMs show that FOBAR achieves state-of-the-art performance. In particular, FOBAR outperforms Self-Consistency, which uses forward reasoning alone, demonstrating that combining forward and backward reasoning is more accurate in verification. In addition, FOBAR achieves higher accuracy than existing verification methods, showing the effectiveness of the simple template used in backward reasoning and the proposed combination.</abstract>
      <url hash="6b0a436e">2024.findings-acl.397</url>
      <bibkey>jiang-etal-2024-forward</bibkey>
      <doi>10.18653/v1/2024.findings-acl.397</doi>
    </paper>
    <paper id="398">
      <title>Towards Uncertainty-Aware Language Agent</title>
      <author><first>Jiuzhou</first><last>Han</last></author>
      <author><first>Wray</first><last>Buntine</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University and University of Cambridge</affiliation></author>
      <pages>6662-6685</pages>
      <abstract>While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrate that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscore the unreliability of verbalised confidence of LLMs as a proxy for uncertainty.</abstract>
      <url hash="04e9dc74">2024.findings-acl.398</url>
      <bibkey>han-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-acl.398</doi>
    </paper>
    <paper id="399">
      <title>Detection and Positive Reconstruction of Cognitive Distortion Sentences: <fixed-case>M</fixed-case>andarin Dataset and Evaluation</title>
      <author><first>Shuya</first><last>Lin</last></author>
      <author><first>Yuxiong</first><last>Wang</last></author>
      <author><first>Jonathan</first><last>Dong</last></author>
      <author><first>Shiguang</first><last>Ni</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>6686-6701</pages>
      <abstract>This research introduces a Positive Reconstruction Framework based on positive psychology theory. Overcoming negative thoughts can be challenging, our objective is to address and reframe them through a positive reinterpretation. To tackle this challenge, a two-fold approach is necessary: identifying cognitive distortions and suggesting a positively reframed alternative while preserving the original thought’s meaning. Recent studies have investigated the application of Natural Language Processing (NLP) models in English for each stage of this process. In this study, we emphasize the theoretical foundation for the Positive Reconstruction Framework, grounded in broaden-and-build theory. We provide a shared corpus containing 4001 instances for detecting cognitive distortions and 1900 instances for positive reconstruction in Mandarin. Leveraging recent NLP techniques, including transfer learning, fine-tuning pretrained networks, and prompt engineering, we demonstrate the effectiveness of automated tools for both tasks. In summary, our study contributes to multilingual positive reconstruction, highlighting the effectiveness of NLP in cognitive distortion detection and positive reconstruction.</abstract>
      <url hash="2e75a4a0">2024.findings-acl.399</url>
      <bibkey>lin-etal-2024-detection</bibkey>
      <doi>10.18653/v1/2024.findings-acl.399</doi>
    </paper>
    <paper id="400">
      <title><fixed-case>P</fixed-case>i<fixed-case>V</fixed-case>e: Prompting with Iterative Verification Improving Graph-based Generative Capability of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jiuzhou</first><last>Han</last></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Wray</first><last>Buntine</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Ehsan</first><last>Shareghi</last><affiliation>Monash University and University of Cambridge</affiliation></author>
      <pages>6702-6718</pages>
      <abstract>Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatically generated parallel text-graph datasets.</abstract>
      <url hash="1937ce6f">2024.findings-acl.400</url>
      <bibkey>han-etal-2024-pive</bibkey>
      <doi>10.18653/v1/2024.findings-acl.400</doi>
    </paper>
    <paper id="401">
      <title>Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models</title>
      <author><first>Yifu</first><last>Gao</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Linbo</first><last>Qiao</last></author>
      <author><first>Zhigang</first><last>Kan</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Zhihua</first><last>Wen</last><affiliation>National University of Defence Technology</affiliation></author>
      <author><first>Yongquan</first><last>He</last></author>
      <author><first>Dongsheng</first><last>Li</last></author>
      <pages>6719-6734</pages>
      <abstract>Temporal knowledge graph question answering (TKGQA) poses a significant challenge task, due to the temporal constraints hidden in questions and the answers sought from dynamic structured knowledge. Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative temporal knowledge graph question answering framework, GenTKGQA, which guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation. First, we exploit LLM’s intrinsic knowledge to mine temporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space in both temporal and structural dimensions. Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and the text representations of the LLM in a non-shallow way, which helps the open-source LLM deeply understand the temporal order and structural dependencies among the retrieved facts through instruction tuning. Experimental results on two widely used datasets demonstrate the superiority of our model.</abstract>
      <url hash="d829aa22">2024.findings-acl.401</url>
      <bibkey>gao-etal-2024-two</bibkey>
      <doi>10.18653/v1/2024.findings-acl.401</doi>
    </paper>
    <paper id="402">
      <title><fixed-case>VISREAS</fixed-case>: Complex Visual Reasoning with Unanswerable Questions</title>
      <author><first>Syeda Nahida</first><last>Akter</last></author>
      <author><first>Sangwu</first><last>Lee</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Yingshan</first><last>Chang</last></author>
      <author><first>Yonatan</first><last>Bisk</last><affiliation>Meta and Carnegie Mellon University</affiliation></author>
      <author><first>Eric</first><last>Nyberg</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>6735-6752</pages>
      <abstract>Verifying a question’s validity before answering is crucial in real-world applications, where users may provide imperfect instructions. In this scenario, an ideal model should address the discrepancies in the query and convey them to the users rather than generating the best possible answer. Addressing this requirement, we introduce a new compositional visual question-answering dataset, VisReas, that consists of answerable and unanswerable visual queries formulated by traversing and perturbing commonalities and differences among objects, attributes, and relations. VisReas contains 2.07M semantically diverse queries generated automatically using Visual Genome scene graphs. The unique feature of this task, validating question answerability with respect to an image before answering, and the poor performance of state-of-the-art models inspired the design of a new modular baseline, Logic2Vision that reasons by producing and executing pseudocode without any external modules to generate the answer. Logic2Vision outperforms generative models in VisReas (+4.82% over LLaVA-1.5; +12.23% over InstructBLIP) and achieves a significant gain in performance against the classification models.</abstract>
      <url hash="d8a73e27">2024.findings-acl.402</url>
      <bibkey>akter-etal-2024-visreas</bibkey>
      <doi>10.18653/v1/2024.findings-acl.402</doi>
    </paper>
    <paper id="403">
      <title>A Unified Generative Framework for Bilingual Euphemism Detection and Identification</title>
      <author><first>Yuxue</first><last>Hu</last></author>
      <author><first>Junsong</first><last>Li</last></author>
      <author><first>Tongguan</first><last>Wang</last></author>
      <author><first>Dongyu</first><last>Su</last></author>
      <author><first>Guixin</first><last>Su</last></author>
      <author><first>Ying</first><last>Sha</last></author>
      <pages>6753-6766</pages>
      <abstract>Various euphemisms are emerging in social networks, attracting widespread attention from the natural language processing community. However, existing euphemism datasets are only domain-specific or language-specific. In addition, existing approaches to the study of euphemisms are one-sided. Either only the euphemism detection task or only the euphemism identification task is accomplished, lacking a unified framework. To this end, we construct a large-scale Bilingual Multi-category dataset of Euphemisms named BME, which covers a total of 12 categories for two languages, English and Chinese. Then, we first propose a unified generative model to Jointly conduct the tasks of bilingual Euphemism Detection and Identification named JointEDI. By comparing with LLMs and human evaluation, we demonstrate the effectiveness of the proposed JointEDI and the feasibility of unifying euphemism detection and euphemism identification tasks. Moreover, the BME dataset also provides a new reference standard for euphemism detection and euphemism identification.</abstract>
      <url hash="70c35733">2024.findings-acl.403</url>
      <bibkey>hu-etal-2024-unified</bibkey>
      <doi>10.18653/v1/2024.findings-acl.403</doi>
    </paper>
    <paper id="404">
      <title><fixed-case>S</fixed-case>tyle<fixed-case>D</fixed-case>ubber: Towards Multi-Scale Style Learning for Movie Dubbing</title>
      <author><first>Gaoxiang</first><last>Cong</last></author>
      <author><first>Yuankai</first><last>Qi</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Liang</first><last>Li</last></author>
      <author><first>Amin</first><last>Beheshti</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Zhedong</first><last>Zhang</last></author>
      <author><first>Anton</first><last>Hengel</last><affiliation>University of Adelaide</affiliation></author>
      <author><first>Ming-Hsuan</first><last>Yang</last><affiliation>Google and University of California at Merced</affiliation></author>
      <author><first>Chenggang</first><last>Yan</last><affiliation>Hangzhou Dianzi University, Tsinghua University</affiliation></author>
      <author><first>Qingming</first><last>Huang</last><affiliation>University of Chinese Academy of Sciences</affiliation></author>
      <pages>6767-6779</pages>
      <abstract>Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate embeddings to improve the overall style expression; And (3) a phoneme-guided lip aligner to maintain lip sync. Extensive experiments on two of the primary benchmarks, V2C and Grid, demonstrate the favorable performance of the proposed method as compared to the current state-of-the-art. The code will be made available at https://github.com/GalaxyCong/StyleDubber.</abstract>
      <url hash="c6423d3b">2024.findings-acl.404</url>
      <bibkey>cong-etal-2024-styledubber</bibkey>
      <doi>10.18653/v1/2024.findings-acl.404</doi>
    </paper>
    <paper id="405">
      <title><fixed-case>ETAS</fixed-case>: Zero-Shot Transformer Architecture Search via Network Trainability and Expressivity</title>
      <author><first>Jiechao</first><last>Yang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yong</first><last>Liu</last><affiliation>Renmin University of China and Institute of information engineering, CAS</affiliation></author>
      <pages>6780-6795</pages>
      <abstract>Transformer Architecture Search (TAS) methods aim to automate searching for the optimal Transformer architecture configurations for a given task. However, they are impeded by the prohibitive cost of evaluating Transformer architectures. Recently, several Zero-Shot TAS methods have been proposed to mitigate this problem by utilizing zero-cost proxies to evaluate Transformer architectures without training. Unfortunately, they are limited to specific computer vision or natural language processing tasks. Nonetheless, most of them are developed based on empirical observations and lack theoretical guarantees. To solve this problem, we develop a new zero-cost proxy called NTSR that combines two theoretically-inspired indicators to measure the trainability and expressivity of Transformer networks separately. We then integrate it into an effective regularized evolution framework called ETAS to demonstrate its efficacy on various tasks. The results show that our proposed NTSR proxy can consistently achieve a higher correlation with the true performance of Transformer networks on both computer vision and natural language processing tasks. Further, it can significantly accelerate the search process for finding the best-performing Transformer architecture configurations.</abstract>
      <url hash="7d199ee9">2024.findings-acl.405</url>
      <bibkey>yang-liu-2024-etas</bibkey>
      <doi>10.18653/v1/2024.findings-acl.405</doi>
    </paper>
    <paper id="406">
      <title>Reasoning Like a Doctor: Improving Medical Dialogue Systems via Diagnostic Reasoning Process Alignment</title>
      <author><first>Kaishuai</first><last>Xu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Yi</first><last>Cheng</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Wenjun</first><last>Hou</last></author>
      <author><first>Qiaoyu</first><last>Tan</last><affiliation>New York University Shanghai</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <pages>6796-6814</pages>
      <abstract>Medical dialogue systems have attracted significant attention for their potential to act as medical assistants. Enabling these medical systems to emulate clinicians’ diagnostic reasoning process has been the long-standing research focus. Previous studies rudimentarily realized the simulation of clinicians’ diagnostic process by fine-tuning language models on high-quality dialogue datasets. Nonetheless, they overly focus on the outcomes of the clinician’s reasoning process while ignoring their internal thought processes and alignment with clinician preferences. Our work aims to build a medical dialogue system that aligns with clinicians’ diagnostic reasoning processes. We propose a novel framework, Emulation, designed to generate an appropriate response that relies on abductive and deductive diagnostic reasoning analyses and aligns with clinician preferences through thought process modeling. Experimental results on two datasets confirm the efficacy of Emulation. Crucially, our framework furnishes clear explanations for the generated responses, enhancing its transparency in medical consultations.</abstract>
      <url hash="6f411f71">2024.findings-acl.406</url>
      <bibkey>xu-etal-2024-reasoning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.406</doi>
    </paper>
    <paper id="407">
      <title><fixed-case>C</fixed-case>oncept<fixed-case>M</fixed-case>ath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models</title>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Jie</first><last>Liu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xingyuan</first><last>Bu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Jiaheng</first><last>Liu</last></author>
      <author><first>Zhanhui</first><last>Zhou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yuanxing</first><last>Zhang</last></author>
      <author><first>Chenchen</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>ZhiqiBai</first><last>ZhiqiBai</last></author>
      <author><first>Haibin</first><last>Chen</last></author>
      <author><first>Tiezheng</first><last>Ge</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Wanli</first><last>Ouyang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Wenbo</first><last>Su</last></author>
      <author><first>Bo</first><last>Zheng</last><affiliation>Alibaba Group</affiliation></author>
      <pages>6815-6839</pages>
      <abstract>This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systemically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we then evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models. Code is available at https://github.com/conceptmath/conceptmath.</abstract>
      <url hash="0e27fa08">2024.findings-acl.407</url>
      <bibkey>wu-etal-2024-conceptmath</bibkey>
      <doi>10.18653/v1/2024.findings-acl.407</doi>
    </paper>
    <paper id="408">
      <title><fixed-case>REI</fixed-case>nstruct: Building Instruction Data from Unlabeled Corpus</title>
      <author><first>Shu</first><last>Chen</last></author>
      <author><first>Xinyan</first><last>Guan</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>6840-6856</pages>
      <abstract>Manually annotating instruction data for large language models is difficult, costly, and hard to scale. Meanwhile, current automatic annotation methods typically rely on distilling synthetic data from proprietary LLMs, which not only limits the upper bound of the quality of the instruction data but also raises potential copyright issues. In this paper, we propose REInstruct, a simple and scalable method to automatically build instruction data from an unlabeled corpus without heavy reliance on proprietary LLMs and human annotation.Specifically, REInstruct first selects a subset of unlabeled texts that potentially contain well-structured helpful and insightful content and then generates instructions for these texts. To generate accurate and relevant responses for effective and robust training, REInstruct further proposes a rewriting-based approach to improve the quality of the generated instruction data. By training Llama-7b on a combination of 3k seed data and 32k synthetic data from REInstruct, fine-tuned model achieves a 65.41% win rate on AlpacaEval leaderboard against text-davinci-003, outperforming other open-source, non-distilled instruction data construction methods. The code is publicly available at <url>https://github.com/cs32963/REInstruct</url>.</abstract>
      <url hash="925018c3">2024.findings-acl.408</url>
      <bibkey>chen-etal-2024-reinstruct</bibkey>
      <doi>10.18653/v1/2024.findings-acl.408</doi>
    </paper>
    <paper id="409">
      <title>Learning to Maximize Mutual Information for Chain-of-Thought Distillation</title>
      <author><first>Xin</first><last>Chen</last><affiliation>Intel Corp</affiliation></author>
      <author><first>Hanxian</first><last>Huang</last></author>
      <author><first>Yanjun</first><last>Gao</last><affiliation>University of Colorado Anschutz Medical Campus</affiliation></author>
      <author><first>Yi</first><last>Wang</last></author>
      <author><first>Jishen</first><last>Zhao</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Ke</first><last>Ding</last><affiliation>Intel</affiliation></author>
      <pages>6857-6868</pages>
      <abstract>Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve this optimization problem using a learning-based method. Our experimental results across four datasets demonstrate that our method outperforms the state-of-the-art DSS. Our findings offer insightful guidance for future research on language model distillation as well as applications involving CoT. Codes are available at https://github.com/xinchen9/cot_distillation_ACL2024.</abstract>
      <url hash="83540d23">2024.findings-acl.409</url>
      <bibkey>chen-etal-2024-learning-maximize</bibkey>
      <doi>10.18653/v1/2024.findings-acl.409</doi>
    </paper>
    <paper id="410">
      <title><fixed-case>PEMT</fixed-case>: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning</title>
      <author><first>Zhisheng</first><last>Lin</last></author>
      <author><first>Han</first><last>Fu</last></author>
      <author><first>Chenghao</first><last>Liu</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Zhuo</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jianling</first><last>Sun</last></author>
      <pages>6869-6883</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task description prompt vectors. To fully exploit the task-specific knowledge, we also propose the Task Sparsity Loss to improve the sparsity of the gated unit. We conduct experiments on a broad range of tasks over 17 datasets. The experimental results demonstrate our PEMT yields stable improvements over full fine-tuning, and state-of-the-art PEFT and knowledge transferring methods on various tasks. The results highlight the effectiveness of our method which is capable of sufficiently exploiting the knowledge and correlation features across multiple tasks.</abstract>
      <url hash="37cc2bc7">2024.findings-acl.410</url>
      <bibkey>lin-etal-2024-pemt</bibkey>
      <doi>10.18653/v1/2024.findings-acl.410</doi>
    </paper>
    <paper id="411">
      <title><fixed-case>M</fixed-case>ath<fixed-case>B</fixed-case>ench: Evaluating the Theory and Application Proficiency of <fixed-case>LLM</fixed-case>s with a Hierarchical Mathematics Benchmark</title>
      <author><first>Hongwei</first><last>Liu</last></author>
      <author><first>Zilong</first><last>Zheng</last></author>
      <author><first>Yuxuan</first><last>Qiao</last></author>
      <author><first>Haodong</first><last>Duan</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Zhiwei</first><last>Fei</last><affiliation>Fudan University, Harbin Institute of Technology, Dalian University of Technology, Shanghai Jiaotong University, Shandong University, Peking University, Zhejiang University, University of Science and Technology of China, Hunan University, Beijing Institute of Technology, University of the Chinese Academy of Sciences, Southeast University, Sichuan University, Monash University, Malaysia Campus, Tianjin University, Beijing University of Aeronautics and Astronautics, Wuhan University of Technology, Yale University, Technische Universität München, Wuhan University, nanjing university, Tsinghua University and Wuhan University</affiliation></author>
      <author><first>Fengzhe</first><last>Zhou</last></author>
      <author><first>Wenwei</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <pages>6884-6915</pages>
      <abstract>Recent advancements in large language models (LLMs) have showcased significant improvements in mathematics. However, traditional math benchmarks like GSM8k offer a unidimensional perspective, which fall short in providing a holistic assessment of the LLMs’ math capabilities. To address this gap, we introduce MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large language models. MathBench spans a wide range of mathematical disciplines, offering a detailed evaluation of both theoretical understanding and practical problem-solving skills. The benchmark progresses through five distinct stages, from basic arithmetic to college mathematics, and is structured to evaluate models at various depths of knowledge. Each stage includes theoretical questions and application problems, allowing us to measure a model’s mathematical proficiency and its ability to apply concepts in practical scenarios. MathBench aims to enhance the evaluation of LLMs’ mathematical abilities, providing a nuanced view of their knowledge understanding levels and problem solving skills in a bilingual context.</abstract>
      <url hash="d661569d">2024.findings-acl.411</url>
      <bibkey>liu-etal-2024-mathbench</bibkey>
      <doi>10.18653/v1/2024.findings-acl.411</doi>
    </paper>
    <paper id="412">
      <title>Identifying Semantic Induction Heads to Understand In-Context Learning</title>
      <author><first>Jie</first><last>Ren</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Hang</first><last>Yan</last><affiliation>AI lab</affiliation></author>
      <author><first>Dongrui</first><last>Liu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Quanshi</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6916-6932</pages>
      <abstract>Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to subject tokens, they recall object tokens and increase the output logits of those object tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs.</abstract>
      <url hash="a714a79c">2024.findings-acl.412</url>
      <bibkey>ren-etal-2024-identifying</bibkey>
      <doi>10.18653/v1/2024.findings-acl.412</doi>
    </paper>
    <paper id="413">
      <title><fixed-case>C</fixed-case>hinese Spelling Corrector Is Just a Language Learner</title>
      <author><first>Lai</first><last>Jiang</last></author>
      <author><first>Hongqiu</first><last>Wu</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>6933-6943</pages>
      <abstract>This paper emphasizes Chinese spelling correction by means of self-supervised learning, which means there are no annotated errors within the training data. Our intuition is that humans are naturally good correctors with exposure to error-free sentences, which contrasts with current unsupervised methods that strongly rely on the usage of confusion sets to produce parallel sentences. In this paper, we demonstrate that learning a spelling correction model is identical to learning a language model from error-free data alone, with decoding it in a greater search space. We propose <i>Denoising Decoding Correction (D2C)</i>, which selectively imposes noise upon the source sentence to determine the underlying correct characters. Our method is largely inspired by the ability of language models to perform correction, including both BERT-based models and large language models (LLMs). We show that the self-supervised learning manner generally outperforms the confusion set in specific domains because it bypasses the need to introduce error characters to the training data which can impair the error patterns not included in the introduced error characters.</abstract>
      <url hash="fe3c4fc8">2024.findings-acl.413</url>
      <bibkey>jiang-etal-2024-chinese</bibkey>
      <doi>10.18653/v1/2024.findings-acl.413</doi>
    </paper>
    <paper id="414">
      <title>Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models</title>
      <author><first>Junfei</first><last>Wu</last></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ding</first><last>Wang</last></author>
      <author><first>Jinghao</first><last>Zhang</last></author>
      <author><first>Shu</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Liang</first><last>Wang</last><affiliation>CASIA</affiliation></author>
      <author><first>Tieniu</first><last>Tan</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <pages>6944-6962</pages>
      <abstract>Object hallucination has been an Achilles’ heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely <tex-math>\textbf{LogicCheckGPT}</tex-math>. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.</abstract>
      <url hash="77ed38d8">2024.findings-acl.414</url>
      <bibkey>wu-etal-2024-logical</bibkey>
      <doi>10.18653/v1/2024.findings-acl.414</doi>
    </paper>
    <paper id="415">
      <title><fixed-case>R</fixed-case>etrieval<fixed-case>QA</fixed-case>: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering</title>
      <author><first>Zihan</first><last>Zhang</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <pages>6963-6975</pages>
      <abstract>Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training.</abstract>
      <url hash="07c115dd">2024.findings-acl.415</url>
      <bibkey>zhang-etal-2024-retrievalqa</bibkey>
      <doi>10.18653/v1/2024.findings-acl.415</doi>
    </paper>
    <paper id="416">
      <title><fixed-case>LL</fixed-case>a<fixed-case>ST</fixed-case>: Improved End-to-end Speech Translation System Leveraged by Large Language Models</title>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Songyang</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Qibing</first><last>Bai</last></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Satoshi</first><last>Nakamura</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>6976-6987</pages>
      <abstract>We introduces ***LLaST***, a framework for building high-performance Large Language model based Speech-to-text Translation systems. We address the limitations of end-to-end speech translation (E2E ST) models by exploring model architecture design and optimization techniques tailored for LLMs. Our approach includes LLM-based speech translation architecture design, ASR-augmented training, multilingual data augmentation, and dual-LoRA optimization. Our approach demonstrates superior performance on the CoVoST-2 benchmark and showcases exceptional scaling capabilities powered by LLMs.We believe this effective method will serve as a strong baseline for speech translation and provide insights for futureimprovements of the LLM-based speech translation framework.</abstract>
      <url hash="a79cfd6a">2024.findings-acl.416</url>
      <bibkey>chen-etal-2024-llast</bibkey>
      <doi>10.18653/v1/2024.findings-acl.416</doi>
    </paper>
    <paper id="417">
      <title>Plan, Generate and Complicate: Improving Low-resource Dialogue State Tracking via Easy-to-Difficult Zero-shot Data Augmentation</title>
      <author><first>Ming</first><last>Gu</last></author>
      <author><first>Yan</first><last>Yang</last><affiliation>East China Normal University</affiliation></author>
      <pages>6988-7005</pages>
      <abstract>Data augmentation methods have been a promising direction to improve the performance of small models for low-resource dialogue state tracking. However, traditional methods rely on pre-defined user goals and neglect the importance of data complexity in this task. In this paper, we propose EDZ-DA, an Easy-to-Difficult Zero-shot Data Augmentation framework for low-resource dialogue state tracking that utilizes large language models to automatically catch the relationships of different domains and then generate the dialogue data. We also complicate the dialogues based on the domain relation to enhance the model’s capability for co-reference slot tracking. Furthermore, we permute slot values to mitigate the influence of output orders and the problem of incomplete value generation. Experimental results illustrate the superiority of our proposed method compared to previous strong data augmentation baselines on MultiWOZ.</abstract>
      <url hash="fb8eb601">2024.findings-acl.417</url>
      <bibkey>gu-yang-2024-plan</bibkey>
      <doi>10.18653/v1/2024.findings-acl.417</doi>
    </paper>
    <paper id="418">
      <title><fixed-case>DM</fixed-case>o<fixed-case>ERM</fixed-case>: Recipes of Mixture-of-Experts for Effective Reward Modeling</title>
      <author><first>Shanghaoran</first><last>Quan</last></author>
      <pages>7006-7028</pages>
      <abstract>The performance of the reward model (RM) is a critical factor in improving the effectiveness of the large language model (LLM) during alignment fine-tuning. There remain two challenges in RM training: 1) training the same RM using various categories of data may cause its generalization performance to suffer from multi-task disturbance, and 2) the human annotation consistency rate is generally only 60% to 75%, causing training data to contain a lot of noise. To tackle these two challenges, we introduced the idea of Mixture-of-Experts (MoE) into the field of RM for the first time. We propose the Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After classifying an input into task categories, we route it to the corresponding inner layer task-specific model. The inner layer MoE is a dense model. We decompose the specific task into multiple capability dimensions and individually fine-tune a LoRA expert on each one. Their outputs are then synthesized by an MLP to compute the final rewards. To minimize costs, we call a public LLM API to obtain the capability preference labels. The validation on manually labeled datasets confirms that our model attains superior consistency with human preference and outstrips advanced generative approaches. Meanwhile, through BoN sampling and RL experiments, we demonstrate that our model outperforms state-of-the-art ensemble methods of RM and mitigates the overoptimization problem. Our code is available at: https://github.com/quanshr/DMoERM.</abstract>
      <url hash="56c4198a">2024.findings-acl.418</url>
      <bibkey>quan-2024-dmoerm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.418</doi>
    </paper>
    <paper id="419">
      <title><fixed-case>LEIA</fixed-case>: Facilitating Cross-lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation</title>
      <author><first>Ikuya</first><last>Yamada</last><affiliation>RIKEN and Studio Ousia</affiliation></author>
      <author><first>Ryokan</first><last>Ri</last><affiliation>SB Intuitions</affiliation></author>
      <pages>7029-7039</pages>
      <abstract>Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages.</abstract>
      <url hash="b9bbd7d7">2024.findings-acl.419</url>
      <bibkey>yamada-ri-2024-leia</bibkey>
      <doi>10.18653/v1/2024.findings-acl.419</doi>
    </paper>
    <paper id="420">
      <title>Comments as Natural Logic Pivots: Improve Code Generation via Comment Perspective</title>
      <author><first>Yijie</first><last>Chen</last></author>
      <author><first>Yijin</first><last>Liu</last><affiliation>Wechat AI</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>7040-7051</pages>
      <abstract>Code generation aims to understand the problem description and generate corresponding code snippets, where existing works generally decompose such complex tasks into intermediate steps by prompting strategies, such as Chain-of-Thought and its variants. While these studies have achieved some success, their effectiveness is highly dependent on the capabilities of advanced Large Language Models (LLMs) such as GPT-4, particularly in terms of API calls, which significantly limits their practical applicability. Consequently, how to enhance the code generation capabilities of small and medium-scale code LLMs without significantly increasing training costs is an appealing challenge. In this paper, we suggest that code comments are the natural logic pivot between natural language and code language and propose using comments to boost the code generation ability of code LLMs. Concretely, we propose MANGO (comMents As Natural loGic pivOts), including a comment contrastive training strategy and a corresponding logical comment decoding strategy. Experiments are performed on HumanEval and MBPP, utilizing StarCoder and WizardCoder as backbone models, and encompassing model parameter sizes between 3B and 7B. The results indicate that MANGO significantly improves the code pass rate based on the strong baselines. Meanwhile, the robustness of the logical comment decoding strategy is notably higher than the Chain-of-thoughts prompting.</abstract>
      <url hash="110c4dec">2024.findings-acl.420</url>
      <bibkey>chen-etal-2024-comments</bibkey>
      <doi>10.18653/v1/2024.findings-acl.420</doi>
    </paper>
    <paper id="421">
      <title>Cocktail: A Comprehensive Information Retrieval Benchmark with <fixed-case>LLM</fixed-case>-Generated Documents Integration</title>
      <author><first>Sunhao</first><last>Dai</last></author>
      <author><first>Weihao</first><last>Liu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Yuqi</first><last>Zhou</last></author>
      <author><first>Liang</first><last>Pang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Rongju</first><last>Ruan</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Gang</first><last>Wang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zhenhua</first><last>Dong</last></author>
      <author><first>Jun</first><last>Xu</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ji-Rong</first><last>Wen</last><affiliation>Renmin University of China</affiliation></author>
      <pages>7052-7074</pages>
      <abstract>The proliferation of Large Language Models (LLMs) has led to an influx of AI-generated content (AIGC) on the internet, transforming the corpus of Information Retrieval (IR) systems from solely human-written to a coexistence with LLM-generated content. The impact of this surge in AIGC on IR systems remains an open question, with the primary challenge being the lack of a dedicated benchmark for researchers. In this paper, we introduce Cocktail, a comprehensive benchmark tailored for evaluating IR models in this mixed-sourced data landscape of the LLM era. Cocktail consists of 16 diverse datasets with mixed human-written and LLM-generated corpora across various text retrieval tasks and domains. Additionally, to avoid the potential bias from previously included dataset information in LLMs, we also introduce an up-to-date dataset, named NQ-UTD, with queries derived from recent events. Through conducting over 1,000 experiments to assess state-of-the-art retrieval models against the benchmarked datasets in Cocktail, we uncover a clear trade-off between ranking performance and source bias in neural retrieval models, highlighting the necessity for a balanced approach in designing future IR systems. We hope Cocktail can serve as a foundational resource for IR research in the LLM era, with all data and code publicly available at https://github.com/KID-22/Cocktail.</abstract>
      <url hash="d21d8bb2">2024.findings-acl.421</url>
      <bibkey>dai-etal-2024-cocktail</bibkey>
      <doi>10.18653/v1/2024.findings-acl.421</doi>
    </paper>
    <paper id="422">
      <title>Continual Dialogue State Tracking via Reason-of-Select Distillation</title>
      <author><first>Yujie</first><last>Feng</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Bo</first><last>Liu</last></author>
      <author><first>Xiaoyu</first><last>Dong</last></author>
      <author><first>Zexin</first><last>Lu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Li-Ming</first><last>Zhan</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Xiao-Ming</first><last>Wu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Albert</first><last>Lam</last><affiliation>University of Hong Kong and Fano Labs</affiliation></author>
      <pages>7075-7087</pages>
      <abstract>An ideal dialogue system requires continuous skill acquisition and adaptation to new tasks while retaining prior knowledge. Dialogue State Tracking (DST), vital in these systems, often involves learning new services, confronting catastrophic forgetting and a critical capability loss termed the “Value Selection Quandary”. To address these challenges, we introduce the Reason-of-Select (RoS) distillation method by enhancing smaller models with a novel “meta-reasoning” capability. Meta-reasoning, employing an enhanced multi-domain perspective, combines fragments of meta-knowledge from domain-specific dialogues during continual learning, transcending traditional single-perspective reasoning. This domain bootstrapping process enhances the model’s ability to dissect intricate dialogues from multiple possible values, and its domain-agnostic property aligns data distribution across different domains, effectively mitigating forgetting. Besides, two novel improvements, “multi-value resolution” strategy and Semantic Contrastive Reasoning Selection method, significantly enhance RoS by generating DST-specific selection chains and mitigating hallucinations in teachers’ reasoning, ensuring effective and reliable knowledge transfer. Extensive experiments validate the exceptional performance and robust generalization capabilities of our method.</abstract>
      <url hash="559348f4">2024.findings-acl.422</url>
      <bibkey>feng-etal-2024-continual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.422</doi>
    </paper>
    <paper id="423">
      <title>Spotting <fixed-case>AI</fixed-case>’s Touch: Identifying <fixed-case>LLM</fixed-case>-Paraphrased Spans in Text</title>
      <author><first>Yafu</first><last>Li</last><affiliation>Westlake University</affiliation></author>
      <author><first>Zhilin</first><last>Wang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Wei</first><last>Bi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>7088-7107</pages>
      <abstract>AI-generated text detection has attracted increasing attention as powerful language models approach human-level generation. Limited work is devoted to detecting (partially) AI-paraphrased texts. However, AI paraphrasing is commonly employed in various application scenarios for text refinement and diversity. To this end, we propose a novel detection framework, paraphrased text span detection (PTD), aiming to identify paraphrased text spans within a text. Different from text-level detection, PTD takes in the full text and assigns each of the sentences with a score indicating the paraphrasing degree. We construct a dedicated dataset, PASTED, for paraphrased text span detection. Both in-distribution and out-of-distribution results demonstrate the effectiveness of PTD models in identifying AI-paraphrased text spans. Statistical and model analysis explains the crucial role of the surrounding context of the paraphrased text spans. Extensive experiments show that PTD models can generalize to versatile paraphrasing prompts as well as multiple paraphrased text spans.</abstract>
      <url hash="78d1ed94">2024.findings-acl.423</url>
      <bibkey>li-etal-2024-spotting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.423</doi>
    </paper>
    <paper id="424">
      <title><fixed-case>S</fixed-case>o<fixed-case>FA</fixed-case>: Shielded On-the-fly Alignment via Priority Rule Following</title>
      <author><first>Xinyu</first><last>Lu</last></author>
      <author><first>Bowen</first><last>Yu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Haiyang</first><last>Yu</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Yongbin</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>7108-7136</pages>
      <abstract>The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately.</abstract>
      <url hash="edb6d6d8">2024.findings-acl.424</url>
      <bibkey>lu-etal-2024-sofa</bibkey>
      <doi>10.18653/v1/2024.findings-acl.424</doi>
    </paper>
    <paper id="425">
      <title>Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition</title>
      <author><first>Ariel</first><last>Goldstein</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>Gabriel</first><last>Stanovsky</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>7137-7143</pages>
      <abstract>Recent advances in LLMs have sparked a debate on whether they understand text. In this position paper, we argue that opponents in this debate hold different definitions for understanding, and particularly differ in their view on the role of consciousness. To substantiate this claim, we propose a thought experiment involving an open-source chatbot Z which excels on every possible benchmark, seemingly without subjective experience. We ask whether Z is capable of understanding, and show that different schools of thought within seminal AI research seem to answer this question differently, uncovering their terminological disagreement. Moving forward, we propose two distinct working definitions for understanding which explicitly acknowledge the question of consciousness, and draw connections with a rich literature in philosophy, psychology and neuroscience.</abstract>
      <url hash="6fbf1d6c">2024.findings-acl.425</url>
      <bibkey>goldstein-stanovsky-2024-zombies</bibkey>
      <doi>10.18653/v1/2024.findings-acl.425</doi>
    </paper>
    <paper id="426">
      <title>Modeling Emotional Trajectories in Written Stories Utilizing Transformers and Weakly-Supervised Learning</title>
      <author><first>Lukas</first><last>Christ</last><affiliation>University of Augsburg, Universität Augsburg</affiliation></author>
      <author><first>Shahin</first><last>Amiriparian</last><affiliation>Technical University of Munich</affiliation></author>
      <author><first>Manuel</first><last>Milling</last><affiliation>University of Augsburg</affiliation></author>
      <author><first>Ilhan</first><last>Aslan</last></author>
      <author><first>Björn</first><last>Schuller</last><affiliation>Technische Universität München and Imperial College London</affiliation></author>
      <pages>7144-7159</pages>
      <abstract>Telling stories is an integral part of human communication which can evoke emotions and influence the affective states of the audience. Automatically modeling emotional trajectories in stories has thus attracted considerable scholarly interest. However, as most existing works have been limited to unsupervised dictionary-based approaches, there is no benchmark for this task. We address this gap by introducing continuous valence and arousal labels for an existing dataset of children’s stories originally annotated with discrete emotion categories. We collect additional annotations for this data and map the categorical labels to the continuous valence and arousal space. For predicting the thus obtained emotionality signals, we fine-tune a DeBERTa model and improve upon this baseline via a weakly supervised learning approach. The best configuration achieves a Concordance Correlation Coefficient (CCC) of .8221 for valence and .7125 for arousal on the test set, demonstrating the efficacy of our proposed approach. A detailed analysis shows the extent to which the results vary depending on factors such as the author, the individual story, or the section within the story. In addition, we uncover the weaknesses of our approach by investigating examples that prove to be difficult to predict.</abstract>
      <url hash="0bdea724">2024.findings-acl.426</url>
      <bibkey>christ-etal-2024-modeling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.426</doi>
    </paper>
    <paper id="427">
      <title><fixed-case>RAP</fixed-case>: Efficient Text-Video Retrieval with Sparse-and-Correlated Adapter</title>
      <author><first>Meng</first><last>Cao</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Haoran</first><last>Tang</last></author>
      <author><first>Jinfa</first><last>Huang</last></author>
      <author><first>Peng</first><last>Jin</last></author>
      <author><first>Can</first><last>Zhang</last><affiliation>Tencent MediaLab</affiliation></author>
      <author><first>Ruyang</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Long</first><last>Chen</last><affiliation>The Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <author><first>Li</first><last>Yuan</last><affiliation>Peking University</affiliation></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University Shenzhen Graduate School</affiliation></author>
      <pages>7160-7174</pages>
      <abstract>Text-Video Retrieval (TVR) aims to align relevant video content with natural language queries. To date, most of the state-of-the-art TVR methods learn image-to-video transfer learning based on the large-scale pre-trained vision-language models (e.g., CLIP). However, fully fine-tuning these pre-trained models for TVR incurs prohibitively expensive computation cost. To this end, we propose to conduct efficient text-video Retrieval with a salient-and-correlated AdaPter (RAP), i.e., fine-tuning the pre-trained model with a few parameterized layers. To accommodate the text-video scenario, we equip our RAP with two indispensable characteristics including temporal sparsity and correlation. Specifically, we propose a low-rank modulation module to refine the per-image features from frozen CLIP backbone, which accentuates silent frames within the video features while alleviating temporal redundancy. Besides, we introduce an asynchronous self-attention mechanism which firstly selects top responsive visual patch and augments the correlation modeling between them with learnable temporal and patch offsets. Extensive experiments on four TVR datasets demonstrate that our RAP achieves superior or comparable performance compared to the fully fine-tuned counterpart and other parameter-efficient finetuning methods.</abstract>
      <url hash="5e9dd31e">2024.findings-acl.427</url>
      <bibkey>cao-etal-2024-rap</bibkey>
      <doi>10.18653/v1/2024.findings-acl.427</doi>
    </paper>
    <paper id="428">
      <title>Benchmarking and Improving Long-Text Translation with Large Language Models</title>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Zefeng</first><last>Du</last></author>
      <author><first>Wenxiang</first><last>Jiao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Chenyang</first><last>Lyu</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Jianhui</first><last>Pang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Kaiqiang</first><last>Song</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Derek</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>7175-7187</pages>
      <abstract>Recent studies have illuminated the promising capabilities of large language models (LLMs) in handling long texts. However, their performance in machine translation (MT) of long documents remains underexplored. This paper aims to shed light on how LLMs navigate this complex task, offering a comprehensive evaluation of their capabilities and limitations in long-text MT. First, we collect and construct an instruction-based benchmark dataset, specifically designed for the finetuning and evaluation of LLMs, encompassing multilingual, multi-domain, and document-level parallel data. Second, we conduct a comprehensive comparison between MT and LLM models concerning document-level translation. Our analysis uncovers that LLMs exhibit shortcomings in long-text domains, and their performance diminishes as document size escalates. By exploiting various extrapolation strategies, we enhance the capacity of LLMs to translate longer texts. We release data, code, and models at https://github.com/longyuewangdcu/Document-MT-LLM.</abstract>
      <url hash="640d3022">2024.findings-acl.428</url>
      <bibkey>wang-etal-2024-benchmarking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.428</doi>
    </paper>
    <paper id="429">
      <title>Personalized Topic Selection Model for Topic-Grounded Dialogue</title>
      <author><first>Shixuan</first><last>Fan</last></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Xiaofei</first><last>Wen</last></author>
      <author><first>Xian-Ling</first><last>Mao</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Jixiong</first><last>Chen</last></author>
      <author><first>Dangyang</first><last>Chen</last><affiliation>Pingan Technology</affiliation></author>
      <pages>7188-7202</pages>
      <abstract>Recently, the topic-grounded dialogue (TGD) system has become increasingly popular as its powerful capability to actively guide users to accomplish specific tasks through topic-guided conversations. Most existing works utilize side information (e.g. topics or personas) in isolation to enhance the topic selection ability. However, due to disregarding the noise within these auxiliary information sources and their mutual influence, current models tend to predict user-uninteresting and contextually irrelevant topics. To build user-engaging and coherent dialogue agent, we propose a personalized topic selection model for topic-grounded dialogue, named PETD, which takes account of the interaction of side information to selectively aggregate such information for more accurately predicting subsequent topics. Specifically, we evaluate the correlation between global topics and personas and selectively incorporate the global topics aligned with user personas. Furthermore, we propose a contrastive learning based persona selector to filter relevant personas under the constraint of lacking pertinent persona annotations. Throughout the selection and generation, diverse relevant side information is considered. Extensive experiments demonstrate that our proposed method can generate engaging and diverse responses, outperforming state-of-the-art baselines across various evaluation metrics.</abstract>
      <url hash="4ebcb038">2024.findings-acl.429</url>
      <bibkey>fan-etal-2024-personalized</bibkey>
      <doi>10.18653/v1/2024.findings-acl.429</doi>
    </paper>
    <paper id="430">
      <title>Debiasing In-Context Learning by Instructing <fixed-case>LLM</fixed-case>s How to Follow Demonstrations</title>
      <author><first>Lvxue</first><last>Li</last></author>
      <author><first>Jiaqi</first><last>Chen</last></author>
      <author><first>Xinyu</first><last>Lu</last></author>
      <author><first>Yaojie</first><last>Lu</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Hongyu</first><last>Lin</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shuheng</first><last>Zhou</last><affiliation>Ant Group</affiliation></author>
      <author><first>Huijia</first><last>Zhu</last></author>
      <author><first>Weiqiang</first><last>Wang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Zhongyi</first><last>Liu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Xianpei</first><last>Han</last><affiliation>Institute of Software, CAS</affiliation></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>7203-7215</pages>
      <abstract>In-context learning(ICL) has gained considerable attention due to its data efficiency and task adaptability. Unfortunately, ICL suffers from the demonstration bias, i.e., its performance and robustness are severely affected by the selection and ordering of demonstrations. In this paper, we identify that such demonstration bias may primarily stem from the semantic ambiguity induced by demonstrations, i.e., a demonstration may indicate multiple input-to-label mappings and its mapping can be interpreted differently in different contexts by LLMs. Such semantic ambiguity disrupts task comprehension during ICL and results in performance fluctuations. To resolve the semantic ambiguity problem, this paper further proposes two de-biasing strategies to mitigate demonstration bias in in-context learning. Experiments on six datasets show that our methods can effectively alleviate demonstration bias and significantly improve task performance.</abstract>
      <url hash="02d9f8aa">2024.findings-acl.430</url>
      <bibkey>li-etal-2024-debiasing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.430</doi>
    </paper>
    <paper id="431">
      <title>Comparing Data Augmentation Methods for End-to-End Task-Oriented Dialog Systems</title>
      <author><first>Christos</first><last>Vlachos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Themos</first><last>Stafylakis</last><affiliation>Omilia</affiliation></author>
      <author><first>Ion</first><last>Androutsopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <pages>7216-7240</pages>
      <abstract>Creating effective and reliable task-oriented dialog systems (ToDSs) is challenging, not only because of the complex structure of these systems, but also due to the scarcity of training data, especially when several modules need to be trained separately, each one with its own input/output training examples. Data augmentation (DA), whereby synthetic training examples are added to the training data, has been successful in other NLP systems, but has not been explored as extensively in ToDSs. We empirically evaluate the effectiveness of DA methods in an end-to-end ToDS setting, where a single system is trained to handle all processing stages, from user inputs to system outputs. We experiment with two ToDSs (UBAR, GALAXY) on two datasets (MultiWOZ, KVRET). We consider three types of DA methods (word-level, sentence-level, dialog-level), comparing eight DA methods that have shown promising results in ToDSs and other NLP systems. We show that all DA methods considered are beneficial, and we highlight the best ones, also providing advice to practitioners. We also introduce a more challenging few-shot cross-domain ToDS setting, reaching similar conclusions.</abstract>
      <url hash="a931c062">2024.findings-acl.431</url>
      <bibkey>vlachos-etal-2024-comparing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.431</doi>
    </paper>
    <paper id="432">
      <title><fixed-case>MS</fixed-case>2<fixed-case>SL</fixed-case>: Multimodal Spoken Data-Driven Continuous Sign Language Production</title>
      <author><first>Jian</first><last>Ma</last></author>
      <author><first>Wenguan</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Feng</first><last>Zheng</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <pages>7241-7254</pages>
      <abstract>Sign language understanding has made significant strides; however, there is still no viable solution for generating sign sequences directlyfrom entire spoken content, e.g., text or speech. In this paper, we propose a unified framework for continuous sign language production, easing communication between sign and non-sign language users. In particular, a sequence diffusion model, utilizing embeddings extracted from text or speech, is crafted to generate sign predictions step by step. Moreover, by creating a joint embedding space for text, audio, and sign, we bind these modalities and leverage the semantic consistency among them to provide informative feedback for the model training. This embedding-consistency learning strategy minimizes the reliance on sign triplets and ensures continuous model refinement, evenwith a missing audio modality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our model achieves competitive performance in sign language production.</abstract>
      <url hash="d6dd5b2d">2024.findings-acl.432</url>
      <bibkey>ma-etal-2024-ms2sl</bibkey>
      <doi>10.18653/v1/2024.findings-acl.432</doi>
    </paper>
    <paper id="433">
      <title><fixed-case>BBA</fixed-case>: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models</title>
      <author><first>Xueliang</first><last>Zhao</last></author>
      <author><first>Xinting</first><last>Huang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Tingchen</first><last>Fu</last></author>
      <author><first>Qintong</first><last>Li</last></author>
      <author><first>Shansan</first><last>Gong</last></author>
      <author><first>Lemao</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Wei</first><last>Bi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Lingpeng</first><last>Kong</last><affiliation>Department of Computer Science, The University of Hong Kong</affiliation></author>
      <pages>7255-7279</pages>
      <abstract>Multimodal reasoning stands as a pivotal capability for large vision-language models (LVLMs). The integration with Domain-Specific Languages (DSL), offering precise visual representations, equips these models with the opportunity to execute more accurate reasoning in complex and professional domains. However, the vanilla Chain-of-Thought (CoT) prompting method faces challenges in effectively leveraging the unique strengths of visual and DSL representations, primarily due to their differing reasoning mechanisms. Additionally, it often falls short in addressing critical steps in multi-step reasoning tasks. To mitigate these challenges, we introduce the Bi-Modal Behavioral Alignment (BBA) prompting method, designed to maximize the potential of DSL in augmenting complex multi-modal reasoning tasks. This method initiates by guiding LVLMs to create separate reasoning chains for visual and DSL representations. Subsequently, it aligns these chains by addressing any inconsistencies, thus achieving a cohesive integration of behaviors from different modalities. Our experiments demonstrate that BBA substantially improves the performance of GPT-4V(ision) on geometry problem solving (28.34% <tex-math>\to</tex-math> 34.22%), chess positional advantage prediction (42.08% <tex-math>\to</tex-math> 46.99%) and molecular property prediction (77.47% <tex-math>\to</tex-math> 83.52%).</abstract>
      <url hash="25e90ab1">2024.findings-acl.433</url>
      <bibkey>zhao-etal-2024-bba</bibkey>
      <doi>10.18653/v1/2024.findings-acl.433</doi>
    </paper>
    <paper id="434">
      <title><fixed-case>P</fixed-case>artial<fixed-case>F</fixed-case>ormer: Modeling Part Instead of Whole for Machine Translation</title>
      <author><first>Tong</first><last>Zheng</last></author>
      <author><first>Bei</first><last>Li</last><affiliation>Meituan</affiliation></author>
      <author><first>Huiwen</first><last>Bao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jiale</first><last>Wang</last></author>
      <author><first>Weiqiao</first><last>Shan</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>JingBo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>7280-7294</pages>
      <abstract>The design choices in Transformer feed-forward neural networks have resulted in significant computational and parameter overhead. In this work, we emphasize the importance of hidden dimensions in designing lightweight FFNs, a factor often overlooked in previous architectures. Guided by this principle, we introduce PartialFormer, a parameter-efficient Transformer architecture utilizing multiple smaller FFNs to reduce parameters and computation while maintaining essential hidden dimensions. These smaller FFNs are integrated into a multi-head attention mechanism for effective collaboration. We also propose a tailored head scaling strategy to enhance PartialFormer’s capabilities. Furthermore, we present a residual-like attention calculation to improve depth scaling within PartialFormer. Extensive experiments on 9 translation tasks and 1 abstractive summarization task validate the effectiveness of our PartialFormer approach on machine translation and summarization tasks. Our code would be available at: https://github.com/zhengkid/PartialFormer.</abstract>
      <url hash="5a4eb50d">2024.findings-acl.434</url>
      <bibkey>zheng-etal-2024-partialformer</bibkey>
      <doi>10.18653/v1/2024.findings-acl.434</doi>
    </paper>
    <paper id="435">
      <title>Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy</title>
      <author><first>Jieyong</first><last>Kim</last></author>
      <author><first>Ryang</first><last>Heo</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Yongsik</first><last>Seo</last><affiliation>Yonsei University</affiliation></author>
      <author><first>SeongKu</first><last>Kang</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Jinyoung</first><last>Yeo</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Dongha</first><last>Lee</last><affiliation>Yonsei University</affiliation></author>
      <pages>7295-7303</pages>
      <abstract>In the task of aspect sentiment quad prediction (ASQP), generative methods for predicting sentiment quads have shown promisingresults. However, they still suffer from imprecise predictions and limited interpretability, caused by data scarcity and inadequate modeling of the quadruplet composition process. In this paper, we propose Self-Consistent Reasoning-based Aspect sentiment quadruple Prediction (SCRAP), optimizing its model to generate reasonings and the corresponding sentiment quadruplets in sequence. SCRAP adopts the Extract-Then-Assign reasoning strategy, which closely mimics human cognition. In the end, SCRAP significantly improves the model’s ability to handle complex reasoning tasks and correctly predict quadruplets through consistency voting, resulting in enhanced interpretability and accuracy in ASQP.</abstract>
      <url hash="60998ee5">2024.findings-acl.435</url>
      <bibkey>kim-etal-2024-self-consistent</bibkey>
      <doi>10.18653/v1/2024.findings-acl.435</doi>
    </paper>
    <paper id="436">
      <title><fixed-case>PACE</fixed-case>: Improving Prompt with Actor-Critic Editing for Large Language Model</title>
      <author><first>Yihong</first><last>Dong</last><affiliation>Peking University</affiliation></author>
      <author><first>Kangcheng</first><last>Luo</last><affiliation>Peking University</affiliation></author>
      <author><first>Xue</first><last>Jiang</last><affiliation>Peking University</affiliation></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University and Peking University</affiliation></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University Shenzhen Graduate School</affiliation></author>
      <pages>7304-7323</pages>
      <abstract>Large language models (LLMs) have showcased remarkable potential across various tasks by conditioning on prompts. However, the quality of different human-written prompts leads to substantial discrepancies in LLMs’ performance, and improving prompts usually necessitates considerable human effort and expertise. To this end, this paper proposes Prompt with Actor-Critic Editing (PACE) for LLMs to enable automatic prompt editing. Drawing inspiration from the actor-critic algorithm in reinforcement learning, PACE leverages LLMs as the dual roles of actors and critics, conceptualizing prompt as a type of policy. PACE refines prompt, taking into account the feedback from both actors performing prompt and critics criticizing response. This process helps LLMs better align prompt to a specific task, thanks to real responses and thinking from LLMs.We conduct extensive experiments on 24 instruction induction tasks and 21 big-bench tasks. Experimental results indicate that PACE elevates the relative performance of medium/low-quality human-written prompts by up to 98%, which has comparable performance to high-quality human-written prompts. Moreover, PACE also exhibits notable efficacy for prompt generation.</abstract>
      <url hash="9903a9a0">2024.findings-acl.436</url>
      <bibkey>dong-etal-2024-pace</bibkey>
      <doi>10.18653/v1/2024.findings-acl.436</doi>
    </paper>
    <paper id="437">
      <title>Penetrative <fixed-case>AI</fixed-case>: Making <fixed-case>LLM</fixed-case>s Comprehend the Physical World</title>
      <author><first>Huatao</first><last>Xu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Liying</first><last>Han</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Qirui</first><last>Yang</last><affiliation>Department of Computer Science and Engineering, Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Mo</first><last>Li</last><affiliation>The Hong Kong University of Science and Technology and National Technological University</affiliation></author>
      <author><first>Mani</first><last>Srivastava</last><affiliation>Amazon and University of California, Los Angeles</affiliation></author>
      <pages>7324-7341</pages>
      <abstract>Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term “Penetrative AI”. The paper explores such an extension at two levels of LLMs’ ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs beyond traditional text-based tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems.</abstract>
      <url hash="4ee95285">2024.findings-acl.437</url>
      <bibkey>xu-etal-2024-penetrative</bibkey>
      <doi>10.18653/v1/2024.findings-acl.437</doi>
    </paper>
    <paper id="438">
      <title>The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis</title>
      <author><first>Miaoran</first><last>Zhang</last><affiliation>Saarland University</affiliation></author>
      <author><first>Vagrant</first><last>Gautam</last><affiliation>Saarland University</affiliation></author>
      <author><first>Mingyang</first><last>Wang</last></author>
      <author><first>Jesujoba</first><last>Alabi</last><affiliation>Universität des Saarlandes</affiliation></author>
      <author><first>Xiaoyu</first><last>Shen</last><affiliation>Amazon</affiliation></author>
      <author><first>Dietrich</first><last>Klakow</last><affiliation>Saarland University</affiliation></author>
      <author><first>Marius</first><last>Mosbach</last><affiliation>McGill University and Mila - Quebec Artificial Intelligence Institute</affiliation></author>
      <pages>7342-7371</pages>
      <abstract>In-context learning is a popular inference strategy where large language models solve a task using only a few labeled demonstrations without needing any parameter updates. Although there have been extensive studies on English in-context learning, multilingual in-context learning remains under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that strong instruction-following models including Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.</abstract>
      <url hash="8c4f7f1c">2024.findings-acl.438</url>
      <bibkey>zhang-etal-2024-impact</bibkey>
      <doi>10.18653/v1/2024.findings-acl.438</doi>
    </paper>
    <paper id="439">
      <title>Rich Semantic Knowledge Enhanced Large Language Models for Few-shot <fixed-case>C</fixed-case>hinese Spell Checking</title>
      <author><first>Ming</first><last>Dong</last><affiliation>Central China Normal University</affiliation></author>
      <author><first>Yujing</first><last>Chen</last><affiliation>Central China Normal University</affiliation></author>
      <author><first>Miao</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Tingting</first><last>He</last><affiliation>Central China Normal University</affiliation></author>
      <pages>7372-7383</pages>
      <abstract>Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM <tex-math>(\textbf{R}ich\ \textbf{S}emantic\ based\ LLMs\)</tex-math> to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than most of the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verified the superiority of our proposed framework.</abstract>
      <url hash="935e1822">2024.findings-acl.439</url>
      <bibkey>dong-etal-2024-rich</bibkey>
      <doi>10.18653/v1/2024.findings-acl.439</doi>
    </paper>
    <paper id="440">
      <title>An Empirical Study of In-context Learning in <fixed-case>LLM</fixed-case>s for Machine Translation</title>
      <author><first>Pranjal</first><last>Chitale</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Jay</first><last>Gala</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>7384-7406</pages>
      <abstract>Recent interest has surged in employing Large Language Models (LLMs) for machine translation (MT) via in-context learning (ICL) (Vilar et al., 2023). Most prior studies primarily focus on optimizing translation quality, with limited attention to understanding the specific aspects of ICL that influence the said quality. To this end, we perform the first of its kind, exhaustive study of in-context learning for machine translation (MT). We first establish that ICL is primarily example-driven and not instruction-driven. Following this, we conduct an extensive exploration of various aspects of the examples to understand their influence on downstream performance. Our analysis includes factors such as quality and quantity of demonstrations, spatial proximity, and source versus target originality. Further, we also investigate challenging scenarios involving indirectness and misalignment of examples to understand the limits of ICL. While we establish the significance of the quality of the target distribution over the source distribution of demonstrations, we further observe that perturbations sometimes act as regularizers, resulting in performance improvements. Surprisingly, ICL does not necessitate examples from the same task, and a related task with the same target distribution proves sufficient. We hope that our study acts as a guiding resource for considerations in utilizing ICL for MT. Our code is available on https://github.com/PranjalChitale/in-context-mt-analysis.</abstract>
      <url hash="5604fedf">2024.findings-acl.440</url>
      <bibkey>chitale-etal-2024-empirical</bibkey>
      <doi>10.18653/v1/2024.findings-acl.440</doi>
    </paper>
    <paper id="441">
      <title>“My Answer is <fixed-case>C</fixed-case>”: First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models</title>
      <author><first>Xinpeng</first><last>Wang</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Bolei</first><last>Ma</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Chengzhi</first><last>Hu</last></author>
      <author><first>Leon</first><last>Weber-Genzel</last><affiliation>Ludwig-Maximilians-Universität München</affiliation></author>
      <author><first>Paul</first><last>Röttger</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Frauke</first><last>Kreuter</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Dirk</first><last>Hovy</last><affiliation>Bocconi University</affiliation></author>
      <author><first>Barbara</first><last>Plank</last><affiliation>Ludwig-Maximilians-Universität München and IT University of Copenhagen</affiliation></author>
      <pages>7407-7416</pages>
      <abstract>The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model’s diverse response styles such as starting with “Sure” or refusing to answer. Consequently, first-token evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned <i>on all dimensions</i>, reaching mismatch rates over 60%. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation.</abstract>
      <url hash="9e81b732">2024.findings-acl.441</url>
      <bibkey>wang-etal-2024-answer-c</bibkey>
      <doi>10.18653/v1/2024.findings-acl.441</doi>
    </paper>
    <paper id="442">
      <title><fixed-case>ODA</fixed-case>: Observation-Driven Agent for integrating <fixed-case>LLM</fixed-case>s and Knowledge Graphs</title>
      <author><first>Lei</first><last>Sun</last><affiliation>Panasonic Connect</affiliation></author>
      <author><first>Zhengwei</first><last>Tao</last></author>
      <author><first>Youdi</first><last>Li</last><affiliation>Panasonic Connect</affiliation></author>
      <author><first>Hiroshi</first><last>Arakawa</last></author>
      <pages>7417-7431</pages>
      <abstract>The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM’s analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation, which enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.</abstract>
      <url hash="430f034b">2024.findings-acl.442</url>
      <bibkey>sun-etal-2024-oda</bibkey>
      <doi>10.18653/v1/2024.findings-acl.442</doi>
    </paper>
    <paper id="443">
      <title>A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models</title>
      <author><first>Zihao</first><last>Xu</last></author>
      <author><first>Yi</first><last>Liu</last></author>
      <author><first>Gelei</first><last>Deng</last></author>
      <author><first>Yuekang</first><last>Li</last><affiliation>University of New South Wales</affiliation></author>
      <author><first>Stjepan</first><last>Picek</last><affiliation>Radboud University Nijmegen</affiliation></author>
      <pages>7432-7449</pages>
      <abstract>Large Language Models (LLMs) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of “jailbreaking” — where carefully crafted prompts elicit harmful responses from models — persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks underperform compared to universal techniques and that including special tokens in the input significantly affects the likelihood of successful attacks. This research highlights the need to concentrate on the security facets of LLMs. Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into LLM security. We believe these contributions will facilitate the exploration of security measures within this domain.</abstract>
      <url hash="4f8eb538">2024.findings-acl.443</url>
      <bibkey>xu-etal-2024-comprehensive</bibkey>
      <doi>10.18653/v1/2024.findings-acl.443</doi>
    </paper>
    <paper id="444">
      <title>A Data-Driven Guided Decoding Mechanism for Diagnostic Captioning</title>
      <author><first>Panagiotis</first><last>Kaliosis</last></author>
      <author><first>John</first><last>Pavlopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Foivos</first><last>Charalampakos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Georgios</first><last>Moschovis</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <pages>7450-7466</pages>
      <abstract>Diagnostic Captioning (DC) automatically generates a diagnostic text from one or more medical images (e.g., X-rays, MRIs) of a patient. Treated as a draft, the generated text may assist clinicians, by providing an initial estimation of the patient’s condition, speeding up and helping safeguard the diagnostic process. The accuracy of a diagnostic text, however, strongly depends on how well the key medical conditions depicted in the images are expressed. We propose a new <tex-math>\textit{data-driven}</tex-math> guided decoding method that incorporates medical information, in the form of existing tags capturing key conditions of the image(s), into the beam search of the diagnostic text generation process. We evaluate the proposed method on two medical datasets using four DC systems that range from generic image-to-text systems with CNN encoders and RNN decoders to pre-trained Large Language Models. The latter can also be used in few- and zero-shot learning scenarios. In most cases, the proposed mechanism improves performance with respect to all evaluation measures. We provide an open-source implementation of the proposed method at https://github.com/nlpaueb/dmmcs.</abstract>
      <url hash="a9347c80">2024.findings-acl.444</url>
      <bibkey>kaliosis-etal-2024-data</bibkey>
      <doi>10.18653/v1/2024.findings-acl.444</doi>
    </paper>
    <paper id="445">
      <title>Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model</title>
      <author><first>Hengyuan</first><last>Zhang</last></author>
      <author><first>Yanru</first><last>Wu</last></author>
      <author><first>Dawei</first><last>Li</last></author>
      <author><first>Sak</first><last>Yang</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Rui</first><last>Zhao</last><affiliation>Qing Yuan Research Institute, Shanghai Jiao Tong University and SenseTime Research</affiliation></author>
      <author><first>Yong</first><last>Jiang</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Fei</first><last>Tan</last><affiliation>Sensetime Research</affiliation></author>
      <pages>7467-7509</pages>
      <abstract>Aligned Large Language Models (LLMs) showcase remarkable versatility, capable of handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected to exhibit speciality, excelling in specific applications. However, fine-tuning with extra data, a common practice to gain speciality, often leads to catastrophic forgetting (CF) of previously acquired versatility, hindering the model’s performance across diverse tasks. In response to this challenge, we propose CoFiTune, a coarse to fine framework in an attempt to strike the balance between speciality and versatility. At the coarse-grained level, an empirical tree-search algorithm is utilized to pinpoint and update specific modules that are crucial for speciality, while keeping other parameters frozen; at the fine-grained level, a soft-masking mechanism regulates the update to the LLMs, mitigating the CF issue without harming speciality. In an overall evaluation of both speciality and versatility, CoFiTune consistently outperforms baseline methods across diverse tasks and model scales. Compared to the full-parameter SFT, CoFiTune leads to about 14% versatility improvement and marginal speciality loss on a 13B model. Lastly, based on further analysis, we provide a speculative insight into the information forwarding process in LLMs, which helps explain the effectiveness of the proposed method. The code is available at https://github.com/rattlesnakey/CoFiTune.</abstract>
      <url hash="a44693b4">2024.findings-acl.445</url>
      <bibkey>zhang-etal-2024-balancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.445</doi>
    </paper>
    <paper id="446">
      <title>A Two-Agent Game for Zero-shot Relation Triplet Extraction</title>
      <author><first>Ting</first><last>Xu</last></author>
      <author><first>Haiqin</first><last>Yang</last><affiliation>International Digital Economy Academy (IDEA)</affiliation></author>
      <author><first>Fei</first><last>Zhao</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhen</first><last>Wu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Xinyu</first><last>Dai</last><affiliation>Nanjing University</affiliation></author>
      <pages>7510-7527</pages>
      <abstract>Relation triplet extraction is a fundamental task in natural language processing that aims to identify semantic relationships between entities in text. It is particularly challenging in the zero-shot setting, i.e., zero-shot relation triplet extraction (ZeroRTE), where the relation sets between training and test are disjoint. Existing methods deal with this task by integrating relations into prompts, which may lack sufficient understanding of the unseen relations. To address these limitations, this paper presents a novel Two-Agent Game (TAG) approach to deliberate and debate the semantics of unseen relations. TAG consists of two agents, a generator and an extractor. They iteratively interact in three key steps: attempting, criticizing, and rectifying. This enables the agents to fully debate and understand the unseen relations. Experimental results demonstrate consistent improvement over ALBERT-Large, BART, andGPT3.5, without incurring additional inference costs in all cases. Remarkably, our method outperforms strong baselines by a significant margin, achieving an impressive 6%-16% increase in F1 scores, particularly when dealingwith FewRel with five unseen relations.</abstract>
      <url hash="d1bb0077">2024.findings-acl.446</url>
      <bibkey>xu-etal-2024-two</bibkey>
      <doi>10.18653/v1/2024.findings-acl.446</doi>
    </paper>
    <paper id="447">
      <title>Light-<fixed-case>PEFT</fixed-case>: Lightening Parameter-Efficient Fine-Tuning via Early Pruning</title>
      <author><first>Naibin</first><last>Gu</last></author>
      <author><first>Peng</first><last>Fu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiyu</first><last>Liu</last></author>
      <author><first>Bowen</first><last>Shen</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Weiping</first><last>Wang</last></author>
      <pages>7528-7541</pages>
      <abstract>Parameter-efficient fine-tuning (PEFT) has emerged as the predominant technique for fine-tuning in the era of large language models. However, existing PEFT methods still have inadequate training efficiency. Firstly, the utilization of large-scale foundation models during the training process is excessively redundant for certain fine-tuning tasks. Secondly, as the model size increases, the growth in trainable parameters of empirically added PEFT modules becomes non-negligible and redundant, leading to inefficiency. To achieve task-specific efficient fine-tuning, we propose the Light-PEFT framework, which includes two methods: Masked Early Pruning of the Foundation Model and Multi-Granularity Early Pruning of PEFT. The Light-PEFT framework allows for the simultaneous estimation of redundant parameters in both the foundation model and PEFT modules during the early stage of training. These parameters can then be pruned for more efficient fine-tuning. We validate our approach on GLUE, SuperGLUE, QA tasks, and various models. With Light-PEFT, parameters of the foundation model can be pruned by up to over 40%, while still controlling trainable parameters to be only 25% of the original PEFT method. Compared to utilizing the PEFT method directly, Light-PEFT achieves training and inference speedup, reduces memory usage, and maintains comparable performance and the plug-and-play feature of PEFT.</abstract>
      <url hash="ebc51638">2024.findings-acl.447</url>
      <bibkey>gu-etal-2024-light</bibkey>
      <doi>10.18653/v1/2024.findings-acl.447</doi>
    </paper>
    <paper id="448">
      <title>Building Bridges: A Dataset for Evaluating Gender-Fair Machine Translation into <fixed-case>G</fixed-case>erman</title>
      <author><first>Manuel</first><last>Lardelli</last></author>
      <author><first>Giuseppe</first><last>Attanasio</last><affiliation>Instituto de Telecomunicações</affiliation></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <pages>7542-7550</pages>
      <abstract>The translation of gender-neutral person-referring terms (e.g.,the students) is often non-trivial.Translating from English into German poses an interesting case—in German, person-referring nouns are usually gender-specific, and if the gender of the referent(s) is unknown or diverse, the generic masculine (die Studenten (m.)) is commonly used. This solution, however, reduces the visibility of other genders, such as women and non-binary people. To counteract gender discrimination, a societal movement towards using gender-fair language exists (e.g., by adopting neosystems). However, gender-fair German is currently barely supported in machine translation (MT), requiring post-editing or manual translations. We address this research gap by studying gender-fair language in English-to-German MT. Concretely, we enrich a community-created gender-fair language dictionary and sample multi-sentence test instances from encyclopedic text and parliamentary speeches.Using these novel resources, we conduct the first benchmark study involving two commercial systems and six neural MT models for translating words in isolation and natural contexts across two domains. Our findings show that most systems produce mainly masculine forms and rarely gender-neutral variants, highlighting the need for future research. We release code and data at https://github.com/g8a9/building-bridges-gender-fair-german-mt.</abstract>
      <url hash="3c8a7fb1">2024.findings-acl.448</url>
      <bibkey>lardelli-etal-2024-building</bibkey>
      <doi>10.18653/v1/2024.findings-acl.448</doi>
    </paper>
    <paper id="449">
      <title>Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization</title>
      <author><first>Shichao</first><last>Sun</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Ruifeng</first><last>Yuan</last></author>
      <author><first>Ziqiang</first><last>Cao</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>7551-7558</pages>
      <abstract>Large language models (LLMs) have demonstrated the capacity to improve summary quality by mirroring a human-like iterative process of critique and refinement starting from the initial draft. Two strategies are designed to perform this iterative process: <tex-math>\textit{Prompt Chaining}</tex-math> and <tex-math>\textit{Stepwise Prompt}</tex-math>. Prompt chaining orchestrates the drafting, critiquing, and refining phases through a series of three discrete prompts, while Stepwise prompt integrates these phases within a single prompt. However, the relative effectiveness of the two methods has not been extensively studied. This paper is dedicated to examining and comparing these two methods in the context of text summarization to ascertain which method stands out as the most effective. Experimental results show that the prompt chaining method can produce a more favorable outcome. This might be because stepwise prompt might produce a simulated refinement process according to our various experiments. Since refinement is adaptable to diverse tasks, our conclusions have the potential to be extrapolated to other applications, thereby offering insights that may contribute to the broader development of LLMs.</abstract>
      <url hash="aa7c561e">2024.findings-acl.449</url>
      <bibkey>sun-etal-2024-prompt</bibkey>
      <doi>10.18653/v1/2024.findings-acl.449</doi>
    </paper>
    <paper id="450">
      <title>Trust in Internal or External Knowledge? Generative Multi-Modal Entity Linking with Knowledge Retriever</title>
      <author><first>Xinwei</first><last>Long</last></author>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Bowen</first><last>Zhou</last><affiliation>Tsinghua University</affiliation></author>
      <pages>7559-7569</pages>
      <abstract>Multi-modal entity linking (MEL) is a challenging task that requires accurate prediction of entities within extensive search spaces, utilizing multi-modal contexts. Existing generative approaches struggle with the knowledge gap between visual entity information and the intrinsic parametric knowledge of LLMs. To address this knowledge gap, we introduce a novel approach called GELR, which incorporates a knowledge retriever to enhance visual entity information by leveraging external sources. Additionally, we devise a prioritization scheme that effectively handles noisy retrieval results and manages conflicts arising from the integration of external and internal knowledge. Moreover, we propose a noise-aware instruction tuning technique during training to finely adjust the model’s ability to leverage retrieved information effectively. Through extensive experiments conducted on three benchmarks, our approach showcases remarkable improvements, ranging from 3.0% to 6.5%, across all evaluation metrics compared to strong baselines. These results demonstrate the effectiveness and superiority of our proposed method in tackling the complexities of multi-modal entity linking.</abstract>
      <url hash="2a1881c6">2024.findings-acl.450</url>
      <bibkey>long-etal-2024-trust</bibkey>
      <doi>10.18653/v1/2024.findings-acl.450</doi>
    </paper>
    <paper id="451">
      <title>A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection</title>
      <author><first>Taichi</first><last>Aida</last><affiliation>Tokyo Metropolitan University</affiliation></author>
      <author><first>Danushka</first><last>Bollegala</last><affiliation>Amazon and University of Liverpool</affiliation></author>
      <pages>7570-7584</pages>
      <abstract>Detecting temporal semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions.Lexical Semantic Change Detection (SCD) task involves predicting whether a given target word, <tex-math>w</tex-math>, changes its meaning between two different text corpora, <tex-math>C_1</tex-math> and <tex-math>C_2</tex-math>.For this purpose, we propose a supervised two-staged SCD method that uses existing Word-in-Context (WiC) datasets.In the first stage, for a target word <tex-math>w</tex-math>, we learn two sense-aware encoders that represent the meaning of <tex-math>w</tex-math> in a given sentence selected from a corpus.Next, in the second stage, we learn a sense-aware distance metric that compares the semantic representations of a target word across all of its occurrences in <tex-math>C_1</tex-math> and <tex-math>C_2</tex-math>.Experimental results on multiple benchmark datasets for SCD show that our proposed method achieves strong performance in multiple languages.Additionally, our method achieves significant improvements on WiC benchmarks compared to a sense-aware encoder with conventional distance functions.</abstract>
      <url hash="f85459bc">2024.findings-acl.451</url>
      <bibkey>aida-bollegala-2024-semantic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.451</doi>
    </paper>
    <paper id="452">
      <title>What Have We Achieved on Non-autoregressive Translation?</title>
      <author><first>Yafu</first><last>Li</last><affiliation>Westlake University</affiliation></author>
      <author><first>Huajian</first><last>Zhang</last></author>
      <author><first>Jianhao</first><last>Yan</last><affiliation>Westlake University</affiliation></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>7585-7606</pages>
      <abstract>Recent advances have made non-autoregressive (NAT) translation comparable to autoregressive methods (AT). However, their evaluation using BLEU has been shown to weakly correlate with human annotations. Limited research compares non-autoregressive translation and autoregressive translation comprehensively, leaving uncertainty about the true proximity of NAT to AT. To address this gap, we systematically evaluate four representative NAT methods across various dimensions, including human evaluation. Our empirical results demonstrate that despite narrowing the performance gap, state-of-the-art NAT still underperforms AT under more reliable evaluation metrics. Furthermore, we discover that explicitly modeling dependencies is crucial for generating natural language and generalizing to out-of-distribution sequences.</abstract>
      <url hash="8d7618e2">2024.findings-acl.452</url>
      <bibkey>li-etal-2024-achieved</bibkey>
      <doi>10.18653/v1/2024.findings-acl.452</doi>
    </paper>
    <paper id="453">
      <title>From Zero to Hero: Cold-Start Anomaly Detection</title>
      <author><first>Tal</first><last>Reiss</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <author><first>George</first><last>Kour</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Naama</first><last>Zwerdling</last></author>
      <author><first>Ateret</first><last>Anaby Tavor</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Yedid</first><last>Hoshen</last><affiliation>Google and Hebrew University of Jerusalem</affiliation></author>
      <pages>7607-7617</pages>
      <abstract>When first deploying an anomaly detection system, e.g., to detect out-of-scope queries in chatbots, there are no observed data, making data-driven approaches ineffective. Zero-shot anomaly detection methods offer a solution to such “cold-start” cases, but unfortunately they are often not accurate enough. This paper studies the realistic but underexplored <tex-math>\textit{cold-start}</tex-math> setting where an anomaly detection model is initialized using zero-shot guidance, but subsequently receives a small number of contaminated observations (namely, that may include anomalies). The goal is to make efficient use of both the zero-shot guidance and the observations. We propose ColdFusion, a method that effectively adapts the zero-shot anomaly detector to contaminated observations. To support future development of this new setting, we propose an evaluation suite consisting of evaluation protocols and metrics.</abstract>
      <url hash="f9ec0ad1">2024.findings-acl.453</url>
      <bibkey>reiss-etal-2024-zero</bibkey>
      <doi>10.18653/v1/2024.findings-acl.453</doi>
    </paper>
    <paper id="454">
      <title>Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives</title>
      <author><first>Runcong</first><last>Zhao</last></author>
      <author><first>Qinglin</first><last>Zhu</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Hainiu</first><last>Xu</last></author>
      <author><first>Jiazheng</first><last>Li</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Yuxiang</first><last>Zhou</last><affiliation>King’s College London</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>7618-7638</pages>
      <abstract>Existing datasets for narrative understanding often fail to represent the complexity and uncertainty of relationships in real-life social scenarios. To address this gap, we introduce a new benchmark, Conan, designed for extracting and analysing intricate character relation graphs from detective narratives. Specifically, we designed hierarchical relationship categories and manually extracted and annotated role-oriented relationships from the perspectives of various characters, incorporating both public relationships known to most characters and secret ones known to only a few. Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives. The combination of the Conan dataset and our pipeline strategy is geared towards understanding the ability of LLMs to comprehend nuanced relational dynamics in narrative contexts.</abstract>
      <url hash="ecfba06d">2024.findings-acl.454</url>
      <bibkey>zhao-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.454</doi>
    </paper>
    <paper id="455">
      <title><fixed-case>D</fixed-case>istill<fixed-case>MIKE</fixed-case>: Editing Distillation of Massive In-Context Knowledge Editing in Large Language Models</title>
      <author><first>Shanbao</first><last>Qiao</last></author>
      <author><first>Xuebing</first><last>Liu</last></author>
      <author><first>Seung-Hoon</first><last>Na</last><affiliation>Chonbuk National University</affiliation></author>
      <pages>7639-7654</pages>
      <abstract>Among the recently emerged knowledge editing methods, in-context knowledge editing (IKE) has shown respectable abilities on knowledge editing in terms of generalization and specificity. Noting the promising advantages but unexplored issues of IKE, we propose **DistillMIKE** as a novel extension of IKE, i.e., editing **distill**ation of "**M**assive” **I**n-context **K**nowledge **E**diting in large language models (LLMs), mainly consisting of two expansions; 1) *Massive in-context knowledge editing (MIKE)*, which extends IKE to a massive editing task, aiming to inject not a single edit but a set of massive edits to LLMs; To preserve specificity, our key novel extension is a “selective” retrieval augmentation, where the retrieval-augmented IKE is only applied to “in-scope” examples, whereas the unedited model without IKE is employed for “out-of-scope” ones. 2) *Editing distillation* of MIKE using low-rank adaptation (LoRA), which distills editing abilities of MIKE to parameters of LLMs in a manner of eliminating the need of lengthy in-context demonstrations, thus removing the computational overhead encountered at the inference time. Experimental results on the zsRE and CounterFact datasets demonstrate that MIKE shows the state-of-the-art perfomrances and DistilMIKE show comparable performances with MIKE. Our code is available at https://github.com/JoveReCode/DistillMIKE.git.</abstract>
      <url hash="fb18bf85">2024.findings-acl.455</url>
      <bibkey>qiao-etal-2024-distillmike</bibkey>
      <doi>10.18653/v1/2024.findings-acl.455</doi>
    </paper>
    <paper id="456">
      <title>Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding</title>
      <author><first>Heming</first><last>Xia</last></author>
      <author><first>Zhe</first><last>Yang</last><affiliation>Peking University</affiliation></author>
      <author><first>Qingxiu</first><last>Dong</last></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author id="yongqi-li-hk"><first>Yongqi</first><last>Li</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhifang</first><last>Sui</last><affiliation>Peking University</affiliation></author>
      <pages>7655-7671</pages>
      <abstract>To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.</abstract>
      <url hash="9aedd963">2024.findings-acl.456</url>
      <bibkey>xia-etal-2024-unlocking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.456</doi>
    </paper>
    <paper id="457">
      <title>Hierarchy-aware Biased Bound Margin Loss Function for Hierarchical Text Classification</title>
      <author><first>Gibaeg</first><last>Kim</last></author>
      <author><first>SangHun</first><last>Im</last><affiliation>Korea University of Technology and Education</affiliation></author>
      <author><first>Heung-Seon</first><last>Oh</last><affiliation>Korea University of Technology and Education</affiliation></author>
      <pages>7672-7682</pages>
      <abstract>Hierarchical text classification (HTC) is a challenging problem with two key issues: utilizing structural information and mitigating label imbalance. Recently, the unit-based approach generating unit-based feature representations has outperformed the global approach focusing on a global feature representation. Nevertheless, unit-based models using BCE and ZLPR losses still face static thresholding and label imbalance challenges. Those challenges become more critical in large-scale hierarchies. This paper introduces a novel hierarchy-aware loss function for unit-based HTC models: Hierarchy-aware Biased Bound Margin (HBM) loss. HBM integrates learnable bounds, biases, and a margin to address static thresholding and mitigate label imbalance adaptively. Experimental results on benchmark datasets demonstrate the superior performance of HBM compared to competitive HTC models.</abstract>
      <url hash="b335fab7">2024.findings-acl.457</url>
      <bibkey>kim-etal-2024-hierarchy</bibkey>
      <doi>10.18653/v1/2024.findings-acl.457</doi>
    </paper>
    <paper id="458">
      <title>Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts</title>
      <author><first>Zhuo</first><last>Chen</last></author>
      <author><first>Xinyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Kewei</first><last>Tu</last><affiliation>ShanghaiTech University</affiliation></author>
      <pages>7683-7694</pages>
      <abstract>In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to cover longer contexts in Open-Domain Question-Answering tasks. %It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs.It leverages a small encoder and cross-attention mechanism and effectively encodes contexts. With our method, the original language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings. Our code will be released at https://github.com/Alibaba-NLP/Vec-RA-ODQA.</abstract>
      <url hash="3ab5f013">2024.findings-acl.458</url>
      <bibkey>chen-etal-2024-improving-retrieval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.458</doi>
    </paper>
    <paper id="459">
      <title><fixed-case>CICL</fixed-case>e: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification</title>
      <author><first>Korbinian</first><last>Randl</last><affiliation>Stockholm University</affiliation></author>
      <author><first>John</first><last>Pavlopoulos</last><affiliation>Athens University of Economics and Business</affiliation></author>
      <author><first>Aron</first><last>Henriksson</last><affiliation>Stockholm University</affiliation></author>
      <author><first>Tony</first><last>Lindgren</last><affiliation>Depratment of Computer and Systems Sciences</affiliation></author>
      <pages>7695-7715</pages>
      <abstract>Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and benchmark naive, traditional, and Transformer models. Based on our analysis, Logistic Regression based on a TF-IDF representation outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss different prompting strategies and present an LLM-in-the-loop framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal prompting.</abstract>
      <url hash="152adb8f">2024.findings-acl.459</url>
      <bibkey>randl-etal-2024-cicle</bibkey>
      <doi>10.18653/v1/2024.findings-acl.459</doi>
    </paper>
    <paper id="460">
      <title><fixed-case>I</fixed-case>ntact<fixed-case>KV</fixed-case>: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact</title>
      <author><first>Ruikang</first><last>Liu</last></author>
      <author><first>Haoli</first><last>Bai</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Haokun</first><last>Lin</last></author>
      <author><first>Yuening</first><last>Li</last></author>
      <author><first>Han</first><last>Gao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Zhengzhuo</first><last>Xu</last></author>
      <author><first>Lu</first><last>Hou</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Jun</first><last>Yao</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Chun</first><last>Yuan</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>7716-7741</pages>
      <abstract>Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outliers in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which are crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions with no extra inference overhead. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further with minimal training costs. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement over various quantization methods across different LLMs and downstream tasks, leading to the new state-of-the-art for LLM quantization. The codes are available at https://github.com/ruikangliu/IntactKV.</abstract>
      <url hash="a3b6daa2">2024.findings-acl.460</url>
      <bibkey>liu-etal-2024-intactkv</bibkey>
      <doi>10.18653/v1/2024.findings-acl.460</doi>
    </paper>
    <paper id="461">
      <title>Learning Adverbs with Spectral Mixture Kernels</title>
      <author><first>Tomoe</first><last>Taniguchi</last><affiliation>Ochanomizu Women’s University</affiliation></author>
      <author><first>Daichi</first><last>Mochihashi</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last><affiliation>Ochanomizu University</affiliation></author>
      <pages>7742-7752</pages>
      <abstract>For humans and robots to collaborate more in the real world, robots need to understand human intentions from the different manner of their behaviors. In our study, we focus on the meaning of adverbs which describe human motions. We propose a topic model, Hierarchical Dirichlet Process-Spectral Mixture Latent Dirichlet Allocation, which concurrently learns the relationship between those human motions and those adverbs by capturing the frequency kernels that represent motion characteristics and the shared topics of adverbs that depict such motions. We trained the model on datasets we made from movies about “walking” and “dancing”, and found that our model outperforms representative neural network models in terms of perplexity score. We also demonstrate our model’s ability to determine the adverbs for a given motion and confirmed that the model predicts more appropriate adverbs.</abstract>
      <url hash="1729b6cd">2024.findings-acl.461</url>
      <bibkey>taniguchi-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.461</doi>
    </paper>
    <paper id="462">
      <title><fixed-case>E</fixed-case>-<fixed-case>EVAL</fixed-case>: A Comprehensive <fixed-case>C</fixed-case>hinese K-12 Education Evaluation Benchmark for Large Language Models</title>
      <author><first>Jinchang</first><last>Hou</last></author>
      <author><first>Chang</first><last>Ao</last></author>
      <author><first>Haihong</first><last>Wu</last></author>
      <author><first>Xiangtao</first><last>Kong</last></author>
      <author><first>Zhigang</first><last>Zheng</last></author>
      <author><first>Daijia</first><last>Tang</last></author>
      <author><first>Chengming</first><last>Li</last><affiliation>Shenzhen MSU-BIT University</affiliation></author>
      <author><first>Xiping</first><last>Hu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Shiwen</first><last>Ni</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <pages>7753-7774</pages>
      <abstract>The rapid development of Large Language Models (LLMs) has led to their increasing utilization in Chinese K-12 education. Despite the growing integration of LLMs and education, the absence of a dedicated benchmark for evaluating LLMs within this domain presents a pressing concern. Consequently, there is an urgent need for a comprehensive natural language processing benchmark to precisely assess the capabilities of various LLMs in Chinese K-12 education. In response, we introduce E-EVAL, the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects. Through meticulous evaluation, we find that Chinese-dominant models often outperform English-dominant ones, with many exceeding GPT 4.0. However, most struggle with complex subjects like mathematics. Additionally, our analysis indicates that most Chinese-dominant LLMs do not achieve higher scores at the primary school level compared to the middle school level, highlighting the nuanced relationship between proficiency in higher-order and lower-order knowledge domains. Furthermore, experimental results highlight the effectiveness of the Chain of Thought (CoT) technique in scientific subjects and Few-shot prompting in liberal arts. Through E-EVAL, we aim to conduct a rigorous analysis delineating the strengths and limitations of LLMs in educational applications, thereby contributing significantly to the advancement of Chinese K-12 education and LLMs.</abstract>
      <url hash="b4d4b52d">2024.findings-acl.462</url>
      <bibkey>hou-etal-2024-e</bibkey>
      <doi>10.18653/v1/2024.findings-acl.462</doi>
    </paper>
    <paper id="463">
      <title><fixed-case>C</fixed-case>hart<fixed-case>A</fixed-case>ssistant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning</title>
      <author><first>Fanqing</first><last>Meng</last></author>
      <author><first>Wenqi</first><last>Shao</last></author>
      <author><first>Quanfeng</first><last>Lu</last><affiliation>Shanghai Jiaotong University and Nanjing university</affiliation></author>
      <author><first>Peng</first><last>Gao</last><affiliation>shanghai ai lab</affiliation></author>
      <author><first>Kaipeng</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <author><first>Ping</first><last>Luo</last><affiliation>The University of Hong Kong</affiliation></author>
      <pages>7775-7803</pages>
      <abstract>Charts play a vital role in data visualization, understanding data patterns, and informed decision-making. However, their unique combination of graphical elements (e.g., bars, lines) and textual components (e.g., labels, legends) poses challenges for general-purpose multimodal models. While vision-language models trained on chart data excel in comprehension, they struggle with generalization. To address these challenges, we propose ChartAssistant, a chart-based vision-language model for universal chart comprehension and reasoning. ChartAssistant leverages ChartSFT, a comprehensive dataset covering diverse chart-related tasks with basic (e.g. bars and pies) and specialized (e.g. radars, and bubbles) chart types. It undergoes a two-stage training process, starting with pre-training on chart-to-table parsing to align chart and text, followed by multitask instruction-following fine-tuning. This approach enables ChartAssistant to achieve competitive performance across various chart tasks. Experimental results demonstrate significant performance gains over the state-of-the-art UniChart and ChartLlama methods, especially outperforming them on real-world chart data with zero-shot setting. The code and data are available at https://github.com/OpenGVLab/ChartAst.</abstract>
      <url hash="96c75faa">2024.findings-acl.463</url>
      <bibkey>meng-etal-2024-chartassistant</bibkey>
      <doi>10.18653/v1/2024.findings-acl.463</doi>
    </paper>
    <paper id="464">
      <title>Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering</title>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Shizhu</first><last>He</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Fangyu</first><last>Lei</last></author>
      <author><first>JunYang</first><last>JunYang</last></author>
      <author><first>Tianhuang</first><last>Su</last></author>
      <author><first>Kang</first><last>Liu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jun</first><last>Zhao</last><affiliation>Institute of automation, Chinese academy of science</affiliation></author>
      <pages>7804-7816</pages>
      <abstract>Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (e.g., mathematical question answering) by Chain-of-thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales generated from LLMs such as ChatGPT. However, CoTD has certain limitations that make it unsuitable for knowledge-intensive multi-hop question answering: 1) SLMs have a very limited capacity in memorizing required knowledge compared to LLMs. 2) SLMs do not possess the same powerful integrated abilities in question understanding and knowledge reasoning as LLMs. To address the above limitations, we introduce Decompose-and-Response Distillation (D&amp;R Distillation), which distills two student models, namely Decomposer and Responser separately. The two models solve a knowledge-intensive multi-hop question through an interactive process of asking and answering subquestions. Our method offers two advantages: 1) SLMs have the capability to access external knowledge to address subquestions, which provides more comprehensive knowledge for multi-hop questions. 2) By employing simpler subquestions instead of complex CoT reasoning, SLMs effectively mitigate task complexity and decrease data prerequisites. Experimental results on three knowledge-intensive multi-hop question answering datasets demonstrate that D&amp;R Distillation can surpass previous CoTD methods, even with much less training data.</abstract>
      <url hash="583017cb">2024.findings-acl.464</url>
      <bibkey>li-etal-2024-teaching</bibkey>
      <doi>10.18653/v1/2024.findings-acl.464</doi>
    </paper>
    <paper id="465">
      <title><fixed-case>AL</fixed-case>a<fixed-case>RM</fixed-case>: Align Language Models via Hierarchical Rewards Modeling</title>
      <author><first>Yuhang</first><last>Lai</last><affiliation>Fudan University</affiliation></author>
      <author><first>Siyuan</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Shujun</first><last>Liu</last></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>7817-7831</pages>
      <abstract>We introduce ALaRM, the first framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment. We release our code at https://ALaRM-fdu.github.io.</abstract>
      <url hash="1f881e61">2024.findings-acl.465</url>
      <bibkey>lai-etal-2024-alarm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.465</doi>
    </paper>
    <paper id="466">
      <title><fixed-case>LSTP</fixed-case>rompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting</title>
      <author><first>Haoxin</first><last>Liu</last></author>
      <author><first>Zhiyuan</first><last>Zhao</last></author>
      <author><first>Jindong</first><last>Wang</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Harshavardhan</first><last>Kamarthi</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>B. Aditya</first><last>Prakash</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>7832-7840</pages>
      <abstract>Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.</abstract>
      <url hash="7244b126">2024.findings-acl.466</url>
      <bibkey>liu-etal-2024-lstprompt</bibkey>
      <doi>10.18653/v1/2024.findings-acl.466</doi>
    </paper>
    <paper id="467">
      <title>Mitigating Boundary Ambiguity and Inherent Bias for Text Classification in the Era of Large Language Models</title>
      <author><first>Zhenyi</first><last>Lu</last></author>
      <author><first>Jie</first><last>Tian</last></author>
      <author><first>Wei</first><last>Wei</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Xiaoye</first><last>Qu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wenfeng</first><last>Xie</last></author>
      <author><first>Dangyang</first><last>Chen</last><affiliation>Pingan Technology</affiliation></author>
      <pages>7841-7864</pages>
      <abstract>Text classification is a crucial task encountered frequently in practical scenarios, yet it is still under-explored in the era of large language models (LLMs). This study shows that LLMs are vulnerable to changes in the number and arrangement of options in text classification. Our extensive empirical analyses reveal that the key bottleneck arises from ambiguous decision boundaries and inherent biases towards specific tokens and positions.To mitigate these issues, we make the first attempt and propose a novel two-stage classification framework for LLMs. Our approach is grounded in the empirical observation that pairwise comparisons can effectively alleviate boundary ambiguity and inherent bias. Specifically, we begin with a self-reduction technique to efficiently narrow down numerous options, which contributes to reduced decision space and a faster comparison process. Subsequently, pairwise contrastive comparisons are employed in a chain-of-thought manner to draw out nuances and distinguish confusable options, thus refining the ambiguous decision boundary.Extensive experiments on four datasets (Banking77, HWU64, LIU54, and Clinic150) verify the effectiveness of our framework. Furthermore, benefitting from our framework, various LLMs can achieve consistent improvements. Our code and data are available in <url>https://github.com/Chuge0335/PC-CoT</url>.</abstract>
      <url hash="8c08ebe0">2024.findings-acl.467</url>
      <bibkey>lu-etal-2024-mitigating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.467</doi>
    </paper>
    <paper id="468">
      <title><fixed-case>UOR</fixed-case>: Universal Backdoor Attacks on Pre-trained Language Models</title>
      <author><first>Wei</first><last>Du</last></author>
      <author><first>Peixuan</first><last>Li</last></author>
      <author><first>Haodong</first><last>Zhao</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Tianjie</first><last>Ju</last></author>
      <author><first>Ge</first><last>Ren</last></author>
      <author><first>Gongshen</first><last>Liu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>7865-7877</pages>
      <abstract>Task-agnostic and transferable backdoors implanted in pre-trained language models (PLMs) pose a severe security threat as they can be inherited to any downstream task. However, existing methods rely on manual selection of triggers and backdoor representations, hindering their effectiveness and universality across different PLMs or usage paradigms. In this paper, we propose a new backdoor attack method called UOR, which overcomes these limitations by turning manual selection into automatic optimization. Specifically, we design poisoned supervised contrastive learning, which can automatically learn more uniform and universal backdoor representations. This allows for more even coverage of the output space, thus hitting more labels in downstream tasks after fine-tuning. Furthermore, we utilize gradient search to select appropriate trigger words that can be adapted to different PLMs and vocabularies. Experiments show that UOR achieves better attack performance on various text classification tasks compared to manual methods. Moreover, we test on PLMs with different architectures, usage paradigms, and more challenging tasks, achieving higher scores for universality.</abstract>
      <url hash="aa745baf">2024.findings-acl.468</url>
      <bibkey>du-etal-2024-uor</bibkey>
      <doi>10.18653/v1/2024.findings-acl.468</doi>
    </paper>
    <paper id="469">
      <title>Language models emulate certain cognitive profiles: An investigation of how predictability measures interact with individual differences</title>
      <author><first>Patrick</first><last>Haller</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Lena</first><last>Bolliger</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Lena</first><last>Jäger</last><affiliation>University of Zurich and Universität Potsdam</affiliation></author>
      <pages>7878-7892</pages>
      <abstract>To date, most investigations on surprisal and entropy effects in reading have been conducted on the group level, disregarding individual differences. In this work, we revisit the predictive power (PP) of different LMs’ surprisal and entropy measures on data of human reading times as a measure of processing effort by incorporating information of language users’ cognitive capacities. To do so, we assess the PP of surprisal and entropy estimated from generative language models (LMs) on reading data obtained from individuals who also completed a wide range of psychometric tests.Specifically, we investigate if modulating surprisal and entropy relative to cognitive scores increases prediction accuracy of reading times, and we examine whether LMs exhibit systematic biases in the prediction of reading times for cognitively high- or low-performing groups, revealing what type of psycholinguistic subjects a given LM emulates.Our study finds that in most cases, incorporating cognitive capacities increases predictive power of surprisal and entropy on reading times, and that generally, high performance in the psychometric tests is associated with lower sensitivity to predictability effects. Finally, our results suggest that the analyzed LMs emulate readers with lower verbal intelligence, suggesting that for a given target group (i.e., individuals with high verbal intelligence), these LMs provide less accurate predictability effect estimates.</abstract>
      <url hash="15db2f7b">2024.findings-acl.469</url>
      <bibkey>haller-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.469</doi>
    </paper>
    <paper id="470">
      <title>The State of Relation Extraction Data Quality: Is Bigger Always Better?</title>
      <author><first>Erica</first><last>Cai</last><affiliation>Department of Computer Science, University of Massachusetts at Amherst</affiliation></author>
      <author><first>Brendan</first><last>O’Connor</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <pages>7893-7906</pages>
      <abstract>Relation extraction (RE) extracts structured tuples of relationships (e.g. friend, enemy) between entities (e.g. Sherlock Holmes, John Watson) from text, with exciting potential applications. Hundreds of RE papers have been published in recent years; do their evaluation practices inform these goals? We review recent surveys and a sample of recent RE methods papers, compiling 38 datasets currently being used. Unfortunately, many have frequent label errors, and ones with known problems continue to be used. Many datasets focus on producing labels for a large number of relation types, often through error-prone annotation methods (e.g. distant supervision or crowdsourcing), and many recent papers rely exclusively on such datasets. We draw attention to a promising alternative: datasets with a <i>small</i> number of relations, often in specific domains like chemistry, finance, or biomedicine, where it is possible to obtain high quality expert annotations; such data can more realistically evaluate RE performance. The research community should consider more often using such resources.</abstract>
      <url hash="d9b8d993">2024.findings-acl.470</url>
      <bibkey>cai-oconnor-2024-state</bibkey>
      <doi>10.18653/v1/2024.findings-acl.470</doi>
    </paper>
    <paper id="471">
      <title><fixed-case>N</fixed-case>atural<fixed-case>C</fixed-case>ode<fixed-case>B</fixed-case>ench: Examining Coding Performance Mismatch on <fixed-case>H</fixed-case>uman<fixed-case>E</fixed-case>val and Natural User Queries</title>
      <author><first>Shudan</first><last>Zhang</last></author>
      <author><first>Hanlin</first><last>Zhao</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Qinkai</first><last>Zheng</last></author>
      <author><first>Zehan</first><last>Qi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xiaotao</first><last>Gu</last><affiliation>Zhipu AI</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>7907-7928</pages>
      <abstract>Large language models (LLMs) have manifested strong ability to generate codes for productive activities. However, current benchmarks for code synthesis, such as HumanEval, MBPP, and DS-1000, are predominantly oriented towards introductory tasks on algorithm and data science, insufficiently satisfying challenging requirements prevalent in real-world coding. To fill this gap, we propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror the complexity and variety of scenarios in real coding tasks. NCB comprises 402 high-quality problems in Python and Java, meticulously selected from natural user queries from online coding services, covering 6 different domains. Noting the extraordinary difficulty in creating testing cases for real-world queries, we also introduce a semi-automated pipeline to enhance the efficiency of test case construction. Comparing with manual solutions, it achieves an efficiency increase of more than 4 times. Our systematic experiments on 39 LLMs find that performance gaps on NCB between models with close HumanEval scores could still be significant, indicating a lack of focus on practical code synthesis scenarios or over-specified optimization on HumanEval. On the other hand, even the best-performing GPT-4 is still far from satisfying on NCB. The evaluation toolkit and development set are available at https://github.com/THUDM/NaturalCodeBench.</abstract>
      <url hash="455ba3d0">2024.findings-acl.471</url>
      <bibkey>zhang-etal-2024-naturalcodebench</bibkey>
      <doi>10.18653/v1/2024.findings-acl.471</doi>
    </paper>
    <paper id="472">
      <title><fixed-case>LLMC</fixed-case>rit: Teaching Large Language Models to Use Criteria</title>
      <author><first>Weizhe</first><last>Yuan</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Matthias</first><last>Gallé</last><affiliation>Cohere</affiliation></author>
      <pages>7929-7960</pages>
      <abstract>Humans follow criteria when they execute tasks, and these criteria are directly used to assess the quality of task completion. Therefore, having models learn to use criteria to provide feedback can help humans or models to perform tasks better. However, current research in this area tends to consider only a limited number of criteria, or only a limited number of quality assessment aspects. To fill this gap, we propose a general framework that enables large language models (LLMs) to use comprehensive criteria for a task in delivering natural language feedback on task execution. In particular, we present a model-in-the-loop framework that semi-automatically derives criteria from collected guidelines for different writing tasks and constructs in-context demonstrations for each criterion. We choose three tasks from real-world scenarios to operationalize this idea: paper introduction writing, Python code writing, and Reddit post writing, and evaluate our feedback generation framework using different LLMs. The results reveal the fine-grained effects of adding criteria and demonstrations and provide valuable guidance on how to teach LLMs to use criteria more effectively.</abstract>
      <url hash="b19ceaa1">2024.findings-acl.472</url>
      <bibkey>yuan-etal-2024-llmcrit</bibkey>
      <doi>10.18653/v1/2024.findings-acl.472</doi>
    </paper>
    <paper id="473">
      <title>Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations</title>
      <author><first>Leonardo</first><last>Ranaldi</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Giulia</first><last>Pucci</last></author>
      <author><first>Andre</first><last>Freitas</last><affiliation>Idiap Research Institute and University of Manchester</affiliation></author>
      <pages>7961-7973</pages>
      <abstract>The language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs. In this paper, we propose to empower Instruction-tuned LLMs (It-LLMs) in languages other than English by building semantic alignment between them. Hence, we propose <i>CrossAlpaca</i>, an It-LLM with cross-lingual Instruction-following and Translation-following demonstrations to improve semantic alignment between languages. We validate our approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA and adapted versions of MMLU and BBH.Our models, tested over six different languages, outperform the It-LLMs tuned on monolingual data. The final results show that instruction tuning on non-English data is not enough and that semantic alignment can be further improved by Translation-following demonstrations.</abstract>
      <url hash="f45a820f">2024.findings-acl.473</url>
      <bibkey>ranaldi-etal-2024-empowering</bibkey>
      <doi>10.18653/v1/2024.findings-acl.473</doi>
    </paper>
    <paper id="474">
      <title>Ranking Entities along Conceptual Space Dimensions with <fixed-case>LLM</fixed-case>s: An Analysis of Fine-Tuning Strategies</title>
      <author><first>Nitesh</first><last>Kumar</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Usashi</first><last>Chatterjee</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Steven</first><last>Schockaert</last><affiliation>Cardiff University</affiliation></author>
      <pages>7974-7989</pages>
      <abstract>Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy, but existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having at least some perceptual and subjective features in the training data seems essential for achieving the best results.</abstract>
      <url hash="27c62d6d">2024.findings-acl.474</url>
      <bibkey>kumar-etal-2024-ranking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.474</doi>
    </paper>
    <paper id="475">
      <title>Efficient <tex-math>k</tex-math>-Nearest-Neighbor Machine Translation with Dynamic Retrieval</title>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Zhiwei</first><last>Cao</last></author>
      <author><first>Zhongjian</first><last>Miao</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Shiyu</first><last>Liu</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <pages>7990-8001</pages>
      <abstract>To achieve non-parametric NMT domain adaptation, <tex-math>k</tex-math>-Nearest-Neighbor Machine Translation (<tex-math>k</tex-math>NN-MT) constructs an external datastore to store domain-specific translation knowledge, which derives a <tex-math>k</tex-math>NN distribution to interpolate the prediction distribution of the NMT model via a linear interpolation coefficient <tex-math>\lambda</tex-math>. Despite its success, <tex-math>k</tex-math>NN retrieval at each timestep leads to substantial time overhead. To address this issue, dominant studies resort to <tex-math>k</tex-math>NN-MT with adaptive retrieval (<tex-math>k</tex-math>NN-MT-AR), which dynamically estimates <tex-math>\lambda</tex-math> and skips <tex-math>k</tex-math>NN retrieval if <tex-math>\lambda</tex-math> is less than a fixed threshold. Unfortunately, <tex-math>k</tex-math>NN-MT-AR does not yield satisfactory results. In this paper, we first conduct a preliminary study to reveal two key limitations of <tex-math>k</tex-math>NN-MT-AR: 1) the optimization gap leads to inaccurate estimation of <tex-math>\lambda</tex-math> for determining <tex-math>k</tex-math>NN retrieval skipping, and 2) using a fixed threshold fails to accommodate the dynamic demands for <tex-math>k</tex-math>NN retrieval at different timesteps. To mitigate these limitations, we then propose <tex-math>k</tex-math>NN-MT with dynamic retrieval (<tex-math>k</tex-math>NN-MT-DR) that significantly extends vanilla <tex-math>k</tex-math>NN-MT in two aspects. Firstly, we equip <tex-math>k</tex-math>NN-MT with a MLP-based classifier for determining whether to skip <tex-math>k</tex-math>NN retrieval at each timestep. Particularly, we explore several carefully-designed scalar features to fully exert the potential of the classifier. Secondly, we propose a timestep-aware threshold adjustment method to dynamically generate the threshold, which further improves the efficiency of our model. Experimental results on the widely-used datasets demonstrate the effectiveness and generality of our model.</abstract>
      <url hash="c070d36a">2024.findings-acl.475</url>
      <bibkey>gao-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.findings-acl.475</doi>
    </paper>
    <paper id="476">
      <title>Symmetric Dot-Product Attention for Efficient Training of <fixed-case>BERT</fixed-case> Language Models</title>
      <author><first>Martin</first><last>Courtois</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Malte</first><last>Ostendorff</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Leonhard</first><last>Hennig</last><affiliation>German Research Center for AI</affiliation></author>
      <author><first>Georg</first><last>Rehm</last><affiliation>Humboldt Universität Berlin and Deutsches Forschungszentrum für Künstliche Intelligenz</affiliation></author>
      <pages>8002-8011</pages>
      <abstract>Initially introduced as a machine translation model, the Transformer architecture has now become the foundation for modern deep learning architecture, with applications in a wide range of fields, from computer vision to natural language processing. Nowadays, to tackle increasingly more complex tasks, Transformer-based models are stretched to enormous sizes, requiring increasingly larger training datasets, and unsustainable amount of compute resources. The ubiquitous nature of the Transformer and its core component, the attention mechanism, are thus prime targets for efficiency research.In this work, we propose an alternative compatibility function for the self-attention mechanism introduced by the Transformer architecture. This compatibility function exploits an overlap in the learned representation of the traditional scaled dot-product attention, leading to a symmetric with pairwise coefficient dot-product attention. When applied to the pre-training of BERT-like models, this new symmetric attention mechanism reaches a score of 79.36 on the GLUE benchmark against 78.74 for the traditional implementation, leads to a reduction of 6% in the number of trainable parameters, and reduces the number of training steps required before convergence by half.</abstract>
      <url hash="263cac99">2024.findings-acl.476</url>
      <bibkey>courtois-etal-2024-symmetric</bibkey>
      <doi>10.18653/v1/2024.findings-acl.476</doi>
    </paper>
    <paper id="477">
      <title>Synthesizing Conversations from Unlabeled Documents using Automatic Response Segmentation</title>
      <author><first>Fanyou</first><last>Wu</last><affiliation>Amazon</affiliation></author>
      <author><first>Weijie</first><last>Xu</last></author>
      <author><first>Chandan</first><last>Reddy</last><affiliation>Virginia Tech</affiliation></author>
      <author><first>Srinivasan</first><last>Sengamedu</last><affiliation>Amazon</affiliation></author>
      <pages>8012-8026</pages>
      <abstract>In this study, we tackle the challenge of inadequate and costly training data that has hindered the development of conversational question answering (ConvQA) systems. Enterprises have a large corpus of diverse internal documents. Instead of relying on a searching engine, a more compelling approach for people to comprehend these documents is to create a dialogue system. In this paper, we propose a robust dialog synthesising method. We learn the segmentation of data for the dialog task instead of using segmenting at sentence boundaries. The synthetic dataset generated by our proposed method achieves superior quality when compared to WikiDialog, as assessed through machine and human evaluations. By employing our inpainted data for ConvQA retrieval system pre-training, we observed a notable improvement in performance across OR-QuAC benchmarks.</abstract>
      <url hash="f8101505">2024.findings-acl.477</url>
      <bibkey>wu-etal-2024-synthesizing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.477</doi>
    </paper>
    <paper id="478">
      <title>Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains</title>
      <author><first>Marcio</first><last>Fonseca</last></author>
      <author><first>Shay</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>8027-8042</pages>
      <abstract>Although large language models (LLMs) exhibit remarkable capacity to leverage in-context demonstrations, it is still unclear to what extent they can learn new facts or concept definitions via prompts. To address this question, we examine the capacity of instruction-tuned LLMs to follow in-context concept annotation guidelines for zero-shot sentence labeling tasks. We design guidelines that present different types of factual and counterfactual concept definitions, which are used as prompts for zero-shot sentence classification tasks. Our results show that although concept definitions consistently help in task performance, only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts. Importantly, only proprietary models such as GPT-3.5 can recognize nonsensical guidelines, which we hypothesize is due to more sophisticated alignment methods. Finally, we find that Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which indicates that increasing model scale does not guarantee better adherence to guidelines. Altogether, our simple evaluation method reveals significant gaps in concept understanding between the most capable open-source language models and the leading proprietary APIs.</abstract>
      <url hash="611f5814">2024.findings-acl.478</url>
      <bibkey>fonseca-cohen-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.478</doi>
    </paper>
    <paper id="479">
      <title>Alignment-Based Decoding Policy for Low-Latency and Anticipation-Free Neural <fixed-case>J</fixed-case>apanese Input Method Editors</title>
      <author><first>Armin</first><last>Sarhangzadeh</last></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>8043-8054</pages>
      <abstract>Japanese input method editors (IMEs) are essential tools for inputting Japanese text using a limited set of characters such as the kana syllabary. However, despite their importance, the potential of newer attention-based encoder-decoder neural networks, such as Transformer, has not yet been fully explored for IMEs due to their high computational cost and low-quality intermediate output in simultaneous settings, leading to high latencies. In this work, we propose a simple decoding policy to enable the use of attention-based encoder-decoder networks for simultaneous kana-kanji conversion in the context of Japanese IMEs inspired by simultaneous machine translation (SimulMT). We demonstrate that simply decoding by explicitly considering the word boundaries achieves a fairly strong quality-latency trade-off, as it can be seen as equivalent to performing decoding on aligned prefixes and thus achieving an incremental anticipation-free conversion. We further show how such a policy can be applied in practice to achieve high-quality conversions with minimal computational overhead. Our experiments show that our approach can achieve a noticeably better quality-latency trade-off compared to the baselines, while also being a more practical approach due to its ability to directly handle streaming input. Our code is available at https://anonymous.4open.science/r/transformer_ime-D327.</abstract>
      <url hash="116a20ad">2024.findings-acl.479</url>
      <bibkey>sarhangzadeh-watanabe-2024-alignment</bibkey>
      <doi>10.18653/v1/2024.findings-acl.479</doi>
    </paper>
    <paper id="480">
      <title><fixed-case>EC</fixed-case>o<fixed-case>K</fixed-case>: Emotional Commonsense Knowledge Graph for Mining Emotional Gold</title>
      <author><first>Zhunheng</first><last>Wang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Xiaoyi</first><last>Liu</last></author>
      <author><first>Mengting</first><last>Hu</last><affiliation>Nankai University</affiliation></author>
      <author><first>Rui</first><last>Ying</last></author>
      <author><first>Ming</first><last>Jiang</last><affiliation>Nankai University</affiliation></author>
      <author><first>Jianfeng</first><last>Wu</last></author>
      <author><first>Yalan</first><last>Xie</last><affiliation>Nankai University</affiliation></author>
      <author><first>Hang</first><last>Gao</last><affiliation>Tianjin University of Science and Technology</affiliation></author>
      <author><first>Renhong</first><last>Cheng</last></author>
      <pages>8055-8074</pages>
      <abstract>The demand for understanding and expressing emotions in the field of natural language processing is growing rapidly. Knowledge graphs, as an important form of knowledge representation, have been widely utilized in various emotion-related tasks. However, existing knowledge graphs mainly focus on the representation and reasoning of general factual knowledge, while there are still significant deficiencies in the understanding and reasoning of emotional knowledge. In this work, we construct a comprehensive and accurate emotional commonsense knowledge graph, ECoK. We integrate cutting-edge theories from multiple disciplines such as psychology, cognitive science, and linguistics, and combine techniques such as large language models and natural language processing. By mining a large amount of text, dialogue, and sentiment analysis data, we construct rich emotional knowledge and establish the knowledge generation model COMET-ECoK. Experimental results show that ECoK contains high-quality emotional reasoning knowledge, and the performance of our knowledge generation model surpasses GPT-4-Turbo, which can help downstream tasks better understand and reason about emotions. Our data and code is available from https://github.com/ZornWang/ECoK.</abstract>
      <url hash="c909b841">2024.findings-acl.480</url>
      <bibkey>wang-etal-2024-ecok</bibkey>
      <doi>10.18653/v1/2024.findings-acl.480</doi>
    </paper>
    <paper id="481">
      <title>Deterministic Reversible Data Augmentation for Neural Machine Translation</title>
      <author><first>Jiashu</first><last>Yao</last></author>
      <author><first>Heyan</first><last>Huang</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Yuhang</first><last>Guo</last></author>
      <pages>8075-8089</pages>
      <abstract>Data augmentation is an effective way to diversify corpora in machine translation, but previous methods may introduce semantic inconsistency between original and augmented data because of irreversible operations and random subword sampling procedures. To generate both symbolically diverse and semantically consistent augmentation data, we propose Deterministic Reversible Data Augmentation (DRDA), a simple but effective data augmentation method for neural machine translation. DRDA adopts deterministic segmentations and reversible operations to generate multi-granularity subword representations and pulls them closer together with multi-view techniques. With no extra corpora or model changes required, DRDA outperforms strong baselines on several translation tasks with a clear margin (up to 4.3 BLEU gain over Transformer) and exhibits good robustness in noisy, low-resource, and cross-domain datasets.</abstract>
      <url hash="c28f0b98">2024.findings-acl.481</url>
      <bibkey>yao-etal-2024-deterministic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.481</doi>
    </paper>
    <paper id="482">
      <title>Latent Learningscape Guided In-context Learning</title>
      <author><first>Anlai</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Sunshine</first><last>Jiang</last></author>
      <author><first>Yifei</first><last>Liu</last></author>
      <author><first>Yiquan</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jun</first><last>Xiao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>8090-8101</pages>
      <abstract>The growing interest in leveraging large language models is driven by their exceptional imitation and reasoning capabilities. In-context learning (ICL), a streamlined method, has shown potential in boosting these models’ performance without modifying their underlying parameters, especially when supplied with suitable demonstrations. However, existing methods mainly choose demonstrations by comparing surface-level semantic similarities (e.g., based on embedding) and fall short of identifying the most fitting ones. This paper introduces the concept of a “latent learningscape”, a more nuanced representation that describes the characteristic of the demonstrations. Building on this concept, we develop a results-driven approach to characterize the latent learningscape features of demonstrations, which then inform the creation of more effective prompts. Through comprehensive testing across datasets in arithmetic, commonsense, and symbolic reasoning tasks, our approach outperforms leading models, showing an average increase in scores by 7.4 percentage points.</abstract>
      <url hash="e4fb057b">2024.findings-acl.482</url>
      <bibkey>zhou-etal-2024-latent</bibkey>
      <doi>10.18653/v1/2024.findings-acl.482</doi>
    </paper>
    <paper id="483">
      <title><fixed-case>SMR</fixed-case>: State Memory Replay for Long Sequence Modeling</title>
      <author><first>Biqing</first><last>Qi</last><affiliation>Tsinghua University and Harbin Institute of Technology</affiliation></author>
      <author><first>Junqi</first><last>Gao</last></author>
      <author><first>Kaiyan</first><last>Zhang</last><affiliation>Electronic Engineering, Tsinghua University</affiliation></author>
      <author><first>Dong</first><last>Li</last></author>
      <author><first>Jianxing</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ligang</first><last>Wu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bowen</first><last>Zhou</last><affiliation>Tsinghua University</affiliation></author>
      <pages>8102-8116</pages>
      <abstract>Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM’s hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA).Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.</abstract>
      <url hash="9e36d69d">2024.findings-acl.483</url>
      <bibkey>qi-etal-2024-smr</bibkey>
      <doi>10.18653/v1/2024.findings-acl.483</doi>
    </paper>
    <paper id="484">
      <title>Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks</title>
      <author><first>Aditi</first><last>Mishra</last></author>
      <author><first>Sajjadur</first><last>Rahman</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Kushan</first><last>Mitra</last></author>
      <author><first>Hannah</first><last>Kim</last><affiliation>Megagon Labs</affiliation></author>
      <author><first>Estevam</first><last>Hruschka</last><affiliation>Megagon Labs and Carnegie Mellon University</affiliation></author>
      <pages>8117-8139</pages>
      <abstract>Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. However, their ability to generate rationales for knowledge-intensive tasks (KITs) remains under-explored. Generating rationales for KIT solutions, such as commonsense multiple-choice QA, requires external knowledge to support predictions and refute alternate options. In this work, we consider the task of generating retrieval-augmented rationalization of KIT model predictions via external knowledge guidance within a few-shot setting. Surprisingly, crowd-workers preferred LLM-generated rationales over existing crowd-sourced rationales, generated in a similar knowledge-guided setting, on aspects such as factuality, sufficiency, and convincingness. However, fine-grained evaluation of such rationales highlights the need for further improvements in conciseness, novelty, and domain invariance. Additionally, through an expert-sourced study evaluating the reliability of the rationales, we demonstrate that humans’ trust in LLM-generated rationales erodes when communicated faithfully, i.e., without taking model prediction accuracy into account. We find that even instrumenting simple guardrails can be effective for reliable rationalization.</abstract>
      <url hash="c38dd4c8">2024.findings-acl.484</url>
      <bibkey>mishra-etal-2024-characterizing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.484</doi>
    </paper>
    <paper id="485">
      <title>Challenging Large Language Models with New Tasks: A Study on their Adaptability and Robustness</title>
      <author><first>Chenxi</first><last>Li</last></author>
      <author><first>Yuanhe</first><last>Tian</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Zhaxi</first><last>Zerong</last></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Fei</first><last>Xia</last><affiliation>University of Washington, Seattle</affiliation></author>
      <pages>8140-8162</pages>
      <abstract>Recent progress in large language models (LLMs) has marked a notable milestone in the field of artificial intelligence. The conventional evaluation of LLMs primarily relies on existing tasks and benchmarks, raising concerns about test set contamination and the genuine comprehension abilities of LLMs. To address these concerns, we propose to evaluate LLMs by designing new tasks, automatically generating evaluation datasets for the tasks, and conducting detailed error analyses to scrutinize LLMs’ adaptability to new tasks, their sensitivity to prompt variations, and their error tendencies. We investigate the capacity of LLMs to adapt to new but simple tasks, especially when they diverge from the models’ pre-existing knowledge. Our methodology emphasizes the creation of straightforward tasks, facilitating a precise error analysis to uncover the underlying causes of LLM failures. This strategic approach also aims to uncover effective strategies for enhancing LLM performance based on the detailed error analysis of system output.</abstract>
      <url hash="6036dd46">2024.findings-acl.485</url>
      <bibkey>li-etal-2024-challenging</bibkey>
      <doi>10.18653/v1/2024.findings-acl.485</doi>
    </paper>
    <paper id="486">
      <title>Linear Cross-Lingual Mapping of Sentence Embeddings</title>
      <author><first>Oleg</first><last>Vasilyev</last><affiliation>Primer Technologies</affiliation></author>
      <author><first>Fumika</first><last>Isono</last><affiliation>Primer AI</affiliation></author>
      <author><first>John</first><last>Bohannon</last></author>
      <pages>8163-8171</pages>
      <abstract>Semantics of a sentence is defined with much less ambiguity than semantics of a single word, and we assume that it should be better preserved by translation to another language. If multilingual sentence embeddings intend to represent sentence semantics, then the similarity between embeddings of any two sentences must be invariant with respect to translation. Based on this suggestion, we consider a simple linear cross-lingual mapping as a possible improvement of the multilingual embeddings. We also consider deviation from orthogonality conditions as a measure of deficiency of the embeddings.</abstract>
      <url hash="ece7f7b4">2024.findings-acl.486</url>
      <bibkey>vasilyev-etal-2024-linear</bibkey>
      <doi>10.18653/v1/2024.findings-acl.486</doi>
    </paper>
    <paper id="487">
      <title><fixed-case>ULTRA</fixed-case>: Unleash <fixed-case>LLM</fixed-case>s’ Potential for Event Argument Extraction through Hierarchical Modeling and Pair-wise Self-Refinement</title>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>Carter</first><last>Blum</last></author>
      <author><first>Temma</first><last>Choji</last></author>
      <author><first>Shalin</first><last>Shah</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Alakananda</first><last>Vempala</last><affiliation>Bloomberg</affiliation></author>
      <pages>8172-8185</pages>
      <abstract>Structural extraction of events within discourse is critical since it avails a deeper understanding of communication patterns and behavior trends. Event argument extraction (EAE), at the core of event-centric understanding, is the task of identifying role-specific text spans (i.e., arguments) for a given event. Document-level EAE (DocEAE) focuses on arguments that are scattered across an entire document. In this work, we explore open-source Large Language Models (LLMs) for DocEAE, and propose ULTRA, a hierarchical framework that extracts event arguments more cost-effectively. Further, it alleviates the positional bias issue intrinsic to LLMs. ULTRA sequentially reads text chunks of a document to generate a candidate argument set, upon which non-pertinent candidates are dropped through self-refinement. We introduce LEAFER to address the challenge LLMs face in locating the exact boundary of an argument. ULTRA outperforms strong baselines, including strong supervised models and ChatGPT, by 9.8% when evaluated by Exact Match (EM).</abstract>
      <url hash="b7a8e063">2024.findings-acl.487</url>
      <bibkey>zhang-etal-2024-ultra</bibkey>
      <doi>10.18653/v1/2024.findings-acl.487</doi>
    </paper>
    <paper id="488">
      <title><fixed-case>LLM</fixed-case>s Beyond <fixed-case>E</fixed-case>nglish: Scaling the Multilingual Capability of <fixed-case>LLM</fixed-case>s with Cross-Lingual Feedback</title>
      <author><first>Wen</first><last>Lai</last></author>
      <author><first>Mohsen</first><last>Mesgar</last><affiliation>Bosch</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>8186-8213</pages>
      <abstract>To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.</abstract>
      <url hash="febe4c61">2024.findings-acl.488</url>
      <bibkey>lai-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.findings-acl.488</doi>
    </paper>
    <paper id="489">
      <title><fixed-case>BASS</fixed-case>: Batched Attention-optimized Speculative Sampling</title>
      <author><first>Haifeng</first><last>Qian</last><affiliation>Amazon</affiliation></author>
      <author><first>Sujan Kumar</first><last>Gonugondla</last><affiliation>Amazon</affiliation></author>
      <author><first>Sungsoo</first><last>Ha</last><affiliation>Amazon</affiliation></author>
      <author><first>Mingyue</first><last>Shang</last><affiliation>Amazon</affiliation></author>
      <author><first>Sanjay Krishna</first><last>Gouda</last><affiliation>Amazon</affiliation></author>
      <author><first>Ramesh</first><last>Nallapati</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Sudipta</first><last>Sengupta</last><affiliation>Amazon AWS</affiliation></author>
      <author><first>Xiaofei</first><last>Ma</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Anoop</first><last>Deoras</last><affiliation>Amazon</affiliation></author>
      <pages>8214-8224</pages>
      <abstract>Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15<tex-math>\times</tex-math> speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what’s feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8%, more than 3<tex-math>\times</tex-math> the highest of that of regular decoding and around 10<tex-math>\times</tex-math> of single-sequence speculative decoding.</abstract>
      <url hash="87b1363f">2024.findings-acl.489</url>
      <bibkey>qian-etal-2024-bass</bibkey>
      <doi>10.18653/v1/2024.findings-acl.489</doi>
    </paper>
    <paper id="490">
      <title>Deciphering Digital Detectives: Understanding <fixed-case>LLM</fixed-case> Behaviors and Capabilities in Multi-Agent Mystery Games</title>
      <author><first>Dekun</first><last>Wu</last><affiliation>Université de Montréal</affiliation></author>
      <author><first>Haochen</first><last>Shi</last></author>
      <author><first>Zhiyuan</first><last>Sun</last></author>
      <author><first>Bang</first><last>Liu</last><affiliation>University of Montreal</affiliation></author>
      <pages>8225-8291</pages>
      <abstract>In this study, we explore the application of Large Language Models (LLMs) in Jubensha, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in Jubensha games. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in prompting engineering to enhance the agents’ performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents.</abstract>
      <url hash="832920c2">2024.findings-acl.490</url>
      <bibkey>wu-etal-2024-deciphering</bibkey>
      <doi>10.18653/v1/2024.findings-acl.490</doi>
    </paper>
    <paper id="491">
      <title>It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension</title>
      <author><first>Sagi</first><last>Shaier</last></author>
      <author><first>Lawrence</first><last>Hunter</last><affiliation>University of Colorado at Denver</affiliation></author>
      <author><first>Katharina</first><last>von der Wense</last><affiliation>Johannes-Gutenberg Universität Mainz, University of Colorado, Boulder and New York University</affiliation></author>
      <pages>8292-8305</pages>
      <abstract>Natural language processing has seen rapid progress over the past decade. Due to the speed of developments, some practices get established without proper evaluation. Considering one such case and focusing on reading comprehension, we ask our first research question: 1) How does the order of inputs – i.e., question and context – affect model performance? Additionally, given recent advancements in input emphasis, we ask a second research question: 2) Does emphasizing either the question, the context, or both enhance performance? Experimenting with 9 large language models across 3 datasets, we find that presenting the context before the question improves model performance, with an accuracy increase of up to 31%. Furthermore, emphasizing the context yields superior results compared to question emphasis, and in general, emphasizing parts of the input is particularly effective for addressing questions that models lack the parametric knowledge to answer. Experimenting with both prompt-based and attention-based emphasis methods, we additionally find that the best method is surprisingly simple: it only requires concatenating a few tokens to the input and results in an ac- curacy improvement of up to 36%, allowing smaller models to outperform their significantly larger counterparts.</abstract>
      <url hash="43d3cfa0">2024.findings-acl.491</url>
      <bibkey>shaier-etal-2024-say</bibkey>
      <doi>10.18653/v1/2024.findings-acl.491</doi>
    </paper>
    <paper id="492">
      <title>Large Language Models Relearn Removed Concepts</title>
      <author><first>Michelle</first><last>Lo</last></author>
      <author><first>Fazl</first><last>Barez</last><affiliation>University of Oxford, University of Oxford and University of Edinburgh</affiliation></author>
      <author><first>Shay</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>8306-8323</pages>
      <abstract>Advances in model editing through neuron pruning hold promise for removing undesirable concepts from large language models. However, it remains unclear whether models have the capacity to reacquire pruned concepts after editing. To investigate this, we evaluate concept relearning in models by tracking concept saliency and similarity in pruned neurons during retraining for named entity recognition tasks. Our findings reveal that models can quickly regain performance post-pruning by relocating advanced concepts to earlier layers and reallocating pruned concepts to primed neurons with similar semantics. This suggests that models exhibit polysemantic capacities and can blend old and new concepts in individual neurons. While neuron pruning provides interpretability into model concepts, our results highlight the challenges of permanent concept removal for improved model *safety*. Monitoring concept reemergence and developing techniques to mitigate relearning of unsafe concepts will be important directions for more robust model editing. Overall, our work strongly demonstrates the resilience and fluidity of concept representations in LLMs post concept removal.</abstract>
      <url hash="25ebfed3">2024.findings-acl.492</url>
      <bibkey>lo-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.492</doi>
    </paper>
    <paper id="493">
      <title>Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond</title>
      <author><first>Xinyu</first><last>Wang</last><affiliation>University of Warwick</affiliation></author>
      <author><first>Hainiu</first><last>Xu</last></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Yulan</first><last>He</last><affiliation>King’s College London, University of London</affiliation></author>
      <pages>8324-8340</pages>
      <abstract>Task embedding, a meta-learning technique that captures task-specific information, has gained popularity, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradient-free manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To hardness the potential of task embeddings in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables comparison and analysis of similarities amongst different models, broadening the scope and utility of existing task embedding methods in multi-model scenarios, while maintaining their performance comparable to architecture-specific methods.</abstract>
      <url hash="55674bf4">2024.findings-acl.493</url>
      <bibkey>wang-etal-2024-towards-unified</bibkey>
      <doi>10.18653/v1/2024.findings-acl.493</doi>
    </paper>
    <paper id="494">
      <title><fixed-case>TOAD</fixed-case>: Task-Oriented Automatic Dialogs with Diverse Response Styles</title>
      <author><first>Yinhong</first><last>Liu</last></author>
      <author><first>Yimai</first><last>Fang</last><affiliation>Apple</affiliation></author>
      <author><first>David</first><last>Vandyke</last></author>
      <author><first>Nigel</first><last>Collier</last><affiliation>University of Cambridge</affiliation></author>
      <pages>8341-8356</pages>
      <abstract>In light of recent advances in large language models (LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog (TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs (TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users’ expression mirroring. We benchmark TOAD on two response generation tasks, and the results show that modeling more verbose responses or responses without user expression mirroring is more challenging.</abstract>
      <url hash="6ac4245d">2024.findings-acl.494</url>
      <bibkey>liu-etal-2024-toad</bibkey>
      <doi>10.18653/v1/2024.findings-acl.494</doi>
    </paper>
    <paper id="495">
      <title>Machine-Generated Text Localization</title>
      <author><first>Zhongping</first><last>Zhang</last></author>
      <author><first>Wenda</first><last>Qin</last></author>
      <author><first>Bryan</first><last>Plummer</last><affiliation>Boston University</affiliation></author>
      <pages>8357-8371</pages>
      <abstract>Machine-Generated Text (MGT) detection aims to identify a piece of text as machine or human written. Prior work has primarily formulated MGT detection as a binary classification task over an entire document, with limited work exploring cases where only part of a document is machine generated. This paper provides the first in-depth study of MGT that localizes the portions of a document that were machine generated. Thus, if a bad actor were to change a key portion of a news article to spread misinformation, whole document MGT detection may fail since the vast majority is human written, but our approach can succeed due to its granular approach. A key challenge in our MGT localization task is that short spans of text, *e.g.*, a single sentence, provides little information indicating if it is machine generated due to its short length. To address this, we leverage contextual information, where we predict whether multiple sentences are machine or human written at once. This enables our approach to identify changes in style or content to boost performance. A gain of 4-13% mean Average Precision (mAP) over prior work demonstrates the effectiveness of approach on five diverse datasets: GoodNews, VisualNews, WikiText, Essay, and WP. We release our implementation at https://github.com/Zhongping-Zhang/MGT_Localization.</abstract>
      <url hash="07d0ce06">2024.findings-acl.495</url>
      <bibkey>zhang-etal-2024-machine</bibkey>
      <doi>10.18653/v1/2024.findings-acl.495</doi>
    </paper>
    <paper id="496">
      <title><fixed-case>B</fixed-case>ench<fixed-case>IE</fixed-case>^<fixed-case>FL</fixed-case>: A Manually Re-Annotated Fact-Based Open Information Extraction Benchmark</title>
      <author><first>Fabrice</first><last>Lamarche</last></author>
      <author><first>Philippe</first><last>Langlais</last><affiliation>Université de Montréal</affiliation></author>
      <pages>8372-8394</pages>
      <abstract>Open Information Extraction (OIE) is a field of natural language processing that aims to present textual information in a format that allows it to be organized, analyzed and reflected upon. Numerous OIE systems are developed, claiming ever-increasing performance, marking the need for objective benchmarks. BenchIE is the latest reference we know of. Despite being very well thought out, we noticed a number of issues we believe are limiting. Therefore, we propose BenchIE^FL, a new OIE benchmark which fully enforces the principles of BenchIE while containing fewer errors, omissions and shortcomings when candidate facts are matched towards reference ones. BenchIE^FL allows insightful conclusions to be drawn on the actual performance of OIE extractors.</abstract>
      <url hash="e83c213f">2024.findings-acl.496</url>
      <bibkey>lamarche-langlais-2024-benchie</bibkey>
      <doi>10.18653/v1/2024.findings-acl.496</doi>
    </paper>
    <paper id="497">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>C</fixed-case>ite: A Causal Formulation of Paper Citations</title>
      <author><first>Ishan</first><last>Agrawal</last></author>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Ehsan</first><last>Mokhtarian</last><affiliation>Swiss Federal Institute of Technology Lausanne</affiliation></author>
      <author><first>Siyuan</first><last>Guo</last></author>
      <author><first>Yuen</first><last>Chen</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Bernhard</first><last>Schölkopf</last><affiliation>ELLIS Institute and Max Planck Institute for Intelligent Systems, Max-Planck Institute</affiliation></author>
      <pages>8395-8410</pages>
      <abstract>Citation count of a paper is a commonly used proxy for evaluating the significance of a paper in the scientific community. Yet citation measures are widely criticized for failing to accurately reflect the true impact of a paper. Thus, we propose CausalCite, a new way to measure the significance of a paper by assessing the causal impact of the paper on its follow-up papers. CausalCite is based on a novel causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. TextMatch encodes each paper using text embeddings from large language models (LLMs), extracts similar samples by cosine similarity, and synthesizes a counterfactual sample as the weighted average of similar papers according to their similarity values. We demonstrate the effectiveness of CausalCite on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various subfields of AI. We also provide a set of findings that can serve as suggested ways for future researchers to use our metric for a better understanding of the quality of a paper. Our code is available at https://github.com/causalNLP/causal-cite.</abstract>
      <url hash="1688aa81">2024.findings-acl.497</url>
      <bibkey>agrawal-etal-2024-causalcite</bibkey>
      <doi>10.18653/v1/2024.findings-acl.497</doi>
    </paper>
    <paper id="498">
      <title>Question Translation Training for Better Multilingual Reasoning</title>
      <author><first>Wenhao</first><last>Zhu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Fei</first><last>Yuan</last></author>
      <author><first>Shuaijie</first><last>She</last></author>
      <author><first>Jiajun</first><last>Chen</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>8411-8423</pages>
      <abstract>Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of mathematical chain-of-thought. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English parallel question data. In this way we perform targeted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs’ multilingual reasoning abilities. Experimental results on LLaMA2-13B show that question alignment leads to consistent improvements over the translate-training approach: an average improvement of 11.3% and 16.1% accuracy across ten languages on the MGSM and MSVAMP multilingual reasoning benchmarks.</abstract>
      <url hash="a1734f34">2024.findings-acl.498</url>
      <bibkey>zhu-etal-2024-question</bibkey>
      <doi>10.18653/v1/2024.findings-acl.498</doi>
    </paper>
    <paper id="499">
      <title>Improving <fixed-case>LLM</fixed-case> Generations via Fine-Grained Self-Endorsement</title>
      <author><first>Ante</first><last>Wang</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Baolin</first><last>Peng</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Lifeng</first><last>Jin</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Ye</first><last>Tian</last></author>
      <author><first>Haitao</first><last>Mi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Jinsong</first><last>Su</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>8424-8436</pages>
      <abstract>This work studies mitigating fact-conflicting hallucinations for large language model (LLM) at inference time.Particularly, we propose a self-endorsement framework that leverages the fine-grained fact-level comparisons across multiple sampled responses.Compared with prior ensemble methods (e.g., self-consistency) that perform response-level selection, our approach can better alleviate hallucinations for knowledge-intensive tasks.Our approach can broadly benefit smaller and open-source LLMs as it mainly conducts simple content-based comparisons.Experiments on Biographies show that our method can effectively improve the factuality of generations with simple and intuitive prompts across different scales of LLMs.Besides, comprehensive analyses on TriviaQA and GSM8K demonstrate the potential of self-endorsement for broader application.</abstract>
      <url hash="1d842dcc">2024.findings-acl.499</url>
      <bibkey>wang-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.findings-acl.499</doi>
    </paper>
    <paper id="500">
      <title>Multi-Label Classification for Implicit Discourse Relation Recognition</title>
      <author><first>Wanqiu</first><last>Long</last></author>
      <author><first>Siddharth</first><last>N</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Bonnie</first><last>Webber</last><affiliation>Edinburgh University, University of Edinburgh</affiliation></author>
      <pages>8437-8451</pages>
      <abstract>Discourse relations play a pivotal role in establishing coherence within textual content, uniting sentences and clauses into a cohesive narrative. The Penn Discourse Treebank (PDTB) stands as one of the most extensively utilized datasets in this domain. In PDTB-3, the annotators can assign multiple labels to an example, when they believe the simultaneous presence of multiple relations. Prior research in discourse relation recognition has treated these instances as separate examples during training, with a gold-standard prediction matching one of the labels considered correct at test time. However, this approach is inadequate, as it fails to account for the interdependence of labels in real-world contexts and to distinguish between cases where only one sense relation holds and cases where multiple relations hold simultaneously. In our work, we address this challenge by exploring various multi-label classification frameworks to handle implicit discourse relation recognition. We show that the methods for multi-label prediction don’t depress performance for single-label prediction. Additionally, we give comprehensive analysis of results and data. Our work contributes to advancing the understanding and application of discourse relations and provide a foundation for the future study.</abstract>
      <url hash="4f6c0a8e">2024.findings-acl.500</url>
      <bibkey>long-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.500</doi>
    </paper>
    <paper id="501">
      <title><fixed-case>S</fixed-case>tudent<fixed-case>E</fixed-case>val: A Benchmark of Student-Written Prompts for Large Language Models of Code</title>
      <author><first>Hannah</first><last>Babe</last></author>
      <author><first>Sydney</first><last>Nguyen</last><affiliation>Wellesley College</affiliation></author>
      <author><first>Yangtian</first><last>Zi</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Arjun</first><last>Guha</last><affiliation>Roblox Research and Northeastern University</affiliation></author>
      <author><first>Molly</first><last>Feldman</last><affiliation>Oberlin College</affiliation></author>
      <author><first>Carolyn</first><last>Anderson</last><affiliation>Wellesley College</affiliation></author>
      <pages>8452-8474</pages>
      <abstract>Code LLMs have the potential to make it easier for non-experts to understand and write code. However, current CodeLLM benchmarks rely on a single expert-written prompt per problem, making it hard to generalize their success to non-expert users. In this paper, we present a new natural-language-to-code benchmark of prompts written by a key population of non-experts: beginning programmers. StudentEval contains 1,749 prompts written by 80 students who have only completed one introductory Python course. StudentEval contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. We use StudentEval to evaluate 12 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. Our analysis of student prompting strategies reveals that nondeterministic LLM sampling can mislead students about the quality of their descriptions, a finding with key implications for Code LLMs in education.</abstract>
      <url hash="0b9e4d67">2024.findings-acl.501</url>
      <bibkey>babe-etal-2024-studenteval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.501</doi>
    </paper>
    <paper id="502">
      <title><fixed-case>P</fixed-case>ro<fixed-case>L</fixed-case>ex: A Benchmark for Language Proficiency-oriented Lexical Substitution</title>
      <author><first>Xuanming</first><last>Zhang</last><affiliation>Columbia University</affiliation></author>
      <author><first>Zixun</first><last>Chen</last></author>
      <author><first>Zhou</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>8475-8493</pages>
      <abstract>Lexical Substitution discovers appropriate substitutes for a given target word in a context sentence. However, the task fails to consider substitutes that are of equal or higher proficiency than the target, an aspect that could be beneficial for language learners looking to improve their writing. To bridge this gap, we propose a new task — language proficiency-oriented lexical substitution. We also introduce ProLex, a novel benchmark designed to assess systems’ ability to generate not only appropriate substitutes but also substitutes that demonstrate better language proficiency. Besides the benchmark, we propose models that can automatically perform the new task. We show that our best model, a Llama2-13B model fine-tuned with task-specific synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and achieves comparable results with GPT-4 on ProLex.</abstract>
      <url hash="38c6be49">2024.findings-acl.502</url>
      <bibkey>zhang-etal-2024-prolex</bibkey>
      <doi>10.18653/v1/2024.findings-acl.502</doi>
    </paper>
    <paper id="503">
      <title>Generating Diverse and High-Quality Texts by Minimum <fixed-case>B</fixed-case>ayes Risk Decoding</title>
      <author><first>Yuu</first><last>Jinnai</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Ukyo</first><last>Honda</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Tetsuro</first><last>Morimura</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Peinan</first><last>Zhang</last><affiliation>CyberAgent AI Lab</affiliation></author>
      <pages>8494-8525</pages>
      <abstract>One of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse.Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed to generate diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying decoding algorithms. In this paper, we investigate an alternative approach – we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding.We propose two variants of MBR; (i) Diverse MBR (DMBR) that adds a diversity penalty to the decoding objective and (ii) <tex-math>k</tex-math>-medoids MBR (KMBR) that reformulates the decoding task as a clustering problem.We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms overall.</abstract>
      <url hash="6b1eb81b">2024.findings-acl.503</url>
      <bibkey>jinnai-etal-2024-generating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.503</doi>
    </paper>
    <paper id="504">
      <title><fixed-case>GATE</fixed-case> <fixed-case>X</fixed-case>-<fixed-case>E</fixed-case> : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages</title>
      <author><first>Spencer</first><last>Rarrick</last></author>
      <author><first>Ranjita</first><last>Naik</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sundar</first><last>Poudel</last></author>
      <author><first>Vishal</first><last>Chowdhary</last></author>
      <pages>8526-8546</pages>
      <abstract>Neural Machine Translation (NMT) continues to improve in quality and adoption, yet the in advertent perpetuation of gender bias remains a significant concern. Despite numerous studies on gender bias in translations into English from weakly gendered-languages, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present a translation gender rewriting solution built with GPT-4 and use GATE X-E to evaluate it. We open source our contributions to encourage further research on gender debiasing.</abstract>
      <url hash="5890e082">2024.findings-acl.504</url>
      <bibkey>rarrick-etal-2024-gate</bibkey>
      <doi>10.18653/v1/2024.findings-acl.504</doi>
    </paper>
    <paper id="505">
      <title>Hyperparameter-Free Approach for Faster Minimum <fixed-case>B</fixed-case>ayes Risk Decoding</title>
      <author><first>Yuu</first><last>Jinnai</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Kaito</first><last>Ariu</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <pages>8547-8566</pages>
      <abstract>Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to beam search decoding for a wide range of text generation tasks. However, MBR requires a huge amount of time for inference to compute the MBR objective, which makes the method infeasible in many situations where response time is critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently been proposed to reduce the inference time in machine translation tasks. Although it is shown to significantly reduce the amount of computation, it requires hyperparameter tuning using a development set to be effective. To this end, we propose Adaptive Minimum Bayes-Risk (AMBR) decoding, a hyperparameter-free method to run MBR decoding efficiently. AMBR is derived from the observation that the problem of computing the sample-based MBR objective is the medoid identification problem. AMBR uses the Correlated Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the algorithm with the best performance guarantee to date for the medoid identification problem, to compute the sample-based MBR objective. We evaluate AMBR on machine translation, text summarization, and image captioning tasks. The results show that AMBR achieves on par with CBP, with CBP selecting hyperparameters through an Oracle for each given computation budget.</abstract>
      <url hash="fc7e71ed">2024.findings-acl.505</url>
      <bibkey>jinnai-ariu-2024-hyperparameter</bibkey>
      <doi>10.18653/v1/2024.findings-acl.505</doi>
    </paper>
    <paper id="506">
      <title>Simplifying Translations for Children: Iterative Simplification Considering Age of Acquisition with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Masashi</first><last>Oshika</last></author>
      <author><first>Makoto</first><last>Morishita</last><affiliation>Future Corporation and Tohoku University</affiliation></author>
      <author><first>Tsutomu</first><last>Hirao</last><affiliation>NTT Communication Science Laboratories</affiliation></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Koichi</first><last>Takeda</last><affiliation>Nagoya University</affiliation></author>
      <pages>8567-8577</pages>
      <abstract>In recent years, neural machine translation (NMT) has become widely used in everyday life. However, the current NMT lacks a mechanism to adjust the difficulty level of translations to match the user’s language level. Additionally, due to the bias in the training data for NMT, translations of simple source sentences are often produced with complex words. In particular, this could pose a problem for children, who may not be able to understand the meaning of the translations correctly. In this study, we propose a method that replaces high Age of Acquisitions (AoA) words in translations with simpler words to match the translations to the user’s level. We achieve this by using large language models (LLMs), providing a triple of a source sentence, a translation, and a target word to be replaced. We create a benchmark dataset using back-translation on Simple English Wikipedia. The experimental results obtained from the dataset show that our method effectively replaces high-AoA words with lower-AoA words and, moreover, can iteratively replace most of the high-AoA words while still maintaining high BLEU and COMET scores.</abstract>
      <url hash="257c4e36">2024.findings-acl.506</url>
      <bibkey>oshika-etal-2024-simplifying</bibkey>
      <doi>10.18653/v1/2024.findings-acl.506</doi>
    </paper>
    <paper id="507">
      <title>Bi-Chainer: Automated Large Language Models Reasoning with Bidirectional Chaining</title>
      <author><first>Shuqi</first><last>Liu</last></author>
      <author><first>Bowei</first><last>He</last></author>
      <author><first>Linqi</first><last>Song</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>8578-8598</pages>
      <abstract>Large Language Models (LLMs) have shown human-like reasoning abilities but still face challenges in solving complex logical problems. Existing unidirectional chaining methods, such as forward chaining and backward chaining, suffer from issues like low prediction accuracy and efficiency. To address these, we propose a bidirectional chaining method, Bi-Chainer, which dynamically switches to depth-first reasoning in the opposite reasoning direction when it encounters multiple branching options within the current direction. Thus, the intermediate reasoning results can be utilized as guidance to facilitate the reasoning process. We show that Bi-Chainer achieves sizable accuracy boots over unidirectional chaining frameworks on four challenging logical reasoning datasets. Moreover, Bi-Chainer enhances the accuracy of intermediate proof steps and reduces the average number of inference calls, resulting in more efficient and accurate reasoning.</abstract>
      <url hash="98bb9981">2024.findings-acl.507</url>
      <bibkey>liu-etal-2024-bi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.507</doi>
    </paper>
    <paper id="508">
      <title>Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?</title>
      <author><first>Marcio</first><last>Fonseca</last></author>
      <author><first>Shay</first><last>Cohen</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>8599-8618</pages>
      <abstract>In this work, we investigate the controllability of large language models (LLMs) on scientific summarization tasks. We identify key stylistic and content coverage factors that characterize different types of summaries such as paper reviews, abstracts, and lay summaries. By controlling stylistic features, we find that non-fine-tuned LLMs outperform humans in the MuP review generation task, both in terms of similarity to reference summaries and human preferences. Also, we show that we can improve the controllability of LLMs with keyword-based classifier-free guidance (CFG) while achieving lexical overlap comparable to strong fine-tuned baselines on arXiv and PubMed. However, our results also indicate that LLMs cannot consistently generate long summaries with more than 8 sentences. Furthermore, these models exhibit limited capacity to produce highly abstractive lay summaries. Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fine-tuning remains an open problem for domain-specific applications.</abstract>
      <url hash="eec0cf7f">2024.findings-acl.508</url>
      <bibkey>fonseca-cohen-2024-large-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.508</doi>
    </paper>
    <paper id="509">
      <title>Knowledge Context Modeling with Pre-trained Language Models for Contrastive Knowledge Graph Completion</title>
      <author><first>Guangqian</first><last>Yang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yi</first><last>Liu</last><affiliation>State Key Laboratory of Communication Content Cognition</affiliation></author>
      <author><first>Lei</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Licheng</first><last>Zhang</last></author>
      <author><first>Hongtao</first><last>Xie</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>8619-8630</pages>
      <abstract>Text-based knowledge graph completion (KGC) methods utilize pre-trained language models for triple encoding and further fine-tune the model to achieve completion. Despite their excellent performance, they neglect the knowledge context in inferring process. Intuitively, knowledge contexts, which refer to the neighboring triples around the target triples, are important information for triple inferring, since they provide additional detailed information about the entities. To this end, we propose a novel framework named KnowC, which models the knowledge context as additional prompts with pre-trained language models for knowledge graph completion. Given the substantial number of neighbors typically associated with entities, along with the constrained input token capacity of language models, we further devise several strategies to sample the neighbors. We conduct extensive experiments on common datasets FB15k-237, WN18RR and Wikidata5M, experiments show that KnowC achieves state-of-the-art performance.</abstract>
      <url hash="ce807ed3">2024.findings-acl.509</url>
      <bibkey>yang-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.findings-acl.509</doi>
    </paper>
    <paper id="510">
      <title>Stronger, Lighter, Better: Towards Life-Long Attribute Value Extraction for <fixed-case>E</fixed-case>-Commerce Products</title>
      <author><first>Tao</first><last>Zhang</last></author>
      <author><first>Chenwei</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <author><first>Xian</first><last>Li</last><affiliation>Amazon</affiliation></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Hoang</first><last>Nguyen</last></author>
      <author><first>Philip</first><last>Yu</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <pages>8631-8643</pages>
      <abstract>Attribute value extraction involves identifying the value spans of predetermined attributes in product texts. This area of research has traditionally operated under a closed-world assumption, focusing on products from a static set of categories and their associated attributes. However, products in e-commerce stores are ever-increasing and evolving, calling for life-long learning. If continuously trained on the fast-increasing products and attributes, most existing solutions not only struggle for parameter efficiency but also endure foreseeable defects due to data contamination, catastrophic forgetting, etc. As a remedy, we propose and study a new task, which aims to effectively maintain a strong single model for many domains in a life-long learning fashion, without jeopardizing the model performance and parameter efficiency. We introduce factorization into the model and make it domain-aware by decoupling the modeling of product type and attribute, as a way to promote de-contamination and parameter efficiency while scaling up. Tuning the model with distillation prevents forgetting historical knowledge and enables continuous learning from emerging domains. Experiments on hundreds of domains showed that our model attains the near state-of-the-art performance with affordable parameter size, the least historical knowledge forgetting, and the greatest robustness against noises, whilst adding only a few parameters per domain when compared with competitive baselines.</abstract>
      <url hash="e14f4692">2024.findings-acl.510</url>
      <bibkey>zhang-etal-2024-stronger</bibkey>
      <doi>10.18653/v1/2024.findings-acl.510</doi>
    </paper>
    <paper id="511">
      <title>Exploring Domain Robust Lightweight Reward Models based on Router Mechanism</title>
      <author><first>Hyuk</first><last>Namgoong</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Jeesu</first><last>Jung</last></author>
      <author><first>Sangkeun</first><last>Jung</last></author>
      <author><first>YoonHyung</first><last>Roh</last><affiliation>Electronics and Telecommunications Research Institute</affiliation></author>
      <pages>8644-8652</pages>
      <abstract>Recent advancements in large language models have heavily relied on the large reward model from reinforcement learning from human feedback for fine-tuning. However, the use of a single reward model across various domains may not always be optimal, often requiring retraining from scratch when new domain data is introduced. To address these challenges, we explore the utilization of small language models operating in a domain-specific manner based on router mechanisms. Our three approaches are: 1) utilize mixture of experts to form a single reward model by modularizing an internal router and experts, 2) employing external router to select the appropriate reward model from multiple domain-specific models, and 3) the framework reduces parameter size by loading reward models and router adapters onto a single small language model using adapters. Experimental validation underscores the effectiveness of our approach, demonstrating performance comparable to baseline methods while also reducing the total parameter size.</abstract>
      <url hash="31014dea">2024.findings-acl.511</url>
      <bibkey>namgoong-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.511</doi>
    </paper>
    <paper id="512">
      <title>Generalized Category Discovery with Large Language Models in the Loop</title>
      <author><first>Wenbin</first><last>An</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Wenkai</first><last>Shi</last></author>
      <author><first>Feng</first><last>Tian</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Haonan</first><last>Lin</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>QianYing</first><last>Wang</last></author>
      <author><first>Yaqiang</first><last>Wu</last><affiliation>Lenovo Research</affiliation></author>
      <author><first>Mingxiang</first><last>Cai</last></author>
      <author><first>Luyan</first><last>Wang</last></author>
      <author><first>Yan</first><last>Chen</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Haiping</first><last>Zhu</last><affiliation>Xi’an Jiaotong University</affiliation></author>
      <author><first>Ping</first><last>Chen</last><affiliation>University of Massachusetts, Boston</affiliation></author>
      <pages>8653-8665</pages>
      <abstract>Generalized Category Discovery (GCD) is a crucial task that aims to recognize both known and novel categories from a set of unlabeled data by utilizing a few labeled data with only known categories. Due to the lack of supervision and category information, current methods usually perform poorly on novel categories and struggle to reveal semantic meanings of the discovered clusters, which limits their applications in the real world. To mitigate the above issues, we propose Loop, an end-to-end active-learning framework that introduces Large Language Models (LLMs) into the training loop, which can boost model performance and generate category names without relying on any human efforts. Specifically, we first propose Local Inconsistent Sampling (LIS) to select samples that have a higher probability of falling to wrong clusters, based on neighborhood prediction consistency and entropy of cluster assignment probabilities. Then we propose a Scalable Query strategy to allow LLMs to choose true neighbors of the selected samples from multiple candidate samples. Based on the feedback from LLMs, we perform Refined Neighborhood Contrastive Learning (RNCL) to pull samples and their neighbors closer to learn clustering-friendly representations. Finally, we select representative samples from clusters corresponding to novel categories to allow LLMs to generate category names for them. Extensive experiments on three benchmark datasets show that Loop outperforms SOTA models by a large margin and generates accurate category names for the discovered clusters. Code and data are available at https://github.com/Lackel/LOOP.</abstract>
      <url hash="03f82c28">2024.findings-acl.512</url>
      <bibkey>an-etal-2024-generalized</bibkey>
      <doi>10.18653/v1/2024.findings-acl.512</doi>
    </paper>
    <paper id="513">
      <title><fixed-case>VAEGPT</fixed-case>-Sim: Improving Sentence Representation with Limited Corpus Using Gradually-Denoising <fixed-case>VAE</fixed-case></title>
      <author><first>Zhenyi</first><last>Wang</last></author>
      <author><first>Haiyan</first><last>Ning</last></author>
      <author><first>Qing</first><last>Ling</last></author>
      <author><first>Dan</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>8666-8681</pages>
      <abstract>Text embedding requires a highly efficient method for training domain-specific models on limited data, as general models trained on large corpora lack universal applicability in highly specific fields. Therefore, we have introduced VAEGPT-Sim, an innovative model for generating synonyms that combines a denoising variational autoencoder with a target-specific discriminator to generate synonymous sentences that closely resemble human language. Even when trained with completely unsupervised settings, it maintains a harmonious balance between semantic similarity and lexical diversity, as shown by a comprehensive evaluation metric system with the highest average scores compared to other generative models. When VAEGPT-Sim is utilized as a module for contrastive learning in text representation, it delivers state-of-the-art results in small-dataset training on STS benchmarks, surpassing ConSERT by 2.8 points. This approach optimizes the effectiveness of text representation despite a limited corpus, signifying an advancement in domain-specific embedding technology.</abstract>
      <url hash="d4168a66">2024.findings-acl.513</url>
      <bibkey>wang-etal-2024-vaegpt</bibkey>
      <doi>10.18653/v1/2024.findings-acl.513</doi>
    </paper>
    <paper id="514">
      <title><fixed-case>PPTC</fixed-case> Benchmark: Evaluating Large Language Models for <fixed-case>P</fixed-case>ower<fixed-case>P</fixed-case>oint Task Completion</title>
      <author><first>Yiduo</first><last>Guo</last></author>
      <author><first>Zekai</first><last>Zhang</last></author>
      <author><first>Yaobo</first><last>Liang</last></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>8682-8701</pages>
      <abstract>Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs’ ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session, long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems .</abstract>
      <url hash="ff3afd68">2024.findings-acl.514</url>
      <bibkey>guo-etal-2024-pptc</bibkey>
      <doi>10.18653/v1/2024.findings-acl.514</doi>
    </paper>
    <paper id="515">
      <title>Fact-and-Reflection (<fixed-case>F</fixed-case>a<fixed-case>R</fixed-case>) Improves Confidence Calibration of Large Language Models</title>
      <author><first>Xinran</first><last>Zhao</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Xiaoman</first><last>Pan</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Wenlin</first><last>Yao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Tongshuang</first><last>Wu</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Jianshu</first><last>Chen</last><affiliation>Amazon</affiliation></author>
      <pages>8702-8718</pages>
      <abstract>For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored.In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances.Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known “facts” that are relevant to the input prompt from the LLM. And then it asks the model to “reflect” over them to generate the final answer.Experiments show that FaR prompting achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances.</abstract>
      <url hash="88af5c8a">2024.findings-acl.515</url>
      <bibkey>zhao-etal-2024-fact</bibkey>
      <doi>10.18653/v1/2024.findings-acl.515</doi>
    </paper>
    <paper id="516">
      <title><fixed-case>DB</fixed-case>-<fixed-case>LLM</fixed-case>: Accurate Dual-Binarization for Efficient <fixed-case>LLM</fixed-case>s</title>
      <author><first>Hong</first><last>Chen</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Chengtao</first><last>Lv</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Haotong</first><last>Qin</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Xiabin</first><last>Zhou</last></author>
      <author><first>Yifu</first><last>Ding</last><affiliation>Beihang University</affiliation></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technolgy, Shenzhen</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Jinyang</first><last>Guo</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Xianglong</first><last>Liu</last><affiliation>Beihang University</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>University of Sydney</affiliation></author>
      <pages>8719-8730</pages>
      <abstract>Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically investigate the micro and macro characteristics of ultra-low bit quantization and present a novel <b>D</b>ual-<b>B</b>inarization method for <b>LLM</b>s, namely <b>DB-LLM</b>. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing <i>Flexible Dual Binarization</i> (<b>FDB</b>). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while retaining the inherent high sparsity of ultra-low bit quantization. For the macro-level, we find the distortion that exists in the prediction of LLM after quantization, which is specified as the deviations related to the ambiguity of samples. We propose the <i>Deviation-Aware Distillation</i> (<b>DAD</b>) method, enabling the model to focus differently on various samples. Comprehensive experiments show that our DB-LLM not only significantly surpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization (, perplexity decreased from 9.64 to 7.23), but also achieves an additional 20% reduction in computational consumption compared to the SOTA method under the same bit-width. Our code is available at https://github.com/Hon-Chen/DB-LLM.</abstract>
      <url hash="0086e4e0">2024.findings-acl.516</url>
      <bibkey>chen-etal-2024-db</bibkey>
      <doi>10.18653/v1/2024.findings-acl.516</doi>
    </paper>
    <paper id="517">
      <title><fixed-case>T</fixed-case>emp<fixed-case>C</fixed-case>ompass: Do Video <fixed-case>LLM</fixed-case>s Really Understand Videos?</title>
      <author><first>Yuanxin</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Shicheng</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Yi</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Yuxiang</first><last>Wang</last></author>
      <author><first>Shuhuai</first><last>Ren</last></author>
      <author><first>Lei</first><last>Li</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Sishuo</first><last>Chen</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Lu</first><last>Hou</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <pages>8731-8772</pages>
      <abstract>Recently, there is a surge in interest surrounding video large language models (Video LLMs). However, existing benchmarks fail to provide a comprehensive feedback on the temporal perception ability of Video LLMs. On the one hand, most of them are unable to distinguish between different temporal aspects (e.g., speed, direction) and thus cannot reflect the nuanced performance on these specific aspects. On the other hand, they are limited in the diversity of task formats (e.g., only multi-choice QA), which hinders the understanding of how temporal perception performance may vary across different types of tasks. Motivated by these two problems, we propose the <b>TempCompass</b> benchmark, which introduces a diversity of temporal aspects and task formats. To collect high-quality test data, we devise two novel strategies: (1) In video collection, we construct conflicting videos that share the same static content but differ in a specific temporal aspect, which prevents Video LLMs from leveraging single-frame bias or language priors. (2) To collect the task instructions, we propose a paradigm where humans first annotate meta-information for a video and then an LLM generates the instruction. We also design an LLM-based approach to automatically and accurately evaluate the responses from Video LLMs. Based on TempCompass, we comprehensively evaluate 9 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs, and reveal the discerning fact that these models exhibit notably poor temporal perception ability.</abstract>
      <url hash="cf8f047d">2024.findings-acl.517</url>
      <bibkey>liu-etal-2024-tempcompass</bibkey>
      <doi>10.18653/v1/2024.findings-acl.517</doi>
    </paper>
    <paper id="518">
      <title>“Get Their Hands Dirty, Not Mine”: On Researcher-Annotator Collaboration and the Agency of Annotators</title>
      <author><first>Shengqi</first><last>Zhu</last><affiliation>Cornell University</affiliation></author>
      <author><first>Jeffrey</first><last>Rzeszotarski</last><affiliation>Cornell University</affiliation></author>
      <pages>8773-8782</pages>
      <abstract>Annotation quality is often framed as post-hoc cleanup of annotator-caused issues. This position paper discusses whether, how, and why this narrative limits the scope of improving annotation. We call to consider annotation as a procedural collaboration, outlining three points in this direction:(1) An issue can be either annotator- or researcher-oriented, where one party is accountable and the other party may lack ability to fix it; (2) yet, they can co-occur or have similar consequences, and thus any specific problem we encounter may be a combination;(3) therefore, we need a new language to capture the nuance and holistically describe the full procedure to resolve these issues.To that end, we propose to study how agency is manifested in annotation and picture how this perspective benefits the community more broadly.</abstract>
      <url hash="6215f8fd">2024.findings-acl.518</url>
      <bibkey>zhu-rzeszotarski-2024-get</bibkey>
      <doi>10.18653/v1/2024.findings-acl.518</doi>
    </paper>
    <paper id="519">
      <title>Teaching Large Language Models an Unseen Language on the Fly</title>
      <author><first>Chen</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiao</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Jiuheng</first><last>Lin</last></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>8783-8800</pages>
      <abstract>Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones, for which there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce DiPMT++, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and 5K parallel sentences only, DiPMT++ significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. We also validate the effectiveness of our framework on Kalamang, another unseen language. Furthermore, we demonstrate the practical utility of DiPMT++ in aiding humans in translating completely unseen languages, which could contribute to the preservation of linguistic diversity.</abstract>
      <url hash="eb96bc81">2024.findings-acl.519</url>
      <bibkey>zhang-etal-2024-teaching</bibkey>
      <doi>10.18653/v1/2024.findings-acl.519</doi>
    </paper>
    <paper id="520">
      <title>Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models</title>
      <author><first>Qingyu</first><last>Lu</last></author>
      <author><first>Baopu</first><last>Qiu</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Kanjian</first><last>Zhang</last><affiliation>Schools of Automation, Southeast University</affiliation></author>
      <author><first>Tom</first><last>Kocmi</last><affiliation>Microsoft</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>University of Sydney</affiliation></author>
      <pages>8801-8816</pages>
      <abstract>Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level. To further improve the performance of LLMs on MT quality assessment, we conduct an investigation into several prompting designs, and propose a new prompting method called Error Analysis Prompting (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al., (2021)) and produces explainable and reliable MT evaluations at both the system and segment level. Experimental Results from WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation. We will release our code and scripts to facilitate the community.</abstract>
      <url hash="62bed5a0">2024.findings-acl.520</url>
      <bibkey>lu-etal-2024-error</bibkey>
      <doi>10.18653/v1/2024.findings-acl.520</doi>
    </paper>
    <paper id="521">
      <title><fixed-case>GAOKAO</fixed-case>-<fixed-case>MM</fixed-case>: A <fixed-case>C</fixed-case>hinese Human-Level Benchmark for Multimodal Models Evaluation</title>
      <author><first>Yi</first><last>Zong</last></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>8817-8825</pages>
      <abstract>The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing datasets either focus solely on primary perception abilities and commonsense knowledge, or have a low level of text comprehension difficulty, which are insufficient to reflect the comprehensive capabilities of LVLMs, particularly in terms of Chinese language proficiency. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model’s abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vision (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moderate distance towards Artificial General Intelligence (AGI) and provide insights facilitating the development of multilingual LVLMs. The dataset and evaluation code are available through: https://github.com/OpenMOSS/GAOKAO-MM</abstract>
      <url hash="3b459fed">2024.findings-acl.521</url>
      <bibkey>zong-qiu-2024-gaokao</bibkey>
      <doi>10.18653/v1/2024.findings-acl.521</doi>
    </paper>
    <paper id="522">
      <title><fixed-case>D</fixed-case>iff<fixed-case>C</fixed-case>hat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation</title>
      <author><first>Jiapeng</first><last>Wang</last></author>
      <author><first>Chengyu</first><last>Wang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Tingfeng</first><last>Cao</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Lianwen</first><last>Jin</last><affiliation>South China University of Technology</affiliation></author>
      <pages>8826-8840</pages>
      <abstract>We present DiffChat, a novel method to align Large Language Models (LLMs) to “chat” with prompt-as-input Text-to-Image Synthesis (TIS)models (e.g., Stable Diffusion) for interactive image creation. Given a raw prompt/image and a user-specified instruction, DiffChat can effectively make appropriate modifications and generate the target prompt, which can be leveraged to create the target image of high quality. To achieve this, we first collect an instruction-following prompt engineering dataset named InstructPE for the supervised training of DiffChat.Next, we propose a reinforcement learning framework with the feedback of three core criteria for image creation, i.e., aesthetics, user preference and content integrity. It involves an action-space dynamic modification technique to obtain more relevant positive samples and harder negative samples during the off-policy sampling. Content integrity is also introduced into the value estimation function for further improvement of produced images. Our method can exhibit superior performance than baseline models and strong competitors based on both automatic and human evaluations, which fully demonstrates its effectiveness.</abstract>
      <url hash="7d36a4b6">2024.findings-acl.522</url>
      <bibkey>wang-etal-2024-diffchat</bibkey>
      <doi>10.18653/v1/2024.findings-acl.522</doi>
    </paper>
    <paper id="523">
      <title>Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration</title>
      <author><first>Kejuan</first><last>Yang</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Kaiwen</first><last>Men</last></author>
      <author><first>Aohan</first><last>Zeng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yuxiao</first><last>Dong</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Jie</first><last>Tang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>8841-8852</pages>
      <abstract>We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing window-wise attention and positional embedding techniques. We first show that a simple yet strong baseline, weighted sum ensemble, is missing for the in-context few-shot classification. Moreover, on more challenging Chain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected deterioration regarding question miscomprehension and false inference. Based on our findings, we suggest that the existing PCW design may not guarantee sufficient improvement and practicality in handling lengthy documents in real-world applications. More community efforts on enabling language models’ long context understanding ability should be paid.</abstract>
      <url hash="419ae122">2024.findings-acl.523</url>
      <bibkey>yang-etal-2024-revisiting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.523</doi>
    </paper>
    <paper id="524">
      <title>Rationales for Answers to Simple Math Word Problems Confuse Large Language Models</title>
      <author><first>Yidan</first><last>Zhang</last></author>
      <author><first>Mingfeng</first><last>Xue</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Dayiheng</first><last>Liu</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Zhenan</first><last>He</last><affiliation>Sichuan University</affiliation></author>
      <pages>8853-8869</pages>
      <abstract>Recently, large language models (LLMs) have demonstrated breakthrough mathematical problem-solving capabilities in grade school math word problems (MWP). For example, on the MWP benchmark GSM8K, the accuracy of GPT-3.5-Turbo and MetaMath-70B reaches 80.80% and 82.30%, respectively. One question arises, does it mean that LLMs have truly mastered related mathematical problem-solving abilities? In this paper, by presenting two types of benchmarks, where MCGSM8K aims at selecting one correct solution from four solutions, while GSM8K-Judgement judges whether a solution to a given question is true or false, we demonstrate that the ability of most LLMs to evaluate the mathematical reasoning process of MWP is far from sufficient. To compensate for this issue, we propose hybrid supervised fine-tuning data from the training data of GSM8K, MCGSM8K, and GSM8K-Judgement, which significantly improves performance on the proposed reasoning process evaluation benchmarks. For example, fine-tuning improves the performance of LLaMA-2-13B from 33.51% to 70.89% on MCGSM8K. In conclusion, we experimentally demonstrate that most LLMs have limited ability to evaluate the mathematical reasoning process of MWP, which can be enhanced through fine-tuning.</abstract>
      <url hash="c31c39c7">2024.findings-acl.524</url>
      <bibkey>zhang-etal-2024-rationales</bibkey>
      <doi>10.18653/v1/2024.findings-acl.524</doi>
    </paper>
    <paper id="525">
      <title><fixed-case>R</fixed-case>es<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Identity Residual Mapping in Low-Rank Adaption</title>
      <author><first>Shuhua</first><last>Shi</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Minghui</first><last>Song</last><affiliation>Microsoft</affiliation></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Haizhen</first><last>Huang</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Weiwei</first><last>Deng</last></author>
      <author><first>Feng</first><last>Sun</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>8870-8884</pages>
      <abstract>As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at [this url](https://github.com/microsoft/LMOps/tree/main/reslora).</abstract>
      <url hash="73aaf1cb">2024.findings-acl.525</url>
      <bibkey>shi-etal-2024-reslora</bibkey>
      <doi>10.18653/v1/2024.findings-acl.525</doi>
    </paper>
    <paper id="526">
      <title>Towards Objectively Benchmarking Social Intelligence of Language Agents at the Action Level</title>
      <author><first>Chenxu</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Bin</first><last>Dai</last><affiliation>XiaoIce</affiliation></author>
      <author><first>Huaping</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Baoyuan</first><last>Wang</last><affiliation>Xiaobing.ai</affiliation></author>
      <pages>8885-8897</pages>
      <abstract>Prominent large language models have exhibited human-level performance in many domains, even enabling the derived agents to simulate human and social interactions. While practical works have substantiated the practicability of grounding language agents in sandbox simulation or embodied simulators, current social intelligence benchmarks either stay at the language level or use subjective metrics. In pursuit of a more realistic and objective evaluation, we introduce the Social Tasks in Sandbox Simulation (STSS) benchmark, which assesses language agents objectively at the action level by scrutinizing the goal achievements within the multi-agent simulation.Additionally, we sample conversation scenarios to build a language-level benchmark to provide an economically prudent preliminary evaluation and align with prevailing benchmarks. To gauge the significance of agent architecture, we implement a target-driven planning (TDP) module as an adjunct to the existing agent. Our evaluative findings highlight that the STSS benchmark is challenging for state-of-the-art language agents. Furthermore, it effectively discriminates between distinct language agents, suggesting its usefulness as a benchmark for evaluating both language models and agent architectures. Our code is available at https://github.com/wcx21/Social-Tasks-in-Sandbox-Simulation.</abstract>
      <url hash="c8014ffc">2024.findings-acl.526</url>
      <bibkey>wang-etal-2024-towards-objectively</bibkey>
      <doi>10.18653/v1/2024.findings-acl.526</doi>
    </paper>
    <paper id="527">
      <title>Semantic Role Labeling from <fixed-case>C</fixed-case>hinese Speech via End-to-End Learning</title>
      <author><first>Huiyao</first><last>Chen</last></author>
      <author><first>Xinxin</first><last>Li</last></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China and Tianjin University, China</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>8898-8911</pages>
      <abstract>Semantic Role Labeling (SRL), crucial for understanding semantic relationships in sentences, has traditionally focused on text-based input. However, the increasing use of voice assistants and the need for hands-free interaction have highlighted the importance of SRL from speech.SRL from speech can be accomplished via a two-step pipeline directly: transcribing speech to text via Automatic Speech Recognition (ASR) and then applying text-based SRL, which could lead to error propagation and loss of useful acoustic features.Addressing these challenges, we present the first end-to-end approach for SRL from speech, integrating ASR and SRL in a joint-learning framework, focusing on the Chinese language. By employing a Stright-Through Gumbel-Softmax module for connecting ASR and SRL models, it enables gradient back-propagation and joint optimization, enhancing robustness and effectiveness.Experiments on the Chinese Proposition Bank 1.0 (CPB1.0) and a newly annotated dataset AS-SRL based on AISHELL-1 demonstrate the superiority of the end-to-end model over traditional pipelines, with significantly improved performance.</abstract>
      <url hash="63543793">2024.findings-acl.527</url>
      <bibkey>chen-etal-2024-semantic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.527</doi>
    </paper>
    <paper id="528">
      <title><fixed-case>MEEL</fixed-case>: Multi-Modal Event Evolution Learning</title>
      <author><first>Zhengwei</first><last>Tao</last></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University and Peking University</affiliation></author>
      <author><first>Junqiang</first><last>Huang</last><affiliation>VIPSHOP</affiliation></author>
      <author><first>Xiancai</first><last>Chen</last></author>
      <author><first>Xiaoying</first><last>Bai</last></author>
      <author><first>Yifan</first><last>Zhang</last></author>
      <author><first>Chongyang</first><last>Tao</last><affiliation>Beihang University</affiliation></author>
      <pages>8912-8925</pages>
      <abstract>Multi-modal Event Reasoning (MMER) endeavors to endow machines with the ability to comprehend intricate event relations across diverse data modalities. MMER is fundamental and underlies a wide broad of applications. Despite extensive instruction fine-tuning, current multi-modal large language models still fall short in such ability. The disparity stems from that existing models are insufficient to capture underlying principles governing event evolution in various scenarios. In this paper, we introduce Multi-Modal Event Evolution Learning (MEEL) to enable the model to grasp the event evolution mechanism yielding advanced MMER ability. Specifically, we commence with the design of event diversification to gather seed events from a rich spectrum of scenarios. Subsequently, we employ ChatGPT to generate evolving graphs for these seed events. We propose an instruction encapsulation process that formulates the evolving graphs into instruction-tuning data, aligning the comprehension of event reasoning to humans. Finally, we observe that models trained in this way are still struggling to fully comprehend event evolution. In such a case, we propose the guiding discrimination strategy, in which models are trained to discriminate the improper evolution direction. We collect and curate a benchmark M-EV2 for MMER. Extensive experiments on M-EV2 validate the effectiveness of our approach, showcasing competitive performance in open-source multi-modal LLMs.</abstract>
      <url hash="264e90aa">2024.findings-acl.528</url>
      <bibkey>tao-etal-2024-meel</bibkey>
      <doi>10.18653/v1/2024.findings-acl.528</doi>
    </paper>
    <paper id="529">
      <title><fixed-case>LLM</fixed-case>-<fixed-case>REDIAL</fixed-case>: A Large-Scale Dataset for Conversational Recommender Systems Created from User Behaviors with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Tingting</first><last>Liang</last><affiliation>Hangzhou Dianzi University</affiliation></author>
      <author><first>Chenxin</first><last>Jin</last></author>
      <author><first>Lingzhi</first><last>Wang</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Wenqi</first><last>Fan</last></author>
      <author><first>Congying</first><last>Xia</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Kai</first><last>Chen</last></author>
      <author><first>Yuyu</first><last>Yin</last></author>
      <pages>8926-8939</pages>
      <abstract>The large-scale conversational recommendation dataset is pivotal for the development of conversational recommender systems (CRS). Most existing CRS datasets suffers from the problems of data inextensibility and semantic inconsistency. To tackle these limitations and establish a benchmark in the conversational recommendation scenario, in this paper, we introduce the LLM-REDIAL dataset to facilitate the research in CRS. LLM-REDIAL is constructed by leveraging large language models (LLMs) to generate the high-quality dialogues. To provide the LLMs with detailed guidance, we integrate historical user behavior data with dialogue templates that are carefully designed through the combination of multiple pre-defined goals. LLM-REDIAL has two main advantages. First, it is the largest multi-domain CRS dataset which consists of 47.6k multi-turn dialogues with 482.6k utterances across 4 domains. Second, dialogue semantics and the users’ historical interaction information is highly consistent. Human evaluation are conducted to verify the quality of LLM-REDIAL. In addition, we evaluate the usability of advanced LLM-based models on LLM-REDIAL.</abstract>
      <url hash="c4f153d1">2024.findings-acl.529</url>
      <bibkey>liang-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.529</doi>
    </paper>
    <paper id="530">
      <title>Investigating Subtler Biases in <fixed-case>LLM</fixed-case>s: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models</title>
      <author><first>Mahammed</first><last>Kamruzzaman</last><affiliation>University of South Florida</affiliation></author>
      <author><first>Md.</first><last>Shovon</last><affiliation>Rajshahi University of Engineering and Technology</affiliation></author>
      <author><first>Gene</first><last>Kim</last><affiliation>University of South Florida</affiliation></author>
      <pages>8940-8965</pages>
      <abstract>LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks introducing LLM biases into consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less-studied but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs make between social groups and unrelated positive and negative attributes. Although these subtler biases are understudied they follow people as much as gender and ethnicity do. So, we want to see whether they also follow one with LLMs.We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group. We also reverse the completion task to select the social group based on an attribute. We report the correlations that we find for 4 cutting-edge LLMs. This dataset can be used as a benchmark to evaluate progress in more generalized biases and the templating technique can be used to expand the benchmark with minimal additional human annotation.</abstract>
      <url hash="6f9e672f">2024.findings-acl.530</url>
      <bibkey>kamruzzaman-etal-2024-investigating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.530</doi>
    </paper>
    <paper id="531">
      <title><fixed-case>EVIT</fixed-case>: Event-Oriented Instruction Tuning for Event Reasoning</title>
      <author><first>Zhengwei</first><last>Tao</last></author>
      <author><first>Xiancai</first><last>Chen</last></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University and Peking University</affiliation></author>
      <author><first>Xiaoying</first><last>Bai</last></author>
      <author><first>Haiyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Yiwei</first><last>Lou</last></author>
      <pages>8966-8979</pages>
      <abstract>Events refer to specific occurrences, incidents, or happenings that take place under a particular background. Event reasoning aims to infer events according to certain relations and predict future events. The cutting-edge techniques for event reasoning play a crucial role in various natural language processing applications. Large language models (LLMs) have made significant advancements in event reasoning owing to their wealth of knowledge and reasoning capabilities. However, smaller instruction-tuned models currently in use do not consistently demonstrate exceptional proficiency in managing these tasks. This discrepancy arises from the absence of explicit modeling of events and the interconnections of them within their instruction data. Consequently, these models face challenges in comprehending event structures and semantics while struggling to bridge the gap between their interpretations and human understanding of events. Additionally, their limitations in grasping event relations lead to constrained event reasoning abilities to effectively deduce and incorporate pertinent event knowledge. In this paper, we propose Event-Oriented Instruction Tuning to train our large language model named EvIT specializing in event reasoning tasks. Specifically, we first propose a novel structure named event quadruple which contains the structure and semantics of events and is complete in the event representation. We then design event-relation learning based on the structures. We encapsulate the learning into the instruction-tuning formulation to better stimulate the event reasoning capacity of our model. To implement our training, we design a heuristic unsupervised method to mine event quadruple from a large-scale corpus. At last, we finetune a Llama model on our Event-Oriented Instruction Tuning. We conduct extensive experiments on event reasoning tasks on several datasets. Automatic and human evaluations demonstrate EvIT achieves competitive performances on event reasoning.</abstract>
      <url hash="3d0f081b">2024.findings-acl.531</url>
      <bibkey>tao-etal-2024-evit</bibkey>
      <doi>10.18653/v1/2024.findings-acl.531</doi>
    </paper>
    <paper id="532">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>CMP</fixed-case>: Length Control in Sentence Compression through Instruction-based Large Language Models</title>
      <author><first>Juseon-Do</first><last>Juseon-Do</last></author>
      <author><first>Jingun</first><last>Kwon</last><affiliation>Chungnam National University</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Division of Information Science, Nara Institute of Science and Technology</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>8980-8996</pages>
      <abstract>Extractive summarization can produce faithful summaries but often requires additional constraints such as a desired summary length. Traditional sentence compression models do not typically consider the constraints because of their restricted model abilities, which require model modifications for coping with them. To bridge this gap, we propose Instruction-based Compression (InstructCMP), an approach to the sentence compression task that can consider the length constraint through instructions by leveraging the zero-shot task-solving abilities of Large Language Models (LLMs). For this purpose, we created new evaluation datasets by transforming traditional sentence compression datasets into an instruction format. By using the datasets, we first reveal that the current LLMs still face challenges in accurately controlling the length for a compressed text. To address this issue, we propose an approach named length priming, that incorporates additional length information into the instructions without external resources. While the length priming effectively works in a zero-shot setting, a training dataset with the instructions would further improve the ability of length control. Thus, we additionally created a training dataset in an instruction format to fine-tune the model on it. Experimental results and analysis show that applying the length priming significantly improves performances of InstructCMP in both zero-shot and fine-tuning settings without the need of any model modifications.</abstract>
      <url hash="1800f738">2024.findings-acl.532</url>
      <bibkey>juseon-do-etal-2024-instructcmp</bibkey>
      <doi>10.18653/v1/2024.findings-acl.532</doi>
    </paper>
    <paper id="533">
      <title><fixed-case>S</fixed-case>ym<fixed-case>T</fixed-case>ax: Symbiotic Relationship and Taxonomy Fusion for Effective Citation Recommendation</title>
      <author><first>Karan</first><last>Goyal</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Mayank</first><last>Goel</last></author>
      <author><first>Vikram</first><last>Goyal</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Mukesh</first><last>Mohania</last><affiliation>Indraprastha Institute of Information Technology</affiliation></author>
      <pages>8997-9008</pages>
      <abstract>Citing pertinent literature is pivotal to writing and reviewing a scientific document. Existing techniques mainly focus on the local context or the global context for recommending citations but fail to consider the actual human citation behaviour. We propose SymTax, a three-stage recommendation architecture that considers both the local and the global context, and additionally the taxonomical representations of query-candidate tuples and the Symbiosis prevailing amongst them. SymTax learns to embed the infused taxonomies in the hyperbolic space and uses hyperbolic separation as a latent feature to compute query-candidate similarity. We build a novel and large dataset ArSyTa containing 8.27 million citation contexts and describe the creation process in detail. We conduct extensive experiments and ablation studies to demonstrate the effectiveness and design choice of each module in our framework. Also, combinatorial analysis from our experiments shed light on the choice of language models (LMs) and fusion embedding, and the inclusion of section heading as a signal. Our proposed module that captures the symbiotic relationship solely leads to performance gains of 26.66% and 39.25% in Recall@5 w.r.t. SOTA on ACL-200 and RefSeer datasets, respectively. The complete framework yields a gain of 22.56% in Recall@5 wrt SOTA on our proposed dataset. The code and dataset are available at https://github.com/goyalkaraniit/SymTax.</abstract>
      <url hash="9ec27d99">2024.findings-acl.533</url>
      <bibkey>goyal-etal-2024-symtax</bibkey>
      <doi>10.18653/v1/2024.findings-acl.533</doi>
    </paper>
    <paper id="534">
      <title>Assessing News Thumbnail Representativeness: Counterfactual text can enhance the cross-modal matching ability</title>
      <author><first>Yejun</first><last>Yoon</last><affiliation>Soongsil University</affiliation></author>
      <author><first>Seunghyun</first><last>Yoon</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Kunwoo</first><last>Park</last><affiliation>Soongsil University</affiliation></author>
      <pages>9009-9024</pages>
      <abstract>This paper addresses the critical challenge of assessing the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the actors discussed in the news text. To serve the challenge, we introduce NewsTT, a manually annotated dataset of 1000 news thumbnail images and text pairs. We found that the pretrained vision and language models, such as BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, the pretrained models could have a limited capability to match news actors’ visual and textual appearances. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross-modal matching ability of vision and language models. We propose CFT-CLIP, a contrastive learning framework that updates vision and language bi-encoders according to the hypothesis. We found that our simple method can boost the performance for assessing news thumbnail representativeness, supporting our assumption. Code and data can be accessed at https://github.com/ssu-humane/news-images-acl24.</abstract>
      <url hash="a5fd4b1d">2024.findings-acl.534</url>
      <bibkey>yoon-etal-2024-assessing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.534</doi>
    </paper>
    <paper id="535">
      <title>Towards Better Question Generation in <fixed-case>QA</fixed-case>-based Event Extraction</title>
      <author><first>Zijin</first><last>Hong</last></author>
      <author><first>Jian</first><last>Liu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>9025-9038</pages>
      <abstract>Event Extraction (EE) is an essential information extraction task that aims to extract event-related information from unstructured texts.The paradigm of this task has shifted from conventional classification-based methods to more contemporary question-answering-based (QA-based) approaches. However, in QA-based EE, the quality of the questions dramatically affects the extraction accuracy, and how to generate high-quality questions for QA-based EE remains a challenge. In this work, to tackle this challenge, we suggest four criteria to evaluate the quality of a question and propose a reinforcement learning method, RLQG, for QA-based EE that can generate generalizable, high-quality, and context-dependent questions and provides clear guidance to QA models. The extensive experiments conducted on ACE and RAMS datasets have strongly validated our approach’s effectiveness, which also demonstrates its robustness in scenarios with limited training data. The corresponding code of RLQG is released for further research.</abstract>
      <url hash="7711a44f">2024.findings-acl.535</url>
      <bibkey>hong-liu-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-acl.535</doi>
    </paper>
    <paper id="536">
      <title>Budget-Constrained Tool Learning with Planning</title>
      <author><first>Yuanhang</first><last>Zheng</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>9039-9052</pages>
      <abstract>Despite intensive efforts devoted to tool learning, the problem of budget-constrained tool learning, which focuses on resolving user queries within a specific budget constraint, has been widely overlooked. This paper proposes a novel method for budget-constrained tool learning. Our approach involves creating a preferable plan under the budget constraint before utilizing the tools. This plan outlines the feasible tools and the maximum number of times they can be employed, offering a comprehensive overview of the tool learning process for large language models. This allows them to allocate the budget from a broader perspective. To devise the plan without incurring significant extra costs, we suggest initially estimating the usefulness of the candidate tools based on past experience. Subsequently, we employ dynamic programming to formulate the plan. Experimental results demonstrate that our method can be integrated with various tool learning methods, significantly enhancing their effectiveness under strict budget constraints.</abstract>
      <url hash="0e56dbcc">2024.findings-acl.536</url>
      <bibkey>zheng-etal-2024-budget</bibkey>
      <doi>10.18653/v1/2024.findings-acl.536</doi>
    </paper>
    <paper id="537">
      <title><fixed-case>T</fixed-case>ext<fixed-case>B</fixed-case>ind: Multi-turn Interleaved Multimodal Instruction-following in the Wild</title>
      <author><first>Huayang</first><last>Li</last></author>
      <author><first>Siheng</first><last>Li</last></author>
      <author><first>Deng</first><last>Cai</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Lemao</first><last>Liu</last><affiliation>Tencent</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Graduate School at Shenzhen,Tsinghua University</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>9053-9076</pages>
      <abstract>Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering LLMs with multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. To accommodate interleaved image-text inputs and outputs, we devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models. Extensive quantitative and qualitative experiments demonstrate that MIM trained on TextBind achieves remarkable generation capability in multimodal conversations compared to recent baselines.</abstract>
      <url hash="6eb03d26">2024.findings-acl.537</url>
      <bibkey>li-etal-2024-textbind</bibkey>
      <doi>10.18653/v1/2024.findings-acl.537</doi>
    </paper>
    <paper id="538">
      <title>The Critique of Critique</title>
      <author><first>Shichao</first><last>Sun</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Junlong</first><last>Li</last></author>
      <author><first>Weizhe</first><last>Yuan</last></author>
      <author><first>Ruifeng</first><last>Yuan</last></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>9077-9096</pages>
      <abstract>Critique, as a natural language description for assessing the quality of model-generated content, has played a vital role in the training, evaluation, and refinement of LLMs. However, a systematic method to evaluate the quality of critique is lacking. In this paper, we pioneer the critique of critique, termed <tex-math>\textbf{MetaCritique}</tex-math>, which builds specific quantification criteria. To achieve a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique aggregates each AIU’s judgment for the overall score. Moreover, MetaCritique delivers a natural language rationale for the intricate reasoning within each judgment. Lastly, we construct a meta-evaluation dataset covering 4 tasks across 16 public datasets involving human-written and LLM-generated critiques. Experiments demonstrate that MetaCritique can achieve near-human performance. Our study can facilitate future research in LLM critiques based on our following observations and released resources: (1) superior critiques judged by MetaCritique can lead to better refinements, indicating that it can potentially enhance the alignment of existing LLMs; (2) the leaderboard of critique models reveals that open-source critique models commonly suffer from factuality issues; (3) relevant code and data are publicly available at https://anonymous.4open.science/r/MetaCritique-ARR/ to support deeper exploration; (4) an <tex-math>\textbf{API}</tex-math> at PyPI with the usage documentation in Appendix C allows users to assess the critique conveniently.</abstract>
      <url hash="eef412a3">2024.findings-acl.538</url>
      <bibkey>sun-etal-2024-critique</bibkey>
      <doi>10.18653/v1/2024.findings-acl.538</doi>
    </paper>
    <paper id="539">
      <title><fixed-case>C</fixed-case>o<fixed-case>C</fixed-case>o-Agent: A Comprehensive Cognitive <fixed-case>MLLM</fixed-case> Agent for Smartphone <fixed-case>GUI</fixed-case> Automation</title>
      <author><first>Xinbei</first><last>Ma</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>9097-9110</pages>
      <abstract>Multimodal large language models (MLLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation.However, those GUI agents require comprehensive cognition including exhaustive perception and reliable action response.We propose a Comprehensive Cognitive LLM Agent, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel.Second, CAP decomposes the action prediction into sub-problems: determining the action type and then identifying the action target conditioned on the action type.With our technical design, our agent achieves state-of-the-art performance on AITW and META-GUI benchmarks, showing promising abilities in realistic scenarios. Code is available at <url>https://github.com/xbmxb/CoCo-Agent</url>.</abstract>
      <url hash="e1ab96f1">2024.findings-acl.539</url>
      <bibkey>ma-etal-2024-coco</bibkey>
      <doi>10.18653/v1/2024.findings-acl.539</doi>
    </paper>
    <paper id="540">
      <title><fixed-case>FRVA</fixed-case>: Fact-Retrieval and Verification Augmented Entailment Tree Generation for Explainable Question Answering</title>
      <author><first>Yue</first><last>Fan</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Hu</first><last>Zhang</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Ru</first><last>Li</last><affiliation>Shanxi University</affiliation></author>
      <author><first>YuJie</first><last>Wang</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Hongye</first><last>Tan</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Jiye</first><last>Liang</last><affiliation>Shanxi University</affiliation></author>
      <pages>9111-9128</pages>
      <abstract>Structured entailment tree can exhibit the reasoning chains from knowledge facts to predicted answers, which is important for constructing an explainable question answering system. Existing works mainly include directly generating the entire tree and stepwise generating the proof steps. The stepwise methods can exploit combinatoriality and generalize to longer steps, but they have large fact search spaces and error accumulation problems resulting in the generation of invalid steps. In this paper, inspired by the Dual Process Theory in cognitive science, we propose FRVA, a Fact-Retrieval and Verification Augmented bidirectional entailment tree generation method that contains two systems. Specifically, System 1 makes intuitive judgments through the fact retrieval module and filters irrelevant facts to reduce the search space. System 2 designs a deductive-abductive bidirectional reasoning module, and we construct cross-verification and multi-view contrastive learning to make the generated proof steps closer to the target hypothesis. We enhance the reliability of the stepwise proofs to mitigate error propagation. Experiment results on EntailmentBank show that FRVA outperforms previous models and achieves state-of-the-art performance in fact selection and structural correctness.</abstract>
      <url hash="2a3776ed">2024.findings-acl.540</url>
      <bibkey>fan-etal-2024-frva</bibkey>
      <doi>10.18653/v1/2024.findings-acl.540</doi>
    </paper>
    <paper id="541">
      <title>P4: Plug-and-Play Discrete Prompting for Large Language Models Personalization</title>
      <author><first>Yuansen</first><last>Zhang</last></author>
      <author><first>Xiao</first><last>Wang</last></author>
      <author><first>Tianze</first><last>Chen</last></author>
      <author><first>Jiayi</first><last>Fu</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <pages>9129-9144</pages>
      <abstract>Empowering Large Language Models (LLMs) with distinct human-like personality traits has become an innovative task for developing advanced dialog systems.Although LLMs demonstrate impressive capabilities in following instructions, directly prompting them to exhibit certain personalities through manually crafted instructions may result in sub-optimal performance.In this paper, we propose a plug-and-play prompting method to manipulate the LLMs’ personality traits.Specifically, we append discrete personalized suffixes, automatically generated through an aggregated gradient-based search method, to the user query or dialog histories and induce LLMs to respond with target personalities.In addition, due to the high redundancy of the search space, we adopt a reward-based strategy to prune the vocabulary and focus exclusively on influential tokens.Experiment results on four models ranging from 1.1B to 13B show that our method achieves 79.9% accuracy in customizing LLMs’ personalities, significantly outperforming other prompting methods (65.5%) and model editing methods.Our method also excels in generation fluency and quality with the lowest generation perplexity and the highest GPT-4 evaluation scores.</abstract>
      <url hash="2d9d7aed">2024.findings-acl.541</url>
      <bibkey>zhang-etal-2024-p4</bibkey>
      <doi>10.18653/v1/2024.findings-acl.541</doi>
    </paper>
    <paper id="542">
      <title>Large Language Models Can Learn Representation in Natural Language</title>
      <author><first>Yiduo</first><last>Guo</last></author>
      <author><first>Yaobo</first><last>Liang</last></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <pages>9145-9154</pages>
      <abstract>One major challenge for Large Language Models (LLMs) is completing complex tasks involving multiple entities, such as tool APIs. To tackle this, one approach is to retrieve relevant entities to enhance LLMs in task completion. A crucial issue here is obtaining accurate natural language representations for each entity to aid in retriever precision. In this paper, we propose the Natural Language Representation Optimization Problem, which aims to refine entity descriptions for improved retrieval and LLM utilization. We introduce the Learning to Represent with Natural Language method, which utilizes LLMs to optimize entity representations consisting of text patterns based on environmental feedback. We iteratively prompt LLMs to enhance or adjust patterns based on entity samples and evaluate their effectiveness through environmental feedback. Our method successfully learns human-understandable representations for classification tasks (e.g., instructions and documents) and API call tasks (e.g., APIbench and Virtual Home), significantly improving GPT-4’s task performance.</abstract>
      <url hash="9c46cea1">2024.findings-acl.542</url>
      <bibkey>guo-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.542</doi>
    </paper>
    <paper id="543">
      <title><fixed-case>CTC</fixed-case>-based Non-autoregressive Textless Speech-to-Speech Translation</title>
      <author><first>Qingkai</first><last>Fang</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zhengrui</first><last>Ma</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yan</first><last>Zhou</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>9155-9161</pages>
      <abstract>Direct speech-to-speech translation (S2ST) has achieved impressive translation quality, but it often faces the challenge of slow decoding due to the considerable length of speech sequences. Recently, some research has turned to non-autoregressive (NAR) models to expedite decoding, yet the translation quality typically lags behind autoregressive (AR) models significantly. In this paper, we investigate the performance of CTC-based NAR models in S2ST, as these models have shown impressive results in machine translation. Experimental results demonstrate that by combining pretraining, knowledge distillation, and advanced NAR training techniques such as glancing training and non-monotonic latent alignments, CTC-based NAR models achieve translation quality comparable to the AR model, while preserving up to 26.81<tex-math>\times</tex-math> decoding speedup.</abstract>
      <url hash="63b5a140">2024.findings-acl.543</url>
      <bibkey>fang-etal-2024-ctc</bibkey>
      <doi>10.18653/v1/2024.findings-acl.543</doi>
    </paper>
    <paper id="544">
      <title><fixed-case>RRN</fixed-case>orm: A Novel Framework for <fixed-case>C</fixed-case>hinese Disease Diagnoses Normalization via <fixed-case>LLM</fixed-case>-Driven Terminology Component Recognition and Reconstruction</title>
      <author><first>Yongqi</first><last>Fan</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Yansha</first><last>Zhu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Kui</first><last>Xue</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>9162-9175</pages>
      <abstract>The Clinical Terminology Normalization aims at finding standard terms from a given termbase for mentions extracted from clinical texts. However, we found that extracted mentions suffer from the multi-implication problem, especially disease diagnoses. The reason for this is that physicians often use abbreviations, conjunctions, and juxtapositions when writing diagnoses, and it is difficult to manually decompose. To address this problem, we propose a Terminology Component Recognition and Reconstruction strategy that leverages the reasoning capability of large language models (LLMs) to recognize the components of terms, enabling automated decomposition and transforming original mentions into multiple atomic mentions. Furthermore, we adopt the mainstream “Recall and Rank” framework to apply the benefits of the above strategy to the task flow. By leveraging the LLM incorporating the advanced sampling strategies, we design a sampling algorithm for atomic mentions and train the recall model using contrastive learning. Besides the information about the components is also used as knowledge to guide the final term ranking and selection. The experimental results show that our proposed strategy effectively improves the performance of the terminology normalization task and our proposed approach achieves state-of-the-art on the experimental dataset. We release our code and data on the repository https://github.com/yuugaochyan/RRNorm.</abstract>
      <url hash="5841864a">2024.findings-acl.544</url>
      <bibkey>fan-etal-2024-rrnorm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.544</doi>
    </paper>
    <paper id="545">
      <title>Unexpected Phenomenon: <fixed-case>LLM</fixed-case>s’ Spurious Associations in Information Extraction</title>
      <author><first>Weiyan</first><last>Zhang</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Wanpeng</first><last>Lu</last></author>
      <author><first>Jiacheng</first><last>Wang</last></author>
      <author><first>Yating</first><last>Wang</last></author>
      <author><first>Lihan</first><last>Chen</last><affiliation>beijing institute of control engineering</affiliation></author>
      <author><first>Haiyun</first><last>Jiang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Tong</first><last>Ruan</last></author>
      <pages>9176-9190</pages>
      <abstract>Information extraction plays a critical role in natural language processing. When applying large language models (LLMs) to this domain, we discover an unexpected phenomenon: LLMs’ spurious associations. In tasks such as relation extraction, LLMs can accurately identify entity pairs, even if the given relation (label) is semantically unrelated to the pre-defined original one. To find these labels, we design two strategies in this study, including forward label extension and backward label validation. We also leverage the extended labels to improve model performance. Our comprehensive experiments show that spurious associations occur consistently in both Chinese and English datasets across various LLM sizes. Moreover, the use of extended labels significantly enhances LLM performance in information extraction tasks. Remarkably, there is a performance increase of 9.55%, 11.42%, and 21.27% in F1 scores on the SciERC, ACE05, and DuEE datasets, respectively.</abstract>
      <url hash="b382bf2f">2024.findings-acl.545</url>
      <bibkey>zhang-etal-2024-unexpected</bibkey>
      <doi>10.18653/v1/2024.findings-acl.545</doi>
    </paper>
    <paper id="546">
      <title><fixed-case>A</fixed-case>uto<fixed-case>CAP</fixed-case>: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought</title>
      <author><first>Yongheng</first><last>Zhang</last></author>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Min</first><last>Li</last></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Libo</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>9191-9200</pages>
      <abstract>Cross-lingual chain-of-thought can effectively complete reasoning tasks across languages, which gains increasing attention.Recently, dominant approaches in the literature improve cross-lingual alignment capabilities by integrating reasoning knowledge from different languages. Despite achieving excellent performance, current methods still have two main challenges: (1) Manual language specification: They still highly rely on manually selecting the languages to integrate, severely affecting their generalizability; (2) Static weight allocation: Current methods simply integrate all languages equally. In fact, different language reasoning paths should have different weights to achieve better complementation and integration. Motivated by this, we introduce an Automatic Cross-lingual Alignment Planning (AutoCAP) for zero-shot chain-of-thought to address the above challenges. The core of AutoCAP consists of two components: (1) Automatic Language Selection Prompting to guide LLMs to select appropriate languages and (2) Automatic Weight Allocation Prompting to automatically allocate alignment weight scores to each reasoning path. Extensive experiments on several benchmarks reveal that AutoCAP achieves state-of-the-art performance, surpassing previous methods that required manual effort.</abstract>
      <url hash="84303845">2024.findings-acl.546</url>
      <bibkey>zhang-etal-2024-autocap</bibkey>
      <doi>10.18653/v1/2024.findings-acl.546</doi>
    </paper>
    <paper id="547">
      <title><fixed-case>LCS</fixed-case>: A Language Converter Strategy for Zero-Shot Neural Machine Translation</title>
      <author><first>Zengkui</first><last>Sun</last></author>
      <author><first>Yijin</first><last>Liu</last><affiliation>Wechat AI</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>9201-9214</pages>
      <abstract>Multilingual neural machine translation models generally distinguish translation directions by the language tag (LT) in front of the source or target sentences. However, current LT strategies cannot indicate the desired target language as expected on zero-shot translation, i.e., the off-target issue. Our analysis reveals that the indication of the target language is sensitive to the placement of the target LT. For example, when placing the target LT on the decoder side, the indication would rapidly degrade along with decoding steps, while placing the target LT on the encoder side would lead to copying or paraphrasing the source input. To address the above issues, we propose a simple yet effective strategy named Language Converter Strategy (LCS). By introducing the target language embedding into the top encoder layers, LCS mitigates confusion in the encoder and ensures stable language indication for the decoder. Experimental results on MultiUN, TED, and OPUS-100 datasets demonstrate that LCS could significantly mitigate the off-target issue, with language accuracy up to 95.28%, 96.21%, and 85.35% meanwhile outperforming the vanilla LT strategy by 3.07, 3,3, and 7.93 BLEU scores on zero-shot translation, respectively.</abstract>
      <url hash="713bf468">2024.findings-acl.547</url>
      <bibkey>sun-etal-2024-lcs</bibkey>
      <doi>10.18653/v1/2024.findings-acl.547</doi>
    </paper>
    <paper id="548">
      <title>Are <fixed-case>LLM</fixed-case>s Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data</title>
      <author><first>Xiao</first><last>Liu</last><affiliation>Peking University</affiliation></author>
      <author><first>Zirui</first><last>Wu</last></author>
      <author><first>Xueqing</first><last>Wu</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Pan</first><last>Lu</last><affiliation>Stanford University</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Yansong</first><last>Feng</last><affiliation>Peking University</affiliation></author>
      <pages>9215-9235</pages>
      <abstract>Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models’ capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models’ quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has much room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in https://github.com/xxxiaol/QRData.</abstract>
      <url hash="e990c88b">2024.findings-acl.548</url>
      <bibkey>liu-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.findings-acl.548</doi>
    </paper>
    <paper id="549">
      <title>On the Vulnerability of Safety Alignment in Open-Access <fixed-case>LLM</fixed-case>s</title>
      <author><first>Jingwei</first><last>Yi</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Rui</first><last>Ye</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Qisi</first><last>Chen</last></author>
      <author><first>Bin</first><last>Zhu</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Siheng</first><last>Chen</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Guangzhong</first><last>Sun</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Xing</first><last>Xie</last><affiliation>Microsoft</affiliation></author>
      <author><first>Fangzhao</first><last>Wu</last><affiliation>Microsoft</affiliation></author>
      <pages>9236-9260</pages>
      <abstract>Large language models (LLMs) possess immense capabilities but are susceptible to malicious exploitation. To mitigate the risk, safety alignment is employed to align LLMs with ethical standards. However, safety-aligned LLMs may remain vulnerable to carefully crafted jailbreak attacks, but these attacks often face high rejection rates and limited harmfulness. In this paper, we expose the vulnerabilities of safety alignment in open-access LLMs, which can significantly enhance the success rate and harmfulness of jailbreak attacks. Through reverse alignment, achieved by accessing model parameters, we show the feasibility of efficiently fine-tuning LLMs to undermine their inherent safeguards. We investigate two types of reverse alignment techniques: reverse supervised fine-tuning (RSFT) and reverse preference optimization (RPO). RSFT operates by supervising the fine-tuning of LLMs to reverse their inherent values. We also explore how to prepare data needed for RSFT. RPO optimizes LLMs to enhance their preference for harmful content, reversing the models’ safety alignment. Our extensive experiments reveal that open-access high-performance LLMs can be adeptly reverse-aligned to output harmful content, even in the absence of manually curated malicious datasets. Our research acts as a whistleblower for the community, emphasizing the need to pay more attention to safety of open-accessing LLMs. It also underscores the limitations of current safety alignment approaches and calls for research on robust safety alignment methods to counteract malicious fine-tuning attacks.</abstract>
      <url hash="c5c32a73">2024.findings-acl.549</url>
      <bibkey>yi-etal-2024-vulnerability</bibkey>
      <doi>10.18653/v1/2024.findings-acl.549</doi>
    </paper>
    <paper id="550">
      <title><fixed-case>PEK</fixed-case>: A Parameter-Efficient Framework for Knowledge-Grounded Dialogue Generation</title>
      <author><first>Pan</first><last>Yang</last></author>
      <author><first>Dandan</first><last>Song</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Zhijing</first><last>Wu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yanru</first><last>Zhou</last></author>
      <pages>9261-9273</pages>
      <abstract>Pre-trained language models (PLMs) have shown great dialogue generation capability in different scenarios. However, the huge VRAM consumption when fine-tuning them is one of their drawbacks. PEFT approaches can significantly reduce the number of trainable parameters, which enables us to fine-tune larger dialogue generation models. However, the reduction in parameter quantity can diminish a PLM’s expressive capacity and affect the PLM’s learning from certain specific examples like knowledge-related conversations. Previous works have demonstrated that injecting external knowledge into dialogue generation models can improve the model’s performance in knowledge-related conversations. Nonetheless, these methods are designed for the scenario where most parameters of the entire framework are trainable. In this paper, we propose PEK, a parameter-efficient framework for knowledge-enhanced dialogue generation. It enables PLMs to leverage external knowledge documents and knowledge graphs to enhance its generation capabilities with an acceptable number of trainable parameters. Evaluation results on the Wizard of Wikipedia and CMU_DoG datasets show that our approach outperforms baseline methods on multiple evaluation metrics, which validates the effectiveness of our approach.</abstract>
      <url hash="75d2d60c">2024.findings-acl.550</url>
      <bibkey>yang-etal-2024-pek</bibkey>
      <doi>10.18653/v1/2024.findings-acl.550</doi>
    </paper>
    <paper id="551">
      <title>Evidence Retrieval is almost All You Need for Fact Verification</title>
      <author><first>Liwen</first><last>Zheng</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Chaozhuo</first><last>Li</last></author>
      <author><first>Xi</first><last>Zhang</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Yu-Ming</first><last>Shang</last></author>
      <author><first>Feiran</first><last>Huang</last></author>
      <author><first>Haoran</first><last>Jia</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <pages>9274-9281</pages>
      <abstract>Current fact verification methods generally follow the two-stage training paradigm: evidence retrieval and claim verification. While existing works focus on developing sophisticated claim verification modules, the fundamental importance of evidence retrieval is largely ignored. Existing approaches usually adopt the heuristic semantic similarity-based retrieval strategy, resulting in the task-irrelevant evidence and undesirable performance. In this paper, we concentrate on evidence retrieval and propose a Retrieval-Augmented Verification framework RAV, consisting of two major modules: the hybrid evidence retrieval and the joint fact verification. Hybrid evidence retrieval module incorporates an efficient retriever for preliminary pruning of candidate evidence, succeeded by a ranker that generates more precise sorting results. Under this end-to-end training paradigm, gradients from the claim verification can be back-propagated to enhance evidence selection. Experimental results on FEVER dataset demonstrate the superiority of RAV.</abstract>
      <url hash="4eeaf280">2024.findings-acl.551</url>
      <bibkey>zheng-etal-2024-evidence</bibkey>
      <doi>10.18653/v1/2024.findings-acl.551</doi>
    </paper>
    <paper id="552">
      <title>Outdated Issue Aware Decoding for Factual Knowledge Editing</title>
      <author><first>Zengkui</first><last>Sun</last></author>
      <author><first>Yijin</first><last>Liu</last><affiliation>Wechat AI</affiliation></author>
      <author><first>Jiaan</first><last>Wang</last><affiliation>Soochow University</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>9282-9293</pages>
      <abstract>Recently, Knowledge Editing has received increasing attention, since it could update the specific knowledge from outdated ones in pretrained models without re-training. However, as pointed out by recent studies, existing related methods tend to merely memorize the superficial word composition of the edited knowledge, rather than truly learning and absorbing it. Consequently, on the reasoning questions, we discover that existing methods struggle to utilize the edited knowledge to reason the new answer, and tend to retain outdated responses, which are generated by the original models utilizing original knowledge. Nevertheless, the outdated responses are unexpected for the correct answers to reasoning questions, which we named as the outdated issue. To alleviate this issue, in this paper, we propose a simple yet effective decoding strategy, i.e., outDated ISsue aware deCOding (DISCO), to enhance the performance of edited models on reasoning questions. Specifically, we capture the difference in the probability distribution between the original and edited models. Further, we amplify the difference of the token prediction in the edited model to alleviate the outdated issue, and thus enhance the model performance w.r.t the edited knowledge. Experimental results suggest that applying DISCO could enhance edited models to reason, e.g., on reasoning questions, DISCO outperforms the prior SOTA method by 12.99 F1 scores, and reduces the ratio of the outdated issue to 5.78% on the zsRE dataset.</abstract>
      <url hash="b4fc3845">2024.findings-acl.552</url>
      <bibkey>sun-etal-2024-outdated</bibkey>
      <doi>10.18653/v1/2024.findings-acl.552</doi>
    </paper>
    <paper id="553">
      <title>Disentangling Dialect from Social Bias via Multitask Learning to Improve Fairness</title>
      <author><first>Maximilian</first><last>Spliethöver</last><affiliation>Leibniz University Hannover</affiliation></author>
      <author><first>Sai Nikhil</first><last>Menon</last></author>
      <author><first>Henning</first><last>Wachsmuth</last><affiliation>Leibniz Universität Hannover</affiliation></author>
      <pages>9294-9313</pages>
      <abstract>Dialects introduce syntactic and lexical variations in language that occur in regional or social groups. Most NLP methods are not sensitive to such variations. This may lead to unfair behavior of the methods, conveying negative bias towards dialect speakers. While previous work has studied dialect-related fairness for aspects like hate speech, other aspects of biased language, such as lewdness, remain fully unexplored. To fill this gap, we investigate performance disparities between dialects in the detection of five aspects of biased language and how to mitigate them. To alleviate bias, we present a multitask learning approach that models dialect language as an auxiliary task to incorporate syntactic and lexical variations. In our experiments with African-American English dialect, we provide empirical evidence that complementing common learning approaches with dialect modeling improves their fairness. Furthermore, the results suggest that multitask learning achieves state-of-the-art performance and helps to detect properties of biased language more reliably.</abstract>
      <url hash="f059370f">2024.findings-acl.553</url>
      <bibkey>spliethover-etal-2024-disentangling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.553</doi>
    </paper>
    <paper id="554">
      <title><fixed-case>DP</fixed-case>-<fixed-case>MLM</fixed-case>: Differentially Private Text Rewriting Using Masked Language Models</title>
      <author><first>Stephen</first><last>Meisenbacher</last></author>
      <author><first>Maulik</first><last>Chevli</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Juraj</first><last>Vladika</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>9314-9328</pages>
      <abstract>The task of text privatization using Differential Privacy has recently taken the form of <tex-math>\textit{text rewriting}</tex-math>, in which an input text is obfuscated via the use of generative (large) language models. While these methods have shown promising results in the ability to preserve privacy, these methods rely on autoregressive models which lack a mechanism to contextualize the private rewriting process. In response to this, we propose <tex-math>\textbf{DP-MLM}</tex-math>, a new method for differentially private text rewriting based on leveraging masked language models (MLMs) to rewrite text in a semantically similar <tex-math>\textit{and}</tex-math> obfuscated manner. We accomplish this with a simple contextualization technique, whereby we rewrite a text one token at a time. We find that utilizing encoder-only MLMs provides better utility preservation at lower <tex-math>\varepsilon</tex-math> levels, as compared to previous methods relying on larger models with a decoder. In addition, MLMs allow for greater customization of the rewriting mechanism, as opposed to generative approaches. We make the code for <tex-math>\textbf{DP-MLM}</tex-math> public and reusable, found at https://github.com/sjmeis/DPMLM.</abstract>
      <url hash="b91aca74">2024.findings-acl.554</url>
      <bibkey>meisenbacher-etal-2024-dp</bibkey>
      <doi>10.18653/v1/2024.findings-acl.554</doi>
    </paper>
    <paper id="555">
      <title>Question-Instructed Visual Descriptions for Zero-Shot Video Answering</title>
      <author><first>David</first><last>Mogrovejo</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Thamar</first><last>Solorio</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and University of Houston</affiliation></author>
      <pages>9329-9339</pages>
      <abstract>We present Q-ViD, a simple approach for video question answering (video QA), that unlike prior methods, which are based on complex architectures, computationally expensive pipelines or use closed models like GPTs, Q-ViD relies on a single instruction-aware open vision-language model (InstructBLIP) to tackle videoQA using frame descriptions. Specifically, we create captioning instruction prompts that rely on the target questions about the videos and leverage InstructBLIP to obtain video frame captions that are useful to the task at hand. Subsequently, we form descriptions of the whole video using the question-dependent frame captions, and feed that information, along with a question-answering prompt, to a large language model (LLM). The LLM is our reasoning module, and performs the final step of multiple-choice QA. Our simple Q-ViD framework achieves competitive or even higher performances than current state of the art models on a diverse range of videoQA benchmarks, including NExT-QA, STAR, How2QA, TVQA and IntentQA.</abstract>
      <url hash="2514cccb">2024.findings-acl.555</url>
      <bibkey>mogrovejo-solorio-2024-question</bibkey>
      <doi>10.18653/v1/2024.findings-acl.555</doi>
    </paper>
    <paper id="556">
      <title><fixed-case>EX</fixed-case>-<fixed-case>FEVER</fixed-case>: A Dataset for Multi-hop Explainable Fact Verification</title>
      <author><first>Huanhuan</first><last>Ma</last></author>
      <author><first>Weizhi</first><last>Xu</last></author>
      <author><first>Yifan</first><last>Wei</last></author>
      <author><first>Liuji</first><last>Chen</last></author>
      <author><first>Liang</first><last>Wang</last></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Shu</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Liang</first><last>Wang</last><affiliation>CASIA</affiliation></author>
      <pages>9340-9353</pages>
      <abstract>Fact verification aims to automatically probe the veracity of a claim based on several pieces of evidence. Existing works are always engaging in accuracy improvement, let alone explainability, a critical capability of fact verification systems.Constructing an explainable fact verification system in a complex multi-hop scenario is consistently impeded by the absence of a relevant, high-quality dataset. Previous datasets either suffer from excessive simplification or fail to incorporate essential considerations for explainability. To address this, we present EX-FEVER, a pioneering dataset for multi-hop explainable fact verification. With over 60,000 claims involving 2-hop and 3-hop reasoning, each is created by summarizing and modifying information from hyperlinked Wikipedia documents. Each instance is accompanied by a veracity label and an explanation that outlines the reasoning path supporting the veracity classification. Additionally, we demonstrate a novel baseline system on our EX-FEVER dataset, showcasing document retrieval, explanation generation, and claim verification, and validate the significance of our dataset. Furthermore, we highlight the potential of utilizing Large Language Models in the fact verification task. We hope our dataset could make a significant contribution by providing ample opportunities to explore the integration of natural language explanations in the domain of fact verification.</abstract>
      <url hash="45212c12">2024.findings-acl.556</url>
      <bibkey>ma-etal-2024-ex</bibkey>
      <doi>10.18653/v1/2024.findings-acl.556</doi>
    </paper>
    <paper id="557">
      <title>Agent-<fixed-case>FLAN</fixed-case>: Designing Data and Methods of Effective Agent Tuning for Large Language Models</title>
      <author><first>Zehui</first><last>Chen</last></author>
      <author><first>Kuikun</first><last>Liu</last></author>
      <author><first>Qiuchen</first><last>Wang</last></author>
      <author><first>Wenwei</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Jiangning</first><last>Liu</last></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Chen</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Feng</first><last>Zhao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>9354-9366</pages>
      <abstract>Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem.This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents.Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code and models are available at https://github.com/InternLM/Agent-FLAN.</abstract>
      <url hash="ca03b0fb">2024.findings-acl.557</url>
      <bibkey>chen-etal-2024-agent</bibkey>
      <doi>10.18653/v1/2024.findings-acl.557</doi>
    </paper>
    <paper id="558">
      <title>Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification</title>
      <author><first>Ekaterina</first><last>Fadeeva</last></author>
      <author><first>Aleksandr</first><last>Rubashevskii</last><affiliation>Skolkovo Institute of Science and Technology</affiliation></author>
      <author><first>Artem</first><last>Shelmanov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Sergey</first><last>Petrakov</last></author>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Hamdy</first><last>Mubarak</last></author>
      <author><first>Evgenii</first><last>Tsymbalov</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Gleb</first><last>Kuzmin</last><affiliation>Artificial Intelligence Research Institute and Institute for Systems Analysis of Russian Academy of Sciences</affiliation></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Maxim</first><last>Panov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>9367-9385</pages>
      <abstract>Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven different LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.</abstract>
      <url hash="c88a55ba">2024.findings-acl.558</url>
      <bibkey>fadeeva-etal-2024-fact</bibkey>
      <doi>10.18653/v1/2024.findings-acl.558</doi>
    </paper>
    <paper id="559">
      <title>Deciphering the Impact of Pretraining Data on Large Language Models through Machine Unlearning</title>
      <author><first>Yang</first><last>Zhao</last></author>
      <author><first>Li</first><last>Du</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Kai</first><last>Xiong</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zhouhao</first><last>Sun</last></author>
      <author><first>Shi</first><last>Jun</last></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>9386-9406</pages>
      <abstract>Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of “high-impact data” such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to support more efficient pretraining of LLMs.</abstract>
      <url hash="afdaef06">2024.findings-acl.559</url>
      <bibkey>zhao-etal-2024-deciphering</bibkey>
      <doi>10.18653/v1/2024.findings-acl.559</doi>
    </paper>
    <paper id="560">
      <title>Critical Learning Periods: Leveraging Early Training Dynamics for Efficient Data Pruning</title>
      <author><first>Everlyn</first><last>Chimoto</last></author>
      <author><first>Jay</first><last>Gala</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Orevaoghene</first><last>Ahia</last></author>
      <author><first>Julia</first><last>Kreutzer</last><affiliation>Cohere for AI</affiliation></author>
      <author><first>Bruce</first><last>Bassett</last><affiliation>University of Cape Town</affiliation></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <pages>9407-9426</pages>
      <abstract>Neural Machine Translation models are extremely data and compute-hungry. However, not all datapoints contribute equally to model training and generalization. Data pruning to remove the low-value data points has the benefit of drastically reducing the compute budget without significantdrop in model performance. In this paper, we propose a new data pruning technique: CheckpointsAcross Time (CAT ), that leverages early model training dynamics to identify the most relevantdata points for model performance. We benchmark CAT against several data pruning techniquesincluding COMET-QE, LASER and LaBSE. We find that CAT outperforms the benchmarks onIndo-European languages on multiple test sets. When applied to English-German, English-Frenchand English-Swahili translation tasks, CAT achieves comparable performance to using the fulldataset, while pruning up to 50% of training data. We inspect the data points that CAT selectsand find that it tends to favour longer sentences and sentences with unique or rare words.</abstract>
      <url hash="7b827f3b">2024.findings-acl.560</url>
      <bibkey>chimoto-etal-2024-critical</bibkey>
      <doi>10.18653/v1/2024.findings-acl.560</doi>
    </paper>
    <paper id="561">
      <title>What Are You Token About? Differentiable Perturbed Top-<tex-math>k</tex-math> Token Selection for Scientific Document Summarization</title>
      <author><first>Luca</first><last>Ragazzi</last></author>
      <author><first>Paolo</first><last>Italiani</last></author>
      <author><first>Gianluca</first><last>Moro</last><affiliation>DISI - University of Bologna</affiliation></author>
      <author><first>Mattia</first><last>Panni</last><affiliation>Università degli Studi di Modena e Reggio Emilia</affiliation></author>
      <pages>9427-9440</pages>
      <abstract>Scientific document summarization aims to condense complex and long articles in both technical and plain-language terms to facilitate the accessibility and dissemination of scientific findings. Existing datasets suffer from a deficiency in source heterogeneity, as their data predominantly stem from a single common resource, hindering effective model training and generalizability. First, we introduce SciLay, a novel dataset that includes documents from multiple natural science journals with expert-authored technical and lay summaries. Second, we propose PrunePert, a new transformer-based model that incorporates a differentiable perturbed top-<tex-math>k</tex-math> encoder layer to prune irrelevant tokens in end-to-end learning. Experimental results show that our model achieves a nearly 2x speed-up compared to a state-of-the-art linear transformer, remaining comparable in effectiveness. Additional examinations underscore the importance of employing a training dataset that includes different sources to enhance the generalizability of the models. Code is available at https://github.com/disi-unibo-nlp/sci-lay.</abstract>
      <url hash="a512ff62">2024.findings-acl.561</url>
      <bibkey>ragazzi-etal-2024-token</bibkey>
      <doi>10.18653/v1/2024.findings-acl.561</doi>
    </paper>
    <paper id="562">
      <title>Description Boosting for Zero-Shot Entity and Relation Classification</title>
      <author><first>Gabriele</first><last>Picco</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Leopold</first><last>Fuchs</last><affiliation>Duale Hochschule Baden-Württemberg Stuttgart</affiliation></author>
      <author><first>Marcos</first><last>Martínez Galindo</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Alberto</first><last>Purpura</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Vanessa</first><last>López</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Hoang</first><last>Thanh Lam</last><affiliation>International Business Machines</affiliation></author>
      <pages>9441-9457</pages>
      <abstract>Zero-shot entity and relation classification models leverage available external information of unseen classes – e.g., textual descriptions – to annotate input text data. Thanks to the minimum data requirement, Zero-Shot Learning (ZSL) methods have high value in practice, especially in applications where labeled data is scarce. Even though recent research in ZSL has demonstrated significant results, our analysis reveals that those methods are sensitive to provided textual descriptions of entities (or relations). Even a minor modification of descriptions can lead to a change in the decision boundary between entity (or relation) classes. In this paper, we formally define the problem of identifying effective descriptions for zero shot inference. We propose a strategy for generating variations of an initial description, a heuristic for ranking them and an ensemble method capable of boosting the predictions of zero-shot models through description enhancement. Empirical results on four different entity and relation classification datasets show that our proposed method outperform existing approaches and achieve new SOTA results on these datasets under the ZSL settings. The source code of the proposed solutions and the evaluation framework are open-sourced.</abstract>
      <url hash="339bdc81">2024.findings-acl.562</url>
      <bibkey>picco-etal-2024-description</bibkey>
      <doi>10.18653/v1/2024.findings-acl.562</doi>
    </paper>
    <paper id="563">
      <title>Domain-Aware <tex-math>k</tex-math>-Nearest-Neighbor Knowledge Distillation for Machine Translation</title>
      <author><first>Zhexuan</first><last>Wang</last></author>
      <author><first>Shudong</first><last>Liu</last><affiliation>University of Macau</affiliation></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technolgy, Shenzhen</affiliation></author>
      <author><first>Miao</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Derek</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>9458-9469</pages>
      <abstract><tex-math>k</tex-math>NN-MT has utilized neighborhood knowledge for auxiliary decoding, significantly improving translation performance. Subsequently, <tex-math>k</tex-math>NN-KD transitions the use of neighborhood knowledge from the decoding phase to the training phase, to address the temporal and spatial inefficiencies inherent in <tex-math>k</tex-math>NN-MT. However, <tex-math>k</tex-math>NN-KD transfers all the <tex-math>k</tex-math>NN knowledge arbitrarily, which has the potential to restrict the learning of student models. In this paper, we propose a novel domain-aware <tex-math>k</tex-math>NN-KD method, which filters out domain-relevant neighborhood knowledge for learning in the distillation process. Notably, this entire process exclusively utilizes the neighborhood knowledge of the original model, eliminating the need for establishing any additional datastores. Experiments on four domain translation tasks demonstrate that our method achieves state-of-the-art performance, realizing an average gain of 1.55 COMET and 1.42 BLEU scores, by further enhancing the translation of rare words. Source code can be accessed at https://github.com/wangzx1219/Dk-KD.</abstract>
      <url hash="6a10bc44">2024.findings-acl.563</url>
      <bibkey>wang-etal-2024-domain-aware</bibkey>
      <doi>10.18653/v1/2024.findings-acl.563</doi>
    </paper>
    <paper id="564">
      <title>Beyond Single-Event Extraction: Towards Efficient Document-Level Multi-Event Argument Extraction</title>
      <author><first>Wanlong</first><last>Liu</last></author>
      <author><first>Li</first><last>Zhou</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>DingYi</first><last>Zeng</last></author>
      <author><first>Yichen</first><last>Xiao</last></author>
      <author><first>Shaohuan</first><last>Cheng</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Chen</first><last>Zhang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Grandee</first><last>Lee</last><affiliation>Singapore University of Social Sciences</affiliation></author>
      <author><first>Malu</first><last>Zhang</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Wenyu</first><last>Chen</last></author>
      <pages>9470-9487</pages>
      <abstract>Recent mainstream event argument extraction methods process each event in isolation, resulting in inefficient inference and ignoring the correlations among multiple events. To address these limitations, here we propose a multiple-event argument extraction model DEEIA (Dependency-guided Encoding and Event-specific Information Aggregation), capable of extracting arguments from all events within a document simultaneously. The proposed DEEIA model employs a multi-event prompt mechanism, comprising DE and EIA modules. The DE module is designed to improve the correlation between prompts and their corresponding event contexts, whereas the EIA module provides event-specific information to improve contextual understanding. Extensive experiments show that our method achieves new state-of-the-art performance on four public datasets (RAMS, WikiEvents, MLEE, and ACE05), while significantly saving the inference time compared to the baselines. Further analyses demonstrate the effectiveness of the proposed modules.</abstract>
      <url hash="e27958ff">2024.findings-acl.564</url>
      <bibkey>liu-etal-2024-beyond-single</bibkey>
      <doi>10.18653/v1/2024.findings-acl.564</doi>
    </paper>
    <paper id="565">
      <title>Revisiting Interpolation Augmentation for Speech-to-Text Generation</title>
      <author><first>Chen</first><last>Xu</last><affiliation>Harbin Engineering University</affiliation></author>
      <author><first>Jie</first><last>Wang</last></author>
      <author><first>Xiaoqian</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Qian</first><last>Dong</last><affiliation>ByteDance</affiliation></author>
      <author><first>Chunliang</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>JingBo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Dapeng</first><last>Man</last></author>
      <author><first>Wu</first><last>Yang</last></author>
      <pages>9488-9499</pages>
      <abstract>Speech-to-text (S2T) generation systems frequently face challenges in low-resource scenarios, primarily due to the lack of extensive labeled datasets. One emerging solution is constructing virtual training samples by interpolating inputs and labels, which has notably enhanced system generalization in other domains. Despite its potential, this technique’s application in S2T tasks has remained under-explored. In this paper, we delve into the utility of interpolation augmentation, guided by several pivotal questions. Our findings reveal that employing an appropriate strategy in interpolation augmentation significantly enhances performance across diverse tasks, architectures, and data scales, offering a promising avenue for more robust S2T systems in resource-constrained settings.</abstract>
      <url hash="10d87f68">2024.findings-acl.565</url>
      <bibkey>xu-etal-2024-revisiting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.565</doi>
    </paper>
    <paper id="566">
      <title>Bootstrapping <fixed-case>LLM</fixed-case>-based Task-Oriented Dialogue Agents via Self-Talk</title>
      <author><first>Dennis</first><last>Ulmer</last></author>
      <author><first>Elman</first><last>Mansimov</last><affiliation>Amazon</affiliation></author>
      <author><first>Kaixiang</first><last>Lin</last><affiliation>Amazon</affiliation></author>
      <author><first>Lijia</first><last>Sun</last><affiliation>Amazon</affiliation></author>
      <author><first>Xibin</first><last>Gao</last><affiliation>Amazon AWS AI Labs</affiliation></author>
      <author><first>Yi</first><last>Zhang</last><affiliation>Amazon</affiliation></author>
      <pages>9500-9522</pages>
      <abstract>Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via “self-talk” of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.</abstract>
      <url hash="0e354e19">2024.findings-acl.566</url>
      <bibkey>ulmer-etal-2024-bootstrapping</bibkey>
      <doi>10.18653/v1/2024.findings-acl.566</doi>
    </paper>
    <paper id="567">
      <title>Semantic are Beacons: A Semantic Perspective for Unveiling Parameter-Efficient Fine-Tuning in Knowledge Learning</title>
      <author><first>Renzhi</first><last>Wang</last></author>
      <author><first>Piji</first><last>Li</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <pages>9523-9537</pages>
      <abstract>Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of Large Language Models (LLMs) to various downstream applications. However, the effectiveness of the PEFT diminishes notably when downstream tasks require accurate learning of specific knowledge. In this paper, we adopt a semantic perspective to investigate this phenomenon, uncovering the reasons behind PEFT’s limitations in knowledge learning task. Our findings reveals that: (1) PEFT presents a notable risk of pushing the model away from the intended knowledge target; (2) multiple knowledge interfere with each other, and such interference suppresses the learning and expression of knowledge features. Based on these insights, we introduce a data filtering strategy to exclude data that is detrimental to knowledge learning and a re-weighted learning strategy to make the model attentive to semantic distance during knowledge learning. Experimental results demonstrate the effectiveness of the proposed method on open-source large language model, further validate the semantic challenge in PEFT, thus paving the way for future research.</abstract>
      <url hash="6a443dbc">2024.findings-acl.567</url>
      <bibkey>wang-li-2024-semantic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.567</doi>
    </paper>
    <paper id="568">
      <title>Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction</title>
      <author><first>Gili</first><last>Lior</last><affiliation>The Hebrew University of Jerusalem</affiliation></author>
      <author><first>Yoav</first><last>Goldberg</last><affiliation>Bar-Ilan University, Allen Institute for Artificial Intelligence and Bar Ilan University</affiliation></author>
      <author><first>Gabriel</first><last>Stanovsky</last><affiliation>Hebrew University of Jerusalem</affiliation></author>
      <pages>9538-9550</pages>
      <abstract>Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models.We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models.</abstract>
      <url hash="b590117f">2024.findings-acl.568</url>
      <bibkey>lior-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.findings-acl.568</doi>
    </paper>
    <paper id="569">
      <title>Enhancing Cross Text-Molecule Learning by Self-Augmentation</title>
      <author><first>Yinuo</first><last>Jiang</last></author>
      <author><first>Xiang</first><last>Zhuang</last></author>
      <author><first>Keyan</first><last>Ding</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Qiang</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Huajun</first><last>Chen</last><affiliation>Zhejiang University</affiliation></author>
      <pages>9551-9565</pages>
      <abstract>The development of Large Language Models (LLMs) has greatly advanced the field of drug discovery, with the belief that natural language can enhance human control over molecule design. However, the scarcity of high-quality labeled data remains a challenge for cross text-molecule learning. Existing datasets are limited due to the difficulty of collecting precise molecule-description pairs. Although recent efforts have utilized pseudo data generated by LLMs for augmentation, the lack of specialized chemistry knowledge of LLMs and the absence of an effective high quality data selector may introduce noise into the annotations, compromising the models’ robustness. To address these challenges, this paper introduces a novel framework that interweaves model fine-tuning and data augmentation to overcome the scarcity of high-quality data. The proposed approach involves an iterative procedure where the model plays dual roles in annotating unlabeled data and sampling a subset of high-quality data until convergence is achieved, enhancing the model’s understanding and adaptability. Additionally, a new dataset called SAPubChem-41 is presented, which comprises meticulously curated high-quality parallel molecule-description pairs designed specifically for fine-tuning purposes. This research provides an important contribution to the field by addressing the need for high-quality datasets and presenting an effective framework for cross text-molecule learning.</abstract>
      <url hash="cce3bb3f">2024.findings-acl.569</url>
      <bibkey>jiang-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.569</doi>
    </paper>
    <paper id="570">
      <title><fixed-case>R</fixed-case>e<fixed-case>PALM</fixed-case>: Popular Quote Tweet Generation via Auto-Response Augmentation</title>
      <author><first>Erxin</first><last>Yu</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Chunpu</first><last>Xu</last></author>
      <pages>9566-9579</pages>
      <abstract>A quote tweet enables users to share others’ content while adding their own commentary. In order to enhance public engagement through quote tweets, we investigate the task of generating popular quote tweets. This task aims to produce quote tweets that garner higher popularity, as indicated by increased likes, replies, and retweets. Despite the impressive language generation capabilities of large language models (LLMs), there has been limited research on how LLMs can effectively learn the popularity of text to better engage the public. Therefore, we introduce a novel approach called Response-augmented Popularity-Aligned Language Model (RePALM), which aligns language generation with popularity by leveraging insights from augmented auto-responses provided by readers. We utilize the Proximal Policy Optimization framework with a dual-reward mechanism to jointly optimize for the popularity of the quote tweet and its consistency with the auto-responses. In our experiments, we collected two datasets consisting of quote tweets containing external links and those referencing others’ tweets. Extensive results demonstrate the superiority of RePALM over advanced language models that do not incorporate response augmentation.</abstract>
      <url hash="453a9abf">2024.findings-acl.570</url>
      <bibkey>yu-etal-2024-repalm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.570</doi>
    </paper>
    <paper id="571">
      <title>On the Effect of (Near) Duplicate Subwords in Language Modelling</title>
      <author><first>Anton</first><last>Schäfer</last></author>
      <author><first>Thomas</first><last>Hofmann</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <author><first>Imanol</first><last>Schlag</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Tiago</first><last>Pimentel</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <pages>9580-9597</pages>
      <abstract>Tokenisation is a core part of language models (LMs). It involves splitting a character sequence into subwords which are assigned random indices before being served to the LM. However, this process—while typically lossless—may lead to less efficient LM training, because it removes character-level information, thereby making it more difficult to generalise across similar subwords, such as *now* and *Now*. We refer to such subwords as **near duplicates**. In this paper, we study the impact of near duplicate subwords on LM training efficiency. First, we design an experiment that gives us an upper bound to how much we should expect a model to improve if we could perfectly generalise across near duplicates. We do this, by duplicating each token in our LM’s vocabulary, creating perfectly equivalent classes of subwords. Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting. Second, we investigate the impact of naturally occurring near duplicates on LMs. Here, we see that deduplicating them considerably hurts LM performance; but that this loss in performance can be easily mitigated.</abstract>
      <url hash="56447932">2024.findings-acl.571</url>
      <bibkey>schafer-etal-2024-effect</bibkey>
      <doi>10.18653/v1/2024.findings-acl.571</doi>
    </paper>
    <paper id="572">
      <title>Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the <fixed-case>DUST</fixed-case>!</title>
      <author><first>Frank</first><last>Wildenburg</last></author>
      <author><first>Michael</first><last>Hanna</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Sandro</first><last>Pezzelle</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>9598-9613</pages>
      <abstract>In everyday language use, speakers frequently utter and interpret sentences that are semantically underspecified, namely, whose content is insufficient to fully convey their message or interpret them univocally. For example, to interpret the underspecified sentence “Don’t spend too much”, which leaves implicit what (not) to spend, additional linguistic context or outside knowledge is needed. In this work, we propose a novel Dataset of semantically Underspecified Sentences grouped by Type (DUST) and use it to study whether pre-trained language models (LMs) correctly identify and interpret underspecified sentences. We find that newer LMs are reasonably able to identify underspecified sentences when explicitly prompted. However, interpreting them correctly is much harder for any LMs. Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecification would predict. Overall, our study reveals limitations in current models’ processing of sentence semantics and highlights the importance of using naturalistic data and communicative scenarios when evaluating LMs’ language capabilities.</abstract>
      <url hash="6e88a3bc">2024.findings-acl.572</url>
      <bibkey>wildenburg-etal-2024-pre</bibkey>
      <doi>10.18653/v1/2024.findings-acl.572</doi>
    </paper>
    <paper id="573">
      <title>Visual Hallucinations of Multi-modal Large Language Models</title>
      <author><first>Wen</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Hongbin</first><last>Liu</last><affiliation>Duke University</affiliation></author>
      <author><first>Minxin</first><last>Guo</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Neil</first><last>Gong</last><affiliation>Duke University</affiliation></author>
      <pages>9614-9631</pages>
      <abstract>Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs’ performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.</abstract>
      <url hash="3fed55e6">2024.findings-acl.573</url>
      <bibkey>huang-etal-2024-visual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.573</doi>
    </paper>
    <paper id="574">
      <title><fixed-case>S</fixed-case>um<fixed-case>S</fixed-case>urvey: An Abstractive Dataset of Scientific Survey Papers for Long Document Summarization</title>
      <author><first>Ran</first><last>Liu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences and University of Chinese Academy of Sciences</affiliation></author>
      <author><first>Ming</first><last>Liu</last><affiliation>Deakin University</affiliation></author>
      <author><first>Min</first><last>Yu</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>He</first><last>Zhang</last><affiliation>CNPIEC KEXIN LTD</affiliation></author>
      <author><first>Jianguo</first><last>Jiang</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Gang</first><last>Li</last><affiliation>Deakin University</affiliation></author>
      <author><first>Weiqing</first><last>Huang</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <pages>9632-9651</pages>
      <abstract>With the popularity of large language models (LLMs) and their ability to handle longer input documents, there is a growing need for high-quality long document summarization datasets. Although many models already support 16k input, current lengths of summarization datasets are inadequate, and salient information is not evenly distributed. To bridge these gaps, we collect a new summarization dataset called SumSurvey, consisting of more than 18k scientific survey papers. With an average document length exceeding 12k and a quarter exceeding 16k, as well as the uniformity metric outperforming current mainstream long document summarization datasets, SumSurvey brings new challenges and expectations to both fine-tuned models and LLMs. The informativeness of summaries and the models supporting the evaluation of long document summarization warrant further attention. Automatic and human evaluation results on this abstractive dataset confirm this view. Our dataset and code are available at https://github.com/Oswald1997/SumSurvey.</abstract>
      <url hash="ad6697ff">2024.findings-acl.574</url>
      <bibkey>liu-etal-2024-sumsurvey</bibkey>
      <doi>10.18653/v1/2024.findings-acl.574</doi>
    </paper>
    <paper id="575">
      <title>Pushing the Limits of Low-Resource <fixed-case>NER</fixed-case> Using <fixed-case>LLM</fixed-case> Artificial Data Generation</title>
      <author><first>Joan</first><last>Santoso</last><affiliation>Institut Sains dan Teknologi Terpadu Surabaya</affiliation></author>
      <author><first>Patrick</first><last>Sutanto</last></author>
      <author><first>Billy</first><last>Cahyadi</last><affiliation>Institut Sains dan Teknologi Terpadu Surabaya</affiliation></author>
      <author><first>Esther</first><last>Setiawan</last><affiliation>Institut Sains dan Teknologi Terpadu Surabaya</affiliation></author>
      <pages>9652-9667</pages>
      <abstract>Named Entity Recognition (NER) is an important task, but to achieve great performance, it is usually necessary to collect a large amount of labeled data, incurring high costs. In this paper, we propose using open-source Large Language Models (LLM) to generate NER data with only a few labeled examples, reducing the cost of human annotations. Our proposed method is very simple and can perform well using only a few labeled data points. Experimental results on diverse low-resource NER datasets show that our proposed data generation method can significantly improve the baseline. Additionally, our method can be used to augment datasets with class-imbalance problems and consistently improves model performance on macro-F1 metrics.</abstract>
      <url hash="8679e04c">2024.findings-acl.575</url>
      <bibkey>santoso-etal-2024-pushing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.575</doi>
    </paper>
    <paper id="576">
      <title>Understanding and Patching Compositional Reasoning in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Zhaoyi</first><last>Li</last><affiliation>City University of Hong Kong and University of Science and Technology of China</affiliation></author>
      <author><first>Gangwei</first><last>Jiang</last><affiliation>City University of Hong Kong and University of Science and Technology of China</affiliation></author>
      <author><first>Hong</first><last>Xie</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Linqi</first><last>Song</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Ying</first><last>Wei</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>9668-9688</pages>
      <abstract>LLMs have marked a revolutonary shift, yet they falter when faced with compositional reasoning tasks. Our research embarks on a quest to uncover the root causes of compositional reasoning failures of LLMs, uncovering that most of them stem from the improperly generated or leveraged implicit reasoning results. Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs. This deep dive reveals that implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results. Our exploration further locates multi-head self-attention (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit reasoning results. Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modules. Our empirical evidence stands testament to CREME’s effectiveness, paving the way for autonomously and continuously enhancing compositional reasoning capabilities in language models.</abstract>
      <url hash="933a4710">2024.findings-acl.576</url>
      <bibkey>li-etal-2024-understanding</bibkey>
      <doi>10.18653/v1/2024.findings-acl.576</doi>
    </paper>
    <paper id="577">
      <title>Bilingual Rhetorical Structure Parsing with Large Parallel Annotations</title>
      <author><first>Elena</first><last>Chistova</last><affiliation>FRC CSC RAS</affiliation></author>
      <pages>9689-9706</pages>
      <abstract>Discourse parsing is a crucial task in natural language processing that aims to reveal the higher-level relations in a text. Despite growing interest in cross-lingual discourse parsing, challenges persist due to limited parallel data and inconsistencies in the Rhetorical Structure Theory (RST) application across languages and corpora. To address this, we introduce a parallel Russian annotation for the large and diverse English GUM RST corpus. Leveraging recent advances, our end-to-end RST parser achieves state-of-the-art results on both English and Russian corpora. It demonstrates effectiveness in both monolingual and bilingual settings, successfully transferring even with limited second-language annotation. To the best of our knowledge, this work is the first to evaluate the potential of cross-lingual end-to-end RST parsing on a manually annotated parallel corpus.</abstract>
      <url hash="9485f778">2024.findings-acl.577</url>
      <bibkey>chistova-2024-bilingual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.577</doi>
    </paper>
    <paper id="578">
      <title><fixed-case>B</fixed-case>ook2<fixed-case>D</fixed-case>ial: Generating Teacher Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots</title>
      <author><first>Junling</first><last>Wang</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Jakub</first><last>Macina</last><affiliation>Department of Computer Science, ETHZ - ETH Zurich</affiliation></author>
      <author><first>Nico</first><last>Daheim</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Sankalan</first><last>Pal Chowdhury</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>9707-9731</pages>
      <abstract>Educational chatbots are a promising tool for assisting student learning. However, the development of effective chatbots in education has been challenging, as high-quality data is seldom available in this domain. In this paper, we propose a framework for generating synthetic teacher-student interactions grounded in a set of textbooks. Our approaches capture a key aspect of learning interactions where curious students with partial knowledge interactively ask teachers questions about the material in the textbook. We highlight various quality criteria that such dialogues must fulfill and compare several approaches relying on either prompting or finetuning large language models according to these criteria. We use the synthetic dialogues to train educational chatbots and show the benefits of further fine-tuning in educational domains. However, careful human evaluation shows that our best data synthesis method still suffers from hallucinations and tends to reiterate information from previous conversations. Our findings offer insights for future efforts in synthesizing conversational data that strikes a balance between size and quality. We will open-source our data and code.</abstract>
      <url hash="bd641be8">2024.findings-acl.578</url>
      <bibkey>wang-etal-2024-book2dial</bibkey>
      <doi>10.18653/v1/2024.findings-acl.578</doi>
    </paper>
    <paper id="579">
      <title><fixed-case>SELP</fixed-case>: A Semantically-Driven Approach for Separated and Accurate Class Prototypes in Few-Shot Text Classification</title>
      <author><first>Wenxin</first><last>Liang</last></author>
      <author><first>Tingyu</first><last>Zhang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Han</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Feng</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>9732-9741</pages>
      <abstract>The meta-learning paradigm has demonstrated significant effectiveness in few-shot text classification. Currently, numerous efforts are grounded in metric-based learning, utilizing textual feature vectors for classification, with a common emphasis on enlarging inter-class distances to achieve improved classification effectiveness. However, many methods predominantly focus on enhancing the separation of prototypes without taking the semantic relationships between prototypes and class clusters into consideration. This oversight results in incomplete and inaccurate encoding of prototypes within the semantic space, affecting the generality of the learned metric space. In this paper, we propose the utilization of <tex-math>\textbf{S}</tex-math>emantically <tex-math>\textbf{E}</tex-math>nhanced <tex-math>\textbf{L}</tex-math>abels for calibrating class <tex-math>\textbf{P}</tex-math>rototypes (<tex-math>\textbf{SELP}</tex-math>), thereby obtaining prototypes that are more separated and semantically accurate. Additionally, we have devised a center loss to enhance intra-class compactness, coupled with the introduction of a simulated label distribution method to address the overfitting problem. Extensive experiments on eight few-shot text classification datasets show that the proposed method outperforms baselines significantly. Our code is available at https://github.com/tttyyyzzz-zty/SELP.git.</abstract>
      <url hash="66ac1a21">2024.findings-acl.579</url>
      <bibkey>liang-etal-2024-selp</bibkey>
      <doi>10.18653/v1/2024.findings-acl.579</doi>
    </paper>
    <paper id="580">
      <title>Automated Focused Feedback Generation for Scientific Writing Assistance</title>
      <author><first>Eric</first><last>Chamoun</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Michael</first><last>Schlichtkrull</last><affiliation>Queen Mary, University of London</affiliation></author>
      <author><first>Andreas</first><last>Vlachos</last><affiliation>University of Cambridge</affiliation></author>
      <pages>9742-9763</pages>
      <abstract>Scientific writing is a challenging task, particularly for novice researchers who often rely on feedback from experienced peers. Recent work has primarily focused on improving surface form and style rather than manuscript content. In this paper, we propose a novel task: automated focused feedback generation for scientific writing assistance. We present SWIF<tex-math>^2</tex-math>T: a Scientific WrIting Focused Feedback Tool. It is designed to generate specific, actionable and coherent comments, which identify weaknesses in a scientific paper and/or propose revisions to it. Our approach consists of four components - planner, investigator, reviewer and controller - leveraging multiple Large Language Models (LLMs) to implement them. We compile a dataset of 300 peer reviews citing weaknesses in scientific papers and conduct human evaluation. The results demonstrate the superiority in specificity, reading comprehension, and overall helpfulness of SWIF<tex-math>^2</tex-math>T’s feedback compared to other approaches. In our analysis, we also identified cases where automatically generated reviews were judged better than human ones, suggesting opportunities for integration of AI-generated feedback in scientific writing.</abstract>
      <url hash="76c1738a">2024.findings-acl.580</url>
      <bibkey>chamoun-etal-2024-automated</bibkey>
      <doi>10.18653/v1/2024.findings-acl.580</doi>
    </paper>
    <paper id="581">
      <title><fixed-case>F</fixed-case>ast<fixed-case>GAS</fixed-case>: Fast Graph-based Annotation Selection for In-Context Learning</title>
      <author><first>Zihan</first><last>Chen</last></author>
      <author><first>Song</first><last>Wang</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Cong</first><last>Shen</last><affiliation>University of Virginia</affiliation></author>
      <author><first>Jundong</first><last>Li</last><affiliation>University of Virginia</affiliation></author>
      <pages>9764-9780</pages>
      <abstract>In-context learning (ICL) empowers large language models (LLMs) to tackle new tasks by using a series of training instances as prompts. Since generating the prompts needs to sample from a vast pool of instances and annotate them (e.g., add labels in classification task), existing methods have proposed to select a subset of unlabeled examples for annotation, thus enhancing the quality of prompts and concurrently mitigating annotation costs. However, these methods often require a long time to select instances due to their complexity, hindering their practical viability. To address this limitation, we propose a graph-based selection method, FastGAS, designed to efficiently identify high-quality instances while minimizing computational overhead. Initially, we construct a data similarity graph based on instance similarities. Subsequently, employing a graph partitioning algorithm, we partition the graph into pieces. Within each piece (i.e., subgraph), we adopt a greedy approach to pick the most representative nodes. By aggregating nodes from diverse pieces and annotating the corresponding instances, we identify a set of diverse and representative instances for ICL. Compared to prior approaches, our method not only exhibits superior performance on different tasks but also significantly reduces selection time. In addition, we demonstrate the efficacy of our approach in LLMs of larger sizes.</abstract>
      <url hash="29c65a93">2024.findings-acl.581</url>
      <bibkey>chen-etal-2024-fastgas</bibkey>
      <doi>10.18653/v1/2024.findings-acl.581</doi>
    </paper>
    <paper id="582">
      <title>Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations</title>
      <author><first>Bowen</first><last>Shen</last><affiliation>University of the Chinese Academy of Sciences</affiliation></author>
      <author><first>Zheng</first><last>Lin</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Daren</first><last>Zha</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wei</first><last>Liu</last><affiliation>xiaomi</affiliation></author>
      <author><first>Jian</first><last>Luan</last></author>
      <author><first>Bin</first><last>Wang</last><affiliation>AI Lab, Xiaomi Inc.</affiliation></author>
      <author><first>Weiping</first><last>Wang</last></author>
      <pages>9781-9793</pages>
      <abstract>Structured pruning fundamentally reduces computational and memory overheads of large language models (LLMs) and offers a feasible solution for end-side LLM deployment. Structurally pruned models remain dense and high-precision, highly compatible with further tuning and compression. However, as the coarse-grained structured pruning poses large damage to the highly interconnected model, achieving a high compression ratio for scaled-up LLMs remains a challenge. In this paper, we introduce a task-agnostic structured pruning approach coupled with a compact Transformer architecture design. The proposed approach, named TransAct, reduces transitional activations inside multi-head attention (MHA) and multi-layer perceptron (MLP) modules, while preserving the inter-module activations that are sensitive to perturbations. Hence, the LLM is pruned into an intra-module low-rank architecture, significantly reducing weights, KV Cache and attention computation. TransAct is implemented on the LLaMA model and evaluated on downstream benchmarks. Results verify the optimality of our approach at high compression with respect to both efficiency and performance. Further, ablation studies reveal the strength of activation-guided iterative pruning and provide experimental analysis on the redundancy of MHA and MLP modules.</abstract>
      <url hash="7840ab86">2024.findings-acl.582</url>
      <bibkey>shen-etal-2024-pruning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.582</doi>
    </paper>
    <paper id="583">
      <title>Integrating Multi-scale Contextualized Information for Byte-based Neural Machine Translation</title>
      <author><first>Langlin</first><last>Huang</last></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>9794-9801</pages>
      <abstract>Subword tokenization is a common method for vocabulary building in Neural Machine Translation (NMT) models. However, increasingly complex tasks have revealed its disadvantages. First, a vocabulary cannot be modified once it is learned, making it hard to adapt to new words. Second, in multilingual translation, the imbalance in data volumes across different languages spreads to the vocabulary, exacerbating translations involving low-resource languages. While byte-based tokenization addresses these issues, byte-based models struggle with the low information density inherent in UTF-8 byte sequences. Previous works enhance token semantics through local contextualization but fail to select an appropriate contextualizing scope based on the input. Consequently, we propose the Multi-Scale Contextualization (MSC) method, which learns contextualized information of varying scales across different hidden state dimensions. It then leverages the attention module to dynamically integrate the multi-scale contextualized information. Experiments show that MSC significantly outperforms subword-based and other byte-based methods in both multilingual and out-of-domain scenarios. Code can be found in https://github.com/ictnlp/Multiscale-Contextualization.</abstract>
      <url hash="62762394">2024.findings-acl.583</url>
      <bibkey>huang-feng-2024-integrating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.583</doi>
    </paper>
    <paper id="584">
      <title>Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability</title>
      <author><first>Afra Feyza</first><last>Akyürek</last><affiliation>Boston University</affiliation></author>
      <author><first>Ekin</first><last>Akyürek</last></author>
      <author><first>Leshem</first><last>Choshen</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Derry</first><last>Wijaya</last><affiliation>Monash University and Boston University</affiliation></author>
      <author><first>Jacob</first><last>Andreas</last><affiliation>Massachusetts Institute of Technology and Microsoft</affiliation></author>
      <pages>9802-9818</pages>
      <abstract>While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQuAKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26%; on CREAK, fully unsupervised DCT improves verification accuracy by 12%. These results show that LMs’ reasoning capabilities during inference can be leveraged during training to improve their reliability.</abstract>
      <url hash="d25c64da">2024.findings-acl.584</url>
      <bibkey>akyurek-etal-2024-deductive</bibkey>
      <doi>10.18653/v1/2024.findings-acl.584</doi>
    </paper>
    <paper id="585">
      <title>Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion</title>
      <author><first>Ruiqi</first><last>Li</last></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>FAIR</affiliation></author>
      <author><first>Yongqi</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zhiqing</first><last>Hong</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>9819-9831</pages>
      <abstract>Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data. Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research. This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model.We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion. We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity. SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models. Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors. Audio samples are available at https://speech2sing.github.io.</abstract>
      <url hash="bb927522">2024.findings-acl.585</url>
      <bibkey>li-etal-2024-self-supervised</bibkey>
      <doi>10.18653/v1/2024.findings-acl.585</doi>
    </paper>
    <paper id="586">
      <title>Evaluating Large Language Model Biases in Persona-Steered Generation</title>
      <author><first>Andy</first><last>Liu</last></author>
      <author><first>Mona</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Daniel</first><last>Fried</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>9832-9850</pages>
      <abstract>The task of persona-steered text generation requires large language models (LLMs) to generate text that reflects the distribution of views that an individual fitting a persona could have. People have multifaceted personas, but prior work on bias in LLM-generated opinions has only explored multiple-choice settings or one-dimensional personas. We define an incongruous persona as a persona with multiple traits where one trait makes its other traits less likely in human survey data, e.g. political liberals who support increased military spending. We find that LLMs are 9.7% less steerable towards incongruous personas than congruous ones, sometimes generating the stereotypical stance associated with its demographic rather than the target stance. Models that we evaluate that are fine-tuned with Reinforcement Learning from Human Feedback (RLHF) are more steerable, especially towards stances associated with political liberals and women, but present significantly less diverse views of personas. We also find variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation. Our results show the importance of evaluating models in open-ended text generation, as it can surface new LLM opinion biases. Moreover, such a setup can shed light on our ability to steer models toward a richer and more diverse range of viewpoints.</abstract>
      <url hash="0ef90b4d">2024.findings-acl.586</url>
      <bibkey>liu-etal-2024-evaluating-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.586</doi>
    </paper>
    <paper id="587">
      <title>Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization</title>
      <author><first>Yanghai</first><last>Zhang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Ye</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Shiwei</first><last>Wu</last><affiliation>Peking University, Peking University and The Chinese University of Hong Kong</affiliation></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Xukai</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Enhong</first><last>Chen</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>9851-9862</pages>
      <abstract>The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge distillation from a pre-trained vision-language model. Extensive experiments on public MSMO dataset validate the superiority of the EGMS method, which also prove the necessity to incorporate entity information into MSMO problem.</abstract>
      <url hash="17d4cb0f">2024.findings-acl.587</url>
      <bibkey>zhang-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.findings-acl.587</doi>
    </paper>
    <paper id="588">
      <title><fixed-case>CR</fixed-case>-<fixed-case>UTP</fixed-case>: Certified Robustness against Universal Text Perturbations on Large Language Models</title>
      <author><first>Qian</first><last>Lou</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Xin</first><last>Liang</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Jiaqi</first><last>Xue</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Yancheng</first><last>Zhang</last></author>
      <author><first>Rui</first><last>Xie</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Mengxin</first><last>Zheng</last><affiliation>University of Central Florida</affiliation></author>
      <pages>9863-9875</pages>
      <abstract>It is imperative to ensure the stability of every prediction made by a language model; that is, a language’s prediction should remain consistent despite minor input variations, like word substitutions. In this paper, we investigate the problem of certifying a language model’s robustness against Universal Text Perturbations (UTPs), which have been widely used in universal adversarial attacks and backdoor attacks. Existing certified robustness based on random smoothing has shown considerable promise in certifying the input-specific text perturbations (ISTPs), operating under the assumption that any random alteration of a sample’s clean or adversarial words would negate the impact of sample-wise perturbations. However, with UTPs, masking only the adversarial words can eliminate the attack. A naive method is to simply increase the masking ratio and the likelihood of masking attack tokens, but it leads to a significant reduction in both certified accuracy and the certified radius due to input corruption by extensive masking. To solve this challenge, we introduce a novel approach, the <i>superior prompt search</i> method, designed to identify a <i>superior prompt</i> that maintains higher certified accuracy under extensive masking. Additionally, we theoretically motivate why ensembles are a particularly suitable choice as base prompts for random smoothing. The method is denoted by <i>superior prompt ensembling</i> technique. We also empirically confirm this technique, obtaining state-of-the-art results in multiple settings. These methodologies, for the first time, enable high certified accuracy against both UTPs and ISTPs. The source code of CR-UTP is available at <url>https://github.com/UCF-ML-Research/CR-UTP</url>.</abstract>
      <url hash="a63ee7d7">2024.findings-acl.588</url>
      <bibkey>lou-etal-2024-cr</bibkey>
      <doi>10.18653/v1/2024.findings-acl.588</doi>
    </paper>
    <paper id="589">
      <title>Recovering document annotations for sentence-level bitext</title>
      <author><first>Rachel</first><last>Wicks</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Matt</first><last>Post</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Philipp</first><last>Koehn</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>9876-9890</pages>
      <abstract>In machine translation, historical models were incapable of handling longer contexts, so the lack of document-level datasets was less noticeable. Now, despite the emergence of long-sequence methods, we remain within a sentence-level paradigm and without data to adequately approach context-aware machine translation. Most large-scale datasets have been processed through a pipeline that discards document-level metadata. In this work, we reconstruct document-level information for three (ParaCrawl, News Commentary, and Europarl) large datasets in German, French, Spanish, Italian, Polish, and Portuguese (paired with English). We then introduce a document-level filtering technique as an alternative to traditional bitext filtering. We present this filtering with analysis to show that this method prefers context-consistent translations rather than those that may have been sentence-level machine translated. Last we train models on these longer contexts and demonstrate improvement in document-level translation without degradation of sentence-level translation. We release our dataset, ParaDocs, and resulting models as a resource to the community.</abstract>
      <url hash="e7d06625">2024.findings-acl.589</url>
      <bibkey>wicks-etal-2024-recovering</bibkey>
      <doi>10.18653/v1/2024.findings-acl.589</doi>
    </paper>
    <paper id="590">
      <title><fixed-case>M</fixed-case>eta<fixed-case>P</fixed-case>ro 2.0: Computational Metaphor Processing on the Effectiveness of Anomalous Language Modeling</title>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Kai</first><last>He</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Claudia</first><last>Ong</last></author>
      <author><first>Qian</first><last>Liu</last><affiliation>University of Auckland</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>9891-9908</pages>
      <abstract>Metaphor interpretation is a difficult task in natural language understanding. The development of relevant techniques in this domain is slow, mostly because of the lack of large annotated datasets and effective pre-trained language models (PLMs) for metaphor learning. Thus, we propose a large annotated dataset and a PLM for the metaphor interpretation task. Our foundation model is based on a novel anomalous language modeling (ALM) method, which we benchmark with comparable PLM baselines on the new dataset, finding that it largely improves model performance on metaphor identification and interpretation.</abstract>
      <url hash="75064667">2024.findings-acl.590</url>
      <bibkey>mao-etal-2024-metapro</bibkey>
      <doi>10.18653/v1/2024.findings-acl.590</doi>
    </paper>
    <paper id="591">
      <title>Boosting <fixed-case>LLM</fixed-case> Agents with Recursive Contemplation for Effective Deception Handling</title>
      <author><first>Shenzhi</first><last>Wang</last><affiliation>Department of Automation, Tsinghua University</affiliation></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Zilong</first><last>Zheng</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Siyuan</first><last>Qi</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Shuo</first><last>Chen</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Qisen</first><last>Yang</last></author>
      <author><first>Andrew</first><last>Zhao</last></author>
      <author><first>Chaofei</first><last>Wang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Shiji</first><last>Song</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Gao</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>9909-9953</pages>
      <abstract>Recent advances in large language models (LLMs) have led to significant success in using LLMs as agents. Nevertheless, a common assumption that LLMs always process honest information neglects the widespread deceptive or misleading content in human and AI-generated material. This oversight might expose LLMs to malicious manipulations. To enhance LLMs’ ability to identify and counteract deceptive information, in this paper, inspired by humans’ recursive thinking and perspective-taking, we introduce a novel cognitive framework, Recursive Contemplation (ReCon). ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. Additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. Specifically, the first-order allows an LLM agent to infer others’ mental states, and the second-order involves understanding how others perceive the agent’s mental state. After integrating ReCon with various LLMs, extensive experiment results from the Avalon game and BigTom benchmark indicate ReCon’s efficacy in aiding LLMs to discern and maneuver around deceptive information without extra fine-tuning and data. Finally, we demonstrate ReCon’s scaling trend with model parameters, and explore the current limitations of LLMs in terms of safety and reasoning, potentially furnishing insights for subsequent research. Our project page can be found at https://shenzhi-wang.github.io/avalon_recon.</abstract>
      <url hash="e521a7cc">2024.findings-acl.591</url>
      <bibkey>wang-etal-2024-boosting-llm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.591</doi>
    </paper>
    <paper id="592">
      <title>Direct Preference Optimization with an Offset</title>
      <author><first>Afra</first><last>Amini</last><affiliation>ETHZ - ETH Zurich</affiliation></author>
      <author><first>Tim</first><last>Vieira</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>9954-9972</pages>
      <abstract>Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal. Sometimes, the preferred response is only slightly better than the dispreferred one. In other cases, the preference is much stronger. For instance, if a response contains harmful or toxic content, the annotator will have a strong preference for that response. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.</abstract>
      <url hash="61a75103">2024.findings-acl.592</url>
      <bibkey>amini-etal-2024-direct</bibkey>
      <doi>10.18653/v1/2024.findings-acl.592</doi>
    </paper>
    <paper id="593">
      <title><fixed-case>T</fixed-case>rans<fixed-case>F</fixed-case>ace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation</title>
      <author><first>Xize</first><last>Cheng</last></author>
      <author><first>Rongjie</first><last>Huang</last><affiliation>FAIR</affiliation></author>
      <author><first>Linjun</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Zehan</first><last>Wang</last></author>
      <author><first>Tao</first><last>Jin</last></author>
      <author><first>Aoxiong</first><last>Yin</last><affiliation>Microsoft and Zhejiang University</affiliation></author>
      <author><first>Chen</first><last>Feiyang</last></author>
      <author><first>Xinyu</first><last>Duan</last></author>
      <author><first>Baoxing</first><last>Huai</last></author>
      <author><first>Zhou</first><last>Zhao</last><affiliation>Zhejiang University and Zhejiang University</affiliation></author>
      <pages>9973-9986</pages>
      <abstract>Direct speech-to-speech translation achieves high-quality results through the introduction of discrete units obtained from self-supervised learning. However, talking head translation, converting audio-visual speech (i.e., talking head video) from one language into another, still confronts several challenges compared to audio speech: (1) Existing methods invariably rely on cascading, synthesizing via both audio and text, resulting in delays and cascading errors. (2) Talking head translation has a limited set of reference frames. If the generated translation exceeds the length of the original speech, the video sequence needs to be supplemented by repeating frames, leading to jarring video transitions. In this work, we propose a model for talking head translation, <b>TransFace</b>, which can directly translate audio-visual speech into audio-visual speech in other languages. It consists of a speech-to-unit translation model to convert audio speech into discrete units and a unit-based audio-visual speech synthesizer, Unit2Lip, to re-synthesize synchronized audio-visual speech from discrete units in parallel. Furthermore, we introduce a Bounded Duration Predictor, ensuring isometric talking head translation and preventing duplicate reference frames. Experiments demonstrate that Unit2Lip significantly improves synchronization and boosts inference speed by a factor of 4.35 on LRS2. Additionally, TransFace achieves impressive BLEU scores of 61.93 and 47.55 for Es-En and Fr-En on LRS3-T and 100% isochronous translations. The samples are available at https://transface-demo.github.io .</abstract>
      <url hash="a4dbbfeb">2024.findings-acl.593</url>
      <bibkey>cheng-etal-2024-transface</bibkey>
      <doi>10.18653/v1/2024.findings-acl.593</doi>
    </paper>
    <paper id="594">
      <title>More than Minorities and Majorities: Understanding Multilateral Bias in Language Generation</title>
      <author><first>Jiaxu</first><last>Zhao</last></author>
      <author><first>Zijing</first><last>Shi</last></author>
      <author><first>Yitong</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yulong</first><last>Pei</last><affiliation>Eindhoven University of Technology</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Meng</first><last>Fang</last><affiliation>University of Liverpool and Eindhoven University of Technology</affiliation></author>
      <author><first>Mykola</first><last>Pechenizkiy</last><affiliation>Eindhoven University of Technology</affiliation></author>
      <pages>9987-10001</pages>
      <abstract>Pretrained models learned from real corpora can often capture undesirable features, leading to bias issues against different demographic groups. Most existing studies on bias dataset construction or bias mitigation methods only focus on one demographic group pair to study a certain bias, e.g. <i>black</i> vs. <i>white</i> for racial bias. However, in real-world applications, there are more than two demographic groups that are at risk of the same bias. In this paper, we propose to analyze and reduce biases across multiple demographic groups. We collect and build a multi-demographic bias dataset including five commonly discussed bias dimensions. To mitigate multi-demographic bias, we adopt several novel debiasing methods, including regularisation-based and augmentation-based methods, as well as appropriate evaluation metrics for multi-demographic bias measurement. Experimental results on the proposed multi-demographic dataset show that a fairer model can be achieved using a multi-demographic debiasing approach. Also, the model debiased using the proposed multi-demographic debiasing methods can better transfer to unseen demographics without sacrificing the performance of the pretrained model.</abstract>
      <url hash="13cc40f0">2024.findings-acl.594</url>
      <bibkey>zhao-etal-2024-minorities</bibkey>
      <doi>10.18653/v1/2024.findings-acl.594</doi>
    </paper>
    <paper id="595">
      <title>Fair Federated Learning with Biased Vision-Language Models</title>
      <author><first>Huimin</first><last>Zeng</last></author>
      <author><first>Zhenrui</first><last>Yue</last></author>
      <author><first>Yang</first><last>Zhang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Lanyu</first><last>Shang</last></author>
      <author><first>Dong</first><last>Wang</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>10002-10017</pages>
      <abstract>Existing literature that integrates CLIP into federated learning (FL) largely ignores the inherent group unfairness within CLIP and its ethical implications on FL applications. Furthermore, such CLIP bias may be amplified in FL, due to the unique issue of data heterogeneity across clients. However, in identity-sensitive FL applications, model fairness (i.e., group fairness) is imperative for model development. Therefore, this work explores a critical question ignored by the existing literature: how can we build a fair FL framework using biased pre-trained VLMs (e.g., CLIP)? To address this problem, we propose a fairness-aware adaptation framework tailored for VLM (e.g., CLIP) in the context of FL, named <b>F</b>air <b>F</b>ederated <b>D</b>eep <b>V</b>isiual <b>P</b>rompting or <b>FF-DVP</b>. As implied by its name, trains a fair FL model with fairness-aware deep visual prompting (DVP). Moreover, incorporates modality-fused classification heads to learn client-specific knowledge and fairness constraints. These modules explicitly addresses a unique bias in FL, namely the bias triggered by data heterogeneity. We show that can be readily extended to prevailing parameter-efficient fine-tuning methods (e.g., adapter or LoRA) for debiasing. To the best of our knowledge, is the first to leverage biased VLMs for building fair FL frameworks. Extensive results on human face attribute recognition (FAR) applications suggest that effectively improves model fairness and training convergence, outperforming state-of-the-art baselines.</abstract>
      <url hash="57f92d20">2024.findings-acl.595</url>
      <bibkey>zeng-etal-2024-fair</bibkey>
      <doi>10.18653/v1/2024.findings-acl.595</doi>
    </paper>
    <paper id="596">
      <title><fixed-case>S</fixed-case>peech<fixed-case>G</fixed-case>uard: Exploring the Adversarial Robustness of Multi-modal Large Language Models</title>
      <author><first>Raghuveer</first><last>Peri</last><affiliation>Amazon</affiliation></author>
      <author><first>Sai Muralidhar</first><last>Jayanthi</last><affiliation>Amazon</affiliation></author>
      <author><first>Srikanth</first><last>Ronanki</last></author>
      <author><first>Anshu</first><last>Bhatia</last><affiliation>Amazon</affiliation></author>
      <author><first>Karel</first><last>Mundnich</last></author>
      <author><first>Saket</first><last>Dingliwal</last><affiliation>Amazon</affiliation></author>
      <author><first>Nilaksh</first><last>Das</last><affiliation>Amazon</affiliation></author>
      <author><first>Zejiang</first><last>Hou</last></author>
      <author><first>Goeric</first><last>Huybrechts</last><affiliation>Amazon</affiliation></author>
      <author><first>Srikanth</first><last>Vishnubhotla</last></author>
      <author><first>Daniel</first><last>Garcia-Romero</last><affiliation>Amazon</affiliation></author>
      <author><first>Sundararajan</first><last>Srinivasan</last><affiliation>Amazon</affiliation></author>
      <author><first>Kyu</first><last>Han</last><affiliation>Amazon Web Services (AWS)</affiliation></author>
      <author><first>Katrin</first><last>Kirchhoff</last></author>
      <pages>10018-10035</pages>
      <abstract>Integrated Speech and Large Language Models (SLMs) that can follow speech instructions and generate relevant text responses have gained popularity lately. However, the safety and robustness of these models remains largely unclear. In this work, we investigate the potential vulnerabilities of such instruction-following speech-language models to adversarial attacks and jailbreaking. Specifically, we design algorithms that can generate adversarial examples to jailbreak SLMs in both white-box and black-box attack settings without human involvement. Additionally, we propose countermeasures to thwart such jailbreaking attacks. Our models, trained on dialog data with speech instructions, achieve state-of-the-art performance on spoken question-answering task, scoring over 80% on both safety and helpfulness metrics. Despite safety guardrails, experiments on jailbreaking demonstrate the vulnerability of SLMs to adversarial perturbations and transfer attacks, with average attack success rates of 90% and 10% respectively when evaluated on a dataset of carefully designed harmful questions spanning 12 different toxic categories. However, we demonstrate that our proposed countermeasures reduce the attack success significantly.</abstract>
      <url hash="e8cdaf3d">2024.findings-acl.596</url>
      <bibkey>peri-etal-2024-speechguard</bibkey>
      <doi>10.18653/v1/2024.findings-acl.596</doi>
    </paper>
    <paper id="597">
      <title><fixed-case>ACUE</fixed-case>val: Fine-grained Hallucination Evaluation and Correction for Abstractive Summarization</title>
      <author><first>David</first><last>Wan</last><affiliation>Department of Computer Science, University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Koustuv</first><last>Sinha</last><affiliation>Meta (FAIR)</affiliation></author>
      <author><first>Srini</first><last>Iyer</last><affiliation>University of Washington, Seattle and Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Asli</first><last>Celikyilmaz</last><affiliation>FAIR</affiliation></author>
      <author><first>Mohit</first><last>Bansal</last><affiliation>University of North Carolina at Chapel Hill</affiliation></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <pages>10036-10056</pages>
      <abstract>The impressive generation capabilities of large language models (LLMs) have made it harder to detect the subtle hallucinations they make in abstractive summarization, where generated summaries consist of a blend of correct and incorrect information w.r.t. a given document. Recently-proposed LLM-based evaluation metrics attempt to capture this, but still face challenges: (1) they are biased towards summaries generated from the same underlying LLM, and (2) they lack interpretability, offering only a single score. In this work, we present ACUEval, a metric that leverages the power of LLMs to perform two sub-tasks: decomposing summaries into atomic content units (ACUs), and validating them against the source document. Compared to current strong LLM-based metrics, our two-step evaluation strategy improves correlation with human judgments of faithfulness on three summarization evaluation benchmarks by 3% in balanced accuracy compared to the next-best metric, and also shows reduced preference bias towards LLM-generated summary. Further, we show that errors detected by ACUEval can be used to generate actionable feedback for refining the summary, improving the faithfulness scores by more than 10%.</abstract>
      <url hash="44f4e761">2024.findings-acl.597</url>
      <bibkey>wan-etal-2024-acueval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.597</doi>
    </paper>
    <paper id="598">
      <title>An Empirical Study on Parameter-Efficient Fine-Tuning for <fixed-case>M</fixed-case>ulti<fixed-case>M</fixed-case>odal Large Language Models</title>
      <author><first>Xiongtao</first><last>Zhou</last></author>
      <author><first>Jie</first><last>He</last></author>
      <author><first>Yuhua</first><last>Ke</last></author>
      <author><first>Guangyao</first><last>Zhu</last><affiliation>Waseda University</affiliation></author>
      <author><first>Victor</first><last>Gutierrez Basulto</last><affiliation>Cardiff University</affiliation></author>
      <author><first>Jeff</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>10057-10084</pages>
      <abstract>Multimodal Large Language Models (MLLMs) fine-tuned with multimodal instruction-following data have demonstrated formidable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging due to the rapid growth of the overall model’s parameters. To address this issue, we study Parameter-Efficient Fine-Tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing performance in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies that employ four widely used PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of PEFT module, fine-tuning data scale, model stability based on PEFT method, MLLM’s generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories, unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method in various aspects. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs.</abstract>
      <url hash="9db47324">2024.findings-acl.598</url>
      <bibkey>zhou-etal-2024-empirical</bibkey>
      <doi>10.18653/v1/2024.findings-acl.598</doi>
    </paper>
    <paper id="599">
      <title><fixed-case>PARADISE</fixed-case>: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset</title>
      <author><first>Arda</first><last>Uzunoglu</last><affiliation>KUIS AI Lab</affiliation></author>
      <author><first>Gözde</first><last>Şahin</last><affiliation>Koç University</affiliation></author>
      <author><first>Abdulfattah</first><last>Safa</last></author>
      <pages>10085-10102</pages>
      <abstract>Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q&amp;A format on practical procedural text sourced from wikiHow. It involves tip and warning inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios. Despite advancements, all models fall short of human performance. Notably, our analysis uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of BERT-family and GPT-4 with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks. The PARADISE dataset and associated resources are publicly available for further research exploration with https://anonymous.4open.science/r/paradise-53BD/README.md.</abstract>
      <url hash="1dbbf2d2">2024.findings-acl.599</url>
      <bibkey>uzunoglu-etal-2024-paradise</bibkey>
      <doi>10.18653/v1/2024.findings-acl.599</doi>
    </paper>
    <paper id="600">
      <title><fixed-case>TURNA</fixed-case>: A <fixed-case>T</fixed-case>urkish Encoder-Decoder Language Model for Enhanced Understanding and Generation</title>
      <author><first>Gökçe</first><last>Uludoğan</last></author>
      <author><first>Zeynep</first><last>Balal</last><affiliation>Boğaziçi University</affiliation></author>
      <author><first>Furkan</first><last>Akkurt</last><affiliation>Boğaziçi University</affiliation></author>
      <author><first>Meliksah</first><last>Turker</last><affiliation>Bogazici University</affiliation></author>
      <author><first>Onur</first><last>Gungor</last><affiliation>Boğaziçi University</affiliation></author>
      <author><first>Susan</first><last>Üsküdarlı</last><affiliation>Boğaziçi University</affiliation></author>
      <pages>10103-10117</pages>
      <abstract>The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce TURNA, a language model developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks.TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks and competes with monolingual Turkish models in understanding tasks.</abstract>
      <url hash="ab20c07a">2024.findings-acl.600</url>
      <bibkey>uludogan-etal-2024-turna</bibkey>
      <doi>10.18653/v1/2024.findings-acl.600</doi>
    </paper>
    <paper id="601">
      <title><fixed-case>MELD</fixed-case>-<fixed-case>ST</fixed-case>: An Emotion-aware Speech Translation Dataset</title>
      <author><first>Sirou</first><last>Chen</last></author>
      <author><first>Sakiko</first><last>Yahata</last></author>
      <author><first>Shuichiro</first><last>Shimizu</last></author>
      <author><first>Zhengdong</first><last>Yang</last><affiliation>Kyoto University, Kyoto University</affiliation></author>
      <author><first>Yihang</first><last>Li</last></author>
      <author><first>Chenhui</first><last>Chu</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Sadao</first><last>Kurohashi</last><affiliation>Kyoto University</affiliation></author>
      <pages>10118-10126</pages>
      <abstract>Emotion plays a crucial role in human conversation. This paper underscores the significance of considering emotion in speech translation. We present the MELD-ST dataset for the emotion-aware speech translation task, comprising English-to-Japanese and English-to-German language pairs. Each language pair includes about 10,000 utterances annotated with emotion labels from the MELD dataset. Baseline experiments using the SeamlessM4T model on the dataset indicate that fine-tuning with emotion labels can enhance translation performance in some settings, highlighting the need for further research in emotion-aware speech translation systems.</abstract>
      <url hash="a1ca33e9">2024.findings-acl.601</url>
      <bibkey>chen-etal-2024-meld</bibkey>
      <doi>10.18653/v1/2024.findings-acl.601</doi>
    </paper>
    <paper id="602">
      <title>Designing Informative Metrics for Few-Shot Example Selection</title>
      <author><first>Rishabh</first><last>Adiga</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Lakshmi</first><last>Subramanian</last><affiliation>New York University</affiliation></author>
      <author><first>Varun</first><last>Chandrasekaran</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <pages>10127-10135</pages>
      <abstract>Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the “best” examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.</abstract>
      <url hash="5dfff3bd">2024.findings-acl.602</url>
      <bibkey>adiga-etal-2024-designing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.602</doi>
    </paper>
    <paper id="603">
      <title>Chain-of-Quizzes: Pedagogy-inspired Example Selection in In-Context-Learning</title>
      <author><first>Yiquan</first><last>Wu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Anlai</first><last>Zhou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yuhang</first><last>Liu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yifei</first><last>Liu</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jun</first><last>Xiao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Kun</first><last>Kuang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>10136-10142</pages>
      <abstract>In-context learning (ICL) has emerged as a powerful tool for enhancing large language models (LLMs) in addressing downstream tasks. In this paper, we explore the vital task of example selection in ICL by mimicking the human learning process. We propose a Chain-of-Quizzes (CoQ) framework inspired by educational theories such as Bruner’s Spiral Learning and Mastery Learning theory. Specifically, our framework employs the LLMs to answer the quiz (question in the example) to sift ‘good’ examples, combines these examples iteratively with the increasing complexity, and utilizes a final exam to gauge the combined example chains. Our extensive experiments on diverse reasoning datasets show the proposed approach outperforms baseline models. These findings underscore the framework’s potential for future research.</abstract>
      <url hash="c83e3ceb">2024.findings-acl.603</url>
      <bibkey>wu-etal-2024-chain</bibkey>
      <doi>10.18653/v1/2024.findings-acl.603</doi>
    </paper>
    <paper id="604">
      <title>It’s Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning</title>
      <author><first>Nishant</first><last>Balepur</last></author>
      <author><first>Shramay</first><last>Palta</last></author>
      <author><first>Rachel</first><last>Rudinger</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>10143-10166</pages>
      <abstract>Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.</abstract>
      <url hash="0245a2f0">2024.findings-acl.604</url>
      <bibkey>balepur-etal-2024-easy</bibkey>
      <doi>10.18653/v1/2024.findings-acl.604</doi>
    </paper>
    <paper id="605">
      <title>From Discrimination to Generation: Low-Resource Intent Detection with Language Model Instruction Tuning</title>
      <author><first>Feng</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Fei</first><last>Ding</last><affiliation>Peking University</affiliation></author>
      <author><first>Meng</first><last>Gao</last></author>
      <author><first>Tengjiao</first><last>Wang</last></author>
      <author><first>Jiahui</first><last>Yao</last></author>
      <author><first>Jiabin</first><last>Zheng</last></author>
      <pages>10167-10183</pages>
      <abstract>Intent detection aims to identify user goals from utterances, and is a ubiquitous step towards the satisfaction of user desired needs in many interaction systems. As dynamic and varied intents arise, models that are capable of identifying new intents promptly are required. However, existing studies usually fine-tune discriminative models on the specific defined intent classes, precluding them from being directly adopted to new intent domains. In this paper, we introduce a generative pre-trained intent model that can recognize new intents from different domains in low-resource scenarios. We reformulate intent detection into a generation task and design descriptive and regularized instructions to guide the model effectively to detect new intents in open domains with no parameter updates. To validate the proposed method, we introduce a new intent detection benchmark, including the Meta-Intent Dataset and three types of representative evaluation settings. We conduct extensive experiments which demonstrate that our method outperforms a range of strong baselines that needs further fine-tuning or domain-specific samples.</abstract>
      <url hash="da60414e">2024.findings-acl.605</url>
      <bibkey>zhang-etal-2024-discrimination</bibkey>
      <doi>10.18653/v1/2024.findings-acl.605</doi>
    </paper>
    <paper id="606">
      <title>Efficient Continual Pre-training for Building Domain Specific Large Language Models</title>
      <author><first>Yong</first><last>Xie</last><affiliation>Amazon</affiliation></author>
      <author><first>Karan</first><last>Aggarwal</last><affiliation>Amazon and University of Minnesota, Minneapolis</affiliation></author>
      <author><first>Aitzaz</first><last>Ahmad</last><affiliation>Amazon</affiliation></author>
      <pages>10184-10201</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable open-domain capabilities. LLMs tailored for a domain are typically trained entirely on domain corpus to excel at handling domain-specific tasks. In this work, we explore an alternative strategy of continual pre-training as a means to develop domain-specific LLMs over an existing open-domain LLM. We introduce <i>FinPythia-6.9B</i>, developed through domain-adaptive continual pre-training on the financial domain.Continual pre-trained FinPythia showcases consistent improvements on financial tasks over the original foundational model. We further explore simple but effective data selection strategies for continual pre-training. Our data selection strategies outperform vanilla continual pre-training’s performance with just 10% of corpus size and cost, without any degradation on open-domain standard tasks. Our work proposes an alternative solution to building domain-specific LLMs cost-effectively.</abstract>
      <url hash="8f9c9706">2024.findings-acl.606</url>
      <bibkey>xie-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.findings-acl.606</doi>
    </paper>
    <paper id="607">
      <title>Distantly-Supervised Joint Extraction with Noise-Robust Learning</title>
      <author><first>Yufei</first><last>Li</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Xiao</first><last>Yu</last><affiliation>Stellar Cyber</affiliation></author>
      <author><first>Yanghong</first><last>Guo</last></author>
      <author><first>Yanchi</first><last>Liu</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Haifeng</first><last>Chen</last><affiliation>NEC-Labs</affiliation></author>
      <author><first>Cong</first><last>Liu</last><affiliation>University of California, Riverside</affiliation></author>
      <pages>10202-10217</pages>
      <abstract>Joint entity and relation extraction is a process that identifies entity pairs and their relations using a single model. We focus on the problem of joint extraction in distantly-labeled data, whose labels are generated by aligning entity mentions with the corresponding entity and relation tags using a knowledge base (KB). One key challenge is the presence of noisy labels arising from both incorrect entity and relation annotations, which significantly impairs the quality of supervised learning. Existing approaches, either considering only one source of noise or making decisions using external knowledge, cannot well-utilize significant information in the training data. We propose DENRL, a generalizable framework that 1) incorporates a lightweight transformer backbone into a sequence labeling scheme for joint tagging, and 2) employs a noise-robust framework that regularizes the tagging model with significant relation patterns and entity-relation dependencies, then iteratively self-adapts to instances with less noise from both sources. Surprisingly, experiments on two benchmark datasets show that DENRL, using merely its own parametric distribution and simple data-driven heuristics, outperforms strong baselines by a large margin with better interpretability.</abstract>
      <url hash="5675a260">2024.findings-acl.607</url>
      <bibkey>li-etal-2024-distantly</bibkey>
      <doi>10.18653/v1/2024.findings-acl.607</doi>
    </paper>
    <paper id="608">
      <title><fixed-case>LLM</fixed-case> Factoscope: Uncovering <fixed-case>LLM</fixed-case>s’ Factual Discernment through Measuring Inner States</title>
      <author><first>Jinwen</first><last>He</last><affiliation>Institute of Information Engineering,Chinese Academy of Sciences</affiliation></author>
      <author><first>Yujia</first><last>Gong</last></author>
      <author><first>Zijin</first><last>Lin</last><affiliation>SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China; School of Cyber Security, University of the Chinese Academy of Sciences, China</affiliation></author>
      <author><first>Cheng’an</first><last>Wei</last><affiliation>SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China</affiliation></author>
      <author><first>Yue</first><last>Zhao</last></author>
      <author><first>Kai</first><last>Chen</last></author>
      <pages>10218-10230</pages>
      <abstract>Large Language Models (LLMs) have revolutionized various domains with extensive knowledge and creative capabilities. However, a critical issue with LLMs is their tendency to produce outputs that diverge from factual reality. This phenomenon is particularly concerning in sensitive applications such as medical consultation and legal advice, where accuracy is paramount. Inspired by human lie detectors using physiological responses, we introduce the LLM Factoscope, a novel Siamese network-based model that leverages the inner states of LLMs for factual detection. Our investigation reveals distinguishable patterns in LLMs’ inner states when generating factual versus non-factual content. We demonstrate its effectiveness across various architectures, achieving over 96% accuracy on our custom-collected factual detection dataset. Our work opens a new avenue for utilizing LLMs’ inner states for factual detection and encourages further exploration into LLMs’ inner workings for enhanced reliability and transparency.</abstract>
      <url hash="b1ed1be9">2024.findings-acl.608</url>
      <bibkey>he-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.608</doi>
    </paper>
    <paper id="609">
      <title><fixed-case>D</fixed-case>ict<fixed-case>LLM</fixed-case>: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics</title>
      <author><first>YiQiu</first><last>Guo</last></author>
      <author><first>Yuchen</first><last>Yang</last></author>
      <author><first>Ya</first><last>Zhang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yu</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>10231-10241</pages>
      <abstract>Structured data offers an efficient means of organizing information. Exsisting text-serialization based methods for processing structured data using large language models (LLMs) are not designed to explicitly capture the heterogeneity of structured data. Such methods are suboptimal for LLMs to process structured data, and may lead to large input token size and poor robustness to input perturbation. In this paper, we propose a novel framework called DictLLM, which is an efficient and effective framework for the modeling of medical lab report to deal with the report-assisted diagnosis generation task. DictLLM introduce 1) group positional encoding to maintain the permutation invariance, 2) hierarchical attention bias to capture the inductive bias of structured data, and 3) a optimal transport alignment layer to align the embeddings generated by the dict encoder with the LLM, producing a list of fixed-length virtual tokens. We conduct experiments with multiple LLM models on a large-scale real-world medical lab report dataset for automatic diagnosis generation. The results show that our proposed framework outperforms the baseline methods and few-shot GPT-4 in terms of both Rouge-L and Knowledge F1 score. We also conduct multiple experiments and analyze the scalability and robustness of our proposed framework, demonstrating the superiority of our method in modeling the heterogeneous structure of medical dictionaries data.</abstract>
      <url hash="b76e1a3c">2024.findings-acl.609</url>
      <bibkey>guo-etal-2024-dictllm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.609</doi>
    </paper>
    <paper id="610">
      <title>imap<fixed-case>S</fixed-case>core: Medical Fact Evaluation Made Easy</title>
      <author><first>Huimin</first><last>Wang</last><affiliation>Jarvis Research Center, Tencent YouTu Lab</affiliation></author>
      <author><first>Yutian</first><last>Zhao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xian</first><last>Wu</last><affiliation>Tencent</affiliation></author>
      <author><first>Yefeng</first><last>Zheng</last></author>
      <pages>10242-10257</pages>
      <abstract>Automatic evaluation of natural language generation (NLG) tasks has gained extensive research interests, since it can rapidly assess the performance of large language models (LLMs). However, automatic NLG evaluation struggles with medical QA because it fails to focus on the crucial correctness of medical facts throughout the generated text. To address this, this paper introduces a new data structure, <i>imap</i>, designed to capture key information in questions and answers, enabling evaluators to focus on essential details. The <i>imap</i> comprises three components: Query, Constraint, and Inform, each of which is in the form of term-value pairs to represent medical facts in a structural manner. We then introduce <i>imap</i>Score, which compares the corresponding medical term-value pairs in the <i>imap</i> to score generated texts. We utilize GPT-4 to extract <i>imap</i> from questions, human-annotated answers, and generated responses. To mitigate the diversity in medical terminology for fair term-value pairs comparison, we use a medical knowledge graph to assist GPT-4 in determining matches. To compare <i>imap</i>Score with existing NLG metrics, we establish a new benchmark dataset. The experimental results show that <i>imap</i>Score consistently outperforms state-of-the-art metrics, demonstrating an average improvement of 79.8% in correlation with human scores. Furthermore, incorporating <i>imap</i> into n-gram, embedding, and LLM metrics boosts the base versions, increasing correlation with human scores by averages of 89.9%, 81.7%, and 32.6%, respectively.</abstract>
      <url hash="331c0978">2024.findings-acl.610</url>
      <bibkey>wang-etal-2024-imapscore</bibkey>
      <doi>10.18653/v1/2024.findings-acl.610</doi>
    </paper>
    <paper id="611">
      <title>Making Harmful Behaviors Unlearnable for Large Language Models</title>
      <author><first>Xin</first><last>Zhou</last></author>
      <author><first>Yi</first><last>Lu</last></author>
      <author><first>Ruotian</first><last>Ma</last></author>
      <author><first>Yujian</first><last>Wei</last></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <pages>10258-10273</pages>
      <abstract>Large language models (LLMs) have shown great potential to empower various domains and are often customized by fine-tuning for the requirements of different applications. However, the powerful learning ability of LLMs not only enables them to learn new tasks but also makes them vulnerable to learning undesired behaviors, such as harmfulness and hallucination, as the fine-tuning data often implicitly or explicitly contains such content. Can we fine-tune LLMs on harmful data without learning harmful behaviors? This paper proposes a controllable training framework to make undesired behaviors unlearnable during the fine-tuning process. Specifically, we introduce security vectors to control the model’s behavior and make it consistent with the undesired behavior. Security vectors are activated during fine-tuning, the consistent behavior makes the model believe that such behavior has already been learned and there is no need for further optimization, while inconsistent data can still be learned. After fine-tuning, security vectors are deactivated to restore the LLM’s normal behavior. Our experiments show that the security vectors can prevent LLM from learning harmful and hallucination behavior while preserving the ability to learn other information.</abstract>
      <url hash="30fccefc">2024.findings-acl.611</url>
      <bibkey>zhou-etal-2024-making</bibkey>
      <doi>10.18653/v1/2024.findings-acl.611</doi>
    </paper>
    <paper id="612">
      <title>Debiasing Large Language Models with Structured Knowledge</title>
      <author><first>Congda</first><last>Ma</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Tianyu</first><last>Zhao</last><affiliation>Sakana AI</affiliation></author>
      <author><first>Manabu</first><last>Okumura</last><affiliation>Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>10274-10287</pages>
      <abstract>Due to biases inherently present in data for pre-training, current pre-trained Large Language Models (LLMs) also ubiquitously manifest the same phenomena. Since the bias influences the output from the LLMs across various tasks, the widespread deployment of the LLMs is hampered. We propose a simple method that utilizes structured knowledge to alleviate this issue, aiming to reduce the bias embedded within the LLMs and ensuring they have an encompassing perspective when used in applications. Experimental results indicated that our method has good debiasing ability when applied to existing both autoregressive and masked language models. Additionally, it could ensure that the performances of LLMs on downstream tasks remain uncompromised.Our method outperforms state-of-the-art (SOTA) baselines in the debiasing ability. Importantly, our method obviates the need for training from scratch, thus offering enhanced scalability and cost-effectiveness.</abstract>
      <url hash="89dacbf0">2024.findings-acl.612</url>
      <bibkey>ma-etal-2024-debiasing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.612</doi>
    </paper>
    <paper id="613">
      <title>Contrastive Instruction Tuning</title>
      <author><first>Tianyi</first><last>Yan</last></author>
      <author><first>Fei</first><last>Wang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>James Y.</first><last>Huang</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Wenxuan</first><last>Zhou</last><affiliation>Zoom</affiliation></author>
      <author><first>Fan</first><last>Yin</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Aram</first><last>Galstyan</last><affiliation>Information Sciences Institute, University of Southern California, University of Southern California, University of Southern California and Amazon Alexa</affiliation></author>
      <author><first>Wenpeng</first><last>Yin</last><affiliation>Pennsylvania State University</affiliation></author>
      <author><first>Muhao</first><last>Chen</last><affiliation>University of California, Davis and University of Southern California</affiliation></author>
      <pages>10288-10302</pages>
      <abstract>Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs’ lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs’ robustness to unseen instructions with variations across character, word, sentence, and semantic levels by an average of +2.5% in accuracy.</abstract>
      <url hash="1d78fb55">2024.findings-acl.613</url>
      <bibkey>yan-etal-2024-contrastive</bibkey>
      <doi>10.18653/v1/2024.findings-acl.613</doi>
    </paper>
    <paper id="614">
      <title>Bootstrapped Pre-training with Dynamic Identifier Prediction for Generative Retrieval</title>
      <author><first>Yubao</first><last>Tang</last></author>
      <author><first>Ruqing</first><last>Zhang</last></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Maarten</first><last>Rijke</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Yixing</first><last>Fan</last></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <pages>10303-10317</pages>
      <abstract>Generative retrieval uses differentiable search indexes to directly generate relevant document identifiers in response to a query. Recent studies have highlighted the potential of a strong generative retrieval model, trained with carefully crafted pre-training tasks, to enhance downstream retrieval tasks via fine-tuning. However, the full power of pre-training for generative retrieval remains underexploited due to its reliance on pre-defined static document identifiers, which may not align with evolving model parameters. In this work, we introduce BootRet, a bootstrapped pre-training method for generative retrieval that dynamically adjusts document identifiers during pre-training to accommodate the continuing memorization of the corpus. BootRet involves three key training phases: (i) initial identifier generation, (ii) pre-training via corpus indexing and relevance prediction tasks, and (iii) bootstrapping for identifier updates. To facilitate the pre-training phase, we further introduce noisy documents and pseudo-queries, generated by large language models, to resemble semantic connections in both indexing and retrieval tasks. Experimental results demonstrate that BootRet significantly outperforms existing pre-training generative retrieval baselines and performs well even in zero-shot settings.</abstract>
      <url hash="24cf73a4">2024.findings-acl.614</url>
      <bibkey>tang-etal-2024-bootstrapped</bibkey>
      <doi>10.18653/v1/2024.findings-acl.614</doi>
    </paper>
    <paper id="615">
      <title>Refining and Synthesis: A Simple yet Effective Data Augmentation Framework for Cross-Domain Aspect-based Sentiment Analysis</title>
      <author><first>Haining</first><last>Wang</last></author>
      <author><first>Kang</first><last>He</last></author>
      <author><first>Bobo</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Lei</first><last>Chen</last></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Chong</first><last>Teng</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <pages>10318-10329</pages>
      <abstract>Aspect-based Sentiment Analysis (ABSA) is extensively researched in the NLP community, yet related models face challenges due to data sparsity when shifting to a new domain. Hence, data augmentation for cross-domain ABSA has attracted increasing attention in recent years. However, two key points have been neglected in prior studies: First, target domain unlabeled data are labeled with pseudo labels by the model trained in the source domain with little quality control, leading to inaccuracy and error propagation. Second, the label and text patterns of generated labeled data are monotonous, thus limiting the robustness and generalization ability of trained ABSA models. In this paper, we aim to design a simple yet effective framework to address the above shortages in ABSA data augmentation, called Refining and Synthesis Data Augmentation (RSDA). Our framework roughly includes two steps: First, it refines generated labeled data using a natural language inference (NLI) filter to control data quality. Second, it synthesizes diverse labeled data via novel label composition and paraphrase approaches. We conduct experiments on 4 kinds of ABSA subtasks, and our framework outperforms 7 strong baselines, demonstrating its effectiveness.</abstract>
      <url hash="5ec1f21e">2024.findings-acl.615</url>
      <bibkey>wang-etal-2024-refining</bibkey>
      <doi>10.18653/v1/2024.findings-acl.615</doi>
    </paper>
    <paper id="616">
      <title>Codec-<fixed-case>SUPERB</fixed-case>: An In-Depth Analysis of Sound Codec Models</title>
      <author><first>Haibin</first><last>Wu</last></author>
      <author><first>Ho-Lam</first><last>Chung</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Yi-Cheng</first><last>Lin</last></author>
      <author><first>Yuan-Kuei</first><last>Wu</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Xuanjun</first><last>Chen</last></author>
      <author><first>Yu-Chi</first><last>Pai</last></author>
      <author><first>Hsiu-Hsuan</first><last>Wang</last></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Alexander</first><last>Liu</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>10330-10348</pages>
      <abstract>The sound codec’s dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance.Recent years have witnessed significant developments in codec models.The ideal sound codec should preserve content, paralinguistics, speakers, and audio information.However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings.This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark.It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs.Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons.Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.</abstract>
      <url hash="522880a1">2024.findings-acl.616</url>
      <bibkey>wu-etal-2024-codec</bibkey>
      <doi>10.18653/v1/2024.findings-acl.616</doi>
    </paper>
    <paper id="617">
      <title><fixed-case>CACL</fixed-case>: Community-Aware Heterogeneous Graph Contrastive Learning for Social Media Bot Detection</title>
      <author><first>Sirry</first><last>Chen</last></author>
      <author><first>Shuo</first><last>Feng</last></author>
      <author><first>Liang</first><last>Songsong</last></author>
      <author><first>Chen-Chen</first><last>Zong</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Piji</first><last>Li</last><affiliation>Nanjing University of Aeronautics and Astronautics</affiliation></author>
      <pages>10349-10360</pages>
      <abstract>Social media bot detection is increasingly crucial with the rise of social media platforms. Existing methods predominantly construct social networks as graph and utilize graph neural networks (GNNs) for bot detection. However, most of these methods focus on how to improve the performance of GNNs while neglecting the community structure within social networks. Moreover, GNNs based methods still face problems such as poor model generalization due to the relatively small scale of the dataset and over-smoothness caused by information propagation mechanism. To address these problems, we propose the Community-Aware Heterogeneous Graph Contrastive Learning framework (i.e., CACL), which constructs social network as heterogeneous graph with multiple node types and edge types, and then utilizes community-aware module to mine both hard positive samples and hard negative samples for supervised graph contrastive learning with adaptive graph enhancement algorithms. Extensive experiments demonstrate that our framework addresses the previously mentioned challenges and outperforms competitive baselines on three social media bot benchmarks.</abstract>
      <url hash="5516e409">2024.findings-acl.617</url>
      <bibkey>chen-etal-2024-cacl</bibkey>
      <doi>10.18653/v1/2024.findings-acl.617</doi>
    </paper>
    <paper id="618">
      <title>Are Machines Better at Complex Reasoning? Unveiling Human-Machine Inference Gaps in Entailment Verification</title>
      <author><first>Soumya</first><last>Sanyal</last></author>
      <author><first>Tianyi</first><last>Xiao</last></author>
      <author><first>Jiacheng</first><last>Liu</last><affiliation>Allen Institute for Artificial Intelligence and Paul G. Allen School of Computer Science and Engineering, University of Washington</affiliation></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>10361-10386</pages>
      <abstract>Making inferences in text comprehension to understand the meaning is essential in language processing. This work studies the entailment verification (EV) problem of complex, multi-sentence premises requiring a system to make multiple inferences implicitly. Modern applications of EV in detecting inconsistent model-generated rationales require complex multi-hop reasoning. However, current textual inference datasets mostly contain short-sentence premises that partially focus on this. To address this, we compile an EV benchmark that includes datasets from three NLP domains (NLI, contextual QA, and rationales) containing multi-sentence premises. On benchmarking humans and LLMs, we find that LLMs are better than humans in multi-hop reasoning across extended contexts, while humans perform better in simple deductive reasoning tasks. We also finetune a Flan-T5 model for EV using two training objectives to obtain a strong open-source model that outperforms GPT-3.5 and rivals GPT-4. Finally, we use our finetuned model to filter out inconsistent model-generated rationales in self-consistency decoding, resulting in a 6% accuracy improvement on average across three MCQ datasets.</abstract>
      <url hash="eb572d33">2024.findings-acl.618</url>
      <bibkey>sanyal-etal-2024-machines</bibkey>
      <doi>10.18653/v1/2024.findings-acl.618</doi>
    </paper>
    <paper id="619">
      <title><fixed-case>C</fixed-case>hart<fixed-case>I</fixed-case>nstruct: Instruction Tuning for Chart Comprehension and Reasoning</title>
      <author><first>Ahmed</first><last>Masry</last><affiliation>York University</affiliation></author>
      <author><first>Mehrad</first><last>Shahmohammadi</last></author>
      <author><first>Md Rizwan</first><last>Parvez</last><affiliation>Qatar Computing Research Institute and Bosch</affiliation></author>
      <author><first>Enamul</first><last>Hoque</last><affiliation>York University</affiliation></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>10387-10409</pages>
      <abstract>Charts provide visual representations of data and are widely used for analyzing information, addressing queries, and conveying insights to others. Various chart-related downstream tasks have emerged recently, such as question-answering and summarization. A common strategy to solve these tasks is to fine-tune various models originally trained on vision tasks language. However, such task-specific models are not capable of solving a wide range of chart-related tasks, constraining their real-world applicability. To overcome these challenges, we introduce ChartInsruct: a novel chart-specific vision-language Instruction-following dataset comprising 191K instructions generated with 71K charts. We then present two distinct systems for instruction tuning on such datasets: (1) an end-to-end model that connects a vision encoder for chart understanding with a LLM; and (2) a pipeline model that employs a two-step approach to extract chart data tables and input them into the LLM. In experiments on four downstream tasks, we first show the effectiveness of our model–achieving a new set of state-of-the-art results. Further evaluation shows that our instruction-tuning approach supports a wide array of real-world chart comprehension and reasoning scenarios, thereby expanding the scope and applicability of our models to new kinds of tasks.</abstract>
      <url hash="3ef236a5">2024.findings-acl.619</url>
      <bibkey>masry-etal-2024-chartinstruct</bibkey>
      <doi>10.18653/v1/2024.findings-acl.619</doi>
    </paper>
    <paper id="620">
      <title>Improving Multilingual Neural Machine Translation by Utilizing Semantic and Linguistic Features</title>
      <author><first>Mengyu</first><last>Bu</last></author>
      <author><first>Shuhao</first><last>Gu</last></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>10410-10423</pages>
      <abstract>The many-to-many multilingual neural machine translation can be regarded as the process of integrating semantic features from the source sentences and linguistic features from the target sentences. To enhance zero-shot translation, models need to share knowledge across languages, which can be achieved through auxiliary tasks for learning a universal representation or cross-lingual mapping. To this end, we propose to exploit both semantic and linguistic features between multiple languages to enhance multilingual translation. On the encoder side, we introduce a disentangling learning task that aligns encoder representations by disentangling semantic and linguistic features, thus facilitating knowledge transfer while preserving complete information. On the decoder side, we leverage a linguistic encoder to integrate low-level linguistic features to assist in the target language generation. Experimental results on multilingual datasets demonstrate significant improvement in zero-shot translation compared to the baseline system, while maintaining performance in supervised translation. Further analysis validates the effectiveness of our method in leveraging both semantic and linguistic features.</abstract>
      <url hash="2cd1d761">2024.findings-acl.620</url>
      <bibkey>bu-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.findings-acl.620</doi>
    </paper>
    <paper id="621">
      <title>Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts</title>
      <author><first>Ganesh</first><last>Jawahar</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Haichuan</first><last>Yang</last><affiliation>Facebook</affiliation></author>
      <author><first>Yunyang</first><last>Xiong</last><affiliation>Facebook</affiliation></author>
      <author><first>Zechun</first><last>Liu</last><affiliation>Meta Inc.</affiliation></author>
      <author><first>Dilin</first><last>Wang</last><affiliation>Facebook</affiliation></author>
      <author><first>Fei</first><last>Sun</last><affiliation>Meta Inc.</affiliation></author>
      <author><first>Meng</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <author><first>Aasish</first><last>Pappu</last><affiliation>Meta AI</affiliation></author>
      <author><first>Barlas</first><last>Oguz</last><affiliation>Meta</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Laks</first><last>Lakshmanan</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Raghuraman</first><last>Krishnamoorthi</last><affiliation>Facebook</affiliation></author>
      <author><first>Vikas</first><last>Chandra</last><affiliation>Meta</affiliation></author>
      <pages>10424-10443</pages>
      <abstract>Weight-sharing supernets are crucial for performance estimation in cutting-edge neural architecture search (NAS) frameworks. Despite their ability to generate diverse subnetworks without retraining, the quality of these subnetworks is not guaranteed due to weight sharing. In NLP tasks like machine translation and pre-trained language modeling, there is a significant performance gap between supernet and training from scratch for the same model architecture, necessitating retraining post optimal architecture identification.This study introduces a solution called mixture-of-supernets, a generalized supernet formulation leveraging mixture-of-experts (MoE) to enhance supernet model expressiveness with minimal training overhead. Unlike conventional supernets, this method employs an architecture-based routing mechanism, enabling indirect sharing of model weights among subnetworks. This customization of weights for specific architectures, learned through gradient descent, minimizes retraining time, significantly enhancing training efficiency in NLP. The proposed method attains state-of-the-art (SoTA) performance in NAS for fast machine translation models, exhibiting a superior latency-BLEU tradeoff compared to HAT, the SoTA NAS framework for machine translation. Furthermore, it excels in NAS for building memory-efficient task-agnostic BERT models, surpassing NAS-BERT and AutoDistil across various model sizes. The code can be found at: https://github.com/UBC-NLP/MoS.</abstract>
      <url hash="1811e1a7">2024.findings-acl.621</url>
      <bibkey>jawahar-etal-2024-mixture</bibkey>
      <doi>10.18653/v1/2024.findings-acl.621</doi>
    </paper>
    <paper id="622">
      <title><fixed-case>S</fixed-case>hared<fixed-case>C</fixed-case>on: Implicit Hate Speech Detection using Shared Semantics</title>
      <author><first>Hyeseon</first><last>Ahn</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Youngwook</first><last>Kim</last><affiliation>KT Corporation</affiliation></author>
      <author><first>Jungin</first><last>Kim</last></author>
      <author><first>Yo-Sub</first><last>Han</last><affiliation>Yonsei University</affiliation></author>
      <pages>10444-10455</pages>
      <abstract>The ever-growing presence of hate speech on social network services and other online platforms not only fuels online harassment but also presents a growing challenge for hate speech detection. As this task is akin to binary classification, one of the promising approaches for hate speech detection is the utilization of contrastive learning. Recent studies suggest that classifying hateful posts in just a binary manner may not adequately address the nuanced task of detecting implicit hate speech. This challenge is largely due to the subtle nature and context dependency of such pejorative remarks. Previous studies proposed a modified contrastive learning approach equipped with additional aids such as human-written implications or machine-generated augmented data for better implicit hate speech detection. While this approach can potentially enhance the overall performance by its additional data in general, it runs the risk of overfitting as well as heightened cost and time to obtain. These drawbacks serve as motivation for us to design a methodology that is not dependent on human-written or machine-generated augmented data for training. We propose a straightforward, yet effective, clustering-based contrastive learning approach that leverages the shared semantics among the data.</abstract>
      <url hash="a188e603">2024.findings-acl.622</url>
      <bibkey>ahn-etal-2024-sharedcon</bibkey>
      <doi>10.18653/v1/2024.findings-acl.622</doi>
    </paper>
    <paper id="623">
      <title>Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models</title>
      <author><first>Dheeraj</first><last>Mekala</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Alex</first><last>Nguyen</last></author>
      <author><first>Jingbo</first><last>Shang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>10456-10470</pages>
      <abstract>Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics &amp; humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.</abstract>
      <url hash="34087d4c">2024.findings-acl.623</url>
      <bibkey>mekala-etal-2024-smaller</bibkey>
      <doi>10.18653/v1/2024.findings-acl.623</doi>
    </paper>
    <paper id="624">
      <title><fixed-case>I</fixed-case>njec<fixed-case>A</fixed-case>gent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents</title>
      <author><first>Qiusi</first><last>Zhan</last><affiliation>University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Zhixiang</first><last>Liang</last></author>
      <author><first>Zifan</first><last>Ying</last></author>
      <author><first>Daniel</first><last>Kang</last><affiliation>Department of Computer Science</affiliation></author>
      <pages>10471-10506</pages>
      <abstract>Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We conduct a comprehensive evaluation of 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates. Our findings raise questions about the widespread deployment of LLM Agents.</abstract>
      <url hash="fb92b5d8">2024.findings-acl.624</url>
      <bibkey>zhan-etal-2024-injecagent</bibkey>
      <doi>10.18653/v1/2024.findings-acl.624</doi>
    </paper>
    <paper id="625">
      <title>Generalization-Enhanced Code Vulnerability Detection via Multi-Task Instruction Fine-Tuning</title>
      <author><first>Xiaohu</first><last>Du</last></author>
      <author><first>Ming</first><last>Wen</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Jiahao</first><last>Zhu</last></author>
      <author><first>Zifan</first><last>Xie</last></author>
      <author><first>Bin</first><last>Ji</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Huijun</first><last>Liu</last></author>
      <author><first>Xuanhua</first><last>Shi</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Hai</first><last>Jin</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <pages>10507-10521</pages>
      <abstract>Code Pre-trained Models (CodePTMs) based vulnerability detection have achieved promising results over recent years. However, these models struggle to generalize as they typically learn superficial mapping from source code to labels instead of understanding the root causes of code vulnerabilities, resulting in poor performance in real-world scenarios beyond the training instances. To tackle this challenge, we introduce VulLLM, a novel framework that integrates multi-task learning with Large Language Models (LLMs) to effectively mine deep-seated vulnerability features. Specifically, we construct two auxiliary tasks beyond the vulnerability detection task. First, we utilize the vulnerability patches to construct a vulnerability localization task. Second, based on the vulnerability features extracted from patches, we leverage GPT-4 to construct a vulnerability interpretation task. VulLLM innovatively augments vulnerability classification by leveraging generative LLMs to understand complex vulnerability patterns, thus compelling the model to capture the root causes of vulnerabilities rather than overfitting to spurious features of a single task. The experiments conducted on six large datasets demonstrate that VulLLM surpasses seven state-of-the-art models in terms of effectiveness, generalization, and robustness.</abstract>
      <url hash="baa4a189">2024.findings-acl.625</url>
      <bibkey>du-etal-2024-generalization</bibkey>
      <doi>10.18653/v1/2024.findings-acl.625</doi>
    </paper>
    <paper id="626">
      <title><fixed-case>PPTSER</fixed-case>: A Plug-and-Play Tag-guided Method for Few-shot Semantic Entity Recognition on Visually-rich Documents</title>
      <author><first>Wenhui</first><last>Liao</last></author>
      <author><first>Jiapeng</first><last>Wang</last></author>
      <author><first>Zening</first><last>Lin</last><affiliation>South China University of Technology</affiliation></author>
      <author><first>Longfei</first><last>Xiong</last><affiliation>Kingsoft Office</affiliation></author>
      <author><first>Lianwen</first><last>Jin</last><affiliation>South China University of Technology</affiliation></author>
      <pages>10522-10539</pages>
      <abstract>Visually-rich document information extraction (VIE) is a vital aspect of document understanding, wherein Semantic Entity Recognition (SER) plays a significant role. However, few-shot SER on visually-rich documents remains relatively unexplored despite its considerable potential for practical applications. To address this issue, we propose a simple yet effective Plug-and-Play Tag-guided method for few-shot Semantic Entity Recognition (PPTSER) on visually-rich documents. PPTSER is built upon off-the-shelf multi-modal pre-trained models. It leverages the semantics of the tags to guide the SER task, reformulating SER into entity typing and span detection, handling both tasks simultaneously via cross-attention. Experimental results illustrate that PPTSER outperforms existing fine-tuning and few-shot methods, especially in low-data regimes. With full training data, PPTSER achieves comparable or superior performance to fine-tuning baseline. For instance, on the FUNSD benchmark, our method improves the performance of LayoutLMv3-base in 1-shot, 3-shot and 5-shot scenarios by 15.61%, 2.13%, and 2.01%, respectively. Overall, PPTSER demonstrates promising generalizability, effectiveness, and plug-and-play nature for few-shot SER on visually-rich documents. The codes will be available at [https://github.com/whlscut/PPTSER](https://github.com/whlscut/PPTSER).</abstract>
      <url hash="8aedc86e">2024.findings-acl.626</url>
      <bibkey>liao-etal-2024-pptser</bibkey>
      <doi>10.18653/v1/2024.findings-acl.626</doi>
    </paper>
    <paper id="627">
      <title><fixed-case>LLM</fixed-case> Performance Predictors are good initializers for Architecture Search</title>
      <author><first>Ganesh</first><last>Jawahar</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Laks</first><last>Lakshmanan</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Dujian</first><last>Ding</last><affiliation>Computing Science, University of British Columbia</affiliation></author>
      <pages>10540-10560</pages>
      <abstract>In this work, we utilize Large Language Models (LLMs) for a novel use case: constructing Performance Predictors (PP) that estimate the performance of specific deep neural network architectures on downstream tasks. We create PP prompts for LLMs, comprising (i) role descriptions, (ii) instructions for the LLM, (iii) hyperparameter definitions, and (iv) demonstrations presenting sample architectures with efficiency metrics and ‘training from scratch’ performance. In machine translation (MT) tasks, GPT-4 with our PP prompts (LLM-PP) achieves a SoTA mean absolute error and a slight degradation in rank correlation coefficient compared to baseline predictors. Additionally, we demonstrate that predictions from LLM-PP can be distilled to a compact regression model (LLM-Distill-PP), which surprisingly retains much of the performance of LLM-PP. This presents a cost-effective alternative for resource-intensive performance estimation. Specifically, for Neural Architecture Search (NAS), we introduce a Hybrid-Search algorithm (HS-NAS) employing LLM-Distill-PP for the initial search stages and reverting to the baseline predictor later. HS-NAS performs similarly to SoTA NAS, reducing search hours by approximately 50%, and in some cases, improving latency, GFLOPs, and model size. The code can be found at: https://github.com/UBC-NLP/llmas.</abstract>
      <url hash="91b1a431">2024.findings-acl.627</url>
      <bibkey>jawahar-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.627</doi>
    </paper>
    <paper id="628">
      <title><fixed-case>MODDP</fixed-case>: A Multi-modal Open-domain <fixed-case>C</fixed-case>hinese Dataset for Dialogue Discourse Parsing</title>
      <author><first>Chen</first><last>Gong</last></author>
      <author><first>DeXin</first><last>Kong</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Suxian</first><last>Zhao</last></author>
      <author><first>Xingyu</first><last>Li</last></author>
      <author><first>Guohong</first><last>Fu</last></author>
      <pages>10561-10573</pages>
      <abstract>Dialogue discourse parsing (DDP) aims to capture the relations between utterances in the dialogue. In everyday real-world scenarios, dialogues are typically multi-modal and cover open-domain topics. However, most existing widely used benchmark datasets for DDP contain only textual modality and are domain-specific. This makes it challenging to accurately and comprehensively understand the dialogue without multi-modal clues, and prevents them from capturing the discourse structures of the more prevalent daily conversations. This paper proposes MODDP, the first multi-modal Chinese discourse parsing dataset derived from open-domain daily dialogues, consisting 864 dialogues and 18,114 utterances, accompanied by 12.7 hours of video clips. We present a simple yet effective benchmark approach for multi-modal DDP. Through extensive experiments, we present several benchmark results based on MODDP. The significant improvement in performance from introducing multi-modalities into the original textual unimodal DDP model demonstrates the necessity of integrating multi-modalities into DDP.</abstract>
      <url hash="e67b03be">2024.findings-acl.628</url>
      <bibkey>gong-etal-2024-moddp</bibkey>
      <doi>10.18653/v1/2024.findings-acl.628</doi>
    </paper>
    <paper id="629">
      <title><fixed-case>C</fixed-case>hinese <fixed-case>M</fixed-case>ental<fixed-case>BERT</fixed-case>: Domain-Adaptive Pre-training on Social Media for <fixed-case>C</fixed-case>hinese Mental Health Text Analysis</title>
      <author><first>Wei</first><last>Zhai</last></author>
      <author><first>Hongzhi</first><last>Qi</last></author>
      <author><first>Qing</first><last>Zhao</last></author>
      <author><first>Jianqiang</first><last>Li</last><affiliation>Beijing University of Technology</affiliation></author>
      <author><first>Ziqi</first><last>Wang</last></author>
      <author><first>Han</first><last>Wang</last></author>
      <author><first>Bing</first><last>Yang</last></author>
      <author><first>Guanghui</first><last>Fu</last></author>
      <pages>10574-10585</pages>
      <abstract>In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there’s a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model’s applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language model, we performed adaptive training to develop a model specialized for the psychological domain. We evaluated our model’s performance across six public datasets, where it demonstrated improvements compared to eight other models. Additionally, in the qualitative comparison experiment, our model provided psychologically relevant predictions given the masked sentences. Due to concerns regarding data privacy, the dataset will not be made publicly available. However, we have made the pre-trained models and codes publicly accessible to the community via: https://github.com/zwzzzQAQ/Chinese-MentalBERT.</abstract>
      <url hash="4e6236c6">2024.findings-acl.629</url>
      <bibkey>zhai-etal-2024-chinese</bibkey>
      <doi>10.18653/v1/2024.findings-acl.629</doi>
    </paper>
    <paper id="630">
      <title>Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization</title>
      <author><first>Zhanhui</first><last>Zhou</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Jie</first><last>Liu</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Xiangyu</first><last>Yue</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Chao</first><last>Yang</last></author>
      <author><first>Wanli</first><last>Ouyang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Yu</first><last>Qiao</last></author>
      <pages>10586-10613</pages>
      <abstract>A single language model, even when aligned with labelers through reinforcement learning from human feedback (RLHF), may not suit all human preferences. Recent approaches therefore prefer customization, gathering multi-dimensional feedback, and creating distinct reward models for each dimension.Different language models are then optimized for various preferences using multi-objective RLHF (MORLHF) with varying reward weights.However, RL fine-tuning is unstable and resource-heavy, especially with diverse and usually conflicting objectives.In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free extension of Direct Preference Optimization (DPO) for multiple alignment objectives.Essentially, MODPO folds language modeling directly into reward modeling, training language models as implicit collective reward models that combine all objectives with specific weights. MODPO theoretically yields the same optimal solutions as MORLHF but is practically more stable and efficient.Empirical results in safety alignment and long-form question answering show that MODPO matches or outperforms existing methods, producing a Pareto front of language models catering to diverse preferences with three times less computational resources compared to MORLHF.Code is available at https://github.com/ZHZisZZ/modpo.</abstract>
      <url hash="121982a5">2024.findings-acl.630</url>
      <bibkey>zhou-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.findings-acl.630</doi>
    </paper>
    <paper id="631">
      <title><fixed-case>DORY</fixed-case>: Deliberative Prompt Recovery for <fixed-case>LLM</fixed-case></title>
      <author><first>Lirong</first><last>Gao</last></author>
      <author><first>Ru</first><last>Peng</last></author>
      <author><first>Yiming</first><last>Zhang</last></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>10614-10632</pages>
      <abstract>Prompt recovery in large language models (LLMs) is crucial for understanding how LLMs work and addressing concerns regarding privacy, copyright, etc. The trend towards inference-only APIs complicates this task by restricting access to essential outputs for recovery. To tackle this challenge, we extract prompt-related information from limited outputs and identify a strong(negative) correlation between output probability-based uncertainty and the success of prompt recovery.This finding led to the development of Deliberative PrOmpt RecoverY (DORY), our novel approach that leverages uncertainty to recover prompts accurately. DORY involves reconstructing drafts from outputs, refining these with hints, and filtering out noise based on uncertainty. Our evaluation shows that DORY outperforms existing baselines across diverse LLMs and prompt benchmarks, improving performance by approximately 10.82% and establishing a new state-of-the-art record in prompt recovery tasks. Significantly, DORY operates using a single LLM without any external resources or model, offering a cost-effective, user-friendly prompt recovery solution.</abstract>
      <url hash="5024fd1a">2024.findings-acl.631</url>
      <bibkey>gao-etal-2024-dory</bibkey>
      <doi>10.18653/v1/2024.findings-acl.631</doi>
    </paper>
    <paper id="632">
      <title><fixed-case>STYLE</fixed-case>: Improving Domain Transferability of Asking Clarification Questions in Large Language Model Powered Conversational Agents</title>
      <author><first>Yue</first><last>Chen</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Chen</first><last>Huang</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Yang</first><last>Deng</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Wenqiang</first><last>Lei</last><affiliation>Sichuan University</affiliation></author>
      <author><first>Dingnan</first><last>Jin</last></author>
      <author><first>Jia</first><last>Liu</last></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>10633-10649</pages>
      <abstract>Equipping a conversational search engine with strategies regarding when to ask clarification questions is becoming increasingly important across various domains. Attributing to the context understanding capability of LLMs and their access to domain-specific sources of knowledge, LLM-based clarification strategies feature rapid transfer to various domains in a post-hoc manner.However, they still struggle to deliver promising performance on unseen domains, struggling to achieve effective domain transferability.We take the first step to investigate this issue and existing methods tend to produce one-size-fits-all strategies across diverse domains, limiting their search effectiveness.In response, we introduce a novel method, called STYLE,to achieve effective domain transferability.Our experimental results indicate that STYLE bears strong domain transferability, resulting in an average search performance improvement of 10% on four unseen domains.</abstract>
      <url hash="e9cc8814">2024.findings-acl.632</url>
      <bibkey>chen-etal-2024-style</bibkey>
      <doi>10.18653/v1/2024.findings-acl.632</doi>
    </paper>
    <paper id="633">
      <title>Evaluating Robustness of Generative Search Engine on Adversarial Factoid Questions</title>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Xiaochuan</first><last>Li</last></author>
      <author><first>Junzhe</first><last>Chen</last></author>
      <author><first>Yinghui</first><last>Li</last></author>
      <author><first>Yangning</first><last>Li</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Qun</first><last>Liu</last><affiliation>Huawei Noah’s Ark Lab</affiliation></author>
      <author><first>Lijie</first><last>Wen</last><affiliation>School of Software, Tsinghua University</affiliation></author>
      <author><first>Philip</first><last>Yu</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Zhijiang</first><last>Guo</last><affiliation>University of Cambridge</affiliation></author>
      <pages>10650-10671</pages>
      <abstract>Generative search engines have the potential to transform how people seek information online, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable part of a claim. To this end, we propose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval. These findings highlight the potential security risks of these systems and emphasize the need for rigorous evaluation before deployment. The dataset and code will be publicly available.</abstract>
      <url hash="b38154ca">2024.findings-acl.633</url>
      <bibkey>hu-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.633</doi>
    </paper>
    <paper id="634">
      <title>Automatic Engineering of Long Prompts</title>
      <author><first>Cho-Jui</first><last>Hsieh</last><affiliation>Google and University of California, Los Angeles</affiliation></author>
      <author><first>Si</first><last>Si</last><affiliation>Google</affiliation></author>
      <author><first>Felix</first><last>Yu</last><affiliation>Google</affiliation></author>
      <author><first>Inderjit</first><last>Dhillon</last><affiliation>University of Texas, Austin and Google</affiliation></author>
      <pages>10672-10685</pages>
      <abstract>Large language models (LLMs) have demonstrated remarkable capabilities in solving complex open-domain tasks, guided by comprehensive instructions and demonstrations provided in the form of prompts. However, these prompts can be lengthy, often comprising hundreds of lines and thousands of tokens, and their design often requires considerable human effort. Recent research has explored automatic prompt engineering for short prompts, typically consisting of one or a few sentences. However, the automatic design of long prompts remains a challenging problem due to its immense search space. In this paper, we propose an algorithm named Automated Prompt Engineering Xpert (APEX), a novel algorithm that automatically improves long prompts. Leveraging a greedy algorithm with beam-search for efficiency, APEX utilizes search history to significantly enhance the effectiveness of LLM-based mutation in its search process. Our results show that APEX achieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard and a consistent improvements on GSM8K with various models, highlighting the significance of automating prompt designs to fully harness the capabilities of LLMs.</abstract>
      <url hash="0e9be12c">2024.findings-acl.634</url>
      <bibkey>hsieh-etal-2024-automatic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.634</doi>
    </paper>
    <paper id="635">
      <title><fixed-case>AS</fixed-case>-<fixed-case>ES</fixed-case> Learning: Towards efficient <fixed-case>C</fixed-case>o<fixed-case>T</fixed-case> learning in small models</title>
      <author><first>Nuwa</first><last>Xi</last></author>
      <author><first>Yuhan</first><last>Chen</last></author>
      <author><first>Sendong</first><last>Zhao</last></author>
      <author><first>Haochun</first><last>Wang</last></author>
      <author><first>GongZhang</first><last>GongZhang</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ting</first><last>Liu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>10686-10697</pages>
      <abstract>Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs, especially when it comes to logical reasoning. Attempts have been made to induce such ability in small models as well by distilling from the data with CoT generated by Large Language Models (LLMs). However, existing methods often simply generate and incorporate more data from LLMs and fail to note the importance of efficiently utilizing existing CoT data. We here propose a new training paradigm AS-ES (Abstractive Segments - Extractive Segments) learning, which exploits the inherent information in CoT for iterative generation. Experiments show that our methods surpass the direct seq2seq training on CoT-extensive tasks like MWP and PET summarization, without data augmentation or altering the model itself. Furthermore, we explore the reason behind the inefficiency of small models in learning CoT and provide an explanation of why AS-ES learning works, giving insights into the underlying mechanism of CoT.</abstract>
      <url hash="3b5fad0c">2024.findings-acl.635</url>
      <bibkey>xi-etal-2024-es</bibkey>
      <doi>10.18653/v1/2024.findings-acl.635</doi>
    </paper>
    <paper id="636">
      <title><fixed-case>II</fixed-case>-<fixed-case>MMR</fixed-case>: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering</title>
      <author><first>Jihyung</first><last>Kil</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Farideh</first><last>Tavazoee</last></author>
      <author><first>Dongyeop</first><last>Kang</last><affiliation>University of Minnesota</affiliation></author>
      <author><first>Joo-Kyung</first><last>Kim</last><affiliation>Amazon AGI</affiliation></author>
      <pages>10698-10709</pages>
      <abstract>Visual Question Answering (VQA) often involves diverse reasoning scenarios across Vision and Language (V&amp;L). Most prior VQA studies, however, have merely focused on assessing the model’s overall accuracy without evaluating it on different reasoning cases. Furthermore, some recent works observe that conventional Chain-of-Thought (CoT) prompting fails to generate effective reasoning for VQA, especially for complex scenarios requiring multi-hop reasoning. In this paper, we propose II-MMR, a novel idea to identify and improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA question with an image and finds a reasoning path to reach its answer using two novel language promptings: (i) answer prediction-guided CoT prompt, or (ii) knowledge triplet-guided prompt. II-MMR then analyzes this path to identify different reasoning cases in current VQA benchmarks by estimating how many hops and what types (i.e., visual or beyond-visual) of reasoning are required to answer the question. On popular benchmarks including GQA and A-OKVQA, II-MMR observes that most of their VQA questions are easy to answer, simply demanding “single-hop” reasoning, whereas only a few questions require “multi-hop” reasoning. Moreover, while the recent V&amp;L model struggles with such complex multi-hop reasoning questions even using the traditional CoT method, II-MMR shows its effectiveness across all reasoning cases in both zero-shot and fine-tuning settings.</abstract>
      <url hash="5145c551">2024.findings-acl.636</url>
      <bibkey>kil-etal-2024-ii</bibkey>
      <doi>10.18653/v1/2024.findings-acl.636</doi>
    </paper>
    <paper id="637">
      <title><fixed-case>TAME</fixed-case>-<fixed-case>RD</fixed-case>: Text Assisted Replication of Image Multi-Adjustments for Reverse Designing</title>
      <author><first>Pooja</first><last>Guhan</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Uttaran</first><last>Bhattacharya</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Somdeb</first><last>Sarkhel</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Vahid</first><last>Azizi</last></author>
      <author><first>Xiang</first><last>Chen</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Saayan</first><last>Mitra</last><affiliation>Adobe Research</affiliation></author>
      <author><first>Aniket</first><last>Bera</last><affiliation>Purdue University and University of Maryland, College Park</affiliation></author>
      <author><first>Dinesh</first><last>Manocha</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>10710-10727</pages>
      <abstract>Given a source and its edited version performed based on human instructions in natural language, how do we extract the underlying edit operations, to automatically replicate similar edits on other images? This is the problem of reverse designing, and we present TAME-RD, a model to solve this problem. TAME-RD automatically learns from the complex interplay of image editing operations and the natural language instructions to learn fully specified edit operations. It predicts both the underlying image edit operations as discrete categories and their corresponding parameter values in the continuous space.We accomplish this by mapping together the contextual information from the natural language text and the structural differences between the corresponding source and edited images using the concept of pre-post effect. We demonstrate the efficiency of our network through quantitative evaluations on multiple datasets. We observe improvements of 6-10% on various accuracy metrics and 1.01X-4X on the RMSE score and the concordance correlation coefficient for the corresponding parameter values on the benchmark GIER dataset. We also introduce I-MAD, a new two-part dataset: I-MAD-Dense, a collection of approximately 100K source and edited images, together with automatically generated text instructions and annotated edit operations, and I-MAD-Pro, consisting of about 1.6K source and edited images, together with text instructions and annotated edit operations provided by professional editors. On our dataset, we observe absolute improvements of 1-10% on the accuracy metrics and 1.14X–5X on the RMSE score.</abstract>
      <url hash="f7b61ff4">2024.findings-acl.637</url>
      <bibkey>guhan-etal-2024-tame</bibkey>
      <doi>10.18653/v1/2024.findings-acl.637</doi>
    </paper>
    <paper id="638">
      <title>Batch-<fixed-case>ICL</fixed-case>: Effective, Efficient, and Order-Agnostic In-Context Learning</title>
      <author><first>Kaiyi</first><last>Zhang</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Ang</first><last>Lv</last></author>
      <author><first>Yuhan</first><last>Chen</last><affiliation>Xiaomi Corporation</affiliation></author>
      <author><first>Hansen</first><last>Ha</last></author>
      <author><first>Tao</first><last>Xu</last></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>10728-10739</pages>
      <abstract>In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs <tex-math>N</tex-math> separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to the forward computation of a zero-shot query to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of ICL examples. In some cases, it even exceeds the performance of the best order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple “epochs” of meta-optimization. This variant implicitly explores permutations of ICL examples, further enhancing ICL performance.</abstract>
      <url hash="1a4db92d">2024.findings-acl.638</url>
      <bibkey>zhang-etal-2024-batch</bibkey>
      <doi>10.18653/v1/2024.findings-acl.638</doi>
    </paper>
    <paper id="639">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>V</fixed-case>oices: Towards building an Inclusive Multilingual Speech Dataset for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Tahir</first><last>Javed</last></author>
      <author><first>Janki</first><last>Nawale</last></author>
      <author><first>Eldho</first><last>George</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Sakshi</first><last>Joshi</last></author>
      <author><first>Kaushal</first><last>Bhogale</last></author>
      <author><first>Deovrat</first><last>Mehendale</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Ishvinder</first><last>Sethi</last></author>
      <author><first>Aparna</first><last>Ananthanarayanan</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Hafsah</first><last>Faquih</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Pratiti</first><last>Palit</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Sneha</first><last>Ravishankar</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Saranya</first><last>Sukumaran</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Tripura</first><last>Panchagnula</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Sunjay</first><last>Murali</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Kunal</first><last>Gandhi</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Ambujavalli</first><last>R</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Manickam</first><last>M</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>C</first><last>Vaijayanthi</last><affiliation>Department of Computer Science, Indian Institute of Technology, Madras, Indian Institute of Technology, Madras</affiliation></author>
      <author><first>Krishnan</first><last>Karunganni</last></author>
      <author><first>Pratyush</first><last>Kumar</last><affiliation>Indian Institute of Technology Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Mitesh</first><last>Khapra</last><affiliation>Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>10740-10782</pages>
      <abstract>We present INDICVOICES, a dataset of natural and spontaneous speech containing a total of 7348 hours of read (9%), extempore (74%) and conversational (17%) audio from 16237 speakers covering 145 Indian districts and 22 languages. Of these 7348 hours, 1639 hours have already been transcribed, with a median of 73 hours per language. Through this paper, we share our journey of capturing the cultural, linguistic and demographic diversity of India to create a one-of-its-kind inclusive and representative dataset. More specifically, we share an open-source blueprint for data collection at scale comprising of standardised protocols, centralised tools, a repository of engaging questions, prompts and conversation scenarios spanning multiple domains and topics of interest, quality control mechanisms, comprehensive transcription guidelines and transcription tools. We hope that this open source blueprint will serve as a comprehensive starter kit for data collection efforts in other multilingual regions of the world. Using INDICVOICES, we build IndicASR, the first ASR model to support all the 22 languages listed in the 8th schedule of the Constitution of India.</abstract>
      <url hash="175c3de3">2024.findings-acl.639</url>
      <bibkey>javed-etal-2024-indicvoices</bibkey>
      <doi>10.18653/v1/2024.findings-acl.639</doi>
    </paper>
    <paper id="640">
      <title><fixed-case>V</fixed-case>i<fixed-case>C</fixed-case>or: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models</title>
      <author><first>Kaiwen</first><last>Zhou</last></author>
      <author><first>Kwonjoon</first><last>Lee</last><affiliation>Honda Research Institute USA</affiliation></author>
      <author><first>Teruhisa</first><last>Misu</last><affiliation>Honda Research Institute USA, Inc.</affiliation></author>
      <author><first>Xin</first><last>Wang</last><affiliation>University of California, Santa Cruz</affiliation></author>
      <pages>10783-10795</pages>
      <abstract>In our work, we explore the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) on visual commonsense reasoning (VCR) problems. We find that VLMs and LLMs-based decision pipelines are good at different kinds of VCR problems. Pre-trained VLMs exhibit strong performance for problems involving understanding the literal visual content, which we noted as visual commonsense understanding (VCU). For problems where the goal is to infer conclusions beyond image content, which we noted as visual commonsense inference (VCI), VLMs face difficulties, while LLMs, given sufficient visual evidence, can use commonsense to infer the answer well. We empirically validate this by letting LLMs classify VCR problems into these two categories and show the significant difference between VLM and LLM with image caption decision pipelines on two subproblems. Moreover, we identify a challenge with VLMs’ passive perception, which may miss crucial context information, leading to incorrect reasoning by LLMs. Based on these, we suggest a collaborative approach, named ViCor, where pre-trained LLMs serve as problem classifiers to analyze the problem category, then either use VLMs to answer the question directly or actively instruct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences. We evaluate our framework on two VCR benchmark datasets and outperform all other methods without in-domain fine-tuning.</abstract>
      <url hash="8e00b9fe">2024.findings-acl.640</url>
      <bibkey>zhou-etal-2024-vicor</bibkey>
      <doi>10.18653/v1/2024.findings-acl.640</doi>
    </paper>
    <paper id="641">
      <title>Decomposition for Enhancing Attention: Improving <fixed-case>LLM</fixed-case>-based Text-to-<fixed-case>SQL</fixed-case> through Workflow Paradigm</title>
      <author><first>Yuanzhen</first><last>Xie</last><affiliation>Tencent</affiliation></author>
      <author><first>Xinzhou</first><last>Jin</last></author>
      <author><first>Tao</first><last>Xie</last></author>
      <author><first>Matrixmxlin</first><last>Matrixmxlin</last></author>
      <author><first>Liang</first><last>Chen</last></author>
      <author><first>Chenyun</first><last>Yu</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Cheng</first><last>Lei</last></author>
      <author><first>Chengxiang</first><last>Zhuo</last></author>
      <author><first>Bo</first><last>Hu</last></author>
      <author><first>Zang</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <pages>10796-10816</pages>
      <abstract>In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model’s attention. Additionally, the inclusion of self-correction and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three datasets demonstrate that our approach outperforms other methods by a significant margin. About 2-3 percentage point improvements compared to the existing baseline on the Spider Dev, Spider-Realistic, and Bird Dev datasets and new SOTA results on the Spider Test dataset are achieved. Our code is available on GitHub: <url>https://github.com/FlyingFeather/DEA-SQL</url>.</abstract>
      <url hash="218deaa7">2024.findings-acl.641</url>
      <bibkey>xie-etal-2024-decomposition</bibkey>
      <doi>10.18653/v1/2024.findings-acl.641</doi>
    </paper>
    <paper id="642">
      <title>Unveiling Opinion Evolution via Prompting and Diffusion for Short Video Fake News Detection</title>
      <author><first>Linlin</first><last>Zong</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Jiahui</first><last>Zhou</last></author>
      <author><first>Wenmin</first><last>Lin</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Xinyue</first><last>Liu</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Xianchao</first><last>Zhang</last><affiliation>Dalian University of Technology</affiliation></author>
      <author><first>Bo</first><last>Xu</last><affiliation>Dalian University of Technology</affiliation></author>
      <pages>10817-10826</pages>
      <abstract>Short video fake news detection is crucial for combating the spread of misinformation. Current detection methods tend to aggregate features from individual modalities into multimodal features, overlooking the implicit opinions and the evolving nature of opinions across modalities. In this paper, we mine implicit opinions within short video news and promote the evolution of both explicit and implicit opinions across all modalities. Specifically, we design a prompt template to mine implicit opinions regarding the credibility of news from the textual component of videos. Additionally, we employ a diffusion model that encourages the interplay among diverse modal opinions, including those extracted through our implicit opinion prompts. Experimental results on a publicly available dataset for short video fake news detection demonstrate the superiority of our model over state-of-the-art methods.</abstract>
      <url hash="45e04474">2024.findings-acl.642</url>
      <bibkey>zong-etal-2024-unveiling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.642</doi>
    </paper>
    <paper id="643">
      <title>i<fixed-case>S</fixed-case>ign: A Benchmark for <fixed-case>I</fixed-case>ndian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Processing</title>
      <author><first>Abhinav</first><last>Joshi</last><affiliation>Indian Institute of Technology, Kanpur</affiliation></author>
      <author><first>Romit</first><last>Mohanty</last></author>
      <author><first>Mounika</first><last>Kanakanti</last></author>
      <author><first>Andesha</first><last>Mangla</last><affiliation>Indian Sign Language Research and Training Centre</affiliation></author>
      <author><first>Sudeep</first><last>Choudhary</last></author>
      <author><first>Monali</first><last>Barbate</last></author>
      <author><first>Ashutosh</first><last>Modi</last><affiliation>IIT Kanpur</affiliation></author>
      <pages>10827-10844</pages>
      <abstract>Indian Sign Language has limited resources for developing machine learning and data-driven approaches for automated language processing. Though text/audio-based language processing techniques have shown colossal research interest and tremendous improvements in the last few years, Sign Languages still need to catch up due to the need for more resources. To bridge this gap, in this work, we propose <b>iSign</b>: a benchmark for Indian Sign Language (ISL) Processing. We make three primary contributions to this work. First, we release one of the largest ISL-English datasets with more than video-sentence/phrase pairs. To the best of our knowledge, it is the largest sign language dataset available for ISL. Second, we propose multiple NLP-specific tasks (including SignVideo2Text, SignPose2Text, Text2Pose, Word Prediction, and Sign Semantics) and benchmark them with the baseline models for easier access to the research community. Third, we provide detailed insights into the proposed benchmarks with a few linguistic insights into the working of ISL. We streamline the evaluation of Sign Language processing, addressing the gaps in the NLP research community for Sign Languages. We release the dataset, tasks and models via the following website: https://exploration-lab.github.io/iSign/</abstract>
      <url hash="09aea051">2024.findings-acl.643</url>
      <bibkey>joshi-etal-2024-isign</bibkey>
      <doi>10.18653/v1/2024.findings-acl.643</doi>
    </paper>
    <paper id="644">
      <title>Data Contamination Calibration for Black-box <fixed-case>LLM</fixed-case>s</title>
      <author><first>Wentao</first><last>Ye</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Jiaqi</first><last>Hu</last></author>
      <author><first>Liyao</first><last>Li</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Haobo</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Gang</first><last>Chen</last></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>10845-10861</pages>
      <abstract>The rapid advancements of Large Language Models (LLMs) tightly associate with the expansion of the training data size. However, the unchecked ultra-large-scale training sets introduce a series of potential risks like data contamination, i.e. the benchmark data is used for training. In this work, we propose a holistic method named Polarized Augment Calibration (PAC) along with a new to-be-released dataset to detect the contaminated data and diminish the contamination effect. PAC extends the popular MIA (Membership Inference Attack) — from machine learning community — by forming a more global target at detecting training data to Clarify invisible training data. As a pioneering work, PAC is very much plug-and-play that can be integrated with most (if not all) current white- and black-box LLMs. By extensive experiments, PAC outperforms existing methods by at least 4.5%, towards data contamination detection on more 4 dataset formats, with more than 10 base LLMs. Besides, our application in real-world scenarios highlights the prominent presence of contamination and related issues.</abstract>
      <url hash="64047078">2024.findings-acl.644</url>
      <bibkey>ye-etal-2024-data</bibkey>
      <doi>10.18653/v1/2024.findings-acl.644</doi>
    </paper>
    <paper id="645">
      <title>Truth-Aware Context Selection: Mitigating Hallucinations of Large Language Models Being Misled by Untruthful Contexts</title>
      <author><first>Tian</first><last>Yu</last></author>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <pages>10862-10884</pages>
      <abstract>Although Large Language Models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by untruthful contexts provided by users or knowledge augmentation tools, leading to hallucinations. To alleviate LLMs from being misled by untruthful context and take advantage of knowledge augmentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to adaptively recognize and mask untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs’ ability to accept truthful information and resist untruthful information.Experimental results indicate that TACS can effectively filter untruthful context and significantly improve the overall quality of LLMs’ responses when presented with misleading information.</abstract>
      <url hash="d654903c">2024.findings-acl.645</url>
      <bibkey>yu-etal-2024-truth</bibkey>
      <doi>10.18653/v1/2024.findings-acl.645</doi>
    </paper>
    <paper id="646">
      <title>Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning</title>
      <author><first>Menglong</first><last>Cui</last></author>
      <author><first>Jiangcun</first><last>Du</last></author>
      <author><first>Shaolin</first><last>Zhu</last><affiliation>Tianjin University</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>10885-10897</pages>
      <abstract>Large language models (LLMs) exhibit outstanding performance in machine translation via in-context learning. In contrast to sentence-level translation, document-level translation (DOCMT) by LLMs based on in-context learning faces two major challenges: firstly, document translations generated by LLMs are often incoherent; secondly, the length of demonstration for in-context learning is usually limited. To address these issues, we propose a Context-Aware Prompting method (CAP), which enables LLMs to generate more accurate, cohesive, and coherent translations via in-context learning. CAP takes into account multi-level attention, selects the most relevant sentences to the current one as context, and then generates a summary from these collected sentences. Subsequently, sentences most similar to the summary are retrieved from the datastore as demonstrations, which effectively guide LLMs in generating cohesive and coherent translations. We conduct extensive experiments across various DOCMT tasks, and the results demonstrate the effectiveness of our approach, particularly in zero pronoun translation (ZPT) and literary translation tasks.</abstract>
      <url hash="2381aec7">2024.findings-acl.646</url>
      <bibkey>cui-etal-2024-efficiently</bibkey>
      <doi>10.18653/v1/2024.findings-acl.646</doi>
    </paper>
    <paper id="647">
      <title>Improving Grammatical Error Correction via Contextual Data Augmentation</title>
      <author><first>Yixuan</first><last>Wang</last></author>
      <author><first>Baoxin</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yijun</first><last>Liu</last></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Dayong</first><last>Wu</last></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>10898-10910</pages>
      <abstract>Nowadays, data augmentation through synthetic data has been widely used in the field of Grammatical Error Correction (GEC) to alleviate the problem of data scarcity. However, these synthetic data are mainly used in the pre-training phase rather than the data-limited fine tuning phase due to inconsistent error distribution and noisy labels. In this paper, we propose a synthetic data construction method based on contextual augmentation, which can ensure an efficient augmentation of the original data with a more consistent error distribution. Specifically, we combine rule-based substitution with model-based generation, using the generation model to generate a richer context for the extracted error patterns. Besides, we also propose a relabeling-based data cleaning method to mitigate the effects of noisy labels in synthetic data. Experiments on CoNLL14 and BEA19-Test show that our proposed augmentation method consistently and substantially outperforms strong baselines and achieves the state-of-the-art level with only a few synthetic data.</abstract>
      <url hash="2e27e74a">2024.findings-acl.647</url>
      <bibkey>wang-etal-2024-improving-grammatical</bibkey>
      <doi>10.18653/v1/2024.findings-acl.647</doi>
    </paper>
    <paper id="648">
      <title><fixed-case>RECOST</fixed-case>: External Knowledge Guided Data-efficient Instruction Tuning</title>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Yiming</first><last>Zhang</last></author>
      <author><first>Haobo</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <pages>10911-10921</pages>
      <abstract>In the current landscape of large language models (LLMs), the process of instruction tuning serves as an essential step. Considering the high computing power overhead, data-efficient instruction tuning was proposed to reduce the training data size in this process, aiming at selecting high-quality instructional data. Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples. To address these challenges, we utilized external knowledge (relevant examples or paragraphs) to evaluate those samples synthesized by LLMs with an in-context-based relative predictive entropy. Based on the new metric, we proposed a framework, dubbed as <b>RECOST</b>, which integrates external-knowledge-base re-ranking and diversity-consistent sampling into a single pipeline. Through extensive experiments on several synthetic datasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our method and achieve even better results with only <b>1%</b> of the full dataset.</abstract>
      <url hash="20db411d">2024.findings-acl.648</url>
      <bibkey>zhang-etal-2024-recost</bibkey>
      <doi>10.18653/v1/2024.findings-acl.648</doi>
    </paper>
    <paper id="649">
      <title>Understanding Cross-Lingual <fixed-case>A</fixed-case>lignment—<fixed-case>A</fixed-case> Survey</title>
      <author><first>Katharina</first><last>Hämmerl</last><affiliation>CIS, LMU Munich</affiliation></author>
      <author><first>Jindřich</first><last>Libovický</last><affiliation>Charles University Prague</affiliation></author>
      <author><first>Alexander</first><last>Fraser</last><affiliation>Technical University of Munich</affiliation></author>
      <pages>10922-10943</pages>
      <abstract>Cross-lingual alignment, the meaningful similarity of representations across languages in multilingual language models, has been an active field of research in recent years. We survey the literature of techniques to improve cross-lingual alignment, providing a taxonomy of methods and summarising insights from throughout the field. We present different understandings of cross-lingual alignment and their limitations. We provide a qualitative summary of results from a number of surveyed papers. Finally, we discuss how these insights may be applied not only to encoder models, where this topic has been heavily studied, but also to encoder-decoder or even decoder-only models, and argue that an effective trade-off between language-neutral and language-specific information is key.</abstract>
      <url hash="441a7654">2024.findings-acl.649</url>
      <bibkey>hammerl-etal-2024-understanding</bibkey>
      <doi>10.18653/v1/2024.findings-acl.649</doi>
    </paper>
    <paper id="650">
      <title>Mitigate Negative Transfer with Similarity Heuristic Lifelong Prompt Tuning</title>
      <author><first>Chenyuan</first><last>Wu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Gangwei</first><last>Jiang</last><affiliation>City University of Hong Kong and University of Science and Technology of China</affiliation></author>
      <author><first>Defu</first><last>Lian</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>10944-10959</pages>
      <abstract>Lifelong prompt tuning has significantly advanced parameter-efficient lifelong learning with its efficiency and minimal storage demands on various tasks.Our empirical studies, however, highlights certain transferability constraints in the current methodologies: a universal algorithm that guarantees consistent positive transfer across all tasks is currently unattainable, especially when dealing dissimilar tasks that may engender negative transfer.Identifying the misalignment between algorithm selection and task specificity as the primary cause of negative transfer, we present the Similarity Heuristic Lifelong Prompt Tuning (SHLPT) framework. This innovative strategy partitions tasks into two distinct subsets by harnessing a learnable similarity metric, thereby facilitating fruitful transfer from tasks regardless of their similarity or dissimilarity. Additionally, SHLPT incorporates a parameter pool to combat catastrophic forgetting effectively. Our experiments shows that SHLPT outperforms state-of-the-art techniques in lifelong learning benchmarks and demonstrates robustness against negative transfer in diverse task sequences.</abstract>
      <url hash="c17b2cf9">2024.findings-acl.650</url>
      <bibkey>wu-etal-2024-mitigate</bibkey>
      <doi>10.18653/v1/2024.findings-acl.650</doi>
    </paper>
    <paper id="651">
      <title><fixed-case>PANDA</fixed-case>: Preference Adaptation for Enhancing Domain-Specific Abilities of <fixed-case>LLM</fixed-case>s</title>
      <author><first>An</first><last>Liu</last></author>
      <author><first>Zonghan</first><last>Yang</last><affiliation>Department of Computer Science and Technology, Tsinghua University</affiliation></author>
      <author><first>Zhenhe</first><last>Zhang</last></author>
      <author><first>Qingyuan</first><last>Hu</last></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Fei</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>10960-10977</pages>
      <abstract>While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld. This finding highlights the potential of exploring tuning-free approaches to achieve weak-to-strong generalization.</abstract>
      <url hash="0ba9099b">2024.findings-acl.651</url>
      <bibkey>liu-etal-2024-panda</bibkey>
      <doi>10.18653/v1/2024.findings-acl.651</doi>
    </paper>
    <paper id="652">
      <title>Developing <fixed-case>PUGG</fixed-case> for <fixed-case>P</fixed-case>olish: A Modern Approach to <fixed-case>KBQA</fixed-case>, <fixed-case>MRC</fixed-case>, and <fixed-case>IR</fixed-case> Dataset Construction</title>
      <author><first>Albert</first><last>Sawczyn</last><affiliation>Wroclaw University of Science and Technology</affiliation></author>
      <author><first>Katsiaryna</first><last>Viarenich</last></author>
      <author><first>Konrad</first><last>Wojtasik</last></author>
      <author><first>Aleksandra</first><last>Domogała</last><affiliation>Technical University of Wroclaw</affiliation></author>
      <author><first>Marcin</first><last>Oleksy</last></author>
      <author><first>Maciej</first><last>Piasecki</last><affiliation>Wroclaw University of Science and Technology</affiliation></author>
      <author><first>Tomasz</first><last>Kajdanowicz</last><affiliation>Wroclaw University of Science and Technology</affiliation></author>
      <pages>10978-10996</pages>
      <abstract>Advancements in AI and natural language processing have revolutionized machine-human language interactions, with question answering (QA) systems playing a pivotal role. The knowledge base question answering (KBQA) task, utilizing structured knowledge graphs (KG), allows for handling extensive knowledge-intensive questions. However, a significant gap exists in KBQA datasets, especially for low-resource languages. Many existing construction pipelines for these datasets are outdated and inefficient in human labor, and modern assisting tools like Large Language Models (LLM) are not utilized to reduce the workload. To address this, we have designed and implemented a modern, semi-automated approach for creating datasets, encompassing tasks such as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR), tailored explicitly for low-resource environments. We executed this pipeline and introduced the PUGG dataset, the first Polish KBQA dataset, and novel datasets for MRC and IR. Additionally, we provide a comprehensive implementation, insightful findings, detailed statistics, and evaluation of baseline models.</abstract>
      <url hash="308f3ce2">2024.findings-acl.652</url>
      <bibkey>sawczyn-etal-2024-developing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.652</doi>
    </paper>
    <paper id="653">
      <title>Knowledge-to-<fixed-case>SQL</fixed-case>: Enhancing <fixed-case>SQL</fixed-case> Generation with Data Expert <fixed-case>LLM</fixed-case></title>
      <author><first>Zijin</first><last>Hong</last></author>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Qinggang</first><last>Zhang</last></author>
      <author><first>Feiran</first><last>Huang</last></author>
      <author><first>Xiao</first><last>Huang</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <pages>10997-11008</pages>
      <abstract>Generating accurate SQL queries for user questions (text-to-SQL) has been a long-standing challenge since it requires a deep understanding of both the user’s question and the corresponding database schema in order to retrieve the desired content accurately. Existing methods rely on the comprehensive capability of large language models (LLMs) to generate the SQL. However, some necessary knowledge is not explicitly included in the database schema and user question or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient questions may be inaccurate, negatively influencing the text-to-SQL models’ performance and robustness. To address this challenge, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all text-to-SQL models. Specifically, we introduce the detailed implementation of DELLM regarding table reading and the basic fine-tuning process. We further propose a Preference Learning via Database Feedback (PLDBF) strategy, refining the DELLM to generate more helpful knowledge for LLMs. Extensive experiments verify that DELLM can enhance the state-of-the-art approaches for text-to-SQL tasks. The corresponding code of DELLM is released for further research.</abstract>
      <url hash="a2dc4c46">2024.findings-acl.653</url>
      <bibkey>hong-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.findings-acl.653</doi>
    </paper>
    <paper id="654">
      <title>Centroid-Based Efficient Minimum <fixed-case>B</fixed-case>ayes Risk Decoding</title>
      <author><first>Hiroyuki</first><last>Deguchi</last><affiliation>Nara Institute of Science and Technology, Japan and National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Division of Information Science, Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hideki</first><last>Tanaka</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Masao</first><last>Utiyama</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <pages>11009-11018</pages>
      <abstract>Minimum Bayes risk (MBR) decoding achieved state-of-the-art translation performance by using COMET, a neural metric that has a high correlation with human evaluation.However, MBR decoding requires quadratic time since it computes the expected score between a translation hypothesis and all reference translations.We propose centroid-based MBR (CBMBR) decoding to improve the speed of MBR decoding.Our method clusters the reference translations in the feature space, and then calculates the score using the centroids of each cluster.The experimental results show that our CBMBR not only improved the decoding speed of the expected score calculation 5.7 times, but also outperformed vanilla MBR decoding in translation quality by up to 0.5 COMET in the WMT’22 En<tex-math>\leftrightarrow</tex-math>Ja, En<tex-math>\leftrightarrow</tex-math>De, En<tex-math>\leftrightarrow</tex-math>Zh, and WMT’23 En<tex-math>\leftrightarrow</tex-math>Ja translation tasks.</abstract>
      <url hash="9595d379">2024.findings-acl.654</url>
      <bibkey>deguchi-etal-2024-centroid</bibkey>
      <doi>10.18653/v1/2024.findings-acl.654</doi>
    </paper>
    <paper id="655">
      <title>Enhancing Distractor Generation for Multiple-Choice Questions with Retrieval Augmented Pretraining and Knowledge Graph Integration</title>
      <author><first>Han Cheng</first><last>Yu</last></author>
      <author><first>Yu An</first><last>Shih</last></author>
      <author><first>Kin Man</first><last>Law</last><affiliation>National Chung Hsing University, Taichung</affiliation></author>
      <author><first>KaiYu</first><last>Hsieh</last></author>
      <author><first>Yu Chen</first><last>Cheng</last></author>
      <author><first>Hsin Chih</first><last>Ho</last></author>
      <author><first>Zih An</first><last>Lin</last></author>
      <author><first>Wen-Chuan</first><last>Hsu</last></author>
      <author><first>Yao-Chung</first><last>Fan</last><affiliation>National Chung Hsing University</affiliation></author>
      <pages>11019-11029</pages>
      <abstract>In this paper, we tackle the task of distractor generation (DG) for multiple-choice questions. Our study introduces two key designs. First, we propose the concept of retrieval augmented pretraining, which involves refining the language model pretraining to align it more closely with the downstream task of DG. Second, we explore the integration of knowledge graphs and language models to further enhance the performance of DG. Our study unveils promising directions for further development in DG by showcasing the efficacy of knowledge augmentation and task-specific pretraining. These findings demonstrate the potential for leveraging both strategies to enhance the quality and performance of DG systems.</abstract>
      <url hash="3fc8c6e9">2024.findings-acl.655</url>
      <bibkey>yu-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.655</doi>
    </paper>
    <paper id="656">
      <title>Exploiting Positional Bias for Query-Agnostic Generative Content in Search</title>
      <author><first>Andrew</first><last>Parry</last></author>
      <author><first>Sean</first><last>MacAvaney</last><affiliation>University of Glasgow</affiliation></author>
      <author><first>Debasis</first><last>Ganguly</last><affiliation>University of Glasgow</affiliation></author>
      <pages>11030-11047</pages>
      <abstract>In recent years, research shows that neural ranking models (NRMs) substantially outperform their lexical counterparts in text retrieval. In traditional search pipelines, a combination of features leads to well-defined behaviour. However, as neural approaches become increasingly prevalent as the final scoring component of engines or as standalone systems, their robustness to malicious text and, more generally, semantic perturbation needs to be better understood. We posit that the transformer attention mechanism can induce exploitable defects in search models through sensitivity to token position within a sequence, leading to an attack that could generalise beyond a single query or topic. We demonstrate such defects by showing that non-relevant text–such as promotional content–can be easily injected into a document without adversely affecting its position in search results. Unlike previous gradient-based attacks, we demonstrate the existence of these biases in a query-agnostic fashion. In doing so, without the knowledge of topicality, we can still reduce the negative effects of non-relevant content injection by controlling injection position. Our experiments are conducted with simulated on-topic promotional text automatically generated by prompting LLMs with topical context from target documents. We find that contextualisation of a non-relevant text further reduces negative effects whilst likely circumventing existing content filtering mechanisms. In contrast, lexical models are found to be more resilient to such content injection attacks. We then investigate a simple yet effective compensation for the weaknesses of the NRMs in search, validating our hypotheses regarding transformer bias.</abstract>
      <url hash="d4b95670">2024.findings-acl.656</url>
      <bibkey>parry-etal-2024-exploiting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.656</doi>
    </paper>
    <paper id="657">
      <title><fixed-case>ICC</fixed-case> : Quantifying Image Caption Concreteness for Multimodal Dataset Curation</title>
      <author><first>Moran</first><last>Yanuka</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Morris</first><last>Alper</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Hadar</first><last>Averbuch-Elor</last><affiliation>Tel Aviv University and Cornell University</affiliation></author>
      <author><first>Raja</first><last>Giryes</last><affiliation>Tel Aviv University</affiliation></author>
      <pages>11048-11064</pages>
      <abstract>Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, Image Caption Concreteness (ICC), that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our unsupervised approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and caption-level texts. Moreover, we show that curation using ICC complements existing approaches: It succeeds in selecting the highest quality samples from multimodal web-scale datasets to allow for efficient training in resource-constrained settings.</abstract>
      <url hash="b8140558">2024.findings-acl.657</url>
      <bibkey>yanuka-etal-2024-icc</bibkey>
      <doi>10.18653/v1/2024.findings-acl.657</doi>
    </paper>
    <paper id="658">
      <title>On <fixed-case>LLM</fixed-case>s-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey</title>
      <author><first>Lin</first><last>Long</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Ruixuan</first><last>Xiao</last></author>
      <author><first>Junbo</first><last>Zhao</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Gang</first><last>Chen</last></author>
      <author><first>Haobo</first><last>Wang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>11065-11082</pages>
      <abstract>Within the evolving landscape of deep learning, the dilemma of data quantity and quality has been a long-standing problem. The recent advent of Large Language Models (LLMs) offers a data-centric solution to alleviate the limitations of real-world data with synthetic data generation. However, current investigations into this field lack a unified framework and mostly stay on the surface. Therefore, this paper provides an organization of relevant studies based on a generic workflow of synthetic data generation. By doing so, we highlight the gaps within existing research and outline prospective avenues for future study. This work aims to shepherd the academic and industrial communities towards deeper, more methodical inquiries into the capabilities and applications of LLMs-driven synthetic data generation.</abstract>
      <url hash="638c317b">2024.findings-acl.658</url>
      <bibkey>long-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.findings-acl.658</doi>
    </paper>
    <paper id="659">
      <title>When is a Language Process a Language Model?</title>
      <author><first>Li</first><last>Du</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Holden</first><last>Lee</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Jason</first><last>Eisner</last><affiliation>Microsoft and Johns Hopkins University</affiliation></author>
      <author><first>Ryan</first><last>Cotterell</last><affiliation>Swiss Federal Institute of Technology</affiliation></author>
      <pages>11083-11094</pages>
      <abstract>A language model may be viewed as a <tex-math>\Sigma</tex-math>-valued stochastic process for some alphabet <tex-math>\Sigma</tex-math>.However, in some pathological situations, such a stochastic process may “leak” probability mass onto the set of infinite strings and hence is not equivalent to the conventional view of a language model as a distribution over ordinary (finite) strings.Such ill-behaved language processes are referred to as *non-tight* in the literature.In this work, we study conditions of tightness through the lens of stochastic processes.In particular, by regarding the symbol as marking a stopping time and using results from martingale theory, we give characterizations of tightness that generalize our previous work [(Du et al. 2023)](https://arxiv.org/abs/2212.10502).</abstract>
      <url hash="052f6c16">2024.findings-acl.659</url>
      <bibkey>du-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.659</doi>
    </paper>
    <paper id="660">
      <title>Accelerating Multilingual Language Model for Excessively Tokenized Languages</title>
      <author><first>Jimin</first><last>Hong</last><affiliation>Krafton.Inc and Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Gibbeum</first><last>Lee</last><affiliation>KRAFTON and KAIST</affiliation></author>
      <author><first>Jaewoong</first><last>Cho</last><affiliation>KRAFTON</affiliation></author>
      <pages>11095-11111</pages>
      <abstract>Recent advancements in large language models (LLMs) have remarkably enhanced performances on a variety of tasks in multiple languages. However, tokenizers in LLMs trained primarily on English-centric corpora often overly fragment a text into character or Unicode-level tokens in non-Roman alphabetic languages, leading to inefficient text generation.We introduce a simple yet effective framework to accelerate text generation in such languages. Our approach involves employing a new language model head with a vocabulary set tailored to a specific target language for a pre-trained LLM. This is followed by fine-tuning the new head while incorporating a verification step to ensure the model’s performance is preserved.We show that this targeted fine-tuning, while freezing other model parameters, effectively reduces token fragmentation for the target language. Our extensive experiments demonstrate that the proposed framework increases the generation speed by a factor of 1.7 while maintaining the performance of pre-trained multilingual models on target monolingual tasks.</abstract>
      <url hash="ca40ad65">2024.findings-acl.660</url>
      <bibkey>hong-etal-2024-accelerating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.660</doi>
    </paper>
    <paper id="661">
      <title>Definition Generation for Automatically Induced Semantic Frame</title>
      <author><first>Yi</first><last>Han</last></author>
      <author><first>Ryohei</first><last>Sasano</last><affiliation>Nagoya University</affiliation></author>
      <author><first>Koichi</first><last>Takeda</last><affiliation>Nagoya University</affiliation></author>
      <pages>11112-11118</pages>
      <abstract>In a semantic frame resource such as FrameNet, the definition sentence of a frame is essential for humans to understand the meaning of the frame intuitively. Recently, several attempts have been made to induce semantic frames from large corpora, but the cost of creating the definition sentences for such frames is significant. In this paper, we address a new task of generating frame definitions from a set of frame-evoking words. Specifically, given a cluster of frame-evoking words and associated exemplars induced as the same semantic frame, we utilize a large language model to generate frame definitions. We demonstrate that incorporating frame element reasoning as chain-of-thought can enhance the inclusion of correct frame elements in the generated definitions.</abstract>
      <url hash="93e6bb47">2024.findings-acl.661</url>
      <bibkey>han-etal-2024-definition</bibkey>
      <doi>10.18653/v1/2024.findings-acl.661</doi>
    </paper>
    <paper id="662">
      <title>Distillation Enhanced Generative Retrieval</title>
      <author id="yongqi-li-hk"><first>Yongqi</first><last>Li</last><affiliation>Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zhen</first><last>Zhang</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Liqiang</first><last>Nie</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Wenjie</first><last>Li</last><affiliation>The Hong Kong Polytechnic University, The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Tat-Seng</first><last>Chua</last><affiliation>National University of Singapore</affiliation></author>
      <pages>11119-11129</pages>
      <abstract>Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden to the inference stage. We conduct experiments on four public datasets, and the results indicate that DGR achieves state-of-the-art performance among the generative retrieval methods. Additionally, DGR demonstrates exceptional robustness and generalizability with various teacher models and distillation losses.</abstract>
      <url hash="b36030e9">2024.findings-acl.662</url>
      <bibkey>li-etal-2024-distillation</bibkey>
      <doi>10.18653/v1/2024.findings-acl.662</doi>
    </paper>
    <paper id="663">
      <title><fixed-case>T</fixed-case>ox<fixed-case>V</fixed-case>id<fixed-case>LM</fixed-case>: A Multimodal Framework for Toxicity Detection in Code-Mixed Videos</title>
      <author><first>Krishanu</first><last>Maity</last></author>
      <author><first>A.s.</first><last>Poornash</last></author>
      <author><first>Sriparna</first><last>Saha</last><affiliation>Indian Institute of Technology Patna, India</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>11130-11142</pages>
      <abstract>In an era of rapidly evolving internet technology, the surge in multimodal content, including videos, has expanded the horizons of online communication. However, the detection of toxic content in this diverse landscape, particularly in low-resource code-mixed languages, remains a critical challenge. While substantial research has addressed toxic content detection in textual data, the realm of video content, especially in non-English languages, has been relatively underexplored. This paper addresses this research gap by introducing a benchmark dataset, the first of its kind, consisting of 931 videos with 4021 code-mixed Hindi-English utterances collected from YouTube. Each utterance within this dataset has been meticulously annotated for toxicity, severity, and sentiment labels. We have developed an advanced Multimodal Multitask framework built for Toxicity detection in Video Content by leveraging Language Models (LMs), crafted for the primary objective along with the additional tasks of conducting sentiment and severity analysis. ToxVidLM incorporates three key modules – the Encoder module, Cross-Modal Synchronization module, and Multitask module – crafting a generic multimodal LM customized for intricate video classification tasks. Our experiments reveal that incorporating multiple modalities from the videos substantially enhances the performance of toxic content detection by achieving an Accuracy and Weighted F1 score of 94.29% and 94.35%, respectively.</abstract>
      <url hash="be7060bb">2024.findings-acl.663</url>
      <bibkey>maity-etal-2024-toxvidlm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.663</doi>
    </paper>
    <paper id="664">
      <title><fixed-case>S</fixed-case>table<fixed-case>T</fixed-case>ool<fixed-case>B</fixed-case>ench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models</title>
      <author id="zhicheng-guo-tsinghua"><first>Zhicheng</first><last>Guo</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Sijie</first><last>Cheng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Hao</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Shihao</first><last>Liang</last></author>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Peng</first><last>Li</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author id="yang-liu"><first>Yang</first><last>Liu</last></author>
      <pages>11143-11156</pages>
      <abstract>Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.</abstract>
      <url hash="2c123c6d">2024.findings-acl.664</url>
      <bibkey>guo-etal-2024-stabletoolbench</bibkey>
      <doi>10.18653/v1/2024.findings-acl.664</doi>
    </paper>
    <paper id="665">
      <title>Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence</title>
      <author><first>Weixiang</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zhuojun</first><last>Li</last></author>
      <author><first>Shilong</first><last>Wang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Wang</last></author>
      <author><first>Yulin</first><last>Hu</last></author>
      <author><first>Yanyan</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Chen</first><last>Wei</last><affiliation>xiaomi</affiliation></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>11157-11176</pages>
      <abstract>Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce EiBench, a large-scale collection of EI-related tasks in the text-to-text format with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel Modular Emotional Intelligence enhancement method (**MoEI**), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs without compromise their GI. Extensive experiments on two representative LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness of MoEI to improving EI while maintain GI.</abstract>
      <url hash="75b89568">2024.findings-acl.665</url>
      <bibkey>zhao-etal-2024-matter</bibkey>
      <doi>10.18653/v1/2024.findings-acl.665</doi>
    </paper>
    <paper id="666">
      <title><fixed-case>K</fixed-case>or<fixed-case>NAT</fixed-case>: <fixed-case>LLM</fixed-case> Alignment Benchmark for <fixed-case>K</fixed-case>orean Social Values and Common Knowledge</title>
      <author><first>Jiyoung</first><last>Lee</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Minwoo</first><last>Kim</last></author>
      <author><first>Seungho</first><last>Kim</last></author>
      <author><first>Junghwan</first><last>Kim</last><affiliation>selectstar</affiliation></author>
      <author><first>Seunghyun</first><last>Won</last><affiliation>Seoul National University Bundang Hospital</affiliation></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Edward</first><last>Choi</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>11177-11213</pages>
      <abstract>To reliably deploy Large Language Models (LLMs) in a specific country, they must possess an understanding of the nation’s culture and basic knowledge. To this end, we introduce National Alignment, which measures the alignment between an LLM and a targeted country from two aspects: social value alignment and common knowledge alignment. We constructed KorNAT, the first benchmark that measures national alignment between LLMs and South Korea. KorNat contains 4K and 6K multiple-choice questions for social value and common knowledge, respectively. To attain an appropriately aligned ground truth in the social value dataset, we conducted a large-scale public survey with 6,174 South Koreans. For common knowledge, we created the data based on the South Korea text books and GED exams. Our dataset creation process is meticulously designed based on statistical sampling theory, and we also introduce metrics to measure national alignment, including three variations of social value alignment. We tested seven LLMs and found that only few models passed our reference score, indicating there exists room for improvement. Our dataset has received government approval following an assessment by a government-affiliated organization dedicated to evaluating dataset quality.</abstract>
      <url hash="2070fe1e">2024.findings-acl.666</url>
      <bibkey>lee-etal-2024-kornat</bibkey>
      <doi>10.18653/v1/2024.findings-acl.666</doi>
    </paper>
    <paper id="667">
      <title>Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation and Model Development</title>
      <author><first>Pranab</first><last>Sahoo</last></author>
      <author><first>Ayush</first><last>Singh</last></author>
      <author><first>Sriparna</first><last>Saha</last><affiliation>Indian Institute of Technology Patna, India</affiliation></author>
      <author><first>Aman</first><last>Chadha</last><affiliation>Amazon</affiliation></author>
      <author><first>Samrat</first><last>Mondal</last></author>
      <pages>11214-11226</pages>
      <abstract>The mining of adverse drug events (ADEs) is pivotal in pharmacovigilance, enhancing patient safety by identifying potential risks associated with medications, facilitating early detection of adverse events, and guiding regulatory decision-making. Traditional ADE detection methods are reliable but slow, not easily adaptable to large-scale operations, and offer limited information. With the exponential increase in data sources like social media content, biomedical literature, and Electronic Medical Records (EMR), extracting relevant ADE-related information from these unstructured texts is imperative. Previous ADE mining studies have focused on text-based methodologies, overlooking visual cues, limiting contextual comprehension, and hindering accurate interpretation. To address this gap, we present a MultiModal Adverse Drug Event (MMADE) detection dataset, merging ADE-related textual information with visual aids. Additionally, we introduce a framework that leverages the capabilities of LLMs and VLMs for ADE detection by generating detailed descriptions of medical images depicting ADEs, aiding healthcare professionals in visually identifying adverse events. Using our MMADE dataset, we showcase the significance of integrating visual cues from images to enhance overall performance. This approach holds promise for patient safety, ADE awareness, and healthcare accessibility, paving the way for further exploration in personalized healthcare.</abstract>
      <url hash="9a1fa027">2024.findings-acl.667</url>
      <bibkey>sahoo-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.667</doi>
    </paper>
    <paper id="668">
      <title>Space Decomposition for Sentence Embedding</title>
      <author><first>Wuttikorn</first><last>Ponwitayarat</last><affiliation>Vidyasirimedhi Institute of Science and Technology</affiliation></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last></author>
      <author><first>Ekapol</first><last>Chuangsuwanich</last><affiliation>Chulalongkorn University</affiliation></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>11227-11239</pages>
      <abstract>Determining sentence pair similarity is crucial for various NLP tasks. A common technique to address this is typically evaluated on a continuous semantic textual similarity scale from 0 to 5. However, based on a linguistic observation in STS annotation guidelines, we found that the score in the range [4,5] indicates an upper-range sample, while the rest are lower-range samples. This necessitates a new approach to treating the upper-range and lower-range classes separately. In this paper, we introduce a novel embedding space decomposition method called MixSP utilizing a Mixture of Specialized Projectors, designed to distinguish and rank upper-range and lower-range samples accurately. The experimental results demonstrate that MixSP decreased the overlap representation between upper-range and lower-range classes significantly while outperforming competitors on STS and zero-shot benchmarks.</abstract>
      <url hash="866c468a">2024.findings-acl.668</url>
      <bibkey>ponwitayarat-etal-2024-space</bibkey>
      <doi>10.18653/v1/2024.findings-acl.668</doi>
    </paper>
    <paper id="669">
      <title>Don’t Augment, Rewrite? Assessing Abusive Language Detection with Synthetic Data</title>
      <author><first>Camilla</first><last>Casula</last><affiliation>University of Trento and Fondazione Bruno Kessler</affiliation></author>
      <author><first>Elisa</first><last>Leonardelli</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <pages>11240-11247</pages>
      <abstract>Research on abusive language detection and content moderation is crucial to combat online harm. However, current limitations set by regulatory bodies and social media platforms can make it difficult to share collected data. We address this challenge by exploring the possibility to replace existing datasets in English for abusive language detection with synthetic data obtained by rewriting original texts with an instruction-based generative model.We show that such data can be effectively used to train a classifier whose performance is in line, and sometimes better, than a classifier trained on original data. Training with synthetic data also seems to improve robustness in a cross-dataset setting. A manual inspection of the generated data confirms that rewriting makes it impossible to retrieve the original texts online.</abstract>
      <url hash="e5ef9644">2024.findings-acl.669</url>
      <bibkey>casula-etal-2024-dont</bibkey>
      <doi>10.18653/v1/2024.findings-acl.669</doi>
    </paper>
    <paper id="670">
      <title>Improving Low-Resource Machine Translation for Formosan Languages Using Bilingual Lexical Resources</title>
      <author><first>Francis</first><last>Zheng</last><affiliation>The University of Tokyo, The University of Tokyo</affiliation></author>
      <author><first>Edison</first><last>Marrese-Taylor</last><affiliation>The Univesity of Tokyo and AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Yutaka</first><last>Matsuo</last><affiliation>The University of Tokyo and The University of Tokyo</affiliation></author>
      <pages>11248-11259</pages>
      <abstract>This paper investigates how machine translation for low-resource languages can be improved by incorporating information from bilingual lexicons during the training process for mainly translation between Mandarin and Formosan languages, which are all moribund or critically endangered, and we also show that our techniques work for translation between Spanish and Nahuatl, a language pair consisting of languages from completely different language families. About 70% of the approximately 7,000 languages of the world have data in the form of lexicons, a valuable resource for improving low-resource language translation. We collect a dataset of parallel data and bilingual lexicons between Mandarin and 16 different Formosan languages and examine mainly three different approaches: (1) simply using lexical data as additional parallel data, (2) generating pseudo-parallel sentence data to use during training by replacing words in the original parallel sentence data using the lexicon, and (3) a combination of (1) and (2). All three approaches give us gains in both Bleu scores and chrF scores, and we found that (3) provided the most gains, followed by (1) and then (2), which we observed for both translation between Mandarin and the Formosan languages and Spanish-Nahuatl. With technique (3), we saw an average increase of 5.55 in Bleu scores and 10.33 in chrF scores.</abstract>
      <url hash="9fdad6e2">2024.findings-acl.670</url>
      <bibkey>zheng-etal-2024-improving-low</bibkey>
      <doi>10.18653/v1/2024.findings-acl.670</doi>
    </paper>
    <paper id="671">
      <title><fixed-case>CMMLU</fixed-case>: Measuring massive multitask language understanding in <fixed-case>C</fixed-case>hinese</title>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Yixuan</first><last>Zhang</last></author>
      <author><first>Fajri</first><last>Koto</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Yifei</first><last>Yang</last></author>
      <author><first>Hai</first><last>Zhao</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Timothy</first><last>Baldwin</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence and The University of Melbourne</affiliation></author>
      <pages>11260-11285</pages>
      <abstract>As the capabilities of large language models (LLMs) continue to advance, evaluating their performance is becoming more important and more challenging. This paper aims to address this issue for Mandarin Chinese in the form of CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural sciences, social sciences, engineering, and the humanities. We conduct a thorough evaluation of more than 20 contemporary multilingual and Chinese LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an accuracy of even 60%, which is the pass mark for Chinese exams. This highlights that there is substantial room for improvement in the capabilities of LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models’ performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models for Chinese.</abstract>
      <url hash="a1aff102">2024.findings-acl.671</url>
      <bibkey>li-etal-2024-cmmlu</bibkey>
      <doi>10.18653/v1/2024.findings-acl.671</doi>
    </paper>
    <paper id="672">
      <title>Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation</title>
      <author><first>Seongyun</first><last>Lee</last></author>
      <author><first>Seungone</first><last>Kim</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Sue Hyun</first><last>Park</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Geewook</first><last>Kim</last><affiliation>NAVER Cloud and KAIST</affiliation></author>
      <author><first>Minjoon</first><last>Seo</last><affiliation>Twelve Labs and Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>11286-11315</pages>
      <abstract>Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model.</abstract>
      <url hash="044beece">2024.findings-acl.672</url>
      <bibkey>lee-etal-2024-prometheus</bibkey>
      <doi>10.18653/v1/2024.findings-acl.672</doi>
    </paper>
    <paper id="673">
      <title>Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction</title>
      <author><first>Xiaoyuan</first><last>Li</last></author>
      <author><first>Wenjie</first><last>Wang</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Moxin</first><last>Li</last></author>
      <author><first>Junrong</first><last>Guo</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yang</first><last>Zhang</last></author>
      <author><first>Fuli</first><last>Feng</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>11316-11360</pages>
      <abstract>The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem-solving from the examinee perspective, overlooking a dual perspective of examiner regarding error identification and correction.From the examiner perspective, we define four evaluation tasks for error identification and correction along with a new dataset with annotated error types and steps. We also design diverse prompts to thoroughly evaluate eleven representative LLMs. Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro.Notably, calculation error proves the most challenging error type. Moreover, prompting LLMs with the error types can improve the average correction accuracy by 47.9%. These results reveal potential directions for developing the mathematical reasoning abilities of LLMs.Our code and dataset is available on https://github.com/LittleCirc1e/EIC.</abstract>
      <url hash="b59cb58e">2024.findings-acl.673</url>
      <bibkey>li-etal-2024-evaluating-mathematical</bibkey>
      <doi>10.18653/v1/2024.findings-acl.673</doi>
    </paper>
    <paper id="674">
      <title>Less is <fixed-case>KEN</fixed-case>: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models</title>
      <author><first>Michele</first><last>Mastromattei</last><affiliation>Campus Bio-Medico University of Rome</affiliation></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last><affiliation>University of Rome Tor Vergata</affiliation></author>
      <pages>11361-11374</pages>
      <abstract>Neural network pruning has become increasingly crucial due to the complexity of these models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on demanding calculations, rendering them impractical for real-world applications.This paper introduces KEN: a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformers by selectively preserving the most significant parameters while restoring others to their pre-training state. This strategy preserves model performance while enabling storage of only the optimized subnetwork, leading to substantial memory savings.Extensive evaluations across seven different LLMs demonstrate that KEN achieves equal or better performance than their original unpruned versions, with a minimum parameter reduction of 25%. Furthermore, in-depth comparisons with established pruning and PEFT algorithms confirm KEN effectiveness. We further introduce KEN<tex-math>_{viz}</tex-math>, an explainable tool that visualizes the optimized model composition achieved by KEN from different points of view.</abstract>
      <url hash="4b2fce5f">2024.findings-acl.674</url>
      <bibkey>mastromattei-zanzotto-2024-less</bibkey>
      <doi>10.18653/v1/2024.findings-acl.674</doi>
    </paper>
    <paper id="675">
      <title>When Do <fixed-case>LLM</fixed-case>s Need Retrieval Augmentation? Mitigating <fixed-case>LLM</fixed-case>s’ Overconfidence Helps Retrieval Augmentation</title>
      <author><first>Shiyu</first><last>Ni</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Keping</first><last>Bi</last><affiliation>Chinese Academy of Sciences</affiliation></author>
      <author><first>Jiafeng</first><last>Guo</last><affiliation>Institute of Computing Technolgy, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xueqi</first><last>Cheng</last><affiliation>, Chinese Academy of Sciences</affiliation></author>
      <pages>11375-11388</pages>
      <abstract>Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs’ hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs’ ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs’ such ability and confirm their overconfidence. Then, we study how LLMs’ certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs’ perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls.</abstract>
      <url hash="3848c525">2024.findings-acl.675</url>
      <bibkey>ni-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.findings-acl.675</doi>
    </paper>
    <paper id="676">
      <title>Hybrid Alignment Training for Large Language Models</title>
      <author><first>Chenglong</first><last>Wang</last></author>
      <author><first>Hang</first><last>Zhou</last></author>
      <author><first>Kaiyan</first><last>Chang</last></author>
      <author><first>Bei</first><last>Li</last><affiliation>Meituan</affiliation></author>
      <author><first>Yongyu</first><last>Mu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tongran</first><last>Liu</last></author>
      <author><first>JingBo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>11389-11403</pages>
      <abstract>Alignment training is crucial for enabling large language models (LLMs) to cater to human intentions and preferences. It is typically performed based on two stages with different objectives: instruction-following alignment and human-preference alignment. However, aligning LLMs with these objectives in sequence suffers from an inherent problem: the objectives may conflict, and the LLMs cannot guarantee to simultaneously align with the instructions and human preferences well. To response to these, in this work, we propose a Hybrid Alignment Training (Hbat) approach, based on alternating alignment and modified elastic weight consolidation methods. The basic idea is to alternate between different objectives during alignment training, so that better collaboration can be achieved between the two alignment tasks. We experiment with Hbat on summarization and dialogue tasks. Experimental results show that the proposed Hbat can significantly outperform all baselines. Notably, Hbat yields consistent performance gains over the traditional two-stage alignment training when using both proximal policy optimization and direct preference optimization.</abstract>
      <url hash="665e869f">2024.findings-acl.676</url>
      <bibkey>wang-etal-2024-hybrid</bibkey>
      <doi>10.18653/v1/2024.findings-acl.676</doi>
    </paper>
    <paper id="677">
      <title>Graph-Structured Speculative Decoding</title>
      <author><first>Zhuocheng</first><last>Gong</last></author>
      <author><first>Jiahao</first><last>Liu</last><affiliation>Meituan</affiliation></author>
      <author><first>Ziyue</first><last>Wang</last></author>
      <author><first>Pengfei</first><last>Wu</last></author>
      <author><first>Jingang</first><last>Wang</last><affiliation>Meituan</affiliation></author>
      <author><first>Xunliang</first><last>Cai</last></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>11404-11415</pages>
      <abstract>Speculative decoding has emerged as a promising technique to accelerate the inference of Large Language Models (LLMs) by employing a small language model to draft a hypothesis sequence, which is then validated by the LLM. The effectiveness of this approach heavily relies on the balance between performance and efficiency of the draft model. In our research, we focus on enhancing the proportion of draft tokens that are accepted to the final output by generating multiple hypotheses instead of just one. This allows the LLM more options to choose from and select the longest sequence that meets its standards. Our analysis reveals that hypotheses produced by the draft model share many common token sequences, suggesting a potential for optimizing computation. Leveraging this observation, we introduce an innovative approach utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This structure enables us to efficiently predict and merge recurring token sequences, vastly reducing the computational demands of the draft model. We term this approach Graph-structured Speculative Decoding (GSD). We apply GSD across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and observe a remarkable speedup of 1.70<tex-math>\times</tex-math> to 1.94 <tex-math>\times</tex-math>, significantly surpassing standard speculative decoding.</abstract>
      <url hash="a764ad8e">2024.findings-acl.677</url>
      <bibkey>gong-etal-2024-graph</bibkey>
      <doi>10.18653/v1/2024.findings-acl.677</doi>
    </paper>
    <paper id="678">
      <title>Duwak: Dual Watermarks in Large Language Models</title>
      <author><first>Chaoyi</first><last>Zhu</last></author>
      <author><first>Jeroen</first><last>Galjaard</last></author>
      <author><first>Pin-Yu</first><last>Chen</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Lydia</first><last>Chen</last><affiliation>Delft University of Technology</affiliation></author>
      <pages>11416-11436</pages>
      <abstract>As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms. Existing watermark techniques are shown effective in embedding single human-imperceptible and machine-detectable patterns without significantly affecting generated text quality and semantics. However, the efficiency in detecting watermarks, i.e., the minimum number of tokens required to assert detection with significance and robustness against post-editing, is still debatable. In this paper, we propose, Duwak, to fundamentally enhance the efficiency and quality of watermarking by embedding dual secret patterns in both token probability distribution and sampling schemes. To mitigate expression degradation caused by biasing toward certain tokens, we design a contrastive search to watermark the sampling scheme, which minimizes the token repetition and enhances the diversity. We theoretically explain the interdependency of the two watermarks within Duwak. We evaluate Duwak extensively on Llama2 and Vicuna under various post-editing attacks, against four state-of-the-art watermarking techniques and combinations of them. Our results show that Duwak marked text achieves the highest watermarked text quality at the lowest required token count for detection, up to 70% tokens less than existing approaches, especially under post paraphrasing.</abstract>
      <url hash="7cb76f03">2024.findings-acl.678</url>
      <bibkey>zhu-etal-2024-duwak</bibkey>
      <doi>10.18653/v1/2024.findings-acl.678</doi>
    </paper>
    <paper id="679">
      <title><fixed-case>C</fixed-case>ode<fixed-case>A</fixed-case>ttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion</title>
      <author><first>Qibing</first><last>Ren</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Chang</first><last>Gao</last></author>
      <author><first>Jing</first><last>Shao</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Junchi</first><last>Yan</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Xin</first><last>Tan</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Lizhuang</first><last>Ma</last><affiliation>Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University</affiliation></author>
      <pages>11437-11452</pages>
      <abstract>The rapid advancement of Large Language Models (LLMs) has brought about remarkable generative capabilities but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a new and universal safety vulnerability of these models against code input: CodeAttack bypasses the safety guardrails of all models more than 80% of the time. We find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures. Furthermore, we give our hypotheses about the success of CodeAttack: the misaligned bias acquired by LLMs during code training, prioritizing code completion over avoiding the potential safety risk. Finally, we analyze potential mitigation measures. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs.</abstract>
      <url hash="ad020582">2024.findings-acl.679</url>
      <bibkey>ren-etal-2024-codeattack</bibkey>
      <doi>10.18653/v1/2024.findings-acl.679</doi>
    </paper>
    <paper id="680">
      <title>Mitigating Reversal Curse in Large Language Models via Semantic-aware Permutation Training</title>
      <author><first>Qingyan</first><last>Guo</last></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Junliang</first><last>Guo</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Jiang</first><last>Bian</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Graduate School at Shenzhen,Tsinghua University</affiliation></author>
      <pages>11453-11464</pages>
      <abstract>While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the “reversal curse”. It is a typical example that the model knows “A’s father is B”, but is unable to reason “B’s child is A”. This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models’ ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may disrupt complete phrases or entities, thereby posing challenges for the model to comprehend and learn from training data. To address this issue, we propose Semantic-aware Permutation Training (SPT), which addresses this issue by segmenting the training sentences into semantic units (i.e., entities or phrases) with an assistant language model and permuting these units before feeding into the model. Extensive experiments demonstrate that SPT effectively mitigates the reversal curse since the performance on reversed questions approximates that on the forward ones, and significantly advances the performance of existing works.</abstract>
      <url hash="45d6dc58">2024.findings-acl.680</url>
      <bibkey>guo-etal-2024-mitigating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.680</doi>
    </paper>
    <paper id="681">
      <title>wav2vec-<fixed-case>S</fixed-case>: Adapting Pre-trained Speech Models for Streaming</title>
      <author><first>Biao</first><last>Fu</last></author>
      <author><first>Kai</first><last>Fan</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Minpeng</first><last>Liao</last></author>
      <author><first>Yidong</first><last>Chen</last></author>
      <author><first>Xiaodong</first><last>Shi</last><affiliation>Xiamen University, Tsinghua University</affiliation></author>
      <author><first>Zhongqiang</first><last>Huang</last><affiliation>Alibaba Group</affiliation></author>
      <pages>11465-11480</pages>
      <abstract>Pre-trained speech models, such as wav2vec 2.0, have significantly advanced speech-related tasks, including speech recognition and translation. However, their applicability in streaming scenarios is limited because these models are trained on complete utterances, leading to a mismatch with incremental streaming inputs. This paper identifies three critical design aspects within the architecture of wav2vec 2.0 and proposes a novel model, wav2vec-S, which incorporates simple modifications to ensure consistent speech representations during both training and inference phases for streaming speech inputs. Furthermore, we demonstrate that wav2vec-S models can be efficiently adapted from pre-trained wav2vec 2.0 models through continued pre-training and effectively finetuned to meet various latency requirements in downstream applications. Experiments on speech recognition and translation tasks show that wav2vec-S outperforms strong baseline models and achieves a superior balance between quality and latency.</abstract>
      <url hash="f1eba940">2024.findings-acl.681</url>
      <bibkey>fu-etal-2024-wav2vec</bibkey>
      <doi>10.18653/v1/2024.findings-acl.681</doi>
    </paper>
    <paper id="682">
      <title>Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering</title>
      <author><first>Anirudh</first><last>Phukan</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Shwetha</first><last>Somasundaram</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Apoorv</first><last>Saxena</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Koustava</first><last>Goswami</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last><affiliation>Adobe Research</affiliation></author>
      <pages>11481-11495</pages>
      <abstract>With the enhancement in the field of generative artificial intelligence (AI), contextual question answering has become extremely relevant. Attributing model generations to the input source document is essential to ensure trustworthiness and reliability. We observe that when large language models (LLMs) are used for contextual question answering, the output answer often consists of text copied verbatim from the input prompt which is linked together with “glue text” generated by the LLM. Motivated by this, we propose that LLMs have an inherent awareness from where the text was copied, likely captured in the hidden states of the LLM. We introduce a novel method for attribution in contextual question answering, leveraging the hidden state representations of LLMs. Our approach bypasses the need for extensive model retraining and retrieval model overhead, offering granular attributions and preserving the quality of generated answers. Our experimental results demonstrate that our method performs on par or better than GPT-4 at identifying verbatim copied segments in LLM generations and in attributing these segments to their source. Importantly, our method shows robust performance across various LLM architectures, highlighting its broad applicability. Additionally, we present Verifiability-granular, an attribution dataset which has token level annotations for LLM generations in the contextual question answering setup.</abstract>
      <url hash="d7675892">2024.findings-acl.682</url>
      <bibkey>phukan-etal-2024-peering</bibkey>
      <doi>10.18653/v1/2024.findings-acl.682</doi>
    </paper>
    <paper id="683">
      <title><fixed-case>TRAP</fixed-case>: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification</title>
      <author><first>Martin</first><last>Gubri</last><affiliation>Parameter Lab</affiliation></author>
      <author><first>Dennis</first><last>Ulmer</last></author>
      <author><first>Hwaran</first><last>Lee</last><affiliation>NAVER AI Lab</affiliation></author>
      <author><first>Sangdoo</first><last>Yun</last><affiliation>NAVER</affiliation></author>
      <author><first>Seong Joon</first><last>Oh</last><affiliation>Parameter Lab and Eberhard-Karls-Universität Tübingen</affiliation></author>
      <pages>11496-11517</pages>
      <abstract>Large Language Model (LLM) services and models often come with legal rules on *who* can use them and *how* they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel fingerprinting problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function.</abstract>
      <url hash="f510f850">2024.findings-acl.683</url>
      <bibkey>gubri-etal-2024-trap</bibkey>
      <doi>10.18653/v1/2024.findings-acl.683</doi>
    </paper>
    <paper id="684">
      <title><fixed-case>CLASP</fixed-case>: Cross-modal Alignment Using Pre-trained Unimodal Models</title>
      <author><first>Jianing</first><last>Zhou</last></author>
      <author><first>Ziheng</first><last>Zeng</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Hongyu</first><last>Gong</last><affiliation>FAIR at Meta</affiliation></author>
      <author><first>Suma</first><last>Bhat</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <pages>11518-11531</pages>
      <abstract>Recent advancements in joint speech-text pre-training have significantly advanced the processing of natural language. However, a key limitation is their reliance on parallel speech-text data, posing challenges due to data accessibility. Addressing this, our paper introduces an innovative framework for jointly performing speech and text processing without parallel corpora during pre-training but only downstream. Utilizing pre-trained unimodal models, we extract distinct representations for speech and text, aligning them effectively in a newly defined space using a multi-level contrastive learning mechanism. A unique swap reconstruction mechanism enhances the alignment and is followed by fusion via a multi-head mechanism, seamlessly merging modality-invariant and modality-specific representations. Testing for emotion recognition (SLU task) and idiom usage detection (NLU task) demonstrates robust performance, with commendable robustness to noise in text or speech data.</abstract>
      <url hash="67234c79">2024.findings-acl.684</url>
      <bibkey>zhou-etal-2024-clasp</bibkey>
      <doi>10.18653/v1/2024.findings-acl.684</doi>
    </paper>
    <paper id="685">
      <title><fixed-case>T</fixed-case>ime<fixed-case>T</fixed-case>o<fixed-case>M</fixed-case>: Temporal Space is the Key to Unlocking the Door of Large Language Models’ Theory-of-Mind</title>
      <author><first>Guiyang</first><last>Hou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Wenqi</first><last>Zhang</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Linjuan</first><last>Wu</last></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>11532-11547</pages>
      <abstract>Theory of Mind (ToM)—the cognitive ability to reason about mental states of ourselves and others, is the foundation of social interaction. Although ToM comes naturally to humans, it poses a significant challenge to even the most advanced Large Language Models (LLMs). Due to the complex logical chains in ToM reasoning, especially in higher-order ToM questions, simply utilizing reasoning methods like Chain of Thought (CoT) will not improve the ToM capabilities of LLMs. We present TimeToM, which constructs a temporal space and uses it as the foundation to improve the ToM capabilities of LLMs in multiple scenarios. Specifically, within the temporal space, we construct Temporal Belief State Chain (TBSC) for each character and inspired by the cognition perspective of the social world model, we divide TBSC into self-world beliefs and social world beliefs, aligning with first-order ToM (first-order beliefs) and higher-order ToM (higher-order beliefs) questions, respectively. Moreover, we design a novel tool-belief solver that, by considering belief communication between characters in temporal space, can transform a character’s higher-order beliefs into another character’s first-order beliefs under belief communication period.</abstract>
      <url hash="f38a1e3a">2024.findings-acl.685</url>
      <bibkey>hou-etal-2024-timetom</bibkey>
      <doi>10.18653/v1/2024.findings-acl.685</doi>
    </paper>
    <paper id="686">
      <title>Identifying and Mitigating Annotation Bias in Natural Language Understanding using Causal Mediation Analysis</title>
      <author><first>Sitiporn</first><last>Sae Lim</last></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last><affiliation>Vidyasirimedhi Institute of Science and Technology (VISTEC)</affiliation></author>
      <author><first>Peerat</first><last>Limkonchotiwat</last></author>
      <author><first>Ekapol</first><last>Chuangsuwanich</last><affiliation>Chulalongkorn University</affiliation></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>11548-11563</pages>
      <abstract>NLU models have achieved promising results on standard benchmarks. Despite state-of-the-art accuracy, analysis reveals that many models make predictions using annotation bias rather than the properties we intend the model to learn. Consequently, these models perform poorly on out-of-distribution datasets. Recent advances in bias mitigation show that annotation bias can be alleviated through fine-tuning debiasing objectives. In this paper, we apply causal mediation analysis to gauge how much each model component mediates annotation biases. Using the knowledge from the causal analysis, we improve the model’s robustness against annotation bias through two bias mitigation methods: causal-grounded masking and gradient unlearning. Causal analysis reveals that biases concentrated in specific components, even after employing other training-time debiasing techniques. Manipulating these components by masking out neurons’ activations or updating specific weight blocks both demonstrably improve robustness against annotation artifacts.</abstract>
      <url hash="340d29a8">2024.findings-acl.686</url>
      <bibkey>sae-lim-etal-2024-identifying</bibkey>
      <doi>10.18653/v1/2024.findings-acl.686</doi>
    </paper>
    <paper id="687">
      <title>Perturbed examples reveal invariances shared by language models</title>
      <author><first>Ruchit</first><last>Rawal</last><affiliation>MPI-SWS</affiliation></author>
      <author><first>Mariya</first><last>Toneva</last><affiliation>Max Planck Institute for Software Systems</affiliation></author>
      <pages>11564-11584</pages>
      <abstract>The rapid growth in natural language processing (NLP) research has led to numerous new models, outpacing our understanding of how they compare to established ones. One major reason for this difficulty is saturating benchmarks, which may not well reflect differences in model performance in the wild. In this work, we introduce a novel framework to compare two NLP models by revealing their shared invariance to interpretable input perturbations targeting a specific linguistic capability. Via experiments on models from the same and different architecture families, this framework offers insights about how changes in models (e.g., distillation, size increase) affect linguistic capabilities. Furthermore, our framework enables evaluation of invariances between commercial black-box models (e.g., InstructGPT family) and models that are better understood (e.g., GPT-2). Across experiments, we observe that large language models share many invariances encoded by models of various sizes, whereas the invariances by large models are only shared by other large models. Possessing a wide variety of invariances may be key to the recent successes of large language models, and our framework can shed light on the types of invariances retained or emerging in new models. We make the code publicly available.</abstract>
      <url hash="f74c0753">2024.findings-acl.687</url>
      <bibkey>rawal-toneva-2024-perturbed</bibkey>
      <doi>10.18653/v1/2024.findings-acl.687</doi>
    </paper>
    <paper id="688">
      <title>Dynamic Stochastic Decoding Strategy for Open-Domain Dialogue Generation</title>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Yitong</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Bin</first><last>Sun</last></author>
      <author><first>Shaoxiong</first><last>Feng</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>11585-11596</pages>
      <abstract>Stochastic sampling strategies such as top-k and top-p have been widely used in dialogue generation task. However, as an open-domain chatting system, there will be two different conversation scenarios, i.e. chit-chat and knowledge-based question answering. In the former situation, responses diversity is essential due to the one-to-many nature in dialogue. The latter, on the other hand, requires less randomness given that stochastic decoding strategy entails the risk of generating incorrect information. As a result, an adaptive and flexible decoding strategy is needed to cope with these two scenarios simultaneously. To this end, we propose the dynamic decoding strategy (DDS), which can adjust the decoding space w.r.t. different contexts. In DDS, both sequence-level and token-level adaptive search can be achieved to adjust the decoding process in a unified framework. Besides, our adaptive algorithm can not only be used during model inference, but it can also be applied during the model training stage to further enhance the performance. Comprehensive experiments indicate that the proposed decoding strategy can consistently improve the performance of pre-trained dialogue models when coupled with four well-used stochastic decoding algorithms.</abstract>
      <url hash="3524c649">2024.findings-acl.688</url>
      <bibkey>li-etal-2024-dynamic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.688</doi>
    </paper>
    <paper id="689">
      <title>Discourse Structure-Aware Prefix for Generation-Based End-to-End Argumentation Mining</title>
      <author><first>Yang</first><last>Sun</last></author>
      <author><first>Guanrong</first><last>Chen</last></author>
      <author><first>Caihua</first><last>Yang</last></author>
      <author><first>Jianzhu</first><last>Bao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Xi</first><last>Zeng</last></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>11597-11613</pages>
      <abstract>End-to-end argumentation mining (AM) aims to extract the argumentation structure including argumentation components and their argumentation relations from text. Recent developments in end-to-end AM models have demonstrated significant progress by redefining the AM task as a sequence generation task, exhibiting simplicity and competitive performance. Nevertheless, these models overlook the integration of supplementary discourse structure information, a crucial factor for comprehending argumentation structures, resulting in suboptimal outcomes. In this study, we propose the DENIM framework, which generates discourse structure-aware prefixes for each layer of the generation model. These prefixes imbue the generation-based AM model with discourse structures, thereby augmenting the overall generation process. Moreover, we introduce a multi-task prompt coupled with a three-step decoding strategy, aiming to optimize the efficiency and effectiveness of argumentation structure decoding. Extensive experiments and analyses on two benchmark datasets show that DENIM achieves state-of-the-art performances on two AM benchmarks.</abstract>
      <url hash="f73513f7">2024.findings-acl.689</url>
      <bibkey>sun-etal-2024-discourse</bibkey>
      <doi>10.18653/v1/2024.findings-acl.689</doi>
    </paper>
    <paper id="690">
      <title>Poor-Supervised Evaluation for <fixed-case>S</fixed-case>uper<fixed-case>LLM</fixed-case> via Mutual Consistency</title>
      <author><first>Peiwen</first><last>Yuan</last></author>
      <author><first>Shaoxiong</first><last>Feng</last></author>
      <author><first>Yiwei</first><last>Li</last></author>
      <author><first>Xinglin</first><last>Wang</last></author>
      <author><first>Boyuan</first><last>Pan</last></author>
      <author><first>Heda</first><last>Wang</last></author>
      <author><first>Yao</first><last>Hu</last></author>
      <author><first>Kan</first><last>Li</last></author>
      <pages>11614-11627</pages>
      <abstract>The guidance from capability evaluations has greatly propelled the progress of human society and the development of Artificial Intelligence. However, as LLMs evolve, it becomes challenging to construct evaluation benchmark with accurate labels for SuperLLMs whose capabilities approach or even surpass those of humans. To credibly conduct poor-supervised evaluation without accurate labels, we first prove that the consistency between the model under evaluation and the reference model, when their prediction distributions are independent and the sample size is infinite, can equivalently assess the true capabilities of the model to be evaluated. However, using either humans or LLMs as the reference model cannot sufficiently meet the conditions, for which we propose the PEEM algorithm. By treating all models under evaluation as reference models, PEEM alternately optimizes model weights and filters reference models based on EM algorithm to maximally alleviate the insufficiency of the conditions. Comprehensive experiments across 3 types of tasks with 16 mainstream LLMs validate the efficiency, universality, and effectiveness of PEEM. More generally, PEEM has advanced the evaluation paradigm evolution from human-centric to human&amp;model-centric, alleviating the limitations of human capabilities for evaluating SuperLLMs.</abstract>
      <url hash="d89bbdd7">2024.findings-acl.690</url>
      <bibkey>yuan-etal-2024-poor</bibkey>
      <doi>10.18653/v1/2024.findings-acl.690</doi>
    </paper>
    <paper id="691">
      <title>Addressing Entity Translation Problem via Translation Difficulty and Context Diversity</title>
      <author><first>Tian</first><last>Liang</last></author>
      <author><first>Xing</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Mingming</first><last>Yang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yujiu</first><last>Yang</last><affiliation>Graduate School at Shenzhen,Tsinghua University</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>11628-11638</pages>
      <abstract>Neural machine translation (NMT) systems often produce inadequate translations for named entities. In this study, we conducted preliminary experiments to examine the factors affecting the translation accuracy of named entities, specifically focusing on their translation difficulty and context diversity. Based on our observations, we propose a novel data augmentation strategy to enhance the accuracy of named entity translation. The main concept behind our approach is to increase both the context diversity and translation probability for the targeted named entity pair. To achieve this, we construct additional samples for named entities that exhibit high translation difficulty or low context diversity and use the augmented training data to re-train the final translation model. Furthermore, we propose an entity-aware machine translation metric that prefers the translation output to generate more accurate named entities. Our experimental results demonstrate significant improvements over the baseline in terms of general translation performance and named entity translation accuracy across various test sets, such as WMT news translation and terminology test sets.</abstract>
      <url hash="263c4946">2024.findings-acl.691</url>
      <bibkey>liang-etal-2024-addressing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.691</doi>
    </paper>
    <paper id="692">
      <title><fixed-case>ADAM</fixed-case>: Dense Retrieval Distillation with Adaptive Dark Examples</title>
      <author><first>Chongyang</first><last>Tao</last><affiliation>Beihang University</affiliation></author>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Tao</first><last>Shen</last><affiliation>Oracle</affiliation></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Xiubo</first><last>Geng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Binxing</first><last>Jiao</last></author>
      <author><first>Daxin</first><last>Jiang</last><affiliation>Microsoft</affiliation></author>
      <pages>11639-11651</pages>
      <abstract>To improve the performance of the dual-encoder retriever, one effective approach is knowledge distillation from the cross-encoder ranker. Existing works prepare training instances by pairing each query with one positive and a batch of negatives. However, most hard negatives mined by advanced dense retrieval methods are still too trivial for the teacher to distinguish, preventing the teacher from transferring abundant dark knowledge to the student through its soft label. To alleviate this issue, we propose Adam, a knowledge distillation framework that can better transfer the dark knowledge held in the teacher with adaptive dark examples. Different from previous works that only rely on one positive and hard negatives as candidate passages, we create dark examples that all have moderate relevance to the query by strengthening negatives and masking positives in the discrete space. Furthermore, as the quality of knowledge held in different training instances varies as measured by the teacher’s confidence score, we propose a self-paced distillation strategy that adaptively concentrates on a subset of high-quality instances to conduct our dark-example-based knowledge distillation to help the student learn better. We conduct experiments on two widely-used benchmarks and verify the effectiveness of our method.</abstract>
      <url hash="6d59cf58">2024.findings-acl.692</url>
      <bibkey>tao-etal-2024-adam</bibkey>
      <doi>10.18653/v1/2024.findings-acl.692</doi>
    </paper>
    <paper id="693">
      <title>Instruction Position Matters in Sequence Generation with Large Language Models</title>
      <author><first>Yijin</first><last>Liu</last><affiliation>Wechat AI</affiliation></author>
      <author><first>Xianfeng</first><last>Zeng</last></author>
      <author><first>Chenze</first><last>Shao</last><affiliation>Tencent Inc</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>11652-11663</pages>
      <abstract>Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model’s learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequence generation tasks (translation and summarization), without any additional data or annotation costs. Notably, our method significantly improves the zero-shot performance on conditional sequence generation, e.g., up to 9.7 BLEU points on WMT zero-shot translation tasks. Further analysis reveals that our method can significantly improve the tranditional model’s instruction following ability by 1x over traditional approch.</abstract>
      <url hash="3eb4bbbf">2024.findings-acl.693</url>
      <bibkey>liu-etal-2024-instruction</bibkey>
      <doi>10.18653/v1/2024.findings-acl.693</doi>
    </paper>
    <paper id="694">
      <title><fixed-case>XM</fixed-case>o<fixed-case>E</fixed-case>: Sparse Models with Fine-grained and Adaptive Expert Selection</title>
      <author><first>Yuanhang</first><last>Yang</last></author>
      <author><first>Shiyi</first><last>Qi</last></author>
      <author><first>Wenchao</first><last>Gu</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Chaozheng</first><last>Wang</last></author>
      <author><first>Cuiyun</first><last>Gao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Zenglin</first><last>Xu</last><affiliation>Fudan University</affiliation></author>
      <pages>11664-11674</pages>
      <abstract>Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations by multiplying values by zero or low activation values. To address this issue, we present XMoE, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. XMoE leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that enhances model performance and can decrease the computation load at MoE layers by over 50% without sacrificing performance. Furthermore, we present the versatility of by applying it to dense models, enabling sparse computation during inference. We provide a comprehensive analysis and make our code available at <url>https://anonymous.4open.science/r/XMoE</url>.</abstract>
      <url hash="d69ae064">2024.findings-acl.694</url>
      <bibkey>yang-etal-2024-xmoe</bibkey>
      <doi>10.18653/v1/2024.findings-acl.694</doi>
    </paper>
    <paper id="695">
      <title><fixed-case>B</fixed-case>ranch<fixed-case>N</fixed-case>orm: Robustly Scaling Extremely Deep Transformers</title>
      <author><first>Yijin</first><last>Liu</last><affiliation>Wechat AI</affiliation></author>
      <author><first>Xianfeng</first><last>Zeng</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>11675-11687</pages>
      <abstract>Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000 layers) and reveals the promising potential of deep scaling. To stabilize the training of deep models, DeepNorm attempts to constrain the model update to a constant value. Although applying such a constraint can benefit the early stage of model training, it may lead to undertrained models during the whole training procedure. In this paper, we propose BranchNorm, which dynamically rescales the non-residual branch of Transformer in accordance with the training period. BranchNorm not only theoretically stabilizes the training with smooth gradient norms at the early stage, but also encourages better convergence in the subsequent training stage. Experimental results on multiple translation tasks demonstrate that BranchNorm achieves a better trade-off between training stability and converge performance.</abstract>
      <url hash="97275172">2024.findings-acl.695</url>
      <bibkey>liu-etal-2024-branchnorm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.695</doi>
    </paper>
    <paper id="696">
      <title><fixed-case>M</fixed-case>us<fixed-case>TQ</fixed-case>: A Temporal Knowledge Graph Question Answering Dataset for Multi-Step Temporal Reasoning</title>
      <author><first>Tingyi</first><last>Zhang</last></author>
      <author><first>Jiaan</first><last>Wang</last><affiliation>Soochow University</affiliation></author>
      <author><first>Zhixu</first><last>Li</last></author>
      <author><first>Jianfeng</first><last>Qu</last><affiliation>Soochow University</affiliation></author>
      <author><first>An</first><last>Liu</last><affiliation>Suzhou University</affiliation></author>
      <author><first>Zhigang</first><last>Chen</last><affiliation>iFLYTEK Research</affiliation></author>
      <author><first>Hongping</first><last>Zhi</last></author>
      <pages>11688-11699</pages>
      <abstract>Question answering over temporal knowledge graphs (TKGQA) is an emerging topic, which has attracted increasing interest since it considers the dynamic knowledge in the world. Several datasets along with model developments are proposed in the TKGQA research field. However, existing studies generally focus on fact-centered reasoning, with limited attention to temporal reasoning. To tackle the intricate and comprehensive nature of temporal reasoning, we propose a new TKGQA dataset, MusTQ, which contains 666K multi-step temporal reasoning questions as well as a TKG. The multi-step temporal reasoning is established based on six basic temporal reasoning types derived from a well-established measure theory. Using MusTQ, we evaluate previous TKGQA methods and find that they typically fall short in multi-step temporal reasoning. Furthermore, we propose a TKGQA model, MusTKGQA, which enhances multi-step reasoning ability with entity-time attention mechanism and optimized temporal knowledge graph representation. Extensive experiments on MusTQ show that our model achieves state-of-the-art multi-step temporal reasoning performance.</abstract>
      <url hash="328e1395">2024.findings-acl.696</url>
      <bibkey>zhang-etal-2024-mustq</bibkey>
      <doi>10.18653/v1/2024.findings-acl.696</doi>
    </paper>
    <paper id="697">
      <title>Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models</title>
      <author><first>Anthony</first><last>Sicilia</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Hyunwoo</first><last>Kim</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Khyathi</first><last>Chandu</last></author>
      <author><first>Malihe</first><last>Alikhani</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Jack</first><last>Hessel</last><affiliation>Samaya AI</affiliation></author>
      <pages>11700-11726</pages>
      <abstract>Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing “conversation forecasting” task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their size.</abstract>
      <url hash="f9d49c2c">2024.findings-acl.697</url>
      <bibkey>sicilia-etal-2024-deal</bibkey>
      <doi>10.18653/v1/2024.findings-acl.697</doi>
    </paper>
    <paper id="698">
      <title>Knowledge Fusion By Evolving Weights of Language Models</title>
      <author><first>Guodong</first><last>Du</last></author>
      <author><first>Jing</first><last>Li</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Hanting</first><last>Liu</last></author>
      <author><first>Runhua</first><last>Jiang</last></author>
      <author><first>Shuyang</first><last>Yu</last></author>
      <author><first>Yifei</first><last>Guo</last></author>
      <author><first>Sim Kuan</first><last>Goh</last><affiliation>Xiamen University Malaysia</affiliation></author>
      <author><first>Ho-Kin</first><last>Tang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>11727-11742</pages>
      <abstract>Fine-tuning pre-trained language models, particularly large language models, demands extensive computing resources and can result in varying performance outcomes across different domains and datasets. This paper examines the approach of integrating multiple models from diverse training scenarios into a unified model. This unified model excels across various data domains and exhibits the ability to generalize well on out-of-domain data. We propose a knowledge fusion method named Evolver, inspired by evolutionary algorithms, which does not need further training or additional training data. Specifically, our method involves aggregating the weights of different language models into a population and subsequently generating offspring models through mutation and crossover operations. These offspring models are then evaluated against their parents, allowing for the preservation of those models that show enhanced performance on development datasets. Importantly, our model evolving strategy can be seamlessly integrated with existing model merging frameworks, offering a versatile tool for model enhancement. Experimental results on mainstream language models (i.e., encoder-only, decoder-only, encoder-decoder) reveal that Evolver outperforms previous state-of-the-art models by large margins.</abstract>
      <url hash="1225968b">2024.findings-acl.698</url>
      <bibkey>du-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.findings-acl.698</doi>
    </paper>
    <paper id="699">
      <title><fixed-case>S</fixed-case>ca<fixed-case>L</fixed-case>earn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale</title>
      <author><first>Markus</first><last>Frohmann</last><affiliation>Johannes Kepler Universität Linz</affiliation></author>
      <author><first>Carolin</first><last>Holtermann</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Shahed</first><last>Masoudian</last></author>
      <author><first>Anne</first><last>Lauscher</last><affiliation>Universität Hamburg</affiliation></author>
      <author><first>Navid</first><last>Rekabsaz</last><affiliation>Thomson Reuters</affiliation></author>
      <pages>11743-11776</pages>
      <abstract>Multi-task learning (MTL) has shown considerable practical benefits, particularly when using language models (LMs). While this is commonly achieved by learning tasks under a joint optimization procedure, some methods, such as AdapterFusion, divide the problem into two stages: (i) task learning, where knowledge specific to a task is encapsulated within sets of parameters (e.g., adapters), and (ii) transfer, where this already learned knowledge is leveraged for a target task. This separation of concerns provides numerous benefits (e.g., promoting reusability). However, current two stage MTL introduces a substantial number of additional parameters. We address this issue by leveraging the usefulness of linearly scaling the output representations of source adapters for transfer learning. We introduce ScaLearn, a simple and highly parameter-efficient two-stage MTL method that capitalizes on the knowledge of the source tasks by learning a minimal set of scaling parameters that enable effective transfer to a target task. Our experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) and two encoder LMs show that ScaLearn consistently outperforms strong baselines with a small number of transfer parameters (~0.35% of those of AdapterFusion). Remarkably, we observe that ScaLearn maintains its strong abilities even when further reducing parameters, achieving competitive results with only 8 transfer parameters per target task. Our proposed approach thus demonstrates the power of simple scaling as a promise for more efficient task transfer. Our code is available at https://github.com/CPJKU/ScaLearn.</abstract>
      <url hash="5ac1c9d4">2024.findings-acl.699</url>
      <bibkey>frohmann-etal-2024-scalearn</bibkey>
      <doi>10.18653/v1/2024.findings-acl.699</doi>
    </paper>
    <paper id="700">
      <title>Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models</title>
      <author><first>Chang-Sheng</first><last>Kao</last></author>
      <author><first>Yun-Nung</first><last>Chen</last><affiliation>Department of Computer Science and Informational Engineering, National Taiwan University</affiliation></author>
      <pages>11777-11788</pages>
      <abstract>For dialogue systems, the utilization of multimodal dialogue responses, as opposed to relying solely on text-only responses, offers the capability to describe different concepts through various modalities. This enhances the effectiveness of communication and elevates the overall conversational experience. However, current methods for dialogue-to-image retrieval are constrained by the capabilities of the pre-trained vision language models (VLMs). They struggle to accurately extract key information from conversations and are unable to handle long-turn conversations. In this paper, we leverage the reasoning capabilities of large language models (LLMs) to predict the potential features that may be present in the images to be shared, based on the dialogue context. This approach allows us to obtain succinct and precise descriptors, thereby improving the performance of text-image retrieval. Experimental results shows that our method outperforms previous approaches significantly in terms of Recall@k.</abstract>
      <url hash="0cc4874e">2024.findings-acl.700</url>
      <bibkey>kao-chen-2024-visualizing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.700</doi>
    </paper>
    <paper id="701">
      <title><fixed-case>M</fixed-case>at<fixed-case>P</fixed-case>lot<fixed-case>A</fixed-case>gent: Method and Evaluation for <fixed-case>LLM</fixed-case>-Based Agentic Scientific Data Visualization</title>
      <author><first>Zhiyu</first><last>Yang</last></author>
      <author><first>Zihan</first><last>Zhou</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Xin</first><last>Cong</last></author>
      <author><first>Xu</first><last>Han</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yukun</first><last>Yan</last></author>
      <author><first>Zhenghao</first><last>Liu</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Zhixing</first><last>Tan</last><affiliation>Zhongguancun Laboratory</affiliation></author>
      <author><first>Pengyuan</first><last>Liu</last><affiliation>Beijing Language and Culture University</affiliation></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Zhiyuan</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xiaodong</first><last>Shi</last><affiliation>Xiamen University, Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>11789-11804</pages>
      <abstract>Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.</abstract>
      <url hash="bbb09747">2024.findings-acl.701</url>
      <bibkey>yang-etal-2024-matplotagent</bibkey>
      <doi>10.18653/v1/2024.findings-acl.701</doi>
    </paper>
    <paper id="702">
      <title>Continual Few-shot Relation Extraction via Adaptive Gradient Correction and Knowledge Decomposition</title>
      <author><first>Jianpeng</first><last>Hu</last></author>
      <author><first>Chengxiang</first><last>Tan</last></author>
      <author><first>JiaCheng</first><last>Xu</last></author>
      <author><first>XiangyunKong</first><last>XiangyunKong</last></author>
      <pages>11805-11816</pages>
      <abstract>Continual few-shot relation extraction (CFRE) aims to continually learn new relations with limited samples. However, current methods neglect the instability of embeddings in the process of different task training, which leads to serious catastrophic forgetting. In this paper, we propose the concept of the following degree from the perspective of instability to analyze catastrophic forgetting and design a novel method based on adaptive gradient correction and knowledge decomposition to alleviate catastrophic forgetting. Specifically, the adaptive gradient correction algorithm is designed to limit the instability of embeddings, which adaptively constrains the current gradient to be orthogonal to the embedding space learned from previous tasks. To reduce the instability between samples and prototypes, the knowledge decomposition module decomposes knowledge into general and task-related knowledge from the perspective of model architecture, which is asynchronously optimized during training. Experimental results on two standard benchmarks show that our method outperforms the state-of-the-art CFRE model and effectively improves the following degree of embeddings.</abstract>
      <url hash="25065f3e">2024.findings-acl.702</url>
      <bibkey>hu-etal-2024-continual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.702</doi>
    </paper>
    <paper id="703">
      <title><fixed-case>CM</fixed-case>oral<fixed-case>E</fixed-case>val: A Moral Evaluation Benchmark for <fixed-case>C</fixed-case>hinese Large Language Models</title>
      <author><first>Linhao</first><last>Yu</last></author>
      <author><first>Yongqi</first><last>Leng</last></author>
      <author><first>Yufei</first><last>Huang</last></author>
      <author><first>Shang</first><last>Wu</last></author>
      <author><first>Haixin</first><last>Liu</last></author>
      <author><first>Xinmeng</first><last>Ji</last></author>
      <author><first>Jiahui</first><last>Zhao</last></author>
      <author><first>Jinwang</first><last>Song</last></author>
      <author><first>Tingting</first><last>Cui</last></author>
      <author><first>Xiaoqing</first><last>Cheng</last><affiliation>Zhengzhou University</affiliation></author>
      <author><first>Liutao</first><last>Liutao</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>11817-11837</pages>
      <abstract>What a large language model (LLM) would respond in ethically relevant context? In this paper, we curate a large benchmark CMoralEval for morality evaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a Chinese TV program discussing Chinese moral norms with stories from the society and 2) a collection of Chinese moral anomies from various newspapers and academic papers on morality. With these sources, we aim to create a moral evaluation dataset characterized by diversity and authenticity. We develop a morality taxonomy and a set of fundamental moral principles that are not only rooted in traditional Chinese culture but also consistent with contemporary societal norms. To facilitate efficient construction and annotation of instances in CMoralEval, we establish a platform with AI-assisted instance generation to streamline the annotation process. These help us curate CMoralEval that encompasses both explicit moral scenarios (14,964 instances) and moral dilemma scenarios (15,424 instances), each with instances from different data sources. We conduct extensive experiments with CMoralEval to examine a variety of Chinese LLMs. Experiment results demonstrate that CMoralEval is a challenging benchmark for Chinese LLMs.</abstract>
      <url hash="55ada6df">2024.findings-acl.703</url>
      <bibkey>yu-etal-2024-cmoraleval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.703</doi>
    </paper>
    <paper id="704">
      <title>Cache &amp; Distil: Optimising <fixed-case>API</fixed-case> Calls to Large Language Models</title>
      <author><first>Guillem</first><last>Ramírez</last></author>
      <author><first>Matthias</first><last>Lindemann</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Ivan</first><last>Titov</last><affiliation>University of Edinburgh and University of Amsterdam</affiliation></author>
      <pages>11838-11853</pages>
      <abstract>Large-scale deployment of generative AI tools often depends on costly API calls to a Large Language Model (LLM) to fulfil user queries, a process that also exposes the request stream to external providers. To curtail the frequency of these calls, one can employ a local smaller language model -a student- which is continuously trained on the responses of the LLM. This student gradually gains proficiency in independently handling an increasing number of user requests, a process we term neural caching. The crucial element in neural caching is a policy that decides which requests should be processed by the student alone and which should be redirected to the LLM, subsequently aiding the student’s learning. In this study, we focus on classification tasks, and we consider a range of classic Active Learning-based selection criteria as the policy. Our experiments suggest that Margin Sampling and Query by Committee bring consistent benefits over other policies and baselines across tasks and budgets.</abstract>
      <url hash="a4e86742">2024.findings-acl.704</url>
      <bibkey>ramirez-etal-2024-cache</bibkey>
      <doi>10.18653/v1/2024.findings-acl.704</doi>
    </paper>
    <paper id="705">
      <title>Investigating the Impact of Model Instability on Explanations and Uncertainty</title>
      <author><first>Sara</first><last>Marjanovic</last></author>
      <author><first>Isabelle</first><last>Augenstein</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Christina</first><last>Lioma</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>11854-11879</pages>
      <abstract>Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn’t necessarily imply low explanation plausibility; the correlation between the two metrics can be moderately positive when noise is exposed during the training process. This suggests that noise-augmented models may be better at identifying salient tokens when uncertain. Furthermore, when predictive and epistemic uncertainty measures are over-confident, the robustness of a saliency map to perturbation can indicate model stability issues. Integrated Gradients shows the overall greatest robustness to perturbation, while still showing model-specific patterns in performance; however, this phenomenon is limited to smaller Transformer-based language models.</abstract>
      <url hash="39f5df7a">2024.findings-acl.705</url>
      <bibkey>marjanovic-etal-2024-investigating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.705</doi>
    </paper>
    <paper id="706">
      <title>A Two-Stage Adaptation of Large Language Models for Text Ranking</title>
      <author><first>Longhui</first><last>Zhang</last></author>
      <author><first>Yanzhao</first><last>Zhang</last></author>
      <author><first>Dingkun</first><last>Long</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China and Tianjin University, China</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>11880-11891</pages>
      <abstract>Text ranking is a critical task in information retrieval. Recent advances in pre-trained language models (PLMs), especially large language models (LLMs), present new opportunities for applying them to text ranking. While supervised fine-tuning (SFT) with ranking data has been widely explored to better align PLMs with text ranking goals, previous studies have focused primarily on encoder-only and encoder-decoder PLMs. Research on leveraging decoder-only LLMs for text ranking remains scarce. An exception to this is RankLLaMA, which uses direct SFT to explore LLaMA’s potential for text ranking. In this work, we propose a two-stage progressive paradigm to better adapt LLMs to text ranking. First, we conduct continual pre-training (CPT) of LLMs on a large weakly-supervised corpus. Second, we perform SFT, and propose an improved optimization strategy building upon RankLLaMA. Our experimental results on multiple benchmarks show that our approach outperforms previous methods in both in-domain and out-domain scenarios.</abstract>
      <url hash="8e953c9b">2024.findings-acl.706</url>
      <bibkey>zhang-etal-2024-two</bibkey>
      <doi>10.18653/v1/2024.findings-acl.706</doi>
    </paper>
    <paper id="707">
      <title>Fine-tuning with <fixed-case>HED</fixed-case>-<fixed-case>IT</fixed-case>: The impact of human post-editing for dialogical language models</title>
      <author><first>Daniela</first><last>Occhipinti</last></author>
      <author><first>Michele</first><last>Marchi</last></author>
      <author><first>Irene</first><last>Mondella</last></author>
      <author><first>Huiyuan</first><last>Lai</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Felice</first><last>Dell’Orletta</last><affiliation>Istituto di Linguistica Computazionale “A. Zampolli” (ILC)</affiliation></author>
      <author><first>Malvina</first><last>Nissim</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Marco</first><last>Guerini</last><affiliation>Fondazione Bruno Kessler</affiliation></author>
      <pages>11892-11907</pages>
      <abstract>Automatic methods for generating and gathering linguistic data have proven effective for fine-tuning Language Models (LMs) in languages less resourced than English. Still, while there has been emphasis on data quantity, less attention has been given to its quality. In this work, we investigate the impact of human intervention on machine-generated data when fine-tuning dialogical models. In particular, we study (1) whether post-edited dialogues exhibit higher perceived quality compared to the originals that were automatically generated; (2) whether fine-tuning with post-edited dialogues results in noticeable differences in the generated outputs; and (3) whether post-edited dialogues influence the outcomes when considering the parameter size of the LMs. To this end we created HED-IT, a large-scale dataset where machine-generated dialogues are paired with the version post-edited by humans. Using both the edited and unedited portions of HED-IT, we fine-tuned three different sizes of an LM. Results from both human and automatic evaluation show that the different quality of training data is clearly perceived and it has an impact also on the models trained on such data. Additionally, our findings indicate that larger models are less sensitive to data quality, whereas this has a crucial impact on smaller models. These results enhance our comprehension of the impact of human intervention on training data in the development of high-quality LMs.</abstract>
      <url hash="13b1bbfd">2024.findings-acl.707</url>
      <bibkey>occhipinti-etal-2024-fine</bibkey>
      <doi>10.18653/v1/2024.findings-acl.707</doi>
    </paper>
    <paper id="708">
      <title>Analyze, Generate and Refine: Query Expansion with <fixed-case>LLM</fixed-case>s for Zero-Shot Open-Domain <fixed-case>QA</fixed-case></title>
      <author><first>Xinran</first><last>Chen</last></author>
      <author><first>Xuanang</first><last>Chen</last></author>
      <author><first>Ben</first><last>He</last></author>
      <author><first>Tengfei</first><last>Wen</last></author>
      <author><first>Le</first><last>Sun</last><affiliation>Institute of Software, Chinese Academy of Sciences</affiliation></author>
      <pages>11908-11922</pages>
      <abstract>Query expansion (QE) is a critical component in the open-domain question answering (OpenQA) pipeline, enhancing the retrieval performance by broadening the scope of queries with additional relevant texts. However, existing methods like GAR and EAR rely heavily on supervised training and often struggle to maintain effectiveness across domains and datasets. Meanwhile, although large language models (LLMs) have demonstrated QE capability for information retrieval (IR) tasks, their application in OpenQA is hindered by the inadequate analysis of query’s informational needs and the lack of quality control for generated QEs, failing to meet the unique requirements of OpenQA. To bridge this gap, we propose a novel LLM-based QE approach named AGR for the OpenQA task, leveraging a three-step prompting strategy. AGR begins with an analysis of the query, followed by the generation of answer-oriented expansions, and culminates with a refinement process for better query formulation. Extensive experiments on four OpenQA datasets reveal that AGR not only rivals in-domain supervised methods in retrieval accuracy, but also outperforms state-of-the-art baselines in out-domain zero-shot scenarios. Moreover, it exhibits enhanced performance in end-to-end QA evaluations, underscoring the superiority of AGR for OpenQA.</abstract>
      <url hash="c42134ec">2024.findings-acl.708</url>
      <bibkey>chen-etal-2024-analyze</bibkey>
      <doi>10.18653/v1/2024.findings-acl.708</doi>
    </paper>
    <paper id="709">
      <title>On the Evaluation of Speech Foundation Models for Spoken Language Understanding</title>
      <author><first>Siddhant</first><last>Arora</last></author>
      <author><first>Ankita</first><last>Pasad</last></author>
      <author><first>Chung-Ming</first><last>Chien</last></author>
      <author><first>Jionghao</first><last>Han</last></author>
      <author><first>Roshan</first><last>Sharma</last><affiliation>Google</affiliation></author>
      <author><first>Jee-weon</first><last>Jung</last><affiliation>CMU, Carnegie Mellon University</affiliation></author>
      <author><first>Hira</first><last>Dhamyal</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>William</first><last>Chen</last></author>
      <author><first>Suwon</first><last>Shon</last><affiliation>ASAPP</affiliation></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Karen</first><last>Livescu</last><affiliation>Toyota Technological Institute at Chicago</affiliation></author>
      <author><first>Shinji</first><last>Watanabe</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>11923-11938</pages>
      <abstract>The Spoken Language Understanding Evaluation (SLUE) suite of benchmark tasks was recently introduced to address the need for openresources and benchmarking of complex spoken language understanding (SLU) tasks, including both classification and sequence generation tasks, on natural speech. The benchmark has demonstrated preliminary success in using pre-trained speech foundation models (SFM) for these SLU tasks. However, the community still lacks a fine-grained understanding of the comparative utility of different SFMs. Inspired by this, we ask: which SFMs offer the most benefits for these complex SLU tasks, and what is the most effective approach for incorporating these SFMs? To answer this, we perform an extensive evaluation of multiple supervised and self-supervised SFMs using several evaluation protocols: (i) frozen SFMs with a lightweight prediction head, (ii) frozen SFMs with a complex prediction head, and (iii) fine-tuned SFMs with a lightweight prediction head. Although the supervised SFMs are pre-trained on much more speech recognition data (with labels), they do not always outperform self-supervised SFMs; the latter tend to perform at least as well as, and sometimes better than, supervised SFMs, especially on the sequence generation tasks in SLUE. While there is no universally optimal way of incorporating SFMs, the complex prediction head gives the best performance for most tasks, although it increases the inference time. We also introduce an open-source toolkit and performance leaderboard, SLUE-PERB, for these tasks and modeling strategies.</abstract>
      <url hash="f15260c4">2024.findings-acl.709</url>
      <bibkey>arora-etal-2024-evaluation</bibkey>
      <doi>10.18653/v1/2024.findings-acl.709</doi>
    </paper>
    <paper id="710">
      <title>Towards Multiple References Era – Addressing Data Leakage and Limited Reference Diversity in Machine Translation Evaluation</title>
      <author><first>Xianfeng</first><last>Zeng</last></author>
      <author><first>Yijin</first><last>Liu</last><affiliation>Wechat AI</affiliation></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>11939-11951</pages>
      <abstract>Recent research has shown a weak correlation between n-gram-based metrics and human evaluations in machine translation task, particularly when evaluating large language models (LLMs). Additionally, the data leakage risk in LLMs may cause an overestimation problem when evaluating LLMs on downstream tasks. In this work, we identify the limited diversity of references as the primary cause for the inferior performance of n-gram-based metrics and the overestimation problem. To address this issue, we propose to utilize multiple references generated by LLMs, coupled with an effective selection strategy focused on accuracy and diversity, to improve the alignment between automatic metrics and human evaluations. We validate our approach on the WMT22 Metrics benchmark with 4 languages and observe a maximum accuracy gain of 9.5% in F200spBLEU, which makes it on par with computationally expensive neural-based metrics. We also show that using multi-reference with n-gram-based metrics significantly alleviates the overestimation problem when evaluating LLMs with data leakage. Further analysis explores the factors that affect the quality of generated references, offering insights into data synthesis by LLMs.</abstract>
      <url hash="389a085b">2024.findings-acl.710</url>
      <bibkey>zeng-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-acl.710</doi>
    </paper>
    <paper id="711">
      <title>Prompting open-source and commercial language models for grammatical error correction of <fixed-case>E</fixed-case>nglish learner text</title>
      <author><first>Christopher</first><last>Davis</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Andrew</first><last>Caines</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>O</first><last>Andersen</last><affiliation>Computer Laboratory</affiliation></author>
      <author><first>Shiva</first><last>Taslimipoor</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Helen</first><last>Yannakoudakis</last><affiliation>Computer Laboratory, University of Cambridge and King’s College London</affiliation></author>
      <author><first>Zheng</first><last>Yuan</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Christopher</first><last>Bryant</last><affiliation>Computer Laboratory</affiliation></author>
      <author><first>Marek</first><last>Rei</last><affiliation>Imperial College London</affiliation></author>
      <author><first>Paula</first><last>Buttery</last><affiliation>University of Cambridge</affiliation></author>
      <pages>11952-11967</pages>
      <abstract>Thanks to recent advances in generative AI, we are able to prompt large language models (LLMs) to produce texts which are fluent and grammatical. In addition, it has been shown that we can elicit attempts at grammatical error correction (GEC) from LLMs when prompted with ungrammatical input sentences. We evaluate how well LLMs can perform at GEC by measuring their performance on established benchmark datasets. We go beyond previous studies, which only examined GPT* models on a selection of English GEC datasets, by evaluating seven open-source and three commercial LLMs on four established GEC benchmarks. We investigate model performance and report results against individual error types. Our results indicate that LLMs do not always outperform supervised English GEC models except in specific contexts – namely commercial LLMs on benchmarks annotated with fluency corrections as opposed to minimal edits. We find that several open-source models outperform commercial ones on minimal edit benchmarks, and that in some settings zero-shot prompting is just as competitive as few-shot prompting.</abstract>
      <url hash="1dd67531">2024.findings-acl.711</url>
      <bibkey>davis-etal-2024-prompting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.711</doi>
    </paper>
    <paper id="712">
      <title><fixed-case>BATS</fixed-case>: <fixed-case>B</fixed-case>enchm<fixed-case>A</fixed-case>rking Text Simplicity 🦇</title>
      <author><first>Christin</first><last>Kreutz</last><affiliation>Technische Hochschule Mittelhessen</affiliation></author>
      <author><first>Fabian</first><last>Haak</last><affiliation>Fachhochschule Köln</affiliation></author>
      <author><first>Björn</first><last>Engelmann</last></author>
      <author><first>Philipp</first><last>Schaer</last><affiliation>TH Köln - University of Applied Sciences</affiliation></author>
      <pages>11968-11989</pages>
      <abstract>Evaluation of text simplification currently focuses on the difference of a source text to its simplified variant. Datasets for this evaluation base on a specific topic and group of readers for which is simplified. The broad applicability of text simplification and specifics that come with intended target audiences (e.g., children compared to adult non-experts) are disregarded. An explainable assessment of the overall simplicity of text is missing. This work is BenchmArking Text Simplicity (BATS): we provide an explainable method to assess practical and concrete rules from literature describing features of simplicity and complexity of text. Our experiments on 15 datasets for text simplification highlight differences in features that are important in different domains of text and for different intended target audiences.</abstract>
      <url hash="0adadce1">2024.findings-acl.712</url>
      <bibkey>kreutz-etal-2024-bats</bibkey>
      <doi>10.18653/v1/2024.findings-acl.712</doi>
    </paper>
    <paper id="713">
      <title><fixed-case>A</fixed-case>ustro<fixed-case>T</fixed-case>ox: A Dataset for Target-Based <fixed-case>A</fixed-case>ustrian <fixed-case>G</fixed-case>erman Offensive Language Detection</title>
      <author><first>Pia</first><last>Pachinger</last><affiliation>Technische Universität Wien</affiliation></author>
      <author><first>Janis</first><last>Goldzycher</last></author>
      <author><first>Anna</first><last>Planitzer</last></author>
      <author><first>Wojciech</first><last>Kusa</last><affiliation>Allegro</affiliation></author>
      <author><first>Allan</first><last>Hanbury</last><affiliation>Complexity Science Hub and Technische Universität Wien</affiliation></author>
      <author><first>Julia</first><last>Neidhardt</last><affiliation>Technische Universität Wien</affiliation></author>
      <pages>11990-12001</pages>
      <abstract>Model interpretability in toxicity detection greatly profits from token-level annotations. However, currently, such annotations are only available in English. We introduce a dataset annotated for offensive language detection sourced from a news forum, notable for its incorporation of the Austrian German dialect, comprising 4,562 user comments. In addition to binary offensiveness classification, we identify spans within each comment constituting vulgar language or representing targets of offensive statements. We evaluate fine-tuned Transformer models as well as large language models in a zero- and few-shot fashion. The results indicate that while fine-tuned models excel in detecting linguistic peculiarities such as vulgar dialect, large language models demonstrate superior performance in detecting offensiveness in AustroTox.</abstract>
      <url hash="25405073">2024.findings-acl.713</url>
      <bibkey>pachinger-etal-2024-austrotox</bibkey>
      <doi>10.18653/v1/2024.findings-acl.713</doi>
    </paper>
    <paper id="714">
      <title>Discovering influential text using convolutional neural networks</title>
      <author><first>Megan</first><last>Ayers</last></author>
      <author><first>Luke</first><last>Sanford</last></author>
      <author><first>Margaret</first><last>Roberts</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Eddie</first><last>Yang</last><affiliation>University of California, San Diego</affiliation></author>
      <pages>12002-12027</pages>
      <abstract>Experimental methods for estimating the impacts of text on human evaluation have been widely used in the social sciences. However, researchers in experimental settings are usually limited to testing a small number of pre-specified text treatments. While efforts to mine unstructured texts for features that causally affect outcomes have been ongoing in recent years, these models have primarily focused on the topics or specific words of text, which may not always be the mechanism of the effect. We connect these efforts with NLP interpretability techniques and present a method for flexibly discovering clusters of similar text phrases that are predictive of human reactions to texts using convolutional neural networks. When used in an experimental setting, this method can identify text treatments and their effects under certain assumptions. We apply the method to two data sets. The first enables direct validation of the model’s ability to detect phrases known to cause the outcome. The second demonstrates its ability to flexibly discover text treatments with varying textual structures. In both cases, the model learns a greater variety of text treatments compared to benchmark methods, and these text features quantitatively meet or exceed the ability of benchmark methods to predict the outcome.</abstract>
      <url hash="a7473c46">2024.findings-acl.714</url>
      <bibkey>ayers-etal-2024-discovering</bibkey>
      <doi>10.18653/v1/2024.findings-acl.714</doi>
    </paper>
    <paper id="715">
      <title><fixed-case>LC</fixed-case>4<fixed-case>EE</fixed-case>: <fixed-case>LLM</fixed-case>s as Good Corrector for Event Extraction</title>
      <author><first>Mengna</first><last>Zhu</last></author>
      <author><first>Kaisheng</first><last>Zeng</last></author>
      <author><first>JibingWu</first><last>JibingWu</last></author>
      <author><first>Lihua</first><last>Liu</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Hongbin</first><last>Huang</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Lei</first><last>Hou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>12028-12038</pages>
      <abstract>Event extraction (EE) is a critical task in natural language processing, yet deploying a practical EE system remains challenging. On one hand, powerful large language models (LLMs) currently show poor performance because EE task is more complex than other tasks. On the other hand, state-of-the-art (SOTA) small language models (SLMs) for EE tasks are typically developed through fine-tuning, lack flexibility, and have considerable room for improvement. We propose an approach, **L**LMs-as-**C**orrector for **E**vent **E**xtraction (**LC4EE**), aiming to leverage the superior extraction capability of SLMs and the instruction-following ability of LLMs to construct a robust and highly available EE system. By utilizing LLMs to identify and correct errors of SLMs predictions based on automatically generated feedback information, EE performances can be improved significantly. Experimental results on the representative datasets ACE2005 and MAVEN-Arg for Event Detection (ED) and EE tasks validated the effectiveness of our method.</abstract>
      <url hash="57ac86c1">2024.findings-acl.715</url>
      <bibkey>zhu-etal-2024-lc4ee</bibkey>
      <doi>10.18653/v1/2024.findings-acl.715</doi>
    </paper>
    <paper id="716">
      <title>Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models</title>
      <author><first>Yihong</first><last>Dong</last><affiliation>Peking University</affiliation></author>
      <author><first>Xue</first><last>Jiang</last><affiliation>Peking University</affiliation></author>
      <author><first>Huanyu</first><last>Liu</last></author>
      <author><first>Zhi</first><last>Jin</last><affiliation>Peking University and Peking University</affiliation></author>
      <author><first>Bin</first><last>Gu</last><affiliation>Beijing Institute of Control Engineering</affiliation></author>
      <author><first>Mengfei</first><last>Yang</last><affiliation>China Academy of Space Technology</affiliation></author>
      <author><first>Ge</first><last>Li</last><affiliation>Peking University Shenzhen Graduate School</affiliation></author>
      <pages>12039-12050</pages>
      <abstract>Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs’ training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM’s output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM’s output distribution. To facilitate this study, we introduce two benchmarks, i.e., DETCON and COMIEVAL, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8%-30.2% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect implicit contamination. TED substantially mitigates performance improvements up to 66.9% attributed to data contamination across various contamination setups. In real-world applications, we reveal that ChatGPT exhibits a high potential to suffer from data contamination on HumanEval benchmark.</abstract>
      <url hash="63cf45d9">2024.findings-acl.716</url>
      <bibkey>dong-etal-2024-generalization</bibkey>
      <doi>10.18653/v1/2024.findings-acl.716</doi>
    </paper>
    <paper id="717">
      <title>Efficient Training of Language Models with Compact and Consistent Next Token Distributions</title>
      <author><first>Ashutosh</first><last>Sathe</last></author>
      <author><first>Sunita</first><last>Sarawagi</last><affiliation>IIT Bombay</affiliation></author>
      <pages>12051-12064</pages>
      <abstract>Maximizing the likelihood of the next token is an established, statistically sound objective for pre-training language models. In this paper we show that we can train better models faster by pre-aggregating the corpus with a collapsed <tex-math>n</tex-math>-gram distribution. Previous studies have proposed corpus-level <tex-math>n</tex-math>-gram statistics as a regularizer; however, the construction and querying of such <tex-math>n</tex-math>-grams, if done naively, prove to be costly and significantly impede training speed, thereby limiting their application in modern large language model pre-training.We introduce an alternative compact representation of the next token distribution that, in expectation, aligns with the complete <tex-math>n</tex-math>-gram distribution while markedly reducing variance across mini-batches compared to the standard next-token loss. Empirically, we demonstrate that both the <tex-math>n</tex-math>-gram regularized model and our approximation yield substantial improvements in model quality and convergence rate compared to existing methods. Furthermore, our approximation facilitates scalability of gains to larger datasets and models compared to the straightforward <tex-math>n</tex-math>-gram regularization method.</abstract>
      <url hash="2e64eef6">2024.findings-acl.717</url>
      <bibkey>sathe-sarawagi-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.findings-acl.717</doi>
    </paper>
    <paper id="718">
      <title><fixed-case>A</fixed-case>ncient <fixed-case>C</fixed-case>hinese Glyph Identification Powered by Radical Semantics</title>
      <author><first>Yang</first><last>Chi</last><affiliation>Jilin University</affiliation></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <author><first>Chuntao</first><last>Li</last><affiliation>Jilin University</affiliation></author>
      <author><first>Hao</first><last>Xu</last><affiliation>Jilin University</affiliation></author>
      <pages>12065-12074</pages>
      <abstract>The ancestor of Chinese character – the ancient characters from about 1300 BC to 200 BC are not fixed in their writing glyphs. At the same or different points in time, one character can possess multiple glyphs that are different in shapes or radicals. Nearly half of ancient glyphs have not been deciphered yet. This paper proposes an innovative task of ancient Chinese glyph identification, which aims at inferring the Chinese character label for the unknown ancient Chinese glyphs which are not in the training set based on the image and radical information. Specifically, we construct a Chinese glyph knowledge graph (CGKG) associating glyphs in different historical periods according to the radical semantics, and propose a multimodal Chinese glyph identification framework (MCGI) fusing the visual, textual, and the graph data. The experiment is designed on a real Chinese glyph dataset spanning over 1000 years, it demonstrates the effectiveness of our method, and reports the potentials of each modality on this task. It provides a preliminary reference for the automatic ancient Chinese character deciphering at the glyph level.</abstract>
      <url hash="d6774774">2024.findings-acl.718</url>
      <bibkey>chi-etal-2024-ancient</bibkey>
      <doi>10.18653/v1/2024.findings-acl.718</doi>
    </paper>
    <paper id="719">
      <title><fixed-case>PUB</fixed-case>: A Pragmatics Understanding Benchmark for Assessing <fixed-case>LLM</fixed-case>s’ Pragmatics Capabilities</title>
      <author><first>Settaluri</first><last>Sravanthi</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology, Bombay</affiliation></author>
      <author><first>Meet</first><last>Doshi</last></author>
      <author><first>Pavan</first><last>Tankala</last></author>
      <author><first>Rudra</first><last>Murthy</last><affiliation>IBM India Ltd</affiliation></author>
      <author><first>Raj</first><last>Dabre</last><affiliation>National Institute of Information and Communications Technology (NICT), National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>12075-12097</pages>
      <abstract>LLMs have demonstrated remarkable capability for understanding semantics, but their understanding of pragmatics is not well studied. To this end, we release a Pragmatics Understanding Benchmark (PUB) dataset consisting of fourteen tasks in four pragmatics phenomena, namely; Implicature, Presupposition, Reference, and Deixis. We curate high-quality test sets for each task, consisting of Multiple Choice Question Answers (MCQA). PUB includes a total of 28k data points, 6.1k are newly annotated. We evaluate nine models varying in the number of parameters and type of training. Our study reveals several key observations about the pragmatic capabilities of LLMs: 1. chat-fine-tuning strongly benefits smaller models, 2. large base models are competitive with their chat-fine-tuned counterparts, 3. there is a huge variance in performance across different pragmatics phenomena, and 4. a noticeable performance gap between human capabilities and model capabilities. We hope that PUB will enable comprehensive evaluation of LLM’s pragmatic reasoning capabilities.</abstract>
      <url hash="e9028b54">2024.findings-acl.719</url>
      <bibkey>sravanthi-etal-2024-pub</bibkey>
      <doi>10.18653/v1/2024.findings-acl.719</doi>
    </paper>
    <paper id="720">
      <title><fixed-case>E</fixed-case>mo<fixed-case>T</fixed-case>rans<fixed-case>KG</fixed-case>: An Innovative Emotion Knowledge Graph to Reveal Emotion Transformation</title>
      <author><first>Huan</first><last>Zhao</last><affiliation>Hunan University</affiliation></author>
      <author><first>Xupeng</first><last>Zha</last><affiliation>Hunan University</affiliation></author>
      <author><first>Zixing</first><last>Zhang</last><affiliation>Hunan University</affiliation></author>
      <pages>12098-12110</pages>
      <abstract>This paper introduces EmoTransKG, an innovative Emotion Knowledge Graph (EKG) that establishes connections and transformations between emotions across diverse open-textual events. Compared to existing EKGs, which primarily focus on linking emotion keywords to related terms or on assigning sentiment dimension ratings to emotion words by humans, EmoTransKG aims to represent the general knowledge involved in emotion transformation. Specifically, in conversations, successive emotions expressed by a single speaker are temporally considered as the head and tail entities, with open-text utterances (events) occurring between them representing the relation. To explore the knowledge of emotion transformations described in EmoTransKG, we develop a Transformer-based translational model called EmoTransNet, which predictively trains tail entities by interpreting the relation as an operation that transforms the source emotion into the target emotion. Particularly, our designed EmoTransNet serves as a plug-in module that seamlessly integrates with any conversational emotion recognition (CER) models for emotion retrofitting. Experimental results on two CER datasets demonstrate that the incorporation of EmoTransNet with baseline models results in substantial improvements, and the qualitative visualization of entities and relations clearly clarify their unique roles in emotion transformations. These experiments confirm the quality and effectiveness of EmoTransKG.</abstract>
      <url hash="5367878e">2024.findings-acl.720</url>
      <bibkey>zhao-etal-2024-emotranskg</bibkey>
      <doi>10.18653/v1/2024.findings-acl.720</doi>
    </paper>
    <paper id="721">
      <title>How Vocabulary Sharing Facilitates Multilingualism in <fixed-case>LL</fixed-case>a<fixed-case>MA</fixed-case>?</title>
      <author><first>Fei</first><last>Yuan</last></author>
      <author><first>Shuai</first><last>Yuan</last></author>
      <author><first>Zhiyong</first><last>Wu</last><affiliation>Shanghai Artificial Intelligence Laboratory</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>12111-12130</pages>
      <abstract>Large Language Models (LLMs), often show strong performance on English tasks, while exhibiting limitations on other languages. What is an LLM’s multilingual capability when it is trained only on certain languages? The underlying mechanism remains unclear. This study endeavors to examine the multilingual capability of LLMs from the vocabulary sharing perspective by conducting an exhaustive analysis across 101 languages. Through the investigation of the performance gap before and after embedding fine-tuning, we discovered four distinct quadrants. By delving into each quadrant we provide actionable and efficient guidelines for tuning these languages. Extensive experiments reveal that existing LLMs possess multilingual capabilities that surpass our expectations, and we can significantly improve the multilingual performance of LLMs based on these attributes of each quadrant .</abstract>
      <url hash="7ec326ed">2024.findings-acl.721</url>
      <bibkey>yuan-etal-2024-vocabulary</bibkey>
      <doi>10.18653/v1/2024.findings-acl.721</doi>
    </paper>
    <paper id="722">
      <title>Prefix Text as a Yarn: Eliciting Non-<fixed-case>E</fixed-case>nglish Alignment in Foundation Language Model</title>
      <author><first>Runzhe</first><last>Zhan</last><affiliation>University of Macau</affiliation></author>
      <author><first>Xinyi</first><last>Yang</last></author>
      <author><first>Derek</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Lidia</first><last>Chao</last></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>12131-12145</pages>
      <abstract>While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely “superficial”. We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across seven languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can attain up to 98% of the performance metrics of their SFT counterparts. This method presents a cost-effective alternative to traditional SFT and advances the democratization of multilingual LLMs.</abstract>
      <url hash="b8a7e0d2">2024.findings-acl.722</url>
      <bibkey>zhan-etal-2024-prefix</bibkey>
      <doi>10.18653/v1/2024.findings-acl.722</doi>
    </paper>
    <paper id="723">
      <title>Dual Prompt Tuning based Contrastive Learning for Hierarchical Text Classification</title>
      <author><first>Sishi</first><last>Xiong</last><affiliation>China Telecom</affiliation></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Jie</first><last>Zhang</last></author>
      <author><first>Li</first><last>Mengxiang</last></author>
      <author><first>Zhongjiang</first><last>He</last></author>
      <author><first>Xuelong</first><last>Li</last><affiliation>Northwestern Polytechnical University</affiliation></author>
      <author><first>Shuangyong</first><last>Song</last></author>
      <pages>12146-12158</pages>
      <abstract>Hierarchical text classification aims at categorizing texts into a multi-tiered tree-structured hierarchy of labels. Existing methods pay more attention to capture hierarchy-aware text feature by exploiting explicit parent-child relationships, while interactions between peer labels are rarely taken into account, resulting in severe label confusion within each layer. In this work, we propose a novel Dual Prompt Tuning (DPT) method, which emphasizes identifying discrimination among peer labels by performing contrastive learning on each hierarchical layer. We design an innovative hand-crafted prompt containing slots for both positive and negative label predictions to cooperate with contrastive learning. In addition, we introduce a label hierarchy self-sensing auxiliary task to ensure cross-layer label consistency. Extensive experiments demonstrate that DPT achieves significant improvements and outperforms the current state-of-the-art methods on BGC and RCV1-V2 benchmark datasets.</abstract>
      <url hash="130c8601">2024.findings-acl.723</url>
      <bibkey>xiong-etal-2024-dual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.723</doi>
    </paper>
    <paper id="724">
      <title>Probing the Emergence of Cross-lingual Alignment during <fixed-case>LLM</fixed-case> Training</title>
      <author><first>Hetong</first><last>Wang</last></author>
      <author><first>Pasquale</first><last>Minervini</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <author><first>Edoardo</first><last>Ponti</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>12159-12173</pages>
      <abstract>Multilingual Large Language Models (LLMs) achieve remarkable levels of zero-shot cross-lingual transfer performance. We speculate that this is predicated on their ability to align languages without explicit supervision from parallel sentences. While representations of translationally equivalent sentences in different languages are known to be similar after convergence, however, it remains unclear how such cross-lingual alignment emerges during pre-training of LLMs. Our study leverages intrinsic probing techniques, which identify which subsets of neurons encode linguistic features, to correlate the degree of cross-lingual neuron overlap with the zero-shot cross-lingual transfer performance for a given model. In particular, we rely on checkpoints of BLOOM, a multilingual autoregressive LLM, across different training steps and model scales. We observe a high correlation between neuron overlap and downstream performance, which supports our hypothesis on the conditions leading to effective cross-lingual transfer. Interestingly, we also detect a degradation of both implicit alignment and multilingual abilities in certain phases of the pre-training process, providing new insights into the multilingual pretraining dynamics.</abstract>
      <url hash="844c9332">2024.findings-acl.724</url>
      <bibkey>wang-etal-2024-probing-emergence</bibkey>
      <doi>10.18653/v1/2024.findings-acl.724</doi>
    </paper>
    <paper id="725">
      <title><fixed-case>STSPL</fixed-case>-<fixed-case>SSC</fixed-case>: Semi-Supervised Few-Shot Short Text Clustering with Semantic text similarity Optimized Pseudo-Labels</title>
      <author><first>Wenhua</first><last>Nie</last><affiliation>National Yang Ming Chiao Tung University</affiliation></author>
      <author><first>Lin</first><last>Deng</last></author>
      <author><first>Chang-Bo</first><last>Liu</last></author>
      <author><first>JialingWei</first><last>JialingWei</last></author>
      <author><first>Ruitong</first><last>Han</last></author>
      <author><first>Haoran</first><last>Zheng</last></author>
      <pages>12174-12185</pages>
      <abstract>This study introduces the Semantic Textual Similarity Pseudo-Label Semi-Supervised Clustering (STSPL-SSC) framework. The STSPL-SSC framework is designed to tackle the prevalent issue of scarce labeled data by combining a Semantic Textual Similarity Pseudo-Label Generation process with a Robust Contrastive Learning module. The process begins with employing k-means clustering on embeddings for initial pseudo-Label allocation. Then we use a Semantic Text Similarity-enhanced module to supervise the secondary clustering of pseudo-labels using labeled data to better align with the real clustering centers. Subsequently, an Adaptive Optimal Transport (AOT) approach fine-tunes the pseudo-labels. Finally, a Robust Contrastive Learning module is employed to foster the learning of classification and instance-level distinctions, aiding clusters to better separate. Experiments conducted on multiple real-world datasets demonstrate that with just one label per class, clustering performance can be significantly improved, outperforming state-of-the-art models with an increase of 1-6% in both accuracy and normalized mutual information, approaching the results of fully-labeled classification.</abstract>
      <url hash="5b04c3c1">2024.findings-acl.725</url>
      <bibkey>nie-etal-2024-stspl</bibkey>
      <doi>10.18653/v1/2024.findings-acl.725</doi>
    </paper>
    <paper id="726">
      <title>A Comprehensive Evaluation of Quantization Strategies for Large Language Models</title>
      <author><first>Renren</first><last>Jin</last></author>
      <author><first>Jiangcun</first><last>Du</last></author>
      <author><first>Wuwei</first><last>Huang</last></author>
      <author><first>Wei</first><last>Liu</last><affiliation>xiaomi</affiliation></author>
      <author><first>Jian</first><last>Luan</last></author>
      <author><first>Bin</first><last>Wang</last><affiliation>AI Lab, Xiaomi Inc.</affiliation></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>12186-12215</pages>
      <abstract>Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge &amp; capacity, (2) alignment, and (3) efficiency, and conduct extensive experiments across ten diverse benchmarks. Our experimental results indicate that LLMs with 4-bit quantization can retain performance comparable to their non-quantized counterparts, and perplexity can serve as a proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs with larger parameter scales can outperform smaller LLMs. Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs. Consequently, substantial engineering efforts and hardware support are imperative to achieve a balanced optimization of decoding speed and memory consumption in the context of quantized LLMs.</abstract>
      <url hash="9e524135">2024.findings-acl.726</url>
      <bibkey>jin-etal-2024-comprehensive</bibkey>
      <doi>10.18653/v1/2024.findings-acl.726</doi>
    </paper>
    <paper id="727">
      <title>Exploiting Target Language Data for Neural Machine Translation Beyond Back Translation</title>
      <author><first>Abudurexiti</first><last>Reheman</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Yingfeng</first><last>Luo</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Junhao</first><last>Ruan</last></author>
      <author><first>Chunliang</first><last>Zhang</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Anxiang</first><last>Ma</last><affiliation>Northeastern University</affiliation></author>
      <author><first>Tong</first><last>Xiao</last><affiliation>Northeastern University</affiliation></author>
      <author><first>JingBo</first><last>Zhu</last><affiliation>Northeastern University</affiliation></author>
      <pages>12216-12228</pages>
      <abstract>Neural Machine Translation (NMT) encounters challenges when translating in new domains and low-resource languages. To address these issues, researchers have proposed methods to integrate additional knowledge into NMT, such as translation memories (TMs). However, finding TMs that closely match the input sentence remains challenging, particularly in specific domains. On the other hand, monolingual data is widely accessible in most languages, and back-translation is seen as a promising approach for utilizing target language data. Nevertheless, it still necessitates additional training. In this paper, we introduce Pseudo-<tex-math>k</tex-math>NN-MT, a variant of <tex-math>k</tex-math>-nearest neighbor machine translation (<tex-math>k</tex-math>NN-MT) that utilizes target language data by constructing a pseudo datastore. Furthermore, we investigate the utility of large language models (LLMs) for the <tex-math>k</tex-math>NN component. Experimental results demonstrate that our approach exhibits strong domain adaptation capability in both high-resource and low-resource machine translation. Notably, LLMs are found to be beneficial for robust NMT systems.</abstract>
      <url hash="1cd5569e">2024.findings-acl.727</url>
      <bibkey>reheman-etal-2024-exploiting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.727</doi>
    </paper>
    <paper id="728">
      <title><fixed-case>B</fixed-case>ayesian Prompt Ensembles: Model Uncertainty Estimation for Black-Box Large Language Models</title>
      <author><first>Francesco</first><last>Tonolini</last><affiliation>Amazon</affiliation></author>
      <author><first>Nikolaos</first><last>Aletras</last><affiliation>University of Sheffield, University of Sheffield and Amazon</affiliation></author>
      <author><first>Jordan</first><last>Massiah</last><affiliation>Amazon</affiliation></author>
      <author><first>Gabriella</first><last>Kazai</last><affiliation>Amazon</affiliation></author>
      <pages>12229-12272</pages>
      <abstract>An important requirement for the reliable deployment of pre-trained large language models (LLMs) is the well-calibrated quantification of the uncertainty in their outputs. While the likelihood of predicting the next token is a practical surrogate of the data uncertainty learned during training, model uncertainty is challenging to estimate, i.e., due to lack of knowledge acquired during training. Prior efforts to quantify uncertainty of neural networks require specific architectures or (re-)training strategies, which are impractical to apply to LLMs with several billion parameters, or for black-box models where the architecture and parameters are not available. In this paper, we propose Bayesian Prompts Ensembles (BayesPE), a novel approach to effectively obtain well-calibrated uncertainty for the output of pre-trained LLMs. BayesPE computes output probabilities through a weighted ensemble of different, but semantically equivalent, task instruction prompts. The relative weights of the different prompts in the ensemble are estimated through approximate Bayesian variational inference over a small labeled validation set. We demonstrate that BayesPE approximates a Bayesian input layer for the LLM, providing a lower bound on the expected model error. In our extensive experiments, we show that BayesPE achieves significantly superior uncertainty calibration compared to several baselines over a range of natural language classification tasks, both in zero- and few-shot settings.</abstract>
      <url hash="9434ec2f">2024.findings-acl.728</url>
      <bibkey>tonolini-etal-2024-bayesian</bibkey>
      <doi>10.18653/v1/2024.findings-acl.728</doi>
    </paper>
    <paper id="729">
      <title><fixed-case>X</fixed-case>-<fixed-case>ACE</fixed-case>: Explainable and Multi-factor Audio Captioning Evaluation</title>
      <author><first>Qian</first><last>Wang</last></author>
      <author><first>Jia-Chen</first><last>Gu</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Zhen-Hua</first><last>Ling</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>12273-12287</pages>
      <abstract>Automated audio captioning (AAC) aims to generate descriptions based on audio input, attracting exploration of emerging audio language models (ALMs). However, current evaluation metrics only provide a single score to assess the overall quality of captions without characterizing the nuanced difference by systematically going through an evaluation checklist. To this end, we propose the explainable and multi-factor audio captioning evaluation (X-ACE) paradigm. X-ACE identifies four main factors that constitute the majority of audio features, specifically sound event, source, attribute and relation. To assess a given caption from an ALM, it is firstly transformed into an audio graph, where each node denotes an entity in the caption and corresponds to a factor. On the one hand, graph matching is conducted from part to whole for a holistic assessment. On the other hand, the nodes contained within each factor are aggregated to measure the factor-level performance. The pros and cons of an ALM can be explicitly and clearly demonstrated through X-ACE, pointing out the direction for further improvements. Experiments show that X-ACE exhibits better correlation with human perception and can detect mismatches sensitively.</abstract>
      <url hash="bbe2e5f5">2024.findings-acl.729</url>
      <bibkey>wang-etal-2024-x</bibkey>
      <doi>10.18653/v1/2024.findings-acl.729</doi>
    </paper>
    <paper id="730">
      <title>Reasons to Reject? Aligning Language Models with Judgments</title>
      <author><first>Weiwen</first><last>Xu</last></author>
      <author><first>Deng</first><last>Cai</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Zhisong</first><last>Zhang</last><affiliation>Tencent</affiliation></author>
      <author><first>Wai</first><last>Lam</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Shuming</first><last>Shi</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>12288-12304</pages>
      <abstract>As humans, we consistently interact with our peers and receive feedback in the form of natural language. This language feedback allows us to maintain appropriate behavior, and rectify potential errors. The question arises naturally: can we use language feedback to align large language models (LLMs)? In contrast to previous research that aligns LLMs with scalar rewards, we present the first systematic exploration of alignment through the lens of language feedback (i.e., judgment). We start with an in-depth investigation of potential methods that can be adapted for aligning LLMs with judgments, revealing that these methods cannot fully capitalize on judgments. To facilitate more effective utilization of judgments, we propose a novel framework, Contrastive Unlikelihood Training (CUT), that allows for fine-grained inappropriate content detection and correction based on judgments. Our results show that, with merely 1317 off-the-shelf judgment data, CUT can beat the 175B DaVinci003 and surpass the best baseline by 50.84 points on AlpacaEval using LLaMA2-13b. CUT can also align LLMs in an iterative fashion using up-to-date model-specific judgments, improving performance from 81.09 to 91.68 points on AlpacaEval using LLaMA2-chat-13b. Further analysis suggests that judgments hold greater potential in LLM alignment than rewards.</abstract>
      <url hash="f479ddfd">2024.findings-acl.730</url>
      <bibkey>xu-etal-2024-reasons</bibkey>
      <doi>10.18653/v1/2024.findings-acl.730</doi>
    </paper>
    <paper id="731">
      <title>Decomposing Argumentative Essay Generation via Dialectical Planning of Complex Reasoning</title>
      <author><first>Yuhang</first><last>He</last></author>
      <author><first>Jianzhu</first><last>Bao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yang</first><last>Sun</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>12305-12322</pages>
      <abstract>Argumentative Essay Generation (AEG) is a challenging task in computational argumentation, where detailed logical reasoning and effective rhetorical skills are essential.Previous methods on argument generation typically involve planning prior to generation.However, the planning strategies in these methods overlook the exploration of the logical reasoning process.Inspired by argument structure-related theories, we propose an argumentative planning strategy for prompting large language models (LLMs) to generate high-quality essays.This strategy comprises two stages: (1) Sketch planning, which creates a rough outline of the essay, and (2) Dialectical planning, which refines the outline through critical self-reflection.Such a planning strategy enables LLMs to write argumentative essays that are more logical, diverse, and persuasive.Furthermore, due to the scarcity of existing AEG datasets, we construct three new datasets.These datasets are from two domains: exam essays and news editorials, covering both Chinese and English.Automatic and manual evaluation on four datasets show that our method can generate more dialectical and persuasive essays with higher diversity compared to several strong baselines.</abstract>
      <url hash="370de7b8">2024.findings-acl.731</url>
      <bibkey>he-etal-2024-decomposing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.731</doi>
    </paper>
    <paper id="732">
      <title>Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition</title>
      <author><first>Tariq</first><last>Alhindi</last></author>
      <author><first>Smaranda</first><last>Muresan</last><affiliation>Amazon and Columbia University</affiliation></author>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <pages>12323-12334</pages>
      <abstract>Recognizing fallacies is crucial for ensuring the quality and validity of arguments across various domains. However, computational fallacy recognition faces challenges due to the diverse genres, domains, and types of fallacies found in datasets. This leads to a highly multi-class, and even multi-label, setup with substantial class imbalance. In this study, we aim to enhance existing models for fallacy recognition by incorporating additional context and by leveraging large language models to generate synthetic data, thus increasing the representation of the infrequent classes. We experiment with GPT3.5 to generate synthetic examples and we examine the impact of prompt settings for this. Moreover, we explore zero-shot and few-shot scenarios to evaluate the effectiveness of using the generated examples for training smaller models within a unified fallacy recognition framework. Furthermore, we analyze the overlap between the synthetic data and existing fallacy datasets. Finally, we investigate the usefulness of providing supplementary context for detecting fallacy types that need such context, e.g., diversion fallacies. Our evaluation results demonstrate consistent improvements across fallacy types, datasets, and generators. The code and the synthetic datasets are all publicly available.</abstract>
      <url hash="19175f8c">2024.findings-acl.732</url>
      <bibkey>alhindi-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.732</doi>
    </paper>
    <paper id="733">
      <title>Concept-aware Data Construction Improves In-context Learning of Language Models</title>
      <author><first>Michal</first><last>Štefánik</last></author>
      <author><first>Marek</first><last>Kadlčík</last><affiliation>Masaryk University</affiliation></author>
      <author><first>Petr</first><last>Sojka</last><affiliation>Faculty of Informatics, Masaryk University</affiliation></author>
      <pages>12335-12352</pages>
      <abstract>Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs’ ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in small-scale, synthetic settings.In this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using CoAT, pre-trained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functional deficiencies of the previous models. Finally, we show that concept-aware in-context learners are much more effective in in-context learning a majority of unseen tasks compared to traditional instruction tuning, and fare comparably also to previous in-context learners trained in large-scale multitask learning requiring magnitudes of more training data.</abstract>
      <url hash="4a078248">2024.findings-acl.733</url>
      <bibkey>stefanik-etal-2024-concept</bibkey>
      <doi>10.18653/v1/2024.findings-acl.733</doi>
    </paper>
    <paper id="734">
      <title>Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal Theory for Post-Purchase Intention Analysis</title>
      <author><first>Gerard</first><last>Yeo</last></author>
      <author><first>Shaz</first><last>Furniturewala</last></author>
      <author><first>Kokil</first><last>Jaidka</last><affiliation>National University of Singapore</affiliation></author>
      <pages>12353-12360</pages>
      <abstract>Supervised machine-learning models for predicting user behavior offer a challenging classification problem with lower average prediction performance scores than other text classification tasks. This study evaluates multi-task learning frameworks grounded in Cognitive Appraisal Theory to predict user behavior as a function of users’ self-expression and psychological attributes. Our experiments show that users’ language and traits improve predictions above and beyond models predicting only from text. Our findings highlight the importance of integrating psychological constructs into NLP to enhance the understanding and prediction of user actions. We close with a discussion of the implications for future applications of large language models for computational psychology.</abstract>
      <url hash="00e97e80">2024.findings-acl.734</url>
      <bibkey>yeo-etal-2024-beyond</bibkey>
      <doi>10.18653/v1/2024.findings-acl.734</doi>
    </paper>
    <paper id="735">
      <title>Non-Autoregressive Machine Translation as Constrained <fixed-case>HMM</fixed-case></title>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Zhanming</first><last>Jie</last><affiliation>Salesforce Research</affiliation></author>
      <author><first>Wei</first><last>Lu</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>12361-12372</pages>
      <abstract>In non-autoregressive translation (NAT), directed acyclic Transformers (DAT) have demonstrated their ability to achieve comparable performance to the autoregressive Transformers.In this paper, we first show that DAT is essentially a fully connected left-to-right Hidden Markov Model (HMM), with the source and target sequences being observations and the token positions being latent states.Even though generative models like HMM do not suffer from label bias in traditional task settings (e.g., sequence labeling), we argue here that the left-to-right HMM in NAT may still encounter this issue due to the missing observations at the inference stage.To combat label bias, we propose two constrained HMMs: 1) Adaptive Window HMM, which explicitly balances the number of outgoing transitions at different states; 2) Bi-directional HMM, i.e., a combination of left-to-right and right-to-left HMMs, whose uni-directional components can implicitly regularize each other’s biases via shared parameters.Experimental results on WMT’14 EnDe and WMT’17 ZhEn demonstrate that our methods can achieve better or comparable performance to the original DAT using various decoding methods.We also demonstrate that our methods effectively reduce the impact of label bias.</abstract>
      <url hash="958d038a">2024.findings-acl.735</url>
      <bibkey>li-etal-2024-non</bibkey>
      <doi>10.18653/v1/2024.findings-acl.735</doi>
    </paper>
    <paper id="736">
      <title>Multi-modal Stance Detection: New Datasets and Model</title>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Ang</first><last>Li</last></author>
      <author><first>Jingqian</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Lin</first><last>Gui</last><affiliation>King’s College London, University of London</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Yue</first><last>Yu</last><affiliation>National University of Defense Technology and PengCheng Lab</affiliation></author>
      <author><first>Kam-Fai</first><last>Wong</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>12373-12387</pages>
      <abstract>Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today’s fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our five benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.</abstract>
      <url hash="dacb7779">2024.findings-acl.736</url>
      <bibkey>liang-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.736</doi>
    </paper>
    <paper id="737">
      <title>Enhanced Language Model Truthfulness with Learnable Intervention and Uncertainty Expression</title>
      <author><first>Farima</first><last>Fatahi Bayat</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Xin</first><last>Liu</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>H.</first><last>Jagadish</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>12388-12400</pages>
      <abstract>Large language models (LLMs) can generate long-form and coherent text, yet they often hallucinate facts, which undermines their reliability. To mitigate this issue, inference-time methods steer LLM representations toward the “truthful directions” previously learned for truth elicitation. However, applying these truthful directions with the same intensity fails to generalize across different query contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to each specific context. LITO explores a sequence of model generations based on increasing levels of intervention intensities. It selects the most accurate response or refuses to answer when the predictions are highly uncertain. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters the limitations of one-size-fits-all intervention methods, maximizing truthfulness by reflecting the model’s internal knowledge only when it is confident. Our code is available at https://github.com/launchnlp/LITO.</abstract>
      <url hash="a8f33041">2024.findings-acl.737</url>
      <bibkey>fatahi-bayat-etal-2024-enhanced</bibkey>
      <doi>10.18653/v1/2024.findings-acl.737</doi>
    </paper>
    <paper id="738">
      <title><fixed-case>MM</fixed-case>-<fixed-case>LLM</fixed-case>s: Recent Advances in <fixed-case>M</fixed-case>ulti<fixed-case>M</fixed-case>odal Large Language Models</title>
      <author><first>Duzhen</first><last>Zhang</last></author>
      <author><first>Yahan</first><last>Yu</last><affiliation>Kyoto University, Kyoto University</affiliation></author>
      <author><first>Jiahua</first><last>Dong</last></author>
      <author><first>Chenxing</first><last>Li</last></author>
      <author><first>Dan</first><last>Su</last></author>
      <author><first>Chenhui</first><last>Chu</last><affiliation>Kyoto University</affiliation></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>12401-12430</pages>
      <abstract>In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a [real-time tracking website](https://mm-llms.github.io/) for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.</abstract>
      <url hash="3af3c9e8">2024.findings-acl.738</url>
      <bibkey>zhang-etal-2024-mm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.738</doi>
    </paper>
    <paper id="739">
      <title><fixed-case>CIF</fixed-case>-Bench: A <fixed-case>C</fixed-case>hinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models</title>
      <author><first>Yizhi</first><last>Li</last><affiliation>University of Manchester and University of Sheffield</affiliation></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Xingwei</first><last>Qu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Jiali</first><last>Li</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Zhaoqun</first><last>Li</last></author>
      <author><first>Noah</first><last>Wang</last></author>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Ruibin</first><last>Yuan</last></author>
      <author><first>Yinghao</first><last>Ma</last><affiliation>Queen Mary University of London</affiliation></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Wangchunshu</first><last>Zhou</last><affiliation>AIWaves Inc.</affiliation></author>
      <author><first>Yiming</first><last>Liang</last></author>
      <author><first>Lei</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Ma</last><affiliation>Peking University and Beijing Academy of Artifical Intelligence</affiliation></author>
      <author><first>Jiajun</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Zuowen</first><last>Li</last><affiliation>Beijing Foreign Studies University</affiliation></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <pages>12431-12446</pages>
      <abstract>The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following.Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (**CIF-Bench**), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate data contamination, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances.Our evaluation of 28 selected LLMs reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of LLMs in less familiar language and task contexts.This work not only uncovers the current limitations of LLMs in handling Chinese language tasks but also sets a new standard for future LLM generalizability research, pushing towards the development of more adaptable, culturally informed, and linguistically diverse models.</abstract>
      <url hash="a529c416">2024.findings-acl.739</url>
      <bibkey>li-etal-2024-cif</bibkey>
      <doi>10.18653/v1/2024.findings-acl.739</doi>
    </paper>
    <paper id="740">
      <title>Countering Reward Over-Optimization in <fixed-case>LLM</fixed-case> with Demonstration-Guided Reinforcement Learning</title>
      <author><first>Mathieu</first><last>Rita</last></author>
      <author><first>Florian</first><last>Strub</last><affiliation>DeepMind</affiliation></author>
      <author><first>Rahma</first><last>Chaabouni</last><affiliation>Google</affiliation></author>
      <author><first>Paul</first><last>Michel</last><affiliation>DeepMind</affiliation></author>
      <author><first>Emmanuel</first><last>Dupoux</last><affiliation>EHESS</affiliation></author>
      <author><first>Olivier</first><last>Pietquin</last><affiliation>Cohere and Earth Species Project</affiliation></author>
      <pages>12447-12472</pages>
      <abstract>While reinforcement learning (RL) has been proven essential for tuning large language models (LLMs), it can lead to reward over-optimization (ROO). Existing approaches address ROO by adding KL regularization, requiring computationally expensive hyperparameter tuning. Additionally, KL regularization focuses solely on regularizing the language policy, neglecting a potential source of regularization: the reward function itself. Inspired by demonstration-guided RL, we here introduce the Reward Calibration from Demonstration (RCfD), which leverages human demonstrations and a reward model to recalibrate the reward objective. Formally, given a prompt, the RCfD objective minimizes the distance between the demonstrations’ and LLM’s rewards rather than directly maximizing the reward function. This objective shift avoids incentivizing the LLM to exploit the reward model and promotes more natural and diverse language generation.We show the effectiveness of RCfD in three RL language tasks, where it achieves comparable performance to carefully tuned baselines while mitigating ROO.</abstract>
      <url hash="76430be8">2024.findings-acl.740</url>
      <bibkey>rita-etal-2024-countering</bibkey>
      <doi>10.18653/v1/2024.findings-acl.740</doi>
    </paper>
    <paper id="741">
      <title>Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss</title>
      <author><first>Wei</first><last>He</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Marco</first><last>Idiart</last><affiliation>Universidade Federal do Rio Grande do Sul</affiliation></author>
      <author><first>Carolina</first><last>Scarton</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Aline</first><last>Villavicencio</last><affiliation>University of Exeter and University of Sheffield</affiliation></author>
      <pages>12473-12485</pages>
      <abstract>Accurately modeling idiomatic or non-compositional language has been a longstanding challenge in Natural Language Processing (NLP). This is partly because these expressions do not derive their meanings solely from their constituent words, but also due to the scarcity of relevant data resources, and their impact on the performance of downstream tasks such as machine translation and simplification. In this paper we propose an approach to model idiomaticity effectively using a triplet loss that incorporates the asymmetric contribution of components words to an idiomatic meaning for training language models by using adaptive contrastive learning and resampling miners to build an idiomatic-aware learning objective. Our proposed method is evaluated on a SemEval challenge and outperforms previous alternatives significantly in many metrics.</abstract>
      <url hash="481d7fc9">2024.findings-acl.741</url>
      <bibkey>he-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.741</doi>
    </paper>
    <paper id="742">
      <title><fixed-case>A</fixed-case>da<fixed-case>L</fixed-case>omo: Low-memory Optimization with Adaptive Learning Rate</title>
      <author><first>Kai</first><last>Lv</last></author>
      <author><first>Hang</first><last>Yan</last><affiliation>AI lab</affiliation></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Haijun</first><last>Lv</last></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>12486-12502</pages>
      <abstract>Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter and exhibits superior convergence performance compared to LOMO theoretically. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.</abstract>
      <url hash="b0fd406f">2024.findings-acl.742</url>
      <bibkey>lv-etal-2024-adalomo</bibkey>
      <doi>10.18653/v1/2024.findings-acl.742</doi>
    </paper>
    <paper id="743">
      <title>Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks</title>
      <author><first>Wenyue</first><last>Hua</last><affiliation>Rutgers University, New Brunswick</affiliation></author>
      <author><first>Jiang</first><last>Guo</last></author>
      <author><first>Mingwen</first><last>Dong</last></author>
      <author><first>Henghui</first><last>Zhu</last><affiliation>Amazon</affiliation></author>
      <author><first>Patrick</first><last>Ng</last><affiliation>Amazon</affiliation></author>
      <author><first>Zhiguo</first><last>Wang</last></author>
      <pages>12503-12525</pages>
      <abstract>Current knowledge editing approaches struggle to effectively propagate updates to interconnected facts.In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark, ReCoE (Reasoning-based Counterfactual Editing dataset), which covers six common reasoning schemes in the real world. We conduct an extensive analysis of existing knowledge editing techniques, including input-augmentation, finetuning, and locate-and-edit methods. We found that all model editing methods exhibit notably low performance on this dataset, especially within certain reasoning schemes. Our analysis of the chain-of-thought responses from edited models indicate that, while the models effectively update individual facts, they struggle to recall these facts in reasoning tasks. Moreover, locate-and-edit methods severely deteriorate the models’ language modeling capabilities, leading to poor perplexity and logical coherence in their outputs.</abstract>
      <url hash="582724d5">2024.findings-acl.743</url>
      <bibkey>hua-etal-2024-propagation</bibkey>
      <doi>10.18653/v1/2024.findings-acl.743</doi>
    </paper>
    <paper id="744">
      <title>Exciting Mood Changes: A Time-aware Hierarchical Transformer for Change Detection Modelling</title>
      <author><first>Anthony</first><last>Hills</last></author>
      <author><first>Talia</first><last>Tseriotou</last></author>
      <author><first>Xenia</first><last>Miscouridou</last><affiliation>University of Cyprus and Imperial College London</affiliation></author>
      <author><first>Adam</first><last>Tsakalidis</last><affiliation>Cedefop and Alan Turing Institute</affiliation></author>
      <author><first>Maria</first><last>Liakata</last><affiliation>Queen Mary University London</affiliation></author>
      <pages>12526-12537</pages>
      <abstract>Through the rise of social media platforms, longitudinal language modelling has received much attention over the latest years, especially in downstream tasks such as mental health monitoring of individuals where modelling linguistic content in a temporal fashion is crucial. A key limitation in existing work is how to effectively model temporal sequences within Transformer-based language models. In this work we address this challenge by introducing a novel approach for predicting ‘Moments of Change’ (MoC) in the mood of online users, by simultaneously considering user linguistic and time-aware context. A Hawkes process-inspired transformation layer is applied over the proposed architecture to model the influence of time on users’ posts – capturing both their immediate and historical dynamics. We perform experiments on the two existing datasets for the MoC task and showcase clear performance gains when leveraging the proposed layer. Our ablation study reveals the importance of considering temporal dynamics in detecting subtle and rare mood changes. Our results indicate that considering linguistic and temporal information in a hierarchical manner provide valuable insights into the temporal dynamics of modelling user generated content over time, with applications in mental health monitoring.</abstract>
      <url hash="ea36002d">2024.findings-acl.744</url>
      <bibkey>hills-etal-2024-exciting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.744</doi>
    </paper>
    <paper id="745">
      <title><fixed-case>C</fixed-case>or<fixed-case>N</fixed-case>av: Autonomous Agent with Self-Corrected Planning for Zero-Shot Vision-and-Language Navigation</title>
      <author><first>Xiwen</first><last>Liang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Liang</first><last>Ma</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Shanshan</first><last>Guo</last></author>
      <author><first>Jianhua</first><last>Han</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Hang</first><last>Xu</last><affiliation>Huawei Noah‘s Ark Lab</affiliation></author>
      <author><first>Shikui</first><last>Ma</last><affiliation>Dataa Robotics</affiliation></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <pages>12538-12559</pages>
      <abstract>Understanding and following natural language instructions while navigating through complex, real-world environments poses a significant challenge for general-purpose robots. These environments often include obstacles and pedestrians, making it essential for autonomous agents to possess the capability of self-corrected planning to adjust their actions based on feedback from the surroundings. However, the majority of existing vision-and-language navigation (VLN) methods primarily operate in less realistic simulator settings and do not incorporate environmental feedback into their decision-making processes. To address this gap, we introduce a novel zero-shot framework called CorNav, utilizing a large language model for decision-making and comprising two key components: 1) incorporating environmental feedback for refining future plans and adjusting its actions, and 2) multiple domain experts for parsing instructions, scene understanding, and refining predicted actions. In addition to the framework, we develop a 3D simulator that renders realistic scenarios using Unreal Engine 5. To evaluate the effectiveness and generalization of navigation agents in a zero-shot multi-task setting, we create a benchmark called NavBench. Our empirical study involves deploying 7 baselines across four tasks, i.e., goal-conditioned navigation given a specific object category, goal-conditioned navigation given simple instructions, finding abstract objects based on high-level instructions, and step-by-step instruction following. Extensive experiments demonstrate that CorNav consistently outperforms all baselines by a significant margin across all tasks. On average, CorNav achieves a success rate of 28.1%, surpassing the best baseline’s performance of 20.5%.</abstract>
      <url hash="8675f6b1">2024.findings-acl.745</url>
      <bibkey>liang-etal-2024-cornav</bibkey>
      <doi>10.18653/v1/2024.findings-acl.745</doi>
    </paper>
    <paper id="746">
      <title><fixed-case>S</fixed-case>ci<fixed-case>MMIR</fixed-case>: Benchmarking Scientific Multi-modal Information Retrieval</title>
      <author><first>Siwei</first><last>Wu</last><affiliation>Nanjing University of Science and Technology</affiliation></author>
      <author><first>Yizhi</first><last>Li</last><affiliation>University of Manchester and University of Sheffield</affiliation></author>
      <author><first>Kang</first><last>Zhu</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Yiming</first><last>Liang</last></author>
      <author><first>Kaijing</first><last>Ma</last></author>
      <author><first>Chenghao</first><last>Xiao</last></author>
      <author><first>Haoran</first><last>Zhang</last></author>
      <author><first>Bohao</first><last>Yang</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo and Google</affiliation></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Noura</first><last>Al Moubayed</last><affiliation>Durham University</affiliation></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <pages>12560-12574</pages>
      <abstract>Multi-modal information retrieval (MMIR) is a rapidly evolving field where significant progress has been made through advanced representation learning and cross-modality alignment research, particularly in image-text pairing.However, current benchmarks for evaluating MMIR performance on image-text pairings overlook the scientific domain, which has a notable gap with the generic data since the caption of scientific charts and tables usually describes the analysis of experimental results or scientific principles in contrast to human activity or scenery depicted in generic images.To bridge this gap, we develop a <b>sci</b>entific domain-specific <b>MMIR</b> benchmark (<b>SciMMIR</b>) by leveraging open-access research paper corpora to extract data relevant to the scientific domain. This benchmark comprises <b>530K</b> meticulously curated image-text pairs, extracted from figures and tables with detailed captions from scientific documents.We further annotate the image-text pairs with a two-level subset-subcategory hierarchy to facilitate a more comprehensive evaluation of the baselines. We conduct zero-shot and fine-tuned evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP, BLIP, and BLIP-2.Our findings offer critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the effects of different visual and textual encoders.</abstract>
      <url hash="62304cfd">2024.findings-acl.746</url>
      <bibkey>wu-etal-2024-scimmir</bibkey>
      <doi>10.18653/v1/2024.findings-acl.746</doi>
    </paper>
    <paper id="747">
      <title>Diving Deep into the Motion Representation of Video-Text Models</title>
      <author><first>Chinmaya</first><last>Devaraj</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Cornelia</first><last>Fermuller</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Yiannis</first><last>Aloimonos</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>12575-12584</pages>
      <abstract>Videos are more informative than images becausethey capture the dynamics of the scene.By representing motion in videos, we can capturedynamic activities. In this work, we introduceGPT-4 generated motion descriptions thatcapture fine-grained motion descriptions of activitiesand apply them to three action datasets.We evaluated several video-text models on thetask of retrieval of motion descriptions. Wefound that they fall far behind human expertperformance on two action datasets, raisingthe question of whether video-text models understandmotion in videos. To address it, weintroduce a method of improving motion understandingin video-text models by utilizingmotion descriptions. This method proves tobe effective on two action datasets for the motiondescription retrieval task. The results drawattention to the need for quality captions involvingfine-grained motion information in existingdatasets and demonstrate the effectiveness ofthe proposed pipeline in understanding finegrainedmotion during video-text retrieval.</abstract>
      <url hash="386a97b2">2024.findings-acl.747</url>
      <bibkey>devaraj-etal-2024-diving</bibkey>
      <doi>10.18653/v1/2024.findings-acl.747</doi>
    </paper>
    <paper id="748">
      <title>Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation</title>
      <author><first>Nihal</first><last>Nayak</last><affiliation>Brown University</affiliation></author>
      <author><first>Yiyang</first><last>Nan</last></author>
      <author><first>Avi</first><last>Trost</last></author>
      <author><first>Stephen</first><last>Bach</last><affiliation>Computer Science Department, Brown University and Snorkel AI</affiliation></author>
      <pages>12585-12611</pages>
      <abstract>We introduce Bonito, an open-source model for conditional task generation that converts unannotated text into task-specific training datasets for instruction tuning. We aim to enable zero-shot task adaptation of large language models on users’ specialized, private data. We train Bonito by fine-tuning a pretrained large language model on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains with unannotated text across three task types—yes-no question answering, extractive question answering, and natural language inference—and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at https://github.com/BatsResearch/bonito.</abstract>
      <url hash="ebdd4f68">2024.findings-acl.748</url>
      <bibkey>nayak-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.748</doi>
    </paper>
    <paper id="749">
      <title>Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning</title>
      <author><first>Anirudh</first><last>Som</last><affiliation>SRI International</affiliation></author>
      <author><first>Karan</first><last>Sikka</last><affiliation>SRI International</affiliation></author>
      <author><first>Helen</first><last>Gent</last><affiliation>SRI International</affiliation></author>
      <author><first>Ajay</first><last>Divakaran</last><affiliation>SRI International</affiliation></author>
      <author><first>Andreas</first><last>Kathol</last></author>
      <author><first>Dimitra</first><last>Vergyri</last></author>
      <pages>12612-12627</pages>
      <abstract>Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also often retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as - number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase (CAPP) dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. We evaluate our approach using four closed source and one open source LLM. Our results reveal that ICL is comparable to supervised methods in generation quality, while being qualitatively better by 25% on human evaluation and attaining lower toxicity by 76%. Also, ICL-based paraphrasers only show a slight reduction in performance even with just 10% training data.</abstract>
      <url hash="ceb57ad5">2024.findings-acl.749</url>
      <bibkey>som-etal-2024-demonstrations</bibkey>
      <doi>10.18653/v1/2024.findings-acl.749</doi>
    </paper>
    <paper id="750">
      <title>Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse</title>
      <author><first>Khiem</first><last>Phi</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Noushin</first><last>Salek Faramarzi</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Chenlu</first><last>Wang</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Ritwik</first><last>Banerjee</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <pages>12628-12643</pages>
      <abstract>Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter/X and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the ‘what about’ lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.</abstract>
      <url hash="cd044116">2024.findings-acl.750</url>
      <bibkey>phi-etal-2024-paying</bibkey>
      <doi>10.18653/v1/2024.findings-acl.750</doi>
    </paper>
    <paper id="751">
      <title>Epistemology of Language Models: Do Language Models Have Holistic Knowledge?</title>
      <author><first>Minsu</first><last>Kim</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>James</first><last>Thorne</last><affiliation>KAIST</affiliation></author>
      <pages>12644-12669</pages>
      <abstract>This paper investigates the inherent knowledge in language models from the perspective of epistemological holism. The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism. These characteristics suggest that core knowledge, such as commonsense, general, and specific knowledge, each plays a specific role, serving as the foundation of our knowledge system and being difficult to revise. To assess these traits related to holism, we created a scientific reasoning dataset and examined the epistemology of language models through three tasks: Abduction, Revision, and Argument Generation. In the abduction task, the language models explained situations while avoiding revising the core knowledge. However, in other tasks, the language models were revealed not to distinguish between core and peripheral knowledge, showing an incomplete alignment with holistic knowledge principles.</abstract>
      <url hash="fad9bec1">2024.findings-acl.751</url>
      <bibkey>kim-thorne-2024-epistemology</bibkey>
      <doi>10.18653/v1/2024.findings-acl.751</doi>
    </paper>
    <paper id="752">
      <title>Strong hallucinations from negation and how to fix them</title>
      <author><first>Swarnadeep</first><last>Bhar</last></author>
      <author><first>Nicholas</first><last>Asher</last><affiliation>CNRS</affiliation></author>
      <pages>12670-12687</pages>
      <abstract>Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses strong hallucinations and prove that they follow from an LM’s computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as an operation over an LM’s latent representations that constrains how they may evolve. We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data.</abstract>
      <url hash="852c171a">2024.findings-acl.752</url>
      <bibkey>bhar-asher-2024-strong</bibkey>
      <doi>10.18653/v1/2024.findings-acl.752</doi>
    </paper>
    <paper id="753">
      <title><fixed-case>LLM</fixed-case>s as Narcissistic Evaluators: When Ego Inflates Evaluation Scores</title>
      <author><first>Yiqi</first><last>Liu</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Nafise</first><last>Moosavi</last><affiliation>University of Sheffield</affiliation></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <pages>12688-12701</pages>
      <abstract>Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in a reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more reliable evaluation protocols in the future.</abstract>
      <url hash="02474c30">2024.findings-acl.753</url>
      <bibkey>liu-etal-2024-llms-narcissistic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.753</doi>
    </paper>
    <paper id="754">
      <title><fixed-case>H</fixed-case>ello<fixed-case>F</fixed-case>resh: <fixed-case>LLM</fixed-case> Evalutions on Streams of Real-World Human Editorial Actions across <fixed-case>X</fixed-case> Community Notes and <fixed-case>W</fixed-case>ikipedia edits</title>
      <author><first>Tim</first><last>Franzmeyer</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Aleksandar</first><last>Shtedritski</last></author>
      <author><first>Samuel</first><last>Albanie</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Philip</first><last>Torr</last><affiliation>University of Oxford</affiliation></author>
      <author><first>Joao F.</first><last>Henriques</last></author>
      <author><first>Jakob</first><last>Foerster</last><affiliation>University of Oxford, University of Oxford</affiliation></author>
      <pages>12702-12716</pages>
      <abstract>Benchmarks have been essential for driving progress in machine learning. A better understanding of LLM capabilities on real world tasks is vital for safe development.Designing adequate LLM benchmarks is challenging: Data from real-world tasks is hard to collect, public availability of static evaluation data results in test data contamination and benchmark overfitting, and periodically generating new evaluation data is tedious and may result in temporally inconsistent results. We introduce HelloFresh, based on continuous streams of real-world data generated by intrinsically motivated human labelers. It covers recent events from X (formerly Twitter) community notes and edits of Wikipedia pages, mitigating the risk of test data contamination and benchmark overfitting.Any X user can propose an X note to add additional context to a misleading post (formerly tweet); if the community classifies it as helpful, it is shown with the post. Similarly, Wikipedia relies on community-based consensus, allowing users to edit articles or revert edits made by other users.Verifying whether an X note is helpful or whether a Wikipedia edit should be accepted are hard tasks that require grounding by querying the web.We backtest state-of-the-art LLMs supplemented with simple web search access and find that HelloFresh yields a temporally consistent ranking.To enable continuous evaluation on Hellofresh, we host a public leaderboard and periodically updated evaluation data at https://tinyurl.com/hello-fresh-LLM.</abstract>
      <url hash="998b0020">2024.findings-acl.754</url>
      <bibkey>franzmeyer-etal-2024-hellofresh</bibkey>
      <doi>10.18653/v1/2024.findings-acl.754</doi>
    </paper>
    <paper id="755">
      <title>Chaos with Keywords: Exposing Large Language Models Sycophancy to Misleading Keywords and Evaluating Defense Strategies</title>
      <author><first>Aswin</first><last>Rrv</last></author>
      <author><first>Nemika</first><last>Tyagi</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Md Nayem</first><last>Uddin</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>12717-12733</pages>
      <abstract>This study explores the sycophantic tendencies of Large Language Models (LLMs), where these models tend to provide answers that match what users want to hear, even if they are not entirely correct. The motivation behind this exploration stems from the common behavior observed in individuals searching the internet for facts with partial or misleading knowledge. Similar to using web search engines, users may recall fragments of misleading keywords and submit them to an LLM, hoping for a comprehensive response. Our empirical analysis of several LLMs shows the potential danger of these models amplifying misinformation when presented with misleading keywords. Additionally, we thoroughly assess four existing hallucination mitigation strategies to reduce LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of these strategies for generating factually correct statements. Furthermore, our analyses delve into knowledge-probing experiments on factual keywords and different categories of sycophancy mitigation.</abstract>
      <url hash="baab0a86">2024.findings-acl.755</url>
      <bibkey>rrv-etal-2024-chaos</bibkey>
      <doi>10.18653/v1/2024.findings-acl.755</doi>
    </paper>
    <paper id="756">
      <title>Empowering Large Language Models for Textual Data Augmentation</title>
      <author><first>Yichuan</first><last>Li</last></author>
      <author><first>Kaize</first><last>Ding</last><affiliation>Northwestern University and Arizona State University</affiliation></author>
      <author><first>Jianling</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Kyumin</first><last>Lee</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <pages>12734-12751</pages>
      <abstract>With the capabilities of understanding and executing natural language instructions, Large language models (LLMs) can potentially act as a powerful tool for textual data augmentation. However, the quality of augmented data depends heavily on the augmentation instructions provided, and the effectiveness can fluctuate across different downstream tasks. While manually crafting and selecting instructions can offer some improvement, this approach faces scalability and consistency issues in practice due to the diversity of downstream tasks. In this work, we address these limitations by proposing a new solution, which can automatically generate a large pool of augmentation instructions and select the most suitable task-informed instructions, thereby empowering LLMs to create high-quality augmented data for different downstream tasks. Empirically, the proposed approach consistently generates augmented data with better quality compared to non-LLM and LLM-based data augmentation methods, leading to the best performance on 26 few-shot learning tasks sourced from a wide range of application domains.</abstract>
      <url hash="3c38496e">2024.findings-acl.756</url>
      <bibkey>li-etal-2024-empowering</bibkey>
      <doi>10.18653/v1/2024.findings-acl.756</doi>
    </paper>
    <paper id="757">
      <title>Choose Your Transformer: Improved Transferability Estimation of Transformer Models on Classification Tasks</title>
      <author><first>Lukas</first><last>Garbas</last></author>
      <author><first>Max</first><last>Ploner</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <author><first>Alan</first><last>Akbik</last><affiliation>Humboldt Universität Berlin</affiliation></author>
      <pages>12752-12768</pages>
      <abstract>There currently exists a multitude of pre-trained transformer language models (LMs) that are readily available. From a practical perspective, this raises the question of which pre-trained LM will perform best if fine-tuned for a specific downstream NLP task. However, exhaustively fine-tuning all available LMs to determine the best-fitting model is computationally infeasible. To address this problem, we present an approach that inexpensively estimates a ranking of the expected performance of a given set of candidate LMs for a given task. Following a layer-wise representation analysis, we extend existing approaches such as H-score and LogME by aggregating representations across all layers of the transformer model. We present an extensive analysis of 20 transformer LMs, 6 downstream NLP tasks, and various estimators (linear probing, kNN, H-score, and LogME). Our evaluation finds that averaging the layer representations significantly improves the Pearson correlation coefficient between the true model ranks and the estimate, increasing from 0.58 to 0.86 for LogME and from 0.65 to 0.88 for H-score.</abstract>
      <url hash="786d4623">2024.findings-acl.757</url>
      <bibkey>garbaciauskas-etal-2024-choose</bibkey>
      <doi>10.18653/v1/2024.findings-acl.757</doi>
    </paper>
    <paper id="758">
      <title>Argument-Aware Approach To Event Linking</title>
      <author><first>I-Hung</first><last>Hsu</last></author>
      <author><first>Zihan</first><last>Xue</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Nilay</first><last>Pochhi</last></author>
      <author><first>Sahil</first><last>Bansal</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Prem</first><last>Natarajan</last><affiliation>Amazon/Alexa</affiliation></author>
      <author><first>Jayanth</first><last>Srinivasa</last></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>12769-12781</pages>
      <abstract>Event linking connects event mentions in text with relevant nodes in a knowledge base (KB). Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events. Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments. Moreover, the information-rich nature of events leads to the scarcity of event KBs. This emphasizes the need for event linking models to identify and classify event mentions not in the KB as “out-of-KB,” an area that has received limited attention. In this work, we tackle these challenges by introducing an argument-aware approach. First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions. Subsequently, to help the model handle “out-of-KB” scenarios, we synthesize out-of-KB training examples from in-KB instances through controlled manipulation of event arguments. Our experiment across two test datasets showed significant enhancements in both in-KB and out-of-KB scenarios, with a notable 22% improvement in out-of-KB evaluations.</abstract>
      <url hash="e3af6b2c">2024.findings-acl.758</url>
      <bibkey>hsu-etal-2024-argument</bibkey>
      <doi>10.18653/v1/2024.findings-acl.758</doi>
    </paper>
    <paper id="759">
      <title><fixed-case>C</fixed-case>a<fixed-case>LM</fixed-case>: Contrasting Large and Small Language Models to Verify Grounded Generation</title>
      <author><first>I-Hung</first><last>Hsu</last></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Long</first><last>Le</last><affiliation>Google</affiliation></author>
      <author><first>Lesly</first><last>Miculicich</last><affiliation>Google</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Chen-Yu</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>12782-12803</pages>
      <abstract>Grounded generation aims to equip language models (LMs) with the ability to produce more credible and accountable responses by accurately citing verifiable sources. However, existing methods, by either feeding LMs with raw or preprocessed materials, remain prone to errors. To address this, we introduce CaLM, a novel verification framework. CaLM leverages the insight that a robust grounded response should be consistent with information derived solely from its cited sources. Our framework empowers smaller LMs, which rely less on parametric memory and excel at processing relevant information given a query, to validate the output of larger LMs. Larger LM responses that closely align with the smaller LMs’ output, which relies exclusively on cited documents, are verified. Responses showing discrepancies are iteratively refined through a feedback loop. Experiments on three open-domain question-answering datasets demonstrate significant performance gains of 1.5% to 7% absolute average without any required model fine-tuning.</abstract>
      <url hash="0c450a6e">2024.findings-acl.759</url>
      <bibkey>hsu-etal-2024-calm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.759</doi>
    </paper>
    <paper id="760">
      <title><fixed-case>T</fixed-case>ext<fixed-case>EE</fixed-case>: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction</title>
      <author><first>Kuan-Hao</first><last>Huang</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>I-Hung</first><last>Hsu</last></author>
      <author><first>Tanmay</first><last>Parekh</last></author>
      <author><first>Zhiyu</first><last>Xie</last></author>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Prem</first><last>Natarajan</last><affiliation>Amazon/Alexa</affiliation></author>
      <author><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Nanyun</first><last>Peng</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Heng</first><last>Ji</last><affiliation>University of Illinois, Urbana-Champaign</affiliation></author>
      <pages>12804-12825</pages>
      <abstract>Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 16 datasets spanning eight diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how they struggle to achieve satisfactory performance. Inspired by our reevaluation results and findings, we discuss the role of event extraction in the current NLP era, as well as future challenges and insights derived from TextEE. We believe TextEE, the first standardized comprehensive benchmarking tool, will significantly facilitate future event extraction research.</abstract>
      <url hash="e93bd030">2024.findings-acl.760</url>
      <bibkey>huang-etal-2024-textee</bibkey>
      <doi>10.18653/v1/2024.findings-acl.760</doi>
    </paper>
    <paper id="761">
      <title>Understanding the Impacts of Language Technologies’ Performance Disparities on <fixed-case>A</fixed-case>frican <fixed-case>A</fixed-case>merican Language Speakers</title>
      <author><first>Jay</first><last>Cunningham</last></author>
      <author><first>Su Lin</first><last>Blodgett</last><affiliation>Microsoft</affiliation></author>
      <author><first>Michael</first><last>Madaio</last><affiliation>Google</affiliation></author>
      <author><first>Hal</first><last>Daumé Iii</last><affiliation>University of Maryland - College Park, University of Maryland, College Park and Microsoft</affiliation></author>
      <author><first>Christina</first><last>Harrington</last><affiliation>Google</affiliation></author>
      <author><first>Hanna</first><last>Wallach</last><affiliation>Microsoft</affiliation></author>
      <pages>12826-12833</pages>
      <abstract>This paper examines the experiences of African American Language (AAL) speakers when using language technologies. Previous work has used quantitative methods to uncover performance disparities between AAL speakers and White Mainstream English speakers when using language technologies, but has not sought to understand the impacts of these performance disparities on AAL speakers. Through interviews with 19 AAL speakers, we focus on understanding such impacts in a contextualized and human-centered manner. We find that AAL speakers often undertake invisible labor of adapting their speech patterns to successfully use language technologies, and they make connections between failures of language technologies for AAL speakers and a lack of inclusion of AAL speakers in language technology design processes and datasets. Our findings suggest that NLP researchers and practitioners should invest in developing contextualized and human-centered evaluations of language technologies that seek to understand the impacts of performance disparities on speakers of underrepresented languages and language varieties.</abstract>
      <url hash="f6cdf5d2">2024.findings-acl.761</url>
      <bibkey>cunningham-etal-2024-understanding</bibkey>
      <doi>10.18653/v1/2024.findings-acl.761</doi>
    </paper>
    <paper id="762">
      <title><fixed-case>O</fixed-case>pen<fixed-case>C</fixed-case>ode<fixed-case>I</fixed-case>nterpreter: Integrating Code Generation with Execution and Refinement</title>
      <author><first>Tianyu</first><last>Zheng</last></author>
      <author><first>Ge</first><last>Zhang</last></author>
      <author><first>Tianhao</first><last>Shen</last></author>
      <author><first>Xueling</first><last>Liu</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Wenhu</first><last>Chen</last><affiliation>University of Waterloo and Google</affiliation></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>12834-12859</pages>
      <abstract>The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4’s 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreterbrings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.</abstract>
      <url hash="86d36109">2024.findings-acl.762</url>
      <bibkey>zheng-etal-2024-opencodeinterpreter</bibkey>
      <doi>10.18653/v1/2024.findings-acl.762</doi>
    </paper>
    <paper id="763">
      <title>Measuring and Addressing Indexical Bias in Information Retrieval</title>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>William</first><last>Held</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Jane</first><last>Dwivedi-Yu</last><affiliation>Meta AI</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <pages>12860-12877</pages>
      <abstract>Information Retrieval (IR) systems are designed to deliver relevant content, but traditional systems may not optimize rankings for fairness, neutrality, or the balance of ideas. Consequently, IR can often introduce indexical biases, or biases in the positional order of documents. Although indexical bias can demonstrably affect people’s opinion, voting patterns, and other behaviors, these issues remain understudied as the field lacks reliable metrics and procedures for automatically measuring indexical bias. Towards this end, we introduce the PAIR framework, which supports automatic bias audits for ranked documents or entire IR systems. After introducing DUO, the first general-purpose automatic bias metric, we run an extensive evaluation of 8 IR systems on a new corpus of 32k synthetic and 4.7k natural documents, with 4k queries spanning 1.4k controversial issue topics. A human behavioral study validates our approach, showing that our bias metric can help predict when and how indexical bias will shift a reader’s opinion.</abstract>
      <url hash="6c3fa7fd">2024.findings-acl.763</url>
      <bibkey>ziems-etal-2024-measuring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.763</doi>
    </paper>
    <paper id="764">
      <title><fixed-case>CIDAR</fixed-case>: Culturally Relevant Instruction Dataset For <fixed-case>A</fixed-case>rabic</title>
      <author><first>Zaid</first><last>Alyafeai</last></author>
      <author><first>Khalid</first><last>Almubarak</last><affiliation>Prince Sattam bin Abdulaziz University</affiliation></author>
      <author><first>Ahmed</first><last>Ashraf</last></author>
      <author><first>Deema</first><last>Alnuhait</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Saied</first><last>Alshahrani</last></author>
      <author><first>Gubran</first><last>Abdulrahman</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <author><first>Gamil</first><last>Ahmed</last></author>
      <author><first>Qais</first><last>Gawah</last></author>
      <author><first>Zead</first><last>Saleh</last></author>
      <author><first>Mustafa</first><last>Ghaleb</last></author>
      <author><first>Yousef</first><last>Ali</last></author>
      <author><first>Maged</first><last>Al-shaibani</last><affiliation>King Fahad University of Petroleum and Minerals</affiliation></author>
      <pages>12878-12901</pages>
      <abstract>Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, leading to inherent biases toward Western culture. This bias negatively impacts non-English languages such as Arabic and the unique culture of the Arab region. This paper addresses this limitation by introducing CIDAR, the first open Arabic instruction-tuning dataset culturally aligned by native Arabic speakers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to a few models fine-tuned on other datasets. Our experiments indicate that models fine-tuned on CIDAR achieve better cultural alignment compared to those fine-tuned on 30x more data.</abstract>
      <url hash="404a1713">2024.findings-acl.764</url>
      <bibkey>alyafeai-etal-2024-cidar</bibkey>
      <doi>10.18653/v1/2024.findings-acl.764</doi>
    </paper>
    <paper id="765">
      <title><fixed-case>R</fixed-case>ad<fixed-case>G</fixed-case>raph-<fixed-case>XL</fixed-case>: A Large-Scale Expert-Annotated Dataset for Entity and Relation Extraction from Radiology Reports</title>
      <author><first>Jean-Benoit</first><last>Delbrouck</last><affiliation>Stanford University</affiliation></author>
      <author><first>Pierre</first><last>Chambon</last></author>
      <author><first>Zhihong</first><last>Chen</last><affiliation>Stanford University</affiliation></author>
      <author><first>Maya</first><last>Varma</last><affiliation>Stanford University</affiliation></author>
      <author><first>Andrew</first><last>Johnston</last><affiliation>Stanford University</affiliation></author>
      <author><first>Louis</first><last>Blankemeier</last></author>
      <author><first>Dave</first><last>Van Veen</last><affiliation>Stanford University</affiliation></author>
      <author><first>Tan</first><last>Bui</last></author>
      <author><first>Steven</first><last>Truong</last><affiliation>Vinbrain JSC and Toronto University</affiliation></author>
      <author><first>Curtis</first><last>Langlotz</last><affiliation>Stanford University</affiliation></author>
      <pages>12902-12915</pages>
      <abstract>In order to enable extraction of structured clinical data from unstructured radiology reports, we introduce RadGraph-XL, a large-scale, expert-annotated dataset for clinical entity and relation extraction. RadGraph-XL consists of 2,300 radiology reports, which are annotated with over 410,000 entities and relations by board-certified radiologists. Whereas previous approaches focus solely on chest X-rays, RadGraph-XL includes data from four anatomy-modality pairs - chest CT, abdomen/pelvis CT, brain MR, and chest X-rays. Then, in order to automate structured information extraction, we use RadGraph-XL to train transformer-based models for clinical entity and relation extraction. Our evaluations include comprehensive ablation studies as well as an expert reader study that evaluates trained models on out-of-domain data. Results demonstrate that our model surpasses the performance of previous methods by up to 52% and notably outperforms GPT-4 in this domain. We release RadGraph-XL as well as our trained model to foster further innovation and research in structured clinical information extraction.</abstract>
      <url hash="804d3e32">2024.findings-acl.765</url>
      <bibkey>delbrouck-etal-2024-radgraph</bibkey>
      <doi>10.18653/v1/2024.findings-acl.765</doi>
    </paper>
    <paper id="766">
      <title><fixed-case>SMART</fixed-case>: Submodular Data Mixture Strategy for Instruction Tuning</title>
      <author><first>H S V N S Kowndinya</first><last>Renduchintala</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Sumit</first><last>Bhatia</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ganesh</first><last>Ramakrishnan</last><affiliation>Indian Institute of Technology Bombay, Indian Institute of Technology Bombay</affiliation></author>
      <pages>12916-12934</pages>
      <abstract>Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there’s currently no systematic method beyond manual tuning or relying on practitioners’ intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinya-renduchintala/SMART.</abstract>
      <url hash="560c39c4">2024.findings-acl.766</url>
      <bibkey>renduchintala-etal-2024-smart</bibkey>
      <doi>10.18653/v1/2024.findings-acl.766</doi>
    </paper>
    <paper id="767">
      <title>Selective “Selective Prediction”: Reducing Unnecessary Abstention in Vision-Language Reasoning</title>
      <author><first>Tejas</first><last>Srinivasan</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Jack</first><last>Hessel</last><affiliation>Samaya AI</affiliation></author>
      <author><first>Tanmay</first><last>Gupta</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Yejin</first><last>Choi</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Jesse</first><last>Thomason</last><affiliation>University of Southern California and Amazon</affiliation></author>
      <author><first>Khyathi</first><last>Chandu</last></author>
      <pages>12935-12948</pages>
      <abstract>Selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain. However, when deploying a vision-language system with low tolerance for inaccurate predictions, selective prediction may be over-cautious and abstain too frequently, even on many correct predictions. We introduce ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without increasing the error rate of the system’s predictions. When the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries to find relevant clues in the image that provide additional evidence for the prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining. ReCoVERR enables three VLMs (BLIP2, InstructBLIP and LLaVA-1.5) to answer up to 20% more questions on the VQAv2 and A-OKVQA tasks without decreasing system accuracy, thus improving overall system reliability. Our code is available at https://github.com/tejas1995/ReCoVERR.</abstract>
      <url hash="9ef524cb">2024.findings-acl.767</url>
      <bibkey>srinivasan-etal-2024-selective</bibkey>
      <doi>10.18653/v1/2024.findings-acl.767</doi>
    </paper>
    <paper id="768">
      <title>Language Model Priors and Data Augmentation Strategies for Low-resource Machine Translation: A Case Study Using <fixed-case>F</fixed-case>innish to <fixed-case>N</fixed-case>orthern <fixed-case>S</fixed-case>ámi</title>
      <author><first>Jonne</first><last>Sälevä</last><affiliation>Brandeis University</affiliation></author>
      <author><first>Constantine</first><last>Lignos</last><affiliation>Brandeis University</affiliation></author>
      <pages>12949-12956</pages>
      <abstract>We investigate ways of using monolingual data in both the source and target languages for improving low-resource machine translation. As a case study, we experiment with translation from Finnish to Northern Sámi.Our experiments show that while conventional backtranslation remains a strong contender, using synthetic target-side data when training backtranslation models can be helpful as well.We also show that monolingual data can be used to train a language model which can act as a regularizer without any augmentation of parallel data.</abstract>
      <url hash="e6a041c5">2024.findings-acl.768</url>
      <bibkey>saleva-lignos-2024-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.768</doi>
    </paper>
    <paper id="769">
      <title>Differentially Private Knowledge Distillation via Synthetic Text Generation</title>
      <author><first>James</first><last>Flemings</last></author>
      <author><first>Murali</first><last>Annavaram</last><affiliation>University of Southern California</affiliation></author>
      <pages>12957-12968</pages>
      <abstract>Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy puts pressure on practitioners to train LLMs with Differential Privacy (DP) on private data. Concurrently, the exponential growth in parameter size of LLMs necessitates model compression before deployment of LLMs on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, simultaneously applying both schemes can compound the utility degradation. To this end, we propose DistilDP: a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private teacher LLM. The knowledge of a teacher LLM is transferred onto the student in two ways: one way from the synthetic data itself– the hard labels, and the other way by the output distribution of the teacher evaluated on the synthetic data– the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further distill knowledge by aligning the hidden representations between both. Our experimental results demonstrate that DistilDP can substantially improve the utility over existing baselines, at least 9.0 PPL on the Big Patent dataset, with strong privacy parameters, <tex-math>\epsilon=2</tex-math>. These promising results progress privacy-preserving compression of autoregressive LLMs. Our code can be accessed here: https://github.com/james-flemings/dp_compress.</abstract>
      <url hash="32fa1243">2024.findings-acl.769</url>
      <bibkey>flemings-annavaram-2024-differentially</bibkey>
      <doi>10.18653/v1/2024.findings-acl.769</doi>
    </paper>
    <paper id="770">
      <title><fixed-case>KIWI</fixed-case>: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions</title>
      <author><first>Fangyuan</first><last>Xu</last><affiliation>University of Texas at Austin and University of Texas at Austin</affiliation></author>
      <author><first>Kyle</first><last>Lo</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Luca</first><last>Soldaini</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Eunsol</first><last>Choi</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>David</first><last>Wadden</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>12969-12990</pages>
      <abstract>Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current LLMs on this task, we construct KIWI, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement. Our findings indicate that KIWI will be a valuable resource to measure progress and improve LLMs’ instruction-following capabilities for knowledge intensive writing tasks.</abstract>
      <url hash="ec4da04e">2024.findings-acl.770</url>
      <bibkey>xu-etal-2024-kiwi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.770</doi>
    </paper>
    <paper id="771">
      <title><fixed-case>XL</fixed-case>-<fixed-case>H</fixed-case>ead<fixed-case>T</fixed-case>ags: Leveraging Multimodal Retrieval Augmentation for the Multilingual Generation of News Headlines and Tags</title>
      <author><first>Faisal Tareque</first><last>Shohan</last></author>
      <author><first>Mir Tafseer</first><last>Nayeem</last><affiliation>University of Alberta</affiliation></author>
      <author><first>Samsul</first><last>Islam</last></author>
      <author><first>Abu Ubaida</first><last>Akash</last></author>
      <author><first>Shafiq</first><last>Joty</last><affiliation>SalesForce.com and Nanyang Technological University</affiliation></author>
      <pages>12991-13024</pages>
      <abstract>Millions of news articles published online daily can overwhelm readers. Headlines and entity (topic) tags are essential for guiding readers to decide if the content is worth their time. While headline generation has been extensively studied, tag generation remains largely unexplored, yet it offers readers better access to topics of interest. The need for conciseness in capturing readers’ attention necessitates improved content selection strategies for identifying salient and relevant segments within lengthy articles, thereby guiding language models effectively. To address this, we propose to leverage auxiliary information such as images and captions embedded in the articles to retrieve relevant sentences and utilize instruction tuning with variations to generate both headlines and tags for news articles in a multilingual context. To make use of the auxiliary information, we have compiled a dataset named XL-HeadTags, which includes 20 languages across 6 diverse language families. Through extensive evaluation, we demonstrate the effectiveness of our plug-and-play multimodal-multilingual retrievers for both tasks. Additionally, we have developed a suite of tools for processing and evaluating multilingual texts, significantly contributing to the research community by enabling more accurate and efficient analysis across languages.</abstract>
      <url hash="f03db790">2024.findings-acl.771</url>
      <bibkey>shohan-etal-2024-xl</bibkey>
      <doi>10.18653/v1/2024.findings-acl.771</doi>
    </paper>
    <paper id="772">
      <title><fixed-case>I</fixed-case>n<fixed-case>F</fixed-case>o<fixed-case>B</fixed-case>ench: Evaluating Instruction Following Ability in Large Language Models</title>
      <author><first>Yiwei</first><last>Qin</last></author>
      <author><first>Kaiqiang</first><last>Song</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yebowen</first><last>Hu</last><affiliation>University of Central Florida</affiliation></author>
      <author><first>Wenlin</first><last>Yao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Sangwoo</first><last>Cho</last><affiliation>Capital One</affiliation></author>
      <author><first>Xiaoyang</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xuansheng</first><last>Wu</last></author>
      <author id="fei-liu"><first>Fei</first><last>Liu</last><affiliation>Emory University</affiliation></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Dong</first><last>Yu</last><affiliation>Tencent AI Lab</affiliation></author>
      <pages>13025-13048</pages>
      <abstract>This paper introduces the Decomposed Requirements Following Ratio (DRFR), a new metric for evaluating Large Language Models’ (LLMs) ability to follow instructions. Addressing a gap in current methodologies, DRFR breaks down complex instructions into simpler criteria, facilitating a detailed analysis of LLMs’ compliance with various aspects of tasks. Alongside this metric, we present InFoBench, a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories. Our experiments compare DRFR with traditional scoring methods and explore annotation sources, including human experts, crowd-sourced workers, and GPT-4. The findings demonstrate DRFR’s higher reliability and the effectiveness of using GPT-4 as a cost-efficient annotator. The evaluation of several advanced LLMs using this framework reveals their strengths and areas needing improvement, particularly in complex instruction-following. This study contributes a novel metric and benchmark, offering insights for future LLM development and evaluation.</abstract>
      <url hash="4dca4573">2024.findings-acl.772</url>
      <bibkey>qin-etal-2024-infobench</bibkey>
      <doi>10.18653/v1/2024.findings-acl.772</doi>
    </paper>
    <paper id="773">
      <title><fixed-case>E</fixed-case>co<fixed-case>R</fixed-case>ank: Budget-Constrained Text Re-ranking Using Large Language Models</title>
      <author><first>Muhammad</first><last>Rashid</last></author>
      <author><first>Jannat</first><last>Meem</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Yue</first><last>Dong</last><affiliation>University of California, Riverside and McGill University</affiliation></author>
      <author><first>Vagelis</first><last>Hristidis</last><affiliation>University of California, Riverside</affiliation></author>
      <pages>13049-13063</pages>
      <abstract>Large Language Models (LLMs) have achieved state-of-the-art performance in text re-ranking. This process includes queries and candidate passages in the prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A limitation of these ranking strategies with LLMs is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens. We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of prompt choices, LLM APIs, and budget splits. We propose a suite of budget-constrained methods to perform text re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs. Our experimental results on four popular QA and passage reranking datasets show that EcoRank outperforms other budget-aware supervised and unsupervised baselines.</abstract>
      <url hash="f0c77aef">2024.findings-acl.773</url>
      <bibkey>rashid-etal-2024-ecorank</bibkey>
      <doi>10.18653/v1/2024.findings-acl.773</doi>
    </paper>
    <paper id="774">
      <title><fixed-case>F</fixed-case>in<fixed-case>T</fixed-case>ral: A Family of <fixed-case>GPT</fixed-case>-4 Level Multimodal Financial Large Language Models</title>
      <author><first>Gagan</first><last>Bhatia</last></author>
      <author><first>El Moatez Billah</first><last>Nagoudi</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Hasan</first><last>Cavusoglu</last><affiliation>Sauder School of Business</affiliation></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>13064-13087</pages>
      <abstract>We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&amp;R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to excel in real-time analysis and decision-making in diverse financial contexts.</abstract>
      <url hash="0d409243">2024.findings-acl.774</url>
      <bibkey>bhatia-etal-2024-fintral</bibkey>
      <doi>10.18653/v1/2024.findings-acl.774</doi>
    </paper>
    <paper id="775">
      <title>Aligning Large Multimodal Models with Factually Augmented <fixed-case>RLHF</fixed-case></title>
      <author><first>Zhiqing</first><last>Sun</last><affiliation>OpenAI</affiliation></author>
      <author><first>Sheng</first><last>Shen</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Shengcao</first><last>Cao</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Haotian</first><last>Liu</last><affiliation>Department of Computer Science, University of Wisconsin - Madison</affiliation></author>
      <author><first>Chunyuan</first><last>Li</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Yikang</first><last>Shen</last><affiliation>University of Montreal</affiliation></author>
      <author><first>Chuang</first><last>Gan</last></author>
      <author><first>Liangyan</first><last>Gui</last><affiliation>UIUC</affiliation></author>
      <author><first>Yu-Xiong</first><last>Wang</last><affiliation>School of Computer Science, Carnegie Mellon University and Department of Computer Science, University of Illinois Urbana-Champaign</affiliation></author>
      <author><first>Yiming</first><last>Yang</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <author><first>Kurt</first><last>Keutzer</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Trevor</first><last>Darrell</last><affiliation>Electrical Engineering &amp; Computer Science Department</affiliation></author>
      <pages>13088-13110</pages>
      <abstract>Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in “hallucination”, generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 96% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement of 60% on MMHAL-BENCH over other baselines.</abstract>
      <url hash="5d6e2c77">2024.findings-acl.775</url>
      <bibkey>sun-etal-2024-aligning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.775</doi>
    </paper>
    <paper id="776">
      <title>The Art of Defending: A Systematic Evaluation and Analysis of <fixed-case>LLM</fixed-case> Defense Strategies on Safety and Over-Defensiveness</title>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Pavel</first><last>Dolin</last></author>
      <author><first>Agastya</first><last>Seth</last></author>
      <author><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>13111-13128</pages>
      <abstract>As Large Language Models (LLMs) play an increasingly pivotal role in natural language processing applications, their safety concerns become critical areas of NLP research. This has resulted in the development of various LLM defense strategies. Unfortunately, despite the shared goal of improving the safety of LLMs, the evaluation suites across various research works are disjoint and lack diverse inputs to ensure accurate and precise evaluation estimates. Furthermore, the important factor of ‘over-defensiveness’ on the safe inputs has largely remained overlooked. Addressing these limitations, this paper presents a systematic evaluation, comparison, and analysis of various LLM defense strategies over both ‘safety’ and ‘over-defensiveness’. To this end, we compile a large and diverse collection of safe and unsafe prompts, design precise evaluation methodology, and study the efficacy of various LLM defense strategies on multiple state-of-the-art LLMs. Our work reveals a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the safety of LLMs.</abstract>
      <url hash="d887e5a1">2024.findings-acl.776</url>
      <bibkey>varshney-etal-2024-art</bibkey>
      <doi>10.18653/v1/2024.findings-acl.776</doi>
    </paper>
    <paper id="777">
      <title><fixed-case>PAT</fixed-case>-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering</title>
      <author><first>Jannat</first><last>Meem</last><affiliation>University of California, Riverside</affiliation></author>
      <author><first>Muhammad</first><last>Rashid</last></author>
      <author><first>Yue</first><last>Dong</last><affiliation>University of California, Riverside and McGill University</affiliation></author>
      <author><first>Vagelis</first><last>Hristidis</last><affiliation>University of California, Riverside</affiliation></author>
      <pages>13129-13148</pages>
      <abstract>Existing work on Temporal Question Answering (TQA) has predominantly focused on questions anchored to specific timestamps or events (e.g. ‘Who was the US president in 1970?’). Little work has studied questions whose temporal context is relative to the present time (e.g. ‘Who was the previous US president?’). We refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses unique challenges: (1) large language models (LLMs) may have outdated knowledge, (2) complex temporal relationships (e.g. ‘before’, ‘previous’) are hard to reason, (3) multi-hop reasoning may be required, and (4) the gold answers of benchmarks must be continuously updated. To address these challenges, we introduce the PAT-Questions benchmark, which includes single and multi-hop temporal questions. The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available. We evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model (TEMPREASON-T5) on PAT-Questions through direct prompting and retrieval-augmented generation (RAG). The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities.</abstract>
      <url hash="d01a9e56">2024.findings-acl.777</url>
      <bibkey>meem-etal-2024-pat</bibkey>
      <doi>10.18653/v1/2024.findings-acl.777</doi>
    </paper>
    <paper id="778">
      <title><tex-math>360^\circ</tex-math><fixed-case>REA</fixed-case>: Towards A Reusable Experience Accumulation with <tex-math>360^\circ</tex-math> Assessment for Multi-Agent System</title>
      <author><first>Shen</first><last>Gao</last><affiliation>University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Hao</first><last>Li</last></author>
      <author><first>Zhengliang</first><last>Shi</last></author>
      <author><first>Chengrui</first><last>Huang</last></author>
      <author><first>Quan</first><last>Tu</last></author>
      <author><first>Shuo</first><last>Shang</last></author>
      <author><first>Zhiliang</first><last>Tian</last><affiliation>National University of Defense Technology</affiliation></author>
      <author><first>Minlie</first><last>Huang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>13149-13162</pages>
      <abstract>Large language model agents have demonstrated remarkable advancements across various complex tasks. Recent works focus on optimizing the agent team or employing self-reflection to iteratively solve complex tasks. Since these agents are all based on the same LLM, only conducting self-evaluation or removing underperforming agents does not substantively enhance the capability of the agents. We argue that a comprehensive evaluation and accumulating experience from evaluation feedback is an effective approach to improving system performance. In this paper, we propose Reusable Experience Accumulation with <tex-math>\mathbf{360^\circ}</tex-math> Assessment (<tex-math>360^\circ</tex-math>REA), a hierarchical multi-agent framework inspired by corporate organizational practices. The framework employs a novel <tex-math>360^\circ</tex-math> performance assessment method for multi-perspective performance evaluation with fine-grained assessment. To enhance the capability of agents in addressing complex tasks, we introduce dual-level experience pool for agents to accumulate experience through fine-grained assessment. Extensive experiments on complex task datasets demonstrate the effectiveness of <tex-math>360^\circ</tex-math>REA.</abstract>
      <url hash="1728dc5b">2024.findings-acl.778</url>
      <bibkey>gao-etal-2024-360</bibkey>
      <doi>10.18653/v1/2024.findings-acl.778</doi>
    </paper>
    <paper id="779">
      <title>Extracting Polymer Nanocomposite Samples from Full-Length Documents</title>
      <author><first>Ghazal</first><last>Khalighinejad</last><affiliation>Department of Computer Science, Duke University</affiliation></author>
      <author><first>Defne</first><last>Circi</last></author>
      <author><first>L.</first><last>Brinson</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>13163-13175</pages>
      <abstract>This paper investigates the use of large language models (LLMs) for extracting sample lists of polymer nanocomposites (PNCs) from full-length materials science research papers. The challenge lies in the complex nature of PNC samples, which have numerous attributes scattered throughout the text. The complexity of annotating detailed information on PNCs limits the availability of data, making conventional document-level relation extraction techniques impractical due to the challenge in creating comprehensive named entity span annotations.To address this, we introduce a new benchmark and an evaluation technique for this task and explore different prompting strategies in a zero-shot manner. We also incorporate self-consistency to improve the performance. Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them.</abstract>
      <url hash="299bd2d9">2024.findings-acl.779</url>
      <bibkey>khalighinejad-etal-2024-extracting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.779</doi>
    </paper>
    <paper id="780">
      <title>Leveraging <fixed-case>LLM</fixed-case> Reasoning Enhances Personalized Recommender Systems</title>
      <author><first>Alicia</first><last>Tsai</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Adam</first><last>Kraft</last><affiliation>Google</affiliation></author>
      <author><first>Long</first><last>Jin</last><affiliation>Google</affiliation></author>
      <author><first>Chenwei</first><last>Cai</last></author>
      <author><first>Anahita</first><last>Hosseini</last><affiliation>Google</affiliation></author>
      <author><first>Taibai</first><last>Xu</last></author>
      <author><first>Zemin</first><last>Zhang</last></author>
      <author><first>Lichan</first><last>Hong</last><affiliation>Google</affiliation></author>
      <author><first>Ed</first><last>Chi</last><affiliation>Google</affiliation></author>
      <author><first>Xinyang</first><last>Yi</last><affiliation>Google</affiliation></author>
      <pages>13176-13188</pages>
      <abstract>Recent advancements have showcased the potential of Large Language Models (LLMs) in executing reasoning tasks, particularly facilitated by Chain-of-Thought (CoT) prompting. While tasks like arithmetic reasoning involve clear, definitive answers and logical chains of thought, the application of LLM reasoning in recommendation systems (RecSys) presents a distinct challenge. RecSys tasks revolve around subjectivity and personalized preferences, an under-explored domain in utilizing LLMs’ reasoning capabilities. Our study explores several aspects to better understand reasoning for RecSys and demonstrate how task quality improves by utilizing LLM reasoning for both zero-shot and fine-tuning settings. Additionally, we propose <tex-math>\textbf{Rec-SAVER}</tex-math> (<tex-math>\textbf{Rec}</tex-math>ommender <tex-math>\textbf{S}</tex-math>ystems <tex-math>\textbf{A}</tex-math>utomatic <tex-math>\textbf{V}</tex-math>erification and <tex-math>\textbf{E}</tex-math>valuation of <tex-math>\textbf{R}</tex-math>easoning) to automatically assess the quality of LLM reasoning responses without the requirement of curated gold references or human raters. We show that our framework aligns with real human judgment on the coherence and faithfulness of reasoning responses. Overall, our work shows that incorporating reasoning into RecSys can improve personalized tasks, paving the way for further advancements in recommender system methodologies.</abstract>
      <url hash="48769538">2024.findings-acl.780</url>
      <bibkey>tsai-etal-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.findings-acl.780</doi>
    </paper>
    <paper id="781">
      <title>Toucan: Many-to-Many Translation for 150 <fixed-case>A</fixed-case>frican Language Pairs</title>
      <author><first>AbdelRahim</first><last>Elmadany</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Ife</first><last>Adebara</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last><affiliation>University of British Columbia</affiliation></author>
      <pages>13189-13206</pages>
      <abstract>We address a notable gap in Natural Language Processing (NLP) by introducing a collection of resources designed to improve Machine Translation (MT) for low-resource languages, with a specific focus on African languages. First, We introduce two language models (LMs), Cheetah-1.2B and Cheetah-3.7B, with 1.2 billion and 3.7 billion parameters respectively. Next, we finetune the aforementioned models to create Toucan, an Afrocentric machine translation model designed to support 156 African language pairs. To evaluate Toucan, we carefully develop an extensive machine translation benchmark, dubbed Afro-Lingu-MT, tailored for evaluating machine translation. Toucan significantly outperforms other models, showcasing its remarkable performance on MT for African languages. Finally, we train a new model, spBLEU-1K, to enhance translation evaluation metrics, covering 1K languages, including African languages. This work aims to advance the field of NLP, fostering cross-cultural understanding and knowledge exchange, particularly in regions with limited language resources such as Africa.</abstract>
      <url hash="38cdecf0">2024.findings-acl.781</url>
      <bibkey>elmadany-etal-2024-toucan</bibkey>
      <doi>10.18653/v1/2024.findings-acl.781</doi>
    </paper>
    <paper id="782">
      <title>Few-shot Dialogue Strategy Learning for Motivational Interviewing via Inductive Reasoning</title>
      <author><first>Zhouhang</first><last>Xie</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Mengjie</first><last>Zhao</last><affiliation>Sony</affiliation></author>
      <author><first>Yoshinori</first><last>Maeda</last><affiliation>Sony Group Corporation</affiliation></author>
      <author><first>Keiichi</first><last>Yamada</last><affiliation>Sony Group Corporation</affiliation></author>
      <author><first>Hiromi</first><last>Wakaki</last><affiliation>Sony Group Corporation</affiliation></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <pages>13207-13219</pages>
      <abstract>We consider the task of building a dialogue system that can motivate users to adopt positive lifestyle changes, Motivational Interviewing (MI). Addressing such a task requires a system that could infer <i>how</i> to motivate the user effectively. We propose DIIR, a framework that is capable of learning and applying conversation strategies in the form of natural language inductive rules from expert demonstrations. Automatic and human evaluation on instruction-following large language models show natural language strategies descriptions discovered by DIIR can improve active listening skills, reduce unsolicited advice, and promote more collaborative and less authoritative conversations, outperforming in-context demonstrations that are over 50 times longer.</abstract>
      <url hash="649a1eb1">2024.findings-acl.782</url>
      <bibkey>xie-etal-2024-shot-dialogue</bibkey>
      <doi>10.18653/v1/2024.findings-acl.782</doi>
    </paper>
    <paper id="783">
      <title>Evaluating Structural Generalization in Neural Machine Translation</title>
      <author><first>Ryoma</first><last>Kumon</last></author>
      <author><first>Daiki</first><last>Matsuoka</last></author>
      <author><first>Hitomi</first><last>Yanaka</last><affiliation>the University of Tokyo</affiliation></author>
      <pages>13220-13239</pages>
      <abstract>Compositional generalization refers to the ability to generalize to novel combinations of previously observed words and syntactic structures.Since it is regarded as a desired property of neural models, recent work has assessed compositional generalization in machine translation as well as semantic parsing.However, previous evaluations with machine translation have focused mostly on lexical generalization (i.e., generalization to unseen combinations of known words).Thus, it remains unclear to what extent models can translate sentences that require structural generalization (i.e., generalization to different sorts of syntactic structures).To address this question, we construct SGET, a machine translation dataset covering various types of compositional generalization with control of words and sentence structures.We evaluate neural machine translation models on SGET and show that they struggle more in structural generalization than in lexical generalization.We also find different performance trends in semantic parsing and machine translation, which indicates the importance of evaluations across various tasks.</abstract>
      <url hash="e5148312">2024.findings-acl.783</url>
      <bibkey>kumon-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.783</doi>
    </paper>
    <paper id="784">
      <title>Figuratively Speaking: Authorship Attribution via Multi-Task Figurative Language Modeling</title>
      <author><first>Gregorios</first><last>Katsios</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Ning</first><last>Sa</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <author><first>Tomek</first><last>Strzalkowski</last><affiliation>Rensselaer Polytechnic Institute</affiliation></author>
      <pages>13240-13255</pages>
      <abstract>The identification of Figurative Language (FL) features in text is crucial for various Natural Language Processing (NLP) tasks, where understanding of the author’s intended meaning and its nuances is key for successful communication. At the same time, the use of a specific blend of various FL forms most accurately reflects a writer’s style, rather than the use of any single construct, such as just metaphors or irony. Thus, we postulate that FL features could play an important role in Authorship Attribution (AA) tasks. We believe that our is the first computational study of AA based on FL use. Accordingly, we propose a Multi-task Figurative Language Model (MFLM) that learns to detect multiple FL features in text at once. We demonstrate, through detailed evaluation across multiple test sets, that the our model tends to perform equally or outperform specialized binary models in FL detection. Subsequently, we evaluate the predictive capability of joint FL features towards the AA task on three datasets, observing improved AA performance through the integration of MFLM embeddings.</abstract>
      <url hash="20e981ce">2024.findings-acl.784</url>
      <bibkey>katsios-etal-2024-figuratively</bibkey>
      <doi>10.18653/v1/2024.findings-acl.784</doi>
    </paper>
    <paper id="785">
      <title><fixed-case>CHAMP</fixed-case>: A Competition-level Dataset for Fine-Grained Analyses of <fixed-case>LLM</fixed-case>s’ Mathematical Reasoning Capabilities</title>
      <author><first>Yujun</first><last>Mao</last></author>
      <author><first>Yoon</first><last>Kim</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Yilun</first><last>Zhou</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <pages>13256-13274</pages>
      <abstract>Recent large language models (LLMs) have shown indications of mathematical reasoning ability on challenging competition-level problems, especially with self-generated verbalizations of intermediate reasoning steps (i.e., chain-of-thought prompting). However, current evaluations mainly focus on the end-to-end final answer correctness, and it is unclear whether LLMs can make use of helpful side information such as problem-specific hints. In this paper, we propose a challenging benchmark dataset for enabling such analyses. The Concept and Hint-Annotated Math Problems (CHAMP) consists of high school math competition problems, annotated with concepts, or general math facts, and hints, or problem-specific tricks. These annotations allow us to explore the effects of additional information, such as relevant hints, misleading concepts, or related problems. This benchmark is difficult, with the best model only scoring 58.1% in standard settings. With concepts and hints, performance sometimes improves, indicating that some models can make use of such side information. Furthermore, we annotate model-generated solutions for their correctness. Using this corpus, we find that models often arrive at the correct final answer through wrong reasoning steps. In addition, we test whether models are able to verify these solutions, and find that most models struggle.</abstract>
      <url hash="ff0a2801">2024.findings-acl.785</url>
      <bibkey>mao-etal-2024-champ</bibkey>
      <doi>10.18653/v1/2024.findings-acl.785</doi>
    </paper>
    <paper id="786">
      <title>Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding</title>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Fandong</first><last>Meng</last><affiliation>WeChat AI, Tencent Inc.</affiliation></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>13275-13288</pages>
      <abstract>Contemporary translation engines based on the encoder-decoder framework have made significant strides in development.However, the emergence of Large Language Models (LLMs) has disrupted their position by presenting the potential for achieving superior translation quality.To uncover the circumstances in which LLMs excel and explore how their strengths can be harnessed to enhance translation quality,we first conduct a comprehensive analysis to assess the strengths and limitations of various commercial NMT systems and MT-oriented LLMs. Our findings indicate that neither NMT nor MT-oriented LLMs alone can effectively address all the translation issues, but MT-oriented LLMs show promise as a complementary solution to NMT systems.Building upon these insights, we propose Cooperative Decoding (CoDec), which treats NMT systems as a pretranslation model and MT-oriented LLMs as a supplemental solution to handle complex scenarios beyond the capability of NMT alone.Experimental results on the WMT22 test sets and a newly collected test set WebCrawl demonstrate the effectiveness and efficiency of CoDec, highlighting its potential as a robust solution for combining NMT systems with MT-oriented LLMs in the field of machine translation.</abstract>
      <url hash="86cf5f4b">2024.findings-acl.786</url>
      <bibkey>zeng-etal-2024-improving</bibkey>
      <doi>10.18653/v1/2024.findings-acl.786</doi>
    </paper>
    <paper id="787">
      <title>Integrating Pre-Trained Speech and Language Models for End-to-End Speech Recognition</title>
      <author><first>Yukiya</first><last>Hono</last><affiliation>rinna Co., Ltd. and Nagoya Institute of Technology</affiliation></author>
      <author><first>Koh</first><last>Mitsuda</last><affiliation>rinna Co., Ltd.</affiliation></author>
      <author><first>Tianyu</first><last>Zhao</last><affiliation>Sakana AI</affiliation></author>
      <author><first>Kentaro</first><last>Mitsui</last><affiliation>rinna Co., Ltd.</affiliation></author>
      <author><first>Toshiaki</first><last>Wakatsuki</last><affiliation>rinna Co., Ltd.</affiliation></author>
      <author><first>Kei</first><last>Sawada</last><affiliation>rinna Co., Ltd.</affiliation></author>
      <pages>13289-13305</pages>
      <abstract>Advances in machine learning have made it possible to perform various text and speech processing tasks, such as automatic speech recognition (ASR), in an end-to-end (E2E) manner. E2E approaches utilizing pre-trained models are gaining attention for conserving training data and resources. However, most of their applications in ASR involve only one of either a pre-trained speech or a language model. This paper proposes integrating a pre-trained speech representation model and a large language model (LLM) for E2E ASR. The proposed model enables the optimization of the entire ASR process, including acoustic feature extraction and acoustic and language modeling, by combining pre-trained models with a bridge network and also enables the application of remarkable developments in LLM utilization, such as parameter-efficient domain adaptation and inference optimization. Experimental results demonstrate that the proposed model achieves a performance comparable to that of modern E2E ASR models by utilizing powerful pre-training models with the proposed integrated approach.</abstract>
      <url hash="4bca9d6a">2024.findings-acl.787</url>
      <bibkey>hono-etal-2024-integrating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.787</doi>
    </paper>
    <paper id="788">
      <title>Proving membership in <fixed-case>LLM</fixed-case> pretraining data via data watermarks</title>
      <author><first>Johnny</first><last>Wei</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ryan</first><last>Wang</last></author>
      <author><first>Robin</first><last>Jia</last><affiliation>University of Southern California</affiliation></author>
      <pages>13306-13320</pages>
      <abstract>Detecting whether copyright holders’ works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design - watermark length, number of duplications, and interference - affect the power of the hypothesis test. Next, we study how a watermark’s detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks remain strong if the model size also increases. Finally, we view SHA hashes as natural watermarks and show that we can robustly detect hashes from BLOOM-176B’s training data, as long as they occurred at least 90 times. Together, our results point towards a promising future for data watermarks in real world use.</abstract>
      <url hash="701a40d9">2024.findings-acl.788</url>
      <bibkey>wei-etal-2024-proving</bibkey>
      <doi>10.18653/v1/2024.findings-acl.788</doi>
    </paper>
    <paper id="789">
      <title>Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses</title>
      <author><first>Dongxu</first><last>Zhang</last><affiliation>ASAPP, Inc.</affiliation></author>
      <author><first>Varun</first><last>Gangal</last><affiliation>ASAPP Inc.</affiliation></author>
      <author><first>Barrett</first><last>Lattimer</last><affiliation>ASAPP</affiliation></author>
      <author><first>Yi</first><last>Yang</last></author>
      <pages>13321-13332</pages>
      <abstract>Detecting hallucinations in large language model (LLM) outputs is pivotal, yet traditional fine-tuning for this classification task is impeded by the expensive and quickly outdated annotation process, especially across numerous vertical domains and in the face of rapid LLM advancements. In this study, we introduce an approach that automatically generates both faithful and hallucinated outputs by rewriting system responses. Experimental findings demonstrate that a T5-base model, fine-tuned on our generated dataset, surpasses state-of-the-art zero-shot detectors and existing synthetic generation methods in both accuracy and latency, indicating efficacy of our approach.</abstract>
      <url hash="c796a5ff">2024.findings-acl.789</url>
      <bibkey>zhang-etal-2024-enhancing-hallucination</bibkey>
      <doi>10.18653/v1/2024.findings-acl.789</doi>
    </paper>
    <paper id="790">
      <title><fixed-case>S</fixed-case>ec<fixed-case>F</fixed-case>ormer: Fast and Accurate Privacy-Preserving Inference for Transformer Models via <fixed-case>SMPC</fixed-case></title>
      <author><first>Jinglong</first><last>Luo</last></author>
      <author><first>Yehong</first><last>Zhang</last><affiliation>Peng Cheng Laboratory</affiliation></author>
      <author><first>Zhuo</first><last>Zhang</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Jiaqi</first><last>Zhang</last><affiliation>PengCheng Laboratory</affiliation></author>
      <author><first>Xin</first><last>Mu</last></author>
      <author><first>Hui</first><last>Wang</last></author>
      <author><first>Yue</first><last>Yu</last><affiliation>National University of Defense Technology and PengCheng Lab</affiliation></author>
      <author><first>Zenglin</first><last>Xu</last><affiliation>Fudan University</affiliation></author>
      <pages>13333-13348</pages>
      <abstract>With the growing use of Transformer models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for Transformer models often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and are difficult to circumvent or optimize effectively. To address this concern, we introduce a comprehensive PPI framework called SecFormer to achieve fast and accurate PPI for Transformer models. We successfully eliminate the high-cost exponential and maximum operations in PPI without sacrificing model performance and develop a suite of efficient SMPC protocols by employing suitable numerical computation methods to boost other complex nonlinear functions in PPI, including GeLU, LayerNorm, and a redesigned Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer in performance, showing improvements of 3.4% and 24.7% for <tex-math>BERT_{\text{BASE}}</tex-math> and <tex-math>BERT_{\text{LARGE}}</tex-math>, respectively. In terms of efficiency, SecFormer is 3.57 and 3.58 times faster than PUMA for <tex-math>BERT_{\text{BASE}}</tex-math> and <tex-math>BERT_{\text{LARGE}}</tex-math>, demonstrating its effectiveness and speed.</abstract>
      <url hash="c1fd4e2c">2024.findings-acl.790</url>
      <bibkey>luo-etal-2024-secformer</bibkey>
      <doi>10.18653/v1/2024.findings-acl.790</doi>
    </paper>
    <paper id="791">
      <title>Raccoon: Prompt Extraction Benchmark of <fixed-case>LLM</fixed-case>-Integrated Applications</title>
      <author><first>Junlin</first><last>Wang</last></author>
      <author><first>Tianyi</first><last>Yang</last></author>
      <author><first>Roy</first><last>Xie</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last><affiliation>Duke University</affiliation></author>
      <pages>13349-13365</pages>
      <abstract>With the proliferation of LLM-integrated applications such as GPT-s, millions are deployed, offering valuable services through proprietary instruction prompts. These systems, however, are prone to prompt extraction attacks through meticulously designed queries. To help mitigate this problem, we introduce the Raccoon benchmark which comprehensively evaluates a model’s susceptibility to prompt extraction attacks. Our novel evaluation method assesses models under both defenseless and defended scenarios, employing a dual approach to evaluate the effectiveness of existing defenses and the resilience of the models. The benchmark encompasses 14 categories of prompt extraction attacks, with additional compounded attacks that closely mimic the strategies of potential attackers, alongside a diverse collection of defense templates. This array is, to our knowledge, the most extensive compilation of prompt theft attacks and defense mechanisms to date. Our findings highlight universal susceptibility to prompt theft in the absence of defenses, with OpenAI models demonstrating notable resilience when protected. This paper aims to establish a more systematic benchmark for assessing LLM robustness against prompt extraction attacks, offering insights into their causes and potential countermeasures.</abstract>
      <url hash="272c489e">2024.findings-acl.791</url>
      <bibkey>wang-etal-2024-raccoon</bibkey>
      <doi>10.18653/v1/2024.findings-acl.791</doi>
    </paper>
    <paper id="792">
      <title>History-Aware Conversational Dense Retrieval</title>
      <author><first>Fengran</first><last>Mo</last></author>
      <author><first>Chen</first><last>Qu</last></author>
      <author><first>Kelong</first><last>Mao</last></author>
      <author><first>Tianyu</first><last>Zhu</last></author>
      <author><first>Zhan</first><last>Su</last></author>
      <author><first>Kaiyu</first><last>Huang</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Jian-Yun</first><last>Nie</last><affiliation>University of Montreal</affiliation></author>
      <pages>13366-13378</pages>
      <abstract>Conversational search facilitates complex information retrieval by enabling multi-turn interactions between users and the system. Supporting such interactions requires a comprehensive understanding of the conversational inputs to formulate a good search query based on historical information. In particular, the search query should include the relevant information from the previous conversation turns.However, current approaches for conversational dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever using the whole conversational search session, which can be lengthy and noisy. Moreover, existing approaches are limited by the amount of manual supervision signals in the existing datasets.To address the aforementioned issues, we propose a **H**istory-**A**ware **Conv**ersational **D**ense **R**etrieval (HAConvDR) system, which incorporates two ideas: context-denoised query reformulation and automatic mining of supervision signals based on the actual impact of historical turns.Experiments on two public conversational search datasets demonstrate the improved history modeling capability of HAConvDR, in particular for long conversations with topic shifts.</abstract>
      <url hash="303c3318">2024.findings-acl.792</url>
      <bibkey>mo-etal-2024-history</bibkey>
      <doi>10.18653/v1/2024.findings-acl.792</doi>
    </paper>
    <paper id="793">
      <title>Light Up the Shadows: Enhance Long-Tailed Entity Grounding with Concept-Guided Vision-Language Models</title>
      <author><first>Yikai</first><last>Zhang</last></author>
      <author><first>Qianyu</first><last>He</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xintao</first><last>Wang</last></author>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <pages>13379-13389</pages>
      <abstract>Multi-Modal Knowledge Graphs (MMKGs) have proven valuable for various downstream tasks. However, scaling them up is challenging because building large-scale MMKGs often introduces mismatched images (i.e., noise). Most entities in KGs belong to the long tail, meaning there are few images of them available online. This scarcity makes it difficult to determine whether a found image matches the entity. To address this, we draw on the Triangle of Reference Theory and suggest enhancing vision-language models with concept guidance. Specifically, we introduce COG, a two-stage framework with COncept-Guided vision-language models. The framework comprises a Concept Integration module, which effectively identifies image-text pairs of long-tailed entities, and an Evidence Fusion module, which offers explainability and enables human verification. To demonstrate the effectiveness of COG, we create a dataset of 25k image-text pairs of long-tailed entities. Our comprehensive experiments show that COG not only improves the accuracy of recognizing long-tailed image-text pairs compared to baselines but also offers flexibility and explainability.</abstract>
      <url hash="10005a66">2024.findings-acl.793</url>
      <bibkey>zhang-etal-2024-light</bibkey>
      <doi>10.18653/v1/2024.findings-acl.793</doi>
    </paper>
    <paper id="794">
      <title><fixed-case>Z</fixed-case>ero<fixed-case>S</fixed-case>tance: Leveraging <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> for Open-Domain Stance Detection via Dataset Generation</title>
      <author><first>Chenye</first><last>Zhao</last></author>
      <author><first>Yingjie</first><last>Li</last><affiliation>Westlake University</affiliation></author>
      <author><first>Cornelia</first><last>Caragea</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>13390-13405</pages>
      <abstract>Zero-shot stance detection that aims to detect the stance (typically against, favor, or neutral) towards unseen targets has attracted considerable attention. However, most previous studies only focus on targets from a single or limited text domains (e.g., financial domain), and thus zero-shot models cannot generalize well to unseen targets of diverse domains (e.g., political domain). In this paper, we consider a more realistic task, i.e., open-domain stance detection, which aims at training a model that is able to generalize well to unseen targets across multiple domains of interest. Particularly, we propose a novel dataset generation method ZeroStance, which leverages ChatGPT to construct a synthetic open-domain dataset CHATStance that covers a wide range of domains. We then train an open-domain model on our synthetic dataset after proper data filtering. Extensive results indicate that our model, when trained on this synthetic dataset, shows superior generalization to unseen targets of diverse domains over baselines on most benchmarks. Our method requires only a task description in the form of a prompt and is much more cost-effective and data-efficient than previous methods. We will release our code and data to facilitate future research.</abstract>
      <url hash="01d34813">2024.findings-acl.794</url>
      <bibkey>zhao-etal-2024-zerostance</bibkey>
      <doi>10.18653/v1/2024.findings-acl.794</doi>
    </paper>
    <paper id="795">
      <title>Boosting Zero-Shot Crosslingual Performance using <fixed-case>LLM</fixed-case>-Based Augmentations with Effective Data Selection</title>
      <author><first>Barah</first><last>Fazili</last></author>
      <author><first>Ashish</first><last>Agrawal</last></author>
      <author><first>Preethi</first><last>Jyothi</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <pages>13406-13422</pages>
      <abstract>Large language models (LLMs) are very proficient text generators. We leverage this capability of LLMs to generate task-specific data via zero-shot prompting and promote cross-lingual transfer for low-resource target languages. Given task-specific data in a source language and a teacher model trained on this data, we propose using this teacher to label LLM generations and employ a set of simple data selection strategies that use the teacher’s label probabilities. Our data selection strategies help us identify a representative subset of diverse generations that help boost zero-shot accuracies while being efficient, in comparison to using all the LLM generations (without any subset selection). We also highlight other important design choices that affect cross-lingual performance such as the use of translations of source data and what labels are best to use for the LLM generations. We observe significant performance gains across sentiment analysis and natural language inference tasks (of up to a maximum of 7.13 absolute points and 1.5 absolute points on average) across a number of target languages (Hindi, Marathi, Urdu, Swahili) and domains.</abstract>
      <url hash="3c2b85be">2024.findings-acl.795</url>
      <bibkey>fazili-etal-2024-boosting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.795</doi>
    </paper>
    <paper id="796">
      <title>Reinforcement Tuning for Detecting Stances and Debunking Rumors Jointly with Large Language Models</title>
      <author><first>Ruichao</first><last>Yang</last></author>
      <author><first>Wei</first><last>Gao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Jing</first><last>Ma</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Hongzhan</first><last>Lin</last><affiliation>Hong Kong Baptist University</affiliation></author>
      <author><first>Bo</first><last>Wang</last><affiliation>School of Artificial Intelligence, Jilin University</affiliation></author>
      <pages>13423-13439</pages>
      <abstract>Learning multi-task models for jointly detecting stance and verifying rumors poses challenges due to the need for training data of stance at post level and rumor veracity at claim level, which are difficult to obtain. To address this issue, we leverage large language models (LLMs) as the foundation annotators for the joint stance detection (SD) and rumor verification (RV) tasks, dubbed as JSDRV. We introduce a novel reinforcement tuning framework to enhance the joint predictive capabilities of LLM-based SD and RV components. Specifically, we devise a policy for selecting LLM-annotated data at the two levels, employing a hybrid reward mechanism to choose high-quality labels for effective LLM fine-tuning on both tasks. Results demonstrate that JSDRV improves the capabilities of LLMs in the joint tasks, not only outperforming state-of-the-art methods but also generalizing to non-LLMs accommodated as task models.</abstract>
      <url hash="fce3bb74">2024.findings-acl.796</url>
      <bibkey>yang-etal-2024-reinforcement</bibkey>
      <doi>10.18653/v1/2024.findings-acl.796</doi>
    </paper>
    <paper id="797">
      <title>Exploring the Potential of Dense Information in Multimodal Alignment</title>
      <author><first>Zhiyuan</first><last>Fan</last></author>
      <author><first>Zhihong</first><last>Chen</last><affiliation>Stanford University</affiliation></author>
      <author><first>Benyou</first><last>Wang</last><affiliation>The Chinese University of Hong Kong, Shenzhen</affiliation></author>
      <pages>13440-13451</pages>
      <abstract>Despite the success of data augmentation in improving CLIP model, existing methods that utilize LLM or SAM to enrich the information in captions still suffer from several limitations, including insufficient detail and excessive hallucinations, ultimately resulting in compromised alignment and masking the true potential of dense information. This can lead to erroneous conclusions about CLIP’s ability to handle rich data, impeding the development of more effective models. To address the limitations of existing methods, we introduce a novel pipeline that generates highly detailed, factually accurate captions for images, which facilitates in-depth analysis of the potential for dense information in multimodal alignment. Contrary to previous findings, our investigation revealed that lengthening captions boosts performance across diverse benchmarks, even surpassing the effectiveness of meticulously crafted hard negative samples. Building on these insights, DELIP is introduced, demonstrably enhancing both foundational multimodal alignment and compositional reasoning abilities. Finally, we explore strategies to expand the context window of the text encoder, unlocking the potential of richer data for CLIP and paving the way for advancements in leveraging dense information for multimodal alignment.</abstract>
      <url hash="39a1071a">2024.findings-acl.797</url>
      <bibkey>fan-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.797</doi>
    </paper>
    <paper id="798">
      <title>Referral Augmentation for Zero-Shot Information Retrieval</title>
      <author><first>Michael</first><last>Tang</last></author>
      <author><first>Shunyu</first><last>Yao</last><affiliation>Princeton University</affiliation></author>
      <author><first>John</first><last>Yang</last></author>
      <author><first>Karthik</first><last>Narasimhan</last><affiliation>Princeton University</affiliation></author>
      <pages>13452-13461</pages>
      <abstract>We propose Referral-Augmented Retrieval (RAR), a simple technique that concatenates document indices with referrals: text from other documents that cite or link to the given document. We find that RAR provides significant performance gains for tasks across paper retrieval, entity retrieval, and open-domain question-answering in both zero-shot and in-domain (e.g., fine-tuned) settings. We examine how RAR provides especially strong improvements on more structured tasks, and can greatly outperform generative text expansion techniques such as DocT5Query and Query2Doc, with a 37% and 21% absolute improvement on ACL paper retrieval, respectively. We also compare three ways to aggregate referrals for RAR. Overall, we believe RAR can help revive and re-contextualize the classic information retrieval idea of using anchor texts to improve the representations of documents in a wide variety of corpuses in the age of neural retrieval.</abstract>
      <url hash="a2baf5f7">2024.findings-acl.798</url>
      <bibkey>tang-etal-2024-referral</bibkey>
      <doi>10.18653/v1/2024.findings-acl.798</doi>
    </paper>
    <paper id="799">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>E</fixed-case>val: Instruction-Tuned Text Evaluator from Human Preference</title>
      <author><first>Wenhao</first><last>Wu</last></author>
      <author><first>Wei</first><last>Li</last><affiliation>Institute of Computing Technology, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xinyan</first><last>Xiao</last><affiliation>Baidu</affiliation></author>
      <author><first>Jiachen</first><last>Liu</last><affiliation>Baidu Inc.</affiliation></author>
      <author><first>Sujian</first><last>Li</last><affiliation>Peking University</affiliation></author>
      <pages>13462-13474</pages>
      <abstract>This paper explores to construct a general text evaluator based on open-source Large Language Models (LLMs), a domain predominantly occupied by commercial counterparts such as GPT-4. Recognizing the limitations of open-source models like Llama in evaluative tasks, we introduce InstructEval, a general multi-aspect text evaluator developed through instruction tuning of open-source LLMs. To overcome the shortage of annotated resources for multi-aspect evaluations, InstructEval combines extensive open Human Preference Modeling (HPM) datasets with a small set of multi-aspect annotated data.This approach not only enhances effectiveness in overall evaluation tasks but also exhibits improved performance in multi-aspect evaluation tasks.As demonstrated by our extensive experiments, InstructEval achieves comparable or superior performance to commercial LLMs like ChatGPT or GPT-4 in terms of both overall and multi-aspect evaluation.</abstract>
      <url hash="cab47bd4">2024.findings-acl.799</url>
      <bibkey>wu-etal-2024-instructeval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.799</doi>
    </paper>
    <paper id="800">
      <title>A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models</title>
      <author><first>Dang</first><last>Cuong</last></author>
      <author><first>Dung</first><last>Le</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Thai</first><last>Le</last><affiliation>Indiana University</affiliation></author>
      <pages>13475-13491</pages>
      <abstract>Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done <i>only after</i> fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate and (b) features with the most influence on the model robustness have a clear correlation with the robustness. Second, our framework can be used as a fast and effective additional tool for robustness evaluation since it (a) saves 30x-193x runtime compared to the traditional technique, (b) is transferable across models, (c) can be used under adversarial training, and (d) robust to statistical randomness. Our code is publicly available at <url>https://github.com/CaptainCuong/RobustText_ACL2024</url>.</abstract>
      <url hash="2144db08">2024.findings-acl.800</url>
      <bibkey>cuong-etal-2024-curious</bibkey>
      <doi>10.18653/v1/2024.findings-acl.800</doi>
    </paper>
    <paper id="801">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>G</fixed-case>raph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment</title>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Junda</first><last>Wu</last></author>
      <author><first>Yupeng</first><last>Hou</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Yao</first><last>Liu</last><affiliation>East China Normal University</affiliation></author>
      <author><first>Ming</first><last>Gao</last></author>
      <author><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <pages>13492-13510</pages>
      <abstract>Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose <b>InstructGraph</b>, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output’s reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13% and 38%, respectively.</abstract>
      <url hash="d261a5e0">2024.findings-acl.801</url>
      <bibkey>wang-etal-2024-instructgraph</bibkey>
      <doi>10.18653/v1/2024.findings-acl.801</doi>
    </paper>
    <paper id="802">
      <title><fixed-case>R</fixed-case>a<fixed-case>DA</fixed-case>: Retrieval-augmented Web Agent Planning with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Minsoo</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Victor</first><last>Bursztyn</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Eunyee</first><last>Koh</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Shunan</first><last>Guo</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <pages>13511-13525</pages>
      <abstract>Agents powered by large language models (LLMs) inherit important limitations, such as the restricted context length, dependency on human-engineered exemplars (e.g., for task decomposition), and insufficient generalization. To address these challenges, we propose RaDA, a novel planning method for Web agents that does not require manual exemplars, efficiently leverages the LLMs’ context, and enhances generalization. RaDA disentangles planning into two stages: for a new given task, during Retrieval-augmented Task Decomposition (RaD), it decomposes tasks into high-level subtasks; next, during Retrieval-augmented Action Generation (RaA), it traverses the trajectory obtained with RaD to iteratively synthesize actions based on dynamically retrieved exemplars. We compare RaDA with strong baselines covering a broad space of design choices, using both GPT-3.5 and GPT-4 as backbones; and we find consistent improvements over previous SOTA in two challenging benchmarks, CompWoB and Mind2Web, covering settings with different complexities. We show the contributions of RaDA via ablation studies and qualitative analysis; and we discuss the structural benefits of our more compositional design.</abstract>
      <url hash="3f8a4fad">2024.findings-acl.802</url>
      <bibkey>kim-etal-2024-rada</bibkey>
      <doi>10.18653/v1/2024.findings-acl.802</doi>
    </paper>
    <paper id="803">
      <title>Competition-Level Problems are Effective <fixed-case>LLM</fixed-case> Evaluators</title>
      <author><first>Yiming</first><last>Huang</last></author>
      <author><first>Zhenghao</first><last>Lin</last></author>
      <author><first>Xiao</first><last>Liu</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Shuai</first><last>Lu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Fangyu</first><last>Lei</last></author>
      <author><first>Yaobo</first><last>Liang</last></author>
      <author><first>Yelong</first><last>Shen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chen</first><last>Lin</last><affiliation>Xiamen University</affiliation></author>
      <author><first>Nan</first><last>Duan</last><affiliation>Microsoft Research Asia</affiliation></author>
      <author><first>Weizhu</first><last>Chen</last><affiliation>Microsoft GenAI</affiliation></author>
      <pages>13526-13544</pages>
      <abstract>Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4’s perceived zero-shot performance on this task, considering various aspects such as problems’ release time, difficulties, and types of errors encountered. Surprisingly, the perceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification. Unfortunately, none of them is able to consistently mitigate the challenges. Through our work, we emphasize the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future.</abstract>
      <url hash="448b07e4">2024.findings-acl.803</url>
      <bibkey>huang-etal-2024-competition</bibkey>
      <doi>10.18653/v1/2024.findings-acl.803</doi>
    </paper>
    <paper id="804">
      <title>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery</title>
      <author><first>Zonglin</first><last>Yang</last></author>
      <author><first>Xinya</first><last>Du</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Junxian</first><last>Li</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Jie</first><last>Zheng</last></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Erik</first><last>Cambria</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>13545-13565</pages>
      <abstract>Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation.To the best of our knowledge, this is the first work showing that LLMs are able to generate novel (”not existing in literature”) and valid (”reflecting reality”) scientific hypotheses.</abstract>
      <url hash="f5be3975">2024.findings-acl.804</url>
      <bibkey>yang-etal-2024-large-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.804</doi>
    </paper>
    <paper id="805">
      <title><fixed-case>GRADUAL</fixed-case>: Granularity-aware Dual Prototype Learning for Better Few-Shot Relation Extraction</title>
      <author><first>Zhiming</first><last>Li</last><affiliation>Yan Shan University</affiliation></author>
      <author><first>Yuchen</first><last>Lyu</last></author>
      <pages>13566-13577</pages>
      <abstract>Recent studies have shown that fusing text labels and context sentences is an effective method for learning prototype representations in few-shot relation extraction. However, the **inconsistency of prototype representations** across different few-shot tasks persists due to different context sentences for the same relation, even with the integration of text labels into prototype representations. Conversely, the text label for each relation is unique and consistent, 1)which prompts us to propose a **dual prototype learning method**. Unlike previous methods that only construct support-based prototypes, we additionally construct label-based prototypes. Furthermore, we introduce a graph-based prototype adjustment module to construct topological information between support-based and label-based prototypes, thereby generating a more effective similarity measure through a simple linear combination. In addition, relations of different granularities have different distribution widths in the same semantic space, the **imbalanced distribution in the semantic space** leads to a lack of comparability among relations. To create a more discriminative semantic space, 2)we propose a **granularity-aware prototype learning method** that unifies the distribution width of relations, making relations of different granularities have similar distribution widths. Experimental results on two public benchmark datasets show that our proposed methods achieve state-of-the-art performance in few-shot relation classification.</abstract>
      <url hash="12567f05">2024.findings-acl.805</url>
      <bibkey>li-lyu-2024-gradual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.805</doi>
    </paper>
    <paper id="806">
      <title>Training a Better <fixed-case>C</fixed-case>hinese Spelling Correction Model via Prior-knowledge Guided Teacher</title>
      <author><first>Chi</first><last>Wei</last></author>
      <author><first>Shaobin</first><last>Huang</last></author>
      <author><first>Rongsheng</first><last>Li</last><affiliation>Harbin Engineering University</affiliation></author>
      <author><first>Naiyu</first><last>Yan</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <pages>13578-13589</pages>
      <abstract>Recent advancements in Chinese Spelling Correction (CSC) predominantly leverage pre-trained language models (PLMs). However, a notable challenge with fine-tuned PLM-based CSC models is their tendency to over-correct, leading to poor generalization for error patterns outside the standard distribution. To address this, we developed a teacher network guided by prior knowledge for distillation learning of CSC models. Unlike traditional teacher networks, which depend on task-related pre-training, our method infuses task-related prior information into the teacher network, offering guidance beyond mere labels to the student network. This strategy significantly enhances the CSC model’s language modeling capabilities, crucial for minimizing over-correction. Importantly, our approach is model-independent and the teacher network does not require task-related pre-training, making it broadly applicable for enhancing various PLM-based CSC models with minimal additional computational resources. Extensive experiments on widely used benchmarks demonstrate that our method achieves new state-of-the-art results. Additionally, we explored the potential of generalizing our method to other non-autoregressive text-generation tasks.</abstract>
      <url hash="df889d77">2024.findings-acl.806</url>
      <bibkey>wei-etal-2024-training</bibkey>
      <doi>10.18653/v1/2024.findings-acl.806</doi>
    </paper>
    <paper id="807">
      <title>The Revolution of Multimodal Large Language Models: A Survey</title>
      <author><first>Davide</first><last>Caffagni</last></author>
      <author><first>Federico</first><last>Cocchi</last><affiliation>University of Pisa</affiliation></author>
      <author><first>Luca</first><last>Barsellotti</last></author>
      <author><first>Nicholas</first><last>Moratelli</last></author>
      <author><first>Sara</first><last>Sarto</last></author>
      <author><first>Lorenzo</first><last>Baraldi</last></author>
      <author><first>Lorenzo</first><last>Baraldi</last><affiliation>Università degli Studi di Modena e Reggio Emilia</affiliation></author>
      <author><first>Marcella</first><last>Cornia</last><affiliation>University of Modena and Reggio Emilia</affiliation></author>
      <author><first>Rita</first><last>Cucchiara</last><affiliation>Università di Modena e Reggio Emilia</affiliation></author>
      <pages>13590-13618</pages>
      <abstract>Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in terms of performance and computational requirements. Overall, this survey offers a comprehensive overview of the current state of the art, laying the groundwork for future MLLMs.</abstract>
      <url hash="28c2333c">2024.findings-acl.807</url>
      <bibkey>caffagni-etal-2024-revolution</bibkey>
      <doi>10.18653/v1/2024.findings-acl.807</doi>
    </paper>
    <paper id="808">
      <title><fixed-case>OOP</fixed-case>: Object-Oriented Programming Evaluation Benchmark for Large Language Models</title>
      <author><first>Shuai</first><last>Wang</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Li</first><last>Shen</last><affiliation>Sun Yat-Sen University</affiliation></author>
      <author><first>Yong</first><last>Luo</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>University of Sydney</affiliation></author>
      <pages>13619-13639</pages>
      <abstract/>
      <url hash="dedf093f">2024.findings-acl.808</url>
      <bibkey>wang-etal-2024-oop</bibkey>
      <doi>10.18653/v1/2024.findings-acl.808</doi>
    </paper>
    <paper id="809">
      <title>Code Needs Comments: Enhancing Code <fixed-case>LLM</fixed-case>s with Comment Augmentation</title>
      <author><first>Demin</first><last>Song</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Honglin</first><last>Guo</last><affiliation>Fudan University</affiliation></author>
      <author><first>Yunhua</first><last>Zhou</last></author>
      <author><first>Shuhao</first><last>Xing</last></author>
      <author><first>Yudong</first><last>Wang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Zifan</first><last>Song</last><affiliation>Tongji University</affiliation></author>
      <author><first>Wenwei</first><last>Zhang</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Qipeng</first><last>Guo</last><affiliation>Shanghai AI Laboratory</affiliation></author>
      <author><first>Hang</first><last>Yan</last><affiliation>AI lab</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <pages>13640-13656</pages>
      <abstract>The programming skill is one crucial ability for Large Language Models (LLMs), necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs). We examine the impact of pre-training data on code-focused LLMs’ performance by assessing the comment density as a measure of PL-NL alignment. Given the scarcity of code-comment aligned data in pre-training corpora, we introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out code data poorly correlated with natural language. We conducted experiments on three code-focused LLMs and observed consistent improvements in performance on two widely-used programming skill benchmarks. Notably, the model trained on the augmented data outperformed both the model used for generating comments and the model further trained on the data without augmentation.</abstract>
      <url hash="259d1172">2024.findings-acl.809</url>
      <bibkey>song-etal-2024-code</bibkey>
      <doi>10.18653/v1/2024.findings-acl.809</doi>
    </paper>
    <paper id="810">
      <title>Efficient Domain Adaptation for Non-Autoregressive Machine Translation</title>
      <author><first>WangJie</first><last>You</last></author>
      <author><first>Pei</first><last>Guo</last></author>
      <author><first>Juntao</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>13657-13670</pages>
      <abstract>Domain adaptation remains a challenge in the realm of Neural Machine Translation (NMT), even in the era of large language models (LLMs). Existing non-parametric approaches like nearest neighbor machine translation have made small Autoregressive Translation (AT) models achieve efficient domain generalization and adaptation without updating parameters, but leaving the Non-Autoregressive Translation (NAT) counterparts under-explored. To fill this blank, we introduce <tex-math>Bi</tex-math>-<tex-math>k</tex-math>NN, an innovative and efficient domain adaptation approach for NAT models that tailors a k-nearest-neighbor algorithm for NAT. Specifically, we introduce an effective datastore construction and correlated updating strategies to conform the parallel nature of NAT. Additionally, we train a meta-network that seamlessly integrates the NN distribution with the NMT distribution robustly during the iterative decoding process of NAT. Our experimental results across four benchmark datasets demonstrate that our <tex-math>Bi</tex-math>-<tex-math>k</tex-math>NN not only achieves significant improvements over the Base-NAT model (7.8 BLEU on average) but also exhibits enhanced efficiency.</abstract>
      <url hash="127e7326">2024.findings-acl.810</url>
      <bibkey>you-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.findings-acl.810</doi>
    </paper>
    <paper id="811">
      <title>Exploring Reversal Mathematical Reasoning Ability for Large Language Models</title>
      <author><first>Pei</first><last>Guo</last></author>
      <author><first>WangJie</first><last>You</last></author>
      <author><first>Juntao</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Yan</first><last>Bowen</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>13671-13685</pages>
      <abstract>Large language models (LLMs) have presented remarkable capabilities in the wide range of natural language understanding and reasoning tasks. Despite their success, a few works indicate that LLMs suffer from the “reversal curse”, in which LLMs can’t employ the inverted structure “B is A” when they are trained based on “A is B”. To explore the effect of the “reversal curse” for LLMs on complex mathematical reasoning tasks, we present two reversal datasets upon GSM8K and MathQA and verify that LLMs also struggle to solve reversal mathematical problems. We analyze the potential reason and attribute it to the insufficient modeling of the relationship between reasoning steps caused by the left-to-right objective. Consequently, based on the characteristics of multi-step reasoning, we design a novel training method to improve the general and reversal reasoning abilities. Finally, we conduct experiments on four mathematical datasets, and the results demonstrate that our method significantly improves the general reasoning capacities and alleviates the reversal problem. Our datasets and codes are available at https: //github.com/AllForward/ReversalMath.</abstract>
      <url hash="0a8bb5ef">2024.findings-acl.811</url>
      <bibkey>guo-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.811</doi>
    </paper>
    <paper id="812">
      <title>A Unified Joint Approach with Topological Context Learning and Rule Augmentation for Knowledge Graph Completion</title>
      <author><first>Jingtao</first><last>Guo</last></author>
      <author><first>Chunxia</first><last>Zhang</last><affiliation>School of Computer Science and Technology, Beijing Institute of Technology</affiliation></author>
      <author><first>Lingxi</first><last>Li</last></author>
      <author><first>Xiaojun</first><last>Xue</last></author>
      <author><first>Zhendong</first><last>Niu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>13686-13696</pages>
      <abstract>Knowledge graph completion (KGC) task is to infer the missing knowledge in the knowledge graph based on known factual triples. However, present KGC approaches still face the following two challenges. Those methods perform simple linear update on relation representation, and only local neighborhood information is aggregated, which makes it difficult to capture logic semantic between relations and global topological context information. To tackle the above challenges, we propose a unified joint approach with Topological Context learning and Rule Augmentation (TCRA) for KGC. The TCRA framework consists of an entity topological context learning mechanism based on dual-branch hierarchical graph attention network, and a relation rule context learning mechanism based on Rule-Transformer and rule-to-relation aggregator. The former mechanism encodes the topological structure features of entities, aggregates the local neighborhood topological context information of entities on the three levels (entity, relation and triple), and build clusters of global head or tail entities related to the same relation. It can capture the local and global topological context information of entities related to the same relation. The latter mechanism introduces chain-like Horn rules as the context information of relations, and encodes the logical semantic of relations to enrich the relation representation. Experimental performances on three benchmark datasets FB15k-237, WN18RR and Kinship indicate the effectiveness and superiority of our proposed approach. The codes are publicly available.</abstract>
      <url hash="cdb22546">2024.findings-acl.812</url>
      <bibkey>guo-etal-2024-unified</bibkey>
      <doi>10.18653/v1/2024.findings-acl.812</doi>
    </paper>
    <paper id="813">
      <title><fixed-case>F</fixed-case>resh<fixed-case>LLM</fixed-case>s: Refreshing Large Language Models with Search Engine Augmentation</title>
      <author><first>Tu</first><last>Vu</last><affiliation>Virginia Tech and Google</affiliation></author>
      <author><first>Mohit</first><last>Iyyer</last><affiliation>University of Massachusetts Amherst</affiliation></author>
      <author><first>Xuezhi</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Noah</first><last>Constant</last></author>
      <author><first>Jerry</first><last>Wei</last><affiliation>Anthropic and Stanford University</affiliation></author>
      <author><first>Jason</first><last>Wei</last><affiliation>OpenAI</affiliation></author>
      <author><first>Chris</first><last>Tar</last></author>
      <author><first>Yun-Hsuan</first><last>Sung</last><affiliation>Google</affiliation></author>
      <author><first>Denny</first><last>Zhou</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Quoc</first><last>Le</last><affiliation>Google</affiliation></author>
      <author><first>Thang</first><last>Luong</last><affiliation>Google</affiliation></author>
      <pages>13697-13720</pages>
      <abstract>Since most large language models (LLMs) are trained once and never updated, they struggle to dynamically adapt to our ever-changing world. In this work, we present FreshQA, a dynamic QA benchmark that tests a model’s ability to answer questions that may require reasoning over up-to-date world knowledge. We develop a two-mode human evaluation procedure to measure both correctness and hallucination, which we use to benchmark both closed and open-source LLMs by collecting &gt;50K human judgments. We observe that all LLMs struggle to answer questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. In response, we develop FreshPrompt, a few-shot prompting method that curates and organizes relevant information from a search engine into an LLM’s prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. To facilitate future work, we additionally develop FreshEval, a reliable autorater for quick evaluation and comparison on FreshQA. Our latest results with FreshEval suggest that open-source LLMs such as Mixtral (Jiang et al., 2024), when combined with FreshPrompt, are competitive with closed-source and commercial systems on search-augmented QA.</abstract>
      <url hash="08eabf88">2024.findings-acl.813</url>
      <bibkey>vu-etal-2024-freshllms</bibkey>
      <doi>10.18653/v1/2024.findings-acl.813</doi>
    </paper>
    <paper id="814">
      <title><fixed-case>ROSE</fixed-case> Doesn’t Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding</title>
      <author><first>Qihuang</first><last>Zhong</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Juhua</first><last>Liu</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Bo</first><last>Du</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Dacheng</first><last>Tao</last><affiliation>University of Sydney</affiliation></author>
      <pages>13721-13736</pages>
      <abstract>With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-purpose ability of LLMs. In-depth analyses explore the underlying mechanism of ROSE, and reveal when and where to use it.</abstract>
      <url hash="0cb19a6e">2024.findings-acl.814</url>
      <bibkey>zhong-etal-2024-rose</bibkey>
      <doi>10.18653/v1/2024.findings-acl.814</doi>
    </paper>
    <paper id="815">
      <title><fixed-case>CR</fixed-case>-<fixed-case>LLM</fixed-case>: A Dataset and Optimization for Concept Reasoning of Large Language Models</title>
      <author><first>Nianqi</first><last>Li</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jingping</first><last>Liu</last><affiliation>East China University of Science and Technology</affiliation></author>
      <author><first>Sihang</first><last>Jiang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Haiyun</first><last>Jiang</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Yanghua</first><last>Xiao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Jiaqing</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zujie</first><last>Liang</last><affiliation>Ant Group</affiliation></author>
      <author><first>Feng</first><last>Wei</last></author>
      <author><first>Jinglei</first><last>Chen</last><affiliation>ANT GROUP</affiliation></author>
      <author><first>Zhenghong</first><last>Hao</last></author>
      <author><first>Bing</first><last>Han</last><affiliation>mybank, antgroup</affiliation></author>
      <pages>13737-13747</pages>
      <abstract>Concept reasoning is an important capability for models to understand the world. However, the existing datasets, such as concept extraction and concept generation, suffer from modeledge leakage and context leakage. To address these limitations, we construct a dataset of concept reasoning for large language models (CR-LLM) with modeledge leakage prevention and context leakage prevention, which consists of 2,167 samples and covers different concept types. In addition, we propose a hybrid reasoning method, consisting of inductive reasoning, deductive reasoning and a controller. This method allows large language models to adaptively select the optimal reasoning method for each input sample. Finally, we conduct extensive experiments on CR-LLM using different models and methods. The results show that existing large language models and reasoning methods perform sub-optimally in the concept reasoning task. In contrast, our proposed method significantly improves the capabilities, achieving a 7% increase in accuracy compared to CoT and demonstrating better granularity. We release CR-LLM and code at https://github.com/Nianqi-Li/Concept-Reasoning-for-LLMs.</abstract>
      <url hash="4ae2b3a5">2024.findings-acl.815</url>
      <bibkey>li-etal-2024-cr</bibkey>
      <doi>10.18653/v1/2024.findings-acl.815</doi>
    </paper>
    <paper id="816">
      <title><fixed-case>DATA</fixed-case>-<fixed-case>CUBE</fixed-case>: Data Curriculum for Instruction-based Sentence Representation Learning</title>
      <author><first>Yingqian</first><last>Min</last></author>
      <author><first>Kun</first><last>Zhou</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Dawei</first><last>Gao</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Xin</first><last>Zhao</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>He</first><last>Hu</last><affiliation>Renmin University of China, Renmin University of China</affiliation></author>
      <author><first>Yaliang</first><last>Li</last><affiliation>Alibaba Group</affiliation></author>
      <pages>13748-13761</pages>
      <abstract>Recently, multi-task instruction tuning has been utilized to improve sentence representation learning (SRL). It enables SRL models to generate task-specific representations with the guidance of task instruction, thus exhibiting strong generalization ability on unseen tasks. However, these methods mostly neglect the potential interference problems across different tasks and instances, which may affect the training of the model.To address this issue, we propose a data curriculum method, namely **Data-CUBE**, that arranges the order of all the multi-task data for training, to minimize the interference risks from two aspects.At the task level, we aim to find the optimal task order to minimize the total cross-task interference risk and formulate this problem as the traveling salesman problem, which is further solved by a specially designed simulated annealing algorithm. At the instance level, we propose a measurement method to quantify the difficulty of all instances per task, and then arrange instances in an easy-to-difficult order for training.Experimental results show that our approach can boost the performance of state-of-the-art methods. Our code and data will be publicly released.</abstract>
      <url hash="445ab017">2024.findings-acl.816</url>
      <bibkey>min-etal-2024-data</bibkey>
      <doi>10.18653/v1/2024.findings-acl.816</doi>
    </paper>
    <paper id="817">
      <title>Combating Label Sparsity in Short Text Topic Modeling via Nearest Neighbor Augmentation</title>
      <author><first>Yang</first><last>Lin</last></author>
      <author><first>Xinyu</first><last>Ma</last></author>
      <author><first>Xin</first><last>Gao</last><affiliation>Peking University</affiliation></author>
      <author><first>Ruiqing</first><last>Li</last></author>
      <author><first>Yasha</first><last>Wang</last></author>
      <author><first>Xu</first><last>Chu</last></author>
      <pages>13762-13774</pages>
      <abstract>Extracting semantic topics from short texts presents a significant challenge in the field of data mining. While efforts have been made to mitigate data sparsity issue, the limited length of short documents also results in the absence of semantically relevant words, causing biased evidence lower bound and incomplete labels for likelihood maximization. We refer to this issue as the label sparsity problem. To combat this problem, we propose kNNTM, a neural short text topic model that incorporates a <tex-math>k</tex-math>-Nearest-Neighbor-based label completion algorithm by augmenting the reconstruction label with <tex-math>k</tex-math>-nearest documents to complement these relevant but unobserved words. Furthermore, seeking a precise reflection of distances between documents, we propose a fused multi-view distances metric that takes both local word similarities and global topic semantics into consideration. Extensive experiments on multiple public short-text datasets show that kNNTM model outperforms the state-of-the-art baseline models and can derive both high-quality topics and document representations.</abstract>
      <url hash="2184f999">2024.findings-acl.817</url>
      <bibkey>lin-etal-2024-combating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.817</doi>
    </paper>
    <paper id="818">
      <title><fixed-case>R</fixed-case>efute<fixed-case>B</fixed-case>ench: Evaluating Refuting Instruction-Following for Large Language Models</title>
      <author><first>Jianhao</first><last>Yan</last><affiliation>Westlake University</affiliation></author>
      <author><first>Yun</first><last>Luo</last><affiliation>westlake university</affiliation></author>
      <author><first>Yue</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <pages>13775-13791</pages>
      <abstract>The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model’s output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users’ refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, <b>RefuteBench</b>, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user’s stated feedback and roll back to their own responses. We further propose a <i>recall-and-repeat</i> prompts as a simple and effective way to enhance the model’s responsiveness to feedback.</abstract>
      <url hash="f05abc59">2024.findings-acl.818</url>
      <bibkey>yan-etal-2024-refutebench</bibkey>
      <doi>10.18653/v1/2024.findings-acl.818</doi>
    </paper>
    <paper id="819">
      <title>Complex Logical Query Answering by Calibrating Knowledge Graph Completion Models</title>
      <author><first>Changyi</first><last>Xiao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <pages>13792-13803</pages>
      <abstract>Complex logical query answering (CLQA) is a challenging task that involves finding answer entities for complex logical queries over incomplete knowledge graphs (KGs). Previous research has explored the use of pre-trained knowledge graph completion (KGC) models, which can predict the missing facts in KGs, to answer complex logical queries. However, KGC models are typically evaluated using ranking evaluation metrics, which may result in values of predictions of KGC models that are not well-calibrated. In this paper, we propose a method for calibrating KGC models, namely CKGC, which enables KGC models to adapt to answering complex logical queries. Notably, CKGC is lightweight and effective. The adaptation function is simple, allowing the model to quickly converge during the adaptation process. The core concept of CKGC is to map the values of predictions of KGC models to the range [0, 1], ensuring that values associated with true facts are close to 1, while values linked to false facts are close to 0. Through experiments on three benchmark datasets, we demonstrate that our proposed calibration method can significantly boost model performance in the CLQA task. Moreover, our approach can enhance the performance of CLQA while preserving the ranking evaluation metrics of KGC models. The code is available at https://github.com/changyi7231/CKGC.</abstract>
      <url hash="2a157b32">2024.findings-acl.819</url>
      <bibkey>xiao-cao-2024-complex</bibkey>
      <doi>10.18653/v1/2024.findings-acl.819</doi>
    </paper>
    <paper id="820">
      <title>Argument-Based Sentiment Analysis on Forward-Looking Statements</title>
      <author><first>Chin-Yi</first><last>Lin</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chung-Chi</first><last>Chen</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Hen-Hsen</first><last>Huang</last><affiliation>Institute of Information Science, Academia Sinica</affiliation></author>
      <author><first>Hsin-Hsi</first><last>Chen</last><affiliation>National Taiwan University</affiliation></author>
      <pages>13804-13815</pages>
      <abstract>This paper introduces a novel approach to analyzing the forward-looking statements in equity research reports by integrating argument mining with sentiment analysis. Recognizing the limitations of traditional models in capturing the nuances of future-oriented analysis, we propose a refined categorization of argument units into claims, premises, and scenarios, coupled with a unique sentiment analysis framework. Furthermore, we incorporate a temporal dimension to categorize the anticipated impact duration of market events. To facilitate this study, we present the Equity Argument Mining and Sentiment Analysis (Equity-AMSA) dataset. Our research investigates the extent to which detailed domain-specific annotations can be provided, the necessity of fine-grained human annotations in the era of large language models, and whether our proposed framework can improve performance in downstream tasks over traditional methods. Experimental results reveal the significance of manual annotations, especially for scenario identification and sentiment analysis. The study concludes that our annotation scheme and dataset contribute to a deeper understanding of forward-looking statements in equity research reports.</abstract>
      <url hash="99860fe7">2024.findings-acl.820</url>
      <bibkey>lin-etal-2024-argument</bibkey>
      <doi>10.18653/v1/2024.findings-acl.820</doi>
    </paper>
    <paper id="821">
      <title>Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model</title>
      <author><first>Hongbin</first><last>Zhang</last></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Yang</first><last>Xiang</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>13816-13836</pages>
      <abstract>Large language models (LLMs) have showcased their remarkable capabilities to handle various downstream tasks, including multilingual machine translation ability. Despite their impressive performance, decoder-only LLMs lack an explicit alignment between source and target contexts, leading to translation that may not faithfully represent the original content. To address this, we propose three learning strategies to encourage LLMs to pay more attention to the source context during translation: 1) adjusting attention weights on the source context by adaptive attention re-weighting; 2) suppressing the irrelevant target prefix using contrastive decoding; 3) avoiding excessive reliance on the target prefix through target-constrained tuning. To verify the effectiveness of our model, we curate a new dataset specifically focusing on unfaithful translations generated by LLMs. Experimental results on both human-collected and general test sets verify the effectiveness of our model across multiple language pairs. Further human evaluation demonstrates the efficacy of our method in reducing hallucinatory translation and improving the fidelity of translations.</abstract>
      <url hash="2d555727">2024.findings-acl.821</url>
      <bibkey>zhang-etal-2024-paying</bibkey>
      <doi>10.18653/v1/2024.findings-acl.821</doi>
    </paper>
    <paper id="822">
      <title>Unveiling the Power of Integration: Block Diagram Summarization through Local-Global Fusion</title>
      <author><first>Shreyanshu</first><last>Bhushan</last></author>
      <author><first>Eun-Soo</first><last>Jung</last></author>
      <author><first>Minho</first><last>Lee</last><affiliation>Kyungpook National University</affiliation></author>
      <pages>13837-13856</pages>
      <abstract>Block Diagrams play an essential role in visualizing the relationships between components or systems. Generating summaries of block diagrams is important for document understanding or question answering (QA) tasks by providing concise overviews of complex systems. However, it’s a challenging task as it requires compressing complex relationships into informative descriptions. In this paper, we present “BlockNet”, a fusion framework that summarizes block diagrams by integrating local and global information, catering to both English and Korean languages. Additionally, we introduce a new multilingual method to produce block diagram data, resulting in a high-quality dataset called “BD-EnKo”. In BlockNet, we develop “BlockSplit”, an Optical Character Recognition (OCR) based algorithm employing the divide-and-conquer principle for local information extraction. We train an OCR-free transformer architecture for global information extraction using BD-EnKo and public data. To assess the effectiveness of our model, we conduct thorough experiments on different datasets. The assessment shows that BlockNet surpasses all previous methods and models, including GPT-4V, for block diagram summarization.</abstract>
      <url hash="9d8a454d">2024.findings-acl.822</url>
      <bibkey>bhushan-etal-2024-unveiling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.822</doi>
    </paper>
    <paper id="823">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>SQL</fixed-case>: A Schema-Integrated Context-Dependent <fixed-case>T</fixed-case>ext2<fixed-case>SQL</fixed-case> Dataset with Diverse <fixed-case>SQL</fixed-case> Operations</title>
      <author><first>Chunhui</first><last>Li</last></author>
      <author><first>Yifan</first><last>Wang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhen</first><last>Wu</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Zhen</first><last>Yu</last><affiliation>Tencent</affiliation></author>
      <author><first>Fei</first><last>Zhao</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Shujian</first><last>Huang</last><affiliation>Nanjing University</affiliation></author>
      <author><first>Xinyu</first><last>Dai</last><affiliation>Nanjing University</affiliation></author>
      <pages>13857-13867</pages>
      <abstract>Text2SQL is a task that translates natural language into SQL statements. Context-dependent Text2SQL offers a more natural database interaction by simulating dialogues between users and databases, with CoSQL and SparC as representative datasets. Yet, these datasets struggle to accurately replicate real-world situations. To address this, we introduce MultiSQL, which extends them in three key aspects: (1) Diverse SQL Operations. We incorporate diverse SQL types such as Create, Update, and Insert to broaden the scope of SQL operations. (2) Schema-Integrated Context. We integrated query context with database schema dependencies to better depict database complexity. (3) Extended Dialogues. We expand dialogue length to better simulate long conversations and complex interactions. This multi-type, schema-integrated, context-dependent Text2SQL dataset comprises nearly 800 dialogue groups and over 9,000 interaction turns across 166 complex databases, offering a better benchmark for interactive user-database dialogue.Addressing MultiSQL’s challenges, we refined evaluation metrics to better capture diverse SQL types and schema dependencies. We designed a prompt framework that leverages historical data and self-refinement to accurately capture the dependency between text queries and database structures. Experiments with GPT-3.5, GPT-4, and LLaMA2-7B show both the effectiveness of our strategies and the challenges of MultiSQL. The datasets is available at https://github.com/grandchicken/MultiSQL.</abstract>
      <url hash="8a4a9d20">2024.findings-acl.823</url>
      <bibkey>li-etal-2024-multisql</bibkey>
      <doi>10.18653/v1/2024.findings-acl.823</doi>
    </paper>
    <paper id="824">
      <title>Towards Demonstration-Aware Large Language Models for Machine Translation</title>
      <author><first>Chen</first><last>Li</last></author>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China and Tianjin University, China</affiliation></author>
      <author><first>Xuebo</first><last>Liu</last><affiliation>Harbin Institute of Technolgy, Shenzhen</affiliation></author>
      <author><first>Zhaocong</first><last>Li</last></author>
      <author><first>Derek</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>13868-13881</pages>
      <abstract>Tuning-based large language models for machine translation (aka large translation model, LTM) have demonstrated significant performance in the field of machine translation. Despite their success, these models often face difficulties in leveraging demonstrations to further improve their performance. To tackle this challenge, we introduce a novel approach that integrates demonstration-aware training and inference strategies within the framework of tuning-based LTMs, hereby referred to as demonstration-aware LTMs. During training, we enrich the model’s learning process by incorporating both sentence- and document-level demonstrations derived from its original training dataset. During inference, the model synergizes its own contextual translations with retrieved high-quality demonstrations, leading to more precise and contextually appropriate outputs. Empirical results reveal that our demonstration-aware LTM not only mitigates the negative impacts traditionally associated with demonstrations but also secures substantial improvements in translation accuracy, particularly in domain-specific and document-level translation tasks. Source code and scripts are freely available at https://github.com/ChenLi0620/Demo-Aware-LLM-MT.</abstract>
      <url hash="4a17f2ea">2024.findings-acl.824</url>
      <bibkey>li-etal-2024-towards-demonstration</bibkey>
      <doi>10.18653/v1/2024.findings-acl.824</doi>
    </paper>
    <paper id="825">
      <title><fixed-case>DADA</fixed-case>: Distribution-Aware Domain Adaptation of <fixed-case>PLM</fixed-case>s for Information Retrieval</title>
      <author><first>Dohyeon</first><last>Lee</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jongyoon</first><last>Kim</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Seung-won</first><last>Hwang</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Joonsuk</first><last>Park</last><affiliation>University of Richmond</affiliation></author>
      <pages>13882-13893</pages>
      <abstract>Pre-trained language models (PLMs) exhibit promise in retrieval tasks but struggle with out-of-domain data due to distribution shifts.Addressing this, generative domain adaptation (DA), known as GPL, tackles distribution shifts by generating pseudo queries and labels to train models for predicting query-document relationships in new domains.However, it overlooks the domain distribution, causing the model to struggle with aligning the distribution in the target domain.We, therefore, propose a Distribution-Aware Domain Adaptation (DADA) to guide the model to consider the domain distribution knowledge at the level of both a single document and the corpus, which is referred to as observation-level feedback and domain-level feedback, respectively.Our method effectively adapts the model to the target domain and expands document representation to unseen gold query terms using domain and observation feedback, as demonstrated by empirical results on the BEIR benchmark.</abstract>
      <url hash="2bb841ff">2024.findings-acl.825</url>
      <bibkey>lee-etal-2024-dada</bibkey>
      <doi>10.18653/v1/2024.findings-acl.825</doi>
    </paper>
    <paper id="826">
      <title><fixed-case>LLM</fixed-case>s cannot find reasoning errors, but can correct them given the error location</title>
      <author><first>Gladys</first><last>Tyen</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Hassan</first><last>Mansoor</last><affiliation>Google</affiliation></author>
      <author><first>Victor</first><last>Carbune</last><affiliation>Google</affiliation></author>
      <author><first>Peter</first><last>Chen</last><affiliation>Google</affiliation></author>
      <author><first>Tony</first><last>Mak</last><affiliation>Google</affiliation></author>
      <pages>13894-13908</pages>
      <abstract>While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023b; Madaan et al.,2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we show that poor self-correction performance stems from LLMs’ inability tofind logical mistakes, rather than their ability to correct a known mistake. Firstly, we benchmark several state-of-the-art LLMs ontheir mistake-finding ability and demonstrate that they generally struggle with the task, even in highly objective, unambiguous cases. Secondly, we test the correction abilities of LLMs – separately from mistake finding – using a backtracking setup that feeds ground truth mistake location information to the model. We show that this boosts downstream task performance across our 5 reasoning tasks, indicating that LLMs’ correction abilities are robust. Finally, we show that it is possible to obtain mistake location information without ground truth labels or in-domain training data. We train a small classifier with out-of-domain data, which exhibits stronger mistake-finding performance than prompting a large model. We release our dataset of LLM-generated logical mistakes, BIG-Bench Mistake, to enable further research into locating LLM reasoning mistakes.</abstract>
      <url hash="973a7a42">2024.findings-acl.826</url>
      <bibkey>tyen-etal-2024-llms</bibkey>
      <doi>10.18653/v1/2024.findings-acl.826</doi>
    </paper>
    <paper id="827">
      <title>Investigating the Impact of Data Contamination of Large Language Models in Text-to-<fixed-case>SQL</fixed-case> translation</title>
      <author><first>Federico</first><last>Ranaldi</last><affiliation>University of Roma “Tor Vergata”</affiliation></author>
      <author><first>Elena Sofia</first><last>Ruzzetti</last><affiliation>Università degli Studi di Roma Tor Vergata</affiliation></author>
      <author><first>Dario</first><last>Onorati</last><affiliation>“La Sapienza” University of Rome</affiliation></author>
      <author><first>Leonardo</first><last>Ranaldi</last><affiliation>Idiap Research Institute</affiliation></author>
      <author><first>Cristina</first><last>Giannone</last></author>
      <author><first>Andrea</first><last>Favalli</last><affiliation>Almawave</affiliation></author>
      <author><first>Raniero</first><last>Romagnoli</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Fabio Massimo</first><last>Zanzotto</last><affiliation>University of Rome Tor Vergata</affiliation></author>
      <pages>13909-13920</pages>
      <abstract>Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination.In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5’s Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5’s efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to-SQL translation tasks.</abstract>
      <url hash="d0bb8beb">2024.findings-acl.827</url>
      <bibkey>ranaldi-etal-2024-investigating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.827</doi>
    </paper>
    <paper id="828">
      <title><fixed-case>C</fixed-case>hart<fixed-case>C</fixed-case>heck: Explainable Fact-Checking over Real-World Chart Images</title>
      <author><first>Mubashara</first><last>Akhtar</last></author>
      <author><first>Nikesh</first><last>Subedi</last><affiliation>, University of Utah</affiliation></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>University of Pennsylvania, United States</affiliation></author>
      <author><first>Sahar</first><last>Tahmasebi</last><affiliation>TIB – Leibniz Information Centre for Science and Technology</affiliation></author>
      <author><first>Oana</first><last>Cocarascu</last><affiliation>King’s College London</affiliation></author>
      <author><first>Elena</first><last>Simperl</last><affiliation>King’s College London</affiliation></author>
      <pages>13921-13937</pages>
      <abstract>Whilst fact verification has attracted substantial interest in the natural language processing community, verifying misinforming statements against data visualizations such as charts has so far been overlooked. Charts are commonly used in the real-world to summarize and com municate key information, but they can also be easily misused to spread misinformation and promote certain agendas. In this paper, we introduce ChartCheck, a novel, large-scale dataset for explainable fact-checking against real-world charts, consisting of 1.7k charts and 10.5k human-written claims and explanations. We systematically evaluate ChartCheck using vision-language and chart-to-table models, and propose a baseline to the community. Finally, we study chart reasoning types and visual attributes that pose a challenge to these models.</abstract>
      <url hash="271d8d40">2024.findings-acl.828</url>
      <bibkey>akhtar-etal-2024-chartcheck</bibkey>
      <doi>10.18653/v1/2024.findings-acl.828</doi>
    </paper>
    <paper id="829">
      <title>Real World Conversational Entity Linking Requires More Than Zero-Shots</title>
      <author><first>Mohanna</first><last>Hoveyda</last></author>
      <author><first>Arjen</first><last>Vries</last><affiliation>Institute for Computing and Information Sciences, Radboud University Nijmegen, Radboud University</affiliation></author>
      <author><first>Faegheh</first><last>Hasibi</last><affiliation>Radboud University</affiliation></author>
      <author><first>Maarten</first><last>Rijke</last><affiliation>University of Amsterdam</affiliation></author>
      <pages>13938-13946</pages>
      <abstract>Entity linking (EL) in conversations faces notable challenges in practical applications, primarily due to scarcity of entity-annotated conversational datasets and sparse knowledge bases (KB) containing domain-specific, long-tail entities. We designed targeted evaluation scenarios to measure the efficacy of EL models under resource constraints. Our evaluation employs two KBs: Fandom, exemplifying real-world EL complexities, and the widely used Wikipedia. First, we assess EL models’ ability to generalize to a new unfamiliar KB using Fandom and a novel zero-shot conversational entity linking dataset that we curated based on Reddit discussions on Fandom entities. We then evaluate the adaptability of EL models to conversational settings without prior training. Our results indicate that current zero-shot EL models falter when introduced to new, domain-specific KBs without prior training, significantly dropping in performance.Our findings reveal that previous evaluation approaches fall short of capturing real-world complexities for zero-shot EL, highlighting the necessity for new approaches to design and assess conversational EL models to adapt to limited resources. The evaluation frame-work and dataset proposed are tailored to facilitate this research.</abstract>
      <url hash="4eda4d75">2024.findings-acl.829</url>
      <bibkey>hoveyda-etal-2024-real</bibkey>
      <doi>10.18653/v1/2024.findings-acl.829</doi>
    </paper>
    <paper id="830">
      <title><fixed-case>CP</fixed-case>sy<fixed-case>C</fixed-case>oun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for <fixed-case>C</fixed-case>hinese Psychological Counseling</title>
      <author><first>Chenhao</first><last>Zhang</last><affiliation>Shanghai Artificial Intelligence Laboratory, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences and Huazhong University of Science and Technology</affiliation></author>
      <author><first>Renhao</first><last>Li</last><affiliation>University of Macau</affiliation></author>
      <author><first>Minghuan</first><last>Tan</last><affiliation>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Jingwei</first><last>Zhu</last></author>
      <author><first>Di</first><last>Yang</last></author>
      <author><first>Jiahao</first><last>Zhao</last></author>
      <author><first>Guancheng</first><last>Ye</last></author>
      <author><first>Chengming</first><last>Li</last><affiliation>Shenzhen MSU-BIT University</affiliation></author>
      <author><first>Xiping</first><last>Hu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <pages>13947-13966</pages>
      <abstract>Using large language models (LLMs) to assist psychological counseling is a significant but challenging task at present. Attempts have been made on improving empathetic conversations or acting as effective assistants in the treatment with LLMs. However, the existing datasets lack consulting knowledge, resulting in LLMs lacking professional consulting competence. Moreover, how to automatically evaluate multi-turn dialogues within the counseling process remains an understudied area. To bridge the gap, we propose CPsyCoun, a report-based multi-turn dialogue reconstruction and evaluation framework for Chinese psychological counseling. To fully exploit psychological counseling reports, a two-phase approach is devised to construct high-quality dialogues while a comprehensive evaluation benchmark is developed for the effective automatic evaluation of multi-turn psychological consultations. Competitive experimental results demonstrate the effectiveness of our proposed framework in psychological counseling. We open-source the datasets and model for future research.</abstract>
      <url hash="f6165a5b">2024.findings-acl.830</url>
      <bibkey>zhang-etal-2024-cpsycoun</bibkey>
      <doi>10.18653/v1/2024.findings-acl.830</doi>
    </paper>
    <paper id="831">
      <title>Tox-<fixed-case>BART</fixed-case>: Leveraging Toxicity Attributes for Explanation Generation of Implicit Hate Speech</title>
      <author><first>Neemesh</first><last>Yadav</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Sarah</first><last>Masud</last><affiliation>Indraprastha Institute of Information Technology Delhi (IIIT-Delhi)</affiliation></author>
      <author><first>Vikram</first><last>Goyal</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Md Shad</first><last>Akhtar</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Tanmoy</first><last>Chakraborty</last><affiliation>Indian Institute of Technology, Delhi</affiliation></author>
      <pages>13967-13983</pages>
      <abstract>Employing language models to generate explanations for an incoming implicit hate post is an active area of research. The explanation is intended to make explicit the underlying stereotype and aid content moderators. The training often combines top-k relevant knowledge graph (KG) tuples to provide world knowledge and improve performance on standard metrics. Interestingly, our study presents conflicting evidence for the role of the quality of KG tuples in generating implicit explanations. Consequently, simpler models incorporating external toxicity signals outperform KG-infused models. Compared to the KG-based setup, we observe a comparable performance for SBIC (LatentHatred) datasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and -4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore. Further human evaluation and error analysis reveal that our proposed setup produces more precise explanations than zero-shot GPT-3.5, highlighting the intricate nature of the task.</abstract>
      <url hash="3d78a662">2024.findings-acl.831</url>
      <bibkey>yadav-etal-2024-tox</bibkey>
      <doi>10.18653/v1/2024.findings-acl.831</doi>
    </paper>
    <paper id="832">
      <title><fixed-case>T</fixed-case>ext<fixed-case>G</fixed-case>en<fixed-case>SHAP</fixed-case>: Scalable Post-Hoc Explanations in Text Generation with Long Documents</title>
      <author><first>James</first><last>Enouen</last></author>
      <author><first>Hootan</first><last>Nakhost</last></author>
      <author><first>Sayna</first><last>Ebrahimi</last><affiliation>Google</affiliation></author>
      <author><first>Sercan</first><last>Arik</last><affiliation>Google</affiliation></author>
      <author><first>Yan</first><last>Liu</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>13984-14011</pages>
      <abstract>Large language models (LLMs) have attracted great interest in many real-world applications; however, their “black-box” nature necessitates scalable and faithful explanations. Shapley values have matured as an explainability method for deep learning, but extending them to LLMs is difficult due to long input contexts and autoregressive output generation. We introduce , an efficient post-hoc explanation method incorporating LLM-specific techniques, which leads to significant runtime improvements: token-level explanations in minutes not hours, and document-level explanations within seconds. We demonstrate how such explanations can improve end-to-end performance of retrieval augmented generation by localizing important words within long documents and reranking passages collected by retrieval systems. On various open-domain question answering benchmarks, we show TextGenSHAP improves the retrieval recall and prediction accuracy significantly.</abstract>
      <url hash="45a66ab9">2024.findings-acl.832</url>
      <bibkey>enouen-etal-2024-textgenshap</bibkey>
      <doi>10.18653/v1/2024.findings-acl.832</doi>
    </paper>
    <paper id="833">
      <title>Balanced Data Sampling for Language Model Training with Clustering</title>
      <author><first>Yunfan</first><last>Shao</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Zhaoye</first><last>Fei</last></author>
      <author><first>Hang</first><last>Yan</last><affiliation>AI lab</affiliation></author>
      <author><first>Dahua</first><last>Lin</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Xipeng</first><last>Qiu</last><affiliation>Fudan University</affiliation></author>
      <pages>14012-14023</pages>
      <abstract>Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and large language models.</abstract>
      <url hash="4f6080d1">2024.findings-acl.833</url>
      <bibkey>shao-etal-2024-balanced</bibkey>
      <doi>10.18653/v1/2024.findings-acl.833</doi>
    </paper>
    <paper id="834">
      <title>Length Generalization of Causal Transformers without Position Encoding</title>
      <author><first>Jie</first><last>Wang</last></author>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Hang</first><last>Yan</last><affiliation>AI lab</affiliation></author>
      <author><first>Tao</first><last>Gui</last><affiliation>Fudan University</affiliation></author>
      <author><first>Qi</first><last>Zhang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Xiaoling</first><last>Wang</last><affiliation>East China Normal University</affiliation></author>
      <pages>14024-14040</pages>
      <abstract>Generalizing to longer sentences is important for recent Transformer-based language models. Besides algorithms manipulating explicit position features, the success of Transformers without position encodings (NoPE) provides a new way to overcome the challenge. In this paper, we study the length generalization property of NoPE. We find that although NoPE can extend to longer sequences than the commonly used explicit position encodings, it still has a limited context length. We identify a connection between the failure of NoPE’s generalization and the distraction of attention distributions. We propose a parameter-efficient tuning for searching attention heads’ best temperature hyper-parameters, which substantially expands NoPE’s context size. Experiments on long sequence language modeling, the synthetic passkey retrieval task and real-world long context tasks show that NoPE can achieve competitive performances with state-of-the-art length generalization algorithms. The source code is publicly accessible</abstract>
      <url hash="5a6273b6">2024.findings-acl.834</url>
      <bibkey>wang-etal-2024-length</bibkey>
      <doi>10.18653/v1/2024.findings-acl.834</doi>
    </paper>
    <paper id="835">
      <title>Unsupervised Sign Language Translation and Generation</title>
      <author><first>Zhengsheng</first><last>Guo</last></author>
      <author><first>Zhiwei</first><last>He</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Wenxiang</first><last>Jiao</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Xing</first><last>Wang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Rui</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Kehai</first><last>Chen</last><affiliation>Harbin Institute of Technology (Shenzhen)</affiliation></author>
      <author><first>Zhaopeng</first><last>Tu</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Yong</first><last>Xu</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>14041-14055</pages>
      <abstract>Motivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure. Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences. We propose a sliding window method to address the issues of aligning variable-length text with video sequences. To our knowledge, USLNet is the first unsupervised sign language translation and generation model capable of generating both natural language text and sign language video in a unified manner. Experimental results on the BBC-Oxford Sign Language dataset and Open-Domain American Sign Language dataset reveal that USLNet achieves competitive results compared to supervised baseline models, indicating its effectiveness in sign language translation and generation.</abstract>
      <url hash="a19bacdb">2024.findings-acl.835</url>
      <bibkey>guo-etal-2024-unsupervised</bibkey>
      <doi>10.18653/v1/2024.findings-acl.835</doi>
    </paper>
    <paper id="836">
      <title>Mitigating Data Scarcity in Semantic Parsing across Languages with the Multilingual Semantic Layer and its Dataset</title>
      <author><first>Abelardo Carlos</first><last>Martinez Lorenzo</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Pere-Lluís</first><last>Huguet Cabot</last></author>
      <author><first>Karim</first><last>Ghonim</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Lu</first><last>Xu</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Hee-Soo</first><last>Choi</last></author>
      <author><first>Alberte</first><last>Fernández-Castro</last></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>14056-14080</pages>
      <abstract>Data scarcity is a prevalent challenge in the era of Large Language Models (LLMs). The insatiable hunger of LLMs for large corpora becomes even more pronounced when dealing with non-English and low-resource languages. The issue is particularly exacerbated in Semantic Parsing (SP), i.e. the task of converting text into a formal representation. The complexity of semantic formalisms makes training human annotators and subsequent data annotation unfeasible on a large scale, especially across languages. To mitigate this, we first introduce the Multilingual Semantic Layer (MSL), a conceptual evolution of previous formalisms, which decouples from disambiguation and external inventories and simplifies the task. MSL provides the necessary tools to encode the meaning across languages, paving the way for developing a high-quality semantic parsing dataset across different languages in a semi-automatic strategy. Subsequently, we manually refine a portion of this dataset and fine-tune GPT-3.5 to propagate these refinements across the dataset. Then, we manually annotate 1,100 sentences in eleven languages, including low-resource ones. Finally, we assess our dataset’s quality, showcasing the performance gap reduction across languages in Semantic Parsing.</abstract>
      <url hash="4a59f287">2024.findings-acl.836</url>
      <bibkey>martinez-lorenzo-etal-2024-mitigating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.836</doi>
    </paper>
    <paper id="837">
      <title>Efficient Sparse Attention needs Adaptive Token Release</title>
      <author><first>Chaoran</first><last>Zhang</last></author>
      <author><first>Lixin</first><last>Zou</last><affiliation>School of Cyber Science and Engineering, Wuhan University</affiliation></author>
      <author><first>Dan</first><last>Luo</last><affiliation>Lehigh University</affiliation></author>
      <author><first>Xiangyang</first><last>Luo</last><affiliation>State Key Lab of Mathematical Engineering and Advanced Computing</affiliation></author>
      <author><first>Zihao</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Min</first><last>Tang</last><affiliation>Monash University</affiliation></author>
      <author><first>Chenliang</first><last>Li</last></author>
      <pages>14081-14094</pages>
      <abstract>In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide array of text-centric tasks. However, their ‘large’ scale introduces significant computational and storage challenges, particularly in managing the key-value states of the transformer, which limits their wider applicability. Therefore, we propose to adaptively release resources from caches and rebuild the necessary key-value states. Particularly, we accomplish this by a lightweight controller module to approximate an ideal top-<tex-math>K</tex-math> sparse attention. This module retains the tokens with the highest top-<tex-math>K</tex-math> attention weights and simultaneously rebuilds the discarded but necessary tokens, which may become essential for future decoding. Comprehensive experiments in natural language generation and modeling reveal that our method is not only competitive with full attention in terms of performance but also achieves a significant throughput improvement of up to <tex-math>\textbf{221.8}</tex-math>%. The code for replication is available on the https://github.com/WHUIR/ADORE.</abstract>
      <url hash="b0aac1b8">2024.findings-acl.837</url>
      <bibkey>zhang-etal-2024-efficient</bibkey>
      <doi>10.18653/v1/2024.findings-acl.837</doi>
    </paper>
    <paper id="838">
      <title>Learning Fine-Grained Grounded Citations for Attributed Large Language Models</title>
      <author><first>Lei</first><last>Huang</last></author>
      <author><first>Xiaocheng</first><last>Feng</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Weitao</first><last>Ma</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yuxuan</first><last>Gu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Weihong</first><last>Zhong</last></author>
      <author><first>Xiachong</first><last>Feng</last><affiliation>University of Hong Kong</affiliation></author>
      <author><first>Weijiang</first><last>Yu</last></author>
      <author><first>Weihua</first><last>Peng</last></author>
      <author><first>Duyu</first><last>Tang</last><affiliation>Tencent AI Lab</affiliation></author>
      <author><first>Dandan</first><last>Tu</last></author>
      <author><first>Bing</first><last>Qin</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>14095-14113</pages>
      <abstract>Despite the impressive performance on information-seeking tasks, large language models (LLMs) still struggle with hallucinations. Attributed LLMs, which augment generated text with in-line citations, demonstrate potential in mitigating hallucinations and improving verifiability. However, current approaches suffer from suboptimal citation quality due to their reliance on in-context learning. Furthermore, the practice of merely citing document identifiers complicates the process for users to pinpoint specific supporting evidence. In this work, we introduce FRONT, a training framework that teaches LLMs to generate Fine-grained grounded citations. By initially grounding fine-grained supporting quotes, which then guide the generation process, these quotes not only provide supervision signals to improve citation quality but also serve as fine-grained attributions. Experiments on the ALCE benchmark demonstrate the efficacy of FRONT in generating superior grounded responses and highly supportive citations. With LLaMA-2-7B, the framework significantly outperforms all the baselines, achieving an average of 14.21% improvement in citation quality across all datasets, even surpassing ChatGPT.</abstract>
      <url hash="4e1f9b2a">2024.findings-acl.838</url>
      <bibkey>huang-etal-2024-learning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.838</doi>
    </paper>
    <paper id="839">
      <title><fixed-case>R</fixed-case>e<fixed-case>L</fixed-case>i<fixed-case>K</fixed-case>: Retrieve and <fixed-case>L</fixed-case>in<fixed-case>K</fixed-case>, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget</title>
      <author><first>Riccardo</first><last>Orlando</last></author>
      <author><first>Pere-Lluís</first><last>Huguet Cabot</last></author>
      <author><first>Edoardo</first><last>Barba</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>14114-14132</pages>
      <abstract>Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in a wide range of applications. In this paper, we propose ReLiK, a Retriever-Reader architecture for both EL and RE, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass and to fully leverage pre-trained language models contextualization capabilities, in contrast with previous Retriever-Reader-based methods, which require a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed compared to competitors. Finally, we show how our architecture can be used seamlessly for Information Extraction (cIE), i.e. EL + RE, and setting a new state of the art by employing a shared Reader that simultaneously extracts entities and relations.</abstract>
      <url hash="2cbf4cf1">2024.findings-acl.839</url>
      <bibkey>orlando-etal-2024-relik</bibkey>
      <doi>10.18653/v1/2024.findings-acl.839</doi>
    </paper>
    <paper id="840">
      <title>Synergizing Large Language Models and Pre-Trained Smaller Models for Conversational Intent Discovery</title>
      <author><first>Jinggui</first><last>Liang</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Lizi</first><last>Liao</last><affiliation>Singapore Management University</affiliation></author>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Jing</first><last>Jiang</last><affiliation>Singapore Management University</affiliation></author>
      <pages>14133-14147</pages>
      <abstract>In Conversational Intent Discovery (CID), Small Language Models (SLMs) struggle with overfitting to familiar intents and fail to label newly discovered ones. This issue stems from their limited grasp of semantic nuances and their intrinsically discriminative framework. Therefore, we propose Synergizing Large Language Models (LLMs) with pre-trained SLMs for CID (SynCID). It harnesses the profound semantic comprehension of LLMs alongside the operational agility of SLMs. By utilizing LLMs to refine both utterances and existing intent labels, SynCID significantly enhances the semantic depth, subsequently realigning these enriched descriptors within the SLMs’ feature space to correct cluster distortion and promote robust learning of representations. A key advantage is its capacity for the early identification of new intents, a critical aspect for deploying conversational agents successfully. Additionally, SynCID leverages the in-context learning strengths of LLMs to generate labels for new intents. Thorough evaluations across a wide array of datasets have demonstrated its superior performance over traditional CID methods.</abstract>
      <url hash="1321bcdd">2024.findings-acl.840</url>
      <bibkey>liang-etal-2024-synergizing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.840</doi>
    </paper>
    <paper id="841">
      <title><fixed-case>FENICE</fixed-case>: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction</title>
      <author><first>Alessandro</first><last>Scirè</last></author>
      <author><first>Karim</first><last>Ghonim</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>14148-14161</pages>
      <abstract>Recent advancements in text summarization, particularly with the advent of Large Language Models (LLMs), have shown remarkable performance. However, a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations. In response to this issue, various approaches for the evaluation of consistency for summarization have emerged. Yet, these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles), and computational impracticality, especially for LLM-based metrics. To address these shortcomings, we propose Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction (FENICE), a more interpretable and efficient factuality-oriented metric. FENICE leverages an NLI-based alignment between information in the source document and a set of atomic facts, referred to as claims, extracted from the summary. Our metric sets a new state of the art on AGGREFACT, the de-facto benchmark for factuality evaluation. Moreover, we extend our evaluation to a more challenging setting by conducting a human annotation process of long-form summarization. In the hope of fostering research in summarization factuality evaluation, we release the code of our metric and our factuality annotations of long-form summarization at https://github.com/Babelscape/FENICE.</abstract>
      <url hash="678004ec">2024.findings-acl.841</url>
      <bibkey>scire-etal-2024-fenice</bibkey>
      <doi>10.18653/v1/2024.findings-acl.841</doi>
    </paper>
    <paper id="842">
      <title>Self-Para-Consistency: Improving Reasoning Tasks at Low Cost for Large Language Models</title>
      <author><first>Wenqing</first><last>Chen</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Weicheng</first><last>Wang</last></author>
      <author><first>Zhixuan</first><last>Chu</last><affiliation>Ant Group</affiliation></author>
      <author><first>Kui</first><last>Ren</last></author>
      <author><first>Zibin</first><last>Zheng</last><affiliation>SUN YAT-SEN UNIVERSITY</affiliation></author>
      <author><first>Zhichao</first><last>Lu</last></author>
      <pages>14162-14167</pages>
      <abstract>Recently, the self-consistency decoding strategy has shown the ability to improve performance for complex reasoning tasks with large language models (LLMs). However, the costs may be high because the sampling process of the strategy generates some low-probability text, resulting in low-quality reasoning paths. As a consequence, it requires a relatively large sampling number to obtain good aggregation performance. In this paper, we propose an alternative strategy, <i>self-para-consistency</i>. It first generates multiple paraphrases for each test question, then generates reasoning paths for the original and all the paraphrased questions based on greedy decoding, and finally selects the most consistent answer. Since all the candidate paths have relatively high probabilities, the sampling number could be much smaller than the self-consistency strategy. Extensive experiments on complex reasoning datasets demonstrate the effectiveness of our method in reducing the sampling number.</abstract>
      <url hash="5d54878c">2024.findings-acl.842</url>
      <bibkey>chen-etal-2024-self-para</bibkey>
      <doi>10.18653/v1/2024.findings-acl.842</doi>
    </paper>
    <paper id="843">
      <title>Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only <fixed-case>LLM</fixed-case>s for Sequence Labeling</title>
      <author><first>David</first><last>Dukić</last><affiliation>Faculty of Electrical Engineering and Computing, University of Zagreb</affiliation></author>
      <author><first>Jan</first><last>Snajder</last><affiliation>UniZg-FER, University of Zagreb</affiliation></author>
      <pages>14168-14181</pages>
      <abstract>Pre-trained language models based on masked language modeling (MLM) excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, recent decoder-only large language models (LLMs) perform on par with smaller MLM-based encoders. Although their performance improves with scale, LLMs fall short of achieving state-of-the-art results in information extraction (IE) tasks, many of which are formulated as sequence labeling (SL). We hypothesize that LLMs’ poor SL performance stems from causal masking, which prevents the model from attending to tokens on the right of the current token. Yet, how exactly and to what extent LLMs’ performance on SL can be improved remains unclear. We explore techniques for improving the SL performance of open LLMs on IE tasks by applying layer-wise removal of the causal mask (CM) during LLM fine-tuning. This approach yields performance gains competitive with state-of-the-art SL models, matching or outperforming the results of CM removal from all blocks. Our findings hold for diverse SL tasks, demonstrating that open LLMs with layer-dependent CM removal outperform strong MLM-based encoders and even instruction-tuned LLMs.</abstract>
      <url hash="d5f50579">2024.findings-acl.843</url>
      <bibkey>dukic-snajder-2024-looking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.843</doi>
    </paper>
    <paper id="844">
      <title>m<fixed-case>CSQA</fixed-case>: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans</title>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Division of Information Science, Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>14182-14214</pages>
      <abstract>It is very challenging to curate a dataset for language-specific knowledge and common sense in order to evaluate natural language understanding capabilities of language models. Due to the limitation in the availability of annotators, most current multilingual datasets are created through translation, which cannot evaluate such language-specific aspects. Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification. Constructed dataset is a benchmark for cross-lingual language-transfer capabilities of multilingual LMs, and experimental results showed high language-transfer capabilities for questions that LMs could easily solve, but lower transfer capabilities for questions requiring deep knowledge or commonsense. This highlights the necessity of language-specific datasets for evaluation and training. Finally, our method demonstrated that multilingual LMs could create QA including language-specific knowledge, significantly reducing the dataset creation cost compared to manual creation. The datasets are available at https://huggingface.co/datasets/yusuke1997/mCSQA.</abstract>
      <url hash="87403369">2024.findings-acl.844</url>
      <bibkey>sakai-etal-2024-mcsqa</bibkey>
      <doi>10.18653/v1/2024.findings-acl.844</doi>
    </paper>
    <paper id="845">
      <title>Dual-Stage Multi-Task Syntax-Oriented Pre-Training for Syntactically Controlled Paraphrase Generation</title>
      <author><first>Hongxu</first><last>Liu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Xiaojie</first><last>Wang</last><affiliation>Beijing University of Post and Telecommunication</affiliation></author>
      <author><first>Jiashen</first><last>Sun</last></author>
      <author><first>Ke</first><last>Zeng</last></author>
      <author><first>Wan</first><last>Guanglu</last><affiliation>Meituan</affiliation></author>
      <pages>14215-14231</pages>
      <abstract>Syntactically Controlled Paraphrase Generation (SCPG), which aims at generating sentences having syntactic structures resembling given exemplars, is attracting more research efforts in recent years. We took an empirical survey on previous SCPG datasets and methods and found three tacitly approved while seldom mentioned intrinsic shortcomings/trade-offs in terms of data obtaining, task formulation, and pre-training strategies. As a mitigation to these shortcomings, we proposed a novel Dual-Stage Multi-Task (DSMT) pre-training scheme, involving a series of structure-oriented and syntax-oriented tasks, which, in our opinion, gives sequential text models the ability of com-prehending intrinsically non-sequential structures like Linearized Constituency Trees (LCTs), understanding the underlying syntactics, and even generating them by parsing sentences. We performed further pre-training of the popular T5 model on these novel tasks and fine-tuned the trained model on every possible variant of SCPG task in literature, finding that our models significantly outperformed (up to 10+ BLEU-4) previous state-of-the-art methods. Finally, we carried out ablation studies which demonstrated the effectiveness of our DSMT methods and emphasized on the SCPG performance gains compared to vanilla T5 models, especially on hard samples or under few-shot settings.</abstract>
      <url hash="1f970c1f">2024.findings-acl.845</url>
      <bibkey>liu-etal-2024-dual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.845</doi>
    </paper>
    <paper id="846">
      <title>Demonstration Augmentation for Zero-shot In-context Learning</title>
      <author><first>Yi</first><last>Su</last></author>
      <author><first>Yunpeng</first><last>Tai</last></author>
      <author><first>Yixin</first><last>Ji</last><affiliation>Soochow University</affiliation></author>
      <author><first>Juntao</first><last>Li</last><affiliation>Soochow University, China</affiliation></author>
      <author><first>Yan</first><last>Bowen</last></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>14232-14244</pages>
      <abstract>Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates.However, many studies have highlighted that the model’s performance is sensitive to the choice of demonstrations, presenting a significant challenge for practical applications where we lack prior knowledge of user queries.Consequently, we need to construct an extensive demonstration pool and incorporate external databases to assist the model, leading to considerable time and financial costs.In light of this, some recent research has shifted focus towards zero-shot ICL, aiming to reduce the model’s reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches, the content generated by the model may be unreliable, and the generation process is time-consuming.To address these issues, we propose Demonstration Augmentation for In-context Learning (DAIL), which employs the model’s previously predicted historical samples as demonstrations for subsequent ones.DAIL brings no additional inference cost and does not rely on the model’s generative capabilities.Our experiments reveal that DAIL can significantly improve the model’s performance over direct zero-shot inference and can even outperform few-shot ICL without any external information.</abstract>
      <url hash="a6d3aae2">2024.findings-acl.846</url>
      <bibkey>su-etal-2024-demonstration</bibkey>
      <doi>10.18653/v1/2024.findings-acl.846</doi>
    </paper>
    <paper id="847">
      <title>Pushing the Limits of Zero-shot End-to-End Speech Translation</title>
      <author><first>Ioannis</first><last>Tsiamas</last><affiliation>Apple and Universidad Politécnica de Cataluna</affiliation></author>
      <author><first>Gerard I.</first><last>Gállego</last><affiliation>Universidad Politécnica de Cataluna</affiliation></author>
      <author><first>José</first><last>Fonollosa</last><affiliation>Universitat Politècnica de Catalunya</affiliation></author>
      <author><first>Marta</first><last>Costa-jussà</last><affiliation>Meta</affiliation></author>
      <pages>14245-14267</pages>
      <abstract>Data scarcity and the modality gap between the speech and text modalities are two major obstacles of end-to-end Speech Translation (ST) systems, thus hindering their performance. Prior work has attempted to mitigate these challenges by leveraging external MT data and optimizing distance metrics that bring closer the speech-text representations. However, achieving competitive results typically requires some ST data. For this reason, we introduce ZeroSwot, a method for zero-shot ST that bridges the modality gap without any paired ST data. Leveraging a novel CTC compression and Optimal Transport, we train a speech encoder using only ASR data, to align with the representation space of a massively multilingual MT model. The speech encoder seamlessly integrates with the MT model at inference, enabling direct translation from speech to text, across all languages supported by the MT model. Our experiments show that we can effectively close the modality gap without ST data, while our results on MuST-C and CoVoST demonstrate our method’s superiority over not only previous zero-shot models, but also supervised ones, achieving state-of-the-art results.</abstract>
      <url hash="5e4faa95">2024.findings-acl.847</url>
      <bibkey>tsiamas-etal-2024-pushing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.847</doi>
    </paper>
    <paper id="848">
      <title><fixed-case>NUMC</fixed-case>o<fixed-case>T</fixed-case>: Numerals and Units of Measurement in Chain-of-Thought Reasoning using Large Language Models</title>
      <author><first>Ancheng</first><last>Xu</last></author>
      <author><first>Minghuan</first><last>Tan</last><affiliation>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Lei</first><last>Wang</last><affiliation>SalesForce</affiliation></author>
      <author><first>Min</first><last>Yang</last><affiliation>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences</affiliation></author>
      <author><first>Ruifeng</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>14268-14290</pages>
      <abstract>Numeral systems and units of measurement are two conjoined topics in activities of human beings and have mutual effects with the languages expressing them. Currently, the evaluation of Large Language Models (LLMs) often involves mathematical reasoning, yet little attention is given to how minor changes in numbers or units can drastically alter the complexity of problems and the performance of LLMs. In this paper, we scrutinize existing LLMs on processing of numerals and units of measurement by constructing datasets with perturbations. We first anatomize the reasoning of math word problems to different sub-procedures like numeral conversions from language to numbers and measurement conversions based on units. Then we further annotate math word problems from ancient Chinese arithmetic works which are challenging in numerals and units of measurement. Experiments on perturbed datasets demonstrate that LLMs still encounter difficulties in handling numeral and measurement conversions.</abstract>
      <url hash="dae824c5">2024.findings-acl.848</url>
      <bibkey>xu-etal-2024-numcot</bibkey>
      <doi>10.18653/v1/2024.findings-acl.848</doi>
    </paper>
    <paper id="849">
      <title>On The Persona-based Summarization of Domain-Specific Documents</title>
      <author><first>Ankan</first><last>Mullick</last></author>
      <author><first>Sombit</first><last>Bose</last><affiliation>Indian Institute of Technology Kharagpur,</affiliation></author>
      <author><first>Rounak</first><last>Saha</last></author>
      <author><first>Ayan</first><last>Bhowmick</last><affiliation>Merlyn Mind Inc.</affiliation></author>
      <author><first>Pawan</first><last>Goyal</last><affiliation>IIT Kharagpur</affiliation></author>
      <author><first>Niloy</first><last>Ganguly</last><affiliation>Indian Institute of Technology Kharagpur,</affiliation></author>
      <author><first>Prasenjit</first><last>Dey</last></author>
      <author><first>Ravi</first><last>Kokku</last></author>
      <pages>14291-14307</pages>
      <abstract>In an ever-expanding world of domain-specific knowledge, the increasing complexity of consuming, and storing information necessitates the generation of summaries from large information repositories. However, every persona of a domain has different requirements of information and hence their summarization. For example, in the healthcare domain, a persona-based (such as Doctor, Nurse, Patient etc.) approach is imperative to deliver targeted medical information efficiently. Persona-based summarization of domain-specific information by humans is a high cognitive load task and is generally not preferred. The summaries generated by two different humans have high variability and do not scale in cost and subject matter expertise as domains and personas grow. Further, AI-generated summaries using generic Large Language Models (LLMs) may not necessarily offer satisfactory accuracy for different domains unless they have been specifically trained on domain-specific data and can also be very expensive to use in day-to-day operations. Our contribution in this paper is two-fold: 1) We present an approach to efficiently fine-tune a domain-specific small foundation LLM using a healthcare corpus and also show that we can effectively evaluate the summarization quality using AI-based critiquing. 2) We further show that AI-based critiquing has good concordance with Human-based critiquing of the summaries. Hence, such AI-based pipelines to generate domain-specific persona-based summaries can be easily scaled to other domains such as legal, enterprise documents, education etc. in a very efficient and cost-effective manner.</abstract>
      <url hash="88a49504">2024.findings-acl.849</url>
      <bibkey>mullick-etal-2024-persona</bibkey>
      <doi>10.18653/v1/2024.findings-acl.849</doi>
    </paper>
    <paper id="850">
      <title>Evaluating Large Language Models for Health-related Queries with Presuppositions</title>
      <author><first>Navreet</first><last>Kaur</last><affiliation>Indian Institute of Science, Indian institute of science, Bangalore</affiliation></author>
      <author><first>Monojit</first><last>Choudhury</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Danish</first><last>Pruthi</last><affiliation>Indian Institute of Science, Bangalore</affiliation></author>
      <pages>14308-14331</pages>
      <abstract>As corporations rush to integrate large language models (LLMs) it is critical that they provide factually accurate information, that is robust to any presuppositions that a user may express. In this work, we introduce UPHILL, a dataset consisting of health-related queries with varying degrees of presuppositions. Using UPHILL, we evaluate the factual accuracy and consistency of InstructGPT, ChatGPT, GPT-4 and Bing Copilot models. We find that while model responses rarely contradict true health claims (posed as questions), all investigated models fail to challenge false claims. Alarmingly, responses from these models agree with 23-32% of the existing false claims, and 49-55% with novel fabricated claims. As we increase the extent of presupposition in input queries, responses from all models except Bing Copilot agree with the claim considerably more often, regardless of its veracity. Given the moderate factual accuracy, and the inability of models to challenge false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios.</abstract>
      <url hash="5a6f7f07">2024.findings-acl.850</url>
      <bibkey>kaur-etal-2024-evaluating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.850</doi>
    </paper>
    <paper id="851">
      <title>Word Sense Linking: Disambiguating Outside the Sandbox</title>
      <author><first>Andrei</first><last>Bejgu</last></author>
      <author><first>Edoardo</first><last>Barba</last><affiliation>University of Roma “La Sapienza”</affiliation></author>
      <author><first>Luigi</first><last>Procopio</last></author>
      <author><first>Alberte</first><last>Fernández-Castro</last></author>
      <author><first>Roberto</first><last>Navigli</last><affiliation>Sapienza University of Rome</affiliation></author>
      <pages>14332-14347</pages>
      <abstract>Word Sense Disambiguation (WSD) is the task of associating a word in a given context with its most suitable meaning among a set of possible candidates. While the task has recently witnessed renewed interest, with systems achieving performances above the estimated inter-annotator agreement, at the time of writing it still struggles to find downstream applications. We argue that one of the reasons behind this is the difficulty of applying WSD to plain text. Indeed, in the standard formulation, models work under the assumptions that a) all the spans to disambiguate have already been identified, and b) all the possible candidate senses of each span are provided, both of which are requirements that are far from trivial. In this work, we present a new task called Word Sense Linking (WSL) where, given an input text and a reference sense inventory, systems have to both identify which spans to disambiguate and then link them to their most suitable meaning.We put forward a transformer-based architecture for the task and thoroughly evaluate both its performance and those of state-of-the-art WSD systems scaled to WSL, iteratively relaxing the assumptions of WSD. We hope that our work will foster easier integration of lexical semantics into downstream applications.</abstract>
      <url hash="eaa659d6">2024.findings-acl.851</url>
      <bibkey>bejgu-etal-2024-word</bibkey>
      <doi>10.18653/v1/2024.findings-acl.851</doi>
    </paper>
    <paper id="852">
      <title>Generalisation First, Memorisation Second? Memorisation Localisation for Natural Language Classification Tasks</title>
      <author><first>Verna</first><last>Dankers</last></author>
      <author><first>Ivan</first><last>Titov</last><affiliation>University of Edinburgh and University of Amsterdam</affiliation></author>
      <pages>14348-14366</pages>
      <abstract>Memorisation is a natural part of learning from real-world data: neural models pick up on atypical input-output combinations and store those training examples in their parameter space. That this happens is well-known, but how and where are questions that remain largely unanswered. Given a multi-layered neural model, where does memorisation occur in the millions of parameters?Related work reports conflicting findings: a dominant hypothesis based on image classification is that lower layers learn generalisable features and that deeper layers specialise and memorise. Work from NLP suggests this does not apply to language models, but has been mainly focused on memorisation of facts.We expand the scope of the localisation question to 12 natural language classification tasks and apply 4 memorisation localisation techniques.Our results indicate that memorisation is a gradual process rather than a localised one, establish that memorisation is task-dependent, and give nuance to the generalisation first, memorisation second hypothesis.</abstract>
      <url hash="69476010">2024.findings-acl.852</url>
      <bibkey>dankers-titov-2024-generalisation</bibkey>
      <doi>10.18653/v1/2024.findings-acl.852</doi>
    </paper>
    <paper id="853">
      <title>Towards Multi-Relational Multi-Hop Reasoning over Dense Temporal Knowledge Graphs</title>
      <author><first>Jian</first><last>Liu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <author><first>Zihe</first><last>Liu</last></author>
      <author><first>Xueqiang</first><last>Lyu</last><affiliation>Beijing Information Science And Technology University</affiliation></author>
      <author><first>Peng</first><last>Jin</last></author>
      <author><first>Jinan</first><last>Xu</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>14367-14378</pages>
      <abstract>Temporal knowledge graph reasoning has emerged as a crucial task for answering time-dependent questions within a knowledge graph (KG).Despite tremendous progress, the present research is impeded by the sparsity of a temporal KG and an over-reliance on simple single-relational reasoning patterns. To overcome these challenges, we introduce MulQuestions, a new temporal KG reasoning benchmark featuring over 200k entities and 960k questions designed to facilitate complex, multi-relational and multi-hop reasoning. Additionally, we propose a new model adept at conducting pattern-aware and time-sensitive reasoning across temporal KGs. The model’s efficacy is confirmed through rigorous evaluations, showcasing its effectiveness in sparse data conditions and adeptness at handling questions with long reasoning chains. We have made our benchmark and model publicly accessible at [https://anonymous].</abstract>
      <url hash="c8042a1a">2024.findings-acl.853</url>
      <bibkey>liu-etal-2024-towards-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.853</doi>
    </paper>
    <paper id="854">
      <title>Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models</title>
      <author><first>Weihang</first><last>Su</last></author>
      <author><first>Changyue</first><last>Wang</last></author>
      <author><first>Qingyao</first><last>Ai</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yiran</first><last>Hu</last></author>
      <author><first>Zhijing</first><last>Wu</last><affiliation>Beijing Institute of Technology</affiliation></author>
      <author><first>Yujia</first><last>Zhou</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yiqun</first><last>Liu</last><affiliation>Tsinghua University</affiliation></author>
      <pages>14379-14391</pages>
      <abstract>Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM’s inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.</abstract>
      <url hash="7dd8031a">2024.findings-acl.854</url>
      <bibkey>su-etal-2024-unsupervised</bibkey>
      <doi>10.18653/v1/2024.findings-acl.854</doi>
    </paper>
    <paper id="855">
      <title>Progressive Tuning: Towards Generic Sentiment Abilities for Large Language Models</title>
      <author><first>Guiyang</first><last>Hou</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Weiming</first><last>Lu</last><affiliation>Zhejiang University</affiliation></author>
      <pages>14392-14402</pages>
      <abstract>Understanding sentiment is arguably an advanced and important capability of AI agents in the physical world. In previous works, many efforts have been devoted to individual sentiment subtasks, without considering interrelated sentiment knowledge among these subtasks. Although some recent works model multiple sentiment subtasks in a unified manner, they merely simply combine these subtasks without deeply exploring the hierarchical relationships among subtasks. In this paper, we introduce GSA-7B, an open-source large language model specific to the sentiment domain. Specifically, we deeply explore the hierarchical relationships between sentiment subtasks, proposing progressive sentiment reasoning benchmark and progressive task instructions. Subsequently, we use Llama2-7B as the backbone model and propose parameter-efficient progressive tuning paradigm which is implemented by constructing chain of LoRA, resulting in the creation of GSA-7B. Experimental results show that GSA-7B as a unified model performs well across all datasets in the progressive sentiment reasoning benchmark. Additionally, under the few-shot setting, GSA-7B also exhibits good generalization ability for sentiment subtasks and datasets that were not encountered during its training phase.</abstract>
      <url hash="bebfacce">2024.findings-acl.855</url>
      <bibkey>hou-etal-2024-progressive</bibkey>
      <doi>10.18653/v1/2024.findings-acl.855</doi>
    </paper>
    <paper id="856">
      <title>Fooling the Textual Fooler via Randomizing Latent Representations</title>
      <author><first>Duy</first><last>Hoang</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Nguyen</first><last>Hung-Quang</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Saurav</first><last>Manchanda</last><affiliation>Amazon</affiliation></author>
      <author><first>Minlong</first><last>Peng</last><affiliation>Baidu</affiliation></author>
      <author><first>Kok-Seng</first><last>Wong</last><affiliation>VinUniversity</affiliation></author>
      <author><first>Khoa</first><last>Doan</last><affiliation>VinUniversity</affiliation></author>
      <pages>14403-14421</pages>
      <abstract>Despite outstanding performance in a variety of Natural Language Processing (NLP) tasks, recent studies have revealed that NLP models are vulnerable to adversarial attacks that slightly perturb the input to cause the models to misbehave. Several attacks can even compromise the model without requiring access to the model architecture or model parameters (i.e., a blackbox setting), and thus are detrimental to existing NLP applications. To perform these attacks, the adversary queries the victim model many times to determine the most important parts in an input text and transform. In this work, we propose a lightweight and attack-agnostic defense whose main goal is to perplex the process of generating an adversarial example in these query-based black-box attacks; that is to fool the textual fooler. This defense, named AdvFooler, works by randomizing the latent representation of the input at inference time. Different from existing defenses, AdvFooler does not necessitate additional computational overhead during training nor does it rely on assumptions about the potential adversarial perturbation set while having a negligible impact on the model’s accuracy. Our theoretical and empirical analyses highlight the significance of robustness resulting from confusing the adversary via randomizing the latent space, as well as the impact of randomization on clean accuracy. Finally, we empirically demonstrate near state-of-the-art robustness of AdvFooler against representative adversarial attacks on two benchmark datasets.</abstract>
      <url hash="ef2a095e">2024.findings-acl.856</url>
      <bibkey>hoang-etal-2024-fooling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.856</doi>
    </paper>
    <paper id="857">
      <title>Part-of-speech Tagging for Extremely Low-resource <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Sanjeev</first><last>Kumar</last></author>
      <author><first>Preethi</first><last>Jyothi</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last><affiliation>Indian Institute of Technology, Bombay, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>14422-14431</pages>
      <abstract>Modern natural language processing (NLP) systems thrive when given access to large datasets. However, a large fraction of the world’s languages are not privy to such benefits due to sparse documentation and inadequate digital representation. This is especially true for Indian regional languages. As a first step towards expanding the reach of NLP technologies to extremely low-resource Indian languages, we present a new parallel part-of-speech (POS) evaluation dataset for Angika, Magahi, Bhojpuri and Hindi. Angika, Magahi, Bhojpuri, along with the more well-known Hindi, are all languages spoken in the Indian states of Bihar, Jharkhand and West Bengal. Ours is notably the first NLP resource, even for a shallow NLP task like POS-tagging, for Angika. We establish POS-tagging baselines using state-of-the-art multilingual pretrained language models (PLMs) finetuned on Hindi data, and show zero-shot evaluations on the other three languages. While all four languages use the same Devanagari script, pretrained tokenizers underperform in zero-shot on the three languages. We propose a simple look-back fix to address the tokenization challenge yielding F1-score improvements of up to 8% on Angika and show how it comes very close to an oracle setting when the underlying Hindi word is known (and can be accurately tokenized).</abstract>
      <url hash="d4075694">2024.findings-acl.857</url>
      <bibkey>kumar-etal-2024-part</bibkey>
      <doi>10.18653/v1/2024.findings-acl.857</doi>
    </paper>
    <paper id="858">
      <title><fixed-case>FOCUS</fixed-case>: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models</title>
      <author><first>Kaixin</first><last>Lan</last></author>
      <author><first>Tao</first><last>Fang</last><affiliation>University of Macau</affiliation></author>
      <author><first>Derek</first><last>Wong</last><affiliation>University of Macau</affiliation></author>
      <author><first>Yabo</first><last>Xu</last></author>
      <author><first>Lidia</first><last>Chao</last></author>
      <author><first>Cecilia</first><last>Zhao</last><affiliation>University of Macau, New York University and Ohio State University, Columbus</affiliation></author>
      <pages>14432-14447</pages>
      <abstract>Pre-trained Language Models (PLMs) have shown impressive results in various Natural Language Generation (NLG) tasks, such as powering chatbots and generating stories. However, an ethical concern arises due to their potential to produce verbatim copies of paragraphs from their training data. This is problematic as PLMs are trained on corpora constructed by human authors. As such, there is a pressing need for research to promote the generation of original content by these models. In this study, we introduce a unique “self-plagiarism” contrastive decoding strategy, aimed at boosting the originality of text produced by PLMs. Our method entails modifying prompts in LLMs to develop an amateur model and a professional model. Specifically, the amateur model is urged to plagiarize using three plagiarism templates we have designed, while the professional model maintains its standard language model status. This strategy employs prompts to stimulate the model’s capacity to identify non-original candidate token combinations and subsequently impose penalties. The application of this strategy is integrated prior to the model’s final layer, ensuring smooth integration with most existing PLMs (T5, GPT, LLaMA) without necessitating further adjustments. Implementing our strategy, we noted a significant decline in non-original sequences comprised of more than three words in the academic AASC dataset and the story-based ROCStories dataset. Source code and scripts will be released after the paper’s acceptance and publication.</abstract>
      <url hash="bdb52674">2024.findings-acl.858</url>
      <bibkey>lan-etal-2024-focus</bibkey>
      <doi>10.18653/v1/2024.findings-acl.858</doi>
    </paper>
    <paper id="859">
      <title>Amanda: Adaptively Modality-Balanced Domain Adaptation for Multimodal Emotion Recognition</title>
      <author><first>Xinxin</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Sun</last></author>
      <author><first>Simin</first><last>Hong</last></author>
      <author><first>Taihao</first><last>Li</last><affiliation>Zhejiang Lab</affiliation></author>
      <pages>14448-14458</pages>
      <abstract>This paper investigates unsupervised multimodal domain adaptation for multimodal emotion recognition, which is a solution for data scarcity yet remains under studied. Due to the varying distribution discrepancies of different modalities between source and target domains, the primary challenge lies in how to balance the domain alignment across modalities to guarantee they are all well aligned. To achieve this, we first develop our model based on the information bottleneck theory to learn optimal representation for each modality independently. Then, we align the domains via matching the label distributions and the representations. In order to balance the representation alignment, we propose to minimize a surrogate of the alignment losses, which is equivalent to adaptively adjusting the weights of the modalities throughout training, thus achieving balanced domain alignment across modalities. Overall, the proposed approach features <b>A</b>daptively <b>m</b>odality-bal<b>an</b>ced <b>d</b>omain <b>a</b>daptation, dubbed <b>Amanda</b>, for multimodal emotion recognition. Extensive empirical results on commonly used benchmark datasets demonstrate that Amanda significantly outperforms competing approaches. The code is available at <url>https://github.com/sunjunaimer/Amanda</url>.</abstract>
      <url hash="56ece6c8">2024.findings-acl.859</url>
      <bibkey>zhang-etal-2024-amanda</bibkey>
      <doi>10.18653/v1/2024.findings-acl.859</doi>
    </paper>
    <paper id="860">
      <title><fixed-case>M</fixed-case>ed<fixed-case>REQAL</fixed-case>: Examining Medical Knowledge Recall of Large Language Models via Question Answering</title>
      <author><first>Juraj</first><last>Vladika</last><affiliation>Technische Universität München</affiliation></author>
      <author><first>Phillip</first><last>Schneider</last></author>
      <author><first>Florian</first><last>Matthes</last><affiliation>Technische Universität München</affiliation></author>
      <pages>14459-14469</pages>
      <abstract>In recent years, Large Language Models (LLMs) have demonstrated an impressive ability to encode knowledge during pre-training on large text corpora. They can leverage this knowledge for downstream tasks like question answering (QA), even in complex areas involving health topics. Considering their high potential for facilitating clinical work in the future, understanding the quality of encoded medical knowledge and its recall in LLMs is an important step forward. In this study, we examine the capability of LLMs to exhibit medical knowledge recall by constructing a novel dataset derived from systematic reviews – studies synthesizing evidence-based answers for specific medical questions. Through experiments on the new MedREQAL dataset, comprising question-answer pairs extracted from rigorous systematic reviews, we assess six LLMs, such as GPT and Mixtral, analyzing their classification and generation performance. Our experimental insights into LLM performance on the novel biomedical QA dataset reveal the still challenging nature of this task.</abstract>
      <url hash="cf408afd">2024.findings-acl.860</url>
      <bibkey>vladika-etal-2024-medreqal</bibkey>
      <doi>10.18653/v1/2024.findings-acl.860</doi>
    </paper>
    <paper id="861">
      <title>Deepfake Defense: Constructing and Evaluating a Specialized <fixed-case>U</fixed-case>rdu Deepfake Audio Dataset</title>
      <author><first>Sheza</first><last>Munir</last></author>
      <author><first>Wassay</first><last>Sajjad</last></author>
      <author><first>Mukeet</first><last>Raza</last><affiliation>Lahore University of Management Sciences</affiliation></author>
      <author><first>Emaan</first><last>Abbas</last></author>
      <author><first>Abdul Hameed</first><last>Azeemi</last><affiliation>Lahore University of Management Sciences</affiliation></author>
      <author><first>Ihsan Ayyub</first><last>Qazi</last><affiliation>Lahore University of Management Sciences</affiliation></author>
      <author><first>Agha Ali</first><last>Raza</last><affiliation>Lahore University of Management Sciences</affiliation></author>
      <pages>14470-14480</pages>
      <abstract>Deepfakes, particularly in the auditory domain, have become a significant threat, necessitating the development of robust countermeasures. This paper addresses the escalating challenges posed by deepfake attacks on Automatic Speaker Verification (ASV) systems. We present a novel Urdu deepfake audio dataset for deepfake detection, focusing on two spoofing attacks – Tacotron and VITS TTS. The dataset construction involves careful consideration of phonemic cover and balance and comparison with existing corpora like PRUS and PronouncUR. Evaluation with AASIST-L model shows EERs of 0.495 and 0.524 for VITS TTS and Tacotron-generated audios, respectively, with variability across speakers. Further, this research implements a detailed human evaluation, incorporating a user study to gauge whether people are able to discern deepfake audios from real (bonafide) audios. The ROC curve analysis shows an area under the curve (AUC) of 0.63, indicating that individuals demonstrate a limited ability to detect deepfakes (approximately 1 in 3 fake audio samples are regarded as real). Our work contributes a valuable resource for training deepfake detection models in low-resource languages like Urdu, addressing the critical gap in existing datasets. The dataset is publicly available at: https://github.com/CSALT-LUMS/urdu-deepfake-dataset.</abstract>
      <url hash="ac3e39fe">2024.findings-acl.861</url>
      <bibkey>munir-etal-2024-deepfake</bibkey>
      <doi>10.18653/v1/2024.findings-acl.861</doi>
    </paper>
    <paper id="862">
      <title>Leveraging Entailment Judgements in Cross-Lingual Summarisation</title>
      <author><first>Huajian</first><last>Zhang</last></author>
      <author><first>Laura</first><last>Perez-Beltrachini</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>14481-14497</pages>
      <abstract>Synthetically created Cross-Lingual Summarisation (CLS) datasets are prone to include document-summary pairs where the reference summary is unfaithful to the corresponding document as it contains content not supported by the document (i.e., hallucinated content). This low data quality misleads model learning and obscures evaluation results. Automatic ways to assess hallucinations and improve training have been proposed for monolingual summarisation, predominantly in English. For CLS, we propose to use off-the-shelf cross-lingual Natural Language Inference (X-NLI) to evaluate faithfulness of reference and model generated summaries. Then, we study training approaches that are aware of faithfulness issues in the training data and propose an approach that uses unlikelihood loss to teach a model about unfaithful summary sequences. Our results show that it is possible to train CLS models that yield more faithful and at the same time informative summaries.</abstract>
      <url hash="0a7c56e4">2024.findings-acl.862</url>
      <bibkey>zhang-perez-beltrachini-2024-leveraging</bibkey>
      <doi>10.18653/v1/2024.findings-acl.862</doi>
    </paper>
    <paper id="863">
      <title>Recognizing Everything from All Modalities at Once: Grounded Multimodal Universal Information Extraction</title>
      <author><first>Meishan</first><last>Zhang</last><affiliation>Harbin Institute of Technology (Shenzhen), China and Tianjin University, China</affiliation></author>
      <author><first>Hao</first><last>Fei</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Shengqiong</first><last>Wu</last></author>
      <author><first>Yixin</first><last>Cao</last><affiliation>Fudan University</affiliation></author>
      <author><first>Fei</first><last>Li</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <pages>14498-14511</pages>
      <abstract>In the field of information extraction (IE), tasks across a wide range of modalities and their combinations have been traditionally studied in isolation, leaving a gap in deeply recognizing and analyzing cross-modal information. To address this, this work for the first time introduces the concept of grounded Multimodal Universal Information Extraction (MUIE), providing a unified task framework to analyze any IE tasks over various modalities, along with their fine-grained groundings. To tackle MUIE, we tailor a multimodal large language model (MLLM), Reamo, capable of extracting and grounding information from all modalities, i.e., recognizing everything from all modalities at once. Reamo is updated via varied tuning strategies, equipping it with powerful capabilities for information recognition and fine-grained multimodal grounding. To address the absence of a suitable benchmark for grounded MUIE, we curate a high-quality, diverse, and challenging test set, which encompasses IE tasks across 9 common modality combinations with the corresponding multimodal groundings. The extensive comparison of Reamo with existing MLLMs integrated into pipeline approaches demonstrates its advantages across all evaluation dimensions, establishing a strong benchmark for the follow-up research. Our resources are publicly released at https://haofei.vip/MUIE.</abstract>
      <url hash="1d29e927">2024.findings-acl.863</url>
      <bibkey>zhang-etal-2024-recognizing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.863</doi>
    </paper>
    <paper id="864">
      <title>Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data</title>
      <author><first>Yanda</first><last>Li</last></author>
      <author><first>Chi</first><last>Zhang</last><affiliation>Westlake University</affiliation></author>
      <author><first>Gang</first><last>Yu</last><affiliation>Tencent</affiliation></author>
      <author><first>Wanqi</first><last>Yang</last></author>
      <author><first>Zhibin</first><last>Wang</last><affiliation>Tencent LightAI Lab</affiliation></author>
      <author><first>Bin</first><last>Fu</last></author>
      <author><first>Guosheng</first><last>Lin</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Chunhua</first><last>Shen</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Ling</first><last>Chen</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Yunchao</first><last>Wei</last><affiliation>Beijing Jiaotong University</affiliation></author>
      <pages>14512-14531</pages>
      <abstract>The remarkable multimodal capabilities demonstrated by OpenAI’s GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions.Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets using the open-source LLAVA model as a testbed for our proposed pipeline. Our results underscore marked enhancements across more than ten commonly assessed capabilities.</abstract>
      <url hash="4d5beb0e">2024.findings-acl.864</url>
      <bibkey>li-etal-2024-enhanced</bibkey>
      <doi>10.18653/v1/2024.findings-acl.864</doi>
    </paper>
    <paper id="865">
      <title>Modeling Overregularization in Children with Small Language Models</title>
      <author><first>Akari</first><last>Haga</last></author>
      <author><first>Saku</first><last>Sugawara</last><affiliation>National Institute of Informatics</affiliation></author>
      <author><first>Akiyo</first><last>Fukatsu</last><affiliation>Tokyo University, Tokyo Institute of Technology</affiliation></author>
      <author><first>Miyu</first><last>Oba</last></author>
      <author><first>Hiroki</first><last>Ouchi</last><affiliation>NAIST</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Yohei</first><last>Oseki</last><affiliation>University of Tokyo</affiliation></author>
      <pages>14532-14550</pages>
      <abstract>The imitation of the children’s language acquisition process has been explored to make language models (LMs) more efficient.In particular, errors caused by children’s regularization (so-called overregularization, e.g., using wroted for the past tense of write) have been widely studied to reveal the mechanisms of language acquisition. Existing research has analyzed regularization in language acquisition only by modeling word inflection directly, which is unnatural in light of human language acquisition. In this paper, we hypothesize that language models that imitate the errors children make during language acquisition have a learning process more similar to humans. To verify this hypothesis, we analyzed the learning curve and error preferences of verb inflections in small-scale LMs using acceptability judgments. We analyze the differences in results by model architecture, data, and tokenization. Our model shows child-like U-shaped learning curves clearly for certain verbs, but the preferences for types of overgeneralization did not fully match the observations in children.</abstract>
      <url hash="acc626c7">2024.findings-acl.865</url>
      <bibkey>haga-etal-2024-modeling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.865</doi>
    </paper>
    <paper id="866">
      <title>Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative <fixed-case>LLM</fixed-case>s Reflect Lexical Semantics</title>
      <author><first>Zhu</first><last>Liu</last></author>
      <author><first>Cunliang</first><last>Kong</last></author>
      <author><first>Ying</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>14551-14558</pages>
      <abstract>Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance via the hidden states for the last meaningless symbols, such as punctuation, in the prompting strategy. Our codes are available at https://github.com/RyanLiut/LLM_LexSem.</abstract>
      <url hash="68762475">2024.findings-acl.866</url>
      <bibkey>liu-etal-2024-fantastic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.866</doi>
    </paper>
    <paper id="867">
      <title>Harnessing Large Language Models as Post-hoc Correctors</title>
      <author><first>Zhiqiang</first><last>Zhong</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Kuangyu</first><last>Zhou</last><affiliation>Microsoft</affiliation></author>
      <author><first>Davide</first><last>Mottin</last><affiliation>Aarhus University</affiliation></author>
      <pages>14559-14574</pages>
      <abstract>As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML’s performance at a minimal cost? We show that, through our proposed training-free framework LLMCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset’s label information and the ML model’s predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can transfer its acquired knowledge to suggest corrections for the ML model’s predictions. Our experimental results on text analysis and the challenging molecular predictions show that LLMCorr improves the performance of a number of models by up to 39%.</abstract>
      <url hash="4194a9f1">2024.findings-acl.867</url>
      <bibkey>zhong-etal-2024-harnessing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.867</doi>
    </paper>
    <paper id="868">
      <title>Debatrix: Multi-dimensional Debate Judge with Iterative Chronological Analysis Based on <fixed-case>LLM</fixed-case></title>
      <author><first>Jingcong</first><last>Liang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Rong</first><last>Ye</last><affiliation>ByteDance</affiliation></author>
      <author><first>Meng</first><last>Han</last></author>
      <author><first>Ruofei</first><last>Lai</last></author>
      <author><first>Xinyu</first><last>Zhang</last><affiliation>Huawei Technologies Ltd.</affiliation></author>
      <author><first>Xuanjing</first><last>Huang</last><affiliation>Fudan University</affiliation></author>
      <author><first>Zhongyu</first><last>Wei</last><affiliation>Fudan University</affiliation></author>
      <pages>14575-14595</pages>
      <abstract>How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments.At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate.In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration.To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system’s performance to actual debate outcomes.The findings indicate a notable enhancement over directly using LLMs for debate evaluation.Source code and benchmark data are available at https://github.com/ljcleo/debatrix.</abstract>
      <url hash="6538bbbf">2024.findings-acl.868</url>
      <bibkey>liang-etal-2024-debatrix</bibkey>
      <doi>10.18653/v1/2024.findings-acl.868</doi>
    </paper>
    <paper id="869">
      <title><fixed-case>C</fixed-case>ycle<fixed-case>A</fixed-case>lign: Iterative Distillation from Black-box <fixed-case>LLM</fixed-case> to White-box Models for Better Human Alignment</title>
      <author><first>Jixiang</first><last>Hong</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Quan</first><last>Tu</last></author>
      <author><first>Changyu</first><last>Chen</last><affiliation>Renmin University of China</affiliation></author>
      <author><first>Gao</first><last>Xing</last></author>
      <author><first>Ji</first><last>Zhang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>14596-14609</pages>
      <abstract>Language models trained on large-scale corpus often generate harmful responses that are harmful and contrary to human values. A prevalent approach for human alignment is reinforcement learning from human feedback (RLHF), utilizing algorithms such as proximal policy optimization (PPO). However, these methods are often characterized by complexity, instability, and substantial resource consumption. Considering that existing large language models (LLMs) like ChatGPT are already relatively well-aligned and cost-friendly, researchers propose to align the language model with human preferences from AI feedback. Nevertheless, the common practices, that unidirectionally distill the responses, are constrained by the inherent capability of LLMs. To address it, we introduce CycleAlign, a framework that distills alignment capabilities from the parameter-invisible LLMs (black-box) to the parameter-visible models (white-box) in an iterative manner. CycleAlign iteratively improves both the white-box and black-box models by integrating static and dynamic in-context learning and a belief alignment method.Empirical results illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing methods, and achieves the state-of-the-art performance in alignment with human value.</abstract>
      <url hash="243e2bc4">2024.findings-acl.869</url>
      <bibkey>hong-etal-2024-cyclealign</bibkey>
      <doi>10.18653/v1/2024.findings-acl.869</doi>
    </paper>
    <paper id="870">
      <title>Towards a new research agenda for multimodal enterprise document understanding: What are we missing?</title>
      <author><first>Armineh</first><last>Nourbakhsh</last><affiliation>School of Computer Science, Carnegie Mellon University and J.P. Morgan Chase</affiliation></author>
      <author><first>Sameena</first><last>Shah</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>14610-14622</pages>
      <abstract>The field of multimodal document understanding has produced a suite of models that have achieved stellar performance across several tasks, even coming close to human performance on certain benchmarks. Nevertheless, the application of these models to real-world enterprise datasets remains constrained by a number of limitations. In this position paper, we discuss these limitations in the context of three key aspects of research: dataset curation, model development, and evaluation on downstream tasks. By analyzing 14 datasets and 7 SotA models, we identify major gaps in their utility in the context of a real-world scenario. We demonstrate how each limitation impedes the widespread use of SotA models in enterprise settings, and present a set of research challenges that are motivated by these limitations. Lastly, we propose a research agenda that is aimed at driving the field towards higher impact in enterprise applications.</abstract>
      <url hash="33c0aee6">2024.findings-acl.870</url>
      <bibkey>nourbakhsh-etal-2024-towards</bibkey>
      <doi>10.18653/v1/2024.findings-acl.870</doi>
    </paper>
    <paper id="871">
      <title><fixed-case>CAUSE</fixed-case>: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems</title>
      <author><first>Amin</first><last>Abolghasemi</last></author>
      <author><first>Zhaochun</first><last>Ren</last><affiliation>Leiden University</affiliation></author>
      <author><first>Arian</first><last>Askari</last></author>
      <author><first>Mohammad</first><last>Aliannejadi</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Maarten</first><last>Rijke</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Suzan</first><last>Verberne</last><affiliation>Universiteit Leiden</affiliation></author>
      <pages>14623-14635</pages>
      <abstract>An important unexplored aspect in previous work on user satisfaction estimation for Task-Oriented Dialogue (TOD) systems is their evaluation in terms of robustness for the identification of user dissatisfaction: current benchmarks for user satisfaction estimation in TOD systems are highly skewed towards dialogues for which the user is satisfied. The effect of having a more balanced set of satisfaction labels on performance is unknown. However, balancing the data with more dissatisfactory dialogue samples requires further data collection and human annotation, which is costly and time-consuming. In this work, we leverage large language models (LLMs) and unlock their ability to generate satisfaction-aware counterfactual dialogues to augment the set of original dialogues of a test collection. We gather human annotations to ensure the reliability of the generated samples. We evaluate two open-source LLMs as user satisfaction estimators on our augmented collection against state-of-the-art fine-tuned models. Our experiments show that when used as few-shot user satisfaction estimators, open-source LLMs show higher robustness to the increase in the number of dissatisfaction labels in the test collection than the fine-tuned state-of-the-art models. Our results shed light on the need for data augmentation approaches for user satisfaction estimation in TOD systems. We release our aligned counterfactual dialogues, which are curated by human annotation, to facilitate further research on this topic.</abstract>
      <url hash="f937feee">2024.findings-acl.871</url>
      <bibkey>abolghasemi-etal-2024-cause</bibkey>
      <doi>10.18653/v1/2024.findings-acl.871</doi>
    </paper>
    <paper id="872">
      <title>Measuring Retrieval Complexity in Question Answering Systems</title>
      <author><first>Matteo</first><last>Gabburo</last><affiliation>University of Trento</affiliation></author>
      <author><first>Nicolaas</first><last>Jedema</last><affiliation>Amazon</affiliation></author>
      <author><first>Siddhant</first><last>Garg</last><affiliation>Meta</affiliation></author>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last><affiliation>Amazon</affiliation></author>
      <author><first>Alessandro</first><last>Moschitti</last><affiliation>Amazon AGI</affiliation></author>
      <pages>14636-14650</pages>
      <abstract>In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA). We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the difficulty of answering questions, and (ii) propose an unsupervised pipeline to measure RC given an arbitrary retrieval system.Our proposed pipeline measures RC more accurately than alternative estimators, including LLMs, on six challenging QA benchmarks. Further investigation reveals that RC scores strongly correlate with both QA performance and expert judgment across five of the six studied benchmarks, indicating that RC is an effective measure of question difficulty.Subsequent categorization of high-RC questions shows that they span a broad set of question shapes, including multi-hop, compositional, and temporal QA, indicating that RC scores can categorize a new subset of complex questions. Our system can also have a major impact on retrieval-based systems by helping to identify more challenging questions on existing datasets.</abstract>
      <url hash="2f60c41b">2024.findings-acl.872</url>
      <bibkey>gabburo-etal-2024-measuring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.872</doi>
    </paper>
    <paper id="873">
      <title>Combining Hierachical <fixed-case>VAE</fixed-case>s with <fixed-case>LLM</fixed-case>s for clinically meaningful timeline summarisation in social media</title>
      <author><first>Jiayu</first><last>Song</last><affiliation>Queen Mary, University of London</affiliation></author>
      <author><first>Jenny</first><last>Chim</last><affiliation>Queen Mary University London</affiliation></author>
      <author><first>Adam</first><last>Tsakalidis</last><affiliation>Cedefop and Alan Turing Institute</affiliation></author>
      <author><first>Julia</first><last>Ive</last><affiliation>Queen Mary, University of London</affiliation></author>
      <author><first>Dana</first><last>Atzil-Slonim</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Maria</first><last>Liakata</last><affiliation>Queen Mary University London</affiliation></author>
      <pages>14651-14672</pages>
      <abstract>We introduce a hybrid abstractive summarisation approach combining hierarchical VAEs with LLMs to produce clinically meaningful summaries from social media user timelines, appropriate for mental health monitoring. The summaries combine two different narrative points of view: (a) clinical insights in third person, generated by feeding into an LLM clinical expert-guided prompts, and importantly, (b) a temporally sensitive abstractive summary of the user’s timeline in first person, generated by a novel hierarchical variational autoencoder, TH-VAE. We assess the generated summaries via automatic evaluation against expert summaries and via human evaluation with clinical experts, showing that timeline summarisation by TH-VAE results in more factual and logically coherent summaries rich in clinical utility and superior to LLM-only approaches in capturing changes over time.</abstract>
      <url hash="abe76989">2024.findings-acl.873</url>
      <bibkey>song-etal-2024-combining</bibkey>
      <doi>10.18653/v1/2024.findings-acl.873</doi>
    </paper>
    <paper id="874">
      <title><fixed-case>PIXAR</fixed-case>: Auto-Regressive Language Modeling in Pixel Space</title>
      <author><first>Yintao</first><last>Tai</last></author>
      <author><first>Xiyang</first><last>Liao</last></author>
      <author><first>Alessandro</first><last>Suglia</last><affiliation>Heriot-Watt University</affiliation></author>
      <author><first>Antonio</first><last>Vergari</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>14673-14695</pages>
      <abstract>Recent work showed the possibility of building open-vocabulary large language models (LLMs) that directly operate on pixel representations. These models are implemented as autoencoders that reconstruct masked patches of rendered text.However, these pixel-based LLMs are limited to discriminative tasks (e.g., classification) and, similar to BERT, cannot be used to generate text.Therefore, they cannot be used for generative tasks such as free-form question answering. In this work, we introduce PIXAR, the first pixel-based autoregressive LLM that performs text generation. Consisting of only a decoder, PIXAR can perform free-form generative tasks while keeping the number of parameters on par with previous encoder-decoder models.Furthermore, we highlight the challenges of generating text as non-noisy images and show this is due to using a maximum likelihood objective. To overcome this problem, we propose an adversarial pretraining stage that improves the readability and accuracy of PIXAR by 8.1 on LAMBADA and 8.5 on bAbI— making it comparable to GPT-2 on text generation tasks.This paves the way to build open-vocabulary LLMs that operate on perceptual input only and calls into question the necessity of the usual symbolic input representation, i.e., text as (sub)tokens.</abstract>
      <url hash="183fd99c">2024.findings-acl.874</url>
      <bibkey>tai-etal-2024-pixar</bibkey>
      <doi>10.18653/v1/2024.findings-acl.874</doi>
    </paper>
    <paper id="875">
      <title>Sparsity-Accelerated Training for Large Language Models</title>
      <author><first>Da</first><last>Ma</last></author>
      <author><first>Lu</first><last>Chen</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Pengyu</first><last>Wang</last></author>
      <author><first>Hongshen</first><last>Xu</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Hanqi</first><last>Li</last></author>
      <author><first>Liangtai</first><last>Sun</last></author>
      <author><first>Su</first><last>Zhu</last></author>
      <author><first>Shuai</first><last>Fan</last></author>
      <author><first>Kai</first><last>Yu</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>14696-14707</pages>
      <abstract>Large language models (LLMs) have demonstrated proficiency across various natural language processing (NLP) tasks but often require additional training, such as continual pre-training and supervised fine-tuning. However, the costs associated with this, primarily due to their large parameter count, remain high. This paper proposes leveraging <i>sparsity</i> in pre-trained LLMs to expedite this training process. By observing sparsity in activated neurons during forward iterations, we identify the potential for computational speed-ups by excluding inactive neurons. We address associated challenges by extending existing neuron importance evaluation metrics and introducing a ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that Sparsity-Accelerated Training (SAT) achieves comparable or superior performance to standard training while significantly accelerating the process. Specifically, SAT achieves a 45% throughput improvement in continual pre-training and saves 38% training time in supervised fine-tuning. It offers a simple, hardware-agnostic, and easily deployable framework for additional LLM training.</abstract>
      <url hash="1135e9f6">2024.findings-acl.875</url>
      <bibkey>ma-etal-2024-sparsity</bibkey>
      <doi>10.18653/v1/2024.findings-acl.875</doi>
    </paper>
    <paper id="876">
      <title>Preemptive Answer “Attacks” on Chain-of-Thought Reasoning</title>
      <author><first>Rongwu</first><last>Xu</last></author>
      <author><first>Zehan</first><last>Qi</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <pages>14708-14726</pages>
      <abstract>Large language models (LLMs) showcase impressive reasoning capabilities when coupled with Chain-of-Thought (CoT) prompting. However, the robustness of this approach warrants further investigation. In this paper, we introduce a novel scenario termed preemptive answers, where the LLM obtains an answer before engaging in reasoning. This situation can arise inadvertently or induced by malicious users by prompt injection attacks. Experiments reveal that preemptive answers significantly impair the model’s reasoning capability across various CoT methods and a broad spectrum of datasets. To bolster the robustness of reasoning, we propose two measures aimed at mitigating this issue to some extent.</abstract>
      <url hash="6d423ef2">2024.findings-acl.876</url>
      <bibkey>xu-etal-2024-preemptive</bibkey>
      <doi>10.18653/v1/2024.findings-acl.876</doi>
    </paper>
    <paper id="877">
      <title>Do Language Models Exhibit Human-like Structural Priming Effects?</title>
      <author><first>Jaap</first><last>Jumelet</last></author>
      <author><first>Willem</first><last>Zuidema</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Arabella</first><last>Sinclair</last><affiliation>University of Aberdeen</affiliation></author>
      <pages>14727-14742</pages>
      <abstract>We explore which linguistic factors—at the sentence and token level—play an important role in influencing language model predictions, and investigate whether these are reflective of results found in humans and human corpora (Gries and Kootstra, 2017). We make use of the structural priming paradigm—where recent exposure to a structure facilitates processing of the same structure—to investigate where priming effects manifest, and what factors predict them. We find these effects can be explained via the inverse frequency effect found in human priming, where rarer elements within a prime increase priming effects, as well as lexical dependence between prime and target. Our results provide an important piece in the puzzle of understanding how properties within their context affect structural prediction in language models.</abstract>
      <url hash="b1ba1ef4">2024.findings-acl.877</url>
      <bibkey>jumelet-etal-2024-language</bibkey>
      <doi>10.18653/v1/2024.findings-acl.877</doi>
    </paper>
    <paper id="878">
      <title><fixed-case>R</fixed-case>ole<fixed-case>LLM</fixed-case>: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models</title>
      <author><first>Noah</first><last>Wang</last></author>
      <author><first>Z.y.</first><last>Peng</last></author>
      <author><first>Haoran</first><last>Que</last></author>
      <author><first>Jiaheng</first><last>Liu</last></author>
      <author><first>Wangchunshu</first><last>Zhou</last><affiliation>AIWaves Inc.</affiliation></author>
      <author><first>Yuhan</first><last>Wu</last></author>
      <author><first>Hongcheng</first><last>Guo</last></author>
      <author><first>Ruitong</first><last>Gan</last><affiliation>The Hong Kong Polytechnic University, Hong Kong Polytechnic University</affiliation></author>
      <author><first>Zehao</first><last>Ni</last></author>
      <author><first>Jian</first><last>Yang</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Man</first><last>Zhang</last></author>
      <author><first>Zhaoxiang</first><last>Zhang</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Wanli</first><last>Ouyang</last><affiliation>Shanghai AI Lab</affiliation></author>
      <author><first>Ke</first><last>Xu</last><affiliation>Beijing University of Aeronautics and Astronautics</affiliation></author>
      <author><first>Wenhao</first><last>Huang</last></author>
      <author><first>Jie</first><last>Fu</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Junran</first><last>Peng</last></author>
      <pages>14743-14777</pages>
      <abstract>The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).</abstract>
      <url hash="a9a03120">2024.findings-acl.878</url>
      <bibkey>wang-etal-2024-rolellm</bibkey>
      <doi>10.18653/v1/2024.findings-acl.878</doi>
    </paper>
    <paper id="879">
      <title><fixed-case>L</fixed-case>ang<fixed-case>S</fixed-case>uit·<fixed-case>E</fixed-case>: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments</title>
      <author><first>Zixia</first><last>Jia</last></author>
      <author><first>Mengmeng</first><last>Wang</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Baichen</first><last>Tong</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Song-Chun</first><last>Zhu</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <author><first>Zilong</first><last>Zheng</last><affiliation>Beijing Institute for General Artificial Intelligence</affiliation></author>
      <pages>14778-14814</pages>
      <abstract>Recent advances in Large Language Models (LLMs) have shown inspiring achievements in constructing autonomous agents that rely onlanguage descriptions as inputs. However, it remains unclear how well LLMs can function as few-shot or zero-shot embodied agents in dynamic interactive environments. To address this gap, we introduce LangSuit·E, a versatile and simulation-free testbed featuring 6 representative embodied tasks in textual embodied worlds. Compared with previous LLM-based testbeds, LangSuit·E (i) offers adaptability to diverse environments without multiple simulation engines, (ii) evaluates agents’ capacity to develop “internalized world knowledge” with embodied observations, and (iii) allows easy customization of communication and action strategies. To address the embodiment challenge, we devise a novel chain-of-thought (CoT) schema, EmMem, which summarizes embodied states w.r.t. history information. Comprehensive benchmark results illustrate challenges and insights of embodied planning. LangSuit·E represents a significant step toward building embodied generalists in the context of language models.</abstract>
      <url hash="085f40d2">2024.findings-acl.879</url>
      <bibkey>jia-etal-2024-langsuit</bibkey>
      <doi>10.18653/v1/2024.findings-acl.879</doi>
    </paper>
    <paper id="880">
      <title>Views Are My Own, but Also Yours: Benchmarking Theory of Mind Using Common Ground</title>
      <author><first>Adil</first><last>Soubki</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>John</first><last>Murzaku</last><affiliation>, State University of New York at Stony Brook</affiliation></author>
      <author><first>Arash</first><last>Yousefi Jordehi</last><affiliation>University of Guilan</affiliation></author>
      <author><first>Peter</first><last>Zeng</last><affiliation>State University of New York at Stony Brook</affiliation></author>
      <author><first>Magdalena</first><last>Markowska</last></author>
      <author><first>Seyed Abolghasem</first><last>Mirroshandel</last><affiliation>University of Guilan</affiliation></author>
      <author><first>Owen</first><last>Rambow</last><affiliation>Stony Brook University</affiliation></author>
      <pages>14815-14823</pages>
      <abstract>Evaluating the theory of mind (ToM) capabilities of language models (LMs) has recently received a great deal of attention. However, many existing benchmarks rely on synthetic data, which risks misaligning the resulting experiments with human behavior. We introduce the first ToM dataset based on naturally occurring spoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We then show that integrating a simple, explicit representation of beliefs improves LM performance on Common-ToM.</abstract>
      <url hash="80522231">2024.findings-acl.880</url>
      <bibkey>soubki-etal-2024-views</bibkey>
      <doi>10.18653/v1/2024.findings-acl.880</doi>
    </paper>
    <paper id="881">
      <title><fixed-case>MAPLE</fixed-case>: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models</title>
      <author><first>Divyanshu</first><last>Aggarwal</last></author>
      <author><first>Ashutosh</first><last>Sathe</last></author>
      <author><first>Ishaan</first><last>Watts</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Sunayana</first><last>Sitaram</last><affiliation>Microsoft</affiliation></author>
      <pages>14824-14867</pages>
      <abstract>Parameter efficient finetuning has emerged as a viable solution for improving the performance of Large Language Models without requiring massive resources and compute. Prior work on multilingual evaluation has shown that there is a large gap between the performance of LLMs on English and other languages. Further, there is also a large gap between the performance of smaller open-source models and larger LLMs. Finetuning can be an effective way to bridge this gap and make language models more equitable. In this work, we finetune the Llama-2 and Mistral models on two synthetic multilingual instruction tuning datasets to determine its effect on model performance on six downstream tasks covering forty one languages in all. Additionally, we experiment with various parameters, such as rank for low-rank adaptation and values of quantisation to determine their effects on downstream performance and find that higher rank and higher quantisation values benefit low-resource languages. We find that parameter efficient finetuning of smaller open-source models sometimes bridges the gap between the performance of these models and the larger ones, however, English performance can take a hit. We also find that finetuning sometimes improves performance on low-resource languages, while degrading performance on high-resource languages.</abstract>
      <url hash="33642153">2024.findings-acl.881</url>
      <bibkey>aggarwal-etal-2024-maple</bibkey>
      <doi>10.18653/v1/2024.findings-acl.881</doi>
    </paper>
    <paper id="882">
      <title><fixed-case>M</fixed-case>o<fixed-case>E</fixed-case>-<fixed-case>SLU</fixed-case>: Towards <fixed-case>ASR</fixed-case>-Robust Spoken Language Understanding via Mixture-of-Experts</title>
      <author><first>Xuxin</first><last>Cheng</last></author>
      <author><first>Zhihong</first><last>Zhu</last><affiliation>Tencent</affiliation></author>
      <author><first>Xianwei</first><last>Zhuang</last></author>
      <author><first>Zhanpeng</first><last>Chen</last></author>
      <author><first>Zhiqi</first><last>Huang</last><affiliation>Tencent Game</affiliation></author>
      <author><first>Yuexian</first><last>Zou</last><affiliation>Peking University</affiliation></author>
      <pages>14868-14879</pages>
      <abstract>As a crucial task in the task-oriented dialogue systems, spoken language understanding (SLU) has garnered increasing attention. However, errors from automatic speech recognition (ASR) often hinder the performance of understanding. To tackle this problem, we propose MoE-SLU, an ASR-Robust SLU framework based on the mixture-of-experts technique. Specifically, we first introduce three strategies to generate additional transcripts from clean transcripts. Then, we employ the mixture-of-experts technique to weigh the representations of the generated transcripts, ASR transcripts, and the corresponding clean manual transcripts. Additionally, we also regularize the weighted average of predictions and the predictions of ASR transcripts by minimizing the Jensen-Shannon Divergence (JSD) between these two output distributions. Experiment results on three benchmark SLU datasets demonstrate that our MoE-SLU achieves state-of-the-art performance. Further model analysis also verifies the superiority of our method.</abstract>
      <url hash="25e45b46">2024.findings-acl.882</url>
      <bibkey>cheng-etal-2024-moe</bibkey>
      <doi>10.18653/v1/2024.findings-acl.882</doi>
    </paper>
    <paper id="883">
      <title>Multi-Task Transfer Matters During Instruction-Tuning</title>
      <author><first>David</first><last>Mueller</last><affiliation>Johns Hopkins University</affiliation></author>
      <author><first>Mark</first><last>Dredze</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <author><first>Nicholas</first><last>Andrews</last><affiliation>Johns Hopkins University</affiliation></author>
      <pages>14880-14891</pages>
      <abstract>Instruction-tuning trains a language model on hundreds of tasks jointly to improve a model’s ability to learn in-context;however, the mechanisms that drive in-context learning are poorly understood and, as a result, the role of instruction-tuning on in-context generalization is poorly understood as well.In this work, we study the impact of instruction-tuning on multi-task transfer: how well a model’s parameters adapt to an unseen task via fine-tuning.We find that instruction-tuning negatively impacts a model’s transfer to unseen tasks, and that model transfer and in-context generalization are highly correlated, suggesting that this catastrophic forgetting may impact in-context learning.We study methods to improve model transfer, finding that multi-task training—how well the training tasks are optimized—can significantly impact ICL generalization; additionally, we find that continual training on unsupervised pre-training data can mitigate forgetting and improve ICL generalization as well.Finally, we demonstrate that, early into training, the impact of instruction-tuning on model transfer to tasks impacts in-context generalization on that task.Overall, we provide significant evidence that multi-task transfer is deeply connected to a model’s ability to learn a task in-context.</abstract>
      <url hash="94ebbb41">2024.findings-acl.883</url>
      <bibkey>mueller-etal-2024-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.883</doi>
    </paper>
    <paper id="884">
      <title>What Makes a Good Order of Examples in In-Context Learning</title>
      <author><first>Qi</first><last>Guo</last></author>
      <author><first>Leiyu</first><last>Wang</last><affiliation>nanjing university</affiliation></author>
      <author><first>Yidong</first><last>Wang</last></author>
      <author><first>Wei</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>Shikun</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>14892-14904</pages>
      <abstract>Although large language models (LLMs) have demonstrated impressive few-shot learning capabilities via in-context learning (ICL), ICL performance is known to be highly sensitive to the order of examples provided. To identify appropriate orders, recent studies propose heuristic methods to evaluate order performance using a set of unlabeled data. However, the requirement of in-domain data limits their utility in real-world scenarios where additional annotated data is challenging to acquire. Additionally, these dataset-based approaches are prone to being sub-optimal for a lack of consideration for individual differences. To address the problems, we first analyze the properties of performant example orders at both corpus level and instance level. Based on the analysis we propose **DEmO** to adaptively identify performant example order for each instance without extra data. DEmO works by filtering out a subset of orders featuring label fairness, then selecting the most influential order for each test instance. The employment of a content-free metric makes DEmO independent of in-domain data. Extensive experiments indicate the superiority of DEmO over a wide range of strong baselines. Further analysis validates the generalizability across various settings.</abstract>
      <url hash="40fa8afe">2024.findings-acl.884</url>
      <bibkey>guo-etal-2024-makes</bibkey>
      <doi>10.18653/v1/2024.findings-acl.884</doi>
    </paper>
    <paper id="885">
      <title><fixed-case>B</fixed-case>loom<fixed-case>VQA</fixed-case>: Assessing Hierarchical Multi-modal Comprehension</title>
      <author><first>Yunye</first><last>Gong</last><affiliation>SRI International</affiliation></author>
      <author><first>Robik</first><last>Shrestha</last><affiliation>Rochester Institute of Technology</affiliation></author>
      <author><first>Jared</first><last>Claypoole</last><affiliation>SRI International</affiliation></author>
      <author><first>Michael</first><last>Cogswell</last><affiliation>SRI International</affiliation></author>
      <author><first>Arijit</first><last>Ray</last><affiliation>Boston University</affiliation></author>
      <author><first>Christopher</first><last>Kanan</last><affiliation>University of Rochester</affiliation></author>
      <author><first>Ajay</first><last>Divakaran</last><affiliation>SRI International</affiliation></author>
      <pages>14905-14918</pages>
      <abstract>We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom’s Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demonstrates improved accuracy over all comprehension levels and also shows a tendency of bypassing visual inputs especially for higher-level tasks. Current models also show consistency patterns misaligned with human comprehension in various scenarios, demonstrating the need for improvement based on theoretically-grounded criteria. The dataset can be accessed at https://huggingface.co/datasets/ygong/BloomVQA.</abstract>
      <url hash="8b8b1c35">2024.findings-acl.885</url>
      <bibkey>gong-etal-2024-bloomvqa</bibkey>
      <doi>10.18653/v1/2024.findings-acl.885</doi>
    </paper>
    <paper id="886">
      <title><fixed-case>A</fixed-case>ttribution<fixed-case>B</fixed-case>ench: How Hard is Automatic Attribution Evaluation?</title>
      <author><first>Yifei</first><last>Li</last></author>
      <author><first>Xiang</first><last>Yue</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Zeyi</first><last>Liao</last><affiliation>Ohio State University, Columbus</affiliation></author>
      <author><first>Huan</first><last>Sun</last><affiliation>The Ohio State University, Columbus</affiliation></author>
      <pages>14919-14935</pages>
      <abstract>Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer’s attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model’s inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do.</abstract>
      <url hash="3042bd9d">2024.findings-acl.886</url>
      <bibkey>li-etal-2024-attributionbench</bibkey>
      <doi>10.18653/v1/2024.findings-acl.886</doi>
    </paper>
    <paper id="887">
      <title>Diffusion Guided Language Modeling</title>
      <author><first>Justin</first><last>Lovelace</last><affiliation>Cornell University</affiliation></author>
      <author><first>Varsha</first><last>Kishore</last><affiliation>Cornell University</affiliation></author>
      <author><first>Yiwei</first><last>Chen</last></author>
      <author><first>Kilian</first><last>Weinberger</last><affiliation>Cornell University, Cornell University and Cornell University</affiliation></author>
      <pages>14936-14952</pages>
      <abstract>Current language models demonstrate remarkable proficiency in text generation. However, for many applications it is desirable to control attributes, such as sentiment, or toxicity, of the generated language—ideally tailored towards each specific use case and target audience. For auto-regressive language models, existing guidance methods are prone to decoding errors that cascade during generation and degrade performance. In contrast, text diffusion models can easily be guided with, for example, a simple linear sentiment classifier—however they do suffer from significantly higher perplexity than auto-regressive alternatives. In this paper we use a guided diffusion model to produce a latent proposal that steers an auto-regressive language model to generate text with desired properties. Our model inherits the unmatched fluency of the auto-regressive approach and the plug-and-play flexibility of diffusion. We show that it outperforms previous plug-and-play guidance methods across a wide range of benchmark data sets. Further, controlling a new attribute in our framework is reduced to training a single logistic regression classifier.</abstract>
      <url hash="dfce6c4b">2024.findings-acl.887</url>
      <bibkey>lovelace-etal-2024-diffusion</bibkey>
      <doi>10.18653/v1/2024.findings-acl.887</doi>
    </paper>
    <paper id="888">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>E</fixed-case>d: Soft-Instruction Tuning for Model Editing with Hops</title>
      <author><first>XiaoQi</first><last>Han</last></author>
      <author><first>Ru</first><last>Li</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <author><first>Jiye</first><last>Liang</last><affiliation>Shanxi University</affiliation></author>
      <author><first>Zifang</first><last>Zhang</last></author>
      <author><first>Jeff</first><last>Pan</last><affiliation>University of Edinburgh, University of Edinburgh</affiliation></author>
      <pages>14953-14968</pages>
      <abstract>The task of model editing becomes popular for correcting inaccurate or outdated parametric knowledge in Large Language Models (LLMs). However, there are major limitations of state of the art (SOTA) model editing methods, including the excessive memorization issue caused by the direct editing methods, as well as the error propagation and knowledge conflict issues from the memory enhancement methods, resulting in hindering models’ *portability*, e.g., the ability to transfer the new knowledge to related one-hop or multi-hop content. To address these issues, we propose the InstructEd method, the idea of which is to insert soft instructions into the attention module so as to facilitate interactions between instructions and questions and to understand and utilize new facts. Our main findings are: (i) InstructEd has achieved SOTA performance on three datasets for one-hop/multi-hop evaluation with LLaMAs and GPT2, achieving 10% (5%) improvement in one-hop (multi-hop) model editing.(ii) Different from earlier methods on editing parameters in FFN, we show that editing attention can also help. (iii) Model editing is highly related to retrieval augmented methods, which can help improve the locality of model editing while slightly decrease the editing performance with hops.</abstract>
      <url hash="d3d47b26">2024.findings-acl.888</url>
      <bibkey>han-etal-2024-instructed</bibkey>
      <doi>10.18653/v1/2024.findings-acl.888</doi>
    </paper>
    <paper id="889">
      <title><fixed-case>TLCR</fixed-case>: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback</title>
      <author><first>Eunseop</first><last>Yoon</last><affiliation>KAIST</affiliation></author>
      <author><first>Hee Suk</first><last>Yoon</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>SooHwan</first><last>Eom</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Gunsoo</first><last>Han</last><affiliation>Kakao Brain</affiliation></author>
      <author><first>Daniel</first><last>Nam</last><affiliation>Kakao Brain Corp.</affiliation></author>
      <author><first>Daejin</first><last>Jo</last><affiliation>Korea University and Kakao Brain</affiliation></author>
      <author><first>Kyoung-Woon</first><last>On</last><affiliation>Kakao</affiliation></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Sungwoong</first><last>Kim</last><affiliation>Korea University</affiliation></author>
      <author><first>Chang</first><last>Yoo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>14969-14981</pages>
      <abstract>Reinforcement Learning from Human Feedback (RLHF) leverages human preference data to train language models to align more closely with human essence. These human preference data, however, are labeled at the sequence level, creating a mismatch between sequence-level preference labels and tokens, which are autoregressively generated from the language model. Although several recent approaches have tried to provide token-level (i.e., dense) rewards for each individual token, these typically rely on predefined discrete reward values (e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying degrees of preference inherent to each token. To address this limitation, we introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a discriminator trained to distinguish positive and negative tokens, and the confidence of the discriminator is used to assign continuous rewards to each token considering the context. Extensive experiments show that our proposed TLCR leads to consistent performance improvements over previous sequence-level or token-level discrete rewards on open-ended generation benchmarks.</abstract>
      <url hash="c17aab89">2024.findings-acl.889</url>
      <bibkey>yoon-etal-2024-tlcr</bibkey>
      <doi>10.18653/v1/2024.findings-acl.889</doi>
    </paper>
    <paper id="890">
      <title>Found in the middle: Calibrating Positional Attention Bias Improves Long Context Utilization</title>
      <author><first>Cheng-Yu</first><last>Hsieh</last><affiliation>University of Washington</affiliation></author>
      <author><first>Yung-Sung</first><last>Chuang</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Chun-Liang</first><last>Li</last><affiliation>Google</affiliation></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Long</first><last>Le</last><affiliation>Google</affiliation></author>
      <author><first>Abhishek</first><last>Kumar</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>James</first><last>Glass</last></author>
      <author><first>Alexander</first><last>Ratner</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Chen-Yu</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Ranjay</first><last>Krishna</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Tomas</first><last>Pfister</last><affiliation>Google</affiliation></author>
      <pages>14982-14995</pages>
      <abstract>Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. In this work, we make three contributions. First, we set out to understand the factors that cause this phenomenon. In doing so, we establish a connection between lost-in-the-middle to LLMs’ intrinsic attention bias: LLMs exhibit an U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance. Second, we mitigate this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, we show found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 10 percentage point. These findings open up future directions in understanding LLM attention bias and its potential consequences.</abstract>
      <url hash="ac0b1207">2024.findings-acl.890</url>
      <bibkey>hsieh-etal-2024-found</bibkey>
      <doi>10.18653/v1/2024.findings-acl.890</doi>
    </paper>
    <paper id="891">
      <title>S3-<fixed-case>DST</fixed-case>: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Sarkar Snigdha Sarathi</first><last>Das</last></author>
      <author><first>Chirag</first><last>Shah</last><affiliation>University of Washington</affiliation></author>
      <author><first>Mengting</first><last>Wan</last><affiliation>Microsoft</affiliation></author>
      <author><first>Jennifer</first><last>Neville</last><affiliation>Purdue University and Purdue University</affiliation></author>
      <author><first>Longqi</first><last>Yang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Reid</first><last>Andersen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Georg</first><last>Buscher</last></author>
      <author><first>Tara</first><last>Safavi</last><affiliation>Microsoft Research</affiliation></author>
      <pages>14996-15014</pages>
      <abstract>Traditional Dialogue State Tracking (DST) has focused on tracking preferences and intents in conversations centered around specific tasks (e.g. booking services). These conventional systems assume a relatively restricted conversation flow in which each turn gradually offers new information. However, advancements in Large Language Models (LLMs) have ushered in more versatile open-domain chat systems in which extended dialogue sessions encompassing numerous tasks and topics are common—in turn requiring new conversational tracking tools in order to successfully orchestrate such systems. Addressing these challenges, we introduce a novel approach combining dialogue segmentation and state tracking within open-domain dialogues, tailored for zero-shot applications appropriate to a true open-domain dialogue system. Our proposed method S3-DST employs a unique structured prompting technique and *Pre-Analytical Recollection*, a novel grounding mechanism we designed for improving long context tracking. Tested on proprietary anonymized open-domain dialogue datasets as well as publicly available DST and segmentation datasets, S3-DST consistently outperforms the state-of-the-art, showcasing its effectiveness and adaptability state tracking in the next wave of LLM-based chat systems. We also release S3-DST annotations with GPT-4 on a curated subset of LMSYS-Chat-1M to be used as a testbed to fuel research in this direction.</abstract>
      <url hash="60720c69">2024.findings-acl.891</url>
      <bibkey>das-etal-2024-s3</bibkey>
      <doi>10.18653/v1/2024.findings-acl.891</doi>
    </paper>
    <paper id="892">
      <title>Set the Clock: Temporal Alignment of Pretrained Language Models</title>
      <author><first>Bowen</first><last>Zhao</last></author>
      <author><first>Zander</first><last>Brumbaugh</last><affiliation>Department of Computer Science</affiliation></author>
      <author><first>Yizhong</first><last>Wang</last><affiliation>Department of Computer Science, University of Washington</affiliation></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last><affiliation>University of Washington, University of Washington, Allen Institute for Artificial Intelligence and University of Washington, Seattle</affiliation></author>
      <author><first>Noah</first><last>Smith</last><affiliation>University of Washington and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>15015-15040</pages>
      <abstract>Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. This work investigates the temporal chaos of pretrained LMs and explores various methods to align their internal knowledge to a target time, which we call “temporal alignment.” To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from prompting to finetuning, to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments demonstrate that aligning LLaMa2 to the year 2022 can enhance its performance by up to 62% according to that year’s answers. This improvement occurs even without explicitly mentioning time information, indicating the possibility of aligning models’ internal sense of time after pretraining. Finally, we find that alignment to a historical time is also possible, with up to <tex-math>2.8\times</tex-math> the performance of the unaligned LM in 2010 if finetuning models to that year. These findings hint at the sophistication of LMs’ internal knowledge organization and the necessity of tuning them properly.</abstract>
      <url hash="700029f8">2024.findings-acl.892</url>
      <bibkey>zhao-etal-2024-set</bibkey>
      <doi>10.18653/v1/2024.findings-acl.892</doi>
    </paper>
    <paper id="893">
      <title>From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models</title>
      <author><first>Beyza</first><last>Ermis</last><affiliation>Cohere AI</affiliation></author>
      <author><first>Luiza</first><last>Pozzobon</last></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <pages>15041-15058</pages>
      <abstract>To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it’s crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field.</abstract>
      <url hash="247e68ad">2024.findings-acl.893</url>
      <bibkey>ermis-etal-2024-one</bibkey>
      <doi>10.18653/v1/2024.findings-acl.893</doi>
    </paper>
    <paper id="894">
      <title>Here’s a Free Lunch: Sanitizing Backdoored Models with Model Merge</title>
      <author><first>Ansh</first><last>Arora</last></author>
      <author><first>Xuanli</first><last>He</last><affiliation>University College London, University of London</affiliation></author>
      <author><first>Maximilian</first><last>Mozes</last><affiliation>Cohere</affiliation></author>
      <author><first>Srinibas</first><last>Swain</last></author>
      <author><first>Mark</first><last>Dras</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Qiongkai</first><last>Xu</last><affiliation>Macquarie University</affiliation></author>
      <pages>15059-15075</pages>
      <abstract>The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can significantly remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we verify our hypothesis on various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks on classification and instruction-tuned tasks without additional resources or specific knowledge. Our approach consistently outperforms recent advanced baselines, leading to an average of about 75% reduction in the attack success rate. Since model merging has been an established approach for improving model performance, the extra advantage it provides regarding defense can be seen as a cost-free bonus.</abstract>
      <url hash="5606af36">2024.findings-acl.894</url>
      <bibkey>arora-etal-2024-heres</bibkey>
      <doi>10.18653/v1/2024.findings-acl.894</doi>
    </paper>
    <paper id="895">
      <title>Enhancing Sentence Simplification in <fixed-case>P</fixed-case>ortuguese: Leveraging Paraphrases, Context, and Linguistic Features</title>
      <author><first>Arthur</first><last>Scalercio</last></author>
      <author><first>Maria</first><last>Finatto</last><affiliation>Universidade Federal do Rio Grande do Sul</affiliation></author>
      <author><first>Aline</first><last>Paes</last><affiliation>Universidade Federal Fluminense</affiliation></author>
      <pages>15076-15091</pages>
      <abstract>Automatic text simplification focuses on transforming texts into a more comprehensible version without sacrificing their precision. However, automatic methods usually require (paired) datasets that can be rather scarce in languages other than English. This paper presents a new approach to automatic sentence simplification that leverages paraphrases, context, and linguistic attributes to overcome the absence of paired texts in Portuguese.We frame the simplification problem as a textual style transfer task and learn a style representation using the sentences around the target sentence in the document and its linguistic attributes. Moreover, unlike most unsupervised approaches that require style-labeled training data, we fine-tune strong pre-trained models using sentence-level paraphrases instead of annotated data. Our experiments show that our model achieves remarkable results, surpassing the current state-of-the-art (BART+ACCESS) while competitively matching a Large Language Model.</abstract>
      <url hash="68f127e2">2024.findings-acl.895</url>
      <bibkey>scalercio-etal-2024-enhancing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.895</doi>
    </paper>
    <paper id="896">
      <title>How Far can 100 Samples Go? Unlocking Zero-Shot Translation with Tiny Multi-Parallel Data</title>
      <author><first>Di</first><last>Wu</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Shaomu</first><last>Tan</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Yan</first><last>Meng</last></author>
      <author><first>David</first><last>Stap</last></author>
      <author><first>Christof</first><last>Monz</last><affiliation>University of Amsterdam, University of Amsterdam</affiliation></author>
      <pages>15092-15108</pages>
      <abstract>Zero-shot translation aims to translate between language pairs not seen during training in Multilingual Machine Translation (MMT) and is widely considered an open problem. A common, albeit resource-consuming, solution is to add as many related translation directions as possible to the training corpus. In this paper, we show that for an English-centric model, surprisingly large zero-shot improvements can be achieved by simply fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we obtain up to +21.7 ChrF++ non-English overall improvements (870 directions) by using only 100 multi-parallel samples while preserving English-centric translation quality. This performance exceeds M2M100 by an average of 5.9 ChrF++ in the involved non-English directions. When investigating the size effect of fine-tuning data on translation quality, we found that already a small, randomly sampled set of fine-tuning directions is sufficient to achieve comparable improvements. The resulting non-English performance is close to the complete translation upper bound. Even in a minimal setting—fine-tuning with only one single sample—the well-known off-target issue is almost completely resolved, explaining parts—but not all—of the observed improvements in translation quality.</abstract>
      <url hash="c4daa086">2024.findings-acl.896</url>
      <bibkey>wu-etal-2024-far</bibkey>
      <doi>10.18653/v1/2024.findings-acl.896</doi>
    </paper>
    <paper id="897">
      <title>Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Dataset</title>
      <author><first>Satanu</first><last>Ghosh</last></author>
      <author><first>Neal</first><last>Brodnik</last></author>
      <author><first>Carolina</first><last>Frey</last></author>
      <author><first>Collin</first><last>Holgate</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Tresa</first><last>Pollock</last><affiliation>University of California-Santa Barbara</affiliation></author>
      <author><first>Samantha</first><last>Daly</last><affiliation>University of Michigan Ann Arbor</affiliation></author>
      <author><first>Samuel</first><last>Carton</last><affiliation>University of New Hampshire, Durham</affiliation></author>
      <pages>15109-15123</pages>
      <abstract>We explore the ability of GPT-4 to perform ad-hoc schema-based information extraction from scientific literature. We assess specifically whether it can, with a basic one-shot prompting approach over the full text of the included manusciprts, replicate two existing material science datasets, one pertaining to multi-principal element alloys (MPEAs), and one to silicate diffusion. We collaborate with materials scientists to perform a detailed manual error analysis to assess where and why the model struggles to faithfully extract the desired information, and draw on their insights to suggest research directions to address this broadly important task.</abstract>
      <url hash="0496ee24">2024.findings-acl.897</url>
      <bibkey>ghosh-etal-2024-toward</bibkey>
      <doi>10.18653/v1/2024.findings-acl.897</doi>
    </paper>
    <paper id="898">
      <title>Structural Optimization Ambiguity and Simplicity Bias in Unsupervised Neural Grammar Induction</title>
      <author><first>Jinwook</first><last>Park</last><affiliation>Gwangju Institute of Science and Technology</affiliation></author>
      <author><first>Kangil</first><last>Kim</last><affiliation>Gwangju Institute of Science and Technology</affiliation></author>
      <pages>15124-15139</pages>
      <abstract>Neural parameterization has significantly advanced unsupervised grammar induction. However, training these models with a traditional likelihood loss for all possible parses exacerbates two issues: 1) *structural optimization ambiguity* that arbitrarily selects one among structurally ambiguous optimal grammars despite the specific preference of gold parses, and 2) *structural simplicity bias* that leads a model to underutilize rules to compose parse trees. These challenges subject unsupervised neural grammar induction (UNGI) to inevitable prediction errors, high variance, and the necessity for extensive grammars to achieve accurate predictions. This paper tackles these issues, offering a comprehensive analysis of their origins. As a solution, we introduce *sentence-wise parse-focusing* to reduce the parse pool per sentence for loss evaluation, using the structural bias from pre-trained parsers on the same dataset.In unsupervised parsing benchmark tests, our method significantly improves performance while effectively reducing variance and bias toward overly simplistic parses. Our research promotes learning more compact, accurate, and consistent explicit grammars, facilitating better interpretability.</abstract>
      <url hash="b35971fb">2024.findings-acl.898</url>
      <bibkey>park-kim-2024-structural</bibkey>
      <doi>10.18653/v1/2024.findings-acl.898</doi>
    </paper>
    <paper id="899">
      <title><fixed-case>LMDX</fixed-case>: Language Model-based Document Information Extraction and Localization</title>
      <author><first>Vincent</first><last>Perot</last><affiliation>Google</affiliation></author>
      <author><first>Kai</first><last>Kang</last><affiliation>Google</affiliation></author>
      <author><first>Florian</first><last>Luisier</last><affiliation>Google</affiliation></author>
      <author><first>Guolong</first><last>Su</last><affiliation>Google</affiliation></author>
      <author><first>Xiaoyu</first><last>Sun</last><affiliation>Google</affiliation></author>
      <author><first>Ramya Sree</first><last>Boppana</last><affiliation>Google</affiliation></author>
      <author><first>Zilong</first><last>Wang</last><affiliation>University of California, San Diego</affiliation></author>
      <author><first>Zifeng</first><last>Wang</last><affiliation>Google</affiliation></author>
      <author><first>Jiaqi</first><last>Mu</last><affiliation>Google</affiliation></author>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Chen-Yu</first><last>Lee</last><affiliation>Google</affiliation></author>
      <author><first>Nan</first><last>Hua</last><affiliation>Google</affiliation></author>
      <pages>15140-15168</pages>
      <abstract>Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art and exhibiting emergent capabilities across various tasks. However, their application in extracting information from visually rich documents, which is at the core of many document processing workflows and involving the extraction of key entities from semi-structured documents, has not yet been successful. The main obstacles to adopting LLMs for this task include the absence of layout encoding within LLMs, which is critical for high quality extraction, and the lack of a grounding mechanism to localize the predicted entities within the document. In this paper, we introduce Language Model-based Document Information EXtraction and Localization (LMDX), a methodology to reframe the document information extraction task for a LLM. LMDX enables extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and localizing the entities within the document. Finally, we apply LMDX to the PaLM 2-S and Gemini Pro LLMs and evaluate it on VRDU and CORD benchmarks, setting a new state-of-the-art and showing how LMDX enables the creation of high quality, data-efficient parsers.</abstract>
      <url hash="b80e69e1">2024.findings-acl.899</url>
      <bibkey>perot-etal-2024-lmdx</bibkey>
      <doi>10.18653/v1/2024.findings-acl.899</doi>
    </paper>
    <paper id="900">
      <title><fixed-case>DBQR</fixed-case>-<fixed-case>QA</fixed-case>: A Question Answering Dataset on a Hybrid of Database Querying and Reasoning</title>
      <author><first>Rungsiman</first><last>Nararatwong</last></author>
      <author><first>Chung-Chi</first><last>Chen</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Natthawut</first><last>Kertkeidkachorn</last><affiliation>Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology</affiliation></author>
      <author><first>Hiroya</first><last>Takamura</last><affiliation>AIST, National Institute of Advanced Industrial Science and Technology</affiliation></author>
      <author><first>Ryutaro</first><last>Ichise</last><affiliation>National Intitute of Informatics and Tokyo Institute of Technology, Tokyo Institute of Technology</affiliation></author>
      <pages>15169-15182</pages>
      <abstract>This paper introduces the Database Querying and Reasoning Dataset for Question Answering (DBQR-QA), aimed at addressing the gap in current question-answering (QA) research by emphasizing the essential processes of database querying and reasoning to answer questions. Specifically designed to accommodate sequential questions and multi-hop queries, DBQR-QA more accurately mirrors the dynamics of real-world information retrieval and analysis, with a particular focus on the financial reports of US companies. The dataset’s construction, the challenges encountered during its development, the performance of large language models on this dataset, and a human evaluation are thoroughly discussed to illustrate the dataset’s complexity and highlight future research directions in querying and reasoning tasks.</abstract>
      <url hash="bab449ac">2024.findings-acl.900</url>
      <bibkey>nararatwong-etal-2024-dbqr</bibkey>
      <doi>10.18653/v1/2024.findings-acl.900</doi>
    </paper>
    <paper id="901">
      <title><fixed-case>N</fixed-case>ote<fixed-case>C</fixed-case>hat: A Dataset of Synthetic Patient-Physician Conversations Conditioned on Clinical Notes</title>
      <author><first>Junda</first><last>Wang</last></author>
      <author><first>Zonghai</first><last>Yao</last><affiliation>University of Massachusetts at Amherst</affiliation></author>
      <author><first>Zhichao</first><last>Yang</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <author><first>Huixue</first><last>Zhou</last></author>
      <author><first>Rumeng</first><last>Li</last><affiliation>University of Massachusetts, Amherst</affiliation></author>
      <author><first>Xun</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Yucheng</first><last>Xu</last></author>
      <author><first>Hong</first><last>Yu</last><affiliation>Columbia University</affiliation></author>
      <pages>15183-15201</pages>
      <abstract>We introduce NoteChat, a novel cooperative multi-agent framework leveraging Large Language Models (LLMs) to generate patient-physician dialogues. NoteChat embodies the principle that an ensemble of role-specific LLMs, through structured role-play and strategic prompting, can perform their assigned roles more effectively. The synergy among these role-playing LLMs results in a cohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a benchmark dataset for patient-physician dialogues-note pairs, shows that models trained with the augmented synthetic patient-physician dialogues by NoteChat outperforms other state-of-the-art models for generating clinical notes. Our comprehensive automatic and human evaluation demonstrates that NoteChat substantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to 22.78% by domain experts in generating superior synthetic patient-physician dialogues based on clinical notes. NoteChat has the potential to engage patients directly and help clinical documentation, a leading cause of physician burnout.</abstract>
      <url hash="5f35b0a2">2024.findings-acl.901</url>
      <bibkey>wang-etal-2024-notechat</bibkey>
      <doi>10.18653/v1/2024.findings-acl.901</doi>
    </paper>
    <paper id="902">
      <title>Model Editing at Scale leads to Gradual and Catastrophic Forgetting</title>
      <author><first>Akshat</first><last>Gupta</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Anurag</first><last>Rao</last></author>
      <author><first>Gopala</first><last>Anumanchipalli</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>15202-15232</pages>
      <abstract>Editing knowledge in large language models is an attractive capability that allows us to correct incorrectly learned facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate current model editing methods at scale, focusing on two state of the art methods - ROME and MEMIT. With the lens of scalability, we evaluate model editing methods for three crucial properties - editing proficiency, fact forgetting and downstream performance. We find that as a model is edited sequentially with multiple facts, it continually becomes less editable, forgets previously edited facts and loses the ability to perform downstream tasks. For ROME and MEMIT, this “forgetting” happens in two phases - an initial gradual but progressive forgetting phase followed by an abrupt or catastrophic forgetting. Both gradual and catastrophic forgetting limit the usefulness of model editing methods at scale - the former makes model editing less effective as multiple edits are made to the model while the latter caps the scalability of such model editing methods. Our analysis also highlights other key limitations of ROME and MEMIT at scale. With our work, we push for better evaluation of model editing and development of model editing methods keeping scalability in mind.</abstract>
      <url hash="0122ddd4">2024.findings-acl.902</url>
      <bibkey>gupta-etal-2024-model</bibkey>
      <doi>10.18653/v1/2024.findings-acl.902</doi>
    </paper>
    <paper id="903">
      <title>3<fixed-case>MVRD</fixed-case>: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding</title>
      <author><first>Yihao</first><last>Ding</last></author>
      <author><first>Lorenzo</first><last>Vaiani</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <author><first>Caren</first><last>Han</last><affiliation>University of Melbourne, University of Western Australia and University of Sydney</affiliation></author>
      <author><first>Jean</first><last>Lee</last></author>
      <author><first>Paolo</first><last>Garza</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <author><first>Josiah</first><last>Poon</last><affiliation>University of Sydney</affiliation></author>
      <author><first>Luca</first><last>Cagliero</last><affiliation>Polytechnic Institute of Turin</affiliation></author>
      <pages>15233-15244</pages>
      <abstract>This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.</abstract>
      <url hash="b96384e8">2024.findings-acl.903</url>
      <bibkey>ding-etal-2024-3mvrd</bibkey>
      <doi>10.18653/v1/2024.findings-acl.903</doi>
    </paper>
    <paper id="904">
      <title>Faithful Persona-based Conversational Dataset Generation with Large Language Models</title>
      <author><first>Pegah</first><last>Jandaghi</last></author>
      <author><first>Xianghai</first><last>Sheng</last><affiliation>Google</affiliation></author>
      <author><first>Xinyi</first><last>Bai</last><affiliation>Google</affiliation></author>
      <author><first>Jay</first><last>Pujara</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Hakim</first><last>Sidahmed</last></author>
      <pages>15245-15270</pages>
      <abstract>High-quality conversational datasets are essential for developing AI models that can communicate with users.One way to foster deeper interactions between a chatbot and its user is through *personas*, aspects of the user’s character that provide insights into their personality, motivations, and behaviors.Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations.The Generator is an LLM prompted to output conversations.The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations.These experts select the best generated conversations, which we then use to improve the Generator.We release Synthetic-Persona-Chat, consisting of 20k conversations seeded from Persona-Chat.We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during an AI detection test decreases from 17.2% to 8.8% over three iterations.</abstract>
      <url hash="771706ff">2024.findings-acl.904</url>
      <bibkey>jandaghi-etal-2024-faithful-persona</bibkey>
      <doi>10.18653/v1/2024.findings-acl.904</doi>
    </paper>
    <paper id="905">
      <title>Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning</title>
      <author><first>Zhiyang</first><last>Xu</last></author>
      <author><first>Chao</first><last>Feng</last><affiliation>University of Michigan - Ann Arbor and University of Electronic Science and Technology of China</affiliation></author>
      <author><first>Rulin</first><last>Shao</last></author>
      <author><first>Trevor</first><last>Ashby</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Di</first><last>Jin</last><affiliation>Meta</affiliation></author>
      <author><first>Yu</first><last>Cheng</last><affiliation>The Chinese University of Hong Kong</affiliation></author>
      <author><first>Qifan</first><last>Wang</last><affiliation>Meta AI</affiliation></author>
      <author><first>Lifu</first><last>Huang</last><affiliation>Virginia Tech</affiliation></author>
      <pages>15271-15342</pages>
      <abstract>Despite vision-language models’ (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual instruction tuning framework and achieves the state-of-the-art performance across a wide range of multi-modal evaluation benchmarks. Finally, we conduct in-depth analyses to understand visual instruction tuning and our findings reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs’ capabilities but rather modulates the model’s responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can effectively align VLM responses with human-preference; (3) Visual instruction tuning mainly helps large-language models (LLMs) to understand visual features.</abstract>
      <url hash="9a739414">2024.findings-acl.905</url>
      <bibkey>xu-etal-2024-vision</bibkey>
      <doi>10.18653/v1/2024.findings-acl.905</doi>
    </paper>
    <paper id="906">
      <title><fixed-case>TAXI</fixed-case>: Evaluating Categorical Knowledge Editing for Language Models</title>
      <author><first>Derek</first><last>Powell</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Walter</first><last>Gerych</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Thomas</first><last>Hartvigsen</last><affiliation>University of Virginia, Charlottesville</affiliation></author>
      <pages>15343-15352</pages>
      <abstract>Humans rarely learn one fact in isolation. Instead, learning a new fact induces knowledge of other facts about the world. For example, in learning a korat is a type of cat, you also infer it is a mammal and has claws, ensuring your model of the world is consistent. Knowledge editing aims to inject new facts into language models to improve their factuality, but current benchmarks fail to evaluate consistency, which is critical to ensure efficient, accurate, and generalizable edits. We manually create TAXI, a new benchmark dataset specifically created to evaluate consistency in categorical knowledge edits. TAXI contains 11,120 multiple-choice queries for 976 edits spanning 41 categories (e.g., Dogs), 164 subjects (e.g., Labrador), and 183 properties (e.g., is a mammal). We then use TAXI to evaluate popular editors’ categorical consistency, measuring how often editing a subject’s category appropriately edits its properties. We find that 1) the editors achieve marginal, yet non-random consistency, 2) their consistency far underperforms human baselines, and 3) consistency is more achievable when editing atypical subjects.</abstract>
      <url hash="ced7045e">2024.findings-acl.906</url>
      <bibkey>powell-etal-2024-taxi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.906</doi>
    </paper>
    <paper id="907">
      <title>Automatic Bug Detection in <fixed-case>LLM</fixed-case>-Powered Text-Based Games Using <fixed-case>LLM</fixed-case>s</title>
      <author><first>Claire</first><last>Jin</last></author>
      <author><first>Sudha</first><last>Rao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Xiangyu</first><last>Peng</last><affiliation>SalesForce.com</affiliation></author>
      <author><first>Portia</first><last>Botchway</last><affiliation>Vanderbilt University</affiliation></author>
      <author><first>Jessica</first><last>Quaye</last><affiliation>Harvard University</affiliation></author>
      <author><first>Chris</first><last>Brockett</last><affiliation>Microsoft</affiliation></author>
      <author><first>Bill</first><last>Dolan</last></author>
      <pages>15353-15368</pages>
      <abstract>Advancements in large language models (LLMs) are revolutionizing interactive game design, enabling dynamic plotlines and interactions between players and non-player characters (NPCs). However, LLMs may exhibit flaws such as hallucinations, forgetfulness, or misinterpretations of prompts, causing logical inconsistencies and unexpected deviations from intended designs. Automated techniques for detecting such game bugs are still lacking. To address this, we propose a systematic LLM-based method for automatically identifying such bugs from player game logs, eliminating the need for collecting additional data such as post-play surveys. Applied to a text-based game DejaBoom!, our approach effectively identifies bugs inherent in LLM-powered interactive games, surpassing unstructured LLM-powered bug-catching methods and filling the gap in automated detection of logical and design flaws.</abstract>
      <url hash="cadb41c1">2024.findings-acl.907</url>
      <bibkey>jin-etal-2024-automatic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.907</doi>
    </paper>
    <paper id="908">
      <title>Embodied Language Learning: Opportunities, Challenges, and Future Directions</title>
      <author><first>Nadine</first><last>Amin</last></author>
      <author><first>Julia</first><last>Rayz</last><affiliation>Purdue University</affiliation></author>
      <pages>15369-15379</pages>
      <abstract>While large language and vision-language models showcase impressive capabilities, they face a notable limitation: the inability to connect language with the physical world. To bridge this gap, research has focused on embodied language learning, where the language learner is situated in the world, perceives it, and interacts with it. This article explores the current standing of research in embodied language learning, highlighting opportunities and discussing common challenges. Lastly, it identifies existing gaps from the perspective of language understanding research within the embodied world and suggests potential future directions.</abstract>
      <url hash="8500f9ed">2024.findings-acl.908</url>
      <bibkey>amin-rayz-2024-embodied</bibkey>
      <doi>10.18653/v1/2024.findings-acl.908</doi>
    </paper>
    <paper id="909">
      <title>Challenges to Evaluating the Generalization of Coreference Resolution Models: A Measurement Modeling Perspective</title>
      <author><first>Ian</first><last>Porada</last><affiliation>McGill University</affiliation></author>
      <author><first>Alexandra</first><last>Olteanu</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Kaheer</first><last>Suleman</last></author>
      <author><first>Adam</first><last>Trischler</last></author>
      <author><first>Jackie</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <pages>15380-15395</pages>
      <abstract>It is increasingly common to evaluate the same coreference resolution (CR) model on multiple datasets. Do these multi-dataset evaluations allow us to draw meaningful conclusions about model generalization? Or, do they rather reflect the idiosyncrasies of a particular experimental setup (e.g., the specific datasets used)? To study this, we view evaluation through the lens of measurement modeling, a framework commonly used in the social sciences for analyzing the validity of measurements. By taking this perspective, we show how multi-dataset evaluations risk conflating different factors concerning what, precisely, is being measured. This in turn makes it difficult to draw more generalizable conclusions from these evaluations. For instance, we show that across seven datasets, measurements intended to reflect CR model generalization are often correlated with differences in both how coreference is defined and how it is operationalized; this limits our ability to draw conclusions regarding the ability of CR models to generalize across any singular dimension. We believe the measurement modeling framework provides the needed vocabulary for discussing challenges surrounding what is actually being measured by CR evaluations.</abstract>
      <url hash="c970429a">2024.findings-acl.909</url>
      <bibkey>porada-etal-2024-challenges</bibkey>
      <doi>10.18653/v1/2024.findings-acl.909</doi>
    </paper>
    <paper id="910">
      <title><fixed-case>SAGA</fixed-case>: A Participant-specific Examination of Story Alternatives and Goal Applicability for a Deeper Understanding of Complex Events</title>
      <author><first>Sai</first><last>Vallurupalli</last></author>
      <author><first>Katrin</first><last>Erk</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Francis</first><last>Ferraro</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <pages>15396-15420</pages>
      <abstract>Interpreting and assessing goal driven actions is vital to understanding and reasoning over complex events. It is important to be able to acquire the knowledge needed for this understanding, though doing so is challenging. We argue that such knowledge can be elicited through a participant achievement lens. We analyze a complex event in a narrative according to the intended achievements of the participants in that narrative, the likely future actions of the participants, and the likelihood of goal success. We collect 6.3K high quality goal and action annotations reflecting our proposed participant achievement lens, with an average weighted Fleiss-Kappa IAA of 80%. Our collection contains annotated alternate versions of each narrative. These alternate versions vary minimally from the “original” story, but can license drastically different inferences. Our findings suggest that while modern large language models can reflect some of the goal-based knowledge we study, they find it challenging to fully capture the design and intent behind concerted actions, even when the model pretraining included the data from which we extracted the goal knowledge. We show that smaller models fine-tuned on our dataset can achieve performance surpassing larger models.</abstract>
      <url hash="375675ad">2024.findings-acl.910</url>
      <bibkey>vallurupalli-etal-2024-saga</bibkey>
      <doi>10.18653/v1/2024.findings-acl.910</doi>
    </paper>
    <paper id="911">
      <title><fixed-case>SLIDE</fixed-case>: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation</title>
      <author><first>Kun</first><last>Zhao</last></author>
      <author><first>Bohao</first><last>Yang</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Chen</first><last>Tang</last></author>
      <author><first>Chenghua</first><last>Lin</last><affiliation>University of Manchester</affiliation></author>
      <author><first>Liang</first><last>Zhan</last><affiliation>University of Pittsburgh</affiliation></author>
      <pages>15421-15435</pages>
      <abstract>The long-standing one-to-many problem of gold standard responses in open-domain dialogue systems presents challenges for automatic evaluation metrics. Though prior works have demonstrated some success by applying powerful Large Language Models (LLMs), existing approaches still struggle with the one-to-many problem, and exhibit subpar performance in domain-specific scenarios. We assume the commonsense reasoning biases within LLMs may hinder their performance in domain-specific evaluations. To address both issues, we propose a novel framework SLIDE (Small and Large Integrated for Dialogue Evaluation), that leverages both a small, specialised model (SLM), and LLMs for the evaluation of open domain dialogues. Our approach introduces several techniques: (1) Contrastive learning to differentiate between robust and non-robust response embeddings; (2) A novel metric for semantic sensitivity that combines embedding cosine distances with similarity learned through neural networks, and (3) A strategy for incorporating the evaluation results from both the SLM and LLMs. Our empirical results demonstrate that our approach achieves state-of-the-art performance in both the classification and evaluation tasks, and additionally the SLIDE evaluator exhibits better correlation with human judgements. Our code is available at https://github.com/hegehongcha/SLIDE-ACL2024.</abstract>
      <url hash="2a1af695">2024.findings-acl.911</url>
      <bibkey>zhao-etal-2024-slide</bibkey>
      <doi>10.18653/v1/2024.findings-acl.911</doi>
    </paper>
    <paper id="912">
      <title>Deep Exploration of Cross-Lingual Zero-Shot Generalization in Instruction Tuning</title>
      <author><first>Janghoon</first><last>Han</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Changho</first><last>Lee</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Joongbo</first><last>Shin</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Stanley Jungkyu</first><last>Choi</last><affiliation>Language Lab, LG AI Research</affiliation></author>
      <author><first>Honglak</first><last>Lee</last><affiliation>University of Michigan - Ann Arbor and LG AI Research</affiliation></author>
      <author><first>Kyunghoon</first><last>Bae</last></author>
      <pages>15436-15452</pages>
      <abstract>Instruction tuning has emerged as a powerful technique, significantly boosting zero-shot performance on unseen tasks. While recent work has explored cross-lingual generalization by applying instruction tuning to multilingual models, previous studies have primarily focused on English, with a limited exploration of non-English tasks. For in-depth exploration of cross-lingual generalization in instruction tuning, we perform instruction tuning individually for two distinct language meta-datasets. Subsequently, we assess the performance on unseen tasks in the language different from the one used for training. To facilitate this investigation, we introduce a novel non-English meta-dataset named “KORANI” (Korean Natural Instruction), comprising 51 Korean benchmarks. Moreover, we design cross-lingual templates to mitigate discrepancies in language and instruction-format of the template between training and inference within the cross-lingual setting. Our experiments reveal consistent improvements through cross-lingual generalization in both English and Korean, outperforming baseline by average scores of 20.7% and 13.6%, respectively. Remarkably, these enhancements are comparable to those achieved by mono-lingual instruction tuning and even surpass them in some tasks. The result underscores the significance of relevant data acquisition across languages over linguistic congruence with unseen tasks during instruction tuning.</abstract>
      <url hash="7567d2ca">2024.findings-acl.912</url>
      <bibkey>han-etal-2024-deep</bibkey>
      <doi>10.18653/v1/2024.findings-acl.912</doi>
    </paper>
    <paper id="913">
      <title>What Makes Language Models Good-enough?</title>
      <author><first>Daiki</first><last>Asami</last></author>
      <author><first>Saku</first><last>Sugawara</last><affiliation>National Institute of Informatics</affiliation></author>
      <pages>15453-15467</pages>
      <abstract>Psycholinguistic research suggests that humans may build a representation of linguistic input that is ‘good-enough’ for the task at hand. This study examines what architectural features make language models learn human-like good-enough language processing. We focus on the number of layers and self-attention heads in Transformers. We create a good-enough language processing (GELP) evaluation dataset (7,680 examples), which is designed to test the effects of two plausibility types, eight construction types, and three degrees of memory cost on language processing. To annotate GELP, we first conduct a crowdsourcing experiment whose design follows prior psycholinguistic studies. Our model evaluation against the annotated GELP then reveals that the full model as well as models with fewer layers and/or self-attention heads exhibit a good-enough performance. This result suggests that models with shallower depth and fewer heads can learn good-enough language processing.</abstract>
      <url hash="be46c5eb">2024.findings-acl.913</url>
      <bibkey>asami-sugawara-2024-makes</bibkey>
      <doi>10.18653/v1/2024.findings-acl.913</doi>
    </paper>
    <paper id="914">
      <title>Refining Corpora from a Model Calibration Perspective for <fixed-case>C</fixed-case>hinese Spelling Correction</title>
      <author><first>Dingyao</first><last>Yu</last></author>
      <author><first>Yang</first><last>An</last></author>
      <author><first>Wei</first><last>Ye</last><affiliation>Peking University</affiliation></author>
      <author><first>Xiongfeng</first><last>Xiao</last></author>
      <author><first>Shaoguang</first><last>Mao</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Shikun</first><last>Zhang</last><affiliation>Peking University</affiliation></author>
      <pages>15468-15480</pages>
      <abstract>Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality corpora, due to the labor-intensive labeling of spelling errors in real-life human writing or typing scenarios. Two data augmentation methods are widely adopted: (1) *Random Replacement* with the guidance of confusion sets and (2) *OCR/ASR-based Generation* that simulates character misusing. However, both methods inevitably introduce noisy data (e.g., false spelling errors), potentially leading to over-correction. By carefully analyzing the two types of corpora, we find that though the latter achieves more robust generalization performance, the former yields better-calibrated CSC models. We then provide a theoretical analysis of this empirical observation, based on which a corpus refining strategy is proposed. Specifically, OCR/ASR-based data samples are fed into a well-calibrated CSC model trained on random replacement-based corpora and then filtered based on prediction confidence. By learning a simple BERT-based model on the refined OCR/ASR-based corpus, we set up impressive state-of-the-art performance on three widely-used benchmarks, while significantly alleviating over-correction (e.g., lowering false positive predictions).</abstract>
      <url hash="0116517b">2024.findings-acl.914</url>
      <bibkey>yu-etal-2024-refining</bibkey>
      <doi>10.18653/v1/2024.findings-acl.914</doi>
    </paper>
    <paper id="915">
      <title><fixed-case>C</fixed-case>ounter<fixed-case>C</fixed-case>urate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</title>
      <author><first>Jianrui</first><last>Zhang</last></author>
      <author><first>Mu</first><last>Cai</last><affiliation>Department of Computer Science, University of Wisconsin, Madison</affiliation></author>
      <author><first>Tengyang</first><last>Xie</last><affiliation>Department of Computer Science, University of Wisconsin - Madison</affiliation></author>
      <author><first>Yong Jae</first><last>Lee</last><affiliation>Department of Computer Sciences, University of Wisconsin - Madison and Cruise</affiliation></author>
      <pages>15481-15495</pages>
      <abstract>We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two critical under- explored problems: the neglect of physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach in addressing these gaps.We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using the grounded image generation model GLIGEN to generate fine-tuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V.To facilitate future research, we release ourcode, dataset, benchmark, and checkpoints at https://countercurate.github.io/</abstract>
      <url hash="279ef3e0">2024.findings-acl.915</url>
      <bibkey>zhang-etal-2024-countercurate</bibkey>
      <doi>10.18653/v1/2024.findings-acl.915</doi>
    </paper>
    <paper id="916">
      <title>Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models</title>
      <author><first>Ran</first><last>Xu</last><affiliation>Emory University</affiliation></author>
      <author><first>Hejie</first><last>Cui</last><affiliation>Stanford University</affiliation></author>
      <author><first>Yue</first><last>Yu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Xuan</first><last>Kan</last></author>
      <author><first>Wenqi</first><last>Shi</last><affiliation>University of Texas Southwestern Medical Center</affiliation></author>
      <author><first>Yuchen</first><last>Zhuang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>May Dongmei</first><last>Wang</last></author>
      <author><first>Wei</first><last>Jin</last><affiliation>Emory University</affiliation></author>
      <author><first>Joyce</first><last>Ho</last><affiliation>Emory University</affiliation></author>
      <author><first>Carl</first><last>Yang</last><affiliation>Emory University</affiliation></author>
      <pages>15496-15523</pages>
      <abstract>Clinical natural language processing faces challenges like complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation with LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 8 clinical NLP tasks and 18 datasets reveals that ClinGen consistently enhances performance across various tasks by 7.7%-8.7% on average, effectively aligning the distribution of real datasets and enriching the diversity of generated training instances.</abstract>
      <url hash="9d1862bf">2024.findings-acl.916</url>
      <bibkey>xu-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.findings-acl.916</doi>
    </paper>
    <paper id="917">
      <title>Textless Acoustic Model with Self-Supervised Distillation for Noise-Robust Expressive Speech-to-Speech Translation</title>
      <author><first>Min-Jae</first><last>Hwang</last><affiliation>Facebook</affiliation></author>
      <author><first>Ilia</first><last>Kulikov</last><affiliation>Facebook</affiliation></author>
      <author><first>Benjamin</first><last>Peloquin</last><affiliation>Facebook</affiliation></author>
      <author><first>Hongyu</first><last>Gong</last><affiliation>FAIR at Meta</affiliation></author>
      <author><first>Peng-Jen</first><last>Chen</last><affiliation>Facebook</affiliation></author>
      <author><first>Ann</first><last>Lee</last><affiliation>Facebook</affiliation></author>
      <pages>15524-15541</pages>
      <abstract>In this paper, we propose a textless acoustic model with a self-supervised distillation strategy for noise-robust expressive speech-to-speech translation (S2ST).Recently proposed expressive S2ST systems have achieved impressive expressivity preservation performances by cascading unit-to-speech (U2S) generator to the speech-to-unit translation model. However, these systems are vulnerable to the presence of noise in input speech, which is an assumption in real-world translation scenarios. To address this limitation, we propose a U2S generator that incorporates a distillation with no label (DINO) self-supervised training strategy into it’s pretraining process.Because the proposed method captures noise-agnostic expressivity representation, it can generate qualified speech even in noisy environment.Objective and subjective evaluation results verified that the proposed method significantly improved the performance of the expressive S2ST system in noisy environments while maintaining competitive performance in clean environments.</abstract>
      <url hash="484a4ea9">2024.findings-acl.917</url>
      <bibkey>hwang-etal-2024-textless</bibkey>
      <doi>10.18653/v1/2024.findings-acl.917</doi>
    </paper>
    <paper id="918">
      <title>Knowledge-Infused Legal Wisdom: Navigating <fixed-case>LLM</fixed-case> Consultation through the Lens of Diagnostics and Positive-Unlabeled Reinforcement Learning</title>
      <author><first>Yang</first><last>Wu</last></author>
      <author><first>Chenghao</first><last>Wang</last><affiliation>Peking University</affiliation></author>
      <author><first>Ece</first><last>Gumusel</last></author>
      <author><first>Xiaozhong</first><last>Liu</last><affiliation>Worcester Polytechnic Institute</affiliation></author>
      <pages>15542-15555</pages>
      <abstract>The integration of generative Large Language Models (LLMs) into various applications, including the legal domain, has been accelerated by their expansive and versatile nature. However, when facing a legal case, users without a legal background often struggle to formulate professional queries and may inadvertently overlook critical legal factors when presenting their case narrative to LLMs. To address this issue, we propose the Diagnostic Legal Large Language Model (D3LM), which utilizes adaptive lawyer-like diagnostic questions to collect additional case information and then provides high-quality feedback. D3LM incorporates an innovative graph-based Positive-Unlabeled Reinforcement Learning (PURL) algorithm, enabling the generation of critical questions and enhancing user-LLM interactions. Moreover, an integrated LLM-based stopping criterion facilitates precise Court Views Generation (CVG). Our research also introduces a new English-language CVG dataset based on the US case law database, enriching the realm of LLM research and deployment with a vital dimension. D3LM surpasses classical LLMs by delivering outstanding performance and a remarkable user experience in the legal domain.</abstract>
      <url hash="fe393781">2024.findings-acl.918</url>
      <bibkey>wu-etal-2024-knowledge</bibkey>
      <doi>10.18653/v1/2024.findings-acl.918</doi>
    </paper>
    <paper id="919">
      <title><fixed-case>TELLER</fixed-case>: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection</title>
      <author><first>Hui</first><last>Liu</last><affiliation>City University of Hong Kong</affiliation></author>
      <author><first>Wenya</first><last>Wang</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Haoru</first><last>Li</last></author>
      <author><first>Haoliang</first><last>Li</last><affiliation>City University of Hong Kong</affiliation></author>
      <pages>15556-15583</pages>
      <abstract>The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose TELLER, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process. Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework.</abstract>
      <url hash="45017d79">2024.findings-acl.919</url>
      <bibkey>liu-etal-2024-teller</bibkey>
      <doi>10.18653/v1/2024.findings-acl.919</doi>
    </paper>
    <paper id="920">
      <title>Verifiable Generation with Subsentence-Level Fine-Grained Citations</title>
      <author><first>Shuyang</first><last>Cao</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>15584-15596</pages>
      <abstract>Verifiable generation requires large language models (LLMs) to cite source documents supporting their outputs, thereby improve output transparency and trustworthiness. Yet, previous work mainly targets the generation of sentence-level citations, lacking specificity about which parts of a sentence are backed by the cited sources. This work studies verifiable generation with subsentence-level fine-grained citations for more precise location of generated content supported by the cited sources. We first present a dataset, SCiFi, comprising 10K Wikipedia paragraphs with subsentence-level citations. Each paragraph is paired with a set of candidate source documents for citation and a query that triggers the generation of the paragraph content. On SCiFi, we evaluate the performance of state-of-the-art LLMs and strategies for processing long documents designed for these models. Our experiment results reveals key factors that could enhance the quality of citations, including the expansion of the source documents’ context accessible to the models and the implementation of specialized model tuning.</abstract>
      <url hash="eec48a2b">2024.findings-acl.920</url>
      <bibkey>cao-wang-2024-verifiable</bibkey>
      <doi>10.18653/v1/2024.findings-acl.920</doi>
    </paper>
    <paper id="921">
      <title>Tailoring with Targeted Precision: Edit-Based Agents for Open-Domain Procedure Customization</title>
      <author><first>Yash Kumar</first><last>Lal</last><affiliation>State University of New York, Stony Brook</affiliation></author>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Faeze</first><last>Brahman</last><affiliation>Allen Institute for AI</affiliation></author>
      <author><first>Bodhisattwa Prasad</first><last>Majumder</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Peter</first><last>Clark</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Niket</first><last>Tandon</last><affiliation>Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>15597-15611</pages>
      <abstract>How-to procedures, such as how to plant a garden, are now used by millions of users, but sometimes need customizing to meet a user’s specific needs, e.g., planting a garden without pesticides. Our goal is to measure and improve an LLM’s ability to perform such customization. Our approach is to test several simple multi-LLM-agent architectures for customization, as well as an end-to-end LLM, using a new evaluation set, called CustomPlans, of over 200 WikiHow procedures each with a customization need. We find that a simple architecture with two LLM agents used sequentially performs best, one that edits a generic how-to procedure and one that verifies its executability, significantly outperforming (10.5% absolute) an end-to-end prompted LLM. This suggests that LLMs can be configured reasonably effectively for procedure customization. This also suggests that multi-agent editing architectures may be worth exploring further for other customization applications (e.g. coding, creative writing) in the future.</abstract>
      <url hash="e5550d9f">2024.findings-acl.921</url>
      <bibkey>lal-etal-2024-tailoring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.921</doi>
    </paper>
    <paper id="922">
      <title>A Meta-Learning Perspective on Transformers for Causal Language Modeling</title>
      <author><first>Xinbo</first><last>Wu</last><affiliation>University of Illinois, Urbana Champaign</affiliation></author>
      <author><first>Lav</first><last>Varshney</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <pages>15612-15622</pages>
      <abstract>The Transformer architecture has become prominent in developing large causal language models. However, mechanisms to explain its capabilities are not well understood. Focused on the training process, here we establish a meta-learning view of the Transformer architecture when trained for the causal language modeling task, by explicating an inner optimization process that may happen within the Transformer. Further, from within the inner optimization, we discover and theoretically analyze a special characteristic of the norms of learned token representations within Transformer-based causal language models. Our analysis is supported by experiments conducted on pre-trained large language models and real-world data.</abstract>
      <url hash="8641677f">2024.findings-acl.922</url>
      <bibkey>wu-varshney-2024-meta</bibkey>
      <doi>10.18653/v1/2024.findings-acl.922</doi>
    </paper>
    <paper id="923">
      <title><fixed-case>PL</fixed-case>a<fixed-case>D</fixed-case>: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs</title>
      <author><first>Rongzhi</first><last>Zhang</last><affiliation>Georgia Institute of Technology and Zhejiang University</affiliation></author>
      <author><first>Jiaming</first><last>Shen</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Tianqi</first><last>Liu</last><affiliation>Google</affiliation></author>
      <author><first>Haorui</first><last>Wang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Zhen</first><last>Qin</last><affiliation>Google</affiliation></author>
      <author><first>Feng</first><last>Han</last><affiliation>Research, Google</affiliation></author>
      <author><first>Jialu</first><last>Liu</last><affiliation>Google Research</affiliation></author>
      <author><first>Simon</first><last>Baumgartner</last><affiliation>Google</affiliation></author>
      <author><first>Michael</first><last>Bendersky</last><affiliation>Google</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>15623-15636</pages>
      <abstract>Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate the student’s estimation of sequence likelihood, which steers the student’s focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM’s internal states, tackles the student’s expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework.</abstract>
      <url hash="3fca82ca">2024.findings-acl.923</url>
      <bibkey>zhang-etal-2024-plad</bibkey>
      <doi>10.18653/v1/2024.findings-acl.923</doi>
    </paper>
    <paper id="924">
      <title>Small Language Models Need Strong Verifiers to Self-Correct Reasoning</title>
      <author><first>Yunxiang</first><last>Zhang</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Muhammad</first><last>Khalifa</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Lajanugen</first><last>Logeswaran</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Jaekyeom</first><last>Kim</last><affiliation>LG AI Research</affiliation></author>
      <author><first>Moontae</first><last>Lee</last><affiliation>University of Illinois, Chicago</affiliation></author>
      <author><first>Honglak</first><last>Lee</last><affiliation>University of Michigan - Ann Arbor and LG AI Research</affiliation></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>15637-15653</pages>
      <abstract>Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether small (<tex-math>\leq 13</tex-math>B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs. We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities. First, we leverage correct solutions to guide the model in critiquing their incorrect responses. Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement. Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.</abstract>
      <url hash="94b0bf6d">2024.findings-acl.924</url>
      <bibkey>zhang-etal-2024-small</bibkey>
      <doi>10.18653/v1/2024.findings-acl.924</doi>
    </paper>
    <paper id="925">
      <title>Hire a Linguist!: Learning Endangered Languages in <fixed-case>LLM</fixed-case>s with In-Context Linguistic Descriptions</title>
      <author><first>Kexun</first><last>Zhang</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Yee</first><last>Choi</last></author>
      <author><first>Zhenqiao</first><last>Song</last></author>
      <author><first>Taiqi</first><last>He</last></author>
      <author><first>William Yang</first><last>Wang</last><affiliation>UC Santa Barbara</affiliation></author>
      <author><first>Lei</first><last>Li</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>15654-15669</pages>
      <abstract>How can large language models (LLMs) process and translate endangered languages? Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages. On the contrary, we observe that 2000 endangered languages, though without a large corpus, have a grammar book or a dictionary. We propose LingoLLM, a training-free approach to enable an LLM to process unseen languages that hardly occur in its pre-training. Our key insight is to demonstrate linguistic knowledge of an unseen language in an LLM’s prompt, including a dictionary, a grammar book, and morphologically analyzed input text. We implement LingoLLM on top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks across 8 endangered or low-resource languages. Our results show that LingoLLM elevates translation capability from GPT-4’s 0 to 10.5 BLEU for 10 language directions. Our findings demonstrate the tremendous value of linguistic knowledge in the age of LLMs for endangered languages. Our data, code, and model generations will be released to the public. Our data, code, and model generations can be found at <url>https://github.com/LLiLab/llm4endangeredlang</url>.</abstract>
      <url hash="865d5cd3">2024.findings-acl.925</url>
      <bibkey>zhang-etal-2024-hire</bibkey>
      <doi>10.18653/v1/2024.findings-acl.925</doi>
    </paper>
    <paper id="926">
      <title>From Tarzan to <fixed-case>T</fixed-case>olkien: Controlling the Language Proficiency Level of <fixed-case>LLM</fixed-case>s for Content Generation</title>
      <author><first>Ali</first><last>Malik</last><affiliation>Stanford University</affiliation></author>
      <author><first>Stephen</first><last>Mayhew</last><affiliation>Duolingo</affiliation></author>
      <author><first>Christopher</first><last>Piech</last></author>
      <author><first>Klinton</first><last>Bicknell</last><affiliation>Duolingo</affiliation></author>
      <pages>15670-15693</pages>
      <abstract>We study the problem of controlling the difficulty level of text generated by Large Language Models (LLMs) for contexts where end-users are not fully proficient, such as language learners. Using a novel framework, we evaluate the effectiveness of several key approaches for this task, including few-shot prompting, supervised finetuning, and reinforcement learning (RL), utilising both GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.Our findings reveal a large performance gap between GPT-4 and the open source models when using prompt-based strategies. However, we show how to bridge this gap with a careful combination of finetuning and RL alignment. Our best model, CALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and other strategies, at only a fraction of the cost. We further validate the quality of our results through a small-scale human study.</abstract>
      <url hash="497662da">2024.findings-acl.926</url>
      <bibkey>malik-etal-2024-tarzan</bibkey>
      <doi>10.18653/v1/2024.findings-acl.926</doi>
    </paper>
    <paper id="927">
      <title>From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards</title>
      <author><first>Khaoula</first><last>Chehbouni</last></author>
      <author><first>Megha</first><last>Roshan</last></author>
      <author><first>Emmanuel</first><last>Ma</last></author>
      <author><first>Futian</first><last>Wei</last></author>
      <author><first>Afaf</first><last>Taik</last></author>
      <author><first>Jackie</first><last>Cheung</last><affiliation>McGill University, Mila Research Institute and Microsoft</affiliation></author>
      <author><first>Golnoosh</first><last>Farnadi</last></author>
      <pages>15694-15710</pages>
      <abstract>Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations.Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluating models on already mitigated biases. Using the case of Llama 2 as an example, we illustrate how LLMs’ safety responses can still encode harmful assumptions. To do so, we create a set of non-toxic prompts, which we then use to evaluate Llama models. Through our new taxonomy of LLMs responses to users, we observe that the safety/helpfulness trade-offs are more pronounced for certain demographic groups which can lead to different kinds of harms such as quality-of-service harms for marginalized populations.</abstract>
      <url hash="961bc360">2024.findings-acl.927</url>
      <bibkey>chehbouni-etal-2024-representational</bibkey>
      <doi>10.18653/v1/2024.findings-acl.927</doi>
    </paper>
    <paper id="928">
      <title><fixed-case>CT</fixed-case>ool<fixed-case>E</fixed-case>val: A <fixed-case>C</fixed-case>hinese Benchmark for <fixed-case>LLM</fixed-case>-Powered Agent Evaluation in Real-World <fixed-case>API</fixed-case> Interactions</title>
      <author><first>Zishan</first><last>Guo</last></author>
      <author><first>Yufei</first><last>Huang</last></author>
      <author><first>Deyi</first><last>Xiong</last><affiliation>Tianjin University</affiliation></author>
      <pages>15711-15724</pages>
      <abstract>Assessing the capabilities of large language models (LLMs) as agents in decision making and operational tasks is crucial for the development of LLM-as-agent service. We propose CToolEval, a benchmark designed to evaluate LLMs in the context of Chinese societal applications, featuring 398 APIs across 27 widely-used Apps (e.g., Apps for shopping, map, music, travel, etc.) that cover 14 domains. We further present an evaluation framework that simulates real-life scenarios, to facilitate the assessment of tool invocation ability of LLMs for tool learning and task completion ability for user interation. Our extensive experiments with CToolEval evaluate 11 LLMs, revealing that while GPT-3.5-turbo excels in tool invocation, Chinese LLMs usually struggle with issues like hallucination and a lack of comprehensive tool understanding. Our findings highlight the need for further refinement in decision-making capabilities of LLMs, offering insights into bridging the gap between current functionalities and agent-level performance. To promote further research for LLMs to fully act as reliable agents in complex, real-world situations, we release our data and codes at https://github.com/tjunlp-lab/CToolEval.</abstract>
      <url hash="1bfb4ec5">2024.findings-acl.928</url>
      <bibkey>guo-etal-2024-ctooleval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.928</doi>
    </paper>
    <paper id="929">
      <title>Token Alignment via Character Matching for Subword Completion</title>
      <author><first>Ben</first><last>Athiwaratkun</last><affiliation>Amazon</affiliation></author>
      <author><first>Shiqi</first><last>Wang</last><affiliation>Amazon</affiliation></author>
      <author><first>Mingyue</first><last>Shang</last><affiliation>Amazon</affiliation></author>
      <author><first>Yuchen</first><last>Tian</last></author>
      <author><first>Zijian</first><last>Wang</last><affiliation>Amazon AWS AI Labs</affiliation></author>
      <author><first>Sujan Kumar</first><last>Gonugondla</last><affiliation>Amazon</affiliation></author>
      <author><first>Sanjay Krishna</first><last>Gouda</last><affiliation>Amazon</affiliation></author>
      <author><first>Robert</first><last>Kwiatkowski</last><affiliation>Amazon</affiliation></author>
      <author><first>Ramesh</first><last>Nallapati</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Parminder</first><last>Bhatia</last></author>
      <author><first>Bing</first><last>Xiang</last><affiliation>Amazon</affiliation></author>
      <pages>15725-15738</pages>
      <abstract>Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens. This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model’s generation aligns with the prompt. This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like code completion and text.</abstract>
      <url hash="d3683a39">2024.findings-acl.929</url>
      <bibkey>athiwaratkun-etal-2024-token</bibkey>
      <doi>10.18653/v1/2024.findings-acl.929</doi>
    </paper>
    <paper id="930">
      <title>Rethinking Efficient Multilingual Text Summarization Meta-Evaluation</title>
      <author><first>Rilyn</first><last>Han</last></author>
      <author><first>Jiawen</first><last>Chen</last></author>
      <author><first>Yixin</first><last>Liu</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>15739-15746</pages>
      <abstract>Evaluating multilingual summarization evaluation metrics, i.e., meta-evaluation, is challenging because of the difficulty of human annotation collection. Therefore, we investigate an efficient multilingual meta-evaluation framework that uses machine translation systems to transform a monolingual meta-evaluation dataset into multilingual versions. To this end, we introduce a statistical test to verify the transformed dataset quality by checking the meta-evaluation result consistency on the original dataset and back-translated dataset. With this quality verification method, we transform an existing English summarization meta-evaluation dataset, RoSE, into 30 languages, and conduct a multilingual meta-evaluation of several representative automatic evaluation metrics. In our meta-evaluation, we find that metric performance varies in different languages and neural metrics generally outperform classical text-matching-based metrics in non-English languages. Moreover, we identify a two-stage evaluation method with superior performance, which first translates multilingual texts into English and then performs evaluation. We make the transformed datasets publicly available to facilitate future research.</abstract>
      <url hash="e86474dc">2024.findings-acl.930</url>
      <bibkey>han-etal-2024-rethinking</bibkey>
      <doi>10.18653/v1/2024.findings-acl.930</doi>
    </paper>
    <paper id="931">
      <title>emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation</title>
      <author><first>Ziyang</first><last>Ma</last></author>
      <author><first>Zhisheng</first><last>Zheng</last><affiliation>Research, Microsoft</affiliation></author>
      <author><first>Jiaxin</first><last>Ye</last></author>
      <author><first>Jinchao</first><last>Li</last></author>
      <author><first>Zhifu</first><last>Gao</last></author>
      <author><first>ShiLiang</first><last>Zhang</last></author>
      <author><first>Xie</first><last>Chen</last></author>
      <pages>15747-15760</pages>
      <abstract>We propose emotion2vec, a universal speech emotion representation model. emotion2vec is pre-trained on open-source unlabeled emotion data through self-supervised online distillation, combining utterance-level loss and frame-level loss during pre-training. emotion2vec outperforms state-of-the-art pre-trained universal models and emotion specialist models by only training linear layers for the speech emotion recognition task on the mainstream IEMOCAP dataset. In addition, emotion2vec shows consistent improvements among 10 different languages of speech emotion recognition datasets. emotion2vec also shows excellent results on other emotion tasks, such as song emotion recognition, emotion prediction in conversation, and sentiment analysis. Comparison experiments, ablation experiments, and visualization comprehensively demonstrate the universal capability of the proposed emotion2vec. To the best of our knowledge, emotion2vec is the first universal representation model in various emotion-related tasks, filling a gap in the field.</abstract>
      <url hash="a796224d">2024.findings-acl.931</url>
      <bibkey>ma-etal-2024-emotion2vec</bibkey>
      <doi>10.18653/v1/2024.findings-acl.931</doi>
    </paper>
    <paper id="932">
      <title>Language-Informed Beam Search Decoding for Multilingual Machine Translation</title>
      <author><first>Yilin</first><last>Yang</last><affiliation>Oregon State University</affiliation></author>
      <author><first>Stefan</first><last>Lee</last><affiliation>Oregon State University</affiliation></author>
      <author><first>Prasad</first><last>Tadepalli</last><affiliation>Oregon State University and Oregon State University</affiliation></author>
      <pages>15761-15772</pages>
      <abstract>Beam search decoding is the de-facto method for decoding auto-regressive Neural Machine Translation (NMT) models, including multilingual NMT where the target language is specified as an input. However, decoding multilingual NMT models commonly produces off-target translations – yielding translation outputs not in the intended language.In this paper, we first conduct an error analysis of off-target translations for a strong multilingual NMT model and identify how these decodings are produced during beam search. We then propose Language-informed Beam Search (LiBS), a general decoding algorithm incorporating an off-the-shelf Language Identification (LiD) model into beam search decoding to reduce off-target translations. LiBS is an inference-time procedure that is NMT-model agnostic and does not require any additional parallel data. Results show that our proposed LiBS algorithm on average improves +1.1 BLEU and +0.9 BLEU on WMT and OPUS datasets, and reduces off-target rates from 22.9% to 7.7% and 65.8% to 25.3% respectively.</abstract>
      <url hash="a7ba9516">2024.findings-acl.932</url>
      <bibkey>yang-etal-2024-language-informed</bibkey>
      <doi>10.18653/v1/2024.findings-acl.932</doi>
    </paper>
    <paper id="933">
      <title><fixed-case>RA</fixed-case>-<fixed-case>L</fixed-case>o<fixed-case>RA</fixed-case>: Rank-Adaptive Parameter-Efficient Fine-Tuning for Accurate 2-bit Quantized Large Language Models</title>
      <author><first>Minsoo</first><last>Kim</last></author>
      <author><first>Sihwa</first><last>Lee</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Wonyong</first><last>Sung</last><affiliation>Seoul National University</affiliation></author>
      <author><first>Jungwook</first><last>Choi</last><affiliation>Hanyang University</affiliation></author>
      <pages>15773-15786</pages>
      <abstract>Deploying large language models (LLMs) with their extensive parameters and high memory demands challenges computational efficiency, particularly in fine-tuning for specific applications with limited resources. Techniques like Low-Rank Adaptation (LoRA) help by training a smaller, modifiable extension of the base model to reduce memory usage. However, combining quantization with LoRA, especially in low-bit scenarios, can lead to performance losses due to quantization errors. Our innovative Rank-Adaptive LoRA (RA-LoRA) addresses this by dynamically adjusting the adapter’s rank using rank-subspace analysis, optimizing performance with fewer parameters. We tested RA-LoRA on state-of-the-art LLMs for 2-bit efficient fine-tuning, showing it can improve model accuracy with minimal trainable parameters, marking a leap forward in quantization-aware fine-tuning methods and highlighting the significance of rank dynamics in optimizing quantized LLMs.</abstract>
      <url hash="11a99399">2024.findings-acl.933</url>
      <bibkey>kim-etal-2024-ra</bibkey>
      <doi>10.18653/v1/2024.findings-acl.933</doi>
    </paper>
    <paper id="934">
      <title>The <fixed-case>PGNSC</fixed-case> Benchmark: How Do We Predict Where Information Spreads?</title>
      <author><first>Alexander</first><last>Taylor</last><affiliation>UCLA Computer Science Department, University of California, Los Angeles</affiliation></author>
      <author><first>Wei</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <pages>15787-15803</pages>
      <abstract>Social networks have become ideal vehicles for news dissemination because posted content is easily able to reach users beyond a news outlet’s direct audience. Understanding how information is transmitted among communities of users is a critical step towards understanding the impact social networks have on real-world events. Two significant barriers in this vein of work are identifying user clusters and meaningfully characterizing these communities. Thus, we propose the PGNSC benchmark, which builds information pathways based on the audiences of influential news sources and uses their content to characterize the communities. We present methods of aggregating these news-source-centric communities and for constructing the community feature representations that are used sequentially to construct information pathway prediction pipelines. Lastly, we perform extensive experiments to demonstrate the performance of baseline pipeline constructions and to highlight the possibilities for future work.</abstract>
      <url hash="49d1b7ac">2024.findings-acl.934</url>
      <bibkey>taylor-wang-2024-pgnsc</bibkey>
      <doi>10.18653/v1/2024.findings-acl.934</doi>
    </paper>
    <paper id="935">
      <title><fixed-case>STARLING</fixed-case>: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models</title>
      <author><first>Shreyas</first><last>Basavatia</last></author>
      <author><first>Keerthiram</first><last>Murugesan</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Shivam</first><last>Ratnakar</last><affiliation>International Business Machines</affiliation></author>
      <pages>15804-15819</pages>
      <abstract>Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents. Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills. In this work, we introduce an interactive environment for self-supervised RL, STARLING, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment. These games let the agent hone their skills on a predefined set of tasks. We create and test an environment with 100 games, generated using this automated framework that uses large language models (GPT3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision. Experimental results based on both the human participants and baseline text-based RL agents reveal that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can. These results enforce STARLING’s potential to serve as a sandbox environment for further research in self-supervised text-based RL.</abstract>
      <url hash="8af2edd2">2024.findings-acl.935</url>
      <bibkey>basavatia-etal-2024-starling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.935</doi>
    </paper>
    <paper id="936">
      <title>Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models</title>
      <author><first>Dohyun</first><last>Lee</last></author>
      <author><first>Daniel</first><last>Rim</last><affiliation>Hyundai Motor Group and KAIST</affiliation></author>
      <author><first>Minseok</first><last>Choi</last></author>
      <author><first>Jaegul</first><last>Choo</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>15820-15839</pages>
      <abstract>Although language models (LMs) demonstrate exceptional capabilities on various tasks, they are potentially vulnerable to extraction attacks, which represent a significant privacy risk.To mitigate the privacy concerns of LMs, machine unlearning has emerged as an important research area, which is utilized to induce the LM to selectively forget about some of its training data.While completely retraining the model will guarantee successful unlearning and privacy assurance, it is impractical for LMs, as it would be time-consuming and resource-intensive.Prior works efficiently unlearn the target token sequences, but upon subsequent iterations, the LM displays significant degradation in performance.In this work, we propose <tex-math>\textbf{P}</tex-math>rivacy Protection via <tex-math>\textbf{O}</tex-math>ptimal <tex-math>\textbf{P}</tex-math>arameters (POP), a novel unlearning method that effectively forgets the target token sequences from the pretrained LM by applying optimal gradient updates to the parameters.Inspired by the gradient derivation of complete retraining, we approximate the optimal training objective that successfully unlearns the target sequence while retaining the knowledge from the rest of the training data.Experimental results demonstrate that POP exhibits remarkable retention performance post-unlearning across 9 classification and 4 dialogue benchmarks, outperforming the state-of-the-art by a large margin.Furthermore, we introduce Remnant Memorization Accuracy that quantifies privacy risks based on token likelihood and validate its effectiveness through both qualitative and quantitative analyses.</abstract>
      <url hash="6b3e8dda">2024.findings-acl.936</url>
      <bibkey>lee-etal-2024-protecting</bibkey>
      <doi>10.18653/v1/2024.findings-acl.936</doi>
    </paper>
    <paper id="937">
      <title>Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding</title>
      <author><first>Xintong</first><last>Wang</last></author>
      <author><first>Jingheng</first><last>Pan</last></author>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Chris</first><last>Biemann</last><affiliation>U Hamburg</affiliation></author>
      <pages>15840-15853</pages>
      <abstract>Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations. Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs.</abstract>
      <url hash="5260c549">2024.findings-acl.937</url>
      <bibkey>wang-etal-2024-mitigating</bibkey>
      <doi>10.18653/v1/2024.findings-acl.937</doi>
    </paper>
    <paper id="938">
      <title>Fine-tuning Language Models for Joint Rewriting and Completion of Code with Potential Bugs</title>
      <author><first>Dingmin</first><last>Wang</last></author>
      <author><first>Jinman</first><last>Zhao</last><affiliation>Amazon</affiliation></author>
      <author><first>Hengzhi</first><last>Pei</last></author>
      <author><first>Samson</first><last>Tan</last><affiliation>Amazon</affiliation></author>
      <author><first>Sheng</first><last>Zha</last><affiliation>Amazon</affiliation></author>
      <pages>15854-15868</pages>
      <abstract>Handling drafty partial code remains a notable challenge in real-time code suggestion applications. Previous work has demonstrated shortcomings of large language models of code (CodeLLMs) in completing partial code with potential bugs. In this study, we view partial code as implementation hints and fine-tune CodeLLMs to jointly rewrite and complete partial code into functional full programs. We explore two strategies: one-pass generation and multi-pass iterative refinement. We construct new training and testing datasets using semantic-altering code transformations and iterative self-generations.We conduct comprehensive experiments over three representative open-sourced CodeLLMs – InCoder, CodeGen, and StarCoder.Results show that CodeLLMs fine-tuned using our approach achieve superior pass rates compared to the previous baselines across existing and newly-created benchmarks, effectively handle both potentially buggy and clean code, and largely preserve the integrity of the original partial implementations. We further present findings on the properties of the potential bugs we tested and on the design choices of our methods.</abstract>
      <url hash="df895caf">2024.findings-acl.938</url>
      <bibkey>wang-etal-2024-fine-tuning</bibkey>
      <doi>10.18653/v1/2024.findings-acl.938</doi>
    </paper>
    <paper id="939">
      <title>A Critical Study of What Code-<fixed-case>LLM</fixed-case>s (Do Not) Learn</title>
      <author><first>Abhinav</first><last>Anand</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Shweta</first><last>Verma</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Krishna</first><last>Narasimhan</last></author>
      <author><first>Mira</first><last>Mezini</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <pages>15869-15889</pages>
      <abstract>Large Language Models trained on code corpora (code-LLMs) have demonstrated impressive performance in various coding assistance tasks. However, despite their increased size and training dataset, code-LLMs still have limitations such as suggesting codes with syntactic errors, variable misuse etc. Some studies argue that code-LLMs perform well on coding tasks because they use self-attention and hidden representations to encode relations among input tokens. However, previous works have not studied what code properties are not encoded by code-LLMs. In this paper, we conduct a fine-grained analysis of attention maps and hidden representations of code-LLMs. Our study indicates that code-LLMs only encode relations among specific subsets of input tokens. Specifically, by categorizing input tokens into syntactic tokens and identifiers, we found that models encode relations among syntactic tokens and among identifiers, but they fail to encode relations between syntactic tokens and identifiers. We also found that fine-tuned models encode these relations poorly compared to their pre-trained counterparts. Additionally, larger models with billions of parameters encode significantly less information about code than models with only a few hundred million parameters.</abstract>
      <url hash="236e58f6">2024.findings-acl.939</url>
      <bibkey>anand-etal-2024-critical</bibkey>
      <doi>10.18653/v1/2024.findings-acl.939</doi>
    </paper>
    <paper id="940">
      <title>Visual In-Context Learning for Large Vision-Language Models</title>
      <author><first>Yucheng</first><last>Zhou</last><affiliation>University of Macau</affiliation></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Qianning</first><last>Wang</last></author>
      <author><first>Jianbing</first><last>Shen</last><affiliation>University of Macau</affiliation></author>
      <pages>15890-15902</pages>
      <abstract>In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning (ICL) remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual In-Context Learning (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented Demonstration Composition. Our approach retrieves images via ”Retrieval &amp; Rerank” paradigm, summarises images with task intent and task-specific visual parsing, and composes language-based demonstrations that reduce token count and alleviate cross-modal interaction problem. Experimental evaluations on five visual reasoning datasets demonstrate the effectiveness of our method. Moreover, our extensive experiments leverage information flow analysis to elucidate the effectiveness of our method, and investigate the impact of length and position of demonstrations for LVLM. The use of in-context unlearning further shows promise in resetting specific model knowledge without retraining.</abstract>
      <url hash="21c31c26">2024.findings-acl.940</url>
      <bibkey>zhou-etal-2024-visual</bibkey>
      <doi>10.18653/v1/2024.findings-acl.940</doi>
    </paper>
    <paper id="941">
      <title><fixed-case>SCALE</fixed-case>: Synergized Collaboration of Asymmetric Language Translation Engines</title>
      <author><first>Xin</first><last>Cheng</last><affiliation>Peking University</affiliation></author>
      <author><first>Xun</first><last>Wang</last><affiliation>Microsoft</affiliation></author>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Si-Qing</first><last>Chen</last></author>
      <author><first>Furu</first><last>Wei</last><affiliation>Microsoft Research</affiliation></author>
      <author><first>Dongyan</first><last>Zhao</last><affiliation>Peking University</affiliation></author>
      <author><first>Rui</first><last>Yan</last><affiliation>Renmin University of China</affiliation></author>
      <pages>15903-15918</pages>
      <abstract>In this paper, we introduce SCALE, a collaborative framework that connects a compact Specialized Translation Model (STM) and a general-purpose Large Language Model (LLM) as one unified translation engine. By introducing translation from STM into the triplet in-context demonstrations, SCALE unlocks refinement and pivoting ability of LLM, thus 1) mitigating language bias of LLMs and parallel data bias of STMs, 2) enhancing LLM speciality without sacrificing generality, and 3) facilitating continual learning in a LLM-tuning-free way.Our comprehensive experiments show that SCALE significantly outperforms both LLMs (GPT-4, GPT-3.5) and supervised models (NLLB, M2M) in either high-resource or challenging low-resource settings. Moreover SCALE shows great scalability by only updating the lightweight STM and witness consistent system improvement, an averaged 4 BLEURT score across 4 languages without tuning LLM. Interestingly, SCALE could also effectively exploit the existing language bias of LLMs by using an English-centric STM as a pivot to conduct translation between any language pairs, outperforming GPT-4 by an average of 6 COMET points across eight translation directions. Furthermore we provide an in-depth analysis of SCALE’s robustness, translation characteristics, latency costs and inherent language bias, providing solid foundation for future studies exploring the potential synergy between LLMs and more specialized models.</abstract>
      <url hash="6e32413d">2024.findings-acl.941</url>
      <bibkey>cheng-etal-2024-scale</bibkey>
      <doi>10.18653/v1/2024.findings-acl.941</doi>
    </paper>
    <paper id="942">
      <title>No perspective, no perception!! Perspective-aware Healthcare Answer Summarization</title>
      <author><first>Gauri</first><last>Naik</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <author><first>Sharad</first><last>Chandakacherla</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Shweta</first><last>Yadav</last><affiliation>University of Illinois at Chicago</affiliation></author>
      <author><first>Md Shad</first><last>Akhtar</last><affiliation>Indraprastha Institute of Information Technology, Delhi</affiliation></author>
      <pages>15919-15932</pages>
      <abstract>Healthcare Community Question Answering (CQA) forums offer an accessible platform for individuals seeking information on various healthcare-related topics. People find such platforms suitable for self-disclosure, seeking medical opinions, finding simplified explanations for their medical conditions, and answering others’ questions. However, answers on these forums are typically diverse and prone to off-topic discussions. It can be challenging for readers to sift through numerous answers and extract meaningful insights, making answer summarization a crucial task for CQA forums. While several efforts have been made to summarize the community answers, most of them are limited to the open domain and overlook the different perspectives offered by these answers. To address this problem, this paper proposes a novel task of perspective-specific answer summarization. We identify various perspectives, within healthcare-related responses and frame a perspective-driven abstractive summary covering all responses. To achieve this, we annotate 3167 CQA threads with 6193 perspective-aware summaries in our PUMA dataset. Further, we propose PLASMA, a prompt-driven controllable summarization model. To encapsulate the perspective-specific conditions, we design an energy-controlled loss function for the optimization. We also leverage the prefix tuner to learn the intricacies of the healthcare perspective summarization. Our evaluation against five baselines suggests the superior performance of PLASMA by a margin of ~1.5 - 21% improvement. We supplement our experiments with ablation and qualitative analysis.</abstract>
      <url hash="327e6155">2024.findings-acl.942</url>
      <bibkey>naik-etal-2024-perspective</bibkey>
      <doi>10.18653/v1/2024.findings-acl.942</doi>
    </paper>
    <paper id="943">
      <title>Retrieval-Augmented Retrieval: Large Language Models are Strong Zero-Shot Retriever</title>
      <author><first>Tao</first><last>Shen</last><affiliation>Oracle</affiliation></author>
      <author><first>Guodong</first><last>Long</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Xiubo</first><last>Geng</last><affiliation>Microsoft</affiliation></author>
      <author><first>Chongyang</first><last>Tao</last><affiliation>Beihang University</affiliation></author>
      <author><first>Yibin</first><last>Lei</last><affiliation>University of Amsterdam</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Michael</first><last>Blumenstein</last><affiliation>University of Technology Sydney</affiliation></author>
      <author><first>Daxin</first><last>Jiang</last><affiliation>Microsoft</affiliation></author>
      <pages>15933-15946</pages>
      <abstract>We propose a simple method that applies a large language model (LLM) to large-scale retrieval in zero-shot scenarios. Our method, the Large language model as Retriever (LameR), is built upon no other neural models but an LLM in a retrieval-augmented retrieval fashion, while breaking brute-force combinations of retrievers with LLMs and lifting the performance of zero-shot retrieval to be very competitive on benchmark datasets. Essentially, we propose to augment a query with its potential answers by prompting LLMs with a composition of the query and the query’s in-domain candidates. The candidates, regardless of correct or wrong, are obtained by a vanilla retrieval procedure on the target collection. As a part of the prompts, they are likely to help LLM generate more precise answers by pattern imitation or candidate summarization. Even if all the candidates are wrong, the prompts at least make LLM aware of in-collection patterns and genres. Moreover, due to the low performance of a self-supervised retriever, the LLM-based query augmentation becomes less effective as the retriever bottlenecks the whole pipeline. Therefore, we propose to leverage a non-parametric lexicon-based method (e.g., BM25) as the retrieval module to capture query-document overlap in a literal fashion. As such, LameR makes the retrieval procedure transparent to the LLM, thus circumventing the bottleneck.</abstract>
      <url hash="7b840b08">2024.findings-acl.943</url>
      <bibkey>shen-etal-2024-retrieval</bibkey>
      <doi>10.18653/v1/2024.findings-acl.943</doi>
    </paper>
    <paper id="944">
      <title>A Survey on Predicting the Factuality and the Bias of News Media</title>
      <author><first>Preslav</first><last>Nakov</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Jisun</first><last>An</last><affiliation>Indiana University</affiliation></author>
      <author><first>Haewoon</first><last>Kwak</last><affiliation>Indiana University</affiliation></author>
      <author><first>Muhammad Arslan</first><last>Manzoor</last></author>
      <author><first>Zain</first><last>Muhammad Mujahid</last></author>
      <author><first>Husrev</first><last>Sencar</last><affiliation>QCRI</affiliation></author>
      <pages>15947-15962</pages>
      <abstract>The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim or article, either manually or automatically. An increasing number of scholars are focusing on a coarser granularity, aiming to profile entire news outlets, which allows fast identification of potential “fake news” by checking the reliability of their source. Source factuality is also an important element of systems for automatic fact-checking and “fake news” detection, as they need to assess the reliability of the evidence they retrieve online. Political bias detection, which in the Western political landscape is about predicting left-center-right bias, is an equally important topic, which has experienced a similar shift toward profiling entire news outlets. Moreover, there is a clear connection between the two, as highly biased media are less likely to be factual; yet, the two problems have been addressed separately. In this survey, we review the state of the art on media profiling for factuality and bias, arguing for the need to model them jointly. We also shed light on some of the major challenges for modeling bias and factuality jointly. We further discuss interesting recent advances in using different information sources and modalities, which go beyond the text of the articles the target news outlet has published. Finally, we discuss current challenges and outline future research directions.</abstract>
      <url hash="01e05fe7">2024.findings-acl.944</url>
      <bibkey>nakov-etal-2024-survey</bibkey>
      <doi>10.18653/v1/2024.findings-acl.944</doi>
    </paper>
    <paper id="945">
      <title>Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform</title>
      <author><first>Rana</first><last>Salama</last><affiliation>George Washington University</affiliation></author>
      <author><first>Abdou</first><last>Youssef</last><affiliation>George Washington University</affiliation></author>
      <author><first>Mona</first><last>Diab</last><affiliation>Carnegie Mellon University</affiliation></author>
      <pages>15963-15977</pages>
      <abstract>Wavelet transforms, a powerful mathematical tool, have been widely used in different domains, including Signal and Image processing, to unravel intricate patterns, enhance data representation, and extract meaningful features from data. Tangible results from their application suggest that Wavelet transforms can be applied to NLP capturing a variety of linguistic and semantic properties.In this paper, we empirically leverage the application of Discrete Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase the capabilities of DWT in analyzing embedding representations at different levels of resolution and compressing them while maintaining their overall quality.We assess the effectiveness of DWT embeddings on semantic similarity tasks to show how DWT can be used to consolidate important semantic information in an embedding vector. We show the efficacy of the proposed paradigm using different embedding models, including large language models, on downstream tasks. Our results show that DWT can reduce the dimensionality of embeddings by 50-93% with almost no change in performance for semantic similarity tasks, while achieving superior accuracy in most downstream tasks. Our findings pave the way for applying DWT to improve NLP applications.</abstract>
      <url hash="225a0d96">2024.findings-acl.945</url>
      <bibkey>salama-etal-2024-semantic</bibkey>
      <doi>10.18653/v1/2024.findings-acl.945</doi>
    </paper>
    <paper id="946">
      <title>Improving Multi-hop Logical Reasoning in Knowledge Graphs with Context-Aware Query Representation Learning</title>
      <author><first>Jeonghoon</first><last>Kim</last></author>
      <author><first>Heesoo</first><last>Jung</last></author>
      <author><first>Hyeju</first><last>Jang</last><affiliation>Indiana University/Purdue University at Indianapolis</affiliation></author>
      <author><first>Hogun</first><last>Park</last><affiliation>Sungkyunkwan University</affiliation></author>
      <pages>15978-15991</pages>
      <abstract>Multi-hop logical reasoning on knowledge graphs is a pivotal task in natural language processing, with numerous approaches aiming to answer First-Order Logic (FOL) queries. Recent geometry (e.g., box, cone) and probability (e.g., beta distribution)-based methodologies have effectively addressed complex FOL queries. However, a common challenge across these methods lies in determining accurate geometric bounds or probability parameters for these queries. The challenge arises because existing methods rely on linear sequential operations within their computation graphs, overlooking the logical structure of the query and the relation-induced information that can be gleaned from the relations of the query, which we call the context of the query. To address the problem, we propose a model-agnostic methodology that enhances the effectiveness of existing multi-hop logical reasoning approaches by fully integrating the context of the FOL query graph. Our approach distinctively discerns (1) the structural context inherent to the query structure and (2) the relation-induced context unique to each node in the query graph as delineated in the corresponding knowledge graph. This dual-context paradigm helps nodes within a query graph attain refined internal representations throughout the multi-hop reasoning steps. Through experiments on two datasets, our method consistently enhances the three multi-hop reasoning foundation models, achieving performance improvements of up to 19.5%. Our codes are available at https://github.com/kjh9503/caqr.</abstract>
      <url hash="d9dde969">2024.findings-acl.946</url>
      <bibkey>kim-etal-2024-improving-multi</bibkey>
      <doi>10.18653/v1/2024.findings-acl.946</doi>
    </paper>
    <paper id="947">
      <title><fixed-case>P</fixed-case>rog<fixed-case>G</fixed-case>en: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models</title>
      <author><first>Yuzhao</first><last>Heng</last></author>
      <author><first>Chunyuan</first><last>Deng</last><affiliation>Rice University</affiliation></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Yue</first><last>Yu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Yinghao</first><last>Li</last></author>
      <author><first>Rongzhi</first><last>Zhang</last><affiliation>Georgia Institute of Technology and Zhejiang University</affiliation></author>
      <author><first>Chao</first><last>Zhang</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>15992-16030</pages>
      <abstract>Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs’ challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being more cost-effective than existing alternatives.</abstract>
      <url hash="8e9d670e">2024.findings-acl.947</url>
      <bibkey>heng-etal-2024-proggen</bibkey>
      <doi>10.18653/v1/2024.findings-acl.947</doi>
    </paper>
    <paper id="948">
      <title>Defending <fixed-case>LLM</fixed-case>s against Jailbreaking Attacks via Backtranslation</title>
      <author><first>Yihan</first><last>Wang</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Zhouxing</first><last>Shi</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Andrew</first><last>Bai</last><affiliation>, University of California, Los Angeles</affiliation></author>
      <author><first>Cho-Jui</first><last>Hsieh</last><affiliation>Google and University of California, Los Angeles</affiliation></author>
      <pages>16031-16046</pages>
      <abstract>Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by “backtranslation”. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM’s response and not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiveness and efficiency. We empirically demonstrate that our defense significantly outperforms the baselines, in the cases that are hard for the baselines, and our defense also has little impact on the generation quality for benign input prompts. Our implementation is based on our library for LLM jailbreaking defense algorithms at <url>https://github.com/YihanWang617/llm-jailbreaking-defense</url>, and the code for reproducing our experiments is available at <url>https://github.com/YihanWang617/LLM-Jailbreaking-Defense-Backtranslation</url>.</abstract>
      <url hash="d7c79332">2024.findings-acl.948</url>
      <bibkey>wang-etal-2024-defending</bibkey>
      <doi>10.18653/v1/2024.findings-acl.948</doi>
    </paper>
    <paper id="949">
      <title>A Large Collection of Model-generated Contradictory Responses for Consistency-aware Dialogue Systems</title>
      <author><first>Shiki</first><last>Sato</last><affiliation>CyberAgent, Inc.</affiliation></author>
      <author><first>Reina</first><last>Akama</last><affiliation>Tohoku University and RIKEN</affiliation></author>
      <author><first>Jun</first><last>Suzuki</last><affiliation>Tohoku University</affiliation></author>
      <author><first>Kentaro</first><last>Inui</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence, RIKEN and Tohoku University</affiliation></author>
      <pages>16047-16062</pages>
      <abstract>Mitigating the generation of contradictory responses poses a substantial challenge in dialogue response generation. The quality and quantity of available contradictory response data play a vital role in suppressing these contradictions, offering two significant benefits. First, having access to large contradiction data enables a comprehensive examination of their characteristics. Second, data-driven methods to mitigate contradictions may be enhanced with large-scale contradiction data for training. Nevertheless, no attempt has been made to build an extensive collection of model-generated contradictory responses. In this paper, we build a large dataset of response generation models’ contradictions for the first time. Then, we acquire valuable insights into the characteristics of model-generated contradictions through an extensive analysis of the collected responses. Lastly, we also demonstrate how this dataset substantially enhances the performance of data-driven contradiction suppression methods.</abstract>
      <url hash="11251741">2024.findings-acl.949</url>
      <bibkey>sato-etal-2024-large</bibkey>
      <doi>10.18653/v1/2024.findings-acl.949</doi>
    </paper>
    <paper id="950">
      <title>Exploring Reasoning Biases in Large Language Models Through Syllogism: Insights from the <fixed-case>N</fixed-case>eu<fixed-case>BAROCO</fixed-case> Dataset</title>
      <author><first>Kentaro</first><last>Ozeki</last><affiliation>The University of Tokyo and Keio University</affiliation></author>
      <author><first>Risako</first><last>Ando</last><affiliation>Keio University</affiliation></author>
      <author><first>Takanobu</first><last>Morishita</last></author>
      <author><first>Hirohiko</first><last>Abe</last><affiliation>Keio University</affiliation></author>
      <author><first>Koji</first><last>Mineshima</last><affiliation>Keio University</affiliation></author>
      <author><first>Mitsuhiro</first><last>Okada</last></author>
      <pages>16063-16077</pages>
      <abstract>This paper explores the question of how accurately current large language models can perform logical reasoning in natural language, with an emphasis on whether these models exhibit reasoning biases similar to humans. Specifically, our study focuses on syllogistic reasoning, a form of deductive reasoning extensively studied in cognitive science as a natural form of human reasoning. We present a syllogism dataset called NeuBAROCO, which consists of syllogistic reasoning problems in English and Japanese. This dataset was originally designed for psychological experiments to assess human reasoning capabilities using various forms of syllogisms. Our experiments with leading large language models indicate that these models exhibit reasoning biases similar to humans, along with other error tendencies. Notably, there is significant room for improvement in reasoning problems where the relationship between premises and hypotheses is neither entailment nor contradiction. We also present experimental results and in-depth analysis using a new Chain-of-Thought prompting method, which asks LLMs to translate syllogisms into abstract logical expressions and then explain their reasoning process. Our analysis using this method suggests that the primary limitations of LLMs lie in the reasoning process itself rather than the interpretation of syllogisms.</abstract>
      <url hash="c45155ee">2024.findings-acl.950</url>
      <bibkey>ozeki-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.findings-acl.950</doi>
    </paper>
    <paper id="951">
      <title>Unveiling the Spectrum of Data Contamination in Language Model: A Survey from Detection to Remediation</title>
      <author><first>Chunyuan</first><last>Deng</last><affiliation>Rice University</affiliation></author>
      <author><first>Yilun</first><last>Zhao</last><affiliation>Yale University</affiliation></author>
      <author><first>Yuzhao</first><last>Heng</last></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Jiannan</first><last>Cao</last></author>
      <author><first>Xiangru</first><last>Tang</last><affiliation>Yale University</affiliation></author>
      <author><first>Arman</first><last>Cohan</last><affiliation>Yale University and Allen Institute for Artificial Intelligence</affiliation></author>
      <pages>16078-16092</pages>
      <abstract>Data contamination has garnered increased attention in the era of Large language models (LLMs) due to the reliance on extensive internet-derived training corpora. The issue of training corpus overlap with evaluation benchmarks—referred to as contamination—has been the focus of significant recent research. This body of work aims to identify contamination, understand its impacts, and explore mitigation strategies from diverse perspectives. However, comprehensive studies that provide a clear pathway from foundational concepts to advanced insights are lacking in this nascent field. Therefore, we present the first survey in the field of data contamination. We begin by examining the effects of data contamination across various stages and forms. We then provide a detailed analysis of current contamination detection methods, categorizing them to highlight their focus, assumptions, strengths, and limitations. We also discuss mitigation strategies, offering a clear guide for future research. This survey serves as a succinct overview of the most recent advancements in data contamination research, providing a straightforward guide for the benefit of future research endeavors.</abstract>
      <url hash="e825eb44">2024.findings-acl.951</url>
      <bibkey>deng-etal-2024-unveiling</bibkey>
      <doi>10.18653/v1/2024.findings-acl.951</doi>
    </paper>
    <paper id="952">
      <title><fixed-case>DIMSIM</fixed-case>: Distilled Multilingual Critics for <fixed-case>I</fixed-case>ndic Text Simplification</title>
      <author><first>Sneha</first><last>Mondal</last><affiliation>Google</affiliation></author>
      <author><first>Ritika</first><last>Ritika</last><affiliation>Google</affiliation></author>
      <author><first>Ashish</first><last>Agrawal</last></author>
      <author><first>Preethi</first><last>Jyothi</last><affiliation>Indian Institute of Technology Bombay</affiliation></author>
      <author><first>Aravindan</first><last>Raghuveer</last><affiliation>Google</affiliation></author>
      <pages>16093-16109</pages>
      <abstract>Self-correction techniques have recently emerged as a promising framework to improve the quality of responses generated by large language models (LLMs). Few-shot prompted LLMs act as critics to produce feedback for an input, which is further fed to a refiner (also an LLM) to produce an output. However, these critique-refine steps require multiple expensive LLM calls. To circumvent this large inference cost, we borrow inspiration from prior work on knowledge distillation and propose the use of critique distillation to train critic models. These are smaller sequence-to-sequence models that are trained on input-critique pairs generated by an LLM. We focus on the problem of text simplification for three Indian languages: Hindi, Bengali and Marathi. This task is a good fit for self-correction style techniques. It also hasn’t been systematically explored for Indian languages before. We train two separate critics that focus on lexical and structure complexity, and show that it is surprisingly more effective than using an LLM directly as a critic in both 0-shot and few-shot settings. We also show the benefits of training multilingual critics, as opposed to monolingual critics. Extensive human evaluations show that on average, raters find 80% of DIMSIM’s output to be simple and easy to read.</abstract>
      <url hash="3b8bd498">2024.findings-acl.952</url>
      <bibkey>mondal-etal-2024-dimsim</bibkey>
      <doi>10.18653/v1/2024.findings-acl.952</doi>
    </paper>
    <paper id="953">
      <title><fixed-case>MATTER</fixed-case>: Memory-Augmented Transformer Using Heterogeneous Knowledge Sources</title>
      <author><first>Dongkyu</first><last>Lee</last></author>
      <author><first>Chandana</first><last>Satya Prakash</last><affiliation>Amazon</affiliation></author>
      <author><first>Jack</first><last>FitzGerald</last><affiliation>Amazon</affiliation></author>
      <author><first>Jens</first><last>Lehmann</last><affiliation>Amazon, Technische Universität Dresden, University of Bonn and Fraunhofer IAIS</affiliation></author>
      <pages>16110-16121</pages>
      <abstract>Leveraging external knowledge is crucial for achieving high performance in knowledge-intensive tasks, such as question answering. The retrieve-and-read approach is widely adopted for integrating external knowledge into a language model. However, this approach suffers from increased computational cost and latency due to the long context length, which grows proportionally with the number of retrieved knowledge. Furthermore, existing retrieval-augmented models typically retrieve information from a single type of knowledge source, limiting their scalability to diverse knowledge sources with varying structures. In this work, we introduce an efficient memory-augmented transformer called MATTER, designed to retrieve relevant knowledge from multiple heterogeneous knowledge sources. Specifically, our model retrieves and reads from both unstructured sources (paragraphs) and semi-structured sources (QA pairs) in the form of fixed-length neural memories. We demonstrate that our model outperforms existing efficient retrieval-augmented models on popular QA benchmarks in terms of both accuracy and speed. Furthermore, MATTER achieves competitive results compared to conventional read-and-retrieve models while having 100x throughput during inference.</abstract>
      <url hash="bb4e815d">2024.findings-acl.953</url>
      <bibkey>lee-etal-2024-matter</bibkey>
      <doi>10.18653/v1/2024.findings-acl.953</doi>
    </paper>
    <paper id="954">
      <title>Ask <fixed-case>LLM</fixed-case>s Directly, “What shapes your bias?”: Measuring Social Bias in Large Language Models</title>
      <author><first>Jisu</first><last>Shin</last><affiliation>Korea Advanced Institute of Science &amp; Technology and Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Hoyun</first><last>Song</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Huije</first><last>Lee</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Soyeong</first><last>Jeong</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Jong</first><last>Park</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>16122-16143</pages>
      <abstract>Social bias is shaped by the accumulation of social perceptions towards targets across various demographic identities. To fully understand such social bias in large language models (LLMs), it is essential to consider the composite of social perceptions from diverse perspectives among identities. Previous studies have either evaluated biases in LLMs by indirectly assessing the presence of sentiments towards demographic identities in the generated text or measuring the degree of alignment with given stereotypes. These methods have limitations in directly quantifying social biases at the level of distinct perspectives among identities. In this paper, we aim to investigate how social perceptions from various viewpoints contribute to the development of social bias in LLMs. To this end, we propose a novel strategy to intuitively quantify these social perceptions and suggest metrics that can evaluate the social biases within LLMs by aggregating diverse social perceptions. The experimental results show the quantitative demonstration of the social attitude in LLMs by examining social perception. The analysis we conducted shows that our proposed metrics capture the multi-dimensional aspects of social bias, enabling a fine-grained and comprehensive investigation of bias in LLMs.</abstract>
      <url hash="3d81237b">2024.findings-acl.954</url>
      <bibkey>shin-etal-2024-ask</bibkey>
      <doi>10.18653/v1/2024.findings-acl.954</doi>
    </paper>
    <paper id="955">
      <title>Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting</title>
      <author><first>Yuwei</first><last>Xia</last></author>
      <author><first>Ding</first><last>Wang</last></author>
      <author><first>Qiang</first><last>Liu</last><affiliation>Institute of Automation, Chinese Academy of Sciences</affiliation></author>
      <author><first>Liang</first><last>Wang</last></author>
      <author><first>Shu</first><last>Wu</last><affiliation>Institute of automation, Chinese academy of science, Chinese Academy of Sciences</affiliation></author>
      <author><first>Xiao-Yu</first><last>Zhang</last><affiliation>Institute of Information Engineering, Chinese Academy of Sciences</affiliation></author>
      <pages>16144-16159</pages>
      <abstract>Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TKG prediction. To address the third issue, we design CoH as a plug-and-play module to enhance the performance of graph-based models for TKG prediction. Extensive experiments on three datasets and backbones demonstrate the effectiveness of CoH.</abstract>
      <url hash="b29b880f">2024.findings-acl.955</url>
      <bibkey>xia-etal-2024-chain</bibkey>
      <doi>10.18653/v1/2024.findings-acl.955</doi>
    </paper>
    <paper id="956">
      <title>Can <fixed-case>LLM</fixed-case>s Speak For Diverse People? Tuning <fixed-case>LLM</fixed-case>s via Debate to Generate Controllable Controversial Statements</title>
      <author><first>Ming</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jiuhai</first><last>Chen</last></author>
      <author><first>Lichang</first><last>Chen</last></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>16160-16176</pages>
      <abstract>Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning (“DEBATUNE”) pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATUNE, we curate the largest dataset of debate topics so far, which covers 710 controversial topics and corresponding arguments for each topic. Evaluations by the GPT-4 judge with a novel controversy controllability metric show that LLMs’ capability of generating diverse perspectives is significantly improved by DEBATUNE. Moreover, such controllability can be generalized to unseen topics, generating high-quality statements supporting controversial arguments.</abstract>
      <url hash="94ecb75c">2024.findings-acl.956</url>
      <bibkey>li-etal-2024-llms-speak</bibkey>
      <doi>10.18653/v1/2024.findings-acl.956</doi>
    </paper>
    <paper id="957">
      <title>Label-aware Hard Negative Sampling Strategies with Momentum Contrastive Learning for Implicit Hate Speech Detection</title>
      <author><first>Jaehoon</first><last>Kim</last><affiliation>Hanyang University</affiliation></author>
      <author><first>Seungwan</first><last>Jin</last></author>
      <author><first>Sohyun</first><last>Park</last></author>
      <author><first>Someen</first><last>Park</last></author>
      <author><first>Kyungsik</first><last>Han</last><affiliation>Hanyang University</affiliation></author>
      <pages>16177-16188</pages>
      <abstract>Detecting implicit hate speech that is not directly hateful remains a challenge. Recent research has attempted to detect implicit hate speech by applying contrastive learning to pre-trained language models such as BERT and RoBERTa, but the proposed models still do not have a significant advantage over cross-entropy loss-based learning. We found that contrastive learning based on randomly sampled batch data does not encourage the model to learn hard negative samples. In this work, we propose Label-aware Hard Negative sampling strategies (LAHN) that encourage the model to learn detailed features from hard negative samples, instead of naive negative samples in random batch, using momentum-integrated contrastive learning. LAHN outperforms the existing models for implicit hate speech detection both in- and cross-datasets. The code is available at https://github.com/Hanyang-HCC-Lab/LAHN</abstract>
      <url hash="63439216">2024.findings-acl.957</url>
      <bibkey>kim-etal-2024-label</bibkey>
      <doi>10.18653/v1/2024.findings-acl.957</doi>
    </paper>
    <paper id="958">
      <title>Selective Reflection-Tuning: Student-Selected Data Recycling for <fixed-case>LLM</fixed-case> Instruction-Tuning</title>
      <author><first>Ming</first><last>Li</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Lichang</first><last>Chen</last></author>
      <author><first>Jiuhai</first><last>Chen</last></author>
      <author><first>Shwai</first><last>He</last><affiliation>University of Maryland, College Park</affiliation></author>
      <author><first>Jiuxiang</first><last>Gu</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tianyi</first><last>Zhou</last><affiliation>University of Maryland, College Park</affiliation></author>
      <pages>16189-16211</pages>
      <abstract>Instruction tuning is critical to large language models (LLMs) for achieving better instruction following and task adaptation capabilities but its success heavily relies on the training data quality. Many recent methods focus on improving the data quality but often overlook the compatibility of the data with the student model being finetuned. This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher LLM’s reflection and introspection for improving existing data quality with the data selection capability of the student LLM, to automatically refine existing instruction-tuning data. This teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance. Selective Reflection-Tuning is a data augmentation and synthesis that generally improves LLM finetuning and self-improvement without collecting brand-new data. We apply our method to Alpaca and WizardLM data and achieve much stronger and top-tier 7B and 13B LLMs.</abstract>
      <url hash="d7c8ee7e">2024.findings-acl.958</url>
      <bibkey>li-etal-2024-selective</bibkey>
      <doi>10.18653/v1/2024.findings-acl.958</doi>
    </paper>
    <paper id="959">
      <title>Selective Prompting Tuning for Personalized Conversations with <fixed-case>LLM</fixed-case>s</title>
      <author><first>Qiushi</first><last>Huang</last></author>
      <author><first>Xubo</first><last>Liu</last><affiliation>Stability AI</affiliation></author>
      <author><first>Tom</first><last>Ko</last><affiliation>ByteDance AI Lab</affiliation></author>
      <author><first>Bo</first><last>Wu</last><affiliation>MIT-IBM Watson AI Lab</affiliation></author>
      <author><first>Wenwu</first><last>Wang</last><affiliation>University of Surrey</affiliation></author>
      <author><first>Yu</first><last>Zhang</last><affiliation>Southern University of Science and Technology</affiliation></author>
      <author><first>Lilian</first><last>Tang</last><affiliation>University of Surrey</affiliation></author>
      <pages>16212-16226</pages>
      <abstract>In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models’ (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose **S**elective **P**rompt **T**uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation. The SPT model code is [publicly available](https://github.com/hqsiswiliam/SPT) for further exploration.</abstract>
      <url hash="0ee1bc5f">2024.findings-acl.959</url>
      <bibkey>huang-etal-2024-selective</bibkey>
      <doi>10.18653/v1/2024.findings-acl.959</doi>
    </paper>
    <paper id="960">
      <title>Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models</title>
      <author><first>Rima</first><last>Hazra</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <author><first>Sayan</first><last>Layek</last></author>
      <author><first>Somnath</first><last>Banerjee</last><affiliation>IIT Kharagpur</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>16227-16239</pages>
      <abstract>In the rapidly advancing field of artificial intelligence, the concept of ‘Red-Teaming’ or ‘Jailbreaking’ large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model’s foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset <tex-math>NicheHazardQA</tex-math> to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model’s safety metrics and guardrails. Our findings show that model editing serves as a cost-effective tool for topical red-teaming by methodically applying targeted edits and evaluating the resultant model behavior.</abstract>
      <url hash="f309d535">2024.findings-acl.960</url>
      <bibkey>hazra-etal-2024-sowing</bibkey>
      <doi>10.18653/v1/2024.findings-acl.960</doi>
    </paper>
    <paper id="961">
      <title><fixed-case>C</fixed-case>ontext<fixed-case>BLIP</fixed-case>: Doubly Contextual Alignment for Contrastive Image Retrieval from Linguistically Complex Descriptions</title>
      <author><first>Honglin</first><last>Lin</last></author>
      <author><first>Siyu</first><last>Li</last></author>
      <author><first>Guoshun</first><last>Nan</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Chaoyue</first><last>Tang</last></author>
      <author><first>Xueting</first><last>Wang</last></author>
      <author><first>Jingxin</first><last>Xu</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Rong</first><last>Yankai</last></author>
      <author><first>Zhouzhili</first><last>Zhouzhili</last><affiliation>Guangzhou University</affiliation></author>
      <author><first>Yutong</first><last>Gao</last><affiliation>Beijing jiaotong univercity, National Taipei University of Technology, Northeastern University and Minzu University of China</affiliation></author>
      <author><first>Qimei</first><last>Cui</last><affiliation>Beijing University of Posts and Telecommunications</affiliation></author>
      <author><first>Xiaofeng</first><last>Tao</last></author>
      <pages>16240-16258</pages>
      <abstract>Image retrieval from contextual descriptions (IRCD) aims to identify an image within a set of minimally contrastive candidates based on linguistically complex text. Despite the success of VLMs, they still significantly lag behind human performance in IRCD. The main challenges lie in aligning key contextual cues in two modalities, where these subtle cues are concealed in tiny areas of multiple contrastive images and within the complex linguistics of textual descriptions. This motivates us to propose ContextBLIP, a simple yet effective method that relies on a doubly contextual alignment scheme for challenging IRCD. Specifically, 1) our model comprises a multi-scale adapter, a matching loss, and a text-guided masking loss. The adapter learns to capture fine-grained visual cues. The two losses enable iterative supervision for the adapter, gradually highlighting the focal patches of a single image to the key textual cues. We term such a way as intra-contextual alignment. 2) Then, ContextBLIP further employs an inter-context encoder to learn dependencies among candidates, facilitating alignment between the text to multiple images. We term this step as inter-contextual alignment. Consequently, the nuanced cues concealed in each modality can be effectively aligned. Experiments on two benchmarks show the superiority of our method. We observe that ContextBLIP can yield comparable results with GPT-4V, despite involving about 7,500 times fewer parameters.</abstract>
      <url hash="96ab2c79">2024.findings-acl.961</url>
      <bibkey>lin-etal-2024-contextblip</bibkey>
      <doi>10.18653/v1/2024.findings-acl.961</doi>
    </paper>
    <paper id="962">
      <title><fixed-case>P</fixed-case>uzzle<fixed-case>VQA</fixed-case>: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns</title>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Vernon</first><last>Toh</last></author>
      <author><first>Deepanway</first><last>Ghosal</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Lidong</first><last>Bing</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Soujanya</first><last>Poria</last><affiliation>Singapore University of Technology and Design</affiliation></author>
      <pages>16259-16273</pages>
      <abstract>Large multimodal models extend the impressive capabilities of large language models by integrating multimodal understanding abilities. However, it is not clear how they can emulate the general intelligence and reasoning ability of humans. As recognizing patterns and abstracting concepts are key to general intelligence, we introduce PuzzleVQA, a collection of 2000 puzzle instances based on abstract patterns. With this dataset, we evaluate large multimodal models with abstract patterns based on fundamental concepts, including colors, numbers, sizes, and shapes. Through our experiments on state-of-the-art large multimodal models, we find that they are not able to generalize well to simple abstract patterns. Notably, GPT-4V achieves a score of 46.4% on single-concept puzzles, which shows that state-of-the-art models struggle on our dataset. To diagnose the reasoning challenges in large multimodal models, we progressively guide the models with our ground truth reasoning explanations for visual perception, inductive reasoning, and deductive reasoning. Our systematic analysis finds that the main bottlenecks of GPT-4V are weaker visual perception and inductive reasoning abilities. Through this work, we hope to shed light on the limitations of large multimodal models and how they can better emulate human cognitive processes in the future.</abstract>
      <url hash="b9e86cf2">2024.findings-acl.962</url>
      <bibkey>chia-etal-2024-puzzlevqa</bibkey>
      <revision id="1" href="2024.findings-acl.962v1" hash="0135c366"/>
      <revision id="2" href="2024.findings-acl.962v2" hash="b9e86cf2" date="2024-08-17">Added Acknowledgments.</revision>
      <doi>10.18653/v1/2024.findings-acl.962</doi>
    </paper>
    <paper id="963">
      <title>How Do Moral Emotions Shape Political Participation? A Cross-Cultural Analysis of Online Petitions Using Language Models</title>
      <author><first>Jaehong</first><last>Kim</last></author>
      <author><first>Chaeyoon</first><last>Jeong</last></author>
      <author><first>Seongchan</first><last>Park</last></author>
      <author><first>Meeyoung</first><last>Cha</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Wonjae</first><last>Lee</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>16274-16289</pages>
      <abstract>Understanding the interplay between emotions in language and user behaviors is critical. We study how moral emotions shape the political participation of users based on cross-cultural online petition data. To quantify moral emotions, we employ a context-aware NLP model that is designed to capture the subtle nuances of emotions across cultures. For model training, we construct and share a moral emotion dataset comprising nearly 50,000 petition sentences in Korean and English each, along with emotion labels annotated by a fine-tuned LLM. We examine two distinct types of user participation: general support (i.e., registered signatures of petitions) and active support (i.e., sharing petitions on social media). We discover that moral emotions like other-suffering increase both forms of participation and help petitions go viral, while self-conscious have the opposite effect. The most prominent moral emotion, other-condemning, led to polarizing responses among the audience. In contrast, other-praising was perceived differently by culture; it led to a rise in active support in Korea but a decline in the UK. Our findings suggest that both moral emotions embedded in language and cultural perceptions are critical to shaping the public’s political discourse.</abstract>
      <url hash="3e4f586e">2024.findings-acl.963</url>
      <bibkey>kim-etal-2024-moral</bibkey>
      <doi>10.18653/v1/2024.findings-acl.963</doi>
    </paper>
    <paper id="964">
      <title><fixed-case>V</fixed-case>illager<fixed-case>A</fixed-case>gent: A Graph-Based Multi-Agent Framework for Coordinating Complex Task Dependencies in <fixed-case>M</fixed-case>inecraft</title>
      <author><first>Yubo</first><last>Dong</last></author>
      <author><first>Xukun</first><last>Zhu</last><affiliation>Tongji University</affiliation></author>
      <author><first>Zhengzhe</first><last>Pan</last></author>
      <author><first>Linchao</first><last>Zhu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>16290-16314</pages>
      <abstract>In this paper, we aim to evaluate multi-agent systems against complex dependencies, including spatial, causal, and temporal constraints. First, we construct a new benchmark, named VillagerBench, within the Minecraft environment. VillagerBench comprises diverse tasks crafted to test various aspects of multi-agent collaboration, from workload distribution to dynamic adaptation and synchronized task execution. Second, we introduce a Directed Acyclic Graph Multi-Agent Framework (VillagerAgent) to resolve complex inter-agent dependencies and enhance collaborative efficiency. This solution incorporates a task decomposer that creates a directed acyclic graph (DAG) for structured task management, an agent controller for task distribution, and a state manager for tracking environmental and agent data.Our empirical evaluation on VillagerBench demonstrates that VillagerAgentoutperforms the existing AgentVerse model, reducing hallucinations and improving task decomposition efficacy. The results underscore VillagerAgent’s potential in advancing multi-agent collaboration, offering a scalable and generalizable solution in dynamic environments. Source code is open-source on GitHub.</abstract>
      <url hash="34ada041">2024.findings-acl.964</url>
      <bibkey>dong-etal-2024-villageragent</bibkey>
      <doi>10.18653/v1/2024.findings-acl.964</doi>
    </paper>
    <paper id="965">
      <title><fixed-case>CF</fixed-case>-<fixed-case>TCIR</fixed-case>: A Compositor-Free Framework for Hierarchical Text-Conditioned Image Retrieval</title>
      <author><first>Yuchen</first><last>Yang</last></author>
      <author><first>Yu</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <author><first>Yanfeng</first><last>Wang</last><affiliation>Shanghai Jiao Tong University</affiliation></author>
      <pages>16315-16325</pages>
      <abstract>In text-conditioned image retrieval (TCIR), the combination of a reference image and modification text forms a query tuple, aiming to locate the most congruent target image within a dataset. The advantages of rich image semantic information and text flexibility are combined in this manner for more accurate retrieval. While traditional techniques often employ attention-driven compositors to craft a unified image-text representation, our paper introduces a compositor-free framework, CF-TCIR, which eschews the standard compositor. Compositor-based methods are designed to learn a joint representation of images and text, but they struggle to directly capture the correlations between attributes across the image and text modalities. Instead, we reformulate the retrieval process as a cross-modal interaction between a synthesized image feature and its corresponding text descriptor. This novel methodology offers advantages in terms of computational efficiency, scalability, and superior performance. To optimize the retrieval performance, we advocate a tiered retrieval mechanism, blending both coarse-grain and fine-grain paradigms. Moreover, to enrich the contextual relationship within the query tuple, we integrate a generative cross-modal alignment technique, ensuring synchronization of sequential attributes between image and text data.</abstract>
      <url hash="8ff880aa">2024.findings-acl.965</url>
      <bibkey>yang-etal-2024-cf</bibkey>
      <doi>10.18653/v1/2024.findings-acl.965</doi>
    </paper>
    <paper id="966">
      <title><fixed-case>DMIN</fixed-case>: A Discourse-specific Multi-granularity Integration Network for Conversational Aspect-based Sentiment Quadruple Analysis</title>
      <author><first>Peijie</first><last>Huang</last><affiliation>South China Agricultural University</affiliation></author>
      <author><first>Xisheng</first><last>Xiao</last><affiliation>South China Agricultural University</affiliation></author>
      <author><first>Yuhong</first><last>Xu</last><affiliation>South China Agricultural University</affiliation></author>
      <author><first>Jiawei</first><last>Chen</last></author>
      <pages>16326-16338</pages>
      <abstract>Conversational Aspect-based Sentiment Quadruple Analysis (DiaASQ) aims to extract fine-grained sentiment quadruples from dialogues. Previous research has primarily concentrated on enhancing token-level interactions, still lacking in sufficient modeling of the discourse structure information in dialogue. Firstly, it does not incorporate interactions among different utterances in the encoding stage, resulting in a limited token-level context understanding for subsequent modules. Secondly, it ignores the critical fact that discourse information is naturally organized at the utterance level and learning it solely at the token level is incomplete. In this work, we strengthen the token-level encoder by utilizing a discourse structure called “thread” and graph convolutional networks to enhance the token interaction among different utterances. Moreover, we propose an utterance-level encoder to learn the structured speaker and reply information, providing a macro understanding of dialogue discourse. Furthermore, we introduce a novel Multi-granularities Integrator to integrate token-level and utterance-level representations, resulting in a comprehensive and cohesive dialogue contextual understanding. Experiments on two datasets demonstrate that our model achieves state-of-the-art performance. Our codes are publicly available at https://github.com/SIGSDSscau/DMIN.</abstract>
      <url hash="f83d6a59">2024.findings-acl.966</url>
      <bibkey>huang-etal-2024-dmin</bibkey>
      <doi>10.18653/v1/2024.findings-acl.966</doi>
    </paper>
    <paper id="967">
      <title>Are Decoder-Only Language Models Better than Encoder-Only Language Models in Understanding Word Meaning?</title>
      <author><first>Muhammad</first><last>Qorib</last></author>
      <author><first>Geonsik</first><last>Moon</last><affiliation>National University of Singapore</affiliation></author>
      <author><first>Hwee Tou</first><last>Ng</last><affiliation>National University of Singapore</affiliation></author>
      <pages>16339-16347</pages>
      <abstract>The natural language processing field has been evolving around language models for the past few years, from the usage of n-gram language models for re-ranking, to transfer learning with encoder-only (BERT-like) language models, and finally to large language models (LLMs) as general solvers. LLMs are dominated by the decoder-only type, and they are popular for their efficacy in numerous tasks. LLMs are regarded as having strong comprehension abilities and strong capabilities to solve new unseen tasks. As such, people may quickly assume that decoder-only LLMs always perform better than the encoder-only ones, especially for understanding word meaning. In this paper, we demonstrate that decoder-only LLMs perform worse on word meaning comprehension than an encoder-only language model that has vastly fewer parameters.</abstract>
      <url hash="ebc2c15d">2024.findings-acl.967</url>
      <bibkey>qorib-etal-2024-decoder</bibkey>
      <doi>10.18653/v1/2024.findings-acl.967</doi>
    </paper>
    <paper id="968">
      <title><fixed-case>F</fixed-case>rag<fixed-case>R</fixed-case>el: Exploiting Fragment-level Relations in the External Memory of Large Language Models</title>
      <author><first>Xihang</first><last>Yue</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Linchao</first><last>Zhu</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Yi</first><last>Yang</last><affiliation>Zhejiang University</affiliation></author>
      <pages>16348-16361</pages>
      <abstract>To process contexts with unlimited length using Large Language Models (LLMs), recent studies explore hierarchically managing the long text. Only several text fragments are taken from the external memory and passed into the temporary working memory, i.e., LLM’s context window. However, existing approaches isolatedly handle the text fragments without considering their structural connections, thereby suffering limited capability on texts with intensive inter-relations, e.g., coherent stories and code repositories. This work attempts to resolve this by exploiting the fragment-level relations in external memory. First, we formulate the fragment-level relations and present several instantiations for different text types. Next, we introduce a relation-aware fragment assessment criteria upon previous independent fragment assessment. Finally, we present the fragment-connected Hierarchical Memory based LLM. We validate the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting.</abstract>
      <url hash="df68d8c5">2024.findings-acl.968</url>
      <bibkey>yue-etal-2024-fragrel</bibkey>
      <doi>10.18653/v1/2024.findings-acl.968</doi>
    </paper>
    <paper id="969">
      <title>On the Robustness of Document-Level Relation Extraction Models to Entity Name Variations</title>
      <author><first>Shiao</first><last>Meng</last><affiliation>Tsinghua University</affiliation></author>
      <author><first>Xuming</first><last>Hu</last><affiliation>The Hong Kong University of Science and Technology (Guangzhou) and Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Aiwei</first><last>Liu</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Fukun</first><last>Ma</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Yawen</first><last>Yang</last><affiliation>Tsinghua University, Tsinghua University</affiliation></author>
      <author><first>Shuang</first><last>Li</last><affiliation>Tencent</affiliation></author>
      <author><first>Lijie</first><last>Wen</last><affiliation>School of Software, Tsinghua University</affiliation></author>
      <pages>16362-16374</pages>
      <abstract>Driven by the demand for cross-sentence and large-scale relation extraction, document-level relation extraction (DocRE) has attracted increasing research interest. Despite the continuous improvement in performance, we find that existing DocRE models which initially perform well may make more mistakes when merely changing the entity names in the document, hindering the generalization to novel entity names. To this end, we systematically investigate the robustness of DocRE models to entity name variations in this work. We first propose a principled pipeline to generate entity-renamed documents by replacing the original entity names with names from Wikidata. By applying the pipeline to DocRED and Re-DocRED datasets, we construct two novel benchmarks named Env-DocRED and Env-Re-DocRED for robustness evaluation. Experimental results show that both three representative DocRE models and two in-context learned large language models consistently lack sufficient robustness to entity name variations, particularly on cross-sentence relation instances and documents with more entities. Finally, we propose an entity variation robust training method which not only improves the robustness of DocRE models but also enhances their understanding and reasoning capabilities. We further verify that the basic idea of this method can be effectively transferred to in-context learning for DocRE as well.</abstract>
      <url hash="e16a1297">2024.findings-acl.969</url>
      <bibkey>meng-etal-2024-robustness</bibkey>
      <doi>10.18653/v1/2024.findings-acl.969</doi>
    </paper>
    <paper id="970">
      <title><fixed-case>RESEMO</fixed-case>: A Benchmark <fixed-case>C</fixed-case>hinese Dataset for Studying Responsive Emotion from Social Media Content</title>
      <author><first>Bo</first><last>Hu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Chenfei</first><last>Xie</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yuanhe</first><last>Tian</last><affiliation>University of Washington, Seattle</affiliation></author>
      <author><first>Yan</first><last>Song</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhendong</first><last>Mao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <pages>16375-16387</pages>
      <abstract>On social media platforms, users’ emotions are triggered when they encounter particular content from other users,where such emotions are different from those that spontaneously emerged, owing to the “responsive” nature. Analyzing the aforementioned responsive emotions from user interactions is a task of significant importance for understanding human cognition, the mechanisms of emotion generation, and behavior on the Internet, etc. Performing the task with artificial intelligence generally requires human-annotated data to help train a well-performing system, while existing data resources do not cover this specific area, with none of them focusing on responsive emotion analysis. In this paper, we propose a Chinese dataset named ResEmo for responsive emotion analysis, including 3813 posts with 68,781 comments collected from Weibo, the largest social media platform in China. ResEmo contains three types of human annotations with respect to responsive emotions, namely, responsive relationship, responsive emotion cause, and responsive emotion category. Moreover, to test this dataset, we build large language model (LLM) baseline methods for responsive relation extraction, responsive emotion cause extraction, and responsive emotion detection, which show the potential of the proposed ResEmo being a benchmark for future studies on responsive emotions.</abstract>
      <url hash="36bce09d">2024.findings-acl.970</url>
      <bibkey>hu-etal-2024-resemo</bibkey>
      <doi>10.18653/v1/2024.findings-acl.970</doi>
    </paper>
    <paper id="971">
      <title><fixed-case>EHR</fixed-case>-<fixed-case>S</fixed-case>eq<fixed-case>SQL</fixed-case> : A Sequential Text-to-<fixed-case>SQL</fixed-case> Dataset For Interactively Exploring Electronic Health Records</title>
      <author><first>Jaehee</first><last>Ryu</last></author>
      <author><first>Seonhee</first><last>Cho</last><affiliation>Korea Advanced Institute of Science &amp; Technology</affiliation></author>
      <author><first>Gyubok</first><last>Lee</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Edward</first><last>Choi</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <pages>16388-16407</pages>
      <abstract>In this paper, we introduce EHR-SeqSQL, a novel sequential text-to-SQL dataset for Electronic Health Record (EHR) databases. EHR-SeqSQL is designed to address critical yet underexplored aspects in text-to-SQL parsing: interactivity, compositionality, and efficiency. To the best of our knowledge, EHR-SeqSQL is not only the largest but also the first medical text-to-SQL dataset benchmark to include sequential and contextual questions. We provide a data split and the new test set designed to assess compositional generalization ability. Our experiments demonstrate the superiority of a multi-turn approach over a single-turn approach in learning compositionality. Additionally, our dataset integrates specially crafted tokens into SQL queries to improve execution efficiency. With EHR-SeqSQL, we aim to bridge the gap between practical needs and academic research in the text-to-SQL domain.</abstract>
      <url hash="aeb6445e">2024.findings-acl.971</url>
      <bibkey>ryu-etal-2024-ehr</bibkey>
      <doi>10.18653/v1/2024.findings-acl.971</doi>
    </paper>
    <paper id="972">
      <title><fixed-case>KEEP</fixed-case> <fixed-case>CHATTING</fixed-case>! An Attractive Dataset for Continuous Conversation Agents</title>
      <author><first>Yihe</first><last>Wang</last></author>
      <author><first>Jin</first><last>Liu</last><affiliation>Wuhan University</affiliation></author>
      <author><first>Yao</first><last>Wan</last><affiliation>Huazhong University of Science and Technology</affiliation></author>
      <author><first>Yitong</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Zifeng</first><last>Liu</last></author>
      <author><first>Weipeng</first><last>Chen</last></author>
      <pages>16408-16414</pages>
      <abstract>Ongoing chatting is an important step for conversational agents to build long-term connections with people. However, people tend to quickly lose interest in chatting if the conversational agent’s words are not engaging enough. In this paper, we present a novel task of increasing users’ willingness to continue talking to the agent.We collect a dataset named ContinuousChat by: (i) collecting personas and revising them, and then expanding the personas to detailed-personas through experiences, daily life, future plans, or interesting stories; (ii) expanding detailed-personas into the dialogues, and inject emotions and feelings into them; (iii) rewriting the dialogues in specific styles through few-shot prompt, conditioning on handwritten style-specific examples.We benchmark LLMs on ContinuousChat Dataset using both fine-tuning and in-context learning settings. Experiments over publicly available models demonstrate that although there is substantial room for improvement in generating style-specific dialogues, our ContinuousChat dataset is valuable in guiding conversational agents to generate more attractive dialogues and increase users’ willingness to continue the conversations.</abstract>
      <url hash="1e36c1ad">2024.findings-acl.972</url>
      <bibkey>wang-etal-2024-keep</bibkey>
      <doi>10.18653/v1/2024.findings-acl.972</doi>
    </paper>
    <paper id="973">
      <title><fixed-case>R</fixed-case>e<fixed-case>P</fixed-case>air: Automated Program Repair with Process-based Feedback</title>
      <author><first>Yuze</first><last>Zhao</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Zhenya</first><last>Huang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Yixiao</first><last>Ma</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Hao</first><last>Jiang</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Qi</first><last>Liu</last><affiliation>University of Science and Technology of China</affiliation></author>
      <author><first>Linbo</first><last>Zhu</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <pages>16415-16429</pages>
      <abstract>The gap between the trepidation of program reliability and the expense of repairs underscore the indispensability for Automated Program Repair (APR). APR is instrumental in transforming vulnerable programs into more robust ones, bolstering program reliability while simultaneously diminishing the financial burden of manual repairs. Commercial-scale language models (LM) have taken APR to unprecedented levels. However, due to the limitations of model capabilities by parameters, a one-step substantial modification may not achieve the desired effect for models with parameters less than 100B. Moreover, humans interact with the LLM through explicit prompts, which hinders the LLM from receiving feedback from compiler and test cases to automatically optimize its repair policies. Explicit prompts from humans not only increase additional manpower costs, but also pose potential misunderstandings between human’s intent and LMs.Based on the above considerations, we are exploring how to ensure small-scale LM still outperform through process supervision and feedback. We start by constructing a dataset named CodeNet4Repair, replete with multiple repair records, which supervises the fine-tuning of a foundational mode. Building upon the encouraging outcomes of reinforcement learning, we develop a reward model that serves as a critic, providing feedback for the fine-tuned LM’s action, progressively optimizing its policy. During inference, we require the LM to generate solutions iteratively until the repair effect no longer improves or hits the maximum step limit. The experimental results show that this process-based feedback not only outperforms larger outcome-based generation methods, but also nearly matches the performance of closed-source commercial large-scale LMs.</abstract>
      <url hash="4dd3bf5e">2024.findings-acl.973</url>
      <bibkey>zhao-etal-2024-repair</bibkey>
      <doi>10.18653/v1/2024.findings-acl.973</doi>
    </paper>
    <paper id="974">
      <title>Concise and Precise Context Compression for Tool-Using Language Models</title>
      <author><first>Yang</first><last>Xu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Yunlong</first><last>Feng</last></author>
      <author><first>Honglin</first><last>Mu</last><affiliation>Harbin Institute Of Technology</affiliation></author>
      <author><first>Yutai</first><last>Hou</last></author>
      <author><first>Yitong</first><last>Li</last><affiliation>Huawei Technologies Co., Ltd.</affiliation></author>
      <author><first>Xinghao</first><last>Wang</last></author>
      <author><first>Wanjun</first><last>Zhong</last><affiliation>ByteDance Inc.</affiliation></author>
      <author><first>Zhongyang</first><last>Li</last></author>
      <author><first>Dandan</first><last>Tu</last></author>
      <author><first>Qingfu</first><last>Zhu</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <author><first>Min</first><last>Zhang</last><affiliation>Harbin Institute of Technology, Shenzhen</affiliation></author>
      <author><first>Wanxiang</first><last>Che</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>16430-16441</pages>
      <abstract>Through reading the documentation in the context, tool-using language models can dynamically extend their capability using external tools. The cost is that we have to input lengthy documentation every time the model needs to use the tool, occupying the input window as well as slowing down the decoding process.Given the progress in general-purpose compression, soft context compression is a suitable approach to alleviate the problem. However, when compressing tool documentation, existing methods suffer from the weaknesses of key information loss (specifically, tool/parameter name errors) and difficulty in adjusting the length of compressed sequences based on documentation lengths.To address these problems, we propose two strategies for compressing tool documentation into concise and precise summary sequences for tool-using language models. 1) Selective compression strategy mitigates key information loss by deliberately retaining key information as raw text tokens. 2) Block compression strategy involves dividing tool documentation into short chunks and then employing a fixed-length compression model to achieve variable-length compression. This strategy facilitates the flexible adjustment of the compression ratio.Results on API-Bank and APIBench show that our approach reaches a performance comparable to the upper-bound baseline under up to 16x compression ratio.</abstract>
      <url hash="270b8046">2024.findings-acl.974</url>
      <bibkey>xu-etal-2024-concise</bibkey>
      <doi>10.18653/v1/2024.findings-acl.974</doi>
    </paper>
    <paper id="975">
      <title><fixed-case>M</fixed-case>ed<fixed-case>D</fixed-case>ec: A Dataset for Extracting Medical Decisions from Discharge Summaries</title>
      <author><first>Mohamed</first><last>Elgaar</last><affiliation>University of Massachusetts, Lowell</affiliation></author>
      <author><first>Jiali</first><last>Cheng</last><affiliation>University of Massachusetts at Lowell</affiliation></author>
      <author><first>Nidhi</first><last>Vakil</last><affiliation>University of Massachusetts, Lowell</affiliation></author>
      <author><first>Hadi</first><last>Amiri</last><affiliation>University of Massachusetts Lowell</affiliation></author>
      <author><first>Leo Anthony</first><last>Celi</last><affiliation>Massachusetts Institute of Technology and Beth Israel Deaconess Medical Center</affiliation></author>
      <pages>16442-16455</pages>
      <abstract>Medical decisions directly impact individuals’ health and well-being. Extracting decision spans from clinical notes plays a crucial role in understanding medical decision-making processes. In this paper, we develop a new dataset called “MedDec,” which contains clinical notes of eleven different phenotypes (diseases) annotated by ten types of medical decisions. We introduce the task of medical decision extraction, aiming to jointly extract and classify different types of medical decisions within clinical notes. We provide a comprehensive analysis of the dataset, develop a span detection model as a baseline for this task, evaluate recent span detection approaches, and employ a few metrics to measure the complexity of data samples. Our findings shed light on the complexities inherent in clinical decision extraction and enable future work in this area of research. The dataset and code are available through https://github.com/CLU-UML/MedDec.</abstract>
      <url hash="02ba40ce">2024.findings-acl.975</url>
      <bibkey>elgaar-etal-2024-meddec</bibkey>
      <revision id="1" href="2024.findings-acl.975v1" hash="ee1cb169"/>
      <revision id="2" href="2024.findings-acl.975v2" hash="02ba40ce" date="2024-08-23">Dataset and results update.</revision>
      <doi>10.18653/v1/2024.findings-acl.975</doi>
    </paper>
  </volume>
</collection>
